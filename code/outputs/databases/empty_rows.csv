,Paper Title,Header Number,Header Title,Text,Volume
6,AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_19.,1
17,Unsupervised Domain Adaptation for Anatomical Landmark Detection (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_66.,1
22,"CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image (vol1)",,Stage 1. Let's denote the downsampling function as D(•).,"In this stage, we pretrain the SRNet using the HR CT images, as shown in Fig.  Stage 3. The MR images generated by the model pretrained at the first two stages can be further improved. In stage 3, we conduct joint optimization of the SRNet and the CITNet as shown in Fig.  and the SRNet by minimizing L disentangle as defined in Eq.  The training procedure of our method is illustrated by Algorithm 1. Implementation Details. To train the proposed network, each training sample is unpaired LR MRI and HR CT images. All images are normalized to the range between -1.0 and 1.0. Optimization is performed using Adam with a batch size of 1. The initial learning rate is set to 0.0001 and decreased by a factor of 5 every 2 epochs. We empirically set λ 1 = 10, λ 2 = λ 3 = 1 and T = 100, 000. Table  Table  Our method is trained in two pretrain stages and one joint optimization stage. We thus conduct ablation study on dataset from Site1 to analyze the quality of the generated pseudo HR MR images at each stage. As shown in Table ",1
32,Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_9.,1
35,Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities (vol1),,Multi-modal Normative Modelling.,"We propose two mVAE normative modelling frameworks shown in Fig.  To compare our normVAE models to a classical normative approach, we trained one GPR (using the PCNToolkit) per feature on a sub-set of 2000 healthy UK Biobank individuals and used extreme value statistics to calculate subjectlevel abnormality index  2 . Kumar et al.  where μ norm d norm ij is the mean and σ norm d norm ij the standard deviation of the deviations d norm ij of a holdout healthy control cohort. However, in the multi-modal setting, feature space-based deviation metrics may not highlight the benefits of multi-modal models over their uni-modal counterparts. The goal of the joint latent representation is to capture information from all modalities. Thus, decoders for each modality must extract the information from the joint latent representation, which now carries information from all other modalities as well. Therefore, data reconstructions capture only information relevant to a particular modality and may also be poorer compared to uni-modal methods. As such, particularly when incorporating modalities with a high degree of modality-specific variation, we believe latent space deviation metrics would better capture deviations from normative behaviour across multiple modalities. Then, once an abnormal subject has been identified, feature space metrics can be used to identify deviating brain regions (e.g. Supp. Fig.  We propose a latent deviation metric to measure deviations from the joint normative distribution. To account for correlation between latent vectors and derive a single multivariate measure of deviation, we measure the Mahalanobis distance from the encoding distribution of the training cohort: where z j ∼ q (z j | X j ) is a sample from the joint posterior distribution for subject j, μ(z norm ) is the mean and Σ(z norm ) the covariance of the healthy cohort latent position. We use robust estimates of the mean and covariance to account for outliers within the healthy control cohort. For closer comparison with D ml , we derive the following multivariate feature space metric: where d j = {d ij , . . . , d Ij } is the reconstruction error for subject j for brain regions (i = 1, ..., I), μ(d norm ) is the mean and Σ(d norm ) the covariance of the healthy cohort reconstruction error. Assessing Deviation Metric Performance. For each model, we calculated D ml and D mf for a healthy holdout cohort and disease cohort. For each deviation metric, we identified individuals whose deviations were significantly different from the healthy training distribution (p < 0.001)  In order to calculate significance ratios, we calculated D uf relative to the training cohort for the healthy holdout and disease cohorts (Bonferroni adjusted p=0.05/N features ) ",1
37,Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities (vol1),,UK Biobank Results.,"As expected, we see greater significance ratios for all models when using D ml rather than D mf (Table  ADNI Results. Previous work ",1
39,Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities (vol1),,Acknowledgements,. This work is supported by the ,1
40,Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 41.,1
46,Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction (vol1),,(a). In order,"Moreover, we don't only map the x h into Decoder1 and Decoder2 while dropping the X l \{x h } to implement the burden reduction, because the low-level information in X l \{x h } is vital for restoring artifact-free images. Furthermore, x h will be disturbed by noise from the approaching target x a of Decoder2 while information X l \ {x h } upstream from the HDE can counteract the noise disturbance to a certain extent. The reason behind the counteraction is that the update to upstream parameters is not as large as that of the downstream parameters.",1
52,Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_8.,1
57,MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking (vol1),,Knowledge Word-Driven Masking (KWM).,"MeSH words shown in Fig.  where N MeSH represents the number of MeSH words in the report r. Then, we compute an attention map C MeSH to identify image regions mapped to MeSH words as follows where H = W = N img , T and R represent the transpose and reshape functions, and the softmax function normalizes the elements along the image dimension to find the focused region matched to each MeSH word. The summation operation performs on the text dimension to aggregate the attentions related to all MeSH words. Subsequently, the high-activated masking is presented to remove the discovered attention regions. Here, we define a corresponding binary mask m ∈ {0, 1} H×W formulated as m (i,j) = I(C ). Here C [γ * Nimg] MeSH refers to the (γ * N img )-th largest activation in C MeSH , andγ is the masking ratio that determines how many activations would be suppressed. With this binary mask, we can compute the masked representations produced by KWM as where [MASK] is a masked placeholder.",1
58,MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking (vol1),,Sentence-Driven Masking (SDM).,"Medical reports often contain multiple sentences that describe different findings related to the image, which inspires SDM to introduce sentence-level information during mask generation. For the report r, we randomly select a sentence s and extract its representations as where N s represents the length of s. Then, an attention map C s can be computed to identify regions mapped to this sentence as After that, the high-activated masking is performed based on C s to compute the masked representations M(C s ; λ) sdm . We also select an image-report pair and visualize the corresponding attention map and generated mask procured by KWM and SDM in Fig. ",1
63,MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking (vol1),,Downstream Setup.,We validate the transferability of learned MedIM representations on four X-ray-based downstream tasks: (1) multi-label classification on CheXpert  (2) multi-class classification on COVIDx ,1
66,MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking (vol1),,Methods T2I I2T,R@1 R@5 R@10 R@1 R@5 R@10  struction is performed based on the complete image encoder representations instead of the masked one. Low-activated masking refers to masking the tokens exhibiting a low response in both KWM and SDM strategies. The comparison on the left side of Fig. ,1
69,Unsupervised Domain Transfer with Conditional Invertible Neural Networks (vol1),,Domain Transfer Method:,We present an entirely new sim-to-real transfer approach based on conditional invertible neural networks (cINNs) (cf. Fig. ,1
70,Unsupervised Domain Transfer with Conditional Invertible Neural Networks (vol1),,Instantiation to Spectral Imaging:,We show that our method can generically be applied to two complementary modalities: photoacoustic tomography (PAT; image-level) and hyperspectral imaging (HSI; pixel-level).,1
71,Unsupervised Domain Transfer with Conditional Invertible Neural Networks (vol1),,Comprehensive Validation:,"In comprehensive validation studies based on more than 2,000 PAT images (real: ∼1,000) and more than 6 million spectra for HSI (real: ∼6 million) we investigate and subsequently confirm our two main hypotheses: (H1) Our cINN-based models can close the domain gap between simulated and real spectral data better than current state-of-the-art methods regarding spectral plausibility. (H2) Training models on data transferred by our cINN-based approach can improve their performance on the corresponding (clinical) downstream task without them having seen labeled real data.",1
75,Unsupervised Domain Transfer with Conditional Invertible Neural Networks (vol1),,Benefit of Domain-Transferred Data for Downstream Tasks (H2):,"We examined two classification tasks for which reference data generation was feasible: classification of veins/arteries in PAT and organ classification in HSI. For both modalities, we used the completely untouched real test sets, comprising 162 images in the case of PAT and ∼ 920,000 spectra in the case of HSI. For both tasks, a calibrated random forest classifier (sklearn  As shown in Table ",1
84,Anatomy-Driven Pathology Detection on Chest X-rays (vol1),,Method,"Supervision IoU@10-70 IoU@10 IoU@30 IoU@50 Box Class mAP AP loc-acc AP loc-acc AP loc-acc MIL-ADPD (ours) An Pa For all models, we only consider the predicted boxes with the highest box score per pathology, as the CXR8 dataset never contains more than one box per pathology. We report the standard object detection metrics average precision (AP) at different IoU-thresholds and the mean AP (mAP) over thresholds (0.1, 0.2, . . . , 0.7), commonly used thresholds on this dataset ",1
87,Anatomy-Driven Pathology Detection on Chest X-rays (vol1),,Conclusion.,"We proposed a novel approach tackling pathology detection on chest X-rays using anatomical region bounding boxes. We studied two training approaches, using anatomy-level pathology labels and using image-level labels with MIL. Our experiments demonstrate that using anatomical regions as proxies improves results compared weakly supervised methods and supervised training on little data, thus providing a promising direction for future research.",1
88,Anatomy-Driven Pathology Detection on Chest X-rays (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_6.,1
97,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET (vol1),,(Color figure online),"The most important design choice is the selection of the final activation function. Indeed, the multi-clamp final activation function was proven to be the best both in terms of CS (Exp 4.1: CS = 0.78 ± 0.05) and MAE (Exp 4.2: MAE = 3.27 ± 2.01). Compared to the other final activation functions, when the multi-clamp is used the impact of the max-pooling design is negligible also in terms of MAE. For the rest of the experiments, the selected configuration is the one from Exp. 4.1 (see Table  Figure  In terms of run-time, the DNN needed ≈ 1 min to predict the KPs of the a whole-body scan (≈ 400 slices), whereas curve fit took 8.7 min for a single slice: the time reduction of the DNN is expected to be ≈ 3.500 times.",1
100,Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_28.,1
103,Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance (vol1),,Ark has the following properties:,"• Knowledge-centric. Annotating medical images by radiologists for deep learning is a process of transferring their in-depth knowledge and expertise in interpreting medical images and identifying abnormalities to a medium that is accessible for computers to learn. Ark's superior and robust performance is attributed to the accumulation of expert knowledge conveyed through medical imaging annotations from diverse expert sources worldwide. At the core of Ark is acquiring and sharing knowledge: ""knowledge is power"" (Mac Flecknoe) and ""power comes not from knowledge kept but from knowledge shared"" (Bill Gates). • Label-agnostic, task-scalable and annotation-heterogeneous. Ark is label-agnostic as it does not require prior label ""understanding"" of public datasets, but instead uses their originally-provided labels. It is designed with pluggable multi-task heads and cyclic pretraining to offer flexibility and scalability for adding new tasks without manually consolidating heterogeneous labels or training task-specific controllers/adapters ",1
105,Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance (vol1),,Results and Analysis:,As shown in Table ,1
107,Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance (vol1),,Results and Analysis:,"As seen in Table  For instance, a pneumothorax can be detected by observing a visible ""visceral pleural line"" along part or all of the length of the lateral chest wall ",1
117,LOTUS: Learning to Optimize Task-Based US Representations (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_42.,1
124,Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning (vol1),,Pseudo-Labeling.,"To generate accurate supervision signals, a flexible pseudolabeling promotion strategy is introduced to replace noisy labels with pseudolabels, Benefiting from the unique semi-supervised learning structure, our SSL module can effectively reduce the impact of noise based on statistical classification. The objective function for semi-supervised learning is defined as: where ω ∈ (0, 1] is a tunable focusing parameter, which is utilized to exploit the benefits of both the noise-robustness and the implicit weighting scheme.",1
135,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor (vol1),,Effect of Different Aggregation Approaches:,"In Table  As a baseline, we first conducted the average of four activation maps generated by the multiple-level activation extraction (Avg. ME). We then applied C 2 AM  Table ",1
136,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor (vol1),,Selected Exit,"Dice Effect of Single-Exit and Multiple-Exit: Table  The comparisons show that the activation map obtained from the shallow layer M 1 and the deepest layer M 4 result in low dice scores, around 0.15. This is because the network is not deep enough to learn the tumor region in the shallow layer, and the resolution of the activation map obtained from the deepest layer is too low to contain sufficient information to make a clear boundary for the tumor. Results of the internal classifiers from the middle of the network (M 2 and M 3 ) achieve the highest dice score and IoU, both of which are around 0.5. To evaluate whether using results from all internal classifiers leads to the highest performance, we further apply the proposed method to the two internal classifiers with the highest dice scores, i.e., M 2 and M 3 , called M 2 + M 3 . Compared with using all internal classifiers (M 1 to M 4 ), M 2 + M 3 results in 18.6% and 22.1% lower dice and IoU, respectively. In conclusion, our AME-CAM still achieves the optimal performance among all the experiments of single-exit and multiple-exit. Other ablation studies are presented in the supplementary material due to space limitations.",1
138,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 17.,1
144,Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation (vol1),,Ablation Study.,"In this section, we analyze the effectiveness of the proposed CMA module and OCC module. We implement the MC-Net as our baseline, which uses different up-sampling operations to introduce architecture heterogeneity. Table ",1
146,Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation (vol1),,Acknowledgements,. This work is funded by the ,1
178,You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray (vol1),,Preliminary Pseudo Label Distillation for Semi-supervised Learning.,"Both of our baseline semi-supervised vision and language models follow the teacher-student knowledge distillation (TSD) procedure  where L R cls is the cross-entropy loss, {ŷ} = F R s (r) is the prediction by the student model, {y pr } = F R t (r u ) is the set of pseudo labels generated by the teacher model. In each batch, labeled and unlabeled instances are sampled according to a controlled ratio. The resulting report classification model F R s will be utilized later to help with the primary task of abnormality detection in CXR. Similarly, a student vision model F I s for abnormality detection in CXR is trained in semisupervised setting by distilling from a teacher vision model F I t trained on labeled CXRs, with the loss function: where {(ŷ, B)} = F I s (x) are the predictions by the student model, {(y pv , B pv )} = F I t (x u ) are the pseudo class and bounding box labels generated by the teacher model, L I cls is the focal loss  Self-adaptive Non-maximum Suppression. During the TSD, the teacher vision model F I t is kept fixed. While its knowledge suffices for guiding the student vision model F I s in the early stage of TSD, it may somehow impede the learning of F I s when F I s gradually improves by also learning from the large amount of unlabeled data. Therefore, to gradually improve quality and robustness of the pseudo detection labels as F I s learns, we propose to perform self-adaptive non-maximum suppression (SA-NMS) to combine the pseudo labels {(y pv , B pv )} output by F I t and the predictions {(ŷ, B)} by F I s in each mini batch. Specifically, we perform NMS on the combined set of the pseudo labels and predictions: {(y cv , B cv )} = NMS {(y pv , B pv )} {(ŷ, B)} , and replace {(y pv , B pv )} in Eq. (2) with {(y cv , B cv )} for supervision by unlabeled CXRs. In this way, highly confident predictions by the maturing student can rectify imprecise ones by the teacher, leading to better supervision signals stemming from unlabeled data. Report-Guided Pseudo Label Refinement. In routine clinics, almost every radiograph in archive is accompanied by a report describing findings, abnormalities (if any), and diagnosis. Compared with the captions of natural images, the report texts constitute a unique (to medical image analysis) and rich source of extra information in addition to the image modality. To this end, we propose report-guided pseudo detection label refinement (RPDLR) to make use of this cross-modal information for semi-supervised anatomical abnormality detection in CXR. Specifically, we use the student language model F R s (trained with Eq. (  Eventually, we train the student vision model F I s using {(y v , B v )} in Eq. ( ",1
179,You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray (vol1),,Co-evolutionary Semi-supervised Learning with Cycle Pseudo Label,"Refinement. As the auxiliary student language model F R s plays an important role in RPDLR, it is reasonable to optimize its performance which in turn would benefit the primary task of abnormality detection. Therefore, we further propose an inverse, abnormality-guided pseudo classification labels refinement (APCLR) to help with semi-supervised training of the report classification model. Similarly in concept to the RPDLP, given a pair of unlabeled image x u and report r u , we obtain the set of abnormalities {(ŷ, B)} detected in x u by the student vision model F I s , and the set of classification pseudo labels {y pr } generated for r u by the teacher language model F R t . We retain only the pseudo labels {y pr j |y pr j ∈ {ŷ}}, by excluding the report-classified abnormalities not detected in the paired CXR. Ideally, one should use an optimal report classification model for refinement of the abnormality detection pseudo labels, and vice versa. However, the two models are mutually dependent on each other in a circle. To solve this dilemma, we implement an alternative co-evolution strategy to refine the abnormality detection and report classification pseudo labels iteratively, in generations. As shown in Fig. ",1
182,You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 35.,1
186,Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI (vol1),,Dynamic Graph Construction.,"Considering that brain functional connectivity (FC) patterns change dynamically over time  The original feature for the j-th node is represented by the j-th row in X t for segment t. Considering all connections in an FC network may include some noisy or redundant information, we retain the top 30% strongest edges in each FC network to generate an adjacent matrix A t ∈ {0, 1} N ×N for segment t. Thus, the obtained dynamic graph sequence of each subject can be described as",1
187,Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI (vol1),,Modularity-Constrained Spatiotemporal GNN.,"With the dynamic graph sequence {G t } T t=1 as input, we design a modularity-constrained spatiotemporal graph neural network (MSGNN) to learn interpretable and discriminative graph embeddings, with two unique constraints: 1) a modularity constraint, and 2) a graph topology reconstruction constraint. In MSGNN, we first stack two graph isomorphism network (GIN) layers  where ψ is nonlinear activation, ε (i) is a parameter at the i-th GIN layer, I is an identity matrix, and W (i) is the weight for the fully connected layers in GIN.",1
188,Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI (vol1),,1) Modularity Constraint.,"It has been demonstrated that the central executive network (CEN), salience network (SN) and default mode network (DMN) are three crucial neurocognitive modules in the brain and these three modules have been consistently observed across different individuals and experimental paradigms, where CEN performs high-level cognitive tasks (e.g., decision-making and rule-based problem-solving), SN mainly detects external stimuli and coordinates brain neural resources, and DMN is responsible for self-related cognitive functions  Based on such prior knowledge and clinical experience, we reasonably assume that the learned embeddings of nodes within the same neurocognitive module tend to be similar. We develop a novel modularity constraint to encourage similarity between paired node-level embeddings in the same module. Mathematically, the proposed modularity constraint is formulated as: where h t,k i and h t,k j are embeddings of two nodes in the k-th module (with N k ROIs) at segment t, and K is the number of modules (K = 3 in this work). With Eq. ( ",1
189,Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI (vol1),,2) Graph Topology Reconstruction Constraint.,"To further enhance discriminative ability of learned embeddings, we propose to preserve graph topology by reconstructing adjacent matrices. For the segment t, its adjacent matrix A t can be reconstructed through Ât = σ(H t • H t ), where σ is a nonlinear mapping function. The graph topology reconstruction constraint is then formulated as: where Ψ is a cross-entropy loss function. We then apply an SERO operation  3) Temporal Feature Learning . To further capture temporal information, a single-head transformer is used to fuse features derived from T segments, with a self-attention mechanism to model temporal dynamics across segments. We then sum the learned features {h i } T i=1 to obtain the whole-graph embedding. Prediction and Biomarker Detection. The whole-graph embedding is fed into a fully connected layer with Softmax for prediction, with final loss defined as: where L C is a cross-entropy loss for prediction, and λ 1 and λ 2 are two hyperparameters. To facilitate interpretation of our learned graph embeddings, we calculate PC coefficients between paired node embeddings for each segment and average them across segments to obtain an FC network for each subject. The upper triangle of each FC network is flattened into a vector and Lasso  Implementation. The MDRL is implemented in PyTorch and trained using an Adam optimizer (with learning rate of 0.001, training epochs of 30, batch size of 8 and τ = 20). We set window size Γ = 40 for NYU and Γ = 70 for the rest, and results of MDRL with different Γ values are shown in Supplementary Materials. In the modularity constraint, we randomly select m = 50% of all N k (N k -1) 2 paired ROIs in the k-th module (with N k ROIs) to constrain the MDRL.",1
203,Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models (vol1),,VQ-GAN:,"The VQ-GAN operates on a 3D input of size x ∈ R H×W ×D and consists of an encoder E that compresses to a latent space z ∈ R h×w×d×n , where n is the dimension of the latent embedding vector. This representation is quantised by looking up the nearest value of each representation in a codebook containing K elements and replacing the embedding vector of length d with the codebook index, k, producing z q ∈ R h×w×d . A decoder G operates on this quantised representation to produce a reconstruction, x ∈ R H×W ×D . In a VQ-VAE  The encoder and decoder are convolutional networks of l levels. There is a simple relationship between the spatial dimension of the latent space, the input, and number of levels: h, w, d = H 2 l , W 2 l , D 2 l , so the latent space is 2 3l times smaller spatially than the input image, with a 4 × 2 3l reduction in memory size when accounting for the conversion from a float to integer representation. In practice, most works use l = 3 (512× spatial compression) or l = 4 (4096× spatial compression); it is challenging to train a VQ-GAN at higher compression rates.",1
204,Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models (vol1),,DDPM:,"A DDPM is then trained on the latent embedding z (the de-quantised latent). During training, noise is added to z according to a timestep t and a fixed Gaussian noise schedule defined by β t to produce noised samples z t , such that where we use z 0 to refer to the noise-free latent z, we have 0 ≤ t ≤ T , and α t := 1β t and ᾱt := t s=1 α s . We design β t to increase with t such that the latent z T is close to an isotropic Gaussian. We seek to train a network that can perform the reverse or denoising process, which can also be written as a Gaussian transition: In practice, following  where n ∼ N (0, I). While in most applications an isotropic Gaussian is drawn and iteratively denoised to draw samples from the model, in this work, we take a latent input z 0 and noise to z t for a range of values of t < T and obtain their reconstructions, ẑ0,t = p θ (z 0 |z t ).",1
209,Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models (vol1),,LTM:,The Latent Transformer Models were trained on the same VQ-GAN bases using the procedure described in ,1
211,Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models (vol1),,Dataset,"Recent research shows that at higher resolutions, the effective SNR increases if the noise schedule is kept constant  Memory and time requirements for all models are tabulated in Supplementary Table ",1
213,Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 43.,1
236,S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation (vol1),,Acknowledgements,. This work was supported by ,1
249,M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization (vol1),,Related Works:,"To connect vision and language modalities, the idea of VLP was proposed in CLIP  It has been suggested that optimal vision and language latent spaces should be of different geometry ",1
251,M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization (vol1),,Vision Encoder and Frozen Text Encoder:,"The paired medical image and text are denoted as x v , x t , respectively. As illustrated in Fig.  Vision Embedding: E V , the vision embedding z v ∈ R B×N is extracted from the last pooling layer of E V . N denotes the dimension of the latent space and B represents the batch size. Text Embedding: A text encoder E T extracts text embedding of word tokens from a medical report. Similar to BERT ",1
256,M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization (vol1),,Medical Image Classification:,"The AUC scores on MIMIC, CXP, and CXR14 are reported in Table  Segmentation and Object Detection: Table ",1
263,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy (vol1),,SfM as Supervision for Feature Extraction.,"We generate examples of good features by identifying features that were successfully reconstructed with existing methods for each sequence in our training set. Our training set contains short sequences (4-7 s) from the complete colonoscopy recordings in EndoMapper dataset where COLMAP software was able to obtain a 3D reconstruction. This is a very challenging domain, and existing SfM pipelines fail in longer videos.",1
264,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy (vol1),,3D Reconstruction of Training Set Videos.,"We generate 3D reconstructions for all our training sequences with out-of-the-box COLMAP. In particular, we use the following blocks: feature extractor, exhaustive matcher and mapper. Configuration parameters are detailed in the supplementary materials. We turn on the ""guided matching"" option for the exhaustive matcher module to find the best matches possible. We additionally compute the 3D reconstruction for the same sequences with a modified COLMAP pipeline that uses the official Super-Point and SuperGlue For supervision, we only use reprojected points that fall within a reliable track. A reliable track is an interval bounded by green points. So, the reprojected points selected for training are either green or have preceding and subsequent green points along its track. The different appearances of the same 3D point in different frames of the track are our correspondences for training our models. Figure  Deep Feature Extraction for Endoscopy. SuperPoint uses a fully-convolutional network as backbone and learns to extract good features using homographic adaptation: extracting features that are robust to homographic deformations. It achieves this by using as supervision Y the average detections over several random homographic deformations of the same image. The feature extraction network then is run on an image I and a warped version I of it with a new homography. The network optimizes the loss function where X and D are the detection and description heads' outputs, respectively. Y is the supervision for the detection. S is the correspondence between I and I computed from the homography. L p is the detection loss that measures the discrepancies between the supervision Y and the detection head's output X . λ = 1 is a weighting parameter. L d is the description loss that measures the discrepancies between both description head's outputs D and D using S. Using our new supervision from SfM in the form of tracks of points, we propose a new loss to train SuperPoint that is more aligned with our goal, called tracking adaptation. Instead of an image I and a warped version I , we use different images I a and I b from the same sequence. The supervision Y for the detection in this case is the set of points that have been reprojected on I a and I b from the 3D reconstruction. The detection loss L p is calculated as in the original SuperPoint. We replace the description loss L d for a new tracking loss where D a and D b are the description head's outputs for I a and I b , respectively. T is the set of all the tracks that appear in both images. l t is a common triplet loss that measures the distance between positive pairs (weighting parameter λ t = 1 and positive margin m p = 1) and the distance between negative pairs (negative margin m n = 0.2). Two descriptors from different images d ai and d bj are a positive pair if they belong to the same track (i = j), and negative pair otherwise (i = j).",1
266,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy (vol1),,SfM Results,"Comparison. This experiment compares the performance of the considered baselines against the best configuration of our feature extraction model. Table  To provide quantitative evaluation of the camera motion estimation, we use a simulated dataset ",1
268,Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 56.,1
270,Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training (vol1),,Graph,"Once lack of concept correlation and negation identification, representations with similar semantics are falsely pushed apart and those with opposite semantics are falsely pushed together, interfering with the learning of significant representation  Rethinking the existing methods and challenges of medical contrastive visionlanguage pre-training  In this paper, we propose a novel knowledge-boosting medical contrastive vision-language pre-training framework (KoBo). Our contributions are as followed. 1) Our KoBo pre-trains a powerful image encoder including visual information corresponding with the disease described in texts, where knowledge is embedded in our paradigm (Fig. ",1
274,Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training (vol1),,Estimation of Similarities:,"The semantic similarity is calculated upon sample knowledge. For each image-text pair, a max-match strategy is adopted to match each two sample knowledge embedding with the most similar one for calculating cosine similarities. Sample-wise similarities are aggregated with averages. (2) where N ES is the number of concepts in T Sent i , while N ES is that in T Report i . Knowledge Semantic Enhancement Loss: We utilize the sample-wise semantic similarity to estimate negative sample noise, placed in the sample weight of the contrastive loss  ) where τ G is the global temperature, and λ IT , λ T I is the sample similarity measurement. specifically, λ i,i is fixed to zero to persist the positive sample weight.",1
276,Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training (vol1),,Semantic Bridge Guidance:,"We propose to narrow disperse shifting enlarged by the modality gap between vision and language. Specifically, the gap is bridged by the fusion of domain knowledge which is better compatible with text: where the image-weighted domain knowledge is contrasted with text features between samples. Finally, L SG is aggregated by these four parts as below: 3 Experiment Experiment Protocol: Pre-training performs on MIMIC-CXR ",1
277,Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training (vol1),,KoBo Init,ImageNet Init Basilar Atelectasis. Comparison Study: Table  Ablation Study: As is demonstrated in Fig.  Qualitative Analysis: In Fig. ,1
285,Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 34.,1
288,Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation (vol1),,Knowledge Distillation by Weighted Logits Map:,"We denote D N S = {X s , Y s } N as a collection of N source domains and D T = {X i t , Y j t } as single target domain, where the number of labeled instances Y j t X i t . We are only provided with black-box models {f n s } N n=1 trained on multiple source domains {x i s , y i s } N n=1 for knowledge transfer. The parameters {θ n s } N n=1 of these source domain predictors are not allowed to participate in gradient backpropagation as a result of the privacy policy. Thus, our ultimate objective is to derive a novel student model f t : X t → Y t that is relevant to the source domain task. Accordingly, direct knowledge transfer using the output of the source domain predictor may lead to feature bias in the student model due to the unavoidable covariance  where (i, j) denotes centre of region, and k denotes the size of k-square-neighbors. Firstly, we develop a pixel-level predictive uncertainty algorithm to aid in assessing the correlation between multiple source domains and the target domain. For a given target image x t ∈ X i t , we initially feed it into the source predictors {f n s } N n=1 to obtain their respective prediction {p n s } N n=1 . To leverage the rich semantic information from the source domain predictor predictions, we utilize predictive entropy of the softmax outputs to measure the prediction uncertainty scores. In the semantic segmentation scenario of C-classes classification, we define the pixel-level uncertainty score U (i,j) n as follow: where O n s denotes softmax output,i.e.,O n s = softmax(p n s ) from nth source predictor. Due to the unique characteristics of cell morphology, merely relying on uncertainty information is insufficient to produce high-quality ensemble logits map that accurately capture the relevance between the source and target domains. The target pseudo-label for the nth predictor f n s can be obtained by applying the softmax function to the output and selecting the category with the highest probability score, i.e., Y t = arg max c∈{1,...,C} (softmax(p n s )). Then according to C-classes classification tasks, we divide the cell region into C subsets, After that, we determine the degree of impurity in an area of interest by analyzing the statistics of the boundary region, which represents the level of semantic information ambiguity. Specifically, the number of different objects within the area is considered a proxy for its impurity level, with higher counts indicating higher impurity.The boundary impurity P (i,j) can be calculated as: where | • | denotes the number of pixels in the area. By assigning lower weights to the pixels with high uncertainty and boundary ambiguity, we can obtain pixel-level weight scores W n for each p n s , i.e., where denotes element-wise matrix multiplication. According to the pixellevel weight, we will obtain an ensemble logits map M = N n=1 W n • p n s . And the object of the knowledge distillation is a classical regularization term  where D kl denotes the Kullback-Leibler (KL) divergence loss. Adaptive Pseudo-Cutout Label: As previously mentioned, the outputs from the source domain black-box predictors have been adjusted by the pixel-level weight. However, they are still noisy and only pixel-level information is considered while ignoring structured information in the knowledge distillation process. Thus, we utilize the output of the black-box predictor on the target domain to produce an adaptive pseudo-cutout label, which will be employed to further regularize the knowledge distillation process. We have revised the method in  where α is empirically set as 0.9. Then we will aggregate the voting scores, i.e., V (i,j) = N n=1 V (i,j) n and determine whether to retain each pixel using an adaptive vote gate G ∈ {1, 2, 3, etc.}. By filtering with a threshold and integrating the voting strategy, we generate high-confidence pseudo-labels that remain effective even when the source and target domains exhibit covariance. Finally, we define the ensemble result as a pseudo-cutout label Ps and employ consistency regularization as below: where l ce denotes cross-entropy loss function. Loss Functions: Finally, we incorporate global structural information about the predicted outcome of the target domain into both distillation and semisupervised learning. To mitigate the noise effect of the source domain predictors, we introduce maximize mutual information targets to facilitate discrete representation learning by the network. We define E(p) =i p i log p i as conditional entropy. The object can be described as follow: where the increasing H(Y t ) and the decreasing H(Y t |X t ) help to balances class separation and classifier complexity  Finally, we get the overall objective: where L sup denotes the ordinary cross-entropy loss for supervised learning and we set the weight of each loss function to 1 in the training.",1
299,Additional Positive Enables Better Representation Learning for Medical Images (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_12. ,1
304,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning (vol1),,|X |,"x L(M (x; θ), x), and the local minimum is different in S and T as Θ S in Fig.  seg provides the baseline segmentation performance, D T F T should provide similar feature maps to achieve the baseline performance. To this end, the mid-feature maps generated should be similar, i.e., where C i represents the convolution in D T F T , F i represents i th feature map, and which can be expressed as: Here, we denote w i -w i = f i as the fluctuation vector in the vector space, and the condition f i = 0 indicates that the sum of the fluctuation vectors should be zero under the condition of |f i | < r 1. Hence, we achieve the condition for the parameter fluctuation that the centers of parameters of Θ S and θ T should be the same in the vector space, and the length of the fluctuation vector should be less than a certain small threshold (0 < r 1). Therefore, the parameter fluctuation aims to add random vectors of which length is less than 0 < r 1 on the parameters of Θ S , and the sum of vectors should be zero. To summarize, the parameter fluctuation aims to add randomness on Θ S as follows: (5) 3 Experiments",1
308,Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_52.,1
318,Decoupled Consistency for Semi-supervised Medical Image Segmentation (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 53.,1
335,3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_14.,1
338,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy (vol1),,Mesh Autoencoder (M-AE):,We use EdgeConv ,1
339,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy (vol1),,Implicit Field Decoder (IM-NET):,The IM-NET ,1
341,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy (vol1),,Shape Variation Autoencoder (SP-VAE):,"The VAE  The main difference between M-AE and a SP-VAE lies in the input and output representations they handle. SP-VAE operates directly on sets of landmarks or correspondences, aiding in the analysis of shape models. It takes a set of correspondences describing a shape as input and aims to learn a compressed latent representation of the shape. Importantly, the SP-VAE maintains the same ordering of correspondences at the input and output, so it does not use permutation-invariant layers or operations like pooling.",1
347,Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 59.,1
349,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation (vol1),,3),"We also propose a sufficiently approximation for the Cross-ALD by a multiple particle-based search using semantic feature Stein Variational Gradient Decent (SVGDF), an enhancement of the vanilla SVGD ",1
361,Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 18.,1
372,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_45.,1
377,Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training (vol1),,Networks and Training Settings:,We evaluate our TI-ST framework using two different architectures: (1) DeepLabV3+ ,1
380,Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_32.,1
390,SLPD: Slide-Level Prototypical Distillation for WSIs (vol1),,Number of Prototypes.,As shown in Table  Number of Slide Neighbors. As demonstrated in Table ,1
392,SLPD: Slide-Level Prototypical Distillation for WSIs (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_25.,1
395,Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images (vol1),,Fig. 2.,"The overview of our proposed Masked Frequency Consistency (MFC) module, which can be seamlessly integrated with different UDA methods and backbone networks. The MFC module works by augmenting the input image in the frequency domain using a mask, and then using a teacher-student structure to take both the original and the augmented image as inputs. A consistency loss is applied to facilitate the bridging of domains.",1
402,Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 63.,1
408,Can Point Cloud Networks Learn Statistical Shape Models of Anatomies? (vol1),,Region of CAM lesion,"Difference from normal to CAM group mean captured by PDM 0 0.5  Left Atrium. The left atrium dataset comprises of 1096 shapes segmented from cardiac LGE MRI images from unique atrial fibrillation patients. This cohort contains significant morphological variation in overall size, the size of the left atrium appendage, and the number and arrangement of the pulmonary veins. This variation is reflected in the large compactness values in Table  Pancreas. We utilize the pancreas dataset  Spleen. The spleen dataset ",1
409,Can Point Cloud Networks Learn Statistical Shape Models of Anatomies? (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_47.,1
415,PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_51.,1
417,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks (vol1),,Contribution:,"We propose a cross-validation framework, using separate selfsupervision tasks to minimise overfitting on the synthetic anomalies that are used for training. To make this work effectively we introduce a number of non-trivial and seamlessly-integrated synthetic tasks, each with a distinct feature set so that during validation they can be used to approximate generalisation to unseen, real-world anomalies. To the best of our knowledge, this is the first work to train models to directly identify anomalies on tasks that are deformation-based, tasks that use Poisson blending with patches extracted from external datasets, and tasks that perform efficient Poisson image blending in 3D volumes, which is in itself a new contribution of our work. We also introduce a synthetic anomaly labelling function which takes into account the natural noise and variation in medical images. Together our method achieves an average precision score of 76.2 for localising glioma and 78.4 for identifying pathological chest X-rays, thus setting the state-of-the-art in self-supervised anomaly detection.",1
418,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks (vol1),,Related Work:,"The most prevalent methods for self-supervised anomaly detection are based on generative auto-encoders that analyse the residual error from reconstructing a test sample. This is built on the assumption that a reconstruction model will only be able to correctly reproduce data that is similar to the instances it has been trained on, e.g. only healthy samples. Theoretically, at test time, the residual reconstruction error should be low for healthy tissues but high for anomalous features. This is an active area of research with several recent improvements upon the initial idea  However, the general assumption that reconstruction error is a good basis for an anomaly scoring function has recently been challenged. Auto-encoders are unable to identify anomalies with extreme textures  Self-supervised methods take a more direct approach, training a model to directly predict an anomaly score using synthetic anomalies. Foreign patch interpolation (FPI) ",1
421,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks (vol1),,Ablation and Sensitivity Analysis on Cross-Validation Structure:,We also investigate how performance changes as we vary the number of tasks used for training and validation (Table ,1
422,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks (vol1),,Discussion:,"We demonstrate the effectiveness of our method in multiple settings and across different modalities. A unique aspect of the brain data is the domain shift. The HCP training data was acquired at a much higher isotropic resolution than the BraTS and ISLES test data, which are both anisotropic. Here we achieve the best performance using more tasks for validation, which successfully reduces overfitting and hypersensitivity. Incorporating greater data augmentations, such as simulating anisotropic spacing, could further improve results by training the model to ignore these transformations. We also achieve strong results for the X-ray data, although precise localisation remains a challenging task. The gap between current performance and clinicially useful localisation should therefore be high priority for future research.",1
431,Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 44.,1
439,Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_49.,1
441,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation (vol1),,Tumor epithelial Ɵssue,"Necrosis Ɵssue Tumor-associated stroma Necrosis may appear as areas of pink, amorphous material under the microscope, and may be surrounded by viable tumor cells and stroma. Tumor-associated stroma Ɵssue is the connecƟve Ɵssue that surrounds and supports the tumor epithelial Ɵssue. Fig.  Recent studies on weakly supervised segmentation primarily follow class activation mapping (CAM)  To remedy the limitations of image-level supervision, we advocate for the integration of language knowledge into weakly supervised learning to provide reliable guidance for the accurate localization of target structures. To this end, we propose a text-prompting-based weakly supervised segmentation method (TPRO) for accurate histopathology tissue segmentation. The text information originates from the task's semantic labels and external descriptions of subtype manifestations. For each semantic label, a pre-trained medical language model is utilized to extract the corresponding text features that are matched to each feature point in the image spatial space. A higher similarity represents a higher possibility of this location belonging to the corresponding semantic category. Additionally, the text representations of subtype manifestations, including tissue morphology, color, and relationships to other tissues, are extracted by the language model as external knowledge. The discriminative information can be explored from the text knowledge to help identify and locate complete tissues accurately by jointly modeling long-range dependencies between image and text. We conduct experiments on two weakly supervised histological segmentation benchmarks, LUAD-HistoSeg and BCSS-WSSS, and demonstrate the superior quality of pseudo labels produced by our TPRO model compared to other CAM-based methods. Our contributions are summarized as follows: (1) To the best of our knowledge, this is the first work that leverages language knowledge to improve the quality of pseudo labels for weakly-supervised histopathology image segmentation. ",1
450,TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_11.,1
458,LSOR: Longitudinally-Consistent Self-Organized Representation Learning (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 27.,1
464,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_7.,1
469,Dual Conditioned Diffusion Models for Out-of-Distribution Detection: Application to Fetal Ultrasound Videos(vol1),,Latent Image Feature Conditioning (LIFC):,"The image conditioning dictates the desired appearance of generated images in terms of shape and texture. In our model, we use the features extracted by a pretrained encoder for conditioning. Empirically, we use the same encoder E as our feature extractor to obtain latent feature vector z 0 as shown in Fig. ",1
470,Dual Conditioned Diffusion Models for Out-of-Distribution Detection: Application to Fetal Ultrasound Videos(vol1),,Cross Attention Guidance:,"To integrate the dual-conditioning guidance into the diffusion model, we use a cross-attention ",1
472,Dual Conditioned Diffusion Models for Out-of-Distribution Detection: Application to Fetal Ultrasound Videos(vol1),,Feature-Based OOD Detection,"To evaluate the performance of the DCDM, the cosine similarity between features of the input image x 0 and the generated image x 0 from the in-distribution classifier is calculated and is referred as an OOD score where f 0 and f 0 are the features of x 0 and x 0 , respectively: An input image x 0 is classified as in-distribution (ID) or OOD based on Eq. 5 where τ is a pre-defined threshold and y pred is the prediction of our feature-based OOD detection algorithm.",1
488,Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration (vol1),,S l n h,"where S is the sigmoid function, l is the individual B-scan classification logit, and n is the number of B-scans in a volume, h is the mean saliency in the detected region and Σh is the total mean saliency of all detected regions within the B-scan. A higher confidence score implies a higher possibility that the detected region covers nGA lesions. Since only class labels of 3D OCT volume are required for training, the proposed lesion localization algorithm was weakly supervised. ",1
496,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging (vol1),,Input:,"Training data D, validation data D v , initial model parameter {θ 0 1 , ..., θ 0 d }, LRs {α 0 1 , ..., α 0 d }, batch size n, max iteration T; Output: Final model parameter Step forward for one step to get { θt 1 (α   MetaLR aims to use the validation set to optimize α through an automatic process rather than a manual one. The optimal scheme α * can be found by a nested optimization ",1
503,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging (vol1),,Results on Lesion Detection Tasks.,MetaLR consistently shows the best performance on all downstream tasks (Table  Results on Segmentation Task. MetaLR achieves the best Dice performance on the LiTS segmentation task (Table ,1
505,MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging (vol1),,The Effectiveness of Proportional Hyper-LR and Training Batches,"Validation. We illustrate the LR curves with a constant hyper-LR instead of a proportional one. The LR curves of ""Block 3-1"" and ""Block 4-2"" become much more fluctuated (Fig. ",1
511,Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification (vol1),,AU for Training Classifier.,"Based on the manifold assumption, the real data is gathered on the low-dimensional manifold of the high-dimensional space, and the noise of the data is located on the edge of the manifold for the corresponding category. When using ER to fit the manifold, these noise data will make marginal data with high AU. By optimizing the classifier to iteratively reduce the AU, optimal classification result under the current conditions can be obtained. We use L a to optimize AU: In the above formula λ a = [0.005, 0.01] is a parameter that controls the degree of deviations of the regularization part and w St uses the previous definition, and Φ is the total amount of evidence learned by the model. In order to better motivate the learning of the model, we adopted the work of Liu et al.  EU for Training Extractor. If only the AU part is optimized, there will always be this gap between the model prediction and the real data. EU is mainly used to optimize the feature extractor since EU mainly reflects the bias of the model in the prediction. For data D l , given groundtruth labels, we use which can make the model more conservative about making predictions in the next iteration. This reduces our models being affected by misleading evidence and obtains better performance by retaining higher uncertainty to allow the model to have more room to optimize. In order to effectively combine labeled and unlabeled data we adjust the weights of different data: where μ l + μ u = 1, μ l , μ u ∈ [0, 1], are two weight factors. Model. In terms of the feature extractor, we use the latest EfficientNetV2, which, in Feng et al.  In order to avoid overfitting, we used the minimum model in this network and added Dropout to the output end. At the same time, in order to fill the differences between multi-modality data and model input, we have added the fully connected (FC) layer and convolutional layer (Conv) to adaptive adjust input channels. We employed three evidential FC layers proposed by Amini et al. ",1
516,DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs (vol1),,Single-Scale.,"A classical approach is represented by AB-MIL  Multi-Scale. Recently, different authors focused on multi-resolution approaches. DSMIL-LC  Knowledge Distillation. Distilling knowledge from a more extensive network (teacher ) to a smaller one (student) has been widely investigated in recent years ",1
519,DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs (vol1),,TCGA Lung Dataset.,"It is available on the GDC Data Transfer Portal and comprises two subsets of cancer: Lung Adenocarcinoma (LUAD) and Lung Squamous Cell Carcinoma (LUSC), counting 541 and 513 WSIs, respectively. The aim is to classify LUAD vs LUSC; we follow the split proposed by DSMIL ",1
523,DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 24.,1
533,"An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway (vol1)",,Time and Memory Complexities:,"The training time of cryoTWIN (resp. cryoDRGN) is eleven hours (resp. four hours), whereas their memory complexities are comparable. After training cryoTWIN, the running time to compute the pathways including the evaluation time is around one hour, which is much shorter than the running time of the state-of-the-art protocol of a few days ",1
541,Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_64.,1
546,Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks (vol1),,Stage 1 Masked Graph Auto-encoder for Graph Reconstruction.,"In stage 1, a masked graph auto-encoder, containing a topology-aware encoder and a decoder, is designed to exploit the crucial structural information in brain networks. To sufficiently exploit the graph structure, we randomly mask some nodes and the associated edges. The unmasked nodes and edges fed into the topologyaware encode to learn the latent representations. Let H u indicate the feature map in the encoding stage. We define the adjacent matrix of the unmasked nodes as A u , which is taken as the input of the topology-aware encoder, that is The topology-aware encoder consists of three parts: 1) The edge convolution with multiple cross-shaped filters for capturing the locality in the graph according to where w r ∈ R 1×M and w c ∈ R M ×1 are convolution kernels. 2) The node convolution for learning the latent node embedding. It is defined as: n , where w n is the learned filter vector, n ∈ R M ×Dn is the latent unmasked node embedding and D n is the channels in NC. 3) The graph aggregation for achieving the global graph embedding through: where w g is the learned filter vector, H g ∈ R M ×Dg is the graph embedding and D g is the dimensionality in GA. The decoder takes the masked nodes and the latent unmasked node embeddings as inputs, and then produces predictions for the masked nodes and edges by graph convolution operations and the masked edge prediction. The graph convolution is defined as: (l) , where A ∈ R M ×M is the binary adjacency matrix, W denotes trainable weight, H ∈ R M ×D n is the node embedding and D n is the hidden layer size of graph convolutional layers. The masked edge prediction is defined as: Â = H (l+1) (H (l+1) ) T . The reconstruction loss between the prediction graphs and corresponding targets is , where Ât i is the reconstructed brain networks of subject i at time t. Stage 2 Temporal Contrastive Learning. The longitudinal brain networks of a subject acquired at multiple visits characterize gradual disease progression of the brain over time, which manifests a temporal progression trajectory when projected to the latent space. We assume that brain networks features at two consecutive time points from the same subject are similar, while dissimilar from different subjects. Based on this assumption, we introduce a temporal contrastive loss by enforcing an across-sample relationship in the learning process. Specifically, H t g(i) is the brain network features of subject i at time t, H t g(i) and H t+1 g(j) are considered as the positive sample pair if i = j, otherwise they are considered as the negative sample pair. The temporal contrastive framework aims to enlarge the similarity between positive sample pair, and reduce it between the negative sample pair. The similarity calculation function s can be any distance function, and here we utilize cosine similarity. The loss for temporal contrastive learning can be represented as: where τ is a temperature factor that controls the model's discrimination against negative sample pair and exp(.) is an exponential function.",1
552,Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 30.,1
564,vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_58.,1
579,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation (vol1),,Domain Distance Measurement.,"To overcome the limitations of lacking target domain images, we first augment the few-shot images from the target domain with random combinations of various geometric transformations, including random cropping, rotation, flipping, and JigSaw  , where W 1 is the 1-Wasserstein distance.",1
580,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation (vol1),,DoDiSS Computation.,"With the measured domain difference, we can now compute the DoDiSS map of a model. As shown in Fig.  To analyze the model's generalization weakness with respect to the frequency (i, j), we generate perturbed source domain images by adding the Fourier basis noise N i,j = r • D W (i, j) • U i,j to the original source domain image x s as x s + N i,j . D W (i, j) controls the 2 -norm of N i,j and r is randomly sampled to be either -1 or 1. The N i,j only introduces perturbations at the frequency components (i, j) to the original images. The D W (i, j) guarantees that images are perturbed across all frequency components following the real domain shift. For RGB images, we add N i,j to each channel independently following ",1
609,MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation (vol1),,Image Sequence Generator:,"In the sequence-to-sequence generation task, new slice is the combination of the warped previous slice and newly generated texture, weighted by a continuous mask  Semantic Diffusion Refiner: Despite the high cross-slice consistency and spatial continuity achieved by vid2vid, issues such as blocking, blurriness and suboptimal texture generation persist. Given that diffusion models have been shown to generate superior images  For each of the 3 different views, we train a semantic diffusion model (SDM), which takes 2D masks and noisy images as inputs to generate images aligned with input masks. During inference, we only apply small noising steps (10 steps) to the generated images so that the overall anatomical structure and spatial continuity are preserved. After that, we refine the images using the pre-trained semantic diffusion model. The final refined 3D images are the mean results from 3 views. Experimental results show an evident improvement in the quality of generated images with the help of semantic diffusion refiner.",1
610,MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation (vol1),,Datasets:,"We conducted experiments on the thoracic site using three thoracic CT datasets and the brain site with two brain MRI datasets. For both generative models and downstream segmentation tasks, we utilized the following datasets: -SegTHOR  Implementation: For thoracic datasets, we crop and pad CT scans to (96 × 320 × 320). The annotations of six organs (left lung, right lung, spinal cord, esophagus, heart, and trachea) are examined by an experienced radiation oncologist. We also include a body mask to aid in the image generation of body regions. For brain MRI datasets, we use Freesurfer ",1
611,MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation (vol1),,Setup:,We compare the synthetic image quality with DDPM  According to Table ,1
614,MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 72.,1
618,PET Image Denoising with Score-Based Diffusion Probabilistic Models (vol1),,Algorithm 1:,Training stage. until convergence,1
623,PET Image Denoising with Score-Based Diffusion Probabilistic Models (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 26.,1
637,Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos (vol1),,Barlow Feature Alignment Loss (BFAL).,"We introduce a novel loss, which encourages features between the source and target to be similar to each other while reducing the redundancy between the learnt features. BFAL works on pairs of feature projections of the source and target. More specifically, let f s ∈ R BXD and f t ∈ R BXD be the features corresponding to the source and target domain, respectively. Here B represents the batch size and D represents the feature dimension. Similar to  Finally, the BFAL is computed using the L2 loss between the elements of C 1 and the identity matrix I as follows where μ is a constant. Intuitively, the first term of the loss function can be thought of as a feature alignment term since we push the diagonal elements in the covariance matrix towards 1. In other words, we encourage the feature projections between the source and target to be perfectly correlated. On the other hand, by pushing the off-diagonal elements to 0, we decorrelate different components of the projections. Hence, this term can be considered a redundancy reduction term, since we are pushing each feature vector component to be independent of one another. BFAL is inspired by a recent technique in self-supervised learning, called the Barlow Twins  There are two main sub-parts of the architecture -the Feature Extractor F , and the Source Classifier C. First, we divide the training images randomly into batches of pairs {x s , x t } and apply F on them, which gives us the features extracted from these sets of images. For the Feature Detector, we show the effectiveness of our novel loss using ViT and ResNet50 both of which have been pre-trained on ImageNet. The features obtained are denoted as f s and f t for the source and target domains, respectively. Next, we apply C on these features to get logits for the classification task. The source classifier is a feed forward neural network, which is initialized from scratch. These logits are used, along with the source labels y s to compute the source cross entropy loss as , where M represents the number of classes, B represents the total minibatches, while m and b represent their respective indices. The features f s and f t are further used to compute the Correlation Alignment(CORAL) loss and the BFAL, which enforce the feature extractor to align its weights so as to learn features that are domain agnostic as well as nonredundant. The BFAL is calculated as mentioned in the previous subsection. The CORAL loss is computed as depicted in Eq. 4, following the UDA method Deep CORAL  where Each of these three losses plays a different role in the UDA task. The cross entropy loss encourages the model to learn discriminative features between images with different instruments. The CORAL loss pushes the features between the source and target towards having a similar distribution. Finally, the BFAL tries to make the features between the source and the target non-redundant and same. BFAL is a stricter loss than CORAL as it forces features to not only have the same distribution but also be equal. Further, it also differs from CORAL in learning independent features as it explicitly penalizes non-zero non-diagonal entries in the correlation matrix. While using BFAL alone gives good results, using it in addition to CORAL gives slightly better results empirically. We note these observations in our ablation studies. Between the cross entropy loss and the BFAL, an adversarial game is played where the former makes the features more discriminative and the latter tries to make them equal. The optimal features thus learnt are different in aspects required to identify instruments but are equal for any domain-related aspect. This property of the Barlow Adaptor is especially useful for surgical domains where the background has similar characteristics for most of the images within a domain. For example, for cataract surgery images, the position of the pupil or the presence of blood during the usage of certain instruments might be used by the model for classification along with the instrument features. These features depend highly upon the surgical procedures and the skill of the surgeon, thus making them highly domain-specific and possibly unavailable in the target domain. Using BFAL during training attempts to prevent the model from learning such features.",1
641,Gall Bladder Cancer Detection from US Images with only Image Level Labels (vol1),,Contributions:,The key contributions of this work are: -We design a novel DETR variant based on MIL with self-supervised instance learning towards the weakly supervised disease detection and localization task in medical images. Although MIL and self-supervised instance learning has been used for CNNs ,1
644,Gall Bladder Cancer Detection from US Images with only Image Level Labels (vol1),,Self-supervised Instance Learning:,"Inspired by  The loss over the instances is given by Eq. 4: Here x n ij denotes the score of i-th instance for j-th class at layer n. Following  is applied to stabilize the loss. Assuming λ to be a scaling value, the overall loss function is given in Eq. 5:",1
646,Gall Bladder Cancer Detection from US Images with only Image Level Labels (vol1),,Generality of the Method:,"We assess the generality of our method by applying it to polyp detection on colonoscopy images. The applicability of our method on two different tasks -(1) GBC detection from US and (2) Polyp detection from Colonoscopy, indicates the generality of the method across modalities.",1
647,Gall Bladder Cancer Detection from US Images with only Image Level Labels (vol1),,Ablation Study:,"We show the detection sensitivity to the self-supervised instance learning module in Table  Classification Performance: We compare our model with the standard CNNbased and Transformer-based classifiers, SOTA WSOD-based classifiers, and SOTA classifiers using additional data or annotations (Table ",1
649,Gall Bladder Cancer Detection from US Images with only Image Level Labels (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 20.,1
659,Structured State Space Models for Multiple Instance Learning in Digital Pathology (vol1),,Multitask Learning Results.,"We explored the ability of our model to combine slide-and patch-level information on the CAMEYLON16 dataset. We compared our model with the best performing model on CAMELYON16, TransMIL. Both models were trained according to Eq. 7 with λ = 5 tuned by hand. In Table ",1
675,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision (vol1),,Method,"For data augmentation, we apply random cropping, scaling, rotation, and Gaussian noise injections. A windowing approach that covers the intensity ranges of lungs and soft tissues is used to scale CT intensity values to [-1, 1]. The sampling hyperparameters consist of 100 positive pixel pairs (n pos = 100), 100 hard negative pixel pairs, and 200 diverse negative pixel pairs (n neg = 300).",1
676,Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision (vol1),,Evaluation Metrics:,"We use mean Euclidean distance (MED) to measure the distance between predicted lesion center and ground truth, and the center point matching accuracy (i.e., percentage of accurately matched lesions given the annotated lesion radius), denoted with CPM@Radius. For lesions of large sizes, we set a maximum distance limit of 10 mm as acceptance criteria ",1
689,Geometry-Invariant Abnormality Detection (vol1),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_29.,1
696,Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis (vol2),,Contributions of,"Our Work: our model 1) can be trained efficiently compared to existing spatio-temporal graph deep models with a significantly reduced number of parameters, 2) flexibly incorporates irregular space and time into prediction, 3) yields interpretable results that quantify the contribution of each node   contains F node features for N nodes and E i ∈ R N ×N is a weighted adjacency matrix whose elements denote connection strength between two nodes. Given a population of G m with C classes, STGMLP aims to classify the label of each G m by leveraging both temporal and spatial variations of the graph set from different groups. Note that the label of each sample (i.e., longitudinal graph set) is consistent over time.",2
697,Mixing Temporal Graphs with MLP for Longitudinal Brain Connectome Analysis (vol2),,Overview of STGMLP. STGMLP mixes graph features across space and time,"with Graph Spatial Mixer (GSM) and Graph Temporal Mixer (GTM), respectively. GSM performs a per-graph operation (i.e., node-mixing) and GTM accounts for cross-temporal operation (i.e., graph-mixing) between multiple graphs in a G m . Figure  The overall structure of STGMLP is shown in Fig.  Graph Spatial Mixer. GSM encodes node features and a graph structure of a graph G i with graph convolution. The node-mixing MLP (R F → R F ) acts on rows of X i , and it is shared across G m for all N × T m nodes. Let f (•) be an operation of node-mixing MLP which takes Ẽi and X i as inputs, where Ẽi is a normalized E i to ensure unbiased strength of the connectivity. It includes selfconnections I N (i.e., identity matrix) and computed as Ẽi = D-  where W 0 and W 1 are trainable weights and LN(•) is a layernorm function. The output (X i ) j ∈ R F accounts for a latent vector of local graph structure at node j. Stacking (X i ) j s up to the number of nodes N , an outcome X i ∈ R N ×F is derived for an input G i . In this way, a set of whole outputs from T m GSMs is derived as {X i } Tm i=1 ∈ R Tm×N ×F . Notice that the GSM can be stacked D times by iteratively taking the X i as an updated input to encode a wider range of local graph structures. After performing max pooling on T m and F dimensions of {X i } Tm i=1 , the condensation of spatial features across G m = {G 1 , ..., G Tm } is obtained as a N -dimensional vector S. Graph Temporal Mixer. GTM performs a cross-temporal operation on multiple ""pairs"" of graphs. This graph-mixing encodes the relations between graphs of different time-points. Given T m graphs from a subject, P pairs of graphs, each pair as a set {G i1 , G i2 }, are selected where P is a user parameter. For each pair for {i 1 , i 2 }, an averaged connectivity Ẽp =( Ẽi1 + Ẽi2 )/2 and X p ∈ R N ×2F as a concatenation of X i1 and X i2 are inputted into the graph-mixing MLP g(•). In our work, we choose to input pairs of temporally adjacent graphs together with the first-and-last graph pair to encode a temporal sequence. The g(•) acts on rows of X p , mapping R 2F → R F . It transforms the features of node j (i.e., (X p ) j ) into F -dimensional latent vector, and the projection is performed across the whole node pairs in parallel. Similar to the node-mixing MLP, graph-mixing MLP contains two FC layers with weights W 2 and W 3 and a GELU σ(•) as (2) As in the GSM, each (X p ) j is stacked N times to be a X p . For P GTMs, an output {X p } P p=1 ∈ R P ×N ×F is obtained and reduced into T ∈ R N by max pooling on P pairs and F node features. Note that, unlike GSMs, Ẽp is used only once in Eq. (  where W 4 and W 5 are weight matrices and σ(•) is a nonlinearity. With this STM, irregular space and time components can be flexibly integrated into a prediction. Longitudinal Graph Classifier. To take a full advantage from the extracted features, S and T are combined together with F via a skip connection. These features collected from diverse branching paths contain both low and high-level information extracted from the graphs, and their integration provides strong ensemble-like results  where W 6 is a set of trainable weights of the FC layer for class prediction. Given the ground truth Y , the cross-entropy loss is defined with 2 -regularization as where W is a set of trainable parameters and λ controls a regularization strength.",2
719,NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants (vol2),,Competing Mehtods,ACC AUC SEN SPE SphericalCNN ,2
729,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models (vol2),,Diffusion Sampler,"Fig.  Despite recent progress in these methods for medical image analysis, existing models face two major challenges when applied to colonoscopy image analysis. Firstly, the foreground (polyp) of colonoscopy images contains rich pathological information yet is often tiny compared with the background (intestine wall) and can be easily overwhelmed during training. Thus, naive generative models may generate realistic colonoscopy images but those images seldom contain polyp regions. In addition, in order to generate high-quality annotated samples, it is crucial to maintain the consistency between the polyp morphologies in synthesized images and the original masks, which current generative models struggle to achieve. To tackle these issues and inspired by the remarkable success achieved by diffusion models in generating high-quality CT or MRI data  In summary, our contributions are three-fold: (1) Adaptive Refinement SDM: Based on the standard semantic diffusion model ",2
731,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models (vol2),,Re-Weighting Module,"Diffusion Loss ℒ Thus, instead of training the model μ θ to predict μt , we can train the model θ to predict ˜ , which is easier for parameterization and learning.",2
732,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models (vol2),,Refinement Loss ℒ,"In this paper, we propose an adaptive refinement semantic diffusion model, a variant of DDPM, which has three key parts, i.e., mask conditioning, adaptive loss re-weighting, and prediction-guided sample refinement. The overall illustration of our framework is shown in Fig. ",2
741,ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_32.,2
746,Synthetic Augmentation with Large-Scale Unconditional Pre-training (vol2),,Classifier-guided Conditional Synthesis.,"To enable conditional image generation with our HistoDiffusion, we further apply the classifier-guided diffusion sampling proposed in  where g is the guidance scale. Then the DM reverse process in HistoDiffusion can finally generate a novel latent z0 satisfying the class condition y through a Markov chain starting with a standard Gaussian noise z T ∼ N(0, I) using p θ,φ (z t-1 |z t , y) defined as follows: p θ,φ (zt-1|zt, y) = N (zt-1; μθ (zt|y), Σ θ (zt)) .  Selective Augmentation. To further improve the efficacy of synthetic augmentation, we follow ",2
749,Synthetic Augmentation with Large-Scale Unconditional Pre-training (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 71.,2
755,Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation (vol2),,Model:,"The encoder of G(z) consisted of five convolutional layers and three fully connected layers, with a latent dimension of 12 for z. The decoder consisted of five fully connected layers to output the parameters θ for T θ . We trained the G(z) for a total of 400 epochs and a batch size of 16. We also implemented early stopping if the validation loss does not improve for 20 epochs. We used Adam optimizer  Results: We evaluated G(z) with two criteria. First, the model needs to be able to reconstruct x tgt by generating θ to transform x src . Second, the model needs to be able to generate diverse transformed tumour samples for a given tumour sample. Figure ",2
767,Few Shot Medical Image Segmentation with Cross Attention Transformer (vol2),,Effectiveness of CMAT Block:,"To demonstrate the importance of our proposed CAT-Net in narrowing the information gap between the query and supporting images and obtaining enhanced features, we conducted an ablation study. Specifically, we compared the results of learning foreground information only from the support (S →Q) or query image (Q→S ) and obtaining a single enhanced feature instead of two (S ↔Q). It can be observed that using the enhanced query feature (S →Q) achieves 66.72% in Dice, outperforming only using the enhanced support feature (Q→S ) by 0.74%. With our CMAT block, the mutual boosted support and query feature (S ↔Q) could improve the Dice by 1.90%. Moreover, the iteration refinement framework consistently promotes the above three variations by 0.96%, 0.56%, and 2.26% in Dice, respectively (Table ",2
769,Few Shot Medical Image Segmentation with Cross Attention Transformer (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_22.,2
780,Rethinking Semi-Supervised Federated Learning: How to Co-train Fully-Labeled and Fully-Unlabeled Client Imaging Data (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 39.,2
788,Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification (vol2),,Embeddings:,We generate embeddings of image class labels using BioBERT ,2
794,Class Specific Feature Disentanglement and Text Embeddings for Multi-label Generalized Zero Shot CXR Classification (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 26.,2
804,CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training (vol2),,Method,CheXpert 5x200 MIMIC-CXR Total RSUM ACC R@1 R@5 R@10 R@1 R@5 R@10 Vanila CLIP 58.9 4. ,2
807,CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_10.,2
811,Distilling BlackBox to Interpretable Models for Efficient Transfer Learning (vol2),,Learning the Experts.,"For iteration k, the loss L k (g k ,π k ) distills the expert g k from f k-1 , BB of the previous iteration by solving the following loss: is the cumulative probability of the sample covered by the residuals for all the previous iterations from 1, • • • , k -1 (i.e., k-1 j=1 1 -π j (c i ) ) and the expert g k at iteration k (i.e., π k (c i )). Learning the Residuals. After learning g k , we calculate the residual as, of logits). We fix Φ and optimize the following loss to update h k to specialize on those samples not covered by g k , effectively creating a new BB f k for the next iteration (k + 1): We refer to all the experts as the Mixture of Interpretable Experts (MoIE-CXR). We denote the models, including the final residual, as MoIE-CXR+R. Each expert in MoIE-CXR constructs sample-specific FOLs using the optimization strategy and algorithm discussed in ",2
814,Distilling BlackBox to Interpretable Models for Efficient Transfer Learning (vol2),,Model,Effusion  ,2
815,Distilling BlackBox to Interpretable Models for Efficient Transfer Learning (vol2),,MoIE-CXR does not Compromise BB's Performance. Analysing MoIE-CXR:,Table ,2
816,Distilling BlackBox to Interpretable Models for Efficient Transfer Learning (vol2),,Analysing MoIE-CXR+R:,"To compare the performance on the entire dataset, we additionally report MoIE-CXR+R, the mixture of interpretable experts with the final residual in Table  Identification of Harder Samples by Successive Residuals. Figure  Applying MoIE-CXR to the Unseen Domain. In this experiment, we utilize Algorithm 1 to transfer MoIE-CXR trained on MIMIC-CXR dataset to Stanford Chexpert ",2
818,Distilling BlackBox to Interpretable Models for Efficient Transfer Learning (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 59.,2
820,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification (vol2),,Present Work.,"In this work, we jointly learn from longitudinal medical imaging, demographics, billing codes, medications, and lab values to classify SPNs. We converted 9195 non-imaging event streams from the EHR to longitudinal curves to impute cross-sections and synchronize across modalities. We use Independent Component Analyses (ICA) to disentangle latent clinical signatures from these curves, with the hypothesis that the disease mechanisms known to be important for SPN classification can also be captured with probabilistic independence. We leverage a transformer-based encoder to fuse features from both longitudinal imaging and clinical signature expressions sampled at intervals ranging from weeks to up to five years. Due to the importance of time dynamics in SPN classification, we use the time interval between samples to scale self-attention with the intuition that recent observations are more important to attend to than older observations. Compared with imaging-only and a baseline that aggregates longitudinal data into bins, our approach allowed us to incorporate additional modalities from routinely collected EHRs, which led to improved SPN classification.",2
835,EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 8.,2
851,Prediction of Cognitive Scores by Joint Use of Movie-Watching fMRI Connectivity and Eye Tracking via Attention-CensNet (vol2),,Models,"Data AUC Edge Node Linear [8]  -mfM RI2 41.94±0.81 GCN [16]  Trj2 mfM RI2 48.51±0.94 CensNet [17]  mfM RI2 49.75±0.65 CensNet [17]  Trj2 mfM RI2 50.36±0.71 Ppl2 52.91±0.73 FNN [9]  - All nonlinear deep neural networks yield significant improvement in contrast to the linear method (41.94 ± 0.81). A concatenation of mfMRI features (mfMRI2 +3 rows) does not improve the prediction accuracy in contrast to that on a single dataset, suggesting the importance of the strategy selection for concatenating features from multiple datasets. Within a single movie dataset (unshaded rows), models integrating mfMRI and eye tracking outperform the models (FNN and BrainNetCNN) that used single modality (mfMRI). After integrating mfMRI and eye behavior from multiple datasets, our results outperform all the-state-of-arts. These results demonstrate the effectiveness of integration of brain activity and eye behavior to one framework in cognition prediction. Also, given the limited subjects, multiple loads of stimuli integrated via attention modules could significantly improve the prediction performance.",2
879,Spatiotemporal Hub Identification in Brain Network by Learning Dynamic Graph Embedding on Grassmannian Manifold (vol2),,"(b). In the Grassmannian manifold space, F(t m ) 2 2 can be accurately measured by the squared geodesic distance Log F(t m ) (F(t m-1 )) = P-tr(F(t m )F(t m ) T F(t m-1 )F(t m-1 ) T ).","Therefore, the optimization of Eq. (  ) onto the tangent space via the orthogonal projection  Given F(t m ) , we update the modified F(t m ) using an exponential mapping operation  Optimizing Temporal Hub Node Set. After updatingF(t m ), the energy function of Eq. (  where represents the distance between the temporal graph embedding F(t m ) at the i th and j th nodes. The optimal set of temporal hub nodes s at the t m temporal point can be achieved using the convex optimization scheme proposed in ",2
893,TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 31.,2
911,Aneurysm Pose Estimation with Deep Learning (vol2),,Methods,AP0.1 (%) Sensitivity0.5 (%) FPs/case0. and Faster RCNN  As shown in Table ,2
917,Localized Questions in Medical Visual Question Answering (vol2),,DME-VQA,"We automatically generated binary questions with the structure ""is there [instrument] in this region?"" and corresponding masks as rectangular regions with random locations and sizes. Based on the ground-truth label maps, the binary answers were labeled ""yes"" if the region contained at least one pixel of the corresponding instrument and ""no"" otherwise. The questions were balanced to maintain the same amount of ""yes"" and ""no"" answers. Figure ",2
921,Localized Questions in Medical Visual Question Answering (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 34.,2
925,PLD-AL: Pseudo-label Divergence-Based Active Learning in Carotid Intima-Media Segmentation for Ultrasound Images (vol2),,Algorithm 1: PLD-AL,"1 Input: Initial labeled pool DL = XL × YL; Unlabeled pool XU ; Judgment threshold τ ; Refining threshold λ; 2 Initialize θS and θT ; Fit the mIoU curve In each AL iteration, we use a mean-teacher architecture as the backbone of AL. The student and the teacher networks, respectively parameterized by θ S and θ T , share the same neural network architecture F , which maps the carotid ultrasound image x ∈ R I×J to the extended three-dimensional CIM mask probability p ∈ R I×J×2 , whose 3rd-dimensional component p ij ∈ R 2 denotes the softmax probability output for binary classification at the pixel (i, j). We use the divergence between pseudo-labels generated by student and teacher networks to assist in selecting data for the expert to annotate.",2
940,Federated Uncertainty-Aware Aggregation for Fundus Diabetic Retinopathy Staging (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_21.,2
948,Debiasing Medical Visual Question Answering via Counterfactual Training (vol2),,Influence of Hyperparameters.,"The influence results of the top-K and the hyperparameter α are conducted in Table  Quantitative Analysis. As Fig.  Given the same question but different medical images and answers, the proposed model correctly predict the various answers while the MEVF+BAN ",2
960,FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_65.,2
968,L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space (vol2),,Definition 1. Positive Definite Kernel,"Popular kernel functions (e.g., the Gaussian RBF) operate on flat-curvature Euclidean spaces. In R n , the Gaussian RBF kernel method is defined as However, using the geodesic distance in a hyperbolic space along with an RBF function similar to Eq. (4) (i.e., replacing z i -z j 2 with the geodesic distance) does not lead to a valid positive definite kernel. Theoretically, a valid RBF kernel is impossible to obtain for hyperbolic space using geodesic distance  to embed hyperbolic data to RKHS via the following valid pd kernel (see  Now, in L3 setting, we have two models h t and h t-1 at our hand at time t. We aim to improve h t while ensuring the past knowledge incorporated in h t-1 is kept within h t . Assume Z t and Z t-1 are the extracted feature vectors for input X using current and old feature extractor, h t feat and h t-1 feat , respectively. Unlike other existing distillation methods, we employ an independent 2-layer MLP for each fixed-curvature space to project extracted features to a new lower-dimensional embedding space on which we perform further operations. This has two benefits, (i) it relaxes the strong constraint directly applied on Z t and Z t-1 and (ii) reduce the computation cost of performing kernel method. Since we are interested in modeling embedding structure in zero-curvature Euclidean and negative-curvature hyperbolic spaces, we have two MLP as projection modules attached to feature extractors, namely g e and g h . Our Idea. Our main idea is that, for a rich and overparameterized representation, the data manifold is low-dimensional. Our algorithm makes use of RKHS, which can be intuitively thought of as a neural network with infinite width. Hence, we assume that the data manifold for the model at time t-1 is well-approximated by a low-dimensional hyperplane (our data manifold assumption). Let  = min In Eq. (  In Eq. (  Here, β is a hyper-parameter that controls the weight of distillation between the Euclidean and hyperbolic spaces. We can employ Eq. ( ",2
973,L3DMC: Lifelong Learning Using Distillation via Mixed-Curvature Space (vol2),,"Machine Learning -Explainability,","Bias, and Uncertainty I",2
977,An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography (vol2),,Self Attention Block:,"The self-attention mechanism  , where U denotes the input of self-attention block and φ(•, •) represents linear transformation. Then, Attention Weights are computed using Query and Key: where d k stands for the dimensions of the Key, and √ d k works as a scaling factor. The softmax function was applied to adjust the range of the value in attention weights (M att ) to [0, 1]. Unlike the transformer model, the attention weights are first compressed into a one-dimensional vector by a layer of global average pooling (ψ) and normalized by a sigmoid function. More precisely, we compute Z att = sigmoid(ψ(M att )). Finally, the output of SA Block X is computed by : X = κ(Z att , V ), where κ denotes the electrode-wise production.",2
979,An Interpretable and Attention-Based Method for Gaze Estimation Using Electroencephalography (vol2),,Implementation Details:,"The experiments are implemented with PyTorch  Evaluation: For Position task, Euclidean distance is applied as the evaluation metric in both pixels and visual angles. Compared to pixel distance, visual angles depend on both object size on the screen and the viewing distance, thus enabling the comparison across varied settings. The performance of Direction Task is measured by the square root of the mean squared error (RMSE) for the angle (in radians) and the amplitude (in pixels) of saccades. In order to avoid the error caused by the repeatedness of angles in the plane (i.e. 2π and 0 rad represents the same direction), atan(sin(α), cos(α)) is applied, just like in angle loss.",2
985,Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation (vol2),,Related Work:,"Interactive segmentation methods for Artificial Intelligence (AI) assisted annotation have shown promising applications in the existing literature  Due to their quick adaptability and efficiency, a number of existing online likelihood methods have been applied as interactive segmentation methods  MONet enables human-in-the-loop online learning to perform AI-assisted annotations and should not be mistaken for an end-to-end segmentation model. We perform expert evaluation which shows that adaptively learned MONet outperforms existing state-of-the-art, achieving 5.86% higher Dice score with 24.67% less perceived NASA-TLX workload score evaluated.",2
994,Adaptive Multi-scale Online Likelihood Network for AI-Assisted Interactive Segmentation (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 53.,2
1003,Adapter Learning in Pretrained Feature Extractor for Continual Learning of Diseases (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 7.,2
1009,Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging (vol2),,Method,"No Privacy z = 0.5 z = 1.0 z = 1.5 Privacy Setup. We use the Opacus'  Implementation Details. We use Adam optimize, set the local update epoch to 1, and set total communication rounds to 100. We use DenseNet121 ",2
1012,Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging (vol2),,Stability of Adaptive Intermediary,"Estimation. Finally, we analyze the historical variation of our adaptive intermediary strategy in Fig. ",2
1017,Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation (vol2),,". , n}","Initialize all quantization tables with ones. 5: for t ← 1 to T do 6: Merge all adversarial patches to form X 9: L(X, X , Y) = L dice (M θ (X ), Y) -Lssim(X, X ) 10: end for 13: end function 14: Return X Algorithm 2. Volumetric Adversarial Frequency Training (VAFT) Freq. Attack on clean images. 7: Backward pass and update M θ 9: end for 10: end for 11: M ← M θ AT robust model after training completion. 12: Return M extent to which DCT coefficients are perturbed. The higher the value of q max , the more information is lost. The drop in perception quality of the adversarial sample and the accuracy of the model are directly proportional to the value of q max . To increase the perceptual quality of adversarial samples, we also minimize the structural similarity loss  where L ssim (X, X ) = 1 -1 n n i=1 SSIM(x i , x i ) is structural similarity loss ",2
1021,Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_43.,2
1025,Reconstructing the Hemodynamic Response Function via a Bimodal Transformer (vol2),,Transformers.,"The self-attention mechanism introduced by Transformers  where Q ∈ R N ×d , K ∈ R k×d and V ∈ R k×d represent the packed N queries, k keys and values tensors respectively. Keys, queries and values are obtained using linear transformations of the sequence's elements. A multi-head self-attention layer is defined by extending the self-attention mechanism using h attention heads, i.e. h self-attention functions applied to the input, reprojected to values via a dh × D linear layer.",2
1026,Reconstructing the Hemodynamic Response Function via a Bimodal Transformer (vol2),,Neuronal Encoding.,"To obtain the initial Spatio-Temporal Encoding, for the prediction at time t, we project each neuron to a high d dimensional embedding φ s t ∈ R ts×n×d by modulating it with its spike value such that , where W ∈ R d denotes the neuronal encoding. The embedding is modulated by the magnitude of the spike, such that higher neuronal activities are projected farther in the embedding space. The temporal encoding is defined using sinusoidal encoding  In order to incorporate the spatial information of the neurons, we propose to insert spatial encoding by importing the pairwise information directly into the self-attention layer. For this, we multiply the distance relation by the similarity tensor as follows with denoting the Hadamard product, and ψ S (D S ) : R + → R + an elementwise learnable parameterized similarity function. This way, the similarity function scales the self-attention map according to the distance between the elements (in our case the neurons).",2
1027,Reconstructing the Hemodynamic Response Function via a Bimodal Transformer (vol2),,Vascular Decoding.,"The spatio-temporal encoding of the vascular data is similar to the embedding performed by the encoder. The information on each vascular segment is embedded in a high-dimensional vector φ F t ∈ R tv×m×d to be further projected by the temporal encoding. The spatial geometric information is incorporated via the pairwise vascular segments' distance matrix D F via the decoder's self-attention module A F . The most important element of the decoder is the cross-attention module, which incorporates neuronal information for vascular prediction. Given the final neuronal embeddings φ s t , the cross-attention module performs cross-analysis of the neuronal embeddings such that where Q F and K S represent the affine transform of φ F t and φ s t , respectively. Here also, the (non-square) cross-attention map is modulated by the neuronvessel distance matrix D SF . The spatio-temporal map is of dimensions A SF ∈ R tv×ts×h×m×n where h denotes the number of attention heads. Thus, we perform aggregation by averaging over the neuronal time dimension, in order to remain invariant to the temporal neuronal embedding and to gather all past neuronal influence on blood flow rates. This way, one can observe that the proposed method is not limited to any spatial or time constraint. The model can be deployed in different spatiotemporal settings at test time, thanks to both the geometric spatial encoding and the Transformer's sequential processing ability. Finally, the output module reprojects the last time vessel embedding into the prediction space.",2
1028,Reconstructing the Hemodynamic Response Function via a Bimodal Transformer (vol2),,Architecture and Training.,"The initial encoding defines the model embedding dimension d = 64. The encoder and the decoder are defined as the concatenation of L = 3 layers, each composed of self-attention and feed-forward layers interleaved with normalization layers. The decoder also contains N additional cross-attention modules. The output layer is defined by a fully connected layer that projects the last vascular time embedding into the objective dimension m. An illustration of the model is given in Fig.  The dimension of the feed-forward network is four times that of the embedding  The training objective is the Mean Squared Error loss The Adam optimizer ",2
1031,Reconstructing the Hemodynamic Response Function via a Bimodal Transformer (vol2),,Acknowledgements,. The authors thank ,2
1032,Reconstructing the Hemodynamic Response Function via a Bimodal Transformer (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 35.,2
1043,Continual Learning for Abdominal Multi-organ and Tumor Segmentation (vol2),,Image-Aware Organ-Specific Heads:,"The vanilla Swin UNETR has a Softmax layer as the output layer that predicts the probabilities of each class. We propose to replace the output layer with multiple image-aware organ-specific heads. We first use a global average pooling (GAP) layer on the last encoder features to obtain a global feature f of the current image X. Then for each organ class k, a multilayer perceptron (MLP) module is learned to map the global image feature to a set of parameters θ k : where E(X) denotes the encoder feature of image X. An output head for organ class k is a sequence of convolution layers that use parameters θ k as convolution kernel parameters. These convolution layers are applied to the decoder features, which output the segmentation prediction for organ class k: where E is the encoder, D is the decoder, σ is the Sigmoid non-linear layer and P (Y k j = 1) denotes the predicted probability that pixel j belongs to the organ class k. The predictions for each class are optimized by Binary Cross Entropy loss. The separate heads allow independent probability prediction for newly introduced and previously learned classes, therefore minimizing the impact of new classes on old ones during continual learning. Moreover, this design allows multi-label prediction for cases where a pixel belongs to more than one class (e.g., a tumor on an organ).",2
1044,Continual Learning for Abdominal Multi-organ and Tumor Segmentation (vol2),,Text Driven Head Parameter Generation:,"We further equip the segmentation heads with semantic information about each organ class. With the widespread success of large-scale vision-language models, there have been many efforts that apply these models to the medical domain  where ω k is the text embedding for organ class k. CLIP embeddings carry highlevel semantic meanings and have the ability to connect correlated concepts. Therefore, it guides the MLP module to generate better convolution parameters for each organ class. More importantly, the fixed-length CLIP embedding allows us to adapt the pre-trained model to open-vocabulary segmentation and extend to novel classes. ",2
1045,Continual Learning for Abdominal Multi-organ and Tumor Segmentation (vol2),,Difference from Universal Model,Unlike Liu et al. ,2
1048,Continual Learning for Abdominal Multi-organ and Tumor Segmentation (vol2),,Baselines and Metrics:,"For a fair comparison, all the compared methods use the same Swin UNETR ",2
1049,Continual Learning for Abdominal Multi-organ and Tumor Segmentation (vol2),,Implementation Details:,"The proposed model architecture is trained on new classes with pseudo labeling of old classes. No other distillation techniques are used. We use a lightweight design for the image-aware organ-specific heads. Each head consists of three convolution layers. The number of kernels in the first two layers is 8, and in the last layer is 1. All the compared models are trained using the AdamW optimizer for 100 epochs with a cosine learning rate scheduler. We use a batch size of 2 and a patch size of 96 × 96 × 96 for the training. The initial ",2
1050,Continual Learning for Abdominal Multi-organ and Tumor Segmentation (vol2),,Results:,"The continual segmentation results using the JHH dataset and public datasets are shown in Tables  To evaluate the proposed model designs, we also conduct the ablation study on the JHH dataset, shown in Table ",2
1052,Continual Learning for Abdominal Multi-organ and Tumor Segmentation (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 4.,2
1054,Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations? (vol2),,Related Work on Comparing Explanation Techniques.,"A number of works have studied the quality of post-hoc explanation techniques. The vast majority of work focuses exclusively on gradient-based approaches (e.g.  In their landmark study, Adebayo et al.  A small number of works specifically investigate explanations' sensitivity to spurious correlations. In closely related work to ours, Adebayo et al.  Contributions. We present a rigorous evaluation of post-hoc explanations and inherently interpretable techniques for the identification of spurious correlations in a medical imaging task. Specifically, we focus on the task of diagnosing cardiomegaly from chest x-ray data with three types of synthetically generated spurious correlations (see Fig. ",2
1059,Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations? (vol2),,Confounder Sensitivity (CS).,"Firstly, the explanations should be able to correctly attribute the confounder if classifier bases its decision on it. We assess this property by summing the number of true positive attributions divided by the total number of confounded pixels for each test image. We consider a pixel a true positive if it is part of the pixels affected by the confounder and in the top 10% attributed pixels according to a visual explanation. Thus the maximum sensitivity of 1 is obtained if all confounded pixels are in the top 10% of the attributions. Note that we do not penalise attributions outside of the confounding label as those can still also be correct. To guarantee that we only evaluate on samples for which the prediction is actually influenced by the confounder, we only include images for which the prediction with and without the confounding label is of the opposite class. To reduce computation times we use a maximum of 100 samples for each evaluation. An optimal explanation methods should obtain a CS score of 0 if the data contains p = 0% confounded data points, since in that case the spurious signal should not be attributed. For increasing p the confounder sensitivity should increase, i.e. the explanation should reflect the classifiers increasing reliance on the confounder.",2
1060,Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations? (vol2),,Sensitivity to Prediction Changes via Explanation NCC.,"Secondly, the explanations should not be invariant to changes in classifier prediction. That is, if the classifier's prediction for a specific image changes when adding or removing a confounder, then the explanations should also be different. We measure this property using the average normalised cross correlation (NCC) between explanations of test images when confounders were either present or absent.Again, we only evaluate on images for which the prediction changes when adding the confounder as in these cases, we know the classifier is relying on confounders, and we evaluate a maximum of 100 samples. An optimal explanation method should obtain a high NCC score if the training data contains p = 0% confounded data points, since in that case the explanation with and without the confounder should be similar. For increasing p the NCC score should decrease to reflect the classifiers increasing reliance on the confounder.",2
1068,Efficient Subclass Segmentation in Medical Images (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_25.,2
1074,FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation (vol2),,Models and Training,"Hyper-Paramters. We employ the ResNet-18 architecture as the backbone model. Our approach initiates local-global interpolation at the 75% training phase, consistent with the default hyper-parameter setting of SWA. We utilize the Adam optimizer with learning rate of 1e-3, momentum coefficients of 0.9 and 0.99 and set the batch size to 16. We set the local training epoch to 1 and perform a total of 1, 000 communication rounds.",2
1078,FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 30.,2
1100,Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos (vol2),,Methods,"Accuracy F1-score Precision Recall I3D  In this study, we implemented CSG-3DCT in Pytorch, using an NVIDIA A40 GPU. Unless specified, we trained our model using 8-frame input plaque clips. All frames were resized to 256 × 256. The learnable weights of QKV projection and LayerNorm weights in spatial dimension branch of intra-dimension ST learning module were initialized with those from transformer branch in Conformer  Figure  Ablation Study. We performed ablation experiments in the last 6 rows of Table  1) Effects of Different Key Components of Our Model Design. We compared CSG-3DCT with three variants (i.e., -Base, -SWA * , and -CA * ) to analyze the effects of different key components. Compared with -Base, each of our proposed modules and their combination can help improve the accuracy. We adopt CA in our final model for its good performance. 2) Effects of Plaque Clip Length. We only investigated the effects of our model on 8-frame and 16-frame input clips due to limited GPU memory. We can find in Table  3) Effectiveness of Initialization with ImageNet. We evaluated the value of training models starting from ImageNet-pretrained weights compared with scratch. It can be seen in Table ",2
1102,Inflated 3D Convolution-Transformer for Weakly-Supervised Carotid Stenosis Grading with Ultrasound Videos (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_48.,2
1104,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing (vol2),,Contributions: Our contributions are as follows:,"-We propose a novel image-editing algorithm, U3-Net, to synthesize images for medical education via self-supervised segmentation. -U3-Net can faithfully synthesize intended anatomical elements according to the editing operation on the segmentation labels. -Evaluation by five expert physicians showed that the edited images were natural as medical images with the intended features.",2
1107,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing (vol2),,Random Image Transformation:,"We consider a sequence of image transformations [t 1 , . . . , t n ] specified by the type (e.g., image rotation) and magnitude (e.g., degree of rotation) of each transformation:  Cluster Assignment and Update: In the CL module, K-means clustering in the first iteration initializes K mean vectors µ k ∈ R D . Then, the embedding vector of the i-th pixel e i∈{1,...,H×W } ∈ R D in the embedding maps, E 1 and E 2 , is assigned to the cluster with the nearest mean vector as follows: , where y i is the cluster index of the i-th pixel. By replacing embedding vectors with their respective mean vectors, quantized embedding maps, E q1 and E q2 , are generated g(E) = E q = [µ y1 , . . . , µ yH×W ] ∈ R D×H×W . The cluster indices form the segmentation maps S = [y 1 , . . . , y H×W ] ∈ R H×W , S 1 and S 2 . The mean vectors µ k are updated by using the exponential moving average [9]. Intra-cluster Pull Force: For transformation-invariant pixel-wise clustering, we define four loss terms. The first term, cluster loss, forces the embedding vectors to adhere to the associated mean vector (see Fig.  Inter-cluster Push Force: The second term, distance loss, pushes the distance between the mean vectors above a margin parameter m (see Fig.  where k A and k B indicate two different cluster indices.",2
1108,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing (vol2),,Cross-view Consistency:,"The segmentation maps from the different views, S 1 and S 2 , should overlap after re-transforming to align the coordinates. Such a re-transform is composed of inverse and forward geometric transformations: The inverse transformations of the photometric transformations are not considered. Using the re-transformed segmentation maps, we impose a third term, cross-view consistency loss, which forces the embedding vectors of one view to match the mean vector of the other (see Fig.  Reconstruction Loss: Without user editing, the decoder reconstructs the input images from quantized embedding maps h(E q ) = R ∈ R C×H×W . We thus employ reconstruction loss, which minimizes the mean squared error between the reconstructed and input images. Learning Objective: The weighted sum of the loss functions is set to be minimized:",2
1112,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_38.,2
1114,Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing (vol2),,Evaluation of the Synthesized Images:,"We measured the quality of image reconstruction using mean square error (MSE), structural similarity (SSIM), and peak signal-to-noise ratio (PSNR). The mean ± standard deviations of MSE,",2
1118,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation (vol2),,Local Region,"Contrast. Unlike global contrast, positive and negative pairs for local contrast are only generated from input image x q and its transform x p . We differentiate local regions and formulate the positive and negative pairs by using Felzenszwalb's algorithm. For an input image x, Felzenszwalb's algorithm provides K local regions R = {r 1 , r 2 , .., r K }, where r k is the k-th local region cluster for image x. We then perform elastic transform for both the query image x q and its local regions R q so that we have the augmented image x p = T e (x q ) and its local regions R p = {r 1 p , r 2 p , .., r Kp p }, where r k p = T e (r k q ). Note that K q = K p always holds since R p is a one-to-one mapping from R q . Following the widely used U-Net  , where f k,n q is the n-th vector sampled from feature map f q within the k-th local region r k q . Our sampling strategy is straightforward: we sample random points with replacement following a uniform distribution. We simply refer to this as ""random sampling"". Similarly, for feature map f p , its sample mean f k p can be provided following the same random sampling process. Each local region pair of f k q and f k p is considered a positive pair. For the negative pairs, we sample both f q and f p from the rest of the local regions {r 1 , r 2 , ..., r k-1 , r k+1 , ..., r K }. The local contrastive loss can be defined as follows: where τ l is the local temperature hyper-parameter. Compared to the global contrast branch, in local contrastive learning, we pre-train both E l and D l .",2
1128,An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment (vol2),,Problem definition.,"Assume a set of functional connectomes, G n ∈ R d×d , G 2 , . . . , G N are given, where N is the number of samples and d is the number of ROIs. Each connectome is represented by a weighted, undirected graph G = (V, E, W), where V = {v i } d i=1 is the set of nodes, E ⊆ V × V is the edge set, and W ∈ R |V|×|V| denotes the matrix of edge weights. The weight w ij of an edge e ij ∈ E represents the strength of the functional connection between nodes v i and v j , i.e., the Pearson correlation coefficient of the time series of the pair of the nodes. Each G n contains node attributes X n and edge attributes H n . We develop a model that predicts a gait impairment score, Y n and outputs an individual explanation mask M c ∈ R d×d per class c to assign ROIs to functional brain networks.",2
1134,An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment (vol2),,Software.,"All experiments were implemented in Python 3.10 and ran on Nvidia A100 GPU runtimes. We used PyTorch Geometric  Setup. We used the mean, connectivity profile, i.e., W ",2
1136,An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment (vol2),,Method,Pre Rec F1 AUC GCN*  The results (Table ,2
1139,An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment (vol2),,Acknowledgements,. This work was partially supported by ,2
1140,An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 68.,2
1152,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation (vol2),,Method Accuracy (%),"FedAvg  In federated learning, clients send the updated local models back to the server each round. In round t, α k is represented as α t k . The global model w t+1 g is aggregated by the server: then, the server assigns the global model w t g to all clients. Repeat and until T rounds or other limits.",2
1154,FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation (vol2),,MICCAI FeTS2021,"Training Data. The real-world dataset used in experiments is provided by the FeTS Challenge organizer, which is the training set of the whole dataset about brain tumor segmentation. In order to evaluate the performance of FedGrav, we partition the dataset composed of 341 data samples ",2
1177,Partial Vessels Annotation-Based Coronary Artery Segmentation with Self-training and Prototype Learning (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_28.,2
1184,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_57.,2
1194,SATTA: Semantic-Aware Test-Time Adaptation for Cross-Domain Medical Image Segmentation (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_14.,2
1201,A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos (vol2),,Method,"Type Backbone AP AP50 AP75 GFL  DFF  Evaluation Metrics. Three commonly-used metrics are employed for performance evaluation of breast lesion detection methods on the ultrasound videos, namely average precision (AP), AP 50 , and AP 75 . Implementation Details. We employ the ResNet-50 ",2
1215,Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models (vol2),,Acknowledgements,. This work was supported by the ,2
1216,Reveal to Revise: An Explainable AI Life Cycle for Iterative Bias Correction of Deep Models (vol2),,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 56. We evaluate the effectiveness of model corrections based on two metrics: the attributed fraction of relevance to artifacts and prediction performance on both the original and a poisoned test set (in terms of F1-score and accuracy). Whereas in the synthetic case, we simply insert the artifact into all samples to poison the test set, data-intrinsic artifacts are cropped from random artifactual samples using our artifact localization strategy. Note that artifacts might overlap clinically informative features in poisoned samples, limiting the comparability of poisoned and original test performance. As shown in Tab. 1 (ISIC 2019) and Appendix A.2 (Bone Age), we are generally able to improve model behavior with all methods. The only exception is the synthetic artifact for VGG-16, where only RRR mitigates the bias to a certain extent, indicating that the artifact signal is too strong for the model. Here, fine-tuning only the last layer is not sufficient to learn alternative prediction strategies. Interestingly, despite successfully decreasing the models' output sensitivity towards artifacts,",2
1220,A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis (vol2),,Model and Training.,"A CNN was used as a model for predicting whether datasets belonged to the D or ND class. The model consisted of 5 blocks each containing a convolutional layer with (3×3×3) kernel, batch normalization, sigmoid activation, and (2×2×2) max pooling. The convolutional filter sizes were 32, 64, 128, 256, and 512 for each respective block. The sixth block contained average pooling, dropout (rate=0.2), and a dense classification layer with softmax activation. Binary cross entropy loss, Adam optimizer (learning rate = 1e -4 ), and batch size 4 with early stopping based on validation loss (patience=30) were used for training. Each experiment simulated and used 500 datasets of voxel dimensions (173×211×155) with a 55%/15%/30% train/validation/test split, stratified by disease and bias labels. Evaluation. Model performance was evaluated using accuracy, sensitivity, and specificity computed for the aggregate test set, as well as separately for the bias (B) and no bias (NB) groups. Results are reported as the mean ± standard deviation of the models with 5 different weight initialization seeds on the same train/validation/test splits, following  Results and Discussion. The results of our evaluation are summarized in Fig. ",2
1222,A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 46.,2
1233,Physics-Based Decoding Improves Magnetic Resonance Fingerprinting (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 42.,2
1237,DeDA: Deep Directed Accumulator (vol2),,Network Layer for DA-TR:,"To gain more representation ability and capture long-range contextual information, DA-TR is applied to both intermediate feature maps and original images. As can be seen from Fig. ",2
1241,DeDA: Deep Directed Accumulator (vol2),,Subject-wise Results:,We also evaluated the performance at the subjectlevel. Pearson's correlation coefficient was used to measure the correlation model predicted count and human expert count. Mean Squared Error (MSE) was also used to measure the averaged accuracy for the model predicted count. Figure ,2
1242,DeDA: Deep Directed Accumulator (vol2),,Ablation Study:,"We conducted an ablation study to investigate the effects of each component accompanied with DA-TR. First, we examined the effects of applying the proposed DA-TR to the latent feature maps and raw images. Second, we examined the effects of using V u and V s , because rim+ lesions differ from rim-lesions in both gradient magnitudes and values at the edge of the lesion. We then investigated how multi-radius rim parameterization can affect the results, as the size of rim+ lesions vary greatly with a radius from 5 to 15 among different subjects. Results from models #1, #2 and #4 show that the rim parametrization DA-TR is useful for rim+ identification, and DA-TR used in the latent feature map space performs even better. Comparing model #3 and #4, one can see that accumulating both gradient magnitudes and feature values is beneficial. The consistent performance improvement from model #4 to #5 and from model #5 to #6 has demonstrated the effectiveness of applying multi-radius rim parameterization. More results on backbone networks can be found in the appendix.",2
1245,DeDA: Deep Directed Accumulator (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 72.,2
1266,One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation (vol2),,Global Model Training. KD allows to train a global model with multiple client models,"where λ denotes a noise level stored in memory. L KD (x; W c , W g ) denotes the Kullback-Leibler divergence between p(x; W c ) and p(x; W g ) where p(•) is an ensemble (averaging) prediction of given models with a temperature on softmax inputs ",2
1270,One-Shot Federated Learning on Medical Data Using Knowledge Distillation with Image Synthesis and Client Model Adaptation (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_49.,2
1278,A Video-Based End-to-end Pipeline for Non-nutritive Sucking Action Recognition and Segmentation in Young Infants (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_55.,2
1285,SFusion: Self-attention Based N-to-One Multimodal Fusion Block (vol2),,GFF.,"In the experiments on brain tumor segmentation, we compare SFusion with a gated feature fusion block (GFF)  Our implementations are on an NVIDIA RTX 3090(24G) with PyTorch 1.8.1.",2
1296,VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_11.,2
1298,Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization (vol2),,Contribution:,"In this study, we develop a novel end-to-end deep spatiotemporal residual attention neural network (ST-RAN) for scar detection using whole heart imaging in ischemic and non-ischemic heart diseases. The proposed model leverages spatial information to capture changes in contrast and temporal information to capture WMA to detect scars, in a large heterogeneous dataset. To achieve this, we propose a novel efficient Conv3Plus1D layer that deploys a factorized 4D (3D+time) receptive field, to simultaneously extract hierarchical spatial features and deep temporal features (comprehensive spatiotemporal features), distinguishing between patients with and without a scar. We introduce a multi-scale residual attention block that learns global and local motions to detect significant and subtle changes, the latter more present in patients with small scar sizes and nearly preserved wall motion. We validate our proposed model on a large cohort of patients with and without scars, showing the robustness of the model, outperforming state-of-the-art methods. Architecture overview. Our model takes an input a set of short-axis cine images of the whole heart, consisting of 20 phases, which are fed to a novel Conv3Plus1D layer, to extract spatial and temporal features. After batch normalization and nonlinear transformation, the feature maps are fed to a series of residual attention blocks (RAB) at different scales to extract global and local features, subtle to changes due to myocardial scar. After the RAB, a global average pooling followed by a fully connected are used to predict presence of a scar.",2
1305,Gadolinium-Free Cardiac MRI Myocardial Scar Detection by 4D Convolution Factorization (vol2),,Comparison with State-of-the-Art Methods.,"We then compare our model with state-of-the-art methods trained and tested on the same dataset for myocardial scar detection, including 3D (2D + time) spatiotemporal CNN (3D-STCNN) ",2
1316,ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 23.,2
1333,Multi-objective Point Cloud Autoencoders for Explainable Myocardial Infarction Prediction (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0 50.,2
1340,FairAdaBN: Mitigating Unfairness with Adaptive Batch Normalization and Its Application to Dermatological Disease Classification (vol2),,Results on ISIC 2019 Dataset.,Table  Hyper-parameter α. Our experiments show that α = 1.0 has the best fairness scores and FATE compared to α = 0.1 and α = 2.0. Therefore we select α = 1.0 as our final setting.,2
1357,Self-aware and Cross-Sample Prototypical Learning for Semi-supervised Medical Image Segmentation (vol2),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43895-0_18.,2
1361,CheXstray: A Real-Time Multi-Modal Monitoring Workflow for Medical Imaging AI (vol3),,Metric Measurement and Unification,"Our framework constructs detection windows using a sliding window approach, where the temporal parameters dictate the duration and step size of each window. Specifically, the duration defines the length of time that each window covers, while the step size determines the amount of time between the start of one window and the start of the next. We use ψi (ω t ) = mt i to denote individual metrics calculated at time t from a ω t , and m[a,b] i to represent the collection of individual metric values from time a to b. To mitigate sample size sensitivity, we employ a bootstrap method. This involves repeatedly calculating metrics on fixed-size samples drawn with replacement from the detection window. We then average these repeated measures to yield a final, more robust metric value. Formally: where θ K collects K samples from ω with replacement and ψ i is the metric function calculated on the sample. There remains three main challenges to metric unification: 1) fluctuation normalization, 2) scale standardization, and 3) metric weighting. Fluctuation normalization and scale standardization are necessary to ensure that the metrics are compatible and can be meaningfully compared and aggregated. Without these steps, comparing or combining metrics could lead to misleading results due to the variations in the scale and distribution of different metrics. We address the first two challenges by utilizing a standardization function, Γ , which normalizes each individual metric into a numerical space with consistent upper and lower bounds across all metrics. This function serves to align the metric values so that they fall within a standard range, thereby eliminating the influence of extreme values or discrepancies in the original scales of the metrics. In our experiments, we apply a simple normalization function using scale (η) and offset factors (ζ), specifically: Γ (m) = m-ζ η . Metric weighting is used to reflect the relative importance or reliability of each metric in the final unified metric. The weights are determined through a separate process which takes into account factors such as the sensitivity and specificity of each metric. We then calculate our unified multi-modal concordance metric, MMC , on a detection window ω by aggregating individual metric values across L metrics using predefined weights, α i , for each metric, as follows: where ψi (ω) represents the ith metric calculated on detection window ω, Γ i represents the standardization function, and α i represents the weight used for the ith metric value. Each metric value is derived by a function that measures a specific property or characteristic of the detection window. For instance, one metric could measure the average intensity of the window, while another could measure the variability of intensities. By calculating MMC on a time-indexed detection window set Ω [a,b] , we obtain a robust multi-modal concordance measure that can monitor drift over the given time period from a to b, denoted as MMC [a,b] . This unified metric is advantageous as it provides a single, comprehensive measurement that takes into account multiple aspects of the data, making it easier to track and understand changes over time.",3
1370,CorSegRec: A Topology-Preserving Scheme for Extracting Fully-Connected Coronary Arteries from CT Angiography (vol3),,1),We trained 3d_fullres version of nnU-Net ,3
1375,Joint Dense-Point Representation for Contour-Aware Graph Segmentation (vol3),,Contributions:,"We propose a novel joint architecture and contour loss to address this problem that leverages the benefits of both point and dense approaches. First, we combine image features from an encoder trained using a point-wise distance with image features from a decoder trained using a pixel-level objective. Our motivation is that contrasting training strategies enable diverse image features to be encoded which are highly detailed, discriminative and semantically rich when combined. Our joint learning strategy benefits from the segmentation accuracy of dense-based approaches, but without topological errors that regularly afflict models trained using a pixel-level loss. Second, we propose a novel hybrid contour distance (HCD) loss which biases the distance field towards pre-dictions that fall on the contour boundary using a sampled unsigned distance function which is fully differentiable and computationally efficient. To our knowledge this is the first time unsigned distance fields have been applied to graph segmentation tasks in this way. Our approach is able to generate highly plausible and accurate contour predictions with lower HD and higher DS/JC scores than a variety of dense and graph-based segmentation baselines.",3
1394,DiffMix: Diffusion Model-Based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_33.,3
1399,Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis (vol3),,Gene and Chromosome.,"A gene and a chromosome are modeled to represent fine-tuned parameters (P (i) f ) by jointly using P g and P * f . The gene indicates the internal division between P g and P * p . Figure  k ∈ [-1, 1] be a k th gene in the i th chromosome, and then the k th convolution weight (P where P g | k and P * p | k are the k th convolution parameter in P g and P * f , respectively. Here, is calculated by the internal division between P g and P * p . Crossover. Let crossover (P (i) , P (j) ) be a crossover function by jointly using two chromosomes, and then the k th genes of P (i) | k and P (j) | k are changed in the 50% probability when the constraint of Here, since the convolution parameters in a deep depth are rarely fine-tuned due to a gradient vanishing problem, l k exhibits a relatively larger value when k becomes larger. In addition, since we experimentally demonstrated that l k ≥ 0.15 provides a much longer time to fine-tune APD-Net, we constrained l k ≤ 0.15, and the fixed values of l k are randomly determined for each experiment. Mutation. mutation (P (i) ) represents a mutation function onto a chromosome, of which the k th gene is P (i) | k , such that it is defined as follows: ( where η is the randomly selected value in the constraint range for each individual gene. While training the DL model, we experimentally verified that the convolution weights are changed within the range of the maximum 0.2%. Therefore, here, μ is initially determined as 2e-3 (0.2%), but it depends on the variance of convolution weights in every epoch. Selection. As illustrated in Algorithm I, the newly generated chromosomes, which yield a large value of the fitness score, are contained in a new population. In APD-Net, the fitness score is evaluated by the fitness function that is jointly utilized in the architecture of APD-Net as illustrated in Fig.  where sim(x, y) is cosine similarity between x and y, and exp(x) is the exponential function. Note that APD-Net provides the fitness function related to its architecture, and the fitness function is more reliable than other fitness functions used in accuracy-based GA. Therefore, the APD-Net with our GA offers high accuracy in both the conventional diagnosis for overall patients and the personalized diagnosis for each patient at a specific client.",3
1405,Fine-Tuning Network in Federated Learning for Personalized Skin Diagnosis (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_37.,3
1410,Fourier Test-Time Adaptation with Multi-level Consistency for Robust Classification (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_22.,3
1412,Performance Metrics for Probabilistic Ordinal Classifiers (vol3),,Relation to Calibration:,A popular approach to assess the quality of probabilistic predictions is measuring calibration. A model is well calibrated if its probabilistic predictions are aligned with its accuracy on average. PSRs and calibration are intertwined concepts: PSRs can be decomposed into a calibration and a resolution component  The two most widely adopted PSRs are the Brier and the Logarithmic Score ,3
1431,UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_49.,3
1435,Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy (vol3),,U-Net. The U-Net,"( Factor μ controls the degree of deformation, with larger values allowing for more deformation. Note that the exclusive use of F id would result in the original image, i. e., x = F (x), assuming the deformation factor is being set to μ = 0. The resulting flow field F is Gaussian filtered (kernel size 9, σ = 2) to ensure smooth deformations in the final image. The corresponding parameters were selected manually in preliminary experiments. Auxiliary Classifier. To ensure the preservation of underlying abnormality patterns and image utility during deformation, PriCheXy-Net integrates an auxiliary classifier using CheXNet ",3
1436,Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy (vol3),,Patient Verification Network.,The incorporated patient verification model is represented by the SNN architecture presented by Packhäuser et al. ,3
1439,Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy (vol3),,Pre-training of the Flow Field,"Generator. The incorporated U-Net architecture was pre-trained on an autoencoder-like reconstruction task for 200 epochs using the mean squared error (MSE) loss, Adam  Training of PriCheXy-Net. After pre-training, PriCheXy-Net was trained in an end-to-end fashion for 250 epochs using the Adam optimizer ",3
1440,Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy (vol3),,Re-training and Evaluation of the Verification Model.,"To assess the anonymization capability of PriCheXy-Net and to determine if the anonymized images can reliably deceive the verification model, we re-trained the incorporated SNN for each model configuration by using deformed images only. We then simulated multiple linkage attacks by comparing deformed images with real ones. Training was conducted until early stopping (patience p = 5) using the BCE loss, the Adam optimizer  Evaluation of the Classification Model on Anonymized Data. To assess the extent to which underlying abnormalities, and thus data utility, were preserved during the anonymization process, each individually trained anonymization network was used to perturb the images of our test set. Then, the pre-trained auxiliary classifier was evaluated using the resulting images. We report the mean of the 14 class-wise AUC values. To quantify the uncertainty, the 95% confidence intervals (CIs) from 1,000 bootstrap runs were computed. Comparison with Other Obfuscation-Based Methods. To compare our proposed system with other obfuscation-based methods, we additionally analyzed the anonymization capability and utility preservation of Privacy-Net ",3
1442,Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy (vol3),,PriCheXy-Net.,The results of our proposed PriCheXy-Net (see Table ,3
1444,Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy (vol3),,Data Use,Declaration. This research study was conducted retrospectively using human subject data made available in open access by the National Institutes of Health (NIH) Clinical Center  Code Availability. The source code of this study has been made available at https://github.com/kaipackhaeuser/PriCheXy-Net.,3
1445,Deep Learning-Based Anonymization of Chest Radiographs: A Utility-Preserving Measure for Patient Privacy (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 26.,3
1447,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification (vol3),,Contributions:,"We propose a few-shot colorectal tissue image generation framework, named XM-GAN, which simultaneously focuses on generating highquality yet diverse images. Within our tissue image generation framework, we introduce a novel controllable fusion block (CFB) that enables a dense aggregation of local regions of the reference tissue images based on their congruence to those in the base tissue image. Our CFB employs a cross-attention based feature aggregation between the base (query) and reference (keys, values) tissue image features. Such a cross-attention mechanism enables the aggregation of reference features from a global receptive field, resulting in locally consistent features. Consequently, colorectal tissue images are generated with reduced artifacts. To further enhance the diversity and quality of the generated tissue images, we introduce a mapping network along with a controllable cross-modulated layer normalization (cLN) within our CFB. Our mapping network generates 'metaweights' that are a function of the global-level features of the reference tissue image and the control parameters. These meta-weights are then used to compute the modulation weights for feature re-weighting in our cLN. This enables the cross-attended tissue image features to be re-weighted and enriched in a controllable manner, based on the reference tissue image features and associated control parameters. Consequently, it results in improved diversity of the tissue images generated by our transformer-based framework (see Fig.  We validate our XM-GAN on the FS colorectral tissue image generation task by performing extensive qualitative, quantitative and subject specialist (pathologist) based evaluations. Our XM-GAN generates realistic and diverse colorectal tissue images (see Fig. ",3
1451,Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification (vol3),,Controllable Cross-modulated Layer Normalization (cLN):,"Our cLN learns sample-dependent modulation weights for normalizing features since it is desired to generate images that are similar to the few-shot samples. Such a dynamic modulation of features enables our framework to generate images of high-quality and diversity. To this end, we utilize the meta-weights w i for computing the modulation parameters λ and β in our layer normalization modules. With the cross-attended feature c i as input, our cLN modulates the input to produce an output feature o i ∈ R n×D , given by where μ and σ 2 are the estimated mean and variance of the input c i . Here, λ(w i ) is computed as the element-wise multiplication between meta-weights w i and sample-independent learnable weights λ ∈ R D , as λ w i . A similar computation is performed for β(w i ). Consequently, our proposed normalization mechanism achieves a controllable modulation of the input features based on the reference image inputs and enables enhanced diversity and quality in the generated images. The resulting features o i are then passed through a feed-forward network (FFN) followed by another cLN for preforming point-wise feature refinement, as shown in Fig.  Finally, the decoder F D generates the final image x.",3
1468,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 58.,3
1473,The Role of Subgroup Separability in Group-Fair Medical Image Classification (vol3),,Subgroup Separability in the Real World,"We begin by testing the premise of this article: subgroup separability varies across medical imaging settings. To measure subgroup separability, we train binary subgroup classifiers for each dataset-attribute combination. We use testset area under receiver operating characteristic curve (AUC) as a proxy for separability, reporting results over ten random seeds in Table ",3
1474,The Role of Subgroup Separability in Group-Fair Medical Image Classification (vol3),,Performance Degradation Under Label Bias,We now test our theoretical finding: models are affected by underdiagnosis differently depending on subgroup separability. We inject underdiagnosis bias into each training dataset by randomly mislabelling 25% of positive individuals in Group 1 (see Table ,3
1475,The Role of Subgroup Separability in Group-Fair Medical Image Classification (vol3),,Use of Sensitive Information in Biased Models,"Finally, we investigate how biased models use sensitive information. We apply the post hoc Supervised Prediction Layer Information Test (SPLIT) ",3
1477,The Role of Subgroup Separability in Group-Fair Medical Image Classification (vol3),,Sources of Bias Matter.,"In our experiments, we injected underdiagnosis bias into the training set and treated the uncorrupted test set as an unbiased ground truth. However, this is not an endorsement of the quality of the data. At least some of the datasets may already contain an unknown amount of underdiagnosis bias (among other sources of bias)  Reproducibility and Impact. This work tackles social and technical problems in machine learning for medical imaging and is of interest to researchers and practitioners seeking to develop and deploy medical AI. Given the sensitive nature of this topic, and its potential impact, we have made considerable efforts to ensure full reproducibility of our results. All datasets used in this study are publicly available, with access links in Table ",3
1478,The Role of Subgroup Separability in Group-Fair Medical Image Classification (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 18.,3
1481,Self-adaptive Adversarial Training for Robust Medical Segmentation (vol3),,Notations. Considering a segmentation task with input domain X,", where • is a norm constrain and C is the number of classes. We let D be a dataset containing N pairs of example and segmentation mask drawn i.i.d from the unknown distribution D. Denoted by f w : R n → R n , the segmentation results can be computed via a neural network parameterised over w = vec where d is the number of blocks.",3
1490,M3D-NCA: Robust 3D Segmentation with Built-In Quality Control (vol3),,Batch Duplication:,"Training NCA models is inherently more unstable than classical machine learning models like the UNet, due to two main factors. First, stochastic cell activation can result in significant jumps in the loss trajectory, especially in the beginning of the training. Second, patchification in M3D-NCA can cause serious fluctuations in the loss function, especially with three or more layers, thus it may never converge properly. The solution to this problem is to duplicate the batch input, meaning that the same input images are multiple times in each batch. While this limits the number of images per stack, it greatly improves convergence stability.",3
1491,M3D-NCA: Robust 3D Segmentation with Built-In Quality Control (vol3),,Pseudo Ensemble:,"The stochasticity of NCAs, caused by the random activation of cells gives them an inherent way of predicting multiple valid segmentation masks. We utilize this property by executing the trained model 10 times on the same data sample and then averaging over the outputs. We visualize the variance between several predictions in Fig. ",3
1498,M3D-NCA: Robust 3D Segmentation with Built-In Quality Control (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 17.,3
1503,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform (vol3),,Target SDT Generation.,"There is an inconsistency problem in object skeleton generation: part of the complete instance skeleton can be different from the skeleton of the instance part (Fig.  Instance Extraction from SDT. In the SDT energy map, all boundary pixels share the same energy value and can be processed into segments by direct thresholding and connected component labeling, similar to DWT ",3
1505,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform (vol3),,Methods in Comparison.,"We compare SDT with previous state-of-the-art segmentation methods, including DCAN ",3
1506,Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform (vol3),,Results.,Our SDT framework achieves state-of-the-art performance on 5 out of 6 evaluation metrics on the gland segmentation dataset (Table ,3
1510,Enhance Early Diagnosis Accuracy of Alzheimer’s Disease by Elucidating Interactions Between Amyloid Cascade and Tau Propagation (vol3),,Suppose we have a brain network,", and the output is the clinical outcome η T . From the perspective of brain dynamics, we introduce an evolution state v i (t) for each brain region, which can be regarded as the intrinsic interaction trajectory of the features of each brain node. Herein, we investigate two prominent features on the basis of the brain region, namely tau-x i (t) and Aβ-u i (t), we then explore the interaction between tau propagation and amyloid cascade, which is believed to play a crucial role in the evolution dynamics i=1 of AD progression. In particular, we investigate how Aβ influences the spreading of tau in AD progression. Our study aims to shed light on the complex mechanisms underlying the progression of AD, a critical area of research in the field of neuroscience.",3
1528,Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data (vol3),,Preliminaries.,"A structural brain network is an attributed and weighted graph G = (A, H) with N nodes, where H ∈ R N ×d is the node feature matrix, and A ∈ R N ×N is the adjacency matrix where a i,j ∈ R represents the edge weight between node i and node j. Meanwhile, we utilize X B ∈ R N ×T to represent the BOLD signal matrix derived from functional MRI data of each subject, where each brain ROI has a time series BOLD signal with T points. Reconstruction. For the reconstruction task, we deploy an encoder-decoder architecture and utilize the L 1 loss function. Particularly, we use a multi-layer feed-forward neural network as the encoder and decoder. Our method differs from previous studies ",3
1529,Bidirectional Mapping with Contrastive Learning on Multimodal Neuroimaging Data (vol3),,ROI-Level's Contrastive Representation Learning.,"With latent representation Z B ∈ R N ×dB generated from BOLD signal and Z S ∈ R N ×dS from structural networks, we then conduct ROI-level's contrastive learning to associate the static structural and dynamic functional patterns of multimodal brain measurements. The contrastive learning loss aims to minimize the distinctions between latent representations from two modalities. To this end, we first utilize linear layers to project Z B and Z S to the common space, where we obtain to denote representations from the same ROI, where z B i and z S i are elements of Z B and Z S , respectively. For the same brain ROI, the static structural representation and the dynamic functional counterpart are expected to share a maximum similarity. Conversely, for the pairs that do not match, represented as (z B i , z S j ) i =j , these are drawn from different ROIs and should share a minimum similarity. To formally build up the ROI-level's contrastive loss, it is intuitive to construct positive samples and negative ones based on the match of ROIs. Specifically, we construct (z B i , z S i ) i=1•••N as positive sample pair, and (z B i , z S j ) i =j as negative sample pair. And our contrastive loss can be formulated as follow: where Similarity(•) is substantiated as cosine similarity. Loss Functions. The loss functions within our proposed framework are summarized here. Besides the reconstruction loss (L rec ) and the ROI-level's contrastive loss (L contrast ), we utilize cross-entropy loss (L supervised = L cross-entropy ) for classification tasks, and L 1 loss (L supervised = L mean-absolute-error ) for regression tasks, respectively. In summary, the loss function can be described as: where η 1 , η 2 and η 3 are loss weights. ",3
1540,Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis (vol3),,Stage 1: Identifying Poorly Calibrated Samples (Clustering).,"In this stage, we first train a model f id via ERM  where pi is the confidence score of the predicted class.  where Intuitively, the focal loss penalizes confident predictions with an exponential term (1 -f pred (y i |x i )) γ , thereby reducing the chances of poor calibration  Additionally, due to clustering based on gap(x i ), poorly calibrated samples will end up in the same cluster. The number of samples in this cluster will be small compared to other clusters for any model with good overall performance. As such, doing focal loss separately on each cluster instead of on all samples will implicitly increase the weight of poorly calibrated samples and help reduce bias.",3
1543,Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis (vol3),,Task 2: Future new multiple sclerosis (MS) lesional activity prediction (binary classification).,"We leverage a large multi-centre, multi-scanner proprietary dataset comprised of MRI scans from 602 RRMS (Relapsing-Remitting MS) patients during clinical trials for new treatments ",3
1544,Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis (vol3),,Implementation Details:,We adopt 2D/3D ResNet-18 ,3
1545,Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis (vol3),,Comparisons and Evaluations:,"Macro-F1 is used to measure the performance for Task 1 (7 class), and F1-score is used for Task 2 (binary). Q-ECE ",3
1547,Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis (vol3),,Ablation Experiments:,Further experiments are performed to analyze the different components of our method. The following variant methods are considered: (1) Focal: Removing stage 1 and using regular focal loss for the entire training set; (2) Cluster-ERM: Group-wise focal loss in stage 2 is replaced by standard cross entropy; (3) Cluster-GroupDRO: Group-wise focal loss in stage 2 is replaced by GroupDRO  Calibration Curves: Figure ,3
1549,Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 19.,3
1565,MultiTalent: A Multi-dataset Approach to Medical Image Segmentation (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_62.,3
1569,Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks (vol3),,The optimization becomes θ = argmin,"In order to achieve equivariance with respect to T ψ , the loss is computed after applying the inverse transformation to the logits. Consistency regularization (CR) additionally constrains the logits predicted for similar images to be similar. This is achieved by minimizing a consistency loss L c between logits predicted for two transformed versions of the same image: θ The exact strategy for choosing arguments of L c can vary: as above, we use predictions of the same network θ for different transformations (φ, ψ) and (φ , ψ ) ",3
1571,Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks (vol3),,Special Case of Binary Segmentation:,"To illustrate the pixel-wise regularization effect more clearly, let us consider binary segmentation. Here, we can fix z 1 = 0 and let z 2 = z, as only logit differences matter in the softmax function. Further, let us consider only one pixel, drop the pixel index and assume that its ground truth label is c = 2. Thus, y 1 = 0 and y 2 = 1. With these simplifications, L s =log(σ(z)) and L c = (zz ) 2 . Figure ",3
1573,Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks (vol3),,Datasets:,"We investigate the effect of CR on two public datasets. The NCI  Data Splits: From the N subjects in each dataset, we select N ts test, N vl validation and N tr training subjects. {N ts , N vl } are set to {30, 4} for NCI and {50, 5} for ACDC. We have 3 settings for N tr : small, medium and large, with N tr as 6, 12 and 36 for NCI, and 5, 10 and 95 for ACDC, in the three settings, respectively. All experiments are run thrice, with test subjects fixed across runs, and training and validation subjects randomly sampled from remaining subjects. In each dataset, subjects in all subsets are evenly distributed over different scanners.",3
1574,Boundary-Weighted Logit Consistency Improves Calibration of Segmentation Networks (vol3),,Pre-processing:,We correct bias fields using the N4  Training Details: We use a 2D U-net  Evaluation Criteria: We evaluate segmentation accuracy using Dice similarity coefficient and calibration using Expected Calibration Error (ECE) ,3
1587,Co-assistant Networks for Label Correction (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_16.,3
1594,Label-Preserving Data Augmentation in Latent Space for Diabetic Retinopathy Recognition (vol3),,Experimental Settings.,"All experiments are conducted on a single NVIDIA RTX A5000 GPU with 24GB memory using PyTorch implementation. For Style-GAN3, we initialize the learning rate with 0.0025 for G(•)and 0.002 for D(•). For optimization, we use the Adam optimizer with β 1 = 0.0, β 2 = 0.99. We empirically set λ lpips = 0.6, λ gan = 0.4. For evaluation of classification and detection tasks, we both initialize the learning rate with 0.001 for the training of VGG16. ",3
1601,DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis (vol3),,Expert Sparse Fusion.,"The final output is a weighted sum of each expert's knowledge using the sparse weight W = [W 1 , W 2 , ..., W M ] generated by DRM. Given an input feature X, the output X of an expert bank can be obtained as: where E m (•) represents an operator of E m AT T (•) or E m F F N (•). ",3
1608,DRMC: A Generalist Model with Dynamic Routing for Multi-center PET Image Synthesis (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 4.,3
1614,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement (vol3),,Algorithm 1. Pre-trained DDPM for plug-and-play medical image enhancement,"Require: Pre-trained DDPM θ , low-quality image y, degradation operator H 1: Initialize xT ∼ N (0, I). 2: for t = T to 1 do 3: ",3
1618,Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_1.,3
1629,Robust T-Loss for Medical Image Segmentation (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 68.,3
1639,Multi-Head Multi-Loss Model Calibration (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_11.,3
1644,Scale Federated Learning for Label Set Mismatch in Medical Image Classification (vol3),,Uncertain Data Enhancing (UDE).,"The pseudo label filtering mechanism makes it difficult to acquire pseudo-labels for uncertain data, which results in their inability to contribute to the training process. To overcome this limitation, we propose to MixUp  , where x l ∈ D l k and x h ∈ D h k , and y l and y h are their corresponding labels or pseudo labels, respectively. We generate pseudo labels for uncertain data (x h , y h ) with a relatively smaller confidence threshold. The UDE loss function L UDE is cross-entropy loss.",3
1645,Scale Federated Learning for Label Set Mismatch in Medical Image Classification (vol3),,Overall Loss Function. The complete loss function is defined as:,where λ is a hyperparameter to balance different objectives.,3
1650,Scale Federated Learning for Label Set Mismatch in Medical Image Classification (vol3),,AUC Accuracy F1 Precision Recall,FedAvg with 100% labeled data 0.977 0.889 0.809 0.743 0.800 FedAvg ,3
1665,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images (vol3),,Related Work.,Previous work comparing guidance signals has mostly been limited to small ablation studies. Sofiiuk et al. ,3
1673,Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 61.,3
1675,Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection (vol3),,New Attention Models:,"The redesign of the self-attention mechanism within pure Transformer models is another method aiming to augment feature repre-sentation to enhance the local feature representation ultimately. In this direction, Swin-Unet ",3
1676,Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection (vol3),,Drawbacks of Transformers:,"Recent research has revealed that traditional self-attention mechanisms, while effective in addressing local feature discrepancies, have a tendency to overlook important high-frequency information such as texture and edge details  Our Contributions: ➊ We propose Laplacian-Former, a novel approach that includes new efficient attention (EF-ATT) consisting of two sub-attention mechanisms: efficient attention and frequency attention. The efficient attention mechanism reduces the complexity of self-attention to linear while producing the same output. The frequency attention mechanism is modeled using a Laplacian pyramid to emphasize each frequency information's contribution selectively. Then, a parametric frequency attention fusion strategy to balance the importance of shape and texture features by recalibrating the frequency features. These two attention mechanisms work in parallel. ➋ We also introduce a novel efficient enhancement multi-scale bridge that effectively transfers spatial information from the encoder to the decoder while preserving the fundamental features. ➌ Our method not only alleviates the problem of the traditional self-attention mechanism mentioned above, but also it surpasses all its counterparts in terms of different evaluation metrics for the tasks of medical image segmentation.",3
1682,Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection (vol3),,Datasets:,"We tested our model using the Synapse dataset  Skin Lesion Segmentation: Table  Our method achieves superior performance by utilizing the frequency attention in a pyramid scale to model local textures. Specifically, our frequency attention emphasizes the fine details and texture characteristics that are indicative of skin lesion structures and amplifies regions with significant intensity variations, thus accentuating the texture patterns present in the image and resulting in better performance. In addition, we provided the spectral response of LaplacianFormer vs. Standard Transformer in identical layers in Table ",3
1688,Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging (vol3),,Pipeline and Results,". We follow Sects. 2 and 3 using T1w, FA and MD volumes as features (M = 3) and a FPR α = 0.02. The pipeline is repeated 10 times for cross-validation. Each fold is composed of 64 randomly selected HC images for training (about 70M voxels), the remaining 44 HC and all the PD samples for testing. For the reference model, we test Gaussian and MST mixtures, with respectively K H = 14 and K H = 8, estimated with the slope heuristic. Abnormal voxels are then detected for all test subjects, on the basis of their proximity to the learned reference model, as detailed in Sect. 2. The PPMI does not provide ground truth information at the voxel level. This is a recurring issue in UAD, which limits validations to mainly qualitative ones. For a more quantitative evaluation, we propose to resort to an auxiliary task whose success is likely to be correlated with a good anomaly detection. We consider the classification of test subjects into healthy and Parkinsonian subjects based on their global (over all brain) percentages of abnormal voxels. We exploit the availability of HY values to divide the patients into two HY = 1 and HY = 2 groups, representing the two early stages of the disease's progression. Classification results yield a median g-mean, for stage 1 vs stage 2, respectively of 0.59 vs 0.63 for the Gaussian mixtures model and 0.63 vs 0.65 for the MST mixture. The ability of both mixtures to better differentiate stage 2 than stage 1 patients from HC is consistent with the progression of the disease. Note that the structural differences between these two PD stages remain subtle and difficult to detect, demonstrating the efficiency of the models. The MST mixture model appears better in identifying stage 2 PD patients based on their abnormal voxels. To gain further insights, we report, in Fig.  Regarding efficiency, energy consumption in kilojoules (kJ) is measured using the PowerAPI library ",3
1690,Towards Frugal Unsupervised Detection of Subtle Abnormalities in Medical Imaging (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 40.,3
1695,Understanding Silent Failures in Medical Image Classification (vol3),,None of the Evaluated Methods from the Literature Beats the Maximum Softmax Response Baseline Across a Realistic Range of Failure,"Sources. This result is generally consistent with previous findings in Bernhard et al.  On the Chest X-ray dataset, MCD worsens the performance for darkening corruptions across all CSFs and intensity levels, whereas the opposite is observed for brightening corruptions. Further, on the lung nodule CT dataset, DG-MCD-RES performs best on bright/dark corruptions and the spiculation manifestation shift, but worst on noise corruption and falls behind on the texture manifestation shift. These observations indicate trade-offs, where, within one distribution shift, reliability against one domain might induce susceptibility to other domains. Current Systems are Not Generally Reliable Enough for Clinical Application. Although CSFs can mitigate the rate of silent failures (see Appendix Fig. ",3
1698,Understanding Silent Failures in Medical Image Classification (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 39.,3
1708,Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_30.,3
1715,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation (vol3),,LIDC [1]:,"The LIDC dataset is a publicly available lung CT image database containing 1018 scans, developed by the Lung Image Database Consortium (LIDC). All pulmonary nodules and masses in the dataset have been annotated by multiple raters. To generate the ground truth for each nodule and mass, we combined the segmentation annotations from different raters. Overall, we selected a total of 1625 nodules and masses that were annotated by more than three raters from the LIDC dataset for the experiment.",3
1716,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation (vol3),,LNDb [16]:,"The LNDb dataset published in 2019, comprises 294 CT scans collected between 2016 and 2018. Each CT scan in the dataset has been segmented by at least one radiologist. The nodules included in this dataset are larger than 3 mm. The mean scale of the lesion in LNDb dataset is the shortest among the three datasets. We adopt 1968 nodules and masses from the LNDb dataset.",3
1717,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation (vol3),,In-House Data (ours):,"The in-house data (ours) contains 4055 CT scans and 6864 nodules and masses. Every CT scans are annotated with voxel-level nodule masks by radiologists. We exclude nodules and masses with diameters larger than 64 mm or smaller than 2 mm, as the diameter of the largest mass in the public dataset is no more than 64 mm.",3
1718,Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation (vol3),,Evaluation Metrics:,"The performance of the nodule segmentation is evaluated by three metrics: volume-based Dice Similarity Coefficient (DSC), surface-based Normalized Surface Dice (NSD) ",3
1727,Chest X-ray Image Classification: A Causal Perspective (vol3),,Disentanglement.,"As shown in Fig.  where h ∈ R v×k , Φ(•) represents classifier, and z denotes logits. The causal part aims to estimate the really useful feature, so we apply the supervised classification loss in a cross-entropy format: where d is a sample and D is the training data, y is the corresponding label. The confounding part is undesirable for classification, so we follow the work in CAL  where KL is the KL-Divergence, and y unif orm denotes a predefined uniform distribution. Causal Intervention. The idea of the backdoor adjustment formula in Eq. (  where ẑc is the prediction from a classifier on the ""intervened graph"", ĥc is the stratification feature via Eq. (3), D is the estimated stratification set contains trivial features. The training objective of our framework can be defined as: where α 1 and α 2 are hyper-parameters, which decide how powerful disentanglement and backdoor adjustment are. It pushes the prediction stable because of the shared image features according to our detailed results in the next section.",3
1763,SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation (vol3),,Cross Attention(f,"Multi-view Consistency Loss. We assume consistent segmentation results should be achieved across different views of the same volume. To quantify the consistency of the multi-view results, we introduce a consistency loss L mc , calculated using KL divergence in the fine-tuning stage, as in previous work on mutual learning  We evaluate the effectiveness of different mutual loss functions in an ablation study (see supplementary). The KL divergence calculation is shown in Eq. 7: where V i (x m ) and V j (x m ) denote the different view prediction of m-th voxel. N represents the number of voxels of case x. V i (x) and V j (x) denote different view prediction of case x. We measure segmentation performance using L DiceCE , which combines Dice Loss and Cross Entropy Loss according to  where p m and y i respectively represent the predicted and ground truth labels for the m-th voxel, while N is the total number of voxels. We used L fin during the fine-tuning stage, as specified in Eq. 9, and added weight coefficients β DiceCE and β mc for different loss functions, both set to a default value of 1. ",3
1769,SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_47.,3
1777,Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis (vol3),,ISIC 2019 Dataset.,In Table ,3
1781,Toward Fairness Through Fair Multi-Exit Framework for Dermatological Disease Diagnosis (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 10.,3
1790,SMRD: SURE-Based Robust MRI Reconstruction with Diffusion Models (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_20.,3
1798,Automatic Segmentation of Internal Tooth Structure from CBCT Images Using Hierarchical Deep Learning (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_67.,3
1802,Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts (vol3),,Modulization.,"We first use multiple small MLPs with the same size to process different block features and then up-sample the features to the size of the input scans, i.e., H × W × d. With N as the total number of layers (experts) in the decoder, we treat these upsampled features [F 1 , F 2 , ..., F N ] as expert features. We then train a gating network G to re-weight the features from activated experts with the trainable weight matrices [W 1 , W 2 , ..., W N ], where W ∈ R H×W ×d . Specifically, the gating network or router G outputs these weight matrices satisfying i W i = 1 H×W ×d using a structure depicted as follows: The gating network first concatenates all the expert features along channels and uses several convolutional layers to get where C is the channel dimension. A softmax layer is applied over the last dimension (i.e., N -expert) to output the final weight maps. After that, we feed the resultant output x out to another MLP to fuse multi-block expert features. Finally, the resultant output x out (i.e. the coarse feature) is given as follows: where • denotes the pixel-wise multiplication, and Stochastic Routing. The prior MoE-based model  That is, a model needs to access all its parameters to process all inputs. One drawback of such design often comes at the prohibitive training cost. Moreover, the large model size suffers from the representation collapse issue ",3
1808,Implicit Anatomical Rendering for Medical Image Segmentation with Stochastic Experts (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 54.,3
1831,ACC-UNet: A Completely Convolutional UNet Model for the 2020s (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_66.,3
1833,How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging? (vol3),,Contributions: (1),"We demonstrate the ineffectiveness of common calibration metrics on medical image datasets with limited number of samples. (2) We propose a novel and robust metric to estimate calibration accurately on small datasets. The metric regularizes the probability of predicting a particular confidence value by estimating a parametric density model for each sample. The calibration estimates using the regularized probability estimates have significantly lower bias, and variance. (3) Finally, we also propose a train-time auxiliary loss for calibrating models trained on small datasets. We validate the proposed loss on several public medical datasets and achieve SOTA calibration results.",3
1836,How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging? (vol3),,Static Calibration Error (SCE):,"extends ECE to a multi-class setting as follows: Here B i,k denotes the number of samples of class k in the i th bin.",3
1838,How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging? (vol3),,RECE-G:,"Here, we assume a Gaussian distribution of fixed variance (σ) as the latent distribution for each confidence sample. We estimate the mean of the latent distribution as the observed sample itself. Formally: and To prevent notation clutter, we use denotes the probability of the interval [a, b] for a Gaussian distribution with mean μ, and variance σ. In the above expression, the range [ i-1 M , i M ] corresponds to the range of confidence values corresponding to i th bin. We also normalize the weight values over the set of bins. The value of standard deviation σ is taken as a fixed hyper-parameter. Note that the expression is equivalent to sampling infinitely many confidence values from the distribution N (•; c j , σ) for each sample j, and then computing the ECE value from thus computed large sampled dataset.",3
1839,How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging? (vol3),,RECE-M:,"Note that RECE-G assumes fixed uncertainty in confidence prediction for all the samples as indicated by the choice of single σ for all the samples. To incorporate sample specific confidence uncertainty, we propose RECE-M in which we generate multiple confidence observations for a sample using test time augmentation. In our implementation, we generate 10 observations using random horizontal flip and rotation. We use the 10 observed values to estimate a Gaussian Mixture Model (denoted as G) with 3 components. We use θ j to denote the estimated parameters of mixture model for sample j. Note that, unlike RECE-G, computation of this metric requires additional inference passes through the model. Hence, the computation is more costly, but may lead to more reliable calibration estimates. Formally, RECE-M is computed as: and",3
1842,How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging? (vol3),,Evaluation of Calibration Methods:,We compare our RCR loss with other SOTA calibration techniques in Table ,3
1844,How Reliable are the Metrics Used for Assessing Reliability in Medical Imaging? (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 15.,3
1850,Uncovering Structural-Functional Coupling Alterations for Neurodegenerative Diseases (vol3),,SC-FC Coupling Network for Disease Diagnosis.,"To leverage the rich system-level heuristics from Kuramoto model, it is straightforward to integrate a classification branch on top of Φ which is trained to minimize the cross-entropy loss in classifying healthy and disease subjects. Thus, the tailored deep Kuramoto model for early diagnosis is called SC-FC-Net. ",3
1860,Data AUDIT: Identifying Attribute Utility-and Detectability-Induced Bias in Task Models (vol3),,Datasets:,"We use publicly available skin lesion data from the HAM10000  Training Protocol: For attribute prediction networks used by our detectability procedure, we finetune ResNet18 ",3
1892,A Model-Agnostic Framework for Universal Anomaly Detection of Multi-organ and Multi-modal Images (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_23.,3
1898,Deep Mutual Distillation for Semi-supervised Medical Image Segmentation (vol3),,Evaluation Metric:,"The performance of our method is quantitatively evaluated in terms of Dice, Jaccard, the average surface distance (ASD), and the 95% Hausdorff Distance (95HD) as previous methods  Implementation Details: We implement DMD using PyTorch ",3
1902,Deep Mutual Distillation for Semi-supervised Medical Image Segmentation (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 52.,3
1910,Trust Your Neighbours: Penalty-Based Constraints for Model Calibration (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 55.,3
1914,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation (vol3),,H,", are then combined to these heatmaps to regress the final position μ k and the corresponding variance (σ k x , σ k y ) of each landmark point through the following two equations: where •, • F is the Frobenius inner product, corresponds to the Hadamard product, and (σ ky ) 2 is computed similarly. Thus, for each image x i , the neural network f θ (x i ) predicts a tuple (μ i , σ i ) with μ i ∈ R 2K and σ i ∈ R 2K through the generation of K heatmaps. The network is finally trained using the following univariate aleatoric loss adapted from  where y k i is the k th reference landmark point of image x i . Bivariate Model -One of the limitations of the univariate model is that it assumes no x-y covariance on the regressed uncertainty. This does not hold true in many cases, because the uncertainty can be oblique and thus involve a nonzero x-y covariance. To address this, one can model the uncertainty of each point with a 2 × 2 covariance matrix, Σ, where the variances are expressed with Eq. 2 and the covariance is computed as follows: The network f θ (x i ) thus predicts a tuple (μ i , Σ i ) for each image x i , with μ i ∈ R K×2 and Σ i ∈ R K×2×2 . We propose to train f θ using a new loss function L N2 : Asymmetric Model -One limitation of the bivariate method is that it models a symmetric uncertainty, an assumption that may not hold in some cases as illustrated on the right side of Fig.  where φ n is a multivariate normal, Φ 1 is the cumulative distribution function of a unit normal, Σ = ω Σω and α ∈ R n is the skewness parameter. Note that this is a direct extension of the multivariate normal as the skew-normal distribution is equal to the normal distribution when α = 0. The corresponding network predicts a tuple (μ, Σ, α) with μ ∈ R K×2 , Σ ∈ R K×2×2 and α ∈ R K×2 . The skewness output α is predicted using a sub-network whose input is the latent space of the main network (refer to the supplementary material for an illustration). This model is trained using a new loss function derived from the maximum likelihood estimate of the skew-normal distribution:",3
1919,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation (vol3),,Maximum Calibration Error (MCE),. This common uncertainty metric represents the probability if a classifier (here a segmentation method) of being correct by computing the worst case difference between its predicted confidence and its actual accuracy  Uncertainty Error Mutual-Information. As proposed in ,3
1922,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_21.,3
1930,DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 56.,3
1939,GRACE: A Generalized and Personalized Federated Learning Method for Medical Imaging (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_2. The results are summarized in Table ,3
1944,Deployment of Image Analysis Algorithms Under Prevalence Shifts (vol3),,top).,"Importantly, decision rules optimized on a development dataset do not generalize to unseen data under prevalence shifts (Fig. ",3
1945,Deployment of Image Analysis Algorithms Under Prevalence Shifts (vol3),,Effects of Prevalence Shifts on the Generalizability of Validation,Results. As shown in Fig. ,3
1947,Deployment of Image Analysis Algorithms Under Prevalence Shifts (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_38.,3
1955,Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis (vol3),,Contributions.,"In this work, we introduce regular continuous group convolutions equivariant to SE(3), the group of roto-translations. Motivated by the work on separable group convolutions  1. We introduce separable regular SE(3) equivariant group convolutions that generalize to the continuous setting using RBF interpolation and randomly sampling equidistant SO(3) grids. 2. We show the advantages of our approach on volumetric medical image classification tasks over regular CNNs and discrete subgroup equivariant G-CNNs, achieving up to a 16.5% gain in accuracy over regular CNNs. 3. Our approach generalizes to SE(n) and requires no additional hyperparameters beyond setting the kernel and sample resolutions. 4. We publish our SE(3) equivariant group convolutions and codebase for designing custom regular group convolutions as a Python package. Paper Outline. The remainder of this paper is structured as follows. Section 2 provides an overview of current research in group convolutions. Section 3 introduces the group convolution theory and presents our approach to SE(3) equivariant group convolutions. Section 4 presents our experiments and an evaluation of our results. We give our concluding remarks in Sect. 5.",3
1960,Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis (vol3),,Continuous SO(n) Kernel with Radial Basis Function Interpolation.,"The continuous SO(n) kernel is parameterized via a similarly discrete SO(n) uniform grid. Each grid element R i has corresponding learnable parameters k i . We use radial basis function (RBF) interpolation to evaluate sampled grid elements. Given a grid of resolution N , the continuous kernel k SO(n) is evaluated for any R as: Here, a d,ψ (R, R i ) represents the RBF interpolation coefficient of R corresponding to R i obtained using Gaussian RBF ψ and Riemannian distance d. The uniformity constraint on the grid allows us to scale ψ to the grid resolution dynamically. This ensures that the kernel is smooth and makes our approach hyperparameter-free.",3
1972,TauFlowNet: Uncovering Propagation Mechanism of Tau Aggregates by Neural Transport Equation (vol3),,TauFlowNet: A GAN Network Architecture of TV-Based Transport,"Equation. Here, we present an explainable deep model to uncover the spreading flow of tau aggregates f from the longitudinal tau-PET scans. Our deep model is trained to learn the system dynamics (in Eq. 4), which can predict future tau accumulations. The overall network architecture of TauFlowNet is shown in Fig. ",3
1987,Federated Condition Generalization on Low-dose CT Reconstruction via Cross-domain Learning (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_5.,3
1992,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT (vol3),,Continuous Spatial Point Sampling Based on the Vessel Attention,"Map. In order to obtain the topological relationship between Couinaud segments, a direct strategy is to sample the coordinate point data with 3D spatial information from liver CT and perform point-wise classification. Hence, we first convert the image coordinate points I = i 1 , i 2 , ..., i t , i t ∈ R 3 in liver CT into the world coordinate points P = p 1 , p 2 , ..., p t , p t ∈ R 3 : where Spacing represents the voxel spacing in the CT images, Direction represents the direction of the scan, and Origin represents the world coordinates of the image origin. Based on equation(1), we obtain the world coordinate p t = (x t , y t , z t ) corresponding to each point i t in the liver space. However, directly feeding the transformed point data as input into the point-based branch undoubtedly ignores the vessel structure, which is crucial for Couinaud segmentation.  where R denotes the rounding integer function. Based on this, we achieve arbitrary resolution sampling in the continuous space covered by the M . ",3
1993,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT (vol3),,Re,"where r denotes the voxel resolution, I [•] is the binary indicator of whether the coordinate pt belongs to the voxel grid (u, v, w), f t,c denotes the cth channel feature corresponding to pt , and N u,v,w is the number of points that fall in that voxel grid. Note that the re-voxelization in the model is used three times (as shown in Fig.  Multi-scale Point-Voxel Fusion Network. Intuitively, due to the image intensity between different Couinaud segments being similar, the voxel-based CNN model is difficult to achieve good segmentation performance. We propose a multi-scale point-voxel fusion network for accurate Couinaud segmentation, take advantage of the topological relationship of coordinate points in 3D space, and leverage the semantic information of voxel grids. As shown in Fig.  where the superscript 1 of (p t , f 1 t ) indicates that the fused point data and corresponding features f 1 t are obtained after the first round of point-voxel operation. Then, the point data (p t , f 1 t ) is voxelized again and extracted point features and voxel features through two branches. Note that the resolution of the voxel grid in this round is reduced to half of the previous round. After three rounds of pointvoxel operations, we concatenate the original point feature f t and the features f 1 t , f 2 t , f 3 t with multiple scales, then send them into a point-wise decoder D, parameterized by a fully connected network, to predict the corresponding Couinaud segment category: where {0, 1, ..., 7} denotes the Couinaud segmentation category predicted by our model for the point p t . We employ the BCE loss and the Dice loss to supervise the learning process. More method details are shown in the supplementary materials.",3
1999,Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 45.,3
2006,HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images (vol3),,Acknowledgments,. This work was supported in part by ,3
2011,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels (vol3),,2-Δ IoU,". There exist several alternatives to define Δ IoU , but not all of them are feasible, e.g., SJL. Generally, it is easy to verify the following proposition: Proposition 1. Δ Dice satisfies reflexivity and positivity iff Δ IoU does. Among the definitions of Δ IoU , Wang and Blaschko  Δ Dice that is defined over integers does not satisfy the triangle inequality  ( Functions that satisfy the relaxed triangle inequality for some fixed scalar ρ and conditions (i)-(iii) of a metric are called semimetrics. Δ Dice is a semimetric on {0, 1} p  Theorem 1. Δ DML,1 and Δ DML,2 are semimetrics on [0, 1] p . The proof can be found in Appendix A. Moreover, DMLs have properties that are similar to JMLs and they are presented as follows: The proofs are similar to those given in ",3
2021,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_46.,3
2030,Fully Bayesian VIB-DeepSSM (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1_34.,3
2032,Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation (vol3),,Contribution.,"In this work, we propose a novel method based on entropy maximization to enhance the quality of pixel-level segmentation posteriors. Our hypothesis is that penalizing low entropy on the probability estimates for erroneous pixel predictions during training should help to avoid overconfident estimates in situations of high uncertainty. The underlying idea is that, if a pixel is difficult to classify, it is better assigning uniformly distributed (i.e. high entropy) probabilities to all classes, rather than being overconfident on the wrong class. To this end, we design two simple regularization terms which push the estimated posteriors for misclassified pixels towards a uniform distribution by penalizing low entropy predictions. We benchmark the proposed method in two challenging medical image segmentation tasks. Last, we further show that assessing segmentation models only from a discriminative perspective does not provide a complete overview of the model performance, and argue that including calibration metrics should be preferred. This will allow to not only evaluate the segmentation power of a given model, but also its reliability, of pivotal importance in healthcare.",3
2033,Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation (vol3),,Related Work.,"Obtaining well-calibrated probability estimates of supervised machine learning approaches has attracted the attention of the research community even before the deep learning era, including approaches like histogram  Another alternative is to address the calibration problem during training, for example by clamping over-confident predictions. In  An in-depth analysis of the calibration quality obtained by training segmentation networks with the two most commonly used loss functions, Dice coefficient and cross entropy, was conducted in ",3
2035,Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation (vol3),,Proxy for Entropy Maximization:,"In addition to explicitly maximizing the entropy of predictions (or to minimizing the negative entropy) as proposed in Eq. 1, we resort to an alternative regularizer, which is a variant of the KL divergence  with q being the uniform distribution and the symbol K = representing equality up to an additive or multiplicative constant associated with the number of classes. We refer the reader to the Appendix I in  Global Learning Objective: Our final loss function takes the following form: L = L Seg (y, ŷ) -λL me (ŷ w ), where ŷ is the entire set of pixel predictions, L Seg the segmentation loss 1 , L me is one of the proposed maximum entropy regularization terms and λ balances the importance of each objective. Note that L me can take the form of the standard entropy definition, i.e. L me (ŷ w ) = L H (ŷ w ) (eq. (  Baseline Models: We trained baseline networks using a simple loss composed of a single segmentation objective L Seg , without adding any regularization term. We used the two most popular segmentation losses: cross-entropy (L CE ) and the negative soft Dice coefficient (L dice ) as defined by  We train two baseline models using the aforementioned regularizer L H (ŷ), considering cross-entropy (L CE ) and Dice losses (L dice ). We also assess the performance of focal-loss  Post-hoc Calibration Baselines. We also included two well known calibration methods typically employed for classification ",3
2037,Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation (vol3),,Training Details and Evaluation Metrics.,"As baselines, we used networks trained with L CE and L dice only. We also included the aforementioned posthoc calibration methods (namely IR and PS) as post-processing step for these vanilla models. We also implemented the confidence penalty-based method  To assess segmentation performance we resort to Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD), whereas we use standard calibration metrics: Brier score ",3
2038,Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation (vol3),,Results.,"Our main goal is to improve the estimated uncertainty of the predictions, while retaining the segmentation power of original losses. Thus, we first assess whether integrating our regularizers leads to a performance degradation. Table  Regarding calibration performance, recent empirical evidence  When evaluating the proposed MEEP regularizers (L KL ( Ŷw ) and L H ( Ŷw )) combined with the segmentation losses based on DSC and CE, we observe that DSC with L KL ( Ŷw ) consistently achieves better performance in most of the cases. However, for CE, both regularizers alternate best results, which depend on the dataset used. We hypothesize that this might be due to the different gradient dynamics shown by the two regularizers",3
2040,Maximum Entropy on Erroneous Predictions: Improving Model Calibration for Medical Image Segmentation (vol3),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43898-1 27.,3
2053,Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory (vol3),,Overview:,"The proposed framework for annotation-efficient medical image segmentation is illustrated in Fig.  Feature Extraction: In this work, we employ a U-Net ",3
2054,Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory (vol3),,Segmentation Branch:,"The segmentation branch g seg takes the feature map F as input and produces the final segmentation masks based on the available scribble or point annotations. Following recent works such as  VQ Memory Bank: Motivated by the similar feature patterns observed in medical images, we utilize the Vector Quantization (VQ) memory bank to store texture-oriented and global features, which are then employed for pseudo label generation. The pseudo label generation process involves three stages, as illustrated in Fig. ",3
2055,Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory (vol3),,Memory Bank Definition.,"In accordance with the VQVAE framework  Memory Update Stage. The feature map F recon is obtained from the last layer in the reconstruction branch and is utilized to update the VQ memory bank and retrieve an augmented feature Frecon . For each spatial location f j ∈ R 1×64 in F recon ∈ R 64×256×256 , we use L2 is used to compute the distance between f j and e k and find the nearest feature e i ∈ R 1×64 in the VQ memory bank, as follows: fj = e i , i = arg min k |f j -e k | 2 2 . Following  , where sg denotes the stop-gradient operator.",3
2056,Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory (vol3),,Pseudo Label Table Update,"Stage. The second stage mainly updates a pseudo label table, using the labelled regions on the reconstruction features and assigning pseudo labels on memory vectors. In particular, it uses the labelled pixels and their corresponding reconstruction features and finds the nearest vectors in the memory bank. As shown in Fig.  Pseudo Label Generation Stage. The third stage utilizes the pseudo label table to generate the pseudo labels. It takes the feature map F recon as inputs, then finds their nearest memory vectors, and retrieve the pseudo label according to the vector indices. The generated pseudo label is generated by the repetitive texture patterns on the reconstruction branch, which would include the segmenation information as well as other things.",3
2057,Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory (vol3),,Pseudo Label Generation:,"The generation of pseudo labels from the reconstruction branch is based on a texture-oriented and global view, as the memory bank stores the features extracted from the entire dataset. However, relying solely on it may not be sufficient, and it is necessary to incorporate more segmentation-specific information from the segmentation branch. Therefore, we leverage both approaches to enhance the model training. To incorporate both the segmentation-specific information and the textureoriented and global information, we dynamically mix the predictions y 1 from the segmentation branch and the pseudo labels y 2 from the VQ memory bank to generate the final pseudo labels y *  , where α is uniformly sampled from [0, 1]. The argmax function is used to generate hard pseudo labels. We then use the generated y * to supervise y 1 and assist in the network training. The pseudo label loss is defined as L pl (P L, y 1 ) = 0.5 × L dice (y * , y 1 ), where L dice is the dice loss, which can be substituted with other segmentation loss functions such as cross-entropy loss. Loss Function: Finally, our loss function is calculated as where λ V Q is hyper weights with λ V Q = 0.1.",3
2061,Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory (vol3),,Effect of the Auxiliary Task:,We designed a baseline by removing the reconstruction branch and VQ memory. The results in Table ,3
2062,Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory (vol3),,Effect of Pseudo Labels:,We also designed a baseline using only the predictions as pseudo labels. In Table ,3
2063,Minimal-Supervised Medical Image Segmentation via Vector Quantization Memory (vol3),,Effect of Different Levels of Aannotations:,"We also evaluated the impact of using different levels of annotations in point-supervised learning, ranging from more annotations, e.g. 10 points, to fewer annotations, e.g. 2 points. The results in Table ",3
2076,Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_25.,4
2086,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_47.,4
2090,Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models (vol4),,Employing Multiple Generations. Since calculating x t-1 during inference includes the addition of 1 [t>1],"β 1 is significant variability between different runs of the inference method on the same inputs, see Fig.  In order to exploit this phenomenon, we run the inference algorithm multiple times, then average the results. This way, we stabilize the results of segmentation and improve performance, as demonstrated in Fig.  Architecture. In this architecture, the U-Net's decoder D is conventional and its encoder is broken down into three networks: E, F , and G. The last encodes the input image, while F encodes the segmentation map of the current step x t . The two processed inputs have the same spatial dimensionality and number of channels. Based on the success of residual connections  The input image encoder G is built from Residual in Residual Dense Blocks  The encoder-decoder part of θ , i.e., D and E, is based on U-Net, similarly to  The residual block is composed of two convolutional blocks, where each convolutional block contains group-norm, SiLU activation, and a 2D-convolutional layer. The residual block receives the time embedding through a linear layer, SiLU activation, and another linear layer. The result is then added to the output of the first 2D-convolutional block. Additionally, the residual block has a residual connection that passes all its content. On the encoder side (network E), there is a downsample block after the residual blocks of the same depth, which is a 2D-convolutional layer with a stride of two. On the decoder side (network D), there is an upsample block after the residual blocks of the same depth, which is composed of the nearest interpolation that doubles the spatial size, followed by a 2D-convolutional layer. Each layer in the encoder has a skip connection to the decoder side.",4
2095,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation (vol4),,2),"Our method provides a flexible regularization between differently perturbed data such that a vulnerable network is effectively trained on challenging samples considering their ambiguities. 3) Our method preserves underlying morphological characteristics of medical images by augmenting data with quasiimperceptible perturbation. As a result, our method significantly improves sensitivity and Dice scores over existing augmentation methods on Kvasir-Seg ",4
2099,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation (vol4),,Baselines.,"Along with conventional augmentation methods (i.e., random horizontal and vertical flipping denoted as 'Basic' in Table ",4
2100,Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation (vol4),,Evaluation.,"To verify the effectiveness of our method, evaluations are conducted using various popular backbone architectures such as U-Net ",4
2128,Few-Shot Medical Image Segmentation via a Region-Enhanced Prototypical Transformer (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_26.,4
2130,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk (vol4),,Related Work.,"A literature review by Zhang et al.  Motivation. When segmenting OARs in the HaN region for the purpose of RT planning, a multimodal segmentation model that can leverage the information from CT and MR images of the same patient might be beneficial compared to separate single-modal models. Firstly, as intuition suggests, such a model would rely on the CT image for bone structures and on the MR image for soft tissues, and therefore improve the overall segmentation quality by exploiting the complementary information from both modalities. Secondly, a multimodal model would facilitate cross-modality learning by extracting knowledge from one and applying that knowledge to the other modality, potentially improving the segmentation accuracy. Several studies indicated that such an approach is feasible, for example, for improving video classification by training a model on an auxiliary audio reconstruction task ",4
2131,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk (vol4),,Contributions.,"To tackle these considerations, we propose a mechanism named modality fusion module (MFM) that can generally be applied to any network architecture that learns features from multiple modalities, and shows promising performance also in the missing modality scenario. The advantages of the proposed MFM are the following: 1) it enables the spatial alignment of FMs from one with FMs from the other modality to further reduce errors that persist after deformable registration of input images, and enrich the FMs to improve the final OAR segmentation, 2) it significantly improves the performance of the missing modality scenario compared to other baseline fusion approaches, and 3) it performs well also on single modality out-of-distribution data, therefore facilitating cross-modality learning and contributing to better model generalizability.",4
2133,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk (vol4),,Modality Fusion Module.,"The proposed MFM draws inspiration from the work of Jaderberg et al.  Baseline Comparison. We evaluate the performance of the proposed MFM nnU-Net against three baseline networks: 1) a single modality nnU-Net trained only on CT images, 2) a nnU-Net trained on concatenated CT and MR image pairs, and 3) a model with separate encoders for both modalities, but with a simple concatenation along the channel axis instead of the proposed MFM. In addition, we compare our model with the state-of-the-art modality-aware mutual learning nnU-Net (MAML) that was presented at MICCAI 2021 ",4
2137,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 71.,4
2146,From Sparse to Precise: A Practical Editing Approach for Intracardiac Echocardiography Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_73.,4
2150,Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation (vol4),,Statistics Randomization (SR).,Inspired by effectiveness of unreal styles in the input space ,4
2151,Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation (vol4),,Style Mixing (SM).,"To learn the domain-invariant representations explicitly, SM strategy is designed to randomly mix the augmented and original statistics along the channel wise. We first sample P ∈ R B×C from the Beta distribution: P ∼ Beta(α, α), and use P as the probability to generate the Bernoulli distribution from which to sample λ ∈ R B×C : λ ∼ Bern(P ), where α is set to 0.1 empirically  where f denotes the intermediate features. Finally, the mixed feature statistics is applied to perturb the normalized f similar to Eq. (  Different from MixStyle, we replace the batch-wise fusion with channel-wise mixing, which avoids the sampling preference and introduces original-feature reference, so as to learn the domain-invariant representations explicitly.",4
2157,Treasure in Distribution: A Domain Randomization Based Multi-source Domain Generalization for 2D Medical Image Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_9.,4
2162,CAS-Net: Cross-View Aligned Segmentation by Graph Representation of Knees (vol4),,Random View Masking.,"To reduce impacts of the errors in the initial pseudo-3D segmentation, we randomly masked out views (channels) in the multi-view (multi-channel) patches during training. The 3D volume, obtained from 2D scans by super-resolution, has a clearer tissue texture in the plane parallel to its original scanning plane. Thus, the random view masking helps to produce more accurate segmentation boundaries by forcing the network to learn from different combinations of patch views (channels). We set the probability of each view to be masked as 0.25. During inference, complete multi-view patches are used, reducing errors caused by the pseudo-3D segmentation.",4
2173,Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection (vol4),,Number of crops having RoI Number of crops not having RoI,"(2) Using only foreground regions in problems such as task incremental learning may result in a high degree of false positive segmentation. In such cases, having a few crops of background regions helps in reducing the forgetting in background regions as discussed in Sect. 3.3.",4
2179,Memory Replay for Continual Medical Image Segmentation Through Atypical Sample Selection (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 49.,4
2188,ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_61.,4
2193,Self-supervised Learning via Inter-modal Reconstruction and Feature Projection Networks for Label-Efficient 3D-to-2D Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_56.,4
2203,Fine-Grained Hand Bone Segmentation via Adaptive Multi-dimensional Convolutional Network and Anatomy-Constraint Loss (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_38.,4
2213,Diffusion Transformer U-Net for Medical Image Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 59.,4
2220,ACTION++: Improving Semi-supervised Medical Image Segmentation with Adaptive Anatomical Contrast (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 19.,4
2236,Transformer-Based Annotation Bias-Aware Medical Image Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_3.,4
2256,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation (vol4),,Fig. 2. The SwinUNETR-V2 architecture,"Swin-Transformer. We briefly introduce the 3D swin-transformer as used in Swin-UNETR  W-MSA and SW-MSA represent regular window and shifted window multi-head selfattention, respectively. MLP and LN represent multilayer perceptron and layernorm, respectively. A patch merging layer is applied after every swin transformer block to reduce each spatial dimension by half. Stage-Wise Convolution. Although Swin-transformer uses local window attention to introduce inductive bias like convolutions, self-attentions can still mess up with the local details. We experimented with multiple designs as in Fig.  Decoder. The decoder is the same as SwinUNETR ",4
2258,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation (vol4),,Implementation Details,"The training pipeline is based on the publicly available SwinUNETR codebase (https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR/BT CV, our training recipe is the same as that by SwinUNETR). We changed the initial learning rate to 4e-4, and the training epoch is adapted to each task such that the total training iteration is about 40k. Random Gaussian smooth, Gaussian noise, and random gamma correction are also added as additional data augmentation. There are differences in data preprocessing across tasks. MSD data are resampled to 1 × 1x1 mm resolution and normalized to zero mean and standard deviation (CT images are firstly clipped by .5% and 99.5% foreground intensity percentile). For WORD and FLARE preprocessing, we use the default transforms in SwinUNETR codebase (https://github. com/Project-MONAI/research-contributions/tree/main/SwinUNETR/BTCV, our training recipe is the same as that by SwinUNETR) and 3D UXNet codebase (see footnote 1). Besides these, all other training hyperparameters are the same. We only made those minimal changes for different tasks and show surprisingly good generalizability of the SwinUNETR-V2 and the pipeline across tasks. ",4
2259,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation (vol4),,Results,WORD Result. We follow the data split in  FLARE 2021 Result. We use the 5-fold cross-validation data split and baseline scores from ,4
2260,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation (vol4),,MSD Results.,"For MSD datasets, we perform 5-fold cross-validation and ran the baseline experiments with our codebase using exactly the same hyperparameters as mentioned. nnunet2D/3D baseline experiments are performed using nnunet's original codebase ",4
2261,SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation (vol4),,Variations of SwinUNetR-V2,"In this section, we investigate other variations of adding convolutions into swin transformer. We follow Fig. ",4
2273,Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation (vol4),,Effectiveness of Components.,As shown in Table  Effectiveness of the Attention-Based Smoothing. As shown in Table  Effectiveness of the Ways Modeling Part-Aware Prototypes. In Table ,4
2275,Structure-Decoupled Adaptive Part Alignment Network for Domain Adaptive Mitochondria Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 50.,4
2282,Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation (vol4),,Methods,"Comparative Experiments. We compared our C 2 SDG with two baselines, including 'Intra-Domain' (i.e., training and testing on the data from the same target domain using 3-fold cross-validation) and 'w/o SDG' (i.e., training on the source domain and testing on the target domain), and six SDG methods, including BigAug  Ablation Analysis. To evaluate the effectiveness of low-frequency components replacement (FR) in StyleAug and CFD, we conducted ablation experiments using BinRushed and Magrabia as the source domain, respectively. The average performance is shown in Table  Analysis of CFD. Our CFD is modularly designed and can be incorporated into other SDG methods. We inserted our CFD to ADS ",4
2284,Devil is in Channels: Contrastive Single Domain Generalization for Medical Image Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_2.,4
2286,Robust Segmentation via Topology Violation Detection and Feature Synthesis (vol4),,Contributions.,"(1) This paper presents a novel DL-based method for fast EC calculation, which, to the best of our knowledge, is the first paper introducing DL-compatible EC computation.  So far, topology in segmentation has only been explored through the use of PH ",4
2289,Robust Segmentation via Topology Violation Detection and Feature Synthesis (vol4),,Settings:,"The hyper-parameters in Eq. 3 and Eq. 4 are empirically chosen as Δ = 32 for the CREMI dataset, Δ = 8 for the dHCP dataset, and t = 0.6 for both datasets. We choose Δ to be higher for CREMI than for dHCP because: (1) the resolution of CREMI is higher and (2) the topology of the fetal cortex may change in smaller regions. We choose a default U-Net as the backbone for all methods for comparison, but our method can also be Incorporated into other segmentation frameworks. For the training process, we first use default crossentropy loss to train the first segmentation network, then the TFS network. Note that the parameters in TVD are fixed. Implementation: We use PyTorch 1.13.1 and calculate the Betti number with the GUDHI package  Evaluation Metrics. Segmentation performance is evaluated by Dice score and averaged surface distance (ASD), and the performance of topology is evaluated by Betti errors, which is defined as: e i =| β pred i -β gt i |, where i ∈ {0, 1} indicates the dimension. We also report the mean Betti error as e = e 0 + e 1 . Quantitative Evaluation. We compare the segmentation performance of our method with three baselines which are proposed to preserve shape and topology: cl-Dice loss  Ablation Study. We first evaluate the effectiveness of our TVD design. We train our pipeline without the TVD block. Instead, we use the difference map between the prediction and GT as a substitute for the topology violation map. We also summarize the qualitative results in Fig.  Discussion. This study sheds new light on improving and evaluation of topology-aware medical image segmentation. We observe that most existing methods either do not consider topological constraints, or are limited by their high computational complexity. As a computation-efficient block, our method can be easily integrated into existing segmentation methods to improve the topological structure. A limitation for topology-aware segmentation methods is that they are easy to be affected by noises, so they might be more suitable to datasets with clear topology structures.",4
2291,Robust Segmentation via Topology Violation Detection and Feature Synthesis (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_7.,4
2302,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation (vol4),,", . . . , α s(h,w) Q","], (h, w) ∈ (H, W ). We can compute the belief mass and uncertainty mass of the input image by where b s(h,w) q ≥ 0 and u s(h,w) ≥ 0 denote the probability of the pixel at coordinate (h, w) for the q th class and the overall uncertainty value respectively. We also define U s = {u s(h,w) , (h, w) ∈ (H, W )} as the pixel-wise uncertainty of the segmentation result.",4
2307,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation (vol4),,Acknowledgements,. This work was supported by the ,4
2308,Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 4.,4
2312,Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity (vol4),,Medical Data Sets:,We tested the three variants of the U-Net architecture on three medical binary segmentation data sets: a Breast Ultrasound ,4
2315,Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity (vol4),,Out-of-Domain (Robustness):,Rows in Fig. ,4
2317,Do We Really Need that Skip-Connection? Understanding Its Interplay with Task Complexity (vol4),,Out-of-Domain (Robustness):,"Focusing on the rows ""Harder"" and ""Easier"" in Table ",4
2327,One-Shot Traumatic Brain Segmentation with Adversarial Training and Uncertainty Rectification (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 12.,4
2333,UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation (vol4),,"Can we capture the embedding distribution by considering all voxels, including both labeled and unlabeled, and exploit the knowledge of the entire dataset?","To answer it, we propose to learn fused prototypes through uncertainty-based attention pooling. The fused prototypes represent the most representative and informative examples from both the labeled and unlabeled data for each class. The main contributions of our work can be summarized as follows: 1) We develop a novel uncertainty-informed prototype consistency learning framework, UPCoL, by considering voxel-level consistency in both latent feature space (i.e., prototype) and decision space. 2) Different from previous studies, we design a fused prototype learning scheme, which jointly learns from labeled and unlabeled data embeddings. 3) For stable prototype learning, we propose a new entropy measure to qualify the reliability of unlabeled voxel and an attention-weighted strategy for fusion. 4) We apply UPCoL to two-class and three-class segmentation tasks. UPCoL outperforms the SOTA SSL methods by large margins. The labeled images go through the student model for supervised learning, while the unlabeled images go through the teacher model for segmentation and uncertainty estimation. In Uncertainty-informed Prototype Fusion module, we utilize the reliability map to fuse prototypes learned from labeled and unlabeled embeddings. The similarity between fused prototypes and feature embeddings at each spatial location is then measured for consistency learning.",4
2338,UPCoL: Uncertainty-Informed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_63.,4
2344,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation (vol4),,Types Methods,Dice↑ mIoU↑ HD95↓ infer time CAM GradCAM  CAM GradCAM ,4
2347,Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 72.,4
2351,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation (vol4),,Shifted-W-MSA-Based Local Branch. The image embedding e,"] is fed into the local branch in the encoder block. In the process of window partition (denoted as WP), e i 1 is split into non-overlapping windows after a layer normalization (LN) operation to obtain the window matrix m i 1 . Since we set M as 2, the input of 4×4×4 size is uniformly divided into 8 windows of 2 × 2 × 2. Following 3D Swin-Transformer  where ẑi 1 represents the attention score after W -MSA calculation, "" Shifted-"" represents that we use the shifted-window partition and restoration in the second block of every encoder layer. o i 1 is the final output of the local branch. Shuffle-W-MCA-Based Global Branch. Through the local branch, the network still cannot model the long-distance dependencies between non-adjacent windows in the same layer. Thus, Shuffle-W-MCA is designed to complete the complementary task. After the window-partition process that converts the embedding , inspired by ShuffleNet  ( In the second block, we get the final output o i 2 of the layer through the same process.",4
2353,DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation (vol4),,Shifted-W-MCA-Based Global Branch.,"Apart from employing Shifted-W-MSA to form the local branch of the decoder layer, we design a novel Shifted-W-MCA mechanism for the global branch to ease the information loss during the decoding process and take full advantage of the features from the encoder layers. The global branch receives the query matrix from the split feature map d j 2 ∈ R D j ×H j ×W j ×[C j /2] , while receiving key and value matrices from the encoder block in the corresponding stage, denoted as Q j 2 , K e i , and V e i . The process of Shifted-W-MCA can be formulated as follows: where ẑj 2 denotes the attention score after MCA calculation, o j 2 denotes the final output of the global branch.",4
2377,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets (vol4),,Domain Adapter (DA):,"In multi-domain adaptive training, some methods build domain-specific layers in parallel with the main network  Attention Generation generates attention for each head. We first pass a domain label vector m (we adopt one-hot encoding m ∈ R M but other encodings are possible) through one linear layer with a ReLU activation function to acquire a domain-aware vector d ∈ R K r . K is the channel dimension of features from the heads. We set the reduction ratio r to 2. After that, similar to  where ψ is a softmax operation across heads and Information Selection adaptively selects information from different heads. After getting the feature from the hth head, we utilize a h to calibrate the information along the channel dimension: ũh",4
2378,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets (vol4),,Mutual Knowledge Distillation (MKD):,"Distilling knowledge from domainspecific networks has been found beneficial for universal networks to learn more robust representations  Each Auxiliary Peer is trained on a small, individual dataset specific to that peer (Fig. ",4
2380,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets (vol4),,Datasets and Evaluation Metrics:,"We study 4 skin lesion segmentation datasets collected from varied sources: ISIC 2018 (ISIC)  Implementation Details: We conduct 3 training paradigms: separate (ST), joint (JT), and multi-domain adaptive training (MAT), described in Sect. 1, to train all the models from scratch on the skin datasets. Images are resized to 256 × 256 and then augmented through random scaling, shifting, rotation, flipping, Gaussian noise, and brightness and contrast changes. The encoding transformer blocks' channel dimensions are [64, 128, 320, 512] (Fig. ",4
2381,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets (vol4),,Comparing Against BASE:,"In Table  We employ the two fixed-size (i.e., independent of M ) multi-domain algorithms proposed by Rundo et al. ",4
2382,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets (vol4),,Ablation Studies and Plug-in Capability of DA:,"We conduct ablation studies to demonstrate the efficacy of DA, MKD, and auxiliary peers. Table ",4
2384,MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 43.,4
2394,Semi-supervised Class Imbalanced Deep Learning for Cardiac MRI Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_44.,4
2400,RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization (vol4),,Dataset:,"The proposed RBGNet was evaluated on a dataset consisting of 74 OCTA images obtained using the Heidelberg OCT2 system (Heidelberg, Germany). All images were from AMD patients with CNV progression, captured in a 3 × 3 mm 2 area centered at the fovea. The enface projected OCTA images of the avascular complex were used for our experiments. All the images were resized into a resolution of 384 × 384 for experiments. The CNV areas and vessels were manually annotated by one senior ophthalmologist, and then reviewed and refined by another senior ophthalmologist. All images were acquired with regulatory approvals and patient consents as appropriate, following the Declaration of Helsinki. Implementation Details: Our method is implemented based on the PyTorch framework with NVIDIA GeForce GTX 1080Ti. We train the model using an Adam optimizer with an initial learning rate of 0.0001 and a batch size of 4 for 300 epochs, without implementing a learning rate decay strategy. During training, the model inputs were subject to standard data augmentation pipelines, including random horizontal, vertical flips, random rotation, and random cropping. A 5-fold cross-validation approach is adopted to evaluate the performance.",4
2401,RBGNet: Reliable Boundary-Guided Segmentation of Choroidal Neovascularization (vol4),,Comparison with State-of-the-Arts:,"To benchmark our model's performance, we compared it with several state-of-the-art methods in the medical image segmentation field, including U-Net ",4
2417,M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_14.,4
2433,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection (vol4),,P recision = T P T P + F P,"(3) where T P represents the number of positive samples correctly identified as positive samples, F P represents the number of negative samples incorrectly identified as positive samples and F N represents the number of positive samples incorrectly identified as negative samples. AP 50 is the area under the precision-recall (PR) curve formed by precision and recall. For AP 50:95 , divide 10 IoU threshold of 0.5:0.05:0.95 to acquire the area under the PR curve, then average the results. FPS represents the number of images detected by the model per second.",4
2437,RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_57.,4
2444,Certification of Deep Learning Models for Medical Image Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_58.,4
2445,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data (vol4),,Multi-site semi-supervised learning (MS-SSL),"The unlabeled image pool can be quickly enriched via the support from partner clinical centers with low barriers of entry (only unlabeled images are required) Data heterogeneity due to different scanners, scanning protocols and subject groups, which violate the typical SSL assumption of i.  ",4
2450,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data (vol4),,Stability Under Perturbations.,"Although originally designed for typical SSL, encouraging stability under perturbations  where mean squared error is also adopted as the distance function d(•, •). Overall, the final loss for the multi-site unlabeled data is summarized as:",4
2459,Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images (vol4),,Method Acc,Dice Jaccard w/o text 0.9610 0.8414 0.7262 1 layer 0.9735 0.8920 0.8050 2 layers 0.9748 0.8963 0.8132 3 layers 0.9752 0.8978 0.8144 As can be seen from the Table ,4
2461,Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images (vol4),,Impact of Text Prompts at Different Granularity on Segmentation,"Performance. In Sect. 3.1 we mention that each sample is extended to a text annotation with three parts containing positional information at different granularity, as shown in the Fig. ",4
2462,Ariadne’s Thread: Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray Images (vol4),,Impact of the Size of Training Data on Segmentation Performance.,As shown in Table ,4
2471,A Sheaf Theoretic Perspective for Robust Prostate Segmentation (vol4),,Composition:,"The quantised embedding space, ẑ, is split into c groups. The composition of each group in ẑ to form each class segmentation, Y c in the output, Y involves two steps. Initially, a decoder with grouped convolutions equal to the number of classes followed by the softmax function maps, ẑ ∈ R 128×16×16×12 to C ∈ R p×c×256×256×24 where p is the number of patches for each class c. The second step of the composition uses a cellular sheaf to model the composition of Y c by gluing the patches together in an ordered manner defined by a poset while tracking its topology using persistent homology. This in turn enforces D to be sampled in a topological preserving manner as input into the decoder/composer to improve both the local and global topological correctness of each class segmentation output, Y c after composition.",4
2472,A Sheaf Theoretic Perspective for Robust Prostate Segmentation (vol4),,Illustration:,"We illustrate our methodology of using cellular sheaves with a simple example in Fig.  Each element in P is associated with a subspace in V such that the inclusion relationship is satisfied. Therefore, in Fig. ",4
2473,A Sheaf Theoretic Perspective for Robust Prostate Segmentation (vol4),,Implementation,"We construct cellular sheaves, F over P c and P c and minimise the distance between these cellular sheaves. We firstly plot persistence diagrams, D from the set of vectors (τ i , τ j ) in F(P c i ) and F( P c i ). Next, we minimise the total p th Wasserstein distance (topological loss) between the persistence diagrams D(F(P c i )) and D(F( P c i )) shown in Eq. 4 where η : D(F(P c i )) → D(F( P c i )) is a bijection between the persistence diagrams ",4
2476,A Sheaf Theoretic Perspective for Robust Prostate Segmentation (vol4),,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 24. Pre-processing: All images are resampled to 0.5 × 0.5 × 3 mm, centre cropped to 256 × 256 × 24 and normalised between 0 and 1.",4
2477,A Sheaf Theoretic Perspective for Robust Prostate Segmentation (vol4),,Model:,"In order to address the anisotropic characteristics of Prostate MRI images, we have chosen a hybrid 2D/3D UNet as our baseline model. We use the same encoder and decoder architecture as the baseline model in our method. See supplementary material for further details.",4
2478,A Sheaf Theoretic Perspective for Robust Prostate Segmentation (vol4),,Comparison:,"We compare our method with the nnUNet  Training: In all our experiments, the models were trained using Adam optimization with a learning rate of 0.0001 and weight decay of 0.05. Training was run for up to 500 epochs on three NVIDIA RTX 2080 GPUs. The performance of the models was evaluated using the Dice score, Betti error  In our ablation studies, the minimum number of shape components required in D for the zonal and zonal + tumour segmentation experiments was 64 and 192 respectively before segmentation performance dropped. See supplementary material for ablation experiments analysing each component of our framework.",4
2500,A General Stitching Solution for Whole-Brain 3D Nuclei Instance Segmentation from Microscopy Images (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_5.,4
2510,Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 51.,4
2515,NISF: Neural Implicit Segmentation Functions (vol4),,Results.,"As the latent code is optimized during inference, segmentation metrics follow an overfitting pattern (see Fig.  The benefits of training a prior over the population is investigated by tracking inference-time Dice scores obtained from spaced-out validation runs. Training of the prior is shown to significantly improve performance of segmentation and image reconstruction at inference-time as seen in Fig.  Validation results showed the average optimal number of latent code optmimization steps at inference to be 672. Thus, the test set per-class Dice scores (Table  Right ventricle segmentation in basal slices is notoriously challenging to manually annotate due to the delineation of the atrial and ventricular cavity combined with the sparsity of the resolution along the long axis  We go on to show NISF's ability to generate high-resolution segmentation for out-of-plane views. We optimize on a short-axis volume at inference and subsequently sample coordinates corresponding to long-axis views. Despite never presenting a ground-truth long-axis image, the model reconstructs an interpolated view and provides an accurate segmentation along its plane (Fig. ",4
2529,Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping (vol4),,right).,"Normally, the feature distribution of each tumor type is assumed to be independent normal distribution, so their joint distribution is given by: where F k , k = 1, 2, 3 represents the feature set of oligodendroglioma, astrocytoma, and glioblastoma, respectively, μ k and σ 2 k are mean and variance of F k . To impose the ordinal constraint, we define the desired feature distribution of each tumor type as N (μ 1 , σ2 1 ) for k = 1, and N (μ k-1 + Δ k , σ2 k ) for k = 2 and 3. In this way, the feature distribution of each tumor type depends on its predecessor, and the mean feature of each tumor type μk (except μ1 ) is equal to the mean feature of its predecessor μk-1 shifted by Δ k . Note that Δ k is set to be larger than 3 × σk to ensure the desired ordering  which can be represented as: where μ1 and σk , k = 1, 2, 3 can be learned by the tumor subtyping network. Finally, the ordinal loss, which is in the form of KL divergence, is defined as: In our method, μ k and σ 2 k are calculated by where Φ θ and G are the encoder and GAP of the tumor subtyping network, respectively, θ is the parameter set of the encoder, stands for the subset containing the pre-operative multimodal MR brain images of the patients with the k-th tumor type, N k is the patient number in D k . So we impose the ordinal loss L KL to the features after the GAP of the tumor subtyping network as shown in Fig.  The tumor subtyping network is first trained before being integrated into the survival prediction backbone. In the training stage of the tumor subtyping network, each input batch contains pre-operative multimodal MR brain images of N patients and can be divided into K = 3 subsets according to their corresponding tumor types, i.e., D k , k = 1, 2, 3. With the ordinal constrained feature distribution, high consistent features can be augmented between neighboring tumor types. Based on the original and augmented features, the performance of the tumor subtyping network can be enhanced. Once the tumor subtyping network has been trained, it is then integrated into the survival prediction backbone, which is trained under the constraint of the cox proportional hazard loss L Cox .",4
2534,Medical Boundary Diffusion Model for Skin Lesion Segmentation (vol4),,Boundary Evolution Image,"Small Large Fig.  Two representative boundaries are visualized in Fig.  However, current models for skin lesion segmentation are still struggling with extremely challenging cases, which are often encountered in clinical practice. While some approaches aim to optimize the model architecture by incorporating local and global contexts and multi-task supervision, and others seek to improve performance by collecting more labeled data and building larger models, both strategies are costly and can be limited by the inherent complexity of skin lesion boundaries. Therefore, we propose a novel approach that shifts the focus from merely segmenting lesion boundaries to predicting their evolution. Our approach is inspired by recent advances in image synthesis achieved by diffusion probabilistic models  In this paper, we propose a Medical Boundary Diff usion model (MB-Diff ) to improve the skin lesion segmentation, particularly in cases where the lesion boundaries are ambiguous and have extremely large or small sizes. The MB-Diff model follows the basic design of the plain diffusion model, using a sequential denoising process to generate the lesion mask. However, it also includes two key innovations: Firstly, we have developed an efficient multi-scale image guidance module, which uses a pretrained transformer encoder to extract multi-scale features from prior images. These features are then fused with the evolution features to constrain the direction of evolution. Secondly, we have implemented an evolution uncertainty-based fusion strategy, which takes into account the uncertainty of different initializations to refine the evolution results and obtain more precise lesion boundaries. We evaluate our model on two popular skin lesion segmentation datasets, ISIC-2016 and PH 2 datasets, and find that it performs significantly better than existing models.",4
2557,Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction (vol4),,Contribution.,"In this work, we present Conditional Temporal Attention Network (CoTAN). CoTAN adopts attention mechanism ",4
2561,Conditional Temporal Attention Networks for Neonatal Cortical Surface Reconstruction (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_30.,4
2566,QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality (vol4),,Statistical Analysis:,We assessed the performance of the subject-level segmentation quality prediction in terms of Pearson coefficient r and MAE between the predicted DSC and the ground-truth DSC. The performance of the segmentation error localization was assessed by the DSC err between the predicted segmenta- tion error map and the ground-truth segmentation error map. P-values were computed using a paired t-test between DSC predicted by QCResUNet versus ones predicted by corresponding baselines.,4
2596,MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_13.,4
2600,EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation (vol4),,Comparative Results. The comparative experimental results presented in,Table  Figure  Ablation Results. We conduct extensive ablation experiments to demonstrate the effectiveness of our proposed modules. The baseline utilized in our work is referenced from MALUNet ,4
2616,Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI (vol4),,Competing Methods and Evaluation Metrics:,"To comprehensively evaluate the proposed method, We compare it with 3D segmentation methods, including Dual Attention Net (DANet)  Implementation Details: We implement our proposed framework with PyTorch using two NVIDIA RTX 2080Ti GPUs to accelerate model training. Following DDPM  Once the DM is trained, we extract intermediate feature maps from four resolutions for further segmentation task. Similar to DM, the SM also consists of four resolution blocks. However, unlike channel settings of DM, we set 128, 256,  512, 1024 channels for each stage in the SM to capture expressive and sufficient semantic information. The SM is optimized by Adam with a learning rate 2 × 10 -5 and a weight decay 10 -6 . The model is trained for 500 epochs with the batch size to 1. No data augmentation techniques are used to ensure fairness. Comparison with SOTA Methods: The quantitative comparison of the proposed method to recent state-of-the-art methdos is reported in Table  2) The intermediate activations from  diffusion models effectively capture the semantic information and are excellent pixel-level representations for the segmentation problem  Ablation Study: To explore the effectiveness of the latent kinetic code, we first conduct ablation studies to select the optimal setting. We denote the intermediate features extracted from each stage in the DM as f 1 , f 2 , f 3 , and f 4 , respectively, where f i represents the feature map of i-th stage. Table ",4
2620,Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation (vol4),,Prototype Extraction.,"Considering there exists more plentiful class-aware information in the feature space than the predictions, we propose the class-aware alignment for better adaptation in the feature space. To achieve class-aware alignment, we first derive the class-aware source prototypes from the source features with the corresponding labels. The prototypes can be calculated as the centroid of each class in the feature space: where f s b,h,w ∈ R is the source feature vectors, B s is the batch size, and H s , W s is the height and width of the features. c is the index of class number C. The maximum of C is 1. The prototypes can represent the class knowledge in the source domain.",4
2621,Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation (vol4),,Inter-and Intra-class Constraints.,"To make the source prototypes represent the class-discriminative source knowledge more accurately, we incorporate inter-and intra-class constraints on the prototypes, which can further help better class-aware alignment across domains. The inter-class loss L s inter intends to push the prototypes of different classes far away from each other, which can be implemented by minimizing the average cosine distance of different prototype pairs: In contrast, the intra-class loss L s intra is designed to pull the feature instance point closer to its corresponding prototype, i.e., making the feature distribution of the same class more concentrated/compact. The intra-class loss can be formulated as maximizing the average cosine distance between the prototype and the features belonging to the same class: It is not straightforward to align target domain to source domain in the class level, considering the lack of groundtruth labels in target domain. To achieve more reliable class-aware alignment for target samples, we only perform alignment on the instances with higher confidence. Specifically, we first calculate the cosine distance between each target feature and all the source prototypes, and only select instances { f t } the distance of which is closer than a preset threshold τ . The intra-class alignment loss enforces f t c to be closer to its corresponding prototype p t c : The class-aware alignment loss L align is the combination of these three losses, i.e., L align = L s intra + L s inter + L t intra . It is noteworthy that the alignment loss is optimized directly on the feature space instead of the final output predictions, considering there is more abundant information in the feature space. Pseudo Supervision. The above mentioned alignment loss L align only affects partial target features with higher confidence. To further force the alignment across domains in the feature space, we incorporate a pseudo supervision on the feature space. Specifically, based on the cosine distance between the feature of each location and the source prototypes, we can attain a distance map P g , which can be regarded as a segmentation prediction directly from feature space instead of the prediction head. We utilize a pseudo label map P t2 as groundtruth to supervise the learning of P g , leading to alignment directly on feature space. The formulation of P t2 will be discussed in the later section. The supervision is the standard cross entropy loss: Intra-domain Consistency. The alignment cross domains will borrow the knowledge from source domain to target domain. However, there exists abundant knowledge and information in the target domain itself  We incorporate two consistency losses on the feature level L cf and the final prediction level L cp , respectively: where MSE denotes the standard mean squared error loss. Training and Inference. During the training phase, the total training objective L total is formulated as : where L s seg denotes the supervised segmentation loss with the cross-entropy loss and λ {align,p,cf,cp} are the hyperparameters for balancing different terms. Note Table ",4
2622,Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation (vol4),,Methods,"VNC III → Lucchi (Subset1) VNC III → Lucchi (Subset2) mAP(%) F1(%) MCC(%) IoU(%) mAP(%) F1(%) MCC(%) IoU(%) that the feature extractor and the segmentation head are shared weights in the training phase. Their detailed structures can be found in the supplementary material. During the inference phase, we only adopt the trained feature extractor and segmentation head to predict the target images.",4
2625,Class-Aware Feature Alignment for Domain Adaptative Mitochondria Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 23.,4
2665,EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 22.,4
2685,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness (vol4),,Initial Edge Detection:,"The purpose of initial detection, which is documented in the Initial_edge_detection function of Algorithm 1, is to provide a starting point, i.e., a rough boundary, for the next step of iterative improvement. The high-level idea is that EdgeMixup detects several edge candidates using the color range of ground-truth lesions in both Red-Green-Blue (RGB) and Hue-Saturation-Value (HSV) color space and then selects the target edge using a learning model based on the output confidence score. First, EdgeMixup trains a classification model based on a mixup of the ground-truth segmentation under clinician supervision and the original image (Line 7). Second, EdgeMixup generates many edge candidates. For example, EdgeMixup collects the mean range of lesion color from the training set and use the range as threshold to filter out any given sample for a candidate mask (Line 9). Lastly, EdgeMixup selects an edge candidate with the highest confidence score output by the learning model (Line 11) and returns it as the edge for this given sample. Note that the initial edge detection is irrelevant to the sample size of a particular subpopulation, thus improving the fairness. That is, even if the original dataset is imbalanced, as long as one sample from a subpopulation exists, the color range of the sample's lesion is considered in the initial detection. ",4
2686,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness (vol4),,9:,"Get all edge candidates {edge1, edge2, .., edgen} for each sample x 10: Mixup each edge candidate with x 11: Query mclass using all mixed-up {xedge 1 , ...xedge n } and choose the optimal edge edge opt",4
2687,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness (vol4),,12:,"Generate edged sample xedge = Mixup(x, edge opt , α) ",4
2688,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness (vol4),,21:,while current_Jaccard > best_Jaccard do,4
2689,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness (vol4),,22:,best_Jaccard = current_Jaccard,4
2690,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness (vol4),,23:,"Predict lesion masks using miter, convert them to lesion edge edge",4
2691,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness (vol4),,24:,"Generate new training set for next model Mixup(Dtrain, edge, α)",4
2692,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness (vol4),,25:,Train a model for next iteration miter+1,4
2693,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness (vol4),,26:,Evaluate miter+1 using edged D test edge and get current_Jaccard,4
2694,EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness (vol4),,27:,"iter += 1  affected area, further detection will refine and constrain the detected boundary. Besides, EdgeMixup calculates a linear combination of original image and lesion boundary, i.e., by assigning the weight of original image as α and lesion boundary as 1α. Figure ",4
2707,Scaling up 3D Kernels with Bayesian Frequency Re-parameterization for Medical Image Segmentation (vol4),,Ablation Studies with Block Designs & Optimizers.,"With the plain convolution design, a mean dice score of 0.906 is demonstrated with AdamW optimizer and perform slightly better than that with SGD. With the additional design of a parallel small kernel branch, the segmentation performance significantly improved (SGD: 0.898 to 0.917, AdamW: 0.906 to 0.929) with the optimized parallel branch LR using SR. The performance is further enhanced (SGD: 0.917 to 0.930, AdamW: 0.929 to 0.937) without being saturated with the increase of the training steps. By adapting BFR, the segmentation performance outperforms the parallel branch design significantly with a Dice score of 0.944. Effectiveness on Different Frequency Distribution. From Fig.  Limitations. The shape of the generated Bayesian distribution is fixed across all kernel weights with an unlearnable distance function. Each channel in kernels is expected to extract variable features with different distributions. Exploring different families of distributions to rescale the element-wise convergence in kernels will be our potential future direction.",4
2714,Evolutionary Normalization Optimization Boosts Semantic Segmentation Network Performance (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8 67.,4
2719,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation (vol4),,SA Spectral,The feature maps obtained from the ensemble can be decoded using lightweight 2D decoders to generate segmentation masks. 3 Experimental Results,4
2723,Factor Space and Spectrum for Medical Hyperspectral Image Segmentation (vol4),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43901-8_15.,4
2731,Adult-Like Phase and Multi-scale Assistance for Isointense Infant Brain Tissue Segmentation (vol4),,Effectiveness of Multi-phase Synthesis.,"In Table  Effectiveness of Multi-scale Assistance. In order to demonstrate the effectiveness of the additional branch in TMSN, which contains a larger receptive field, we show the corresponding results as one part of the ablation study in Table  Comparison with State-of-the-Arts (SOTAs). We conduct a comparative evaluation of our framework against several SOTA learning-based tissue segmentation networks, including 1) the 3D UNet segmentation network  To further illustrate the advanced performance of our framework, we provide a visual comparison of two typical cases in Fig. ",4
2757,DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification (vol5),,Visualization of Our Diffusion Procedure:,"To illustrate the diffusion reverse process guided by our dual-granularity conditional encoding, we used the t-SNE tool to visualize the denoised feature embeddings at consecutive time steps. Figure ",5
2759,DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 10.,5
2764,Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation (vol5),,R-wise Fusion Block (RFB),The conventional feature fusion methods (denoted as F in Table ,5
2766,Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation (vol5),,Implementation Details:,"The intensity values of all GT OSMs are normalized from [0, 63] to [0, 1] for loss calculation. We use unified center coordinates as the polar origin for polar transformation since all OCT samples are aligned to the macular fovea during acquisition. Since the background pixels in predicted OSM are hardly equal to 0, which cause many noise points in reconstructed point cloud, only intensity values that are not equal to OSM  Evaluation Metrics: Since our proposal employs 2D OSM regression in polar coordinates to achieve 3D reconstruction in cartesian coordinates, we evaluate its performance using both 2D and 3D metrics. Specifically, we use Chamfer Distance (CD) to measure the mean distance between predicted point clouds and its GT point cloud, and Structural Similarity Index Measure (SSIM) as 2D metrics to evaluate the OSM regression performance. Additionally, we introduce point density (Density) as an extra metric to evaluate the number ratio of total points between predicted and GT point clouds, defined as Density = |1 -N pred /N GT |, where N pred is the number of predicted point clouds, N GT is the number of GT point clouds. The lower Density, the fewer defects in the valid region of OSMs.",5
2767,Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation (vol5),,Method Comparison and Ablation Study:,"The effectiveness of the key components of PESNet is investigated through a comparison and ablation study, as presented in Fig.  In our PESNet, we adopt unshare-weight for two branches, which produces the best metrics in the last row. Therefore, our proposed algorithm reconstructs the eyeball close to GT considering the specificity of the eyeball.",5
2768,Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation (vol5),,Ablation Study About Loss Function:,"We compare different loss functions applied in our task, and the results as shown in Table  Validation on Disease Sample: Although our POS dataset only contains the data of healthy eyeballs, we still test our PESNet on two disease cases: one is high myopia and the other is Diabetic Macular Edema. As shown in Fig. ",5
2770,Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 18.,5
2782,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 74.,5
2785,A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types (vol5),,Data Use Declaration.,"The complete version of the public database used for validating the proposed strategy is provided by Matek et. al., under the TCIA Data Usage Policy and Restrictions, and it is publicly available at TCIA platform, https://doi.org/10.7937/TCIA.AXH3-T579. No ethical compliance statement is presented in this document since it is covered by the original dataset publication ",5
2799,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation (vol5),,Input:,"The input RGB image I ∈ R H×W ×3 ; The module T whose input and output channel numbers are 1 and 3, respectively; Output: The re-colored image Io ∈ R AssignV alue denotes re-assembling the sorted values according to the provided indices. Details of the module T are included in the supplementary material. Via RC, the original fine-grained structure information from I g is recovered in I r . In this way, the re-colored image is advantageous in two aspects. First, the appearance difference between pathological images caused by the change in scanners and staining protocols is eliminated. Second, the re-colored image preserves fine-grained structure information, enabling precise instance segmentation to be possible.",5
2801,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation (vol5),,Input:,"Original feature maps X ∈ R H×W ×C . The C-dimensional feature vector on its pixel (i, j) is denoted as xij; The modules Eμ and E δ that re-estimate feature statistics; Δsra ∈ R 1×1×C that is obtained via running mean of Δs in the training stage; The momentum factor α used to update Δsra ; (Optional) Δs = f (ρ); Output: Normalized feature maps Y ∈ R H×W ×C ; To verify the above viewpoint, we evaluate the baseline model under different foreground-background ratios. Specifically, we first remove the foreground pixels via in-painting  The above problem is common in nucleus segmentation because pathological images from different organs or tissues tend to have significantly different foreground-background ratios. However, this phenomenon is often ignored in existing research. To handle this problem, we propose the Distribution-Aware Instance Normalization (DAIN) method to re-estimate feature statistics that account for different ratios of foreground and background pixels. Details of DAIN is presented in Algorithm 2. The structures of E μ and E δ are included in the supplemental materials. As shown in Fig.  The training of RPH requires an extra loss term L rph , which is formulated as bellow: where ρ g denotes the ground truth foreground-background ratio, and L BCE and L MSE denote the binary cross entropy loss and the mean squared error, respectively.",5
2806,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 57.,5
2813,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment (vol5),,(b) Style Transfer:,"This sub-network creates the stylized IHC image using an attention module, given (1) the input hematoxylin and the mIF marker images and (2) the style and its corresponding marker images. For synthetically generating stylized IHC images, we follow the approach outlined in AdaAttN  For the stylized IHC images with ground truth CD3/CD8 marker images, we also segmented corresponding DAPI images using our interactive deep learning ImPartial  We evaluated the effectiveness of our synthetically generated dataset (stylized IHC images and corresponding segmented/classified masks) using our generated dataset with the NuClick training dataset (containing manually segmented CD3/CD8 cells)  We also tested the trained models on 1,500 randomly selected images from the training set of the Lymphocyte Assessment Hackathon (LYSTO) ",5
2817,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment (vol5),,Data use Declaration and Acknowledgment:,"This study is not Human Subjects Research because it was a secondary analysis of results from biological specimens that were not collected for the purpose of the current study and for which the samples were fully anonymized. This work was supported by MSK Cancer Center Support Grant/Core Grant (P30 CA008748) and by James and Esther King Biomedical Research Grant (7JK02) and Moffitt Merit Society Award to C. H. Chung. It is also supported in part by the Moffitt's Total Cancer Care Initiative, Collaborative Data Services, Biostatistics and Bioinformatics, and Tissue Core Facilities at the H. Lee Moffitt Cancer Center and Research Institute, an NCI-designated Comprehensive Cancer Center (P30-CA076292).",5
2818,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 68.,5
2824,Detection of Basal Cell Carcinoma in Whole Slide Images (vol5),,"Acc(W, d, D","3 Experiments The dataset, comprised of 194 skin slides acquired from the Southern Sun Pathology laboratory, includes 148 BCC cases and 46 other types (common nevus, SCC), all manually annotated by a dermatopathologist. BCC slides served as positive samples and the rest as negatives. These slides were scanned at ×20 magnification with a 0.44 µm pixel size using a Leica Aperio AT2 Scanner. The patient data were separated between training and testing to prevent overlap. Details are shown in Table ",5
2841,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 54.,5
2845,Multi-scale Prototypical Transformer for Whole Slide Image Classification (vol5),,Prototypical Transformer (PT).,"Most tissues in WSIs are redundancy, and therefore, we introduce the instance prototypes to reduce redundant instances. Specifically, for each instance bag X bag ∈ R n×d k , the K-means clustering algorithm is applied on all instances to get K centers (prototypes). These cluster prototypes can be used as instances to represent a new bag P bag ∈ R k×d k . However, the K-means clustering algorithm is sensitive to the initial selection of cluster centers, i.e. different initializations can lead to different results, and the final result may not be the global optimal solution. It is essential to try different initializations and choose the one with the lowest error. However, the WSI dataset generally has a long sequence of instances, which makes the clustering algorithms computationally expensive and slow down as the size of the bag increases. To solve the issue above, we propose to apply the self-attention (SA) mechanism in Transformer to re-calibrate these cluster prototypes. As shown in Fig.  where W q , W k , W v ∈ R d k ×d k are trainable matrices of query P bag and the key-value pair (X bag , X bag ), respectively, and A map ∈ R k×n is the attention matrix to compute the weight of X bag . Thus, the computational complexity of SA is O(nm) instead of O n 2 , and the k is much less than n. Specifically, for a single clustering prototype p k ∈ P, the SA layer scores the pairwise similarity between p k and x n for all x n ∈ X, which can be written as a row vector [a k1 , a k2 , a k3 , . . . , a kn ] in A map . These attention scores are then weighted to X bag to update the p k ∈ R 1×d k for completing the calibration of the clustering prototypes P ∈ R k×d k . As mentioned above, existing clustering-based MIL methods use the K-means clustering to identify instances prototypes in the bag, where the most important instances that contain the key semantic information may be ignored. On the contrary, our PT can efficiently use all the instances to update the cluster prototypes multiple times. Therefore, the combination of bag instances is no longer static and fixed, but diverse and dynamic. It means that different new bags can be fed into the MFFM each time. In addition, by applying PT to each scale, the number of cluster prototypes obtained at different scales is consistent, so there is no need for additional operations to align multi-scale features. Multi-scale Feature Fusion Module (MFFM). To fuse the output clustered prototypes at different scales in MSPT, we proposed an MFFM, which consists of an MLP-Mixer and a Gated Attention Pooling (GAP). The MLP-Mixer is used to enhance the information communication of the prototype representation, and the GAP is used to get the WSI-level representation for WSI classification. As shown in Fig.  We first perform the feature concatenation operation on the multi-scale output clustering prototypes P20× , P10× , P5× to construct a feature pyramid P: where d k is the feature vector dimension of the prototypes. Then, the P is fed to the MLP-Mixer to obtain the corresponding hidden feature representation H ∈ R k×3d k as follows: where LN denotes the layer normalization, σ denotes the activation function implemented by GELU, k are the weight matrices of MLP layers.c and d s are tunable hidden widths in the token-mixing and channel-mixing MLP, respectively. Finally, the H is fed to the gated attention pooling (GAP)  where Y ∈ R 1×d out is the class label probability of the bag, and d out is the number of classes.",5
2855,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol5),,Stage-I: It consists of two modules: Local Contrastive Ordinal Learning (LCOL),"and Global Contrastive Ordinal Learning (GCOL). In these modules, we train the global and local encoder-projector networks individually in an end-to-end manner to extract contrastive feature embeddings.",5
2856,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol5),,Local Contrastive Ordinal Learning:,"In practice, to quantify AAC, clinicians focus on the aortic region adjacent to lumbar vertebrae L1-L4. Following this, in the Localized feature-based Contrastive Ordinal Learning module, LCOL, we integrate a simple yet effective localized attention block with the baseline encoder E l to roughly localize the aorta's position using only regression labels. This attention block is attached with E l after extracting the deep feature map f m from the last convolution layer. Our localized attention block consists of two 2D convolutional layers, followed by batch normalization and ReLu activation layers. This set of layers is then followed by an average pooling layer and sigmoid activation to create an activation map f s for the most salient features in the given image. Multiplying this activation map f s with the initial feature map f m results in extracting the most significant features from f m . These features are then projected into the latent space for processing by our SCOL loss. SCOL encourages the local contrastive embeddings with the same AAC score to move closer and the dissimilar ones apart based on the distance between their labels.",5
2857,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol5),,Global Contrastive Ordinal Learning: In the Global Contrastive Ordinal,"Learning module, we extract the global representation of a given VFA DXA scan. In encoder E g , we replace the fully connected layers of the vanilla CNN model with a global average pooling (GAP ) layer for feature extraction. These feature embeddings are then passed to the projection network P g . SCOL operates on projected embeddings extracted from the whole lumbar region to maximize the feature separability while preserving the ordinal information in latent space. Both projector networks, P l and P g , consist of two Dense layers with 1280 and 128 neurons each, followed by ReLu activation.",5
2858,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol5),,Stage-II:,"In AAC-24 score regression, a small change in pixel-level information can move the patient from low to moderate or moderate to high-risk AAC class. Thus, to decrease the effect of intra-class variations and to increase the inter-class separability, we assimilate the features extracted from encoders E l and E g . The resultant feature vector is fed as input to a feed-forward network consisting of two Dense layers with 1280 and 128 neurons each, followed by ReLu activation. Finally, a linear layer predicts the final AAC regression score. This module is trained using root mean squared error loss L rmse calculated as: , where m is the number of samples, y i are actual and y i are predicted AAC scores. The resulting AAC scores are then further classified into three AAC risk classes.",5
2859,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol5),,Dataset and Annotations:,"We conducted experiments on two de-identified clinical datasets acquired using the Hologic 4500A and GE iDXA scanners. Both datasets are manually annotated by clinicians using the AAC-24 scale  The Hologic Dataset  Implementation Details: Each VFA scan in both datasets contains a full view of the thoracolumbar spine. To extract the region of interest (ROI), i.e., the abdominal aorta near the lumbar spine, we crop the upper half of the image, resize it to 300×300 pixels and rescale it between 0 and 1. We apply data augmentations including rotation, shear and translation. We implement all experiments in TensorFlow ",5
2860,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol5),,Evaluation Metrics:,"The performance of the AAC regression score is evaluated in terms of Pearson's correlation, while for AAC risk classification task Accuracy, F1-Score, Sensitivity, Specificity, Negative Predictive Value (NPV) and Positive Predictive Value (PPV) are used in One Vs. Rest (OvR) setting. Baseline: EfficientNet-V2S model trained in regression mode using RMSE loss. Ablation Study: Table  Comparison with the Baseline: In Table ",5
2861,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol5),,Clinical Analysis and Discussion:,"To signify the clinical significance, we estimate the AUCs (Fig. For the iDXA GE Dataset, in the cohort of 1877 patients with clinical followup, 160 experienced a MACE event. The AUCs of predicted AAC-24 scores were similar AUC to human AAC-24 (0.64 95%CI 0.60-0.69 vs. 0.63 95%CI 0.59-0.68). The predicted AAC groups had 877 (46.7%), 468 (24.9%), and 532 (28.3%) of people in the low, moderate, and high AAC groups, respectively, with MACE events occurring in 5.1%, 7.5%, and 15.0% of these groups, respectively. The age and sex-adjusted HR for MACE in the moderate AAC group was 1.21 95%CI 0.77-1.89, and 2.64 95% CI 1.80-3.86 for the high AAC group, compared to the low predicted AAC group, which were similar to the HRs of human AAC groups, i.e., for moderate and high AAC groups HR 1.15 95%CI 0.72-1.84 and 2.32 95% CI 1.59-3.38, respectively, compared to the human low AAC group.",5
2866,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness (vol5),,Temporal Projection Attention (TPA). Given a feature,"16 after four down-sampling operations in encoder, its original 3D feature map is projected  16 , and we use global average pooling (GAP) and global maximum pooling (GMP) as temporal projection operations. Here, 16 is obtained by a single convolution. This operation can also filter out the irrelevant background and display the key information of the lesions. After the temporal projection, a group convolution with a group size of 4 is employed on K to extract the local temporal attention L ∈ R C× H 16 × W 16 . Then, we concatenate L with Q to further obtain the global attention G ∈ R C×1× H 16 × W 16 by two consecutive 1 × 1 2D convolutions and dimension expend. Those operations are described as follows: where Gonv(•) is the group convolution, σ denotes the normalization, ""⊕"" is the concatenation operation. The global attention G encodes not only the contextual information within isolated query-key pairs but also the attention inside the keys  16 to enhance the feature representation. Meanwhile, to make better use of the channel information, we use 3 16 . Then, we use parallel average pooling and full connection operation to reweight the channel information of F 4th to obtain the reweighted feature The obtained global temporal fusion attention maps M are fused with the reweighted feature F 4th to get the output features F fin . Finally, we input F fin into the decoder of the TLAR to acquire the feature map of lesion.",5
2868,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness (vol5),,Sigmoid Alpha Function (SAF).,"It is generally believed that the differentiation between benign and malignant thyroid nodules is related to the pixels around the boundaries of the lesion  where α is the conversion factor for generating initial probability distribution P D (when α → ∞, the generated P D is binary mask); C is used to control the function value within the range of [0, 1]; (i, j) is the coordinate point in feature map; D (i, j) indicates the shortest distance from (i, j) to lesion's boundaries. Iterative Probabilistic Optimization (IPO) Unit. Based on the fact that IncepText  where ""⊕"" represents the concatenation operation;ConvBlock consists of a group of asymmetric convolutions (e.g., Conv1 × 5, Conv5 × 1 and Conv1 × 1); n denotes the number of the layers of IPO unit. With the lesion's feature map f 0 from the TLAR module, the initial distribution P D obtained by SAF is fed into the first optimize layer of IPO unit to produce the first optimized probability map p1 . Then, p1 is contacted with f 0 , and used to generate optimized probability map p2 through the continuous operation based on SAF and the second optimize layer of IPO unit. The optimized probability map pi-1 provides prior information for producing the next probability map pi . In this way, we can get a group of probability map P to aware the microvascular infiltration.",5
2894,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer (vol5),,TIL density (DenTIL):,"For every patient, multiple density measures including the number of different cells types and their ratios are calculated ",5
2895,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer (vol5),,"GG: A Delaunay triangulation, a Voronoi diagram, and a Minimum Spanning",Tree were constructed ,5
2896,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer (vol5),,CCG:,"For every patient, subgraphs are built on nuclei regardless of their type and only based on their Euclidean distance. Local graph metrics (e.g. clustering coefficient) ",5
2897,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer (vol5),,GNN:,A recent study ,5
2899,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer (vol5),,Results:,"The two top predictive TriAnGIL features were found to be the number of edges between stroma and CD4+ cells, and the number of edges between stroma and tumor cells with more interactions between stromal cells and both CD4+ and tumor cells being associated with response to IO. This finding is concordant with other studies  Result: Figure ",5
2921,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model (vol5),,Algorithm 1:,"Training procedure 1: Input: Input image pairs where is the structure image and is the corresponding dose distribution map, the total number of diffusion steps 2: Initialize: Randomly initialize the noise predictor and pre-trained structure encoder 3: Repeat 4: Sample 5: Sample 6: Perform the gradient step on Equation (9) 7: until converged",5
2932,Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 40.,5
2935,Whole-Heart Reconstruction with Explicit Topology Integrated Learning (vol5),,Surface Loss.,"As in  ), where x and y are the centers of two faces and σ W is a scale controlling parameter that controls the affecting scale between the two faces. Therefore, the surface loss can be expressed as where t 1 , t 2 , t and p 1 , p 2 , p respectively index faces on the reconstructed surfaces S R and those on the corresponding ground truth surfaces S T . L surf ace not only considers each face on the surfaces but also its corresponding direction. When the reconstructed surfaces are exactly the same as the ground truth, the surface loss L surf ace should be 0. Otherwise, L surf ace is a bounded positive value  Minimizing L surf ace enforces the reconstructed surfaces to be progressively close to the ground truth as the training procedure develops. Figure  Loss Function. In addition to the surface loss we introduce above, we also involve two segmentation losses L BCE and L Dice , one point cloud loss L CD , and three regularization losses L laplace , L edge , and L normal that comply with  where w s is the weight for the segmentation loss, and w 1 , w 2 , w 3 and w 4 are respectively the weights for the surface loss, the Chamfer distance, the Laplace loss, and the edge loss. The geometric mean is adopted to combine the five individual mesh losses to accommodate their different magnitudes. L seg ensures useful feature learning of the ROIs. L surf ace enforces the integrity of the reconstructed meshes and makes them topologically similar to the ground truth. L CD makes the point cloud representation of the reconstructed meshes to be close to that of the ground truth. Additionally, L laplace , L edge , and L normal are employed for the smoothness consideration of the reconstructed meshes.",5
2938,Whole-Heart Reconstruction with Explicit Topology Integrated Learning (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 11.,5
2943,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning (vol5),,(III).,"The message passing between the anchors and patches is achieved by a bidirectional cross-attention between the patches and anchors. First, the anchors collect the local information from the patches, which is formulated as where W l ∈ R d f ×de , l = q, k, v are learnable parameters with d e denoting the dimension of the head output, σ represents the softmax function, and ϕ d and ϕ p are the embedding functions that respectively take the distance and polar angle as input and output the corresponding trainable embedding values. Symmetrically, each patch token catches the information of all anchors into their own local representations by the equations The two-way communication makes the patches and anchors timely transmit local information and perceive the dynamic change of global information. The embedding of relative distance and polar angle information helps the model maintain the semantic and structural integrity of the WSI and meanwhile prevents the WSI representation from collapsing to the local area throughout the training process. In terms of efficiency, the computational complexity of self-attention is O(n p 2 ) where n p is the number of patch tokens. In contrast, our proposed PACA's complexity is O(n k × n p ) where n k is the number of anchors. Notice that n k << n p , the complexity is close to O(n p ), i.e. linear correlation with the size of the WSI.",5
2946,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning (vol5),,end end end,"TCGA-Lung dataset is collected from The Cancer Genome Atlas (TCGA) Data Portal. The dataset includes a total of 3,064 WSIs, which consist of three categories, namely Tumor-free (Normal), Lung Adenocarcinoma (LUAD), and Lung Squamous Cancer (LUSC), Endometrial dataset includes 3,654 WSIs of endometrial pathology, which includes 8 categories, namely Well/Moderately/Low-differentiated endometrioid adenocarcinoma, Squamous differentiation carcinoma, Plasmacytoid carcinoma, Clear cell carcinoma, Mixed-cell adenocarcinoma, and benign tumor. Each dataset was randomly divided into training, validation and test sets according to 6:1:3 while keeping each category of data proportionally. We conducted WSI multi-type classification experiments on the two datasets. The validation set was used to perform an early stop. The results of the test set were reported for comparison.",5
2952,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 69.,5
2955,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images (vol5),,Two-Stage Pipeline with Attention Guided Selection.,"The overview of our two-stage pipeline is shown in Fig.  To complete sample-level classification, both stages share basically the same network architecture. The input images are first processed by a CNN encoder to extract features. Then, we propose the pooling transformer, which is modified from the basic transformer module in Sect. 2, to integrate these features for WSI classification. Additionally, the input images for both stages are 256 × 256. In the coarse-grained stage, in order to allow the model to examine as many local images as possible, we resize the cropped local images from 1024 × 1024 to 256 × 256. In the fine-grained stage, we enlarge suspicious local abnormality and thus crop input images to 256 × 256 from 1024 × 1024. For the coarse-grained stage, after passing the resized local images through encoder and pooling transformer, we obtain a rough prediction result at the sample level. We then use the Cross-Entropy (CE) loss to minimize the difference between the predicted WSI label and the ground truth. In addition, we calculate the attention score to identify the local image inputs that are most likely to yield positive reading. We describe the attention score as where x 0 represents classification token (which is a commonly used setting in transformer  Next, in the fine-grained stage, each local image that has passed attention guided selection is cropped into 16 patches of the size 256 × 256. We expect that those patches contain positive cells and are thus critical to diagnosis at the sample level. The network of the fine-grained stage is the same as that of the coarse-grained stage, but the weights of the encoder is pre-trained in an unsupervised manner (Sect. 2). The same CE loss supervised by sample-level ground truth is used for the fine-grained stage here. For inference, the output of the fine-grained stage will be treated as the final result of the test WSI. Pooling Transformer. We use a transformer network to aggregate features of multiple inputs and to derive the sample-level outcome in both coarse-grained and fine-grained stages. We have observed that different local images of the same sample often have patterns of grouped similarity (such as the first two images in the upper-right of Fig.  Therefore, inspired by ",5
2956,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images (vol5),,Contrastive Pre-training of Encoder.,"To make full use of WSI data and provide a better feature encoder, inspired by MoCo  Specifically, in the same training batch, a patch (256 × 256, the same to the input size of the fine-grained stage) and its augmented patch are treated as a positive pair (note that here ""positive/negative"" is defined in the context of contrastive learning), and their features are required to be as similar as possible. Meanwhile, their features are required to be as dissimilar as possible from those of other patches. So the loss function can be described as f i and f ia represent the positive pair, and f j represents another patch negatively paired with f i . Using this method, we can pre-train a feature encoder in an unsupervised manner and initialize it into our encoder for the fine-grained stage.",5
2958,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images (vol5),,Sample Numbers and Inference Time.,"In order to further demonstrate the huge potential of our method, we also perform an ablation study on the number of samples used for training and compare the time consuming of the different methods. For the experiment of sample numbers, We compare the best fully supervised detection-based method (Retinanet+GAT ",5
2965,Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning (vol5),,"S(e i , e top","where m denotes the channel of the feature embeddings. Since the labels from lay annotators might be noisy and erroneous, the W and S are applied in following Eq. ( ",5
2967,Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning (vol5),,Morphology-Molecular Multi-modality Registration.,"The slide-level global translation from Map3D was deployed at a 5× magnification, which is 2 µm per pixel. The 4096×4096 pixels PAS image regions with 1024 pixels overlapping were tiled on anatomical WSIs at a 20× magnification, which is 0.5 µm per pixel. Molecular-Empowered Annotation. The automatic tuft segmentation and molecular knowledge images assisted the lay annotators with identifying glomeruli and cells. ImageJ (version v1.53t) was used throughout the entire annotation process. ""Synchronize Windows"" was used to display cursors across the modalities with spatial correlations for annotation. ""ROI Manager"" was used to store all of the cell binary masks for each cell type. Molecular-Oriented Corrective Learning. Patches were randomly split into training, validation, and testing sets -with a ratio of 6:1:3, respectively -at the WSI level. The distribution of injured glomeruli and normal glomeruli were balanced in the split. Experimental Setting. 2 experienced pathologists and 3 lay annotators without any specialized knowledge were included in the experiment. All anatomical and molecular patches of glomerular structures are extracted from WSI on a workstation equipped with a 12-core Intel Xeon W-2265 Processor, and NVIDIA RTXA6000 GPU. An 8-core AMD Ryzen 7 5800X Processor workstation with XP-PEN Artist 15.6 Pro Wacom is used for drawing the contour of each cell. Annotating 1 cell type on 1 WSI requires 9 h, while staining and scanning 24 IF WSIs (as a batch) requires 3 h. The experimental setup for the 2 experts and the 3 lay annotators is kept strictly the same to ensure a fair comparison. Evaluation Metrics. 100 patches from the testing set with a balanced number of injuries and normal glomeruli were captured by the pathologists for evaluating morphology-based annotation and molecuar-informed annotation. The annotation from one pathologist (over 20 years' experience) with both anatomical and molecular images as gold standard (Fig. ",5
2978,Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 38.,5
2992,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images (vol5),,Data and Evaluation Metrics:,"We evaluated the effectiveness of the proposed model on two datasets: the GlaS dataset and the CRAG dataset. The GlaS dataset comprises 85 training and 80 testing images, divided into 60 images in Test A and 20 images in Test B. The CRAG dataset consists of 173 training and 40 testing images. We have adopted Vahadane method for stain normalization  We assessed the segmentation results using three metrics from the GlaS Challenge: (1) Object F1, which measures the accuracy of detecting individual glands, (2) Object Dice, which evaluates the volume-based accuracy of gland segmentation, and (3) Object Hausdorff, which assesses the shape similarity between the segmentation result and the ground truth. We assigned each method three ranking numbers based on these metrics and computed their sum to determine the final ranking for each method's overall performance.",5
2993,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images (vol5),,Implementation Details:,"In our experiments, we choose the ResNet-50 with FPN as the backbone in the proposed method. The backbone is pretrained on ImageNet. Image decoder, Mask Branch and Mask FCN Head are trained end-toend. We trained on the GlaS and CRAG datasets in a Python 3.8.3 environment on Ubuntu 18.04, using PyTorch 1.10 and CUDA 11.4. During training, we utilized an SGD optimizer with a learning rate of 2.5 × 10 -5 and the weight decay as 10 -4 . We set diffusion timesteps T = 1000 and chose a linear schedule from β 1 = 10 -4 to β T = 0.02. Training was performed on A100 GPU with a batch size of 2.",5
2994,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images (vol5),,Results on the GlaS Challenge Dataset:,"We conducted experiments to evaluate the performance of our proposed model by comparing it with the DSE model  Our proposed model demonstrated a enhancement in performance, surpassing the second-best method on both Test A and Test B datasets. Specifically, on Test A, we observed an improvement of 0.006, 0.01, and 1.793 in Object F1, Object Dice, and Object Hausdorf. Similarly, on Test B, resulting in an improvement of 0.022, 0.014 and 3.694 in Object F1, Object Dice, and Object Hausdorf, respectively. Although Test B presented a more challenging task due to the presence of complex morphology in the images, our proposed model demonstrated accurate segmentation in all cases. The experimental results highlighted the effectiveness of our approach in improving the accuracy of gland instance segmentation.",5
2995,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images (vol5),,Results on the CRAG Dataset:,"The proposed model was additionally evaluated on the CRAG dataset by comparing it against the GCSBA-Net, DoubleU-Net, DSE model, MILD-Net, and DCAN. The average performance of these models is shown in Table  Ablation Studies: Our network utilizes the Mask Branch and Conditional Encoding to enhance performance and segmentation quality. Ablation studies on the GlaS and CRAG datasets confirm the effectiveness of these modules (Table ",5
3002,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 61.,5
3006,Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels (vol5),,Gradient Conformity-Based Selection (GCS).,"The GCS aims to distinguish clean samples from noisy ones by exploring their optimization dynamics in the gradient space. Since training samples from the same class usually exhibit similar optimization dynamics  where f (x i ) is the feature vector of the sample x i , and x j denotes the K-Nearest Neighbors (KNN) of x i . p (x i ) is the probability of the most likely true class predicted by its KNN neighbors. Therefore, the gradient g(x i ) is very likely to reflect the true class information and optimization dynamics of x i . For each class, we select α% samples with the smallest loss as an anchor set A c , which is depicted in the shaded areas of the GCS in Fig.  which is the average gradient of all samples in the anchor set A c of the c-th class. Then, we can measure the similarity between the gradient of the sample x i and the principal gradient of class y i with the cosine similarity s g (x i ) = cos < g(x i ), g yi >. For the sample x i , if y i is a noisy label, g(x i ) should be consistent with the principal gradient of its true class and diverge from g yi , thus yielding small s g (x i ). By fitting Gaussian mixture models (GMM) on the similarity score s g (x i ), we can get c g (x i ) = GM M (s g (x i )), which represents the clean probability of the sample x i decided by the GCS criterion. To the best of our knowledge, this is the first work that explores gradient conformity for clean data selection. Feature Conformity-Based Selection (FCS). Since feature space is more robust to noisy labels than the output space  where f (x i ) denotes the feature vector of the sample x i in the anchor set A c of the c-th class. Then, we perform eigen-decomposition on the gram matrix: , where U c is the eigenvector matrix and Σ c is a diagonal matrix composed of eigenvalues. The principal eigenvector u c of U c is utilized to represent the distribution and contextual information of the c-th class. Then, for each sample x i , we measure its label quality based on the conformity of its feature f (x i ) with the principal eigenvector u yi of its given label: s f (x i ) = cos < f (x i ), u yi >. Samples that better align with the principal eigenvectors of their labeled class are more likely to be clean. According to the FCS criterion, the clean probability of the sample x i is obtained by Compared with existing methods that utilize classwise average features to represent contextual information  Integration of GCS and FCS. Finally, we average the clean probabilities estimated by the GCS and FCS criteria to identify clean data. As shown in Fig. ",5
3011,Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_8.,5
3025,ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography (vol5),,Method,"Ablation Study. We assessed the effect of removing distinct components of our design: uncertainty prototypes (L abs , p u k ), clustering and separation (L clst , L sep ), and push mechanism. As shown in Table ",5
3028,ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 36.,5
3034,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis (vol5),,Patch-Level Block.,"Given the patch-level feature set P = N i=1 P i , the PL block learns long-term relationships within the patch level: where l = 1, 2, ..., L is the index of the HIViT block. P L(•) includes a Separable Self Attention (SSA)  Bidirectional Interaction Block. We propose a Bidirectional Interaction (BI) block to establish communication between different levels within the WSI pyramids. The BI block performs bidirectional interaction, and the interaction progress from region nodes to patch nodes is: where the SE(•) means the Sequeeze-and-Excite layer  where the MEAN(•) is the operation to get the mean value of patch nodes set P l+1 i associated with the i-th region node and P l+1 1 ∈ R 1×C and the C is the feature channel of nodes, and Rl+1 is the region nodes set after interaction. Region-Level Block. The final part of this module is to learn the long-range correlations of the interacted region-level nodes: where l = 1, 2, ..., L is the index of the HIViT module, and RL(•) has a similar structure to P L(•).",5
3038,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 73.,5
3050,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 30.,5
3054,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans (vol5),,MPBD-LSTM.,"The most basic building block in MPBD-LSTM is the 3D-LSTM modules. Each 3D-LSTM module is composed of two E3D-LSTM cells  where -→ h v,t and ←h v,t are the output hidden state of the forward pass and backward pass of phase v at timestamp t, and σ is the function which is used to combine these two outputs, which we choose to use a summation function to get the summation product of these two hidden states. Therefore, the output of the bi-directional LSTM module presented in Fig.  in which ⊕ stands for summation. After this, the output y v,t0 is passed into the bi-directional LSTM module in the next layer and viewed as input for this module. Figure  An alternative approach is to additionally connect two planes by combining the hidden states of 3D-LSTM modules and taking their average if a module receives two inputs. However, we found that such design actually resulted in a worse performance. This issue will be demonstrated and discussed later in the ablation study. In summary, the MPBD-LSTM model comprises two planes, each of which contains three 3D-LSTM stacks with two modules in each stack. It modifies E3D-LSTM by using bi-directional connected LSTMs to enhance communication between different timestamps, and a multi-plane structure to simultaneously process multiple phases.",5
3059,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 37.,5
3069,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology (vol5),,Cellular Explainability Method. The cellular average embedding is,"where e ij ∈ R 256 is the cellular embedding extracted from every detected cell in the tile j i ∈ 1, 2, . . . , N j where N j is the number of cells in the tile j. This can be rewritten as a weighted average of the cellular embeddings where w i ∈ R 256 are the per cell attention weights that if initialized to 0 result in the original cellular average embedding. The re-formulation does not change the result of the forward pass since w i are not all equal. Note that the weights are not learned through training but calculated per cell at inference time to get the per cell contribution. We computed the gradient of the output category (of the classification method applied on top of the computed embedding) with respect to the attention weights w i : grad i = ∂Score i /∂w i and visualized cells that received positive and negative gradients using different colors. Visual Example Results. Examples of our cellular explainability method applied to weakly supervised tumor detection on WSIs from the CAMELYON16 data set using A-MIL are shown in Fig.  In this case, cells with positive attention gradients that shifted the output towards a classification of GCB were labeled green and cells with negative attention gradients that shifted the classification towards ABC were labeled red. Cells with positive attention gradients were mostly smaller lymphoid cells with low grade morphology or were normal lymphocytes, whereas cells with negative attention gradients were more frequently larger lymphoid cells with high grade morphology (Fig. ",5
3089,Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model (vol5),,Ablation Study:,"We conducted an additional study to observe the effects of the proposed multiresolution heatmap learning and HTC. For this study, we compared the HTC with Conformer ",5
3091,Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model (vol5),,Acknowledgements,. This research was partly supported by the ,5
3094,Transformer-Based End-to-End Classification of Variable-Length Volumetric Data (vol5),,Slice Feature Extractor (SFE).,"To obtain the slice representations, we use ViT as our SFE due to its recent success in medical interpretation tasks  Volume Feature Aggregator (VFA). The output of the previous step is a sequence of slice-wise embeddings, to which we append a learnable volumelevel classification token  Volume Classification. Finally, the volume-level classification token is fed to a Fully Connected (FC) layer, which produces individual class scores. As a loss function, we employ the weighted cross-entropy.",5
3112,Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation (vol5),,Comparison and Ablation Study:,The effectiveness of the proposed algorithm is demonstrated in comparison with state-of-the-art methods and an ablation study. The Fourior-based DG methods FACT  (1) Comparison. Quantified comparison of our algorithm with the competing methods is summarized in Table  Visualized comparison is shown in Fig.  (2) Ablation Study. According to Table ,5
3123,Vision Transformer Based Multi-class Lesion Detection in IVOCT (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 32.,5
3132,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_47.,5
3136,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark (vol5),,Method Dice IoU,"Fully-supervised Mask R-CNN  Semi-supervised PseudoSeg  Compared Methods and Evaluation Metrics. We compared TCSegNet with the fully-supervised counterparts, including method specific for segmentation in general image ",5
3146,Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models (vol5),,Self-training and YOLOX.,"The box optimization process of the self-training stage is recorded, and the corresponding results are presented in the supplement. YOLOX and self-training are not the essential reasons for the superior performance of our method. The true key is the utilization of semantic-information-rich VLPMs. To illustrate this point, we employed another commonly used unsupervised detection method, superpixels ",5
3148,Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 67.,5
3158,A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 14.,5
3162,Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy (vol5),,ln(f (x)) = ln(,"Note that the last equation is in polynomial form ln(y) = ax 2 + bx + c, with c = ln(A) --μ 2 2σ 2 , b = μ σ 2 and a = -1 2σ 2 . By defining the error function δ = ln(f (x)) -(ax 2 + bx + c) and differentiating the sum of residuals gives a linear system of equations (Eq. 2). ⎡ After solving the linear system, we can finally retrieve μ, σ and A with μ = -b/2c, σ = -1/2c, and A = e a-b 2 /4c .",5
3163,Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy (vol5),,Guos Algorithm.,"Due to the logarithmic nature of Caruanas algorithm, it is very susceptible towards outliers. Guo ",5
3164,Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy (vol5),,= y[ln(y,"The parameters of the Gaussian function μ, σ and A can be calculated similar to Caruanas algorithm. Recall that polynomial regression has an analytical solution, where the vector of estimated polynomial regression coefficients is β = (X T X) -1 X T ŷ. This formulation is easily parallelizable on a GPU and fully differentiable. Hence, we can efficiently compute the Gaussian coefficients necessary for determining the subpixel position of local maxima. Finally, we can calculate the points position p by simple addition using Eq. 5.",5
3166,Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy (vol5),,HLE++.,"The HLE dataset is a publicly available dataset consisting of 10 labeled in-vivo recordings of human vocal folds during phonation  , where F i is the vocal fold segmentation at frame i and G i the glottal segmentation, respectively. Quantitative and Qualitative Evaluation. Table ",5
3168,Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 4.,5
3172,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis (vol5),,"i=λ2N,j=(1-λ2)N i=1",", which include both the remaining patch tokens and the masked patch tokens. After the process of the mixed attention module, H&E and HER2 patch tokens are fed into the modal-specific decoders respectively to reconstruct the original H&E image X and HER2 image Y . The reconstruction loss is computed by the mean squared error between the original images X, Y and the generative images X , Y , which is computed as We use an adjustable hyperparameter θ to balance the losses of two modalities. The final loss L is defined as",5
3199,Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision (vol5),,PDL1 Results,. We chose to compare our method with unsupervised segmentation methods because existing point-supervised segmentation methods are unable to segment cell membranes. We present their results in Table ,5
3201,Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision (vol5),,Acknowledgements,. This work is supported by the ,5
3202,Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_52.,5
3209,Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair (vol5),,Implementation Details.,"For experiments of our dataset, we manually locate the hip region and crop a 224 × 224 image that is centered on the original hip region whose size is 400 × 600. The learning rate is set as 3e-3 for the endto-end training of the framework with a batch size of 32. We adopt a 10-fold cross-validation and report the average performance of 10 folds. For each fold, we further divide the data (the other 9 folds) into a training set (90%) and a validation set (10%) and take the best model on the validation part for testing. Evaluation Metric. We evaluate our method with Accuracy (Acc), Precision, Recall and F1 score. The Precision and Recall are calculated with one-classversus-all-other-classes and then calculate F1 score F 1 = 2•P recision•Recall P recision+Recall .",5
3223,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 3.,5
3225,Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning (vol5),,Related Work. Considerable progress has been made in segmenting,"With growing research in vision-and-language and contrastive learning  However, these methods are still not suitable for variable-length reports and are inefficient in low-resource settings. Our Contributions. We propose a novel framework for more accurate and efficient computer-aided placenta analysis. Our framework introduces two key enhancements: Pathology Report Feature Recomposition, a first in the medical VLC domain that captures features from pathology reports of variable lengths, and Distributional Feature Recomposition, which provides a more robust, distribution-aware representation. We demonstrate that our approach improves representational power and surpasses previous methods by a significant performance margin, without additional data. Furthermore, we boost training and testing efficiency by eliminating the large language model (LLM) from the training process and incorporating more efficient encoders. To the best of our knowledge, this is the first study to improve both the efficiency and performance of VLC training techniques for placenta analysis.",5
3236,Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 12.,5
3249,Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging (vol5),,Comparison with Traditional Biomarkers. Table 1 shows p-values for,"AutoVT and traditional biomarkers. The proposed biomarker based on the overall loss function L, AutoVT(L), was the most sensitive to the effect of pomegranate with the lowest p-value. This biomarker has learned the volumetric information from the input images, as demonstrated by the correlation coefficient of 0.84 between AutoVT(L) and ΔV W V . AutoVT(L) has also learned the texture information, as demonstrated in Fig.  Comparison with Different Losses. We compared our proposed loss with another two losses, including cross-entropy loss and bi-direction contrastive loss. Cross-entropy loss is expressed as , where σ(•) is a sigmoid function. The bi-direction contrastive loss is a symmetric version of L tcl , expressed as L bd = y max(m + AutoV T, 0) 2 + (1y) max(m -AutoV T, 0) 2 . The margin m in L bd was tuned in the same way as the proposed L, with m = 0.4 being the optimized parameter in all three cross-validation trials. Table ",5
3251,Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 29.,5
3255,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction (vol5),,Results.,In Table ,5
3256,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction (vol5),,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 60. Ablation Analysis. We verify the model efficiency by using fewer amounts of finetuning data in finetuning. For TCGA-COAD dataset, we include 50%, 25%, and 10% of the finetuning data. For the TCGA-READ dataset, as the number of uncensored patients is limited, we use 75%, 50%, and 25% of the finetuning data to allow at least one uncensored patient to be included for finetuning. As shown in Fig. ",5
3296,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble (vol5),,Datasets. Evaluations of StainDiff are conducted on two datasets. (1),Dataset-A: MITOS-ATYPIA 14 Challenge,5
3297,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble (vol5),,Implementations.,"All experiments are implemented in Python 3.8.13 with Pytorch 1.12.1 on two NVIDIA GeForce RTX 3090 GPU cards with 24GiB of memory each in parallel. We leverage the Adam optimizer with a learning rate of 2 × 10 -4 , and a batch size of 4. The learning scheme follows previous work ",5
3307,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors (vol5),,Source Graph Construction,"Here, given patches as nodes V , we first calculated the pairwise distance among different nodes, and select the top 10 percent connections with the smallest distance values as edges E. For each node in V , we followed the study in ",5
3308,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors (vol5),,Calculating TILs-Tumor Interaction via Graph Attention Networks(GATs).,"To characterize the interaction between different TILs and tumor patches, we employed GAT  we calculated the attention coefficients among different nodes, which can be formulated as: Furthermore, a softmax function was then adopted to normalize the attention coefficients e ij : where N i represents all neighbors of node i. The new feature vector v i for node i was calculated via a weighted sum: Finally, the output features of each GAT layer were aggregated in the readout layer. We fed the generated output features from each readout layer into the Cox hazard proportional regression model for the final prognosis predictions. Feature Alignment. In the proposed GAT-based transfer learning framework, the feature alignment component was employed on its first two layers. Then, for the node embeddings with different types (TILs and Tumor) in both the source and target domain, we performed a mean pooling operation to obtain their aggregated features. Next, we aligned the aggregated tumor or TILs features from the two domains separately using Maximum Mean Discrepancy(MMD)  Here, we adopted MMD for feature alignment due to its ability to measure the distance between two distributions without explicit assumptions on the data distribution, we showed the objective function of MMD in our method as follows: where H is a Hilbert space, f represents the features from the source, f represents the feature from the target, r represents the layer number, k ∈ {L, T } referred to TILs or tumor node. In addition, n denotes the number of source samples, while m refers to the number of target samples.  TILs-Tumor Interaction Alignment. To accurately characterize the interaction between TILs and tumors, we further analyzed the extracted interaction weights by dividing them into 10 intervals (i.e., bins). For each interval, we calculated the sum of all source domain interaction weights as i s k and the sum of all target domain interaction weights as i t k , where k represents the k-th interval. Consequently, we obtained two vectors and applied softmax on each of them for normalization that can be denoted as . In order to measure the dissimilarity between p i and q i , the Kullback-Leibler (KL) divergence is adapted on the third layer of GAT, which can be formulated as: According to Eq.( ",5
3309,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors (vol5),,Prognosis Prediction by the Cox Proportional Hazard Model.,"The Cox proportional hazard model was applied to predict the patients' clinical outcome  where x i represents the output of the last layer for the prognosis task and R (t i ) is the risk set at time t i , which represents the set of patients that are still under risk before time t. In addition, δ i is an indicator variable. Sample i refers to censored patient if Overall Objective. To achieve domain-adaptive prognosis prediction, the final loss function included the Cox loss, FA loss, and TTIA loss as the following formula: where α and β represent the weights assigned to the importance of FA component and TTIA component respectively.",5
3318,Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information (vol5),,Non-local Module:,"Given a hip X-ray image X and the corresponding CAM map X m , we apply the backbone of ResNet18 to extract the high-level feature maps x ∈ R C×H ×W . The feature maps are then treated as inputs for the non-local module. For output xi from position index i, we have where concat denotes concatenation, w f is a weight vector that projects the concatenated vector to a scalar, ReLU is the ReLU function, a ij denotes the non-local feature attention that represents correlations between the features at two locations (i.e., x i and x j ), θ, φ, and g are mapping functions as shown in Fig. ",5
3320,Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information (vol5),,Visual-Text Fusion Module:,"In order to learn from clinical data, we first encode 5 numerical variables as a vector and send it to TextNet. As shown in Fig.  and W v are part of the model parameters to be learned. We compute the visual-text self-attentive feature ẑvt i at position i as The softmax operation indicates the attention across each visual and text pair in the multi-modality feature.",5
3335,Gene-Induced Multimodal Pre-training for Image-Omic Classification (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_49.,5
3344,Histopathology Image Classification Using Deep Manifold Contrastive Learning (vol5),,Acknowledgements,. This study was approved by the institutional review board of ,5
3357,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction (vol5),,Loss Function and Training Strategy.,"For the network training, Cox loss  where δ i denote the censorship of i-th patient, O(i) and O(j) denote the survival output of i-th and j-th patient in a batch, respectively. GPUs. Our graph convolutional model is implemented by Pytorch Geometric ",5
3367,TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction (vol5),,Cross-Modal Attention,"where four independent 1×1 convolutions α, β, θ, and γ are used to map image and fusion features to the space for cross-modal attention. At first, M and F are passed into θ(•), β(•) and α(•) for channel transformation, respectively. Following the transformed feature, M θ and F β perform multiplication via a Softmax activation function to take a cross-attention with F α . In the end, the output feature of the cross-attention via γ(•) for feature smoothing is added with F. Given the above operations, the cross-modal features O et and O dt are obtained and passed to the next layer. Knowledge Alignment Module. The above operations only consider the interaction between features in two modalities. A problem is that text embeddings from pre-trained text encoder of CLIP are not well aligned with the image features initialized by ImageNet pre-training. This knowledge shift potentially weakens the proposed cross-modal interaction to assist the prediction of implant position. To mitigate this problem, we propose the knowledge alignment module (KAM) to gradually align image features to the feature space of pre-trained CLIP. Motivated by knowledge distillation  where m e 4 ∈ R 1×D is the transformed feature of M e 4 after attention pooling operation ",5
3372,TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction (vol5),,Comparison to the Mainstream Detectors.,"To demonstrate the superior performance of the proposed TCEIP, we compare the AP value with the mainstream detectors in Table ",5
3374,TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction (vol5),,Acknowledgments,. This work was supported by the ,5
3375,TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 31.,5
3387,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_1.,5
3394,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification (vol5),,Ours,"(w/ AB-MIL) 90.0 firstly embedded into a 1024-dimension vector, and then be projected to a 512dimension hidden space for further bag-level training. For the training of bag classifier f (•), we used an initial learning rate of 2e-4 with Adam ",5
3397,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification (vol5),,Acknowledgements,. This work was supported by the ,5
3398,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 45.,5
3407,TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_41.,5
3418,STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models (vol5),,Radiomic Feature Extraction:,"In first stage, radiomic feature R(I N ) is extracted on each I N of the LVW for each phase (frame) of the echo video, V as given in ",5
3419,STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models (vol5),,Time Series Feature Extraction:,"In the second stage, to model the temporal LVW motion, we consider the sequence of radiomic features from each phase of one heartbeat cycle as individual time-series. Thus, a radiomic feature timeseries t R (V ) is given by (2). Time-series feature T R , as given by (3), is extracted for each radiomic feature time-series using the TSFresh library ",5
3420,STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models (vol5),,R(I,"Thus, the radiomic time-series model M R is trained on time-series features T R (V ) obtained for each V of the patient to predict the outcome O T .",5
3421,STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models (vol5),,Video Transformer Model:,The Video Vision Transformer (ViViT) model ,5
3432,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound (vol5),,MSKUS,"Evaluation Metrics. Five metrics were used to evaluate model performance: Accuracy (ACC), Area Under Curve (AUC), Correlation Coefficient (CC), Similarity (SIM) and Kullback-Leibler divergence (KLD)  Evaluation of ""Thinking like Sonographers"" Mechanism. To evaluate the effectiveness of our proposed mechanism of ""Thinking like Sonographers"" (TLS) that combines ""where to adjust"", ""what to adjust"" and ""how to adjust"", we compared the gout diagnosis results of several classic CNN classification ",5
3433,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound (vol5),,Stability Under Different Gaze Maps via t-Test.,"To evaluate the prediction's stability under the predicted gaze map from the generation model in ""Where to adjust"", we conducted three t-test studies. Specifically, we trained two classification models (M C and M P ), using the actual collected gaze maps, and the predicted maps from the generation model, respectively. During the testing, we used the collected maps as input for M C and M P to get classification results R CC and R P C . Similarly, we used the predicted maps as input for M C andM P As shown in Table ",5
3435,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound (vol5),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 16.,5
3438,A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images (vol5),,Cross-Interaction Module.,"A diagnostic process of skin diseases takes multiple visual information into account, which is relatively complicated and difficult to model in an analytical way. Simple fusion operations such as concatenation are insufficient to simulate the diagnostic logic. Thus, partially inspired by  where g B , g D are the class token, p i b , p i d , i ∈ {1, 2, ..., N p } the corresponding patch tokens. GAP and LN denote the global average pooling and layer normalization, respectively. W Q BD , W K BD , W V BD ∈ R F ×F denote learnable parameters. F denotes the dimension of features. g D B is computed from the patch tokens of disease and the class token of body-part in the same fashion. Similarly, we can obtain the fused class tokens (g D A and g A D ) and the fused local class tokens (l D A and l A D ) between attribute and disease. Note that the disease class token g D is replaced by g B D in the later computations, and local class tokens l A and l D in Fig.  Learning and Optimization. We argue that joint training can enhance the feature representation for each task. Thus, we define a multi-task loss as follows:  where N s denotes the number of samples, n x , n h the number of classes for each task, and p ij , y ij the prediction and label, respectively. Notably, body parts and attributes are defined as multi-label classification tasks, optimized with the binary cross-entropy loss, as shown in Eq. 6. The correspondence of x and h is shown in Fig. ",5
3465,DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification (vol6),,Visualization of Our Diffusion Procedure:,"To illustrate the diffusion reverse process guided by our dual-granularity conditional encoding, we used the t-SNE tool to visualize the denoised feature embeddings at consecutive time steps. Figure ",6
3467,DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 10.,6
3472,Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation (vol6),,R-wise Fusion Block (RFB),The conventional feature fusion methods (denoted as F in Table ,6
3474,Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation (vol6),,Implementation Details:,"The intensity values of all GT OSMs are normalized from [0, 63] to [0, 1] for loss calculation. We use unified center coordinates as the polar origin for polar transformation since all OCT samples are aligned to the macular fovea during acquisition. Since the background pixels in predicted OSM are hardly equal to 0, which cause many noise points in reconstructed point cloud, only intensity values that are not equal to OSM  Evaluation Metrics: Since our proposal employs 2D OSM regression in polar coordinates to achieve 3D reconstruction in cartesian coordinates, we evaluate its performance using both 2D and 3D metrics. Specifically, we use Chamfer Distance (CD) to measure the mean distance between predicted point clouds and its GT point cloud, and Structural Similarity Index Measure (SSIM) as 2D metrics to evaluate the OSM regression performance. Additionally, we introduce point density (Density) as an extra metric to evaluate the number ratio of total points between predicted and GT point clouds, defined as Density = |1 -N pred /N GT |, where N pred is the number of predicted point clouds, N GT is the number of GT point clouds. The lower Density, the fewer defects in the valid region of OSMs.",6
3475,Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation (vol6),,Method Comparison and Ablation Study:,"The effectiveness of the key components of PESNet is investigated through a comparison and ablation study, as presented in Fig.  In our PESNet, we adopt unshare-weight for two branches, which produces the best metrics in the last row. Therefore, our proposed algorithm reconstructs the eyeball close to GT considering the specificity of the eyeball.",6
3476,Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation (vol6),,Ablation Study About Loss Function:,"We compare different loss functions applied in our task, and the results as shown in Table  Validation on Disease Sample: Although our POS dataset only contains the data of healthy eyeballs, we still test our PESNet on two disease cases: one is high myopia and the other is Diabetic Macular Edema. As shown in Fig. ",6
3478,Polar Eyeball Shape Net for 3D Posterior Ocular Shape Representation (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 18.,6
3490,ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 74.,6
3493,A One-Class Variational Autoencoder (OCVAE) Cascade for Classifying Atypical Bone Marrow Cell Sub-types (vol6),,Data Use Declaration.,"The complete version of the public database used for validating the proposed strategy is provided by Matek et. al., under the TCIA Data Usage Policy and Restrictions, and it is publicly available at TCIA platform, https://doi.org/10.7937/TCIA.AXH3-T579. No ethical compliance statement is presented in this document since it is covered by the original dataset publication ",6
3507,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation (vol6),,Input:,"The input RGB image I ∈ R H×W ×3 ; The module T whose input and output channel numbers are 1 and 3, respectively; Output: The re-colored image Io ∈ R AssignV alue denotes re-assembling the sorted values according to the provided indices. Details of the module T are included in the supplementary material. Via RC, the original fine-grained structure information from I g is recovered in I r . In this way, the re-colored image is advantageous in two aspects. First, the appearance difference between pathological images caused by the change in scanners and staining protocols is eliminated. Second, the re-colored image preserves fine-grained structure information, enabling precise instance segmentation to be possible.",6
3509,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation (vol6),,Input:,"Original feature maps X ∈ R H×W ×C . The C-dimensional feature vector on its pixel (i, j) is denoted as xij; The modules Eμ and E δ that re-estimate feature statistics; Δsra ∈ R 1×1×C that is obtained via running mean of Δs in the training stage; The momentum factor α used to update Δsra ; (Optional) Δs = f (ρ); Output: Normalized feature maps Y ∈ R H×W ×C ; To verify the above viewpoint, we evaluate the baseline model under different foreground-background ratios. Specifically, we first remove the foreground pixels via in-painting  The above problem is common in nucleus segmentation because pathological images from different organs or tissues tend to have significantly different foreground-background ratios. However, this phenomenon is often ignored in existing research. To handle this problem, we propose the Distribution-Aware Instance Normalization (DAIN) method to re-estimate feature statistics that account for different ratios of foreground and background pixels. Details of DAIN is presented in Algorithm 2. The structures of E μ and E δ are included in the supplemental materials. As shown in Fig.  The training of RPH requires an extra loss term L rph , which is formulated as bellow: where ρ g denotes the ground truth foreground-background ratio, and L BCE and L MSE denote the binary cross entropy loss and the mean squared error, respectively.",6
3514,DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 57.,6
3521,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment (vol6),,(b) Style Transfer:,"This sub-network creates the stylized IHC image using an attention module, given (1) the input hematoxylin and the mIF marker images and (2) the style and its corresponding marker images. For synthetically generating stylized IHC images, we follow the approach outlined in AdaAttN  For the stylized IHC images with ground truth CD3/CD8 marker images, we also segmented corresponding DAPI images using our interactive deep learning ImPartial  We evaluated the effectiveness of our synthetically generated dataset (stylized IHC images and corresponding segmented/classified masks) using our generated dataset with the NuClick training dataset (containing manually segmented CD3/CD8 cells)  We also tested the trained models on 1,500 randomly selected images from the training set of the Lymphocyte Assessment Hackathon (LYSTO) ",6
3525,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment (vol6),,Data use Declaration and Acknowledgment:,"This study is not Human Subjects Research because it was a secondary analysis of results from biological specimens that were not collected for the purpose of the current study and for which the samples were fully anonymized. This work was supported by MSK Cancer Center Support Grant/Core Grant (P30 CA008748) and by James and Esther King Biomedical Research Grant (7JK02) and Moffitt Merit Society Award to C. H. Chung. It is also supported in part by the Moffitt's Total Cancer Care Initiative, Collaborative Data Services, Biostatistics and Bioinformatics, and Tissue Core Facilities at the H. Lee Moffitt Cancer Center and Research Institute, an NCI-designated Comprehensive Cancer Center (P30-CA076292).",6
3526,An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 68.,6
3532,Detection of Basal Cell Carcinoma in Whole Slide Images (vol6),,"Acc(W, d, D","3 Experiments The dataset, comprised of 194 skin slides acquired from the Southern Sun Pathology laboratory, includes 148 BCC cases and 46 other types (common nevus, SCC), all manually annotated by a dermatopathologist. BCC slides served as positive samples and the rest as negatives. These slides were scanned at ×20 magnification with a 0.44 µm pixel size using a Leica Aperio AT2 Scanner. The patient data were separated between training and testing to prevent overlap. Details are shown in Table ",6
3549,IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 54.,6
3553,Multi-scale Prototypical Transformer for Whole Slide Image Classification (vol6),,Prototypical Transformer (PT).,"Most tissues in WSIs are redundancy, and therefore, we introduce the instance prototypes to reduce redundant instances. Specifically, for each instance bag X bag ∈ R n×d k , the K-means clustering algorithm is applied on all instances to get K centers (prototypes). These cluster prototypes can be used as instances to represent a new bag P bag ∈ R k×d k . However, the K-means clustering algorithm is sensitive to the initial selection of cluster centers, i.e. different initializations can lead to different results, and the final result may not be the global optimal solution. It is essential to try different initializations and choose the one with the lowest error. However, the WSI dataset generally has a long sequence of instances, which makes the clustering algorithms computationally expensive and slow down as the size of the bag increases. To solve the issue above, we propose to apply the self-attention (SA) mechanism in Transformer to re-calibrate these cluster prototypes. As shown in Fig.  where W q , W k , W v ∈ R d k ×d k are trainable matrices of query P bag and the key-value pair (X bag , X bag ), respectively, and A map ∈ R k×n is the attention matrix to compute the weight of X bag . Thus, the computational complexity of SA is O(nm) instead of O n 2 , and the k is much less than n. Specifically, for a single clustering prototype p k ∈ P, the SA layer scores the pairwise similarity between p k and x n for all x n ∈ X, which can be written as a row vector [a k1 , a k2 , a k3 , . . . , a kn ] in A map . These attention scores are then weighted to X bag to update the p k ∈ R 1×d k for completing the calibration of the clustering prototypes P ∈ R k×d k . As mentioned above, existing clustering-based MIL methods use the K-means clustering to identify instances prototypes in the bag, where the most important instances that contain the key semantic information may be ignored. On the contrary, our PT can efficiently use all the instances to update the cluster prototypes multiple times. Therefore, the combination of bag instances is no longer static and fixed, but diverse and dynamic. It means that different new bags can be fed into the MFFM each time. In addition, by applying PT to each scale, the number of cluster prototypes obtained at different scales is consistent, so there is no need for additional operations to align multi-scale features. Multi-scale Feature Fusion Module (MFFM). To fuse the output clustered prototypes at different scales in MSPT, we proposed an MFFM, which consists of an MLP-Mixer and a Gated Attention Pooling (GAP). The MLP-Mixer is used to enhance the information communication of the prototype representation, and the GAP is used to get the WSI-level representation for WSI classification. As shown in Fig.  We first perform the feature concatenation operation on the multi-scale output clustering prototypes P20× , P10× , P5× to construct a feature pyramid P: where d k is the feature vector dimension of the prototypes. Then, the P is fed to the MLP-Mixer to obtain the corresponding hidden feature representation H ∈ R k×3d k as follows: where LN denotes the layer normalization, σ denotes the activation function implemented by GELU, k are the weight matrices of MLP layers.c and d s are tunable hidden widths in the token-mixing and channel-mixing MLP, respectively. Finally, the H is fed to the gated attention pooling (GAP)  where Y ∈ R 1×d out is the class label probability of the bag, and d out is the number of classes.",6
3563,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol6),,Stage-I: It consists of two modules: Local Contrastive Ordinal Learning (LCOL),"and Global Contrastive Ordinal Learning (GCOL). In these modules, we train the global and local encoder-projector networks individually in an end-to-end manner to extract contrastive feature embeddings.",6
3564,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol6),,Local Contrastive Ordinal Learning:,"In practice, to quantify AAC, clinicians focus on the aortic region adjacent to lumbar vertebrae L1-L4. Following this, in the Localized feature-based Contrastive Ordinal Learning module, LCOL, we integrate a simple yet effective localized attention block with the baseline encoder E l to roughly localize the aorta's position using only regression labels. This attention block is attached with E l after extracting the deep feature map f m from the last convolution layer. Our localized attention block consists of two 2D convolutional layers, followed by batch normalization and ReLu activation layers. This set of layers is then followed by an average pooling layer and sigmoid activation to create an activation map f s for the most salient features in the given image. Multiplying this activation map f s with the initial feature map f m results in extracting the most significant features from f m . These features are then projected into the latent space for processing by our SCOL loss. SCOL encourages the local contrastive embeddings with the same AAC score to move closer and the dissimilar ones apart based on the distance between their labels.",6
3565,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol6),,Global Contrastive Ordinal Learning: In the Global Contrastive Ordinal,"Learning module, we extract the global representation of a given VFA DXA scan. In encoder E g , we replace the fully connected layers of the vanilla CNN model with a global average pooling (GAP ) layer for feature extraction. These feature embeddings are then passed to the projection network P g . SCOL operates on projected embeddings extracted from the whole lumbar region to maximize the feature separability while preserving the ordinal information in latent space. Both projector networks, P l and P g , consist of two Dense layers with 1280 and 128 neurons each, followed by ReLu activation.",6
3566,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol6),,Stage-II:,"In AAC-24 score regression, a small change in pixel-level information can move the patient from low to moderate or moderate to high-risk AAC class. Thus, to decrease the effect of intra-class variations and to increase the inter-class separability, we assimilate the features extracted from encoders E l and E g . The resultant feature vector is fed as input to a feed-forward network consisting of two Dense layers with 1280 and 128 neurons each, followed by ReLu activation. Finally, a linear layer predicts the final AAC regression score. This module is trained using root mean squared error loss L rmse calculated as: , where m is the number of samples, y i are actual and y i are predicted AAC scores. The resulting AAC scores are then further classified into three AAC risk classes.",6
3567,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol6),,Dataset and Annotations:,"We conducted experiments on two de-identified clinical datasets acquired using the Hologic 4500A and GE iDXA scanners. Both datasets are manually annotated by clinicians using the AAC-24 scale  The Hologic Dataset  Implementation Details: Each VFA scan in both datasets contains a full view of the thoracolumbar spine. To extract the region of interest (ROI), i.e., the abdominal aorta near the lumbar spine, we crop the upper half of the image, resize it to 300×300 pixels and rescale it between 0 and 1. We apply data augmentations including rotation, shear and translation. We implement all experiments in TensorFlow ",6
3568,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol6),,Evaluation Metrics:,"The performance of the AAC regression score is evaluated in terms of Pearson's correlation, while for AAC risk classification task Accuracy, F1-Score, Sensitivity, Specificity, Negative Predictive Value (NPV) and Positive Predictive Value (PPV) are used in One Vs. Rest (OvR) setting. Baseline: EfficientNet-V2S model trained in regression mode using RMSE loss. Ablation Study: Table  Comparison with the Baseline: In Table ",6
3569,SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans (vol6),,Clinical Analysis and Discussion:,"To signify the clinical significance, we estimate the AUCs (Fig. For the iDXA GE Dataset, in the cohort of 1877 patients with clinical followup, 160 experienced a MACE event. The AUCs of predicted AAC-24 scores were similar AUC to human AAC-24 (0.64 95%CI 0.60-0.69 vs. 0.63 95%CI 0.59-0.68). The predicted AAC groups had 877 (46.7%), 468 (24.9%), and 532 (28.3%) of people in the low, moderate, and high AAC groups, respectively, with MACE events occurring in 5.1%, 7.5%, and 15.0% of these groups, respectively. The age and sex-adjusted HR for MACE in the moderate AAC group was 1.21 95%CI 0.77-1.89, and 2.64 95% CI 1.80-3.86 for the high AAC group, compared to the low predicted AAC group, which were similar to the HRs of human AAC groups, i.e., for moderate and high AAC groups HR 1.15 95%CI 0.72-1.84 and 2.32 95% CI 1.59-3.38, respectively, compared to the human low AAC group.",6
3574,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness (vol6),,Temporal Projection Attention (TPA). Given a feature,"16 after four down-sampling operations in encoder, its original 3D feature map is projected  16 , and we use global average pooling (GAP) and global maximum pooling (GMP) as temporal projection operations. Here, 16 is obtained by a single convolution. This operation can also filter out the irrelevant background and display the key information of the lesions. After the temporal projection, a group convolution with a group size of 4 is employed on K to extract the local temporal attention L ∈ R C× H 16 × W 16 . Then, we concatenate L with Q to further obtain the global attention G ∈ R C×1× H 16 × W 16 by two consecutive 1 × 1 2D convolutions and dimension expend. Those operations are described as follows: where Gonv(•) is the group convolution, σ denotes the normalization, ""⊕"" is the concatenation operation. The global attention G encodes not only the contextual information within isolated query-key pairs but also the attention inside the keys  16 to enhance the feature representation. Meanwhile, to make better use of the channel information, we use 3 16 . Then, we use parallel average pooling and full connection operation to reweight the channel information of F 4th to obtain the reweighted feature The obtained global temporal fusion attention maps M are fused with the reweighted feature F 4th to get the output features F fin . Finally, we input F fin into the decoder of the TLAR to acquire the feature map of lesion.",6
3576,Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness (vol6),,Sigmoid Alpha Function (SAF).,"It is generally believed that the differentiation between benign and malignant thyroid nodules is related to the pixels around the boundaries of the lesion  where α is the conversion factor for generating initial probability distribution P D (when α → ∞, the generated P D is binary mask); C is used to control the function value within the range of [0, 1]; (i, j) is the coordinate point in feature map; D (i, j) indicates the shortest distance from (i, j) to lesion's boundaries. Iterative Probabilistic Optimization (IPO) Unit. Based on the fact that IncepText  where ""⊕"" represents the concatenation operation;ConvBlock consists of a group of asymmetric convolutions (e.g., Conv1 × 5, Conv5 × 1 and Conv1 × 1); n denotes the number of the layers of IPO unit. With the lesion's feature map f 0 from the TLAR module, the initial distribution P D obtained by SAF is fed into the first optimize layer of IPO unit to produce the first optimized probability map p1 . Then, p1 is contacted with f 0 , and used to generate optimized probability map p2 through the continuous operation based on SAF and the second optimize layer of IPO unit. The optimized probability map pi-1 provides prior information for producing the next probability map pi . In this way, we can get a group of probability map P to aware the microvascular infiltration.",6
3602,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer (vol6),,TIL density (DenTIL):,"For every patient, multiple density measures including the number of different cells types and their ratios are calculated ",6
3603,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer (vol6),,"GG: A Delaunay triangulation, a Voronoi diagram, and a Minimum Spanning",Tree were constructed ,6
3604,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer (vol6),,CCG:,"For every patient, subgraphs are built on nuclei regardless of their type and only based on their Euclidean distance. Local graph metrics (e.g. clustering coefficient) ",6
3605,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer (vol6),,GNN:,A recent study ,6
3607,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer (vol6),,Results:,"The two top predictive TriAnGIL features were found to be the number of edges between stroma and CD4+ cells, and the number of edges between stroma and tumor cells with more interactions between stromal cells and both CD4+ and tumor cells being associated with response to IO. This finding is concordant with other studies  Result: Figure ",6
3629,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model (vol6),,Algorithm 1:,"Training procedure 1: Input: Input image pairs where is the structure image and is the corresponding dose distribution map, the total number of diffusion steps 2: Initialize: Randomly initialize the noise predictor and pre-trained structure encoder 3: Repeat 4: Sample 5: Sample 6: Perform the gradient step on Equation (9) 7: until converged",6
3640,Coupling Bracket Segmentation and Tooth Surface Reconstruction on 3D Dental Models (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 40.,6
3643,Whole-Heart Reconstruction with Explicit Topology Integrated Learning (vol6),,Surface Loss.,"As in  ), where x and y are the centers of two faces and σ W is a scale controlling parameter that controls the affecting scale between the two faces. Therefore, the surface loss can be expressed as where t 1 , t 2 , t and p 1 , p 2 , p respectively index faces on the reconstructed surfaces S R and those on the corresponding ground truth surfaces S T . L surf ace not only considers each face on the surfaces but also its corresponding direction. When the reconstructed surfaces are exactly the same as the ground truth, the surface loss L surf ace should be 0. Otherwise, L surf ace is a bounded positive value  Minimizing L surf ace enforces the reconstructed surfaces to be progressively close to the ground truth as the training procedure develops. Figure  Loss Function. In addition to the surface loss we introduce above, we also involve two segmentation losses L BCE and L Dice , one point cloud loss L CD , and three regularization losses L laplace , L edge , and L normal that comply with  where w s is the weight for the segmentation loss, and w 1 , w 2 , w 3 and w 4 are respectively the weights for the surface loss, the Chamfer distance, the Laplace loss, and the edge loss. The geometric mean is adopted to combine the five individual mesh losses to accommodate their different magnitudes. L seg ensures useful feature learning of the ROIs. L surf ace enforces the integrity of the reconstructed meshes and makes them topologically similar to the ground truth. L CD makes the point cloud representation of the reconstructed meshes to be close to that of the ground truth. Additionally, L laplace , L edge , and L normal are employed for the smoothness consideration of the reconstructed meshes.",6
3646,Whole-Heart Reconstruction with Explicit Topology Integrated Learning (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 11.,6
3651,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning (vol6),,(III).,"The message passing between the anchors and patches is achieved by a bidirectional cross-attention between the patches and anchors. First, the anchors collect the local information from the patches, which is formulated as where W l ∈ R d f ×de , l = q, k, v are learnable parameters with d e denoting the dimension of the head output, σ represents the softmax function, and ϕ d and ϕ p are the embedding functions that respectively take the distance and polar angle as input and output the corresponding trainable embedding values. Symmetrically, each patch token catches the information of all anchors into their own local representations by the equations The two-way communication makes the patches and anchors timely transmit local information and perceive the dynamic change of global information. The embedding of relative distance and polar angle information helps the model maintain the semantic and structural integrity of the WSI and meanwhile prevents the WSI representation from collapsing to the local area throughout the training process. In terms of efficiency, the computational complexity of self-attention is O(n p 2 ) where n p is the number of patch tokens. In contrast, our proposed PACA's complexity is O(n k × n p ) where n k is the number of anchors. Notice that n k << n p , the complexity is close to O(n p ), i.e. linear correlation with the size of the WSI.",6
3654,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning (vol6),,end end end,"TCGA-Lung dataset is collected from The Cancer Genome Atlas (TCGA) Data Portal. The dataset includes a total of 3,064 WSIs, which consist of three categories, namely Tumor-free (Normal), Lung Adenocarcinoma (LUAD), and Lung Squamous Cancer (LUSC), Endometrial dataset includes 3,654 WSIs of endometrial pathology, which includes 8 categories, namely Well/Moderately/Low-differentiated endometrioid adenocarcinoma, Squamous differentiation carcinoma, Plasmacytoid carcinoma, Clear cell carcinoma, Mixed-cell adenocarcinoma, and benign tumor. Each dataset was randomly divided into training, validation and test sets according to 6:1:3 while keeping each category of data proportionally. We conducted WSI multi-type classification experiments on the two datasets. The validation set was used to perform an early stop. The results of the test set were reported for comparison.",6
3660,Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 69.,6
3663,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images (vol6),,Two-Stage Pipeline with Attention Guided Selection.,"The overview of our two-stage pipeline is shown in Fig.  To complete sample-level classification, both stages share basically the same network architecture. The input images are first processed by a CNN encoder to extract features. Then, we propose the pooling transformer, which is modified from the basic transformer module in Sect. 2, to integrate these features for WSI classification. Additionally, the input images for both stages are 256 × 256. In the coarse-grained stage, in order to allow the model to examine as many local images as possible, we resize the cropped local images from 1024 × 1024 to 256 × 256. In the fine-grained stage, we enlarge suspicious local abnormality and thus crop input images to 256 × 256 from 1024 × 1024. For the coarse-grained stage, after passing the resized local images through encoder and pooling transformer, we obtain a rough prediction result at the sample level. We then use the Cross-Entropy (CE) loss to minimize the difference between the predicted WSI label and the ground truth. In addition, we calculate the attention score to identify the local image inputs that are most likely to yield positive reading. We describe the attention score as where x 0 represents classification token (which is a commonly used setting in transformer  Next, in the fine-grained stage, each local image that has passed attention guided selection is cropped into 16 patches of the size 256 × 256. We expect that those patches contain positive cells and are thus critical to diagnosis at the sample level. The network of the fine-grained stage is the same as that of the coarse-grained stage, but the weights of the encoder is pre-trained in an unsupervised manner (Sect. 2). The same CE loss supervised by sample-level ground truth is used for the fine-grained stage here. For inference, the output of the fine-grained stage will be treated as the final result of the test WSI. Pooling Transformer. We use a transformer network to aggregate features of multiple inputs and to derive the sample-level outcome in both coarse-grained and fine-grained stages. We have observed that different local images of the same sample often have patterns of grouped similarity (such as the first two images in the upper-right of Fig.  Therefore, inspired by ",6
3664,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images (vol6),,Contrastive Pre-training of Encoder.,"To make full use of WSI data and provide a better feature encoder, inspired by MoCo  Specifically, in the same training batch, a patch (256 × 256, the same to the input size of the fine-grained stage) and its augmented patch are treated as a positive pair (note that here ""positive/negative"" is defined in the context of contrastive learning), and their features are required to be as similar as possible. Meanwhile, their features are required to be as dissimilar as possible from those of other patches. So the loss function can be described as f i and f ia represent the positive pair, and f j represents another patch negatively paired with f i . Using this method, we can pre-train a feature encoder in an unsupervised manner and initialize it into our encoder for the fine-grained stage.",6
3666,Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images (vol6),,Sample Numbers and Inference Time.,"In order to further demonstrate the huge potential of our method, we also perform an ablation study on the number of samples used for training and compare the time consuming of the different methods. For the experiment of sample numbers, We compare the best fully supervised detection-based method (Retinanet+GAT ",6
3673,Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning (vol6),,"S(e i , e top","where m denotes the channel of the feature embeddings. Since the labels from lay annotators might be noisy and erroneous, the W and S are applied in following Eq. ( ",6
3675,Democratizing Pathological Image Segmentation with Lay Annotators via Molecular-Empowered Learning (vol6),,Morphology-Molecular Multi-modality Registration.,"The slide-level global translation from Map3D was deployed at a 5× magnification, which is 2 µm per pixel. The 4096×4096 pixels PAS image regions with 1024 pixels overlapping were tiled on anatomical WSIs at a 20× magnification, which is 0.5 µm per pixel. Molecular-Empowered Annotation. The automatic tuft segmentation and molecular knowledge images assisted the lay annotators with identifying glomeruli and cells. ImageJ (version v1.53t) was used throughout the entire annotation process. ""Synchronize Windows"" was used to display cursors across the modalities with spatial correlations for annotation. ""ROI Manager"" was used to store all of the cell binary masks for each cell type. Molecular-Oriented Corrective Learning. Patches were randomly split into training, validation, and testing sets -with a ratio of 6:1:3, respectively -at the WSI level. The distribution of injured glomeruli and normal glomeruli were balanced in the split. Experimental Setting. 2 experienced pathologists and 3 lay annotators without any specialized knowledge were included in the experiment. All anatomical and molecular patches of glomerular structures are extracted from WSI on a workstation equipped with a 12-core Intel Xeon W-2265 Processor, and NVIDIA RTXA6000 GPU. An 8-core AMD Ryzen 7 5800X Processor workstation with XP-PEN Artist 15.6 Pro Wacom is used for drawing the contour of each cell. Annotating 1 cell type on 1 WSI requires 9 h, while staining and scanning 24 IF WSIs (as a batch) requires 3 h. The experimental setup for the 2 experts and the 3 lay annotators is kept strictly the same to ensure a fair comparison. Evaluation Metrics. 100 patches from the testing set with a balanced number of injuries and normal glomeruli were captured by the pathologists for evaluating morphology-based annotation and molecuar-informed annotation. The annotation from one pathologist (over 20 years' experience) with both anatomical and molecular images as gold standard (Fig. ",6
3686,Diffusion-Based Hierarchical Multi-label Object Detection to Analyze Panoramic Dental X-Rays (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 38.,6
3700,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images (vol6),,Data and Evaluation Metrics:,"We evaluated the effectiveness of the proposed model on two datasets: the GlaS dataset and the CRAG dataset. The GlaS dataset comprises 85 training and 80 testing images, divided into 60 images in Test A and 20 images in Test B. The CRAG dataset consists of 173 training and 40 testing images. We have adopted Vahadane method for stain normalization  We assessed the segmentation results using three metrics from the GlaS Challenge: (1) Object F1, which measures the accuracy of detecting individual glands, (2) Object Dice, which evaluates the volume-based accuracy of gland segmentation, and (3) Object Hausdorff, which assesses the shape similarity between the segmentation result and the ground truth. We assigned each method three ranking numbers based on these metrics and computed their sum to determine the final ranking for each method's overall performance.",6
3701,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images (vol6),,Implementation Details:,"In our experiments, we choose the ResNet-50 with FPN as the backbone in the proposed method. The backbone is pretrained on ImageNet. Image decoder, Mask Branch and Mask FCN Head are trained end-toend. We trained on the GlaS and CRAG datasets in a Python 3.8.3 environment on Ubuntu 18.04, using PyTorch 1.10 and CUDA 11.4. During training, we utilized an SGD optimizer with a learning rate of 2.5 × 10 -5 and the weight decay as 10 -4 . We set diffusion timesteps T = 1000 and chose a linear schedule from β 1 = 10 -4 to β T = 0.02. Training was performed on A100 GPU with a batch size of 2.",6
3702,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images (vol6),,Results on the GlaS Challenge Dataset:,"We conducted experiments to evaluate the performance of our proposed model by comparing it with the DSE model  Our proposed model demonstrated a enhancement in performance, surpassing the second-best method on both Test A and Test B datasets. Specifically, on Test A, we observed an improvement of 0.006, 0.01, and 1.793 in Object F1, Object Dice, and Object Hausdorf. Similarly, on Test B, resulting in an improvement of 0.022, 0.014 and 3.694 in Object F1, Object Dice, and Object Hausdorf, respectively. Although Test B presented a more challenging task due to the presence of complex morphology in the images, our proposed model demonstrated accurate segmentation in all cases. The experimental results highlighted the effectiveness of our approach in improving the accuracy of gland instance segmentation.",6
3703,Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images (vol6),,Results on the CRAG Dataset:,"The proposed model was additionally evaluated on the CRAG dataset by comparing it against the GCSBA-Net, DoubleU-Net, DSE model, MILD-Net, and DCAN. The average performance of these models is shown in Table  Ablation Studies: Our network utilizes the Mask Branch and Conditional Encoding to enhance performance and segmentation quality. Ablation studies on the GlaS and CRAG datasets confirm the effectiveness of these modules (Table ",6
3710,Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 61.,6
3714,Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels (vol6),,Gradient Conformity-Based Selection (GCS).,"The GCS aims to distinguish clean samples from noisy ones by exploring their optimization dynamics in the gradient space. Since training samples from the same class usually exhibit similar optimization dynamics  where f (x i ) is the feature vector of the sample x i , and x j denotes the K-Nearest Neighbors (KNN) of x i . p (x i ) is the probability of the most likely true class predicted by its KNN neighbors. Therefore, the gradient g(x i ) is very likely to reflect the true class information and optimization dynamics of x i . For each class, we select α% samples with the smallest loss as an anchor set A c , which is depicted in the shaded areas of the GCS in Fig.  which is the average gradient of all samples in the anchor set A c of the c-th class. Then, we can measure the similarity between the gradient of the sample x i and the principal gradient of class y i with the cosine similarity s g (x i ) = cos < g(x i ), g yi >. For the sample x i , if y i is a noisy label, g(x i ) should be consistent with the principal gradient of its true class and diverge from g yi , thus yielding small s g (x i ). By fitting Gaussian mixture models (GMM) on the similarity score s g (x i ), we can get c g (x i ) = GM M (s g (x i )), which represents the clean probability of the sample x i decided by the GCS criterion. To the best of our knowledge, this is the first work that explores gradient conformity for clean data selection. Feature Conformity-Based Selection (FCS). Since feature space is more robust to noisy labels than the output space  where f (x i ) denotes the feature vector of the sample x i in the anchor set A c of the c-th class. Then, we perform eigen-decomposition on the gram matrix: , where U c is the eigenvector matrix and Σ c is a diagonal matrix composed of eigenvalues. The principal eigenvector u c of U c is utilized to represent the distribution and contextual information of the c-th class. Then, for each sample x i , we measure its label quality based on the conformity of its feature f (x i ) with the principal eigenvector u yi of its given label: s f (x i ) = cos < f (x i ), u yi >. Samples that better align with the principal eigenvectors of their labeled class are more likely to be clean. According to the FCS criterion, the clean probability of the sample x i is obtained by Compared with existing methods that utilize classwise average features to represent contextual information  Integration of GCS and FCS. Finally, we average the clean probabilities estimated by the GCS and FCS criteria to identify clean data. As shown in Fig. ",6
3719,Gradient and Feature Conformity-Steered Medical Image Classification with Noisy Labels (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_8.,6
3724,A Multi-task Method for Immunoﬁxation Electrophoresis Image Classiﬁcation (vol6),,Inner-Task Regularization.,"To effectively model the patterns in different samples, it is crucial to understand the desired appearance of the attention map for positive and negative samples. As previously discussed, for positive samples, the model should emphasize the horizontal region that displays the alignment of dense bands. However, obtaining human-labeled attention areas for the training phase is a time-consuming and labor-intensive task. To address this issue, the attention map is differentiated between positive and negative samples. The goal is to sharpen the attention map for positive samples and smoothen it for negative samples. The attention map M(F) can be viewed as the distribution of attention intensity along the height dimension of the image, as depicted in Fig.  Here, KLdiv(A • B) = A • (log Alog B). The inner-task attention mechanism allows the model to differentiate the attention map between positive and negative samples. Inter-Task Regularization. It is important to note that the alignment of dense bands between heavy lanes, light lanes, and the ELP lane is a crucial factor in the identification of S&M-samples by human experts. To model this relationship in the proposed method, the two sub-tasks are encouraged to emphasize similar horizontal areas. Specifically, for all S&M-samples, the inter-task regularization can be designed with the Jensen-Shannon Divergence (JSdiv)  Here, JSdiv(A The inter-task regularizer enables the model to better capture the patterns of S&M-samples by modeling the relationship between the two sub-tasks.",6
3727,A Multi-task Method for Immunoﬁxation Electrophoresis Image Classiﬁcation (vol6),,Evaluation Metric and Comparison,"Methods. We assess the efficacy via a suite of evaluation metrics, including accuracy (acc), F1-score, false negative rate (fnr), top-1 minor class accuracy (M1-Acc), and top-3 minor class accuracy (M3-Acc). These metrics are widely adopted for classification problems, with the false negative rate being of particular significance in the medical domain as it quantifies the likelihood of misidentifying positive samples as negative. The top-1 and top-3 minor class accuracy metrics evaluate the model's performance on underrepresented classes. A ResNet18-based multi-class classification approach is selected as the baseline and tuned using an identical range of hyper-parameters as the proposed method. The baseline is trained for 100 epochs with a batch size of 64, an initial learning rate of 0.1, and a decay scheduler consistent with the proposed method. Additionally, the proposed method is compared against Two Stage DNN  Performance Evaluation. The methodology proposed in this study is thoroughly evaluated and compared to both the baseline method and other stateof-the-art techniques on the IFE dataset, as presented in Table  Visualization. The proposed horizontal attention mechanism provides a useful tool for verifying the consistency of model-learned knowledge with expert experience. As illustrated in Fig.  Ablation Study. An ablation study is conducted to examine the contribution of key components of the proposed model, including the multi-task framework and two regularization methods. The results, presented in Table ",6
3742,ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography (vol6),,Method,"Ablation Study. We assessed the effect of removing distinct components of our design: uncertainty prototypes (L abs , p u k ), clustering and separation (L clst , L sep ), and push mechanism. As shown in Table ",6
3745,ProtoASNet: Dynamic Prototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in Echocardiography (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 36.,6
3751,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis (vol6),,Patch-Level Block.,"Given the patch-level feature set P = N i=1 P i , the PL block learns long-term relationships within the patch level: where l = 1, 2, ..., L is the index of the HIViT block. P L(•) includes a Separable Self Attention (SSA)  Bidirectional Interaction Block. We propose a Bidirectional Interaction (BI) block to establish communication between different levels within the WSI pyramids. The BI block performs bidirectional interaction, and the interaction progress from region nodes to patch nodes is: where the SE(•) means the Sequeeze-and-Excite layer  where the MEAN(•) is the operation to get the mean value of patch nodes set P l+1 i associated with the i-th region node and P l+1 1 ∈ R 1×C and the C is the feature channel of nodes, and Rl+1 is the region nodes set after interaction. Region-Level Block. The final part of this module is to learn the long-range correlations of the interacted region-level nodes: where l = 1, 2, ..., L is the index of the HIViT module, and RL(•) has a similar structure to P L(•).",6
3755,HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 73.,6
3767,Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 30.,6
3771,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans (vol6),,MPBD-LSTM.,"The most basic building block in MPBD-LSTM is the 3D-LSTM modules. Each 3D-LSTM module is composed of two E3D-LSTM cells  where -→ h v,t and ←h v,t are the output hidden state of the forward pass and backward pass of phase v at timestamp t, and σ is the function which is used to combine these two outputs, which we choose to use a summation function to get the summation product of these two hidden states. Therefore, the output of the bi-directional LSTM module presented in Fig.  in which ⊕ stands for summation. After this, the output y v,t0 is passed into the bi-directional LSTM module in the next layer and viewed as input for this module. Figure  An alternative approach is to additionally connect two planes by combining the hidden states of 3D-LSTM modules and taking their average if a module receives two inputs. However, we found that such design actually resulted in a worse performance. This issue will be demonstrated and discussed later in the ablation study. In summary, the MPBD-LSTM model comprises two planes, each of which contains three 3D-LSTM stacks with two modules in each stack. It modifies E3D-LSTM by using bi-directional connected LSTMs to enhance communication between different timestamps, and a multi-plane structure to simultaneously process multiple phases.",6
3776,MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 37.,6
3786,Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology (vol6),,Cellular Explainability Method. The cellular average embedding is,"where e ij ∈ R 256 is the cellular embedding extracted from every detected cell in the tile j i ∈ 1, 2, . . . , N j where N j is the number of cells in the tile j. This can be rewritten as a weighted average of the cellular embeddings where w i ∈ R 256 are the per cell attention weights that if initialized to 0 result in the original cellular average embedding. The re-formulation does not change the result of the forward pass since w i are not all equal. Note that the weights are not learned through training but calculated per cell at inference time to get the per cell contribution. We computed the gradient of the output category (of the classification method applied on top of the computed embedding) with respect to the attention weights w i : grad i = ∂Score i /∂w i and visualized cells that received positive and negative gradients using different colors. Visual Example Results. Examples of our cellular explainability method applied to weakly supervised tumor detection on WSIs from the CAMELYON16 data set using A-MIL are shown in Fig.  In this case, cells with positive attention gradients that shifted the output towards a classification of GCB were labeled green and cells with negative attention gradients that shifted the classification towards ABC were labeled red. Cells with positive attention gradients were mostly smaller lymphoid cells with low grade morphology or were normal lymphocytes, whereas cells with negative attention gradients were more frequently larger lymphoid cells with high grade morphology (Fig. ",6
3806,Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model (vol6),,Ablation Study:,"We conducted an additional study to observe the effects of the proposed multiresolution heatmap learning and HTC. For this study, we compared the HTC with Conformer ",6
3808,Anatomical Landmark Detection Using a Multiresolution Learning Approach with a Hybrid Transformer-CNN Model (vol6),,Acknowledgements,. This research was partly supported by the ,6
3811,Transformer-Based End-to-End Classification of Variable-Length Volumetric Data (vol6),,Slice Feature Extractor (SFE).,"To obtain the slice representations, we use ViT as our SFE due to its recent success in medical interpretation tasks  Volume Feature Aggregator (VFA). The output of the previous step is a sequence of slice-wise embeddings, to which we append a learnable volumelevel classification token  Volume Classification. Finally, the volume-level classification token is fed to a Fully Connected (FC) layer, which produces individual class scores. As a loss function, we employ the weighted cross-entropy.",6
3829,Frequency-Mixed Single-Source Domain Generalization for Medical Image Segmentation (vol6),,Comparison and Ablation Study:,The effectiveness of the proposed algorithm is demonstrated in comparison with state-of-the-art methods and an ablation study. The Fourior-based DG methods FACT  (1) Comparison. Quantified comparison of our algorithm with the competing methods is summarized in Table  Visualized comparison is shown in Fig.  (2) Ablation Study. According to Table ,6
3840,Vision Transformer Based Multi-class Lesion Detection in IVOCT (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 32.,6
3849,CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_47.,6
3853,An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark (vol6),,Method Dice IoU,"Fully-supervised Mask R-CNN  Semi-supervised PseudoSeg  Compared Methods and Evaluation Metrics. We compared TCSegNet with the fully-supervised counterparts, including method specific for segmentation in general image ",6
3863,Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models (vol6),,Self-training and YOLOX.,"The box optimization process of the self-training stage is recorded, and the corresponding results are presented in the supplement. YOLOX and self-training are not the essential reasons for the superior performance of our method. The true key is the utilization of semantic-information-rich VLPMs. To illustrate this point, we employed another commonly used unsupervised detection method, superpixels ",6
3865,Zero-Shot Nuclei Detection via Visual-Language Pre-trained Models (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 67.,6
3870,AR2T: Advanced Realistic Rendering Technique for Biomedical Volumes (vol6),,Random Number Generation.,"To provide the uniformly distributed random numbers on the fragment stage of the OpenGL pipeline, we generate a pool of 50 additional two-dimensional textures of viewport size and fill them with uniformly distributed random numbers generated with std::uniform int distribution. In each step, we randomly choose four textures from the pool to provide random numbers: two for advanced Woodcock tracking, one for scattering, and one for sampling direction. Every 100 iterations of the algorithm, we regenerate the pool of random numbers to avoid the quality improvement stuck. On Intel(R) Core(TM) i5-7600K CPU @ 3.80 GHz, the generation of the pool takes ∼ 1600 ms, for viewport size 1727 × 822. It occupies ∼270 Mb of RAM. Advanced Woodcock Tracking. For volume sampling, we implemented the advanced Woodcock tracking technique, described in  Phase Functions. In AR 2 T, we use four well-known phase functions, described in  where α( -→ v ) ∈ [t 0 , t 1 ] is the voxel density (or opacity), m( -→ v ) is the normalized gradient magnitude, and s is the hybrid scattering factor. Image Generation. When the first iteration of AR 2 T is completed, the result contained in the colour texture (see Recursion for rehearse) is blit into the output rendering frame buffer to be immediately displayed. Moreover, it is saved locally to be summed with the results of the next iterations. On the next iterations, the accumulated result is saved into the local buffer, and then the medium (e.g. the sum divided by the iterations number) is blit into the output frame buffer and displayed. Convergence. As a convergence criterion, we use mean square displacement (MSD) between the iterations ",6
3882,A Semantic-Guided and Knowledge-Based Generative Framework for Orthodontic Visual Outcome Preview (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 14.,6
3886,Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy (vol6),,ln(f (x)) = ln(,"Note that the last equation is in polynomial form ln(y) = ax 2 + bx + c, with c = ln(A) --μ 2 2σ 2 , b = μ σ 2 and a = -1 2σ 2 . By defining the error function δ = ln(f (x)) -(ax 2 + bx + c) and differentiating the sum of residuals gives a linear system of equations (Eq. 2). ⎡ After solving the linear system, we can finally retrieve μ, σ and A with μ = -b/2c, σ = -1/2c, and A = e a-b 2 /4c .",6
3887,Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy (vol6),,Guos Algorithm.,"Due to the logarithmic nature of Caruanas algorithm, it is very susceptible towards outliers. Guo ",6
3888,Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy (vol6),,= y[ln(y,"The parameters of the Gaussian function μ, σ and A can be calculated similar to Caruanas algorithm. Recall that polynomial regression has an analytical solution, where the vector of estimated polynomial regression coefficients is β = (X T X) -1 X T ŷ. This formulation is easily parallelizable on a GPU and fully differentiable. Hence, we can efficiently compute the Gaussian coefficients necessary for determining the subpixel position of local maxima. Finally, we can calculate the points position p by simple addition using Eq. 5.",6
3890,Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy (vol6),,HLE++.,"The HLE dataset is a publicly available dataset consisting of 10 labeled in-vivo recordings of human vocal folds during phonation  , where F i is the vocal fold segmentation at frame i and G i the glottal segmentation, respectively. Quantitative and Qualitative Evaluation. Table ",6
3892,Joint Segmentation and Sub-pixel Localization in Structured Light Laryngoscopy (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 4.,6
3896,Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis (vol6),,"i=λ2N,j=(1-λ2)N i=1",", which include both the remaining patch tokens and the masked patch tokens. After the process of the mixed attention module, H&E and HER2 patch tokens are fed into the modal-specific decoders respectively to reconstruct the original H&E image X and HER2 image Y . The reconstruction loss is computed by the mean squared error between the original images X, Y and the generative images X , Y , which is computed as We use an adjustable hyperparameter θ to balance the losses of two modalities. The final loss L is defined as",6
3923,Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision (vol6),,PDL1 Results,. We chose to compare our method with unsupervised segmentation methods because existing point-supervised segmentation methods are unable to segment cell membranes. We present their results in Table ,6
3925,Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision (vol6),,Acknowledgements,. This work is supported by the ,6
3926,Segment Membranes and Nuclei from Histopathological Images via Nuclei Point-Level Supervision (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_52.,6
3933,Cross-View Deformable Transformer for Non-displaced Hip Fracture Classification from Frontal-Lateral X-Ray Pair (vol6),,Implementation Details.,"For experiments of our dataset, we manually locate the hip region and crop a 224 × 224 image that is centered on the original hip region whose size is 400 × 600. The learning rate is set as 3e-3 for the endto-end training of the framework with a batch size of 32. We adopt a 10-fold cross-validation and report the average performance of 10 folds. For each fold, we further divide the data (the other 9 folds) into a training set (90%) and a validation set (10%) and take the best model on the validation part for testing. Evaluation Metric. We evaluate our method with Accuracy (Acc), Precision, Recall and F1 score. The Precision and Recall are calculated with one-classversus-all-other-classes and then calculate F1 score F 1 = 2•P recision•Recall P recision+Recall .",6
3947,Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 3.,6
3949,Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning (vol6),,Related Work. Considerable progress has been made in segmenting,"With growing research in vision-and-language and contrastive learning  However, these methods are still not suitable for variable-length reports and are inefficient in low-resource settings. Our Contributions. We propose a novel framework for more accurate and efficient computer-aided placenta analysis. Our framework introduces two key enhancements: Pathology Report Feature Recomposition, a first in the medical VLC domain that captures features from pathology reports of variable lengths, and Distributional Feature Recomposition, which provides a more robust, distribution-aware representation. We demonstrate that our approach improves representational power and surpasses previous methods by a significant performance margin, without additional data. Furthermore, we boost training and testing efficiency by eliminating the large language model (LLM) from the training process and incorporating more efficient encoders. To the best of our knowledge, this is the first study to improve both the efficiency and performance of VLC training techniques for placenta analysis.",6
3960,Enhancing Automatic Placenta Analysis Through Distributional Feature Recomposition in Vision-Language Contrastive Learning (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 12.,6
3966,Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging (vol6),,Comparison with Traditional Biomarkers. Table 1 shows p-values for,"AutoVT and traditional biomarkers. The proposed biomarker based on the overall loss function L, AutoVT(L), was the most sensitive to the effect of pomegranate with the lowest p-value. This biomarker has learned the volumetric information from the input images, as demonstrated by the correlation coefficient of 0.84 between AutoVT(L) and ΔV W V . AutoVT(L) has also learned the texture information, as demonstrated in Fig.  Comparison with Different Losses. We compared our proposed loss with another two losses, including cross-entropy loss and bi-direction contrastive loss. Cross-entropy loss is expressed as , where σ(•) is a sigmoid function. The bi-direction contrastive loss is a symmetric version of L tcl , expressed as L bd = y max(m + AutoV T, 0) 2 + (1y) max(m -AutoV T, 0) 2 . The margin m in L bd was tuned in the same way as the proposed L, with m = 0.4 being the optimized parameter in all three cross-validation trials. Table ",6
3968,Interpretable Deep Biomarker for Serial Monitoring of Carotid Atherosclerosis Based on Three-Dimensional Ultrasound Imaging (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 29.,6
3972,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction (vol6),,Results.,In Table ,6
3973,Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction (vol6),,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 60. Ablation Analysis. We verify the model efficiency by using fewer amounts of finetuning data in finetuning. For TCGA-COAD dataset, we include 50%, 25%, and 10% of the finetuning data. For the TCGA-READ dataset, as the number of uncensored patients is limited, we use 75%, 50%, and 25% of the finetuning data to allow at least one uncensored patient to be included for finetuning. As shown in Fig. ",6
4013,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble (vol6),,Datasets. Evaluations of StainDiff are conducted on two datasets. (1),Dataset-A: MITOS-ATYPIA 14 Challenge,6
4014,StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble (vol6),,Implementations.,"All experiments are implemented in Python 3.8.13 with Pytorch 1.12.1 on two NVIDIA GeForce RTX 3090 GPU cards with 24GiB of memory each in parallel. We leverage the Adam optimizer with a learning rate of 2 × 10 -4 , and a batch size of 4. The learning scheme follows previous work ",6
4024,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors (vol6),,Source Graph Construction,"Here, given patches as nodes V , we first calculated the pairwise distance among different nodes, and select the top 10 percent connections with the smallest distance values as edges E. For each node in V , we followed the study in ",6
4025,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors (vol6),,Calculating TILs-Tumor Interaction via Graph Attention Networks(GATs).,"To characterize the interaction between different TILs and tumor patches, we employed GAT  we calculated the attention coefficients among different nodes, which can be formulated as: Furthermore, a softmax function was then adopted to normalize the attention coefficients e ij : where N i represents all neighbors of node i. The new feature vector v i for node i was calculated via a weighted sum: Finally, the output features of each GAT layer were aggregated in the readout layer. We fed the generated output features from each readout layer into the Cox hazard proportional regression model for the final prognosis predictions. Feature Alignment. In the proposed GAT-based transfer learning framework, the feature alignment component was employed on its first two layers. Then, for the node embeddings with different types (TILs and Tumor) in both the source and target domain, we performed a mean pooling operation to obtain their aggregated features. Next, we aligned the aggregated tumor or TILs features from the two domains separately using Maximum Mean Discrepancy(MMD)  Here, we adopted MMD for feature alignment due to its ability to measure the distance between two distributions without explicit assumptions on the data distribution, we showed the objective function of MMD in our method as follows: where H is a Hilbert space, f represents the features from the source, f represents the feature from the target, r represents the layer number, k ∈ {L, T } referred to TILs or tumor node. In addition, n denotes the number of source samples, while m refers to the number of target samples.  TILs-Tumor Interaction Alignment. To accurately characterize the interaction between TILs and tumors, we further analyzed the extracted interaction weights by dividing them into 10 intervals (i.e., bins). For each interval, we calculated the sum of all source domain interaction weights as i s k and the sum of all target domain interaction weights as i t k , where k represents the k-th interval. Consequently, we obtained two vectors and applied softmax on each of them for normalization that can be denoted as . In order to measure the dissimilarity between p i and q i , the Kullback-Leibler (KL) divergence is adapted on the third layer of GAT, which can be formulated as: According to Eq.( ",6
4026,Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors (vol6),,Prognosis Prediction by the Cox Proportional Hazard Model.,"The Cox proportional hazard model was applied to predict the patients' clinical outcome  where x i represents the output of the last layer for the prognosis task and R (t i ) is the risk set at time t i , which represents the set of patients that are still under risk before time t. In addition, δ i is an indicator variable. Sample i refers to censored patient if Overall Objective. To achieve domain-adaptive prognosis prediction, the final loss function included the Cox loss, FA loss, and TTIA loss as the following formula: where α and β represent the weights assigned to the importance of FA component and TTIA component respectively.",6
4035,Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information (vol6),,Non-local Module:,"Given a hip X-ray image X and the corresponding CAM map X m , we apply the backbone of ResNet18 to extract the high-level feature maps x ∈ R C×H ×W . The feature maps are then treated as inputs for the non-local module. For output xi from position index i, we have where concat denotes concatenation, w f is a weight vector that projects the concatenated vector to a scalar, ReLU is the ReLU function, a ij denotes the non-local feature attention that represents correlations between the features at two locations (i.e., x i and x j ), θ, φ, and g are mapping functions as shown in Fig. ",6
4037,Multi-modality Contrastive Learning for Sarcopenia Screening from Hip X-rays and Clinical Information (vol6),,Visual-Text Fusion Module:,"In order to learn from clinical data, we first encode 5 numerical variables as a vector and send it to TextNet. As shown in Fig.  and W v are part of the model parameters to be learned. We compute the visual-text self-attentive feature ẑvt i at position i as The softmax operation indicates the attention across each visual and text pair in the multi-modality feature.",6
4052,Gene-Induced Multimodal Pre-training for Image-Omic Classification (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_49.,6
4061,Histopathology Image Classification Using Deep Manifold Contrastive Learning (vol6),,Acknowledgements,. This study was approved by the institutional review board of ,6
4074,Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction (vol6),,Loss Function and Training Strategy.,"For the network training, Cox loss  where δ i denote the censorship of i-th patient, O(i) and O(j) denote the survival output of i-th and j-th patient in a batch, respectively. GPUs. Our graph convolutional model is implemented by Pytorch Geometric ",6
4084,TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction (vol6),,Cross-Modal Attention,"where four independent 1×1 convolutions α, β, θ, and γ are used to map image and fusion features to the space for cross-modal attention. At first, M and F are passed into θ(•), β(•) and α(•) for channel transformation, respectively. Following the transformed feature, M θ and F β perform multiplication via a Softmax activation function to take a cross-attention with F α . In the end, the output feature of the cross-attention via γ(•) for feature smoothing is added with F. Given the above operations, the cross-modal features O et and O dt are obtained and passed to the next layer. Knowledge Alignment Module. The above operations only consider the interaction between features in two modalities. A problem is that text embeddings from pre-trained text encoder of CLIP are not well aligned with the image features initialized by ImageNet pre-training. This knowledge shift potentially weakens the proposed cross-modal interaction to assist the prediction of implant position. To mitigate this problem, we propose the knowledge alignment module (KAM) to gradually align image features to the feature space of pre-trained CLIP. Motivated by knowledge distillation  where m e 4 ∈ R 1×D is the transformed feature of M e 4 after attention pooling operation ",6
4089,TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction (vol6),,Comparison to the Mainstream Detectors.,"To demonstrate the superior performance of the proposed TCEIP, we compare the AP value with the mainstream detectors in Table ",6
4091,TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction (vol6),,Acknowledgments,. This work was supported by the ,6
4092,TCEIP: Text Condition Embedded Regression Network for Dental Implant Position Prediction (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 31.,6
4104,Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_1.,6
4111,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification (vol6),,Ours,"(w/ AB-MIL) 90.0 firstly embedded into a 1024-dimension vector, and then be projected to a 512dimension hidden space for further bag-level training. For the training of bag classifier f (•), we used an initial learning rate of 2e-4 with Adam ",6
4114,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification (vol6),,Acknowledgements,. This work was supported by the ,6
4115,Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 45.,6
4124,TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2_41.,6
4135,STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models (vol6),,Radiomic Feature Extraction:,"In first stage, radiomic feature R(I N ) is extracted on each I N of the LVW for each phase (frame) of the echo video, V as given in ",6
4136,STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models (vol6),,Time Series Feature Extraction:,"In the second stage, to model the temporal LVW motion, we consider the sequence of radiomic features from each phase of one heartbeat cycle as individual time-series. Thus, a radiomic feature timeseries t R (V ) is given by (2). Time-series feature T R , as given by (3), is extracted for each radiomic feature time-series using the TSFresh library ",6
4137,STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models (vol6),,R(I,"Thus, the radiomic time-series model M R is trained on time-series features T R (V ) obtained for each V of the patient to predict the outcome O T .",6
4138,STAR-Echo: A Novel Biomarker for Prognosis of MACE in Chronic Kidney Disease Patients Using Spatiotemporal Analysis and Transformer-Based Radiomics Models (vol6),,Video Transformer Model:,The Video Vision Transformer (ViViT) model ,6
4149,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound (vol6),,MSKUS,"Evaluation Metrics. Five metrics were used to evaluate model performance: Accuracy (ACC), Area Under Curve (AUC), Correlation Coefficient (CC), Similarity (SIM) and Kullback-Leibler divergence (KLD)  Evaluation of ""Thinking like Sonographers"" Mechanism. To evaluate the effectiveness of our proposed mechanism of ""Thinking like Sonographers"" (TLS) that combines ""where to adjust"", ""what to adjust"" and ""how to adjust"", we compared the gout diagnosis results of several classic CNN classification ",6
4150,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound (vol6),,Stability Under Different Gaze Maps via t-Test.,"To evaluate the prediction's stability under the predicted gaze map from the generation model in ""Where to adjust"", we conducted three t-test studies. Specifically, we trained two classification models (M C and M P ), using the actual collected gaze maps, and the predicted maps from the generation model, respectively. During the testing, we used the collected maps as input for M C and M P to get classification results R CC and R P C . Similarly, we used the predicted maps as input for M C andM P As shown in Table ",6
4152,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound (vol6),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43987-2 16.,6
4155,A Novel Multi-task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images (vol6),,Cross-Interaction Module.,"A diagnostic process of skin diseases takes multiple visual information into account, which is relatively complicated and difficult to model in an analytical way. Simple fusion operations such as concatenation are insufficient to simulate the diagnostic logic. Thus, partially inspired by  where g B , g D are the class token, p i b , p i d , i ∈ {1, 2, ..., N p } the corresponding patch tokens. GAP and LN denote the global average pooling and layer normalization, respectively. W Q BD , W K BD , W V BD ∈ R F ×F denote learnable parameters. F denotes the dimension of features. g D B is computed from the patch tokens of disease and the class token of body-part in the same fashion. Similarly, we can obtain the fused class tokens (g D A and g A D ) and the fused local class tokens (l D A and l A D ) between attribute and disease. Note that the disease class token g D is replaced by g B D in the later computations, and local class tokens l A and l D in Fig.  Learning and Optimization. We argue that joint training can enhance the feature representation for each task. Thus, we define a multi-task loss as follows:  where N s denotes the number of samples, n x , n h the number of classes for each task, and p ij , y ij the prediction and label, respectively. Notably, body parts and attributes are defined as multi-label classification tasks, optimized with the binary cross-entropy loss, as shown in Eq. 6. The correspondence of x and h is shown in Fig. ",6
4167,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency (vol7),,Multi-view Projection-Based Class-Similarity Consistency (MPCC).,"For multi-class segmentation tasks, it is important to learn inter-class relationship for better distinguishing them. In addition to using L USP C for pixel-wise supervision, we consider making consistency on class relationship across the outputs of the decoders as illustrated in Fig.  Similarly, P n is projected in the sagittal and coronal views, respectively, and the corresponding normalized class affinity matrices are denoted as Q sagittal n and Q coronal n , respectively. Here, the affinity matrices represents the relationship between any pair of classes along the dimensions. Then we constraint the consistency among the corresponding affinity matrices by Multi-view Projection-based Class-similarity Consistency (MPCC) loss: where v ∈ {axial, sagittal, coronal} is the view index, and Qv is the average class affinity matrix in a certain view obtained by the three decoders.",7
4170,Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency (vol7),,Organ,FullySup ,7
4177,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification (vol7),,Class Activation Map.,"At the final convolutional layer of our model, the activation of the ith feature map f i (x, y) at coordinates (x, y) is associated with a weight w k i for class k. This allows us to generate the attention map H k for class k as: Pyramid Loss. To enhance the learning of important attention areas, we propose a pyramid loss constraint that requires consistency between the network and gaze attention maps. The pyramid loss is based on using a pyramid representation of the attention map: where H is the network attention map generated by the CAM and R is the radiologist's gaze heat map. square error (MSE) between the attention maps generated by the radiologist and the model at each level of the Gaussian pyramid. This allows the model to mimic the attention of radiologists and enhance diagnostic performance. Moreover, the pyramid representation enables the model to learn from the important pathological regions on which radiologists are focusing, without the need for precise pixel-level information. Layernorm is also employed to address the issue of imprecise gaze data. This reduces noise in the consistency process by performing consistency loss only in the regions where radiologist spent most time.",7
4184,Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 7.,7
4193,CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows (vol7),,Acknowledgments,. This work was supported by the ,7
4204,Fundus-Enhanced Disease-Aware Distillation Model for Retinal Disease Classification from OCT Images (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 60.,7
4223,Eye-Guided Dual-Path Network for Multi-organ Segmentation of Abdomen (vol7),,Acknowledgements,. This study was supported by the ,7
4234,Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-Aware Connection Classifier (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_71.,7
4238,3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer (vol7),,Graph-Transformer.,"The graph-transformer is composed of a semantic KNN and a Transformer Encoder Block. For the input C-domain, we first construct a KNN graph based on the semantic biased Euclidean distance formed as: where cell i and cell j indicate the ith cell and jth cell and the Semantic Dist function measures the difference of the two cells which is formulated as: where λ is a positive parameter that can be set according to the specific task and l i indicates the pseudo label of the ith cell. Then we perform a transformer encoder block on each cell to get the local dependencies which can enhance the local features belong to each class. The attention we used is a standard multihead attention, and we set the query and value as the matrices of neighbour features of the cells and the key as the distance matrix between cells and their neighbours.",7
4257,Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_54.,7
4278,DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning (vol7),,Comparison of Performance in Different Tasks:,"For the classification task, the AUC results of abnormal classification are shown in Table ",7
4289,ModusGraph: Automated 3D and 4D Mesh Model Reconstruction from Cine CMR with Improved Accuracy and Efficiency (vol7),,Acknowledgements. YD was funded by the,Kings-China Scholarship Council PhD Scholarship Program. HX was funded by ,7
4294,Full Image-Index Remainder Based Single Low-Dose DR (vol7),,Influence of Regularization Term:,"We proposed a regularization term in Sect. 3, and the hyperparameter λ is used to adjust the regularization term. Figure ",7
4296,Full Image-Index Remainder Based Single Low-Dose DR (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 44.,7
4316,"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition (vol7)",,Compared with Single-Modal and Multi-modal Solutions. As shown in Table,"Ablation Study. The ablation study on the private dataset examines the contribution of three modules, the order of HAF, and the depth of each module. From Table .3, MILR and HAF modules bring 0.03 and 0.02 AP increases, respec-  tively. Reversing the order of the HAF module brings a decrease, which indicates that the modal-agnostic features should be extracted after the Merged-attention.",7
4319,"Representation, Alignment, Fusion: A Generic Transformer-Based Framework for Multi-modal Glaucoma Recognition (vol7)",,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 66.,7
4330,WarpEM: Dynamic Time Warping for Accurate Catheter Registration in EM-Guided Procedures (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 75.,7
4347,Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment (vol7),,Ablation of Conditional Inputs and Multi-scale Mechanism:,As shown in Table ,7
4349,Conditional Physics-Informed Graph Neural Network for Fractional Flow Reserve Assessment (vol7),,Ablation of CPGNN Components:,As shown in Table ,7
4360,AngioMoCo: Learning-Based Motion Correction in Cerebral Digital Subtraction Angiography (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_72.,7
4363,A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images (vol7),,Contrast Influence on Contextual Information.,"Physiological experiments  where C l is a local range measure that returns the range value (maximum value-minimum value) using a patch of N × N neighborhood around the pixel (x, y) in the response of ψ. S d computes the local standard deviation of the N × N patch around the corresponding input pixel of ψ response. We use N = 5 in our implementation. Modulatory Function. The essence of the proposed modulatory model is that the response of the elongated representation at a specific point is modulated by the response of the representation presented in the area outside the region of the representation interest. We integrate the elongated responses with the contextual influences at each pixel to produce spatial coherent responses that enhance vessel structures more conspicuously from their background while reducing noise disturbances with vascular regions. The proposed modulatory model (M) is defined as: where is a small value to avoid division by zero. When there are no spurious signals in the region surrounding the elongated responses, the modulatory influence from (H i and S i ) produces a weak response and the numerator (ψ -S i .H i ) of M becomes almost equal to the ψ. As a result, the term (H i + S i ) in the denominator together performs a faciliatory process for improving the responses of ψ. However, the influence of (H i and S i ) has a strong response when containing spurious signals in the surroundings. Then, the (H i and S i ) behave as an inhibitory process in both the numerator and denominator, which drops off the contribution of ψ to almost zero response.",7
4365,A Modulatory Elongated Model for Delineating Retinal Microvasculature in OCTA Images (vol7),,Results and Comparisons,. Figure  Performance on Challenging Cases. In Fig. ,7
4371,Towards Expert-Amateur Collaboration: Prototypical Label Isolation Learning for Left Atrium Segmentation with Mixed-Quality Labels (vol7),,Implementation and Evaluation Metrics. The framework is based on,"PyTorch using an NVIDIA GeForce RTX 3090 GPU. 3D V-Net  Comparison Study. The quantitative results are presented in Table  Ablation Study and Discussions. To further investigate how our method works, we perform an ablation study under the 4-HQ-sample setting (as presented in Table ",7
4375,Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI (vol7),,Cardiac MRI Preprocessing:,"The preprocessing of cardiac MRI contains (1) normalization of scans, (2) automatic landmark detection, (3) inter-subject registration, and (4) in-plane downsampling. We standardize cardiac MRI intensity levels using Z-score normalization ",7
4376,Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI (vol7),,Landmark Detection and Uncertainty-based Sample Binning:,"We utilize supervised learning to automate landmark detection using an ensemble of Convolutional Neural Networks (CNNs) for each modality (short-axis and fourchamber). We use the U-Net-like architecture and utilize the same training regime implemented in  A minor error in landmark prediction can result in incorrect image registration  Tensor Feature Learning: To extract features from processed cardiac scans, we employ tensor feature learning, i.e. Multilinear Principal Component Analysis (MPCA)  where P n < I n , and × n denotes a mode-wise product. Therefore, the feature dimensions are reduced from I 1 × I 2 × I 3 to P 1 × P 2 × P 3 . We optimize the projection matrices {U (n) } by maximizing total scatter Y m is the mean tensor feature and ||.|| F is the Frobenius norm  Multimodal Feature Integration: To enhance performance, we perform multimodal feature integration using features extracted from the short-axis, fourchamber, and Cardiac Measurements (CM). We adopt two strategies for feature integration, namely the early and late fusion of features  Performance Evaluation: In this paper, we use three primary metrics: Area Under Curve (AUC), accuracy, and Matthew's Correlation Coefficient (MCC), to evaluate the performance of the proposed pipeline. Decision Curve Analysis (DCA) is also conducted to demonstrate the clinical utility of our methodology. ",7
4378,Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI (vol7),,Experimental Design:,"We conducted experiments on short-axis and fourchamber scans across four scales. To determine the optimal parameters, we performed 10-fold cross-validation on the training set. From MPCA, we selected the top 210 features. We employed early and late fusion on short-axis and fourchamber scans, respectively, while CM features were only fused using the late fusion strategy. We divided the data into a training set of 1081 cases and a testing set of 265 cases. To simulate a real testing scenario, we designed the experiments such that patients diagnosed in the early years were part of the training set, while patients diagnosed in recent years were part of the testing set. We also partitioned the test into 5 parts based on the diagnosis time to perform different runs of methods and report standard deviations of methods in comparison results. For SVM, we selected the optimal hyper-parameters from {0.001, 0.01, 0.1, 1} using the grid search technique. The code for the experiments has been implemented in Python (version 3.9). We leveraged the cardiac MRI preprocessing pipeline and MPCA from the Python library PyKale ",7
4379,Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI (vol7),,Uncertainty-Based Sample Binning:,"To improve the quality of training data, we used quantile binning to remove training samples with uncertain landmarks. The landmarks were divided into 50 bins, and then removed one bin at a time in the descending order of their uncertainties. Figure ",7
4380,Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI (vol7),,Unimodal Study:,The performance of three models on single-modality is reported in Table ,7
4381,Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI (vol7),,Bi-modal Study:,"In this experiment, we compared the performance of bimodal models. As shown in Table ",7
4382,Tensor-Based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI (vol7),,Effectiveness of Tri-modal:,"In this experiment, we performed a fusion of CM features with the bi-modal models to create two tri-modal models. The first trimodal is tri-modal late (CM with a late fusion of short-axis and four-chamber) and the second tri-modal is a tri-modal hybrid (CM with an early fusion of shortaxis and four-chamber). As shown in Fig.  Decision Curve Analysis (DCA)  Feature Contributions: Our model is interpretable. The highly-weighted features were detected in the left ventricle and interventricular septum in cardiac MRI. For cardiac measurements, left atrial volume (0.778/1) contributed more than left ventricular mass (0.222/1) to the prediction.",7
4401,Skin Lesion Correspondence Localization in Total Body Photography (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_25.,7
4410,Retinal Thickness Prediction from Multi-modal Fundus Photography (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_55.,7
4422,ASC: Appearance and Structure Consistency for Unsupervised Domain Adaptation in Fetal Brain MRI Segmentation (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 31.,7
4434,Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling (vol7),,Truncated Diffusion Model.,"As mentioned in the previous section, speckle noise follows a gamma distribution and can be transformed into a Gaussian distribution via a logarithmic function. This transformation enables matching the Markov chain procedure in the reverse diffusion process. To speed up the sampling process, this work introduces a truncated reverse procedure that can directly obtain satisfying results from posterior sampling  CPDM Integrated Fidelity Term. Inspired by the fact that the score-based reverse diffusion process is a stochastic contraction mapping so that as long as the data consistency imposing mapping is non-expansive, data consistency incorporated into the reverse diffusion results in a stochastic contraction to a fixed point  The Bayesian maximum a posteriori (MAP) formulation leads to the image despeckling optimization with data fidelity and regularization terms. arg min where R() is the regularization term, and λ is the regularization parameter. The unconstrained minimization optimization problem can be defined as a constrained formulation by variable splitting method  Motivated by the iterative restoration methods with prior information to tackle various tasks become mainstream, we explore the fidelity term Eq. 4 from the posterior distribution of observed images into the iterative reverse diffusion procedure. The fidelity can guarantee data consistency with original images and avoid falling into artificial artifacts. Moreover, we learn reasonable prior from DDPM reverse recover procedure, which can ensure the flexibility with iterative fidelity term incorporated into the loop of prior generation procedure. As shown in Fig.  where the hyperparameter u control the degree of freedom. It is worth mentioning that Eq. 7 is obtained with the trained CPDM model, and Eq. 8 can be solved by the Newton method ",7
4437,Content-Preserving Diffusion Model for Unsupervised AS-OCT Image Despeckling (vol7),,Acknowledgments,. This work was supported in part by ,7
4446,Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_50.,7
4450,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer (vol7),,Intra-operative:,"A stream of iKnife data is collected during a BCS case (27 min) at Kingston Health Sciences Center. At the sampling rate of 1 Hz, a total of 1616 spectra are recorded. Each spectrum is then labeled based both on surgeons comments during the operation and post-operative pathology report. Preprocessing: Each spectrum is converted to a hierarchical graph as illustrated in Fig. ",7
4451,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer (vol7),,Graph Transformer Network:,"The GTN consists of a node embedding layer, L Graph Transformer Layers (GTL), a node aggregation layer, multiple dense layers, and a prediction layer  where Q k,l , K k,l , and V k,l are trainable linear weights. The weights w kl ij defines the k-th attention that is paid by node j to update node i at layer l. The concatenation of all H attention heads multiplied by trainable parameters O l generates final attention ĥl+1 i , which is passed through batch normalization and residual layers to update the node features for the next layer. After the last GTL, features from all nodes are aggregated, then passed to the dense layers to construct a final prediction output.",7
4452,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer (vol7),,Evidential Graph Transformer:,"Evidential deep learning provides a welldefined theoretical framework to jointly quantify classification prediction and uncertainty modeling by assuming the class probability follows a Dirichlet distribution  In the context of surgical margin assessment, the attentions reveal the relevant metabolic ranges to cancerous tissue, while uncertainty helps identify and filter data with unseen pathology. Specifically, the attentions affect the predictions by selectively emphasizing the contributions of relevant nodes, enabling the model to make more accurate predictions. On the other hand, the spread of the outcome probabilities as modeled by the Dirichlet distribution represents the confidence in the final predictions. Combining the two provides interpretable predictions along with the uncertainty estimation. Mathematically, the Dirichlet distribution is characterized by α = [α 1 , ..., α C ] where C is the number of classes in the classification task. The parameters can be estimates as α = f (x i |Θ) + 1 where f (x i |Θ) is the output of the Evidential Graph Transformer parameterized by Θ for each sample(x i ). Then, the expected probability for the c-th class p c and the total uncertainty u for each sample (x i ) can be calculated as p c = αc S , and u = C S , respectively, where S = C c=1 α c . To fit the Dirichlet distribution to the output layer of our network, we use a loss function consisting of the prediction error L p i and the evidence adjustment where λ is the annealing coefficient to balance the two terms. L p i can be crossentropy, negative log-likelihood, or mean square error , while L e i (Θ) is KL divergence to the uniform Dirichlet distribution ",7
4453,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer (vol7),,Network/Graph Ablation:,We explore the hyper-parameters of the proposed model in an extensive ablation study. The attention parameters include the number of attention heads ( 1-15 with step size of 2) and the number of hidden features ,7
4454,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer (vol7),,Ex-vivo Evaluation:,"The performance of the proposed network is compared with 3 baseline models including GTN, graph convolution network  Clinical Relevance: Hormone receptor status plays an important role in determining breast cancer prognosis and tailoring treatment plans for patients ",7
4455,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer (vol7),,Intra-operative Deployment:,"To explore the intra-operative capability of the models, we deploy the ensemble models of the proposed method as well as the baselines from the cross-validation study to the BCS iKnife stream.",7
4457,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer (vol7),,Intra-operative Deployment:,The raw intra-operative iKnife data (y-axis is m/z spectral range and x-axis is the surgery timeline) along with the temporal reference labels extracted from surgeon's call-outs and pathology report are shown in Fig. ,7
4474,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes (vol7),,Type,"Selector We can see that hybrid methods outperform radiomic and deep methods in general. Our method, RIDL, achieves the best results across all metrics however and improves AUC by 1.1% over the baseline method (86.9% v.s. 85.8%) and 3.5% over the best radiomics approach (86.9% v.s. 83.4%).",7
4477,Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 15.,7
4483,Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction (vol7),,Statistical Analysis.,"We compare the similarity between the images reconstructed by MIALSRTK and NiftyMIC using both default and optimized parameters. In this experiment, no reference images are available. Statistical significance of the performance difference is tested using a paired Wilcoxon rank sum test (p < 0.05 for statistical significance).",7
4485,Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction (vol7),,MIALSRTK,Gestational Age-Based Analysis. Since the human brain undergoes drastic morphological changes throughout gestation ,7
4488,Simulation-Based Parameter Optimization for Fetal Brain MRI Super-Resolution Reconstruction (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 32.,7
4497,CenterlinePointNet++: A New Point Cloud Based Architecture for Coronary Artery Pressure Drop and vFFR Estimation (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 73.,7
4509,Shape-Based Pose Estimation for Automatic Standard Views of the Knee (vol7),,(1) Intensity-based multi-task classification and regression module:,"For simultaneous in-plane rotation regression, view recognition, and laterality classification, an EfficientNet-B0 feature extractor  (2) Shape-based pose regression: Following surgical characteristics for recognizing correct standard views of the knee, a view-independent shape-based pose regression framework was developed. The architecture is based on a 2D U-Net ",7
4515,Shape-Based Pose Estimation for Automatic Standard Views of the Knee (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 45.,7
4522,A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference (vol7),,Spatially Adaptive Training:,"While sharp gradients in PDE solutions are challenging for PINNs  where w h > 1 and s is another threshold to be tuned. With the above strategy, PDE residual is further modified to: Sequential PINN Training: Even when propagation causality and sparse regions of sharp gradients are respected, PINN cannot solve for arbitrarily long time domains because the loss landscape becomes increasingly complex as N T increases. We thus further utilize a sequential learning method where we first uniformly discretize the time domain [0, T ] into n segments, and then train the PINN across these segments sequentially as: where PDE solutions obtained from previous time segments become the initial residual for training the PINN for the next time segment. The complete PDE solution for the entire time domain is obtained at the end of the training.",7
4525,A Spatial-Temporally Adaptive PINN Framework for 3D Bi-Ventricular Electrophysiological Simulations and Parameter Inference (vol7),,The Effectiveness of Sequential Training:,"We then compared the computation cost and the accuracy of the PDE solution achieved by the presented PINNs with and without sequence learning, across different lengths of time domains as summarized in Table  PINN for Supporting PDE Parameter Inference: Finally, we tested the feasibility of the presented PINNs to support parameter estimation of the AP model. We assumed measurements of TMP solutions to be available and considered unknown parameter γ in the AP model (Eq. 1) due to its relatively large influence on the PDE solution ",7
4530,Improving Automatic Fetal Biometry Measurement with Swoosh Activation Function (vol7),,FTD Dataset:,"The dataset used in this study consists of 1111 2D-US images acquired during the second trimester of pregnancies and confirmed by boardcertified ultrasonographers to be suitable for measuring FTD  The intraclass correlation coefficient (ICC) score was computed using IBM TM SPSS TM version 28, with the ICC configuration being Two-Way Random and Absolute Agreement  HC18 Dataset: HC18 dataset is available on the Grand Challenge website ",7
4538,UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation (vol7),,Multi-resolution Exploration and Integration.,"The localization map for each image typically provides discriminative object parts, which is insufficient to provide supervision for the segmentation task. As shown in Fig.  where Âb m and Âf m represent the background and foreground probability of Âm , respectively. w m is the weight map for Âm , and P UM is the UM-CAM for the target.",7
4542,UM-CAM: Uncertainty-weighted Multi-resolution Class Activation Maps for Weakly-supervised Fetal Brain Segmentation (vol7),,Stage2: Training Segmentation Model with UM-CAM and SPL.,"To investigate the effectiveness of SPL, we compared it with several segmentation models: 1) Grad-CAM (baseline): only using the pseudo mask generated from Grad-CAM to train the segmentation model, 2) UM-CAM: only using UM-CAM as supervision for the segmentation model, 3) SPL: only using SPL as supervision, 4) UM-CAM+SPL: our proposed method using UM-CAM and SPL supervision for the segmentation model. Quantitative evaluation results in the second section of Table ",7
4549,Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images (vol7),,Polar Feature Extractor Module (PFEM):,"To extract shallow features in different views, we propose PFEM, which consists of a multi-kernel atrous convolution module (MKAC), a multi-kernel pooling module (MKPM), and a convolutional block attention module (CBAM)  (2)",7
4550,Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images (vol7),,Polar Region Importance Module (PRIM):,"To calculate the region importance, we implement PRIM by applying an average pooling after a Grad-CAM ",7
4552,Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images (vol7),,Model ACC AUROC Kappa,"ResNet-34  The width of the transformed images was resized to 224 pixels. Five-fold crossvalidation was employed to fully utilize the data and make the results more reliable. Since there is no standard way to convert existing prior knowledge into matrices, for prior knowledge, we manually generated a 4 × 2 weight matrix according to the study ",7
4553,Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection in OCTA Images (vol7),,Evaluation and Interpretability Assessment:,"We evaluate the performance of the model on the test set using the accuracy score (ACC), area under the receiver operating characteristic (AUROC), and kappa. To evaluate the performance, we compared our method with several state-of-the-art methods in the computer vision field and one in the AD detection field. Table  For the sake of simplicity, here we take the EDTRS grid for analysis. As shown in Fig.  Ablation Study: To evaluate the effectiveness of the polar transformation and Polar-Net, we performed an ablation study. To validate the proposed Polar-Net, we removed the PFEM. The results are shown at the bottom of Table  Extended Experiment: To further verify our detection method's stability and generalisability, we conducted an additional experiment on a public dataset OCTA-500 ",7
4558,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism (vol7),,ADC =ln (S,"where S l and S h represent the image signals obtained from lower b value b l and higher b h , f θ l and f θ h represent the corresponding neural networks for DWI with a lower and higher b value. In the multi-sequence attention module, a channel-based attention mechanism is designed to automatically apply weights (A s ) to feature maps (F concat ) from different sequences to obtain a refined feature map (F concat ), as shown in Eq. 3. The input feature maps (F concat ) go through the maximum pooling layer and the average pooling layer respectively, and then are added element-wise after passing through the shared fully connected neural network, and finally the weight map A s is generated after passing through the activation function, as shown in Eq. 4. where ⊗ represents element-wise multiplication, ⊕ represents element-wise summation, σ represents the sigmoid function, θ fc represents the corresponding network parameters of the shared fully-connected neural network, and AvgP ool and M axP ool represent average pooling and maximum pooling operations, respectively. In the synthesis process, the generator G tries to generate an image according to the input multi-sequence MRI (d 1 , d 2 , d 3 , d 4 , t 1 ), and the discriminator D tries to distinguish the generated image G(d 1 , d 2 , d 3 , d 4 , t 1 ) from the real image y, and at the same time, the generator tries to generate a realistic image to mislead the discriminator. The generator's objective function is as follows: and the discriminator's objective function is as follows: where pro data (d 1 , d 2 , d 3 , d 4 , t 1 ) represents the empirical joint distribution of inputs d 1 (DW I b0 ), d 2 (DW I b150 ), d 3 (DW I b800 ), d 4 (DW I b1500 ) and t 1 (T1weighted MRI), λ 1 is a non-negative trade-off parameter, and l 1 -norm is used to measure the difference between the generated image and the corresponding ground truth. The architecture of the discriminator includes five convolutional layers, and in each convolutional layer, 3 × 3 filters with stride 2 are used. Each filter is followed by batch normalization, and after batch normalization, the activation function LeakyReLU (with a slope of 0.2) is used. The numbers of filters are 32, 64, 128, 256 and 512, respectively.",7
4564,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 8.,7
4576,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma (vol7),,Image,"(ii) Clarity of tumor-to-normal tissue interface. The clarity of tumor-normal tissue interface is critical for tumor delineation, which directly affects the delineation outcomes. Oncologists were asked to use a 5-point Likert scale ranging from 1 (poor) to 5 (excellent) to evaluate the clarity of tumor-to-normal tissue interface. Paired two-tailed t-test (with a significance level of p = 0.05) was applied to analyses if the scores obtained from real patients and synthetic patients are significantly different. (iii) Veracity of contrast enhancement in tumor invasion risk areas. In addition to the critical tumor-normal tissue interface, the areas surrounding the NPC tumor will also be considered during delineation. To better evaluate the veracity of contrast enhancement in VCE-MRI, we selected 25 tumor invasion risk areas according to  The Jaccard index (JI)  where R CE and R VCE represents the set of risk areas that recorded from CE-MRI and corresponding VCE-MRI, respectively. JI measures similarity of two datasets, which ranges from 0% to 100%. Higher JI indicates more similar of the two sets. (iv) Efficacy in primary tumor staging. A critical RT-related application of CE-MRI is tumor staging, which plays a critical role in treatment planning and prognosis prediction  Primary GTV Delineation. GTV delineation is the foremost prerequisite for a successful RT treatment of NPC tumor, which demands excellent precision  To mimic the real clinical setting, contrast-free T1w, T2w MRI and corresponding CT of each patient were imported into the Eclipse system since sometimes T1w and T2w MRI will also be referenced during tumor delineation. Due to both real patients and synthetic patients were involved in delineation, to erase the delineation memory of the same patient, we separated the patients to two datasets, each with the same number of patients, both two datasets with mixed real patients and synthetic patients without overlaps (i.e., the CE-MRI and VCE-MRI from the same patient are not in the same dataset).When finished the first dataset delineation, there was a one-month interval before the delineation of the second dataset. After the delineation of all patients, the Dice similarity coefficient (DSC)  Dice Similarity Coefficient (DSC). DSC is a broadly used metric to compare the agreement between two segmentations  where C CE and C VCE represent the contours delineated from real patients and synthetic patients, respectively.",7
4577,Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma (vol7),,Hausdorff Distance (HD).,"Even though DSC is a well-accepted segmentation comparison metric, it is easily influenced by the size of contours. Small contours typically receive lower DSC than larger contours  where d(x, C VCE ) and d(y, C CE ) represent the distance from point x in contour C CE to contour C VCE and the distance from point y in contour C VCE to contour C CE . (i) Distinguishability between CE-MRI and VCE-MRI. The overall judgement accuracy for the MRI volumes was 53.33%, which is close to a random guess accuracy (i.e., 50%). For Institution-1, 2 real patients were judged as synthetic and 1 synthetic patient was considered as real. For Institution-2, 2 real patients were determined as synthetic and 4 synthetic patients were determined as real. For Institution-3, 2 real patients were judged as synthetic and 3 synthetic patients were considered as real. In total, 6 real patients were judged as synthetic and 8 synthetic patients were judged as real. (ii) Clarity of tumor-to-normal tissue interface. The overall clarity scores of tumorto-normal tissue interface for real and synthetic patients were 3.67 with a median of 4 and 3.47 with a median of 4, respectively. No significant difference was observed between these two scores (p = 0.38). The average scores for real and synthetic patients were 3.6 and 3, 3.6 and 3.8, 3.8 and 3.6 for Institution-1, Institution-2, and Institution-3, respectively. 5 real patients got a higher score than synthetic patients and 3 synthetic patients obtained a higher score than real patients. The scores of the other 7 patient pairs were the same. (iii) Veracity of contrast enhancement in tumor invasion risk areas. The overall JI score between the recorded tumor invasion risk areas from CE-MRI and VCE-MRI was 74.06%. The average JI obtained from Institution-1, Institution-2, and Institution-3 dataset were similar with a result of 71.54%, 74.78% and 75.85%, respectively. In total, 126 risk areas were recorded from the CE-MRI for all of the evaluation patients, while 10 (7.94%) false positive high risk invasion areas and 9 (7.14%) false negative high risk invasion areas were recorded from VCE-MRI. (iv) Efficacy in primary tumor staging. A T-staging accuracy of 86.67% was obtained using VCE-MRI. 13 patient pairs obtained the same staging results. For the Institution-2 data, all synthetic patients observed the same stages as real patients.",7
4583,UWAT-GAN: Fundus Fluorescein Angiography Synthesis via Ultra-Wide-Angle Transformation Multi-scale GAN (vol7),,2.1) and a fusion module works on result of both level generators (Sect. 2.2).,"Then the Attention Transmit Module is put forward to improve the U-Net-like architecture (Sect. 2.3). Additionally, we provide a comprehensive description of up-down sampling process and architecture of multi-scaled discriminators (Sec. 2.4). Eventually, we discuss the proposed loss function terms and their impacts in detail (Sect. 2.5). ",7
4602,Automated CT Lung Cancer Screening Workflow Using 3D Camera (vol7),,Method (lateral),"Mean then measured the performance of our model before and after different degrees of real-time refinement, using the same optimizer and learning rate. We report the comparative results in Table  We observed that our method largely outperforms the direct regression baseline with a mean lateral error 40% lower and a 90 th percentile lateral error over 30% lower. Bringing in real-time refinement greatly improves the results with a mean lateral error over 40% and a 90 th percentile lateral error over 20% lower than before refinement. AP profiles show similar results with a mean AP error improvement of nearly 40% and a 90 th percentile AP error improvement close to 30%. When using our proposed method with a 20 mm window refinement, our proposed approach outperforms the direct regression baseline by over 60% for lateral profile and nearly 80% for AP. Figures 3 highlights the benefits of using real-time refinement. Overall, our approach shows best results with an update frequency of 20 mm, with a mean lateral error of 15.93 mm and a mean AP error of 10.40 mm. Figure  Finally, we evaluated the clinical relevancy of our approach by computing the relative error as described in the International Electrotechnical Commission (IEC) standard IEC 62985:2019 on Methods for calculating size specific dose estimates (SSDE) for computed tomography  where: -Ŵ ED(z) is the predicted water equivalent diameter -W ED(z) is the ground truth water equivalent diameter z is the position along the craniocaudal axis of the patient. IEC standard states the median value of the set of Δ REL (z) along the craniocaudal axis (noted Δ REL ) should be below 0.1. Our method achieved a mean lateral Δ REL error of 0.0426 and a mean AP Δ REL error of 0.0428, falling well within the acceptance criteria.",7
4606,A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy (vol7),,Normalizing Flow,Planner Planner,7
4607,A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy (vol7),,Latent Flow Model,Conv Norm,7
4608,A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy (vol7),,Conv ReLU Norm Residual,Basic Block ,7
4609,A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy (vol7),,Conditional Variational Autoencoder:,"A VAE is a probabilistic generative model/network  Our conditional flow VAE (cVAE-NF) is a graph-convolutional network which takes as input a triangular surface mesh representation of an anatomical structure of interest, i.e., the Left Ventricle (LV) in this study, and its associated covariates/conditioning variables, i.e., the patient demographic data and clinical measurements, such as gender, age, weight, blood cholesterol, etc., and outputs the reconstructed surface mesh. Each mesh is represented by a list of 3D spatial coordinates of its vertices and an adjacency matrix defining vertex connectivity (i.e. edges of mesh triangles). The encoder and decoder contain five residual graph-convolutional blocks, respectively. Each block comprises two Chebyshev graph convolutions, each of which is followed by batch normalisation and ELU activation. A residual connection is added between the input and the output of each graph-convolutional block. Hierarchical mesh down/up-sampling operations proposed in CoMA  Flexible Posterior Using Normalizing Flow: Vanilla cVAEs model the approximate posterior distribution using Gaussian distributions with a diagonal covariance matrix. However, such a unimodal distribution is a poor approximation of the complex true latent posterior distribution in most real-world applications (e.g. for shapes of the LV observed across a population), limiting the anatomical variability captured by the model. In this study, we introduce normalising flows to construct a flexible multi-modal latent posterior distribution by applying a series of differentiable, invertible/diffeomorphic transformations iteratively to the initial simple unimodal latent distribution. As shown in Fig.  where the det ∂f ∂z is the Jacobian determinant of f . Therefore, we can obtain a complex multi-modal density by composing multiple invertible mappings to transform the initial, simple and tractable density sequentially, as follows, The specific mathematical formulation of the normalising flow function is important and must be chosen with care to allow for efficient gradient computation during training, scalable inference, and efficiency in computing the determinant of the Jacobian. In this study, we leverage the planar flow in  where w ∈ R d , u ∈ R d and b ∈ R are learnable parameters; h(•) is a smooth element-wise non-linear function with derivative h (•) (we use tanh in our study) and z denotes the latent variables sampled from the posterior distribution. Therefore, we could compute the log determinant of the Jacobian term in O(D) time as follows: Finally, the network is trained by optimizing the modified ELBO based on Eq. 3: where, ln p(x|c) is the marginal log-likelihood of the observed data x (i.e. here x represents an LV graph/mesh), conditioned on the covariates of interest (i.e. patient demographics and clinical measurements) c; i is the steps of the normalizing flows. p(x|z i , c) is the likelihood of data parameterised by the decoder network, which reconstructs/predicts x given the latent variables z i , transformed by latent (planar) normalising flows, and the conditioning variables c; KL(q(z 0 |x) p(z i )) is the Kullback-Leibler divergence of the approximate posterior initial q(z 0 |x, c) from the prior, p(z) = N (z | 0, I).",7
4611,A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy (vol7),,Implementation Details:,"The framework was implemented using PyTorch on a standard PC with a NVIDIA RTX 2080Ti GPU. We trained our model using the AdamW optimizer with an initial learning rate of 1e-3 and batch size of 16 for 1000 epochs. The feature number for each graph convolutional block in the encoder was 16, 32, 32, 64, 64, and in reverse order in the decoder. The latent dimension was set at 16. The down/up-sampling factor was four, and we used a warm-up strategy ",7
4612,A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy (vol7),,Evaluation Metrics:,"We compared our model (cVAE-NF) with a traditional PCA-based SSM, two generative models without conditioning information including a vanilla VAE and a VAE with normalising flow (VAE-NF) and the vanilla cVAE. Comparison of the vanilla cVAE can also validate the performance of existing approaches  It is essential to capture the distribution of clinically relevant biomarkers (e.g. BPVol) in the synthesised virtual populations (VPs) based on the specified covariates/conditioning information available for real patients, in order to effectively replicate the inclusion/exclusion criteria used during trial design in ISTs. For example, the BPVol of women is known to be lower than that of men  To verify this, we generated VPs using cVAE and our method, conditioned on real patient data (covariates) from the UK Biobank. Figure ",7
4614,A Conditional Flow Variational Autoencoder for Controllable Synthesis of Virtual Populations of Anatomy (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 14.,7
4618,Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation (vol7),,Architecture of Context-Similarity Head.,"Although the target features generated by source encoder do not align with the source segmentor, features of the same classes tend to be in the same cluster while those of different classes are faraway, as shown in Fig.  ( Computing similarities between every pair of coordinates in a feature map is computationally costly. Thus, for each coordinate i, only similarities with coordinates j lying within the circle of radius r are considered in our implementation. Training of Context-Similarity Head. Given a target image x t , initial pseudolabels and uncertainty mask can be obtained from the source model f s and x t , following previous work  (2) , ω ∈{foreground (fg), background (bg)}, In Eq. 2, Monte Carlo Dropout  Binary similarity label is then obtained. For two coordinates i and j, similarity label S * ij is 1 if pseudo-labels ŷi = ŷj , and 0 otherwise. Note only reliable pseudolabels are considered to provide less noisy supervision. The context-similarity head is trained with S * . To address the class imbalance issue, the loss of each type of similarity (fg-fg, bg-bg, fg-bg) is calculated and aggregated ",7
4623,Context-Aware Pseudo-label Refinement for Source-Free Domain Adaptive Fundus Image Segmentation (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 58.,7
4643,Medical Phrase Grounding with Region-Phrase Context Contrastive Alignment (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_35.,7
4647,Speech Audio Synthesis from Tagged MRI and Non-negative Matrix Factorization via Plastic Transformer (vol7),,Ĝi,"Then, these global dependencies are broadcasted to every local token: By adding H local i and H global i , each token can benefit from both local and global features, while maintaining linear complexity with respect to the input size. This brings noticeable improvements with negligible FLOPs increment. However, the sequentially proportional patch merging used in  We cascade four PLT layers with SSPP as our encoder to extract the feature representation f i ∈ R 8×8×d . For the decoder, we adopt a simple 2D CNN with three deconvolutional layers to synthesize the spectrogram si .",7
4662,VF-HM: Vision Loss Estimation Using Fundus Photograph for High Myopia (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_61.,7
4670,AirwayFormer: Structure-Aware Boundary-Adaptive Transformers for Airway Anatomical Labeling (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 37.,7
4675,Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention (vol7),,Incomplete Multimodal,"Learning. Not all cases have complete modalities for the three image modalities (i.e., OCT, SLO, and Ultrasound). For the missing modalities, one possible way is to simply represent them by 0 values  The proposed attentional masks are easy to implement. As shown in Eq. (  in which, Q, K, and V are queries, keys, and values obtained from tokens, respectively. d z is the projection dimension. To avoid interactions between irrelevant tokens, the masked self-attention is computed as: in which, M is the mask matrix. For each element in M , it will be 0 if the interactions should be included, or it will be negative infinity to avoid unnecessary interactions. By adding negative infinity, the results of softmax will be very close to 0. Figure ",7
4681,Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention (vol7),,Acknowledgements,. This work is partially supported by ,7
4682,Incomplete Multimodal Learning for Visual Acuity Prediction After Cataract Surgery Using Masked Self-Attention (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 69.,7
4691,"FEDD - Fair, Efficient, and Diverse Diffusion-Based Lesion Segmentation and Malignancy Classification (vol7)",,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 26.,7
4695,Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks (vol7),,Label,Generator. An autoencoder-based model like U-Net ,7
4700,Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks (vol7),,Training and Test.,"All experiments were conducted on a workstation with an NVIDIA RTX 8000 and using TensorFlow 2.8. For comparisons, we implemented not only EKGAN but also Pix2Pix ",7
4704,Twelve-Lead ECG Reconstruction from Single-Lead Signals Using Generative Adversarial Networks (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 18.,7
4708,HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis (vol7),,Spatial Residual Attention and Deep Feature Extraction. The mask branch output M (j),"i has been aligned with the trunk branch output T (j) i in terms of the field of view. Therefore, M (j) i is multiplied in an element-wise manner with T (j) i as the attention mechanism. Besides, to avoid potential performance drop with attention, residual learning is further considered  i . Note that the attention is not performed directly on deep features because the ROI considered has explicit physical meanings instead of being ""hidden"". The remaining layers of pre-trained ResNet18 (excluding the top three layers used previously) are employed to further extract deep features, as shown in Fig.  } are eventually generated, where F (j) i ∈ R 512 . These features capture the context-aware semantic information of MRI slices.",7
4715,HACL-Net: Hierarchical Attention and Contrastive Learning Network for MRI-Based Placenta Accreta Spectrum Diagnosis (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 29.,7
4717,Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning (vol7),,Background,"In this work, we propose a novel SFDA framework for cross-modality medical image segmentation. Our framework contains two sequentially conducted stages, i.e., Prototype-anchored Feature Alignment (PFA) stage and Contrastive Learning (CL) stage. As previous works ",7
4721,Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning (vol7),,Query Samples.,"During training, we employ the per-pixel entropy as uncertainty metric  where H(•) is the entropy of the input probabilities and γ c is the entropy threshold for class c. Here we set γ c as the α c -th percentile of all the entropy values of pixels assigned a pseudo label c. Positive Prototypes. The positive prototype is the same for all query pixels from the same class. Instead of using the center of query samples like  where r l is the low rank threshold and is set to 3 in our task. With the above definition, we have the pixel-level contrastive loss as: where K is the number of query samples, and z c,k ∈ P c denotes the k-th query sample from class c. Each query sample is paired with a positive prototype z + c and N negative samples z - c,k,j ∈ N c .",7
4725,Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning (vol7),,Supplementary Information,"The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 1. Q. Yu et al. ✔ 70.1 ± 6.9 52.9 ± 14.2 65.7 ± 12.5 70.9 ± 13.2 64.9 ± 8. of 86.1% and the lowest average ASSD of 1.4. Moreover, compared with recent UDA methods, our method obtains competitive results on average Dice and ASSD, which may be due to the use of unreliable predictions. As for ""CT to MRI"" direction, our method similarly shows great superiority on most organs as well, achieving the best performance in terms of both the average Dice (89.2%) and ASSD (1.3) among all SFDA methods. Figure  Ablation Study. In Fig. ",7
4728,Optimizing the 3D Plate Shape for Proximal Humerus Fractures (vol7),,right).,"Bone Registration for Optimal Plate Transfer. Once the optimal plate is defined on the template bone, the goal is to register any new bone to the template, by preserving the relevant anatomic regions for the plate fixation. To do so, we adapt the registration technique designed for vertebrae  We first register each bone of set A by optimizing their Eqs. 1, 2 and 3 from  Second, we register the whole dataset using the learned bone shape model by minimizing where S denotes the scanned bone mesh, t and r are, respectively, the 3D rigid translation and rotation applied to the bone mesh vertices, and F is a 3D pervertex offset applied to each mesh vertex. E p2m (S, .) is the point-to-triangle distance between the scan vertices and the mesh triangles. The function Δ(T) is the mesh Laplacian operator, which is used to regularize the offsets F. We minimize Eq. (  Due to the elongated shape of the humeri and their axial rotation similarity, the registrations B i can contain sliding, i.e. the same vertex does not correspond precisely to the same anatomic location on two different bones. To correct this, we perform a second pass of registration using smooth shells  To validate the bone correspondences we define anatomic regions on the bone template T, and transfer them to all registrations B i on a per-vertex index basis. Figure  Positioning a Plate on a Bone. Now that we can create personalized plates, we want to study if a created plate can be used for surgery on another bone. For that, given an arbitrary plate P i and bone registration B j , we need to position the plate and determine if it fits. We automate the plate positioning by minimizing the distance between the plate and bone fixation points. We start by computing an initial 3D rigid transformation r 0 , t 0 by optimizing Eq. 2 in Sup. Mat. and then obtaining the final positioning r f , t f by optimizing Eq. 3 in Sup. Mat. These equations enforce the fixation areas of the plate P i h to match B j h and P i s to match B j s (see Fig. ",7
4729,Optimizing the 3D Plate Shape for Proximal Humerus Fractures (vol7),,Plate Shapes Set.,A single plate design can not accommodate the whole population due to its morphological variance ,7
4731,Optimizing the 3D Plate Shape for Proximal Humerus Fractures (vol7),,left).,"Plate Set Coverage. One application of our design is that it can generate a plate set that could be preprinted in the hospital and be readily available for immediate use. Using our greedy algorithm on our bone dataset, a set P N =5 S made of the first 5 plates accommodates already 51.04% of the bones and P N =10 S accommodates 73.96% (Fig. ",7
4735,Optimizing the 3D Plate Shape for Proximal Humerus Fractures (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 46.,7
4745,Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_68.,7
4751,Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma (vol7),,2) LC loss:,"In order to fully exploit the co-occurrence probability of different molecular markers, we further devise the LC loss that constrains the similarity between any two output molecular markers F out i and F out j to approach their correspondent co-occurrence probability A j i . Formally, the LC loss is defined as In ",7
4757,Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_52.,7
4766,Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning (vol7),,Method b-values (s/mm,"It is worth noting that the agreement between the different methods (CSD vs. CSA, SFM vs. CSA, CSD vs. DL n , etc.) was also low. The confusion matrices for ΔGS agreement and the different methods can be found in Table  Angular Error -The agreement in terms of the number of peaks does not guarantee that the fibers follow the same orientation. Table  Apparent Fiber Density -The last column in Table ",7
4769,Robust Estimation of the Microstructure of the Early Developing Brain Using Deep Learning (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 28.,7
4770,Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_48.,7
4797,Semantic Difference Guidance for the Uncertain Boundary Segmentation of CT Left Atrial Appendage (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 12.,7
4808,Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset (vol7),,Acknowledgment. This research was supported by Australian Government Research,"Training Program (RTP) scholarship, and supported in part by a ",7
4809,Topology Repairing of Disconnected Pulmonary Airways and Vessels: Baselines and a Dataset (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 36.,7
4817,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition (vol7),,Ablation Study:,"We perform ablation studies to analyze each component of our model, as shown in Table  Trap Set Debiasing: In Fig.  The graph shows that the ERM baseline performs better than our EPVT when the bias is low (0 and 0.3). However, this is because ERM relies heavily on spurious correlations between artifacts and class labels, leading to overfitting on the training set. As the bias degree increases, the correlation between artifacts and class labels decreases, and overfitting the train set causes the performance of ERM to drop dramatically on the test set with a significant distribution difference. In contrast, our EPVT exhibits greater robustness to different bias levels. Notably, our EPVT outperforms the ERM baseline by 9.4% on the bias 1 dataset. Prompt Weights Analysis: To verify whether our model has learned the correct domain prompts for target domain prediction, we analyze and plot the results in Fig.  We then calculate the Frechet distance ",7
4819,EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_24.,7
4823,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data (vol7),,Soft State Embeddings S.,"There is a correlation between labels, e.g. the lesions with indistinct borders tend to be malignant. Therefore, we hypothesize that the states (GT values) of some labels can be a context for helping predict the remaining labels. We use a soft state embedding method. Specifically, we first embed the positive and negative states into s p and s n , both ∈ R d , and then the final state embedding s i is the weighted sum of s p and s n as shown in Equation  Multi-label Inference with Transformer Encoder. In a transformer, each output token is the integration of all input tokens. Taking advantage of this structure, we use a transformer encoder to integrate image embeddings and label embeddings, and used the output label tokens to predict label value. Specifically, embedding set are the input tokens, the attention value α and output token e are computed as follows: where e i is from E, W q , W k and W v are weight matrices of query, key and value, respectively, W r and W o are transformation matrices, and b 1 and b 2 are bias vectors. This update procedure is repeated for L layers, where the e i are fed to the successive transformer layer. Finally, all e i which are label output tokens are fed into M independent FC layers for predicting value of each label. The states of unknown labels cannot provide context, thus, the information interaction between known labels and unknown labels may be weaken. To overcome this problem, we propose a Self-feedback Strategy (SFS) inspired by Recurrent Neural Networks (RNN) to enhance the interaction of labels.",7
4831,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2_49.,7
4852,Machine Learning for Automated Mitral Regurgitation Detection from Cardiac Imaging (vol7),,CUSSP Classification Results,"Fig.  We evaluated various configurations of the CUSSP model, to determine the relative benefits of different components. In the first configuration, the ResNet18 model was combined with a 3-layer MLP to train a classifier using the labeled training set after being trained in the Barlow-Twins network with the unlabeled dataset. During the classifier training, the cross-correlation loss from the Barlow-Twins network and the cross-entropy loss from the binary classification were weighted using three different configurations. For CUSSP-1 the crosscorrelation loss has a weight of 0.9, while the cross-entropy loss has a weight of 0.1. For CUSSP-2, the weights are 0.5 and 0.5, while for CUSSP-3 they are 0.1 and 0.9, respectively. Both CUSSP-1 and CUSSP-3 outperform CUSSP-2, though the performance is low, indicating the importance of fine-tuning, described below. In the second scenario, we fine-tuned the encoder with a siamese network to enhance the quality of the encoded representations after training the Barlow Twins network. To prevent overfitting of the model and to limit its capacity, we froze the parameters of all layers except the last block of the ResNet18 encoder when training the siamese network and the classifier. The resulting model, CUSSP-SIAM, showed a significant improvement in performance. In the final configuration CUSSP-SIAM-25, the number of frames in the training sequences was truncated from 50 frames to the 25 frames that correspond to the interval when mitral regurgitation occurs. The results are summarized in Table  CUSSP attains an F1 score of 0.69, and an ROC AUC of 0.88, outperforming the CNN-LSTM approach. This is dues to CUSSP's focus on the area around the valve to capture the blood flow through the valve, combining the advantages of Barlow Twins and contrastive learning. Meanwhile, the CNN-LSTM relies on attention, which does not seem to work as well. Additionally, using only the frames relevant to the task reduces the number of parameters and makes the model sample-efficient. This is essential for attaining high performance in the low label setting. In the future, we aim to use the pipeline on the large unlabeled dataset to scan for and adjudicate more MR cases. In conclusion, we present the first automated mitral regurgitation classification system using CMR imaging sequences. The CUSSP model we developed, trained with limited supervision, operates on 4CH CMR imaging sequences and attains an F1 score of 0.69 and an ROC AUC of 0.88, opening up the opportunity for large-scale screening for MR.",7
4858,Revolutionizing Space Health (Swin-FSR): Advancing Super-Resolution of Fundus Images for SANS Visual Assessment Technology (vol7),,Data use Declaration and Acknowledgment:,"The AMD and PALM dataset were released as part of REFUGE Challenge, PALM Challenge. The G1020 was published as technical report and benchmark ",7
4871,Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions (vol7),,Compared Methods and Metrics:,"We compare the following six methods: For different fusion stage strategies, a) B-EF Baseline of the early fusion ",7
4872,Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions (vol7),,Comparison and Analysis:,"We reported our algorithm with different methods on the GAMMA and in-house datasets in Table  As shown in Table  Understanding Uncertainty for Unimodality/Multimodality Eye Data: To make progress towards the multimodality ophthalmic clinical application of uncertainty estimation, we conducted unimodality and multimodality uncertainty analysis for eye data. First, we add more Gaussian noise with varying variances to the unimodality (Fundus or OCT) in the GAMMA and in-house datasets to simulate OOD data. The original samples without noise are denoted as in-distribution (ID) data. Figure ",7
4874,Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions (vol7),,Acknowledgements,. This work was supported by the ,7
4875,Reliable Multimodality Eye Disease Screening via Mixture of Student’s t Distributions (vol7),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43990-2 56.,7
4887,Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_44.,8
4889,PAS-Net: Rapid Prediction of Antibiotic Susceptibility from Fluorescence Images of Bacterial Cells Using Parallel Dual-Branch Network (vol8),,CNN→Transformer:,"The feature map is first aligned with the dimensions of the patch embedding by 1 × 1 convolution. Then, the feature resolution is adjusted using the downsampling module to complete the alignment of the spatial dimensions. Finally, the feature maps are summed with the patch embedding of the T-branch. Transformer→CNN: After going through the HMSA module and FFN, the patch embedding is fed back from the T-branch to the C-branch. An up-sampling module needs to be used first for the patch embedding to align the spatial scales. The patch embedding is then aligned to the number of channels of the feature map by 1 × 1 convolution, and added to the feature map of the C-branch.",8
4899,Exploring Brain Function-Structure Connectome Skeleton via Self-supervised Graph-Transformer Approach (vol8),,ScorePool-AE Module.,"In ScorePool-AE module, we adopt the encoder-decoder structure to implement the graph reconstruction task. The encoder is applied to extract node representations of the graph, then the ScorePool is used to obtain the contribution scores of nodes, and finally the decoder is used to reconstruct the node features of the graph. The encoder is based on the Ugformer  (1) where H (k) is the node representations of the graph at the k-th layer of UGformer. V is the set of all nodes of the graph. Inspired by the pooling operation of the GNN  where denotes the element-wise product and X denotes the input of the decoder. Reconstruction Target. Our TSGR framework reconstructs the graph by predicting the nodes features of the graph. The loss function is MSE between the feature matrix F of reconstructed graph nodes and the feature matrix F of input graph nodes.",8
4922,Diffusion-Based Data Augmentation for Nuclei Image Segmentation (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 57.,8
4931,FE-STGNN: Spatio-Temporal Graph Neural Network with Functional and Effective Connectivity Fusion for MCI Diagnosis (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_7.,8
4937,Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping (vol8),,Effectiveness of Each Module:,"We performed an ablation study on the HeLa dataset to investigate the effectiveness of the proposed module. We used random augmentation (i.e., random elastic transformation ",8
4938,Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping (vol8),,Robustness Against Missing Annotations:,We confirmed the robustness of the proposed method against missing annotations on the ES dataset. We changed the missing annotation rate from 0% to 30%. A comparison with the supervised method in terms of F1-score is shown in Fig.  Appearance of Generated Dataset: Fig. ,8
4940,Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 47.,8
4950,Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 64.,8
4957,Joint Representation of Functional and Structural Profiles for Identifying Common and Consistent 3-Hinge Gyral Folding Landmark (vol8),,Fig. 2. Visualization of the distribution of identified 3-hinges in the cerebral cortex,"Through adopting a joint representation of functional and structural profiles, stable sequences of length 65 are determined on the template and average 38 consistent 3hinges can be successfully identified on subjects. Figure  In order to show the consistency of the identified 3-hinges among different subjects subjectively, in Fig. ",8
4962,Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing (vol8),,Spatial Transformation,Module. Spatial normalization is implemented as a variant of the Spatial Transformer Network (STN) ,8
4967,Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing (vol8),,Metrics.,"For skull-stripping, we quantify performance using the Dice overlap coefficient (Dice), Sensitivity (Sens), Specificity (Spec), mean surface distance (MSD), residual mean surface distance (RMSD), and Hausdorff distance (HD), as defined elsewhere  Results. Figure  Table ",8
4970,Neural Pre-processing: A Learning Framework for End-to-End Brain MRI Pre-processing (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 25.,8
5004,Community-Aware Transformer for Autism Prediction in fMRI Connectome (vol8),,ASD vs HC,2) ,8
5009,Community-Aware Transformer for Autism Prediction in fMRI Connectome (vol8),,Interpretibility of Com-BrainTF (Qualitative Results,). We perform extensive experiments to analyze the interpretability of Com-BrainTF. The following results are discussed in detail: 1. Interpretation of the learned attention matrix: Fig. ,8
5013,Community-Aware Transformer for Autism Prediction in fMRI Connectome (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 28.,8
5032,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging (vol8),,Acc,Loss Acc Loss Acc Loss 325 0.948±0.026 0.342±0.126 0.922±0.035 0.363±0.098 0.934±0.034 0.302±0. 124  1 0.863±0.027 0.274±0.091 0.810±0.145 0.458±0.105 0.811±0.047 0.590±0.107 Input channel T2 T1 Pg,8
5040,Learning Asynchronous Common and Individual Functional Brain Network for AD Diagnosis (vol8),,Population-Level Common Pattern:,"As mentioned previously, the above FBN has large variability since it is constructed with the rs-fMRI data of each subject. Previous research indicates that human brains have similar topologies, which can be modeled as prior knowledge separately to regularize the model, helping to alleviate the issue of large variability. Therefore, we parameterize a data-driven common FBN module as a regularization, as shown in Fig. ",8
5056,Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI (vol8),,Direct Evaluation with Manual Ground truth Using HCP:,We first evaluated all competing methods and ablations on the 10 manually labelled subjects. Table  Test-Retest Using LOCAL: Table ,8
5057,Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI (vol8),,Best-Performing CNN:,Considering Table ,8
5058,Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI (vol8),,Group Study Using ADNI:,"We segmented the ADNI subjects with the bestperforming CNN, and computed volumes of the thalamic nuclei normalised by the intracranial volume (estimated with FreeSurfer). We then computed ROC curves for AD discrimination using a threshold on: (i) the whole thalamic volume; and (ii) the likelihood ratio given by a linear discriminant analysis (LDA, ",8
5060,Domain-Agnostic Segmentation of Thalamic Nuclei from Joint Structural and Diffusion MRI (vol8),,Acknowledgments. Work primarily funded by ARUK (IRG2019A003,). Additional support by the ,8
5069,Unsupervised Learning for Feature Extraction and Temporal Alignment of 3D+t Point Clouds of Zebrafish Embryos (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_58.,8
5077,Vertex Correspondence in Cortical Surface Reconstruction (vol8),,Results and Discussion:,"We compare the proposed V2CC method to state-ofthe-art models V2C, CFPP, and Topofit on the ADNI dataset with FreeSurfer's fsaverage6 right hemisphere templates as an input mesh. All methods were trained using the resampled ground truth meshes. For baseline models we used hyperparameters proposed by the original method. We show the results in the top part of Table ",8
5078,Vertex Correspondence in Cortical Surface Reconstruction (vol8),,Downstream Applications:,"We hypothesize that our meshes with vertex correspondence to the template can be directly used for downstream applications such as group comparisons or disease classification, without the need for postprocessing steps. We performed a group comparison of subjects with Alzheimer's disease (AD) and healthy controls of the ADNI test set, where we compare cortical thickness measures on a per-vertex level. We present a visualization of p-values in Fig. ",8
5080,Vertex Correspondence in Cortical Surface Reconstruction (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 31.,8
5091,Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_53.,8
5096,Wasserstein Distance-Preserving Vector Space of Persistent Homology (vol8),,2,". Thus, d p,× is a weighted combination of p-Wasserstein distances, and is simply the p-norm metric between vectors constructed by concatenating v B,i and v D,i . The normed vector space (M B × M D , d p,× ) is termed topological vector space (TopVS). Note the form of d p,× given in (4) results in an unnormalized mass after multiplying m and n by their reciprocals given in (  For a special case in which networks G 1 , G 2 , ..., G N have the same number of nodes, the vectors v B,i and v D,i are simply the original birth set B(G i ) and death set D(G i ), respectively, and the p-norm metric d p,× is expressed in terms of exact Wasserstein distances as",8
5098,Wasserstein Distance-Preserving Vector Space of Persistent Homology (vol8),,Potential Impact and,"Limitation. An open problem in neuroscience is identifying an algorithm that reliably extracts a patient's level of consciousness from passively recorded brain signals (i.e., biomarkers) and is robust to inter-patient variability, including where the signals are recorded in the brain. Conveniently, the anesthesia dataset is labeled according to consciousness state, and electrode placement (node location) was dictated solely by clinical considerations and thus varied across patients. Importantly, the relatively robust performance across patients suggests there are reliable topological signatures of consciousness captured by TopVS. The distinction between Wake and Sedated states involves relatively nuanced differences in connectivity, yet TopVS exploits the subtle differences in topology that differentiate these states better than the com-Fig.  peting methods. Our results suggest that the neural correlates of consciousness can be captured in measurements of brain network topology, a longstanding problem of great significance. Additionally, TopVS is a principled framework that connects persistent homology theory with practical applications. Our versatile vector representation can be used with various vector-based statistical and machine learning models, expanding the potential for analyzing extensive and intricate networks beyond the scope of this paper. While TopVS is limited to representing connected components and cycles, assessment of higher-order topological features beyond cycles is of limited value due to their relative rarity and interpretive challenges, and consequent minimal discriminitive power ",8
5109,Learnable Subdivision Graph Neural Network for Functional Brain Network Analysis and Interpretable Cognitive Disorder Diagnosis (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_6.,8
5132,CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention (vol8),,"MA((x, y), (x ref , y","where r i is the radius of the circle anchor A i , and r i,ref is the reference radius calculated by Deformable Circle Cross Attention. We modify standard deformable attention to deformable circle cross attention by applying radius information as constraint. Given an input feature map F ∈ R C×H×W , let i index a query element with content feature Z i and a reference point P i , the deformable circle cross attention feature is calculated by: where m indexes the attention head, k indexes the sampled keys. M and K are the number of multi-heads and the total sampled key number. W m ∈ R D×d , W m ∈ R d×D are the learnable weights and d = D/M . Attn mik denotes attention weight of the k th sampling point in the m th attention head. Δr mik and Δθ mik are radius offset and angle offset, r i,ref is the reference radius. In circle deformable attention, we transform the offset in polar coordinates to Cartesian coordinates so that the reference point ends up in the circle anchor. Rather than initialize the reference points by uniformly sampling within the rectangle as does Deformable DETR, we explore two ways to initialize the reference points within a circle, random sampling (CDA-r) and uniform sampling (CDA-c) (As in Fig. ",8
5136,CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention (vol8),,CC,", where C A and C B denotes two circles, and C C is the smallest circle containing these two circles. We show that gCIoU can bring consistent improvement on circular object detection. Figure  Circle Training Loss. Following DETR, i-th each element of the groundtruth set is y i = (l i , c i ), where l i is the target class label (which may be ∅) and c i = (x, y, r). We define the matching cost between the predictions and the groundtruth set as:  Finally, the overall loss is: where σ(i) is the index of prediction ŷ corresponding to the i-th ground truth y after completing the match. m i is the ground truth obtained by RoI Align ",8
5144,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction (vol8),,Permutation-equivariant Time Arrow Prediction Head:,"The time arrow prediction task has an inherent symmetry: In other words, h should be equivariant wrt. to permutations of the input. In contrast to common models (e.g. ResNet  with weight matrices L, G ∈ R c×c and a non-linear activation function σ. Note that L operates independently on each temporal axis and thus is trivially permutation equivariant, while G operates on the temporal sum and thus is permutation invariant. The last layer h L includes an additional global average pooling along the spatial dimensions to yield the final logits ŷ ∈ R 2 . Augmentations: To avoid overfitting on artificial image cues that could be discriminative of the temporal order (such as a globally consistent cell drift, or decay of image intensity due to photo-bleaching) we apply the following augmentations (with probability 0.5) to each image patch pair x 1 , x 2 : flips, arbitrary rotations and elastic transformations (jointly for x 1 and x 2 ), translations for x 1 and x 2 (independently), spatial scaling, additive Gaussian noise, and intensity shifting and scaling (jointly+independently).",8
5150,Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_52.,8
5175,Path-Based Heterogeneous Brain Transformer Network for Resting-State Functional Connectivity Analysis (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_32.,8
5179,DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data (vol8),,LSTM for Temporal Seizure Detection:,"We use a bidirectional LSTM to capture evolving patterns in the global encodings of the one-second EEG windows, i.e., {h t 0 } T t=1 . We use a single LSTM layer with 100 hidden units to process the global encodings and capture both long-term and short-term dependencies. The output of the LSTM is passed into a linear layer, followed by a softmax function, to generate window-wise predictions Ŝt ∈ [0, 1]. Here, Ŝt represents the posterior probability of seizure versus baseline activity at window t.",8
5180,DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data (vol8),,Attention-Weighted Multi-Instance Pooling for SOZ Localization:,We treat the localization task as a multi-instance learning problem to predict a channel-wise posterior distribution for the SOZ vector {y i } 19  i=1 by computing a weighted average of the hidden representations from the transformer. We first map the channel-wise encodings {h t i } 19 i=1 ∈ R 200 to scalars ŷt i using the same linear layer across channels. We use the predicted seizure probability Ŝt as our attention to compute the final SOZ prediction as follows: where σ(•) is the sigmoid function. The final patient-level predictions are obtained by averaging ŷi across all seizure recordings for that patient.,8
5183,DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data (vol8),,Baseline Comparisons:,We compare the performance of DeepSOZ with one model ablation and four state-of-the-art methods from the literature. Our ablation replaces the attention-weighted multi-instance pooling in DeepSOZ with a standard maxpool operation within the prediction seizure window (DeepSOZmax). Our baselines consist of the CNN-BLSTM model for seizure detection developed by ,8
5184,DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data (vol8),,Data and Preprocessing:,"We validate DeepSOZ on 642 EEG recordings from 120 adult epilepsy patients in the publicly available Temple University Hospital (TUH) corpus  Seizure Detection Performance: Table  The TGCN and CNN-BLSTM baselines achieve notably worse AU-ROC values, establishing the power of a transformer encoder in extracting more meaningful features. SZTrack is trained using the published strategy in  SOZ Localization Performance: Table ",8
5186,DeepSOZ: A Robust Deep Model for Joint Temporal and Spatial Seizure Onset Localization from Multichannel EEG Data (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 18.,8
5188,Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning (vol8),,Motivation:,"To improve WSI-level analysis, we explore end-to-end training of the entire network using SSL pretrained ViTs. To achieve this, we use the patch batching and gradient retaining techniques in  To address the subpar performance of SSL-pretrained vision transformers, we utilize the prompt tuning techniques. Initially proposed in natural language processing, a prompt is a trainable or a pre-defined natural language statement that is provided as additional input to a transformer to guide the neural network towards learning a specific task or objective  In this paper, we propose a novel framework, Prompt-MIL, which uses prompts for WSI-level classification tasks within an MIL paradigm. Our contributions are: -Fine-tuning: Unlike existing works in histopathology image analysis, Prompt-MIL is fine-tuned using prompts rather than conventional full finetuning methods. -Task-specific representation learning: Our framework employs an SSL pretrained ViT feature extractor with a trainable prompt that calibrates the representations making them task-specific. By doing so, only the prompt parameters together with the classifier, are optimized. This avoids potential overfitting while still injecting task-specific knowledge into the learned representations. Extensive experiments on three public WSI datasets, TCGA-BRCA, TCGA-CRC, and BRIGHT demonstrate the superiority of Prompt-MIL over conventional MIL methods, achieving a relative improvement of 1.49%-4.03% in accuracy and 0.25%-8.97% in AUROC by using only less than 0.3% additional parameters. Compared to the conventional full fine-tuning approach, we finetune less than 1.3% of the parameters, yet achieve a relative improvement of 1.29%-13.61% in accuracy and 3.22%-27.18% in AUROC. Moreover, compared to the full fine-tuning approach, our method reduces GPU memory consumption by 38%-45% and trains 21%-27% faster. To the best of our knowledge, this is the first work where prompts are explored for WSI classification. While our method is quite simple, it is versatile as it is agnostic to the MIL scheme and can be easily applied to different MIL methods. Our code is available at https://github.com/cvlab-stonybrook/PromptMIL.",8
5199,Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images (vol8),,Feature Extraction and Training Configuration.,"We use an nn-Unet  The lesions are connected using a k-nearest neighbor algorithm with k = 5. Further, these connections (edges) are weighted using τ = 0.01 (Eq. 1). Two message-passing layers with hidden dimensions of 64 and 8, respectively, process the generated graph to enrich lesion features. Next, the enriched features are passed through the SPM. The SPM uses a learnable projection vector p ∈ R 8 and sigmoid activation to learn the importance score. Based on this importance score, a mask is produced to select r = 0.5 (i.e., 50%) of the highest-scoring lesions and discard the rest. Next, a sum aggregation is used as the readout function. These aggregated features are passed through 2 feed-forward layers with hidden dimensions of size 8. Finally, the features are passed to a sigmoid function to obtain the final prediction. The model is trained for 300 epochs using AdamW optimizer  Evaluation Strategy, Classifier, and Metrics. We report our results on MS inflammatory disease activity prediction for the clinically relevant one and twoyear intervals ",8
5201,Self-pruning Graph Neural Network for Predicting Inflammatory Disease Activity in Multiple Sclerosis from Brain MR Images (vol8),,Analysis of Hyperparameters.,"The retention ratio r and the number of neighbors k used for building the graph are the two critical hyperparameters in our proposed framework. We discuss the effect of the retention ratio r here and defer discussion about k to the appendix. Effect of Retention Ratio r. The retention ratio r ∈ (0, 1] controls the fraction of lesions retained after the self-pruning module. If we set its value to 1, all the lesions are retained for the final prediction and thus, bypassing the self-pruning module. Any other value implies that we ignore at least a few lesions in the readout layer. Since the number of lesions can vary across graphs, we retain (N.r) lesions after the self-pruning layer. To find the optimal r, we test our model with r between 0.1 and 1.0. The results are summarized in Fig. ",8
5205,Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning (vol8),,Attention Weights Extraction.,"Based on the assumption that not all phenotypes are equally important for the construction of the graph, we train a MLP g θ , with parameters θ, which takes as input both the non-imaging features q i and the imaging features s i for every subject i and outputs an attention weight vector a ∈ R Q+S , where every element of a corresponds to a specific phenotype. Intuitively, we expect that the features that are relevant to brain age estimation will get attention weights close to 1, and close to 0 otherwise. Since we are interested in the overall relevance of the phenotypes for the task, the output weights need to be global and apply to all of the nodes. To do so, we average the attention weights across subjects and normalize them between 0 and 1. Edge Extraction. The weighted phenotypes for each subject i are calculated as f i = a (q i s i ), where f i ∈ R Q+S , a are the attention weights produced by the MLP g θ , (• •) denotes the concatenation function between two vectors and denotes the Hadamard product. We further define the probability p ij (f i , f j ; θ, t) of a pair of nodes (i, j) ∈ V to be connected in Eq. (  where t is a learnable parameter and d is a distance function that calculates the distance between the weighted phenotypes of two nodes. To keep the memory cost low, we create a sparse k-degree graph. We use the Gumbel-Top-k trick  Optimization. The extracted graph is used as input, along with the imaging features X, which are used as node features, to a GCN g ψ , with parameters ψ, which comprises of a number of graph convolutional layers, followed by fully connected layers. The pipeline is trained end-to-end with a loss function L that consists of two components and is defined as in Eq. (  The first component, L GCN , is optimizing the GCN, g ψ . For regression we use the Huber loss  The second component, L graph , optimizes the MLP g θ , whose output are the phenotypes' attention weights. However, the graph is sparse with discrete edges and hence the network cannot be trained with backpropagation as is. To alleviate this issue, we formulate our graph loss in a way that rewards edges that lead to correct predictions and penalize edges that lead to wrong predictions. Inspired by  where ρ(•, •) is the reward function which is defined in Eq. (4): Here ε is the null model's prediction (i.e. the average brain age of the training set). Intuitively, when the prediction error |y ig ψ (x i )| is smaller than the null model's prediction then the reward function will be negative. In turn, this will encourage the maximization of p ij so that L graph is minimized.",8
5211,Multimodal Brain Age Estimation Using Interpretable Adaptive Population-Graph Learning (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 19.,8
5222,Prompt-Based Grouping Transformer for Nucleus Detection and Classification (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_55.,8
5243,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model (vol8),,Loss Functions and Model Convergence:,"The proposed iterative model aims to learn a mapping from the post-contrast & pre-contrast images to the synthesized low-dose images P low and is trained with the true 10% low-dose image P low as the ground truth. We used the L 1 and structural similarity index measure (SSIM) losses. To tackle the problem of gradient explosion or vanishing, ""soft labels"" are generated using linear scaling. These ""soft labels"" serve as a reference to the intermediate outputs during the iterative training process and also aid model convergence, without which the model has to directly learn from post-contrast to low-dose. Given k iterations, the soft label {S i } k-1 i=1 for iteration i is calculated as follows: where P post and P pre denote the skull-stripped post-contrast and pre-contrast images. γ = 0.1 represents the dose level of the final prediction, and τ = 0.1 denotes the threshold to extract the estimated contrast uptake U = ReLU( P post -P preτ ). Finally, the total losses are calculated as where L e = L L1 + L SSIM and α = 0.1 and β = 1. The ""soft labels"" are assigned a small loss weight so that they do not overshadow the contribution of the real low-dose image. Additionally, in order to recover the high frequency texture information and to improve the overall perceptual quality, adversarial  Global Transformer (Gformer): Transformer models have risen to prominence in a wide range of computer vision applications  Subsampling Attention: The sub-sampling is a key element in the Gformer block which generates a number of sub-images from the whole image as attention windows as shown in Fig.  d is the subsampled feature map. We set h, d = 0 to avoid any information loss during subsampling. These d 2 sub-feature maps are stacked onto the batch dimension as the attention windows for the transformer block shown in Fig. ",8
5244,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model (vol8),,(c).,"Rotational Shift: Image rotation has been widely used as a data augmentation technique in preprocessing and model training. Here, to further capture the heterogeneous nature of the contrast uptake areas, we employ the rotational shift as a module to facilitate the representation power of the Gformer. To prevent information loss on the edges due to rotation, only small angles (e.g., 10 • , 20 • ) are used for rotation and residual shortcuts are also applied. Specifically, given the feature map M o ∈ R b×c×h×w , rotational shift is performed around the vertical axis of height/width. The rotated feature map M r ∈ R b×c×h×w is obtained by the following equation: where λ is the rotation angle. (p, q, x, y) and (p , q , x , y ) denote the pixel index in the feature map tensor before and after rotational shift, respectively.",8
5247,Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_9.,8
5256,Learning Normal Asymmetry Representations for Homologous Brain Structures (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 8.,8
5264,TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_40.,8
5280,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps (vol8),,ImageNet Pre-training:,"It is straightforward to exploit the models pre-trained on natural images. In this strategy, we directly extract the gradient-weighted feature map in the ImageNet pre-trained network and generate prior self-activation maps. Contrastiveness: Following the contrastive learning  where L dis is the loss function. Z l , Z r , and Z n are representations of the input sample, the positive sample, and the negative sample, respectively. In addition, dif f (•) is a function that measures the difference of embeddings. Similarity: LeCun et al.  Here, maximizing the similarity of two embeddings is equal to minimizing their difference.",8
5286,Exploring Unsupervised Cell Recognition with Prior Self-activation Maps (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 54.,8
5297,Pick and Trace: Instance Segmentation for Filamentous Objects with a Recurrent Neural Network (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 61.,8
5320,Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies (vol8),,Contribution:,"Our proposed method allows cortical analysis of brain MRI scans of any orientation, resolution, and MRI contrast without retraining, making it possible to use it out of the box for straightforward analysis of large datasets ""in the wild"". The proposed method combines two modules: a convolutional neural network (CNN) that estimates SDFs of the WM and pial surfaces, and a classical geometry processing module that places the surfaces while satisfying geometric constraints (no self-intersections, spherical topology, regularity). The CNN capitalizes on recent advances in domain randomization to provide robustness against changes in acquisition -in contrast with existing learning approaches that can only process images acquired with the same resolution and MRI contrast as the scans they were trained on. Finally, our method's classical geometry processing gives us geometric guarantees and grants instant access to an array of existing methods for cortical thickness estimation, registration, and parcellation ",8
5321,Cortical Analysis of Heterogeneous Clinical Brain MRI Scans for Large-Scale Neuroimaging Studies (vol8),,Further Related Work:,The parameterization of surfaces as SDFs has been combined with deep neural networks in several domains ,8
5336,B-Cos Aligned Transformers Learn Human-Interpretable Features (vol8),,Domain-Expert Evaluation:,"The results show that BvTs are significantly more trustworthy than ViTs (p < 0.05). This indicates that BvT consistently attends to biomedically relevant features such as cancer cells, nuclei, cytoplasm, or membrane ",8
5339,B-Cos Aligned Transformers Learn Human-Interpretable Features (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_50.,8
5343,Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN (vol8),,Multi-contexts Discriminator,"The original discriminator learns the global representation of synthetic or real data to distinguish them. However, it is insufficient to provide powerful feedback to encourage the generator to predict realistic data. Many works improve the discriminator and show that the stronger the discriminator is, the better the generator is  Context Encoding. The context encoding is for feature extraction. For brain network, regions are represented as nodes and the links between regions are represented as edges. Since FC is defined as the correlation between the active brain regions, it represents the edge feature. However, most of the graph convolution networks (GCN) model the node feature extraction. To address the FC encoding, we adopt the edge-based graph convolution method for feature extraction  There are three basic modules, i.e. E2N module (edge to node), N2E module (node to edge), N2G module (node to graph). E2N module. Given a FC E ∈ R n×n , where E i,j denotes the edge between region i and region j, it aggregates the linking edges of region i into a node representation: where w i is trainable weight and N i is the feature of i t h node. N2E module. It propagates the feature of node i and node j to their linking edge: The network stacks the graph convolution layers for FC encoding. A graph convolution layer is comprised of a E2N module and a N2E module for FC feature learning and reserve the structure of FC. Meanwhile, the skip connection  The output layer of the network is comprised of a E2N module and a N2G module to encode the FC representation into the node representation and the graph representation in series. Multi Contexts Discrimination. The original discriminator D(•) classifies the input data to be real or fake. In contrast, our proposed multi-contexts discriminator provides three kind of supervisions, i.e. edge-level, node-level and graph-level, to implicitly and explicitly strengthen the generator. Mathematically, our discriminator loss is comprised of three parts: In the output layer, we first use three MLP to transform the three kinds of context representation. Then the D edge (•), D node (•), D graph (•) respectively classify the transformed edge representation, node representation and graph representation to be real or fake. Correspondingly, the objective of the generator is: The multi contexts supervision improves the discriminator, which implicitly strengthens the generator in adversarial training process. Meanwhile, it feedbacks the fine-grained information to the generator by backpropagation, which explicitly strengthens the generator and encourages it to predict realistic FC.",8
5344,Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN (vol8),,Monte-Carlo Mean Samples of FC,"FC represents the development of brain activities so that it is relatively dynamic. While SC represents the fibers connection to indicate the anatomical structure of brain  where y i denotes multiple real FC of each subject and m can be fixed or dynamic during training, which denotes the monte-carlo sampling, m = 1, 2, ...M , M is the maximum samples for each subject of dataset. The linear combination of FC enlarges the data space for supervision, making the generator predict diverse FC.",8
5345,Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN (vol8),,Reconstruction Loss of Generator,We adopt reconstruction loss in generator to make it predict realistic FC which includes the mean absolute error (MAE) and Pearson's correlation coefficient (PCC) between the generated FC and the training FC sample.,8
5346,Predicting Diverse Functional Connectivity from Structural Connectivity Based on Multi-contexts Discriminator GAN (vol8),,Full Objective,"In summary, the objective function of the MCGAN is defined as: where L adv optimizes the adversarial training of the generator and discriminator, L rec optimizes the generator for realistic prediction.",8
5353,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations (vol8),,Implementation and Training.,"Given the rigidly registered LR images, we compute Ω 1 , Ω 2 ∈ Ω in the scanner reference space using their affine matrices. Subsequently, we normalize Ω to the interval [-1, 1] 3 and independently normalize each contrast's intensities to [0, 1]. We use 512-dimensional Fourier Features in the input. Our model consists of a four-layer MLP with a hidden dimension of 1024 for the shared layers and two layers with a hidden dimension of 512 for the heads. We use Adam optimizer with a learning rate of 4e-4 and a Cosine annealing rate scheduler with a batch size of 1000. For the multi-contrast INR models, we use MI as in Eq. 2 for early stopping. Implemented in PyTorch, we train our model on a single A6000 GPU. Please refer to Table  Model Selection and Inference. Since our model is trained on sparse sets of coordinates, it is prone to overfitting them and has little incentive to generalize in out-of-plane predictions for single contrast settings. A remedy to this is to hold random points as a validation set. However, this will reduce the number of training samples and hinder the reconstruction of fine details. For multi-contrast settings, one can exploit the agreement between the two predicted contrasts. Ideally, the network should reach an equilibrium between the contrasts over the training period, where both contrasts optimally benefit from each other. We empirically show that Mutual Information (MI)  Compared to image registration, we do not use MI as a loss for aligning two images; instead, we use it as a quantitative assessment metric. Given two ground truth HR images for a subject, one can compute the optimum state of MI. We observe that the MI between our model predictions converges close to such an optimum state over the training period without any explicit knowledge about it, c.f. Fig. ",8
5356,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_17.,8
5360,PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents (vol8),,Medical Figure Collection (,Step 1 and 2 in Fig.  Subfigure Separation (Step 3 and 4 in Fig. ,8
5361,PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents (vol8),,Subcaption Separation and Alignment (,Step 5 and 6 in Fig. ,8
5364,PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents (vol8),,Image-Text Contrastive Learning (ITC).,"We implement ITC loss following CLIP  where y i2t , y t2i refer to one-hot matching labels, CE refers to InfoNCE loss ",8
5365,PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents (vol8),,Masked Language Modeling (MLM).,"We implement MLM loss following BERT  Total Training Loss. The final loss is defined as L = L ITC + λL MLM , where λ is a hyper-parameter deciding the weight of L MLM , set as 0.5 by default. Disscussion. While we recognize a lot of progress in VLP methodology ",8
5367,PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents (vol8),,MIMIC-CXR,"PMC-OA contains 1.65M image-text pairs, which we have explicitly conducted deduplication between ROCO.",8
5368,PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents (vol8),,Image-Text Retrieval (ITR). ITR contains both image-to-text(I2T,") and text-to-image(T2I) retrieval. We train PMC-CLIP on different datasets, and sample 2,000 image-text pairs from ROCO's testset for evaluation, following previous works  Classification. We finetune the model for different downstream tasks that focus on image classification. Spcifically, MedMINIST  Visual Question Answering (VQA). We evaluate on the official dataset split of SLAKE ",8
5372,PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents (vol8),,Methods,Pretrain Data DataSize I2T T2I R@1 R@5 R@10 R@1 R@5 R@10 ,8
5374,PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents (vol8),,Methods,Pretrain Data DataSize I2T T2I R@1 R@5 R@10 R@1 R@5 R@10 ViLT  Visual Question Answering. VQA requires model to learn finer grain visual and language representations. As Table ,8
5376,PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents (vol8),,Data Collection Pipeline.,"To demonstrate the effectiveness of subfiguresubcaption alignment, we compare PMC-CLIP with the model pretrained on dataset w/o alignment (Table  Visual Backbone. We have also explored different visual backbones, using the same setting as CLIP ",8
5378,PMC-CLIP: Contrastive Language-Image Pre-training Using Biomedical Documents (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 51.,8
5384,A Coupled-Mechanisms Modelling Framework for Neurodegeneration (vol8),,Selection of Subjects.,"We include N = 110 subjects with at least two tau-PET scans, amyloid beta positive status and at least one region with positive tau signal, including healthy, cognitively impaired and AD subjects, since we aim to focus on the people with a potential to accumulate abnormal pathological tangles. We normalise the data of all the subjects (i = 1,..,N) between 0 and 1 by (tau itau min )/(tau maxtau min ) where tau min and tau max are calculated across all the subjects and regions, thus the differences in data scales among subjects and regions are maintained. Setting of Epicentres. For initialization, we assume pathology starts from candidate epicentres, to simulate the full process of disease progression from very early stages, i.e. prior to the baseline scan. We rank 34 pairs of bilateral cortex regions according to the total number of subjects that have positive tau signals, and pick the top eight pairs of regions as the candidate epicentres where the propagation of pathology is likely to start: inferior temporal cortex, banks of the superior temporal sulcus, fusiform gyrus, lateral orbitofrontal cortex, middle temporal gyrus, entorhinal cortex, parahippocampal gyrus and temporal pole. The four epicentres identified by Vogel et al. ",8
5387,A Coupled-Mechanisms Modelling Framework for Neurodegeneration (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 45.,8
5393,CoactSeg: Learning from Heterogeneous Data for New Multiple Sclerosis Lesion Segmentation (vol8),,"Performance for MS Lesion Segmentation. Two MS tasks (i.e., newlesion segmentation on MICCAI-21 and all-lesion segmentation on our MS-23v1",Figure ,8
5399,Robust and Generalisable Segmentation of Subtle Epilepsy-Causing Lesions: A Graph Convolutional Approach (vol8),,Implementation,"Experiments. Using our graph-based adaptation of nnU-Net (GC-nnU-Net) and the previous MLP model as baseline, we ran an ablation study to measure the impact of the proposed auxiliary losses (Table  Evaluations. Model performances were compared according to their Area Under the Curve (AUC), which was calculated by computing the sensitivity and specificity at a range of prediction thresholds. For sensitivity calculations, due to uncertainty in the lesion masks, a lesion was considered detected if the prediction was within 20 mm of the original mask, as this corresponds with the inter-observer variability measured across annotators ",8
5414,BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior (vol8),,Estimating Pixel-Level Image Clarity in NSCT.,"To define c(•), BigFUSE first uses NSCT, a shift-invariant image representation technique, to estimate pixel-level focus measures by characterizing salient image structures  where R i,l is local directional band-limited image contrast and Si0 is the smoothed image baseline, whereas Dσ i highlights image features that are distributed only on a few directions, which is helpful to exclude noise  Reweighting Image Clarity Measures by Photon Traveling Path. Pixelindependent focus measures may be adversely sensitive to noise, due to the limited receptive field when characterizing local image clarities. Thus, BigFUSE proposes to integrate pixel-independent image clarity measures along columns by taking into consideration the photon propagation in depth. Specifically, given a pair of pixels (X a m,n , X b m,n ), X a m,n is empirically more in-focus than X a m,n , if photons travel through fewer light-scattering tissues from illumination objective a than from b to get to position (m, n), and vice versa. Therefore, BigFUSE defines column-level image clarity measures as: where A :,i is to model the image deterioration due to light scattering. To visualize photon traveling path, BigFUSE uses OTSU thresholding for foreground segmentation (followed by AlphaShape to generalize bounding polygons), and thus obtains sample boundary, i.e., incident points of light sheet, which we refer to as p u and p l for opposing views a and b respectively. Since the derivative of A :,i implicitly refers to the spatially varying index of refraction within the sample, which is nearly impossible to accurately measure from the physics perspective, we model it using a piecewise linear model, without loss of generality: As a result, log(p((X a :,i , X b :,i )|ω i )) is obtained as summed pixel-level image clarity measures with integral factors conditioned on photon propagation in depth.",8
5427,Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI (vol8),,Method,Results in Table ,8
5430,Dynamic Structural Brain Network Construction by Hierarchical Prototype Embedding GCN Using T1-MRI (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_12.,8
5434,Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping (vol8),,Augmented Lesion Network Mapping (A-LNM).,"After lesion mapping, we introduce a modified LNM (called augmented LNM (A-LNM) in this paper) to generate FLN maps for each GBM patient by using resting-state fMRI of all 1000 GSP healthy subjects, as described below. i) For each patient, the lesion is viewed as a seed region to calculate FDC in the healthy subjects with restingstate fMRI. Specifically, to compute FDC, the mean BOLD time series of voxels within each lesion is correlated with the BOLD time series of every voxel in the whole brain for all the 1000 healthy subjects, yielding 1000 FDC maps of voxelwise correlation values (transformed to z-scores), where an FDC map is actually a three-dimensional voxel-wise matrix of size 91 × 109 × 91 (spatial resolution: 2mm 3 voxel size). ii) Different from the commonly used LNM where the resulting 1000 FDC maps are thresholded or averaged to obtain a single FLN map for each patient, the A-LNM generates many FLN maps for each patient in a manner that partitions all the 1000 FDC maps into disjoint subsets of equal size and averages each subset to produce one FLN map. One can clearly see that similar to data augmentation schemes, we artificially boost the number of training samples (i.e., FLN maps) by our A-LNM, which helps to mitigate the risk of over-fitting and improve the performance of overall survival time prediction when learning a deep neural network from such a small sized training set used in this paper. Note that in Sect. 3 of this paper, according to experimental results, we divided the 1000 FDC maps into 100 subsets, and randomly chose 10 out of the resulting 100 FLN maps for each patient as input to the downstream prediction model. Deep Neural Network for Overall Survival Time Prediction. By taking the obtained FLN maps as input, we apply a 3D ResNet-based backbone network transferred from the encoder of MedicalNet ",8
5454,BrainUSL: Unsupervised Graph Structure Learning for Functional Brain Network Analysis (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3_20.,8
5463,Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery (vol8),,Stability of Disease Scores:,"To test the model's stability, we randomly omitted a portion of the control samples before creating a new augmented dataset and training a new model. Then, we compared the correlation between prime and new scores and repeated the test five times. Our model showed enhanced stability due to weak label data augmentation, as opposed to the unstable on-disease score algorithm used in RxRx19a (see Fig. ",8
5464,Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery (vol8),,Robustness of Disease Scores:,We show the robustness of our method for computing disease scores by accurately predicting scores for new samples in a zero-shot learning setup ,8
5468,Weakly-Supervised Drug Efficiency Estimation with Confidence Score: Application to COVID-19 Drug Discovery (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 65.,8
5473,DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics (vol8),,2,">> the number of columns t -s + 1. To solve the first problem, instead of learning ψ(g k ) directly, we embed the BOLD signal x i k of each ROI independently using the encoder to learn the latent embeddings z i k (Fig.  The second problem arises because the Koopman operator A regresses the value of an edge at the next time-point as a linear combination of all the other edges at the current time-point, i.e., g ij k+1 = N p,q=1 w pq g pq k . This results in O(n Other than L lkis , we also train the autoencoder with a reconstruction loss L recon which is the mean-squared error (MSE) between x i k and the reconstructed output from the decoder xi k . Moreover, a regularizer L reg in the form of an MSE loss between g k and the latent ψ(g k ) is also added. The final loss is the following: where α and β are hyper-parameters. We choose α, β, and other hyperparameters using grid search on the validation set. The network architecture and the values of the hyper-parameters of DeepGraphDMD training are shown in Supplementary Fig. ",8
5479,DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics (vol8),,Regression Analysis of Behavioral Measures from HCP:,"In this experiment, we regress the behavioral measures with the DMs within each frequency bin (Sect. 2.3) using Elastic-net. As an input to the Elastic-net, we take the real part of the upper diagonal part of the DM and flatten it into a vector. We then train the Elastic-net in two ways-1. single-band: where we train the Elastic-net independently with the DMs in each frequency bin, and 2. multi-band: we concatenate two DMs in the frequency bins: 0-0.01 Hz and 0.08-0.12 Hz and regress using the concatenated vector. For evaluation, we compute the correlation coefficient r between the predicted and the true values of the measures.",8
5482,DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics (vol8),,Regression Analysis of Behavioral Measures from HCP:,"We show the values of r across different methods in Table  Traditional dynamical functional connectivity analysis methods (such as sliding window-based techniques) consider a sequence of network states. However, our results show that these states can be further decomposed into more atomic network modes. The importance of decoupling these network modes from nonlinearly mixed fMRI signals using DeepGraphDMD has been shown in regressing behavioral measures from HCP data.",8
5484,DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear Functional Brain Network Dynamics (vol8),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43993-3 35.,8
5492,EndoSurf: Neural Surface Reconstruction of Deformable Tissues with Stereo Endoscope Videos (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 2.,9
5503,ConTrack: Contextual Transformer for Device Tracking in X-Ray (vol9),,Context Flow Estimation.,"Obtaining ground truth optical flow in real world data is a challenging task and may require additional hardware such as motion sensors. As such, training a model for optical flow estimation directly in the image space is difficult. Instead, we propose to estimate the flow in the segmentation space, i.e., on the predicted heatmaps of the catheter body between neighboring frames. We use the RAFT  Here corr(g θ (m t-1 ), g θ (m t )) ∈ R H f ×W f ×H f ×W f stands for correlation evaluation: which can be computed via matrix multiplication. Starting with an initial flow f 0 = 0, we follow the same model setup as  We note here that since the segmentations of the catheter body are sparse objects compared to the entire image, computation of the correlation volume and subsequent updates can be restricted to a cropped sub-image which reduces computation cost and flow inference time. As the flow estimation is performed on the segmentation map, one can simply generate synthetic flows and warp them with the existing catheter body annotation to generate data for model training. Refinement with Combined Spatial-temporal Prediction. Finally, we generate a score map with combined information from the spatial localization stage and the temporal prediction by context flow, Here α is a positive scalar. It helps the score map to promote coordinates that are activated jointly on both the spatial prediction xs t and the temporal prediction xf t . Finally, we forward the score map through a refinement module to finalize the prediction. The refinement module consists of a stack of 3 convolutional layers. Similar to the spatial localization stage, a combination of the binary cross-entropy and the dice loss is used as the final loss.",9
5506,ConTrack: Contextual Transformer for Device Tracking in X-Ray (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 65.,9
5518,Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change (vol9),,Temporal sub-network:,"To capture temporal trends, we implemented additional aggregation layers to process features across a sequence of incremental timepoints. Instead of aggregating features across several spatial ""neighborhoods"" as seen in Eq. (  where y t is the temporally aggregated features and x (t) are the features to be aggregated across timepoints (t) to (t -N ). We hypothesized that including more than one previous timepoint in the aggregation layer may improve the network performance (see Sect. 3.4). We used the same LSTM-attention aggregation mechanism as used in the PhysGNN spatial sub-network. Training Strategy: The SIMM network produces two predictions of the incremental tissue deformation, one from the spatial sub-network and the other after spatiotemporal aggregation (Fig.  where l SIMM is the total loss of the SIMM network, l s is the loss of the spatial sub-network predicted deformation d F,s , and l st is the loss of the spatiotemporal predicted deformation d (t) F,st . The combined loss was used to train the network to ensure the spatial sub-network still learns to adequately encode the spatial features.",9
5522,Spatiotemporal Incremental Mechanics Modeling of Facial Tissue Change (vol9),,Single-step,Incremental SIMM 332 nodes >1mm error 230 nodes >1mm error 130 nodes >1mm error Fig. ,9
5532,A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_45.,9
5542,Intraoperative CT Augmentation for Needle-Based Liver Interventions (vol9),,Comparison with VoxelMorph:,"The problem that we address can be seen from different angles. In particular, we could attempt to solve it by registering the preoperative NCCT to the intraoperative one and then applying the resulting displacement field to the known preoperative segmentation. However, state-of-the-art registration methods such as VoxelMorph ",9
5551,Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 58.,9
5567,Semantic Virtual Shadows (SVS) for Improved Perception in 4D OCT Guided Surgery (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_39.,9
5594,Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 34.,9
5605,Self-distillation for Surgical Action Recognition (vol9),,Self-distillation. The concept of self-distillation was achieved by training a,"teacher Swin transformer on one-hot encoded hard labels for 20 epochs, with a batch size of 64, an Adam  The five teacher models shared a common weight intialization seed. The five student models shared a separate weight initialization seed. The student models were trained for 40 epochs with the same augmentations as the teacher models. We saved the weights on the epoch with the best mean Average Precision (mAP) score based on the validation split for the current fold. Ensemble. We combined three trained Swin Transformers (SwinT) of different scales (SwinT base/SwinT large) and configurations for our final ensemble (Ens) model: First, we employed a SwinT base model with multi-task learning of instrument, verb, and target and trained it using self-distillation. Second, we used a SwinT large model using the same approach, and added label smoothing to the soft labels. Third, we included phase annotations as an additional task for the multi-task training of a SwinT base model still employing self-distillation. Please note that every single model mentioned here corresponds to the five aggregated student models of the previous paragraph. All the models were trained using Nvidia GPUs Geforce RTX 3090 and Tesla V100 32GB.",9
5607,Self-distillation for Surgical Action Recognition (vol9),,Table 1. Main quantitative results,"Starting from our backbone model -a Swin Transformer (Swin T) -we gradually added individual components, namely multi-task learning (MultiT), self-distillation (SelfD), and ensembling (Ens). Each component addition leads to an increase in mAP and top-5 accuracy, in both cross-validation (left) and independent external validation (right). This shows that self-distillation specifically leads to increased scores for semantically related classes. Independent External Validation. External validation was conducted on the CholecTriplet challenge test set. The results, shown in Table ",9
5609,Self-distillation for Surgical Action Recognition (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 61.,9
5622,Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks (vol9),,GNN Stage.,"The second stage employs a generalized message-passing GNN following  1. keypoint association prediction: we model association between body keypoints and their corresponding pedicle keypoints as binary edge classification on the over-connected k-NN graph. 2. body keypoint level prediction: for body keypoints, we model the spine level prediction as multi-class node classification. 3. keypoint legitimacy prediction: to filter out false-positive keypoints, we additionally compute an binary legitimacy prediction for each node. To perform these task, our message-passing GNN maintains edge and node embeddings which are updated in each layer. A message-passing layer performs a node update and edge update operation. Denoting the feature vector of a node v by x v , and the feature vector of a directed edge (u, v) by x uv , the node and edge features are updated as follows: Here denotes a symmetric pooling operation (in our case max pooling) over the neighborhood N u . ψ node and edge are trainable parametric functions: in our case, two distinct two-layer MLPs with ReLU nonlinearities. After N such message-passing layers we obtain an embedding vector for each node and edge. Each node/edge embedding is passed through a linear layer (distinct for nodes and edges) to obtain a vector of node class logits or a single edge prediction logit, respectively. The last entry in the node prediction vector is interpreted as a node legitimacy prediction score: nodes predicted as illegitimate are discarded for the output. The node input features x u ∈ R 7 consist of the one-hot encoded keypoint type (body, left or right pedicle) and the segment input information (a pseudoprobability in [0, 1] for each of the four spine segments of belonging to that segment, computed by applying a sigmoid to the heatmap network's output channels which represent the different spine segments). The edge input features x uv ∈ R 4 consist of the normalized direction vector of the edge and the distance between the two endpoints. The output of the GNN contains finer spine-level classification (i.e. C1-C7, T1-T13, L1-L6, S1-S2), keypoint-level legitimacy (legitimate vs. false-positive detection) and body-pedicle association via edge prediction, implicitly defining the orientation of each vertebra. Prediction scores of corresponding directed edges (u, v) and (v, u) are symmetrized by taking the mean. In our experiments we consider variations to our architecture: weight sharing between consecutive GNN layers, multiple heads with a shared backbone (jointly trained) and dedicated networks (separately trained) for edge/node prediction. 4 Experiments",9
5628,Robust Vertebra Identification Using Simultaneous Node and Edge Predicting Graph Neural Networks (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_46.,9
5646,Analysis of Suture Force Simulations for Optimal Orientation of Rhomboid Skin Flaps (vol9),,Fig. 3. Pseudocode of two different suture force analyses,"To investigate the effect of material properties, we repeated the same procedure using the material properties listed in Table  Table  def ault small µ large µ small k1 large k1 small k2 large k2 Single-suture. Instead of taking the maximum suture force after completing all sutures, we also experimented with making one suture at a time (see Algorithm 2 in Fig. ",9
5654,A Transfer Learning Approach to Localising a Deep Brain Stimulation Target (vol9),,φ(x,"where p 1 = 2, p 2 = 0.5, p 3 = 0.2 are the power of the polynomials, and g i is the group-average probability of voxel i classified as Vim. The second pairwise cost encourages assigning similar labels to neighbouring voxels, particularly for those sharing similar connectivity features. We modelled this component as ) is a kernel function modelling the similarity between voxel i and j in the extended feature space, with length scale γ m , chosen via cross-validation. μ(•) is a label compatibility function where μ(y i , y j ) = 0 if Therefore, in a local neighbourhood, the kernel function penalises inconsistent label assignment of voxels that have similar features, thus allowing modelling local smoothness. ρ m controls the relative strength of this pairwise cost weighted by k m (•). Lastly, the L1 and L2 penalty terms serve to prevent overfitting of the model. We used a mean-field algorithm to iteratively approximate the maximum posterior P (y|X) ",9
5656,A Transfer Learning Approach to Localising a Deep Brain Stimulation Target (vol9),,Generalisability of the HQ-Augmentation,"Model to UK Biobank. We also tested whether the HQ-augmentation models trained on HCP were generalisable to other datasets collected under different protocols. It is crucial for a model to be generalisable to unseen protocols, as collecting large datasets for training purposes in clinical contexts can often be impractical. We therefore applied the HQ-augmentation models trained on different HCP LQ datasets to UK Biobank surrogate low-quality data (LQ-UKB) and averaged the outputs to give a single Fig.  Reliability Analysis of the HQ-Augmentation Model. We also conducted a reliability analysis for the HQ-augmentation model and the connectivity-driven approach to assess their consistency in providing results despite variations in data quality (across-quality reliability) and across scanning sessions (test-retest reliability). To evaluate the HQ-augmentation model's across-quality reliability, we trained it using HQ tract-density maps to produce an ""HQ version"" of HQaugmented Vim. The similarity between HQ-augmentation outputs using high-or low-quality features was assessed using the Dice coefficient and centroid displacement measures. Test-retest reliability was determined by comparing the outputs of the HQ-augmentation model on first-visit and repeat scanning sessions. Similarly, we assessed the across-quality reliability and test-retest reliability for the connectivity-driven approach, applied to high-and low-quality data accordingly. The HQ-augmentation model consistently provided more reliable results than the connectivity-driven approach, not only across datasets of different quality but also across different scanning sessions (Fig.  Discussion. Our study presents the HQ-augmentation technique as a robust method to improve the accuracy of Vim targeting, particularly in scenarios where data quality is less than optimal. Compared to existing alternatives, our approach exhibits superior performance, indicating its potential to evolve into a reliable tool for clinical applications. The enhanced accuracy of this technique has significant clinical implications. During typical DBS procedures, one or two electrodes are strategically placed near predetermined targets, with multiple contact points to maximise the likelihood of beneficial outcomes and minimise severe side effects. Greater accuracy translates into improved overlap with the target area, which consequently increases the chances of successful surgical outcomes. Importantly, the utility of our method extends beyond Vim targeting. It can be adapted to target any area in the brain that might benefit from DBS, thus expanding its clinical relevance. As part of our ongoing efforts, we are developing a preoperative tool based on the HQ-augmentation technique. This tool aims to optimise DBS targeting tailored to individual patients' conditions, thereby enhancing therapeutic outcomes and reducing patient discomfort associated with suboptimal electrode placement.",9
5674,SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation (vol9),,Conclusion.,"This work presents a novel semantic segmentation method for surgical scene understanding that significantly reduces the annotation burden by leveraging the temporal consistency of point cloud sequences. We demonstrate the effectiveness of our approach on point clouds from a surgical phase recognition dataset, which we enrich with manual 3D annotations. By incorporating self-supervised temporal priors, our method achieves a high segmentation mIoU of 73.10% using only 0.005% of annotated points. Furthermore, we establish a formal link between semantic segmentation and workflow analysis by demonstrating that our semantic predictions benefit downstream surgical phase recognition methods. Finally, we release all anonymized point clouds, annotations, and code used to ease the deployment of context-aware systems in surgical environments.",9
5675,SegmentOR: Obtaining Efficient Operating Room Semantics Through Temporal Propagation (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 6.,9
5681,SENDD: Sparse Efficient Neural Depth and Deformation for Tissue Tracking (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_23.,9
5690,Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 63.,9
5700,Surgical Video Captioning with Mutual-Modal Concept Alignment (vol9),,Acknowledgments,. This work is supported by ,9
5704,SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery (vol9),,Language-Vision Processing:,"The questions are tokenized using the inherent GPT2 tokenizer. The word tokens are further embedded based on token-id, token type (0) and token position by the inherent GPT2 word embedding layers. To tokenize the input surgical scene (image) into vision tokens, the LV-GPT includes a vision tokenizer (feature extractor): ResNet18 (RN18)  where, T t () is type embedding, P pos () is pose embedding, w x and v x are initial word and vision embedding, and v t are vision tokens. Initial word embeds (w x ) are obtained using word embedding based on word token id. Depending on the size (dim) of each vision token, they undergo additional linear layer embedding (f ()) to match the size of the word token. Token Sequencing: LLMs are observed to process long sentences robustly and hold long-term sentence knowledge while generating coherent paragraphs/reports. Considering GPT's superiority in sequentially processing large sentences and its uni-directional attention, the word tokens are sequenced before the vision tokens. This is also aimed at mimicking human behaviour, where the model understands the question before attending to the image to infer an answer. Classification: Finally, the propagated multi-modality features are then passed through a series of linear layers for answer classification.",9
5706,SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery (vol9),,Cholec80-VQA:,The classification subset of the Cholec80-VQA ,9
5707,SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery (vol9),,PSI-AVA-VQA:,We introduce a novel PSI-AVA-VQA dataset that consists of Q&A pairs for key surgical frames of 8 cases of the holistic surgical scene dataset (PSI-AVA dataset) ,9
5710,SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery (vol9),,Early Vision vs Early Word:,The performance of LV-GPT based on word and vision token sequencing (Table ,9
5711,SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery (vol9),,Pose Embedding for Vision Tokens:,"The influence of positional embedding of the vision tokens (representing a patch region) in all the LV-GPT variants is studied by either embedded with position information (pos = 1, 2, 3, .., n.) or zero-position (pos = 0). Table ",9
5712,SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery (vol9),,Ablation Study on Vision Token,Embedding: An ablation study on the vision token embedding in the LV-GPT model on the EndoVis18-VQA dataset is also shown in Table ,9
5726,LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms (vol9),,Method,4D-OR ,9
5730,LABRAD-OR: Lightweight Memory Scene Graphs for Accurate Bimodal Reasoning in Dynamic Operating Rooms (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_29.,9
5758,Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train (vol9),,Acknowledgements,. This work was supported in part by ,9
5759,Foundation Model for Endoscopy Video Analysis via Large-Scale Self-supervised Pre-train (vol9),,Construction of Global and Local,Views. We conduct a further analysis of the strategies for constructing global (G ∈ ,9
5763,"GLSFormer: Gated -Long, Short Sequence Transformer for Step Recognition in Surgical Videos (vol9)",,Patch Embedding. We decompose each frame of dimension H,"Each patch is flattened into a vector x p,t ∈ R 3Q 2 for each frame t and spatial location, p ∈ (1, N). We linearly map the patches of short and long term videos frames into embedding vector of dimension R K using a shared learnable matrix E ∈ R K×3Q 2 . We concatenate the patch embeddings of the short and long-term streams along the frame dimension to form feature representations x st p,t and x lt p,t of size N × n st × K and N×n lt ×K, respectively. along with a learnable positional embedding e st-pos p,t . ( Note that a special learnable vector z st 0,0 ∈ R K representing the step classification token is added in the first position. Our approach is similar to word embeddings in NLP transformer models  Gated Temporal, Shared Spatial Transformer Encoder. Our Transformer Encoder, consisting of Gated Temporal Attention module and Shared Spatial Attention module takes the sequence of embedding vectors z st p,t and z lt p,t as input. In a self-attention module for spatio-termporal models, computational complexity increases non-linearly O(T 2 S 2 ) with increase in spatial resolution(S) or temporal frames(T). Thus, to reduce the complexity, we sequentially process our gated temporal cross attention module and spatial attention module ",9
5764,"GLSFormer: Gated -Long, Short Sequence Transformer for Step Recognition in Surgical Videos (vol9)",,Gated Temporal Attention. The temporal cross-attention module aligns the long-term(z lt,"l-1 ) and short-term features(z st l-1 ) in the temporal domain, allowing the model to better capture the relationship between the long and short term streams. We concatenate both of these streams to form a strong joint stream that has both fine-grained information from the short-term stream and global context information from the long-term stream. Firstly, a query/key/value vector for each patch in the representations z lt l-1 (p, t), z st l-1 (p, t) and z lt,st l-1 (p, t) using linear transformations with weight matrices U lt qkv , U st qkv and U lt,st qkv respectively and normalization using LayerNorm is computed as follows: where a ranges from 1 to A representing attention heads. The total number of attention heads is denoted by A, and each has a latent dimensionality of K h = K A . The computation of these QKV vectors is essential for multi-head attention in transformer. Now for refining the streams, with most relevant cross-stream information, we gate the individual stream's temporal features(I) (QKV ) a;lt/st with the joint stream temporal features(J) (QKV ) a;lt,st . Gating parameters are calculated by concatenating I and J and passing them through linear and softmax layers which predict Gt st and Gt lt for (QKV ) a;st and (QKV ) a;lt , respectively. By gating the individual stream's temporal features with the joint stream temporal features, the model is able to selectively attend to the most relevant features from both streams, resulting in a more informative representation. This helps in capturing complex relationships between the streams and improves the overall performance of the model. This computation can be described as follows Later, temporal attention is computed by comparing each patch (p, t) with all patches at the same spatial location in other frames of both streams, as follows Here, α is separately calculated for the long-stream and shortstream, where (•) = lt or st. Similar to the vision transformer, encoding blocks for each layer (z lt and z st ) are computed by taking the weighted sum of value vectors (SA a (z)) using self-attention coefficients from each attention head as follows Next, the self-attention block (SA a (z)) for each attention head is projected along with a residual connection from the previous layer. This multi-head self-attention (MSA) operation can be described as follows Here, (z ) l is the concatenation of z lt and z st .",9
5765,"GLSFormer: Gated -Long, Short Sequence Transformer for Step Recognition in Surgical Videos (vol9)",,Shared Spatial Attention.,"Next, we apply self-attention to the patches of the same frame to capture spatial relationships and dependencies within the frame. To accomplish this, we calculate new key/query/value using Eq. (  The encoding blocks are also calculated using Eq. (  The embedding for the entire clip is obtained by taking the output from the final block and passing it through a MLP with one hidden layer. The corresponding computation can be described as y = LN(z L (0,0) ) ∈ R D . The classification token is used as the final input to the MLP for predicting the step class at time t. Our GLSFormer is trained using the cross-entropy loss.",9
5795,From Tissue to Sound: Model-Based Sonification of Medical Imaging (vol9),,Contribution,"This paper presents a novel approach for transforming spatial features of multimodal medical imaging data into a physical model that is capable of generating distinctive sound. The physical model captures complex features of the spatial domain of data, including geometric shapes, textures, and complex anatomical structures, and translates them to sound. This approach aims to enhance experts' ability to interact with medical imaging data and improve their mental mapping regarding complex anatomical structures with an unsupervised approach. The unsupervised nature of the proposed approach facilitates generalization, which enables the use of varied input data for the development of versatile soundcapable models.",9
5798,From Tissue to Sound: Model-Based Sonification of Medical Imaging (vol9),,Implementation of a Mass-Interaction System in Discrete Time.,"To represent and compute discretized modular mass-interaction systems, a widely used method involves applying a second-order central difference scheme to Newton's second law, which states that force F is equal to mass m times acceleration a, or the second derivative of its position vector x with respect to time t. The total force exerted by the dampened spring, denoted as where F s represents the elastic force exerted by a linear spring (the interaction) with stiffness K, connecting two masses m 1 , m 2 located at positions x 1 , x 2 , can be expressed using the discrete-time equivalent of Hooke's law. Similarly, the friction force F d applied by a linear damper with damping parameter z can be derived using the Backward Euler difference scheme with the discrete-time inertial parameter Z = z/ΔT . F (t n ) is applied symmetrically to each mass in accordance with Newton's third law: The combination of forces applied to masses and the connecting spring yields a linear harmonic oscillator as described in which is a fundamental type of the mass-interaction system. This system is achieved by connecting a dampened spring between a mass and a fixed point A mass-interaction system can be extended to a physical model network with an arbitrary topology by connecting the masses via dampened springs. The connections are formalized as a routing matrix T of dimensions r × c, where r denotes the number of mass points in the physical model network, and c represents the number of connecting springs, each having only two connections. A single mass can be connected to multiple springs in the network.",9
5799,From Tissue to Sound: Model-Based Sonification of Medical Imaging (vol9),,Interaction,"Module. An interaction module, act, excites the model P by applying force to one or more input masses of the model I ∈ T r×c . f maps the intensities of the input data to M, K, Z, and F . Therefore, the input force is propagated through the network according to the Eq. 2 and observed by the output masses O ∈ T r×c . To summarize, the output masses are affected by the oscillation of all masses activated in the model with various frequencies and corresponding amplitudes, resulting in a specific sound profile, i.e., tone color. This tone color represents the spatial structure and physical properties of the RoI, which is transformed into the features of the output sound. The wave propagation is significantly influenced by the model's topology and the structure of the inter-mass connections, which have great impact on activating spatial relevant features and sound quality.",9
5805,Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills (vol9),,Finetuning of LiveMAE for Skill Assessment,"After pre-training, we discard the image decoder and only use the pathway from the encoder to the kinematic decoder as our mapping φ. See Fig. ",9
5807,Self-supervised Sim-to-Real Kinematics Reconstruction for Video-Based Assessment of Intraoperative Suturing Skills (vol9),,Metrics and Baselines.,"Across the five institutions, we use 5-fold cross-valid ation to evaluate our model, training and validating on data from 4 institutions while testing on the 5th held-out institution. This allows us to test for generalization on unseen cases across both surgeons and medical centers. We measure and report the mean ± std. dev. for the two metrics: (1) Area-under-the-ROC curve (AUC) and (  To understand the benefits of each data modality, we compare LiveMAE against 3 setups: (1) train/test using only kinematics, (2) train/test using only videos, and (3) train using kinematic and video data while testing only on video (no live kinematics). For kinematics-only baselines, we use two sequential models (1) LSTM recurrent model ",9
5825,Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods (vol9),,ETC-LSTM.,"A simple architecture that consists of an LSTM layer with a hidden size of 128. Following hyperparameters tuning on the validation set, the ETC-LSTM was trained using an SGD optimizer with a constant learning rate of 0.1 and a batch size of 32 videos. ETCouple. ETCouple is a different approach to applying LSTM networks. In contrast to ETC-LSTM and similar methods which predict ETC for a single timestamp, here we randomly select one timestamp from the input video and set it as an anchor sample. The anchor is then paired with a past timestamp using a fixed interval of S = 120 s. The model is given two inputs, the features from the beginning of the procedure up to the anchor and the features up to the pair location. Instead of processing the entire video in an end-to-end manner, we only process past information and are thus able to use a bi-directional LSTM (hidden dimension is 128). The rest of the architecture contains a dropout layer (P = 0.5), the shared FC layer, and a Sigmoid function. We explored various hyperparameters and the final model was trained with a batch size of 16, an AdamW optimizer with a learning rate of 5 • 10 -4 , and a weight decay of 5 • 10 -3 . ETCformer. LSTM networks have been shown to struggle with capturing longterm dependencies ",9
5829,Estimated Time to Surgical Procedure Completion: An Exploration of Video Analysis Methods (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_16.,9
5838,Learnable Query Initialization for Surgical Instrument Instance Segmentation (vol9),,Contributions: (1),"We investigate the reason for the failure of transformerbased object detectors for the medical instrument instance segmentation tasks. Our analysis reveals that incorrect query initialization is to blame. We observe that recall of an instrument based on the initialized queries is a lowly 7.48% at 0.9 IOU, indicating that many of the relevant regions of interest do not even appear in the initialized queries, thus leading to lower accuracy at the last stage. (2) We observe that CNN-based object detectors employ a non-maximal suppression (NMS) at the proposal stage, which helps spread the proposal over the whole image. In contrast in transformer-based detection models, this has been replaced by taking the highest confidence boxes. In this paper, we propose to switch back to NMS-based proposal selection in transformers. (3) The NMS uses only bounding boxes and does not allow content interaction for proposal selection. We propose a Query Proposal Decoder block containing multiple layers of selfattention and deformable cross-attention to perform region-aware refinement of the proposals. The refined proposals are used by a transformer-based decoder backbone for the prediction of the class label, bounding box, and segmentation mask. (4) We show an improvement of 1.84% over the best-performing SOTA technique on the Endovis17 and 2.09% on the Endovis18 dataset as measured by ISI-IOU.",9
5840,Learnable Query Initialization for Surgical Instrument Instance Segmentation (vol9),,Transformer-based Instance Segmentation for Natural Images:,"On the other hand, transformer-based instance segmentation architectures ",9
5842,Learnable Query Initialization for Surgical Instrument Instance Segmentation (vol9),,Query Proposal Network: Content-based Inter-proposal Interaction:,"The top k region proposals from NMS are used to initialize the queries for the proposed Query Proposal Decoder (QPD). Note that the NMS works only on the basis of box coordinates and does not take care of content embeddings into account. We try to make up for the gap through the QPD module. The QPD module consists of self-attention, cross-attention and feed-forward layers. The self-attention layer of QPD allows the queries to interact with each other and the duplicates are avoided. We use the standard deformable cross-attention module as proposed in  For L mask , we use cross entropy and IOU (or dice) loss. The total loss is: Through hyper-parameter tuning, we set λ = [0.19, 0.24, 0.1, 0.24, 0.24]. We use a batch size of 8. The initial learning rate is set to 0.0001, which drops by 0.1 after every 20 epochs. We set 0.9 as the Nesterov momentum coefficient. We train the network for 50 epochs on a server with 8 NVidia A100, 40 GB GPUs. Besides QPN we use the exact same architecture as proposed in MaskDINO ",9
5844,Learnable Query Initialization for Surgical Instrument Instance Segmentation (vol9),,Ablation Study:,We perform an ablation on the EV18 dataset to show the importance of each block in the proposed methodology. The results of the same are summarised in Table ,9
5846,Learnable Query Initialization for Surgical Instrument Instance Segmentation (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 70.,9
5848,Learning Expected Appearances for Intraoperative Registration During Neurosurgery (vol9),,Contribution:,We propose a novel approach for patient-to-image registration that registers the intraoperative 2D view through the surgical microscope to preoperative MRI 3D images by learning Expected Appearances. As shown in Fig. ,9
5849,Learning Expected Appearances for Intraoperative Registration During Neurosurgery (vol9),,Intraoperative Registration Image Synthesis Pose Regression,Pose Sampling Fig. ,9
5854,Learning Expected Appearances for Intraoperative Registration During Neurosurgery (vol9),,Validation and Evaluation of Texture Invariance.,"We chose to follow a Leave-one-Texture-out cross-validation strategy to validate our model. This strategy seemed the most adequate to prevent over-fitting on the textures. We measured the ADD errors of our model for each case and report the results in  We observed a variance in the ADD error that depends on which texture is left out. This supports the need for varying textures to improve the pose estimation. However, the errors remain low, with a 2.01 ± 0.58 mm average ADD error, over all cases. The average ADD error per case (over all left-out textures) is reported in Table  Figure ",9
5856,Learning Expected Appearances for Intraoperative Registration During Neurosurgery (vol9),,Conclusion.,"We introduced Expected Appearances, a novel learning-based method for intraoperative patient-to-image registration that uses synthesized expected images of the operative field to register preoperative scans with intraoperative views through the surgical microscope. We demonstrated state-ofthe-art, real-time performance on challenging neurosurgical images using our method. Our method could be used to improve accuracy in neuronavigation and in image-guided surgery in general.",9
5862,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation (vol9),,Dose Prediction:,"The dose prediction model was trained on an in-house dataset comprising a total of 50 subjects diagnosed with post-operative GBM. This includes CT imaging data, segmentation masks of 13 OARs, and the GTV. GTVs were defined according to the ESTRO-ACROP guidelines  Segmentation Models: To develop and test the proposed approach, we employed a separate in-house dataset (i.e., different cases than those used to train the dose predictor model) of 50 cases from post-operative GMB patients receiving standard RT treatment. We divided the dataset into training (35 cases), validation (5 cases), and testing (10 cases). All cases comprise a planning CT registered to the standard MRI images (T1-post-contrast (Gd), T1-weighted, T2-weighted, FLAIR), and GT segmentations containing OARs as well as the GTV. We note that for this first study, we decided to keep the dose prediction model fixed during the training of the segmentation model for a simpler presentation of the concept and modular pipeline. Hence, only the parameters of the segmentation model are updated.",9
5863,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation (vol9),,Baselines and Implementation Details:,We employed the same U-Net ,9
5865,Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation (vol9),,RM AE,Although it has been shown that geometric-based segmentation metrics poorly correlate with the clinical end-goal in RT ,9
5882,Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography (vol9),,Uncertainty-Aware Feature Stylization (UFS).,"The UFS pipeline is shown in the right part of Fig.  where: μ(f ) and σ(f ) denote channel-wise mean and standard deviation of feature f , respectively. However, due to the complexity of real-world TEE, there are significant intra-domain differences, leading to uncertainties in the feature statistics of real data. To explore the potential space of unknown intra-domain shifts, instead of using fixed feature statistics, we generate multivariate Gaussian distributions to represent the uncertainty of the mean and standard deviation in the real data. Considering this, the new feature statistics of real features f r l , i.e. mean β(f r l ) and standard deviation α(f r l ), are sampled from , respectively and computed as: where: 2 are estimated from the mean and standard deviation of the batch B of real images, I ρ>0.5 is an indicator function and ρ ∼ U(0, 1). Finally, our UFS for l th layer is defined as: To this end, the proposed UFS approach can close the reality gap by generating real-stylized features with sufficient variations, so that the network interprets real data as just another variation.",9
5883,Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography (vol9),,Style Consistency Learning (SCL).,"Through the proposed UFS, we obtain the final real-stylized features f s→r f inal that contain a diverse range of real styles. The f s→r f inal can be seen as style perturbations of the final simulated features f s f inal . We thus incorporate a SCL step, that maximizes the similarity between f s f inal and f s→r f inal to enforce their consistency in the feature level, allowing the encoder to learn robust representations. Specifically, the SCL adds a projector independently of the regressor to transform the f s f inal (f s→r f inal ) in an extra feature embedding, and then matches it to the other one. To prevent the Siamese encoder and Siamese projector (i.e. the top two encoders and projectors in Fig.  is the negative cosine similarity between the input features f 1 and f 2 , and • 2 is L2-normalization. The SCL guides the network to learn domain-invariant features, via various style perturbations, so that it can generalize well to the different visual appearances of the real domain.",9
5884,Regressing Simulation to Real: Unsupervised Domain Adaptation for Automated Quality Assessment in Transoesophageal Echocardiography (vol9),,Task-Specific Learning (TL).,"While alleviating the style differences between the simulated and real domain, UFS filters out some task-specific information (e.g. semantic content) encoded in the simulated features, as content and style are not orthogonal  The TL performs AQA tasks for style variants to complement the loss of information due to feature stylization.",9
5892,Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery (vol9),,Self-Calibrated Heterogeneous Distillation (SH-Dist).,"Works have discussed the use of self-calibration to improve model performance  Subsequently, the self-calibrated feature map F t shall be propagated through the parallel classifier and detector for the multi-task prediction. Overall Framework. Figure  We set α = β = 1 and γ = 5 in our implementation. Furthermore, WA is deployed after training on each time period, to balance the weight bias of new classes on the classification layer. Through the combination of multiple distillation paradigms and model weight adjustment, we successfully realize the general continual learning framework in the VQLA scenario.",9
5896,Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery (vol9),,Acknowledgements,. This work was funded by ,9
5897,Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 7.,9
5910,Encoding Surgical Videos as Latent Spatiotemporal Graphs for Object and Anatomy-Driven Reasoning (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_62.,9
5919,Pelvic Fracture Reduction Planning Based on Morphable Models and Structural Constraints (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_31.,9
5933,Soft-Tissue Driven Craniomaxillofacial Surgical Planning (vol9),,Non-rigid Bony Movement Vector Estimation:,"We adopt the Attentive Correspondence assisted Movement Transformation network (ACMT-Net)  where N Fpre denotes the number of facial points P Fpre . On the other hand, desired facial movement V F is computed by subtracting P F des and P Fpre . V F is then concatenated with P Fpre and fed into a 1D convolution layer to encode the movement information. Then the movement feature of each bony point is estimtated by the normalized summary of facial features using R. Finally, the transformed bony movement features are decoded into movement vectors after being fed into one 1D convolution layer and normalization: Rigid Bony Movement Regression: Upon obtaining the estimated pointwise bony movements, they are added to the corresponding bony points, resulting in a non-rigidly transformed bony point set denoted as P B pdt . The resulting points are grouped based on their respective bony segments, with point sets P Spre and P S pdt representing each bony segment S before and after movement, respectively. To fit the movement between P Spre and P S pdt , we estimate the rigid transformations [R S , T S ] by minimizing the mean square error as follows: where i and N represent the i-th point and the total number of points in P Spre , respectively. First, we define the centroids of P Spre and P S pdt to be P Spre and P S pdt . The cross-covariance matrix H can be computed as Then we can use singular value decomposition (SVD) to decompose then the alignment minimizing E(R S , T S ) can be solved by Finally, the rigid transformation matrices are applied to their corresponding bone segments for virtual planning.",9
5940,Soft-Tissue Driven Craniomaxillofacial Surgical Planning (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 18.,9
5946,A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty (vol9),,Number of Samples and Comparison with Full LS.,The first setup of phantom experiments (Fig.  As shown in Fig.  Comparison with Other Methods. We compared our proposed method (Closed-form) with three other different measurement approaches in the second phantom experiment setup (Fig. ,9
5949,A Closed-Form Solution to Electromagnetic Sensor Based Intraoperative Limb Length Measurement in Total Hip Arthroplasty (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 35.,9
5959,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 25.,9
5972,SEDSkill: Surgical Events Driven Method for Skill Assessment from Thoracoscopic Surgical Videos (vol9),,Difference Regression Block.,"Most surgeries of the same type are performed in similar scenes, leading to subtle differences among surgical videos. For example, in MVR, the surgeon first stitches two lines on one side using a needle, and then passes one of the lines through to the other side, connecting it to the extracorporeal circulation tube. Although these procedures are performed in a similar way, the imperceptible discrepancies are very important for accurately assessing surgical skills. Hence, we first leverage the relation block to capture the inter-video semantics. We use the features of the pairwise videos, i.e., F i and F j , for clarity. Since attention  where linear layers, √ D controls the effect of growing magnitude of dot-product with larger D  After that, we use the difference regression block to map F i-j to the difference scores Δ Ŷ . Then, we minimize the error as follows: where ΔY is the ground-truth of the difference scores between the pair videos, which can be computed by |Y i -Y j |. By optimizing L dif f , the model would be able to distinguish differences between videos for precise SKA. Finally, the overall loss function of our proposed method is as follows: where λ dif f is the hyper-parameter to control the weight between two loss functions (set to 1 empirically).",9
5982,Point Cloud Diffusion Models for Automatic Implant Generation (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 11.,9
5985,Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions (vol9),,Backbone:,"To extract visual features, we utilize ResNet50 ",9
5986,Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions (vol9),,Learning Instrument-Aware Target Class Embeddings:,"To learn target features, we introduce a M ulti-C lass I nstrument-aware T ransformer ( MCIT ) that generates embeddings for each target class. In the standard transformer  MCIT learns meaningful class embeddings of the target enriched with visual and position semantics of the instruments. This instrument-awareness is useful to identify the interacting instrument-target pairs.",9
5987,Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions (vol9),,Learning Instrument-Target Interactions:,"To learn the interaction of the instrument and the target, we introduce a graph based framework I nteraction-Graph ( IG) that relies on the discriminative features of instrument instances and target class embeddings. We create an unidirectional complete bipartite graph, G = (U, V, E), where |U| = O and |V| = N denotes the source and destination nodes respectively, and edges E = {e u v , u ∈ U ∧ v ∈ V}. The node features of U and V correspond to the detected instrument instance features F i and target class embeddings N t respectively. We further project the nodes features to a lower dimensional space d using a linear layer Φ p . This setup provides an intuitive way to model instrument-tissue interactions as a set of active edges. Next, we apply message passing using GAT ",9
5988,Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions (vol9),,Learning Verbs:,"We concatenate the source and destination node features of all the edges E in G to construct the edge feature E f = {e f , e f ∈ R 2d }. Then, we compute the edge confidence score E s = {e s , e s ∈ R} for all edges E in G by applying a linear layer Φ e on E f . As a result, shown in Fig.  Triplet Detection: To perform target and verb association for each instrument instance i, first we select the active edge e i j that corresponds to the target class j = argmax(α(E i s )), where α denotes softmax function and E i s = {e u s , ∀e u s ∈ E s ∧ u = i}. For the selected edge e = e i j , we apply softmax on the verb logits to obtain the verb class id, k = argmax(α(y e v )). The final score for the triplet 〈i, k, j 〉 is given by p(e i j ) × p(y e v k ), where p denotes the probability score. Mixed Supervision: We train our model in two stages. In the first stage, we train MCIT to learn target classwise embeddings with target binary presence label with weighted binary cross entropy on target logits y t for multi-label classification task following Eq. 2: where C refers to total number of target classes, y c and ŷc denotes correct and predicted labels respectively, σ is the sigmoid function and W c is the class balancing weight from  To train IG, we apply categorical cross entropy loss on edge set E i s and verb logits y e v for all instrument instances i to obtain losses L e G and L v G respectively following Eq. 3: where M denotes the number of classes which is N for L e G and V + 1 for L v G . The final loss for training follows Eq. 4: where α and β denote the weights to balance the loss contribution.",9
5991,Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions (vol9),,Comparison with the Baseline:,"We obtain the code and weights of Rendezvous (RDV)  Ablation Study on the Spatial Annotation Need: Here, we study the impact of an instrument localization quality on triplet detection and how the target features can supplement fewer spatial annotations of the instruments for better triplet detection. We compare with ResNet-CAM-YOLOv5 ",9
5992,Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions (vol9),,Ablation Studies on the Components of MCIT-IG:,We analyze the modules used in MCIT-IG and report our results in Table ,9
5993,Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions (vol9),,Comparison with the State-of-the-Art (SOTA) Methods:,Results in Table ,9
5995,Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 48.,9
6004,High-Quality Virtual Single-Viewpoint Surgical Video: Geometric Autocalibration of Multiple Cameras in Surgical Lights (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 26.,9
6008,CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery (vol9),,CAT-ViL Embedding:,"In the following, the extracted features are processed into visual and text embeddings following VisualBERT ",9
6009,CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery (vol9),,Self-Attn,"Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Self-Attn Guided-Attn Guided-Attn Guided-Attn Guided-Attn Guided-Attn Guided-Attn Inspired by  Therefore, the visual embeddings shall be reconstructed with the original query, and the key and value of the text embeddings, which can realize the text embeddings to have instructive information interaction with the visual embeddings, and help the model to focus on the targeted image context related to the question. Six guided-attention layers are applied in our network. Thus, the correlation between questions and image regions can be gradually constructed. Besides, we also build six self-attention blocks for both visual and text embeddings to boost the internal relationship within each modality. This step can also avoid 'over' guidance and seek a trade-off. Then, the attended text embeddings and textguided attended visual embedding shall be output from the co-attention module and propagated through the gated module. Compared to the naive concatenation  Prediction Heads: The classification head, following the normal classification strategy, is a linear layer with Softmax activation. Regarding the localization head, we follow the setup in Detection with Transformers (DETR)  Loss Function: Normally, the cross-entropy loss L CE serves as our classification loss. The combination of L 1 -norm and Generalized Intersection over Union (GIoU) loss ",9
6014,CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery (vol9),,Acknowledgements,. This work was funded by ,9
6015,CAT-ViL: Co-attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 38.,9
6021,Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation (vol9),,Rat Colon Dataset.,"For real-world noisy dataset, we have collected rat colon OCT images using 800nm ultra-high resolution endoscopic spectral domain OCT. We refer readers to  Implementation Details. We adopt Deeplabv3+ ",9
6023,Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation (vol9),,Method,Deeplabv3+ ,9
6026,Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation (vol9),,Acknowledgements,. This work was supported by ,9
6027,Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation (vol9),,Data,Method mIOU (%) Sequence mIOU (%) Dice (%),9
6028,Rectifying Noisy Labels with Sequential Prior: Multi-scale Temporal Feature Affinity Learning for Robust Video Segmentation (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 9.,9
6036,Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues (vol9),,Regularizers.,We apply some regularization to address the limited information available in surgical scene reconstruction. We adopt 2D total variation (TV) loss for space planes in ,9
6037,Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues (vol9),,Warm-Up Training Strategy.,"Since single-view captures cannot provide valid scale information, we leverage pseudo ground truth depth maps D(r) generated by STTR-light  where ΔD(r) = | D(r) -D(r)| represents the absolute depth difference among valid depth values, δ is a threshold at which to change loss type. Considering that the predicted depth maps encounter a lot of unreliable depth values and missing areas  3 Experiments",9
6043,Neural LerPlane Representations for Fast 4D Reconstruction of Deformable Tissues (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_5.,9
6049,ACT-Net: Anchor-Context Action Detection in Surgery Videos (vol9),,Confidence Analysis:,"To analyze the model confidence, we take the best prediction for each instance to calculate the instance accuracy. We can observe   from Table ",9
6058,Intelligent Virtual B-Scan Mirror (IVBM) (vol9),,Conclusion.,"We presented a novel visualization concept that augments a selected volume cross-section with an intelligent virtual mirror for targeted instrument navigation in 4D OCT. We have provided a definition and implementation of an IVBM and demonstrated its potential to effectively support surgical tasks and to be integrated into a 4D OCT system. We demonstrated the IVBM in simulated vitreoretinal surgery, however, we intend to further apply this concept also to other 4D medical imaging modalities.",9
6059,Intelligent Virtual B-Scan Mirror (IVBM) (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4_40.,9
6078,A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation (vol9),,Luo et al. Ours,"The effectiveness of our proposed method lies in several aspects. First, renal interior structures are insensitive to solids and fluids inside the kidneys and can precisely characterize ureteroscopic images. Next, we define a structural point similarity measure as intersection of point sets between real and virtual structural regions. Such a measure does not use any point intensity information for the similarity calculation, leading to an accurate and robust similarity characterization under renal floating solids and fluids. Additionally, CTU images can capture more renal anatomical structures inside the kidneys compared to CT slices, still facilitating an accurate similarity computation. Our method still suffers from certain limitations. Figure ",9
6093,Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts (vol9),,Train-Test Split and Hyperparameter Tuning:,"The SOA surgical scene segmentation algorithms are based on a union of the datasets occlusion and no-occlusion, termed dataset original, which was split into a hold-out test set (166 images from 5 pigs) and a training set (340 images from 15 pigs). To enable a fair comparison, the same train-test split on pig level was used across all networks and scenarios. This also holds for the occlusion scenario, in which the dataset no-occlusion was used instead of original for training. All networks used the geometric transformations shift, scale, rotate, and flip from the SOA prior to applying the augmentation under examination. All hyperparameters were set according to the SOA. Only hyperparameters related to the augmentation under examination, namely the probability p of applying the augmentation, were optimized through a grid search with p ∈ {0.2, 0.4, 0.6, 0.8, 1}. We used five-fold-cross-validation on the datasets original, isolation zero, and isolation bgr to tune p such that good segmentation performance was achieved on both in-distribution and OOD data. Validation Strategy: Following the recommendations of the Metrics Reloaded framework ",9
6095,Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts (vol9),,Comparison to SOA Augmentations:,There is no consistent ranking across all six OOD datasets except for Organ Transplantation always ranking first and baseline usually ranking last (cf. Fig. ,9
6097,Semantic Segmentation of Surgical Hyperspectral Images Under Geometric Domain Shifts (vol9),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43996-4 59.,9
6101,TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope (vol9),,Synthesis Triplet Generation.,"The perspective view synthesis method aims to warp the original image I to generate a new image I . The warping process is based on the camera intrinsic matrix K, the depth map D of the original image and perturbation pose P 0 . For any point q in I, its depth value is denoted as z in D. The corresponding point q on the new image I is calculated by Eq.(  Given the depth maps generated from a pre-trained model, we perform perspective view synthesis with the same pose transformation P 0 on the three frames of raw triplet respectively.  Pose Consistent Loss. As shown in Fig. ",9
6105,TCL: Triplet Consistent Learning for Odometry Estimation of Monocular Endoscope (vol9),,Results.,"To evaluate the effectiveness of the proposed method, TCL was applied to MonoDepth2  Table  Ablations on Proposed Modules. We introduce two proposed modules to the baseline (MonoDepth2) separately. We additionally propose a simple version of AiC that computes Triplet Masks directly using two reference frames without warping, denoted as AiC*. From Table  Ablations on Different Dataset Amounts. To verify the effect of our proposed method on different amounts of training and validation sets, we utilize the main depth metric RMSE and pose metric ATE for comparison. In Fig. ",9
6137,Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising (vol10),,Relationship with Recent Methods:,The proposed weight modulation technique leveraged the recent concept of style-based image synthesis proposed in StyleGAN2 ,10
6138,Noise Conditioned Weight Modulation for Robust and Generalizable Low Dose CT Denoising (vol10),,Implementation Details:,"The proposed dynamic convolutional layer is very generic and can be integrated into various backbone networks. For our denoising task, we opted for the encoder-decoder-based UNet ",10
6156,ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer (vol10),,Acknowledgements,. This work was supported in part by the ,10
6160,CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning (vol10),,Multi-Domain Image-To-Image Translation.,"We apply generator-guided contrastive learning (GGCL) to StarGAN  where L D and L G are the discriminator and generator losses, respectively. They both have L adv , which is the adversarial loss. L r cls and L f cls are the domain classification losses for a real and fake image, respectively. L cyc , which is the cycle consistency loss, has an importance for the translated image to maintain the structure of source image.",10
6161,CT Kernel Conversion Using Multi-domain Image-to-Image Translation with Generator-Guided Contrastive Learning (vol10),,Patch-Based Contrastive Learning.,"Our method is to add PatchNCE loss  Total Objective. GGCL follows the concept of GGDR, which the generator supervises the semantic representations to the discriminator, so it is a kind of the discriminator regularization. Discriminator conducts real/fake classification, domain classification and semantic label map segmentation, so it can be also a kind of the multi-task learning  where λ cls , λ cyc and λ ggcl are hyper-parameters that weight the importance of domain classification loss, cycle consistency loss and GGCL loss, respectively. We used λ cls = 1, λ cyc = 10 and λ ggcl = 2 in our experiments. 3 Experiments and Results Implementation Details. We maintained the original resolution 512 × 512 of CT images and normalized their Hounsfield unit (HU) range from [-1024HU ~3071HU] to [-1 ~1] for pre-processing. For training, the generator and the discriminator were optimized by Adam  Architecture Improvements. Instead of using original StarGAN ",10
6169,$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment (vol10),,Self-hyperedge Reasoning:,We utilize a self-hyperedge reasoning network to capture the self-correlation of hyperedge features. Propagating features achieve this by hyperedge weight matrix within each hypergraph. It can be written as: where s/t ∈ R D×D denotes the learnable parameters of self-hyperedge reasoning network. And σ(•) is the nonlinear activation function.,10
6170,$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment (vol10),,Bipartite Hypergraph Matching:,"We use bipartite matching to determine a soft correspondence matrix between two subjects, which is achieved using the following expression: , where Gbm consists of an Affinity layer, an Instance normalization layer, a quadratic constrain (QC) layer, and a Sinkhorn layer. Initially, the affinity matrix is computed as t , where M (k) ∈ R D×D is the learnable parameter matrix in the affinity layer. Next, we apply instance normalization  where , and this loss can optimize the Eq. 1. The k -scale soft assignment matrix of the MsHM l -th layer output is",10
6171,$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment (vol10),,Cross-Hyperedge Reasoning:,"We further enhance the hyperedge features by exploring cross-correlation through cross-hyperedge reasoning. Different from self-hyperedge reasoning, the proposed cross-hyperedge reasoning enables subject-aware message propagation, facilitating effective interaction between subjects. The more similar a pair of hyperedges is between two subjects, the better features will be aggregated in better alignment. It can be written as: where f cross consists of a feature concatenate and a fully connected layer. Finally, the new GHs features with a symmetric normalization can be written as: where σ(•) is the nonlinear activation function. s/t are the diagonal node degree matrix and hyperedge degree matrix, respectively. Φ ∈ R D×D denotes the learnable parameters of cross-hyperedge reasoning network.",10
6176,$$\mathrm {H^{2}}$$GM: A Hierarchical Hypergraph Matching Framework for Brain Landmark Alignment (vol10),,Sensitivity Analysis:,We conduct experiments with varying hyperparameters to investigate the sensitivity of β in Eq. 8 and record the results in Table  Qualitative Analysis: Figure ,10
6191,PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer (vol10),,Number of Iterations and Decoder Type:,"In this section, we explore how the number of iterations and decoder type of block in the long-range correlation decoder affect the registration performance. We select three different blocks, i.e. LCD, CNN and GRU ",10
6193,PIViT: Large Deformation Image Registration with Pyramid-Iterative Vision Transformer (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_57.,10
6196,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations (vol10),,Multi-modal Variational Auto-Encoders (MVAE).,Multi-modal VAEs  (2),10
6203,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations (vol10),,Conclusion and Future Work.,"We introduced a multi-modal hierarchical variational auto-encoder to perform unified MR/iUS synthesis. By approximating the true posterior using a combination of unimodal approximates and optimizing the ELBO with multi-modal and uni-modal examples, MHVAE demonstrated state-of-the-art performance on the challenging problem of iUS and MR synthesis. Future work will investigate synthesizing additional imaging modalities such as CT and other MR sequences.",10
6204,Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 43.,10
6212,SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid (vol10),,Inter-patient Task on Abdomen:,The Abdomen CT dataset ,10
6213,SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid (vol10),,Inter-patient Task on HeadNeck:,The HeadNeck CT dataset ,10
6214,SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid (vol10),,Intra-patient Task on Lung:,"The images are acquired as part of the radiotherapy planning process for the treatment of malignancies. We collect a lung 4D CT dataset containing 35 patients, each with inspiratory and expiratory breath-hold image pairs, and take this dataset as an extra test set. Each image has labeled malignancies, and we try to align two phases for motion estimation of malignancies. The images are resampled to a spacing of 2 × 2 × 2 mm and cropped to 256 × 256 × 112. All images are used as testing cases. Evaluation Metrics: We use the average Dice score (DSC) to evaluate the accuracy and compute the standard deviation of the logarithm of the Jacobian determinant (SDlogJ) to evaluate the plausibility of deformation fields, also comparing running time (T test ) on the same hardware.",10
6218,SAMConvex: Fast Discrete Optimization for CT Registration Using Self-supervised Anatomical Embedding and Correlation Pyramid (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 53.,10
6220,Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis (vol10),,Contribution:,"In this paper, we propose a new method for video diffusion  Related Work: Video Generation has been a research area within computer vision for many years now. Prior works can be organized in three categories: (1) pixel-level autoregressive models  Ultrasound simulation has been attempted with three major approaches: (1) physics-based simulators  LVEF is a major metric in the assessment of cardiac function and diagnosis of cardiomyopathy. The EchoNet-dynamic dataset ",10
6224,Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_14.,10
6233,Implicit Neural Representations for Joint Decomposition and Registration of Gene Expression Images in the Marmoset Brain (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 61.,10
6237,DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images (vol10),,Methzod,"Further Analysis on Robustness. We examined the robustness of our model to variations in both Z-axis resolutions and domain shifts. Specifically, we investigated the following: (a) Robustness to unseen anisotropic spatial factors. The algorithm may encounter unseen anisotropic resolution due to the need for different imaging settings in practical applications. To assess the model's robustness to unseen anisotropic factors, we evaluated the model trained with the anisotropic factor α = 4. Then we do inference under the scenario of anisotropic factor α = 8. For those methods with a fixed super-resolution factor, we use cubic interpolation to upsample the reconstructed result by 2x along the axis. (b) Robustness to the domain shifts. When encountering unseen data in the real world, domain shifts often exist, such as differences in biological structure features and physical resolution, which can impact the model's performance  Ablation Study. We conducted extensive ablation experiments Fig. ",10
6240,DiffuseIR: Diffusion Models for Isotropic Reconstruction of 3D Microscopic Images (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_31.,10
6245,Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction (vol10),,(b) and (d).,"Encoding Stage. In most DL-based MAR methods, the networks take a metalcorrupted CT image as input and map it to a metal-free CT image domain. Noise-related features are involved in the entire process, which negatively affects the restoration of clean images. In the principal component analysis (PCA)-based image denoising method  where λ is a small constant to prevent division by zero and we set λ = 1e -7 in this paper. Feature maps with high scores will be selected. Therefore, we can dynamically select different feature maps according to the inputs. Decoding Stage. At the encoding stage, features that are helpless to reconstruct the clean image are filtered out. Decoder then maps the remaining features, which contain useful information, back into the image domain. To push the generated image to fall into the clean image domain, we employ conditional normalization layers ",10
6246,Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction (vol10),,Metal Artifacts Reduction and Generation Stage.,"The framework consists of four image translation branches: two metal artifacts reduction branches I s ←I f and I c ←I f , and two metal artifact generation branches I f ←I s and I f ←I c . (1) I s ←I f : In this transformation, we employ G s2f to learn the mapping from the synthetic metal-affected image domain to the metal-free image domain, which is denoted as: where X s is synthetic metal-affected CT image in I s and X s2f is the corrected result of X s . According to Eq. 1, the metal artifacts A s can be obtained as follows: (2) I c ←I f : In this transformation, we use G c2f to learn the mapping from the clinical metal-affected image domain to the metal-free image domain, resulting in metal-corrected CT image X c2f and the metal artifacts A c . This process is the same as the transformation of I s ←I f and can be formulated as follows: where X c is clinical metal-affected CT image in I c . (3): I f ←I s : We use the artifacts of X s to obtain a synthetic domain metalcorrupted image X c2s by adding A s to the learned metal-free CT image X c2f : (4): I f ←I c : Synthesizing clinical domain metal-corrupted image X s2c can be achieved by adding A c to the learned metal-free CT image X s2f :",10
6250,Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction (vol10),,MAR Performance on SY:,The quantitative scores are presented in Table  The sizes of the 16 metal implants in the testing dataset are:  MAR Performance on CL1: Figure ,10
6251,Building a Bridge: Close the Domain Gap in CT Metal Artifact Reduction (vol10),,MAR Performance on CL2:,"To assess the generalization capability of our method, we further evaluated MAR performance on CL2 with the model trained on SYN and CL1. Two practical cases are presented in Fig. ",10
6262,StructuRegNet: Structure-Guided Multimodal 2D-3D Registration (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 73.,10
6272,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_66.,10
6282,RESToring Clarity: Unpaired Retina Image Enhancement Using Scattering Transform (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 45.,10
6290,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation (vol10),,Methods,"Primary: MRI-to-CT Secondary: CycleGAN  We perform an ablation study by removing the cycle shape consistency loss (w/o Shape). Compared with shape-CycleGAN, MaskGAN using only a mask loss significantly reduces MAE by 6.26%. The combination of both mask and cycle shape consistency losses results in the largest improvement, demonstrating the complementary contributions of our two losses. Robustness to Error-Prone Coarse Masks. We compare the performance of our approach with shape-CycleGAN ",10
6292,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 6.,10
6296,Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction (vol10),,Algorithm 1. The Linearized Alternating Minimization Algorithm (LAMA),"Input: Initializations: x0, z0, δ, η, ρ, γ, ε0, σ, λ 1: ",10
6299,Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction (vol10),,Definition 1. (Clarke subdifferential). Suppose that,"where w 1 , v 1 stands for the inner product in R n and similarly for w 2 , v 2 .",10
6300,Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction (vol10),,Definition 2. (Clarke stationary point) For a locally Lipschitz function f defined as in,"We can have the following convergence result. All proofs are given in the supplementary material. Theorem 1. Let {Y k = (x k , z k )} be the sequence generated by the algorithm with arbitrary initial condition Y 0 = (x 0 , z 0 ), arbitrary ε 0 > 0 and ε tol = 0. Let { Ỹl } =: (x k l +1 , z k l +1 )} be the subsequence, where the reduction criterion in the algorithm is met for k = k l and l = 1, 2, .... Then { Ỹl } has at least one accumulation point, and each accumulation point is a Clarke stationary point.",10
6305,Learned Alternating Minimization Algorithm for Dual-Domain Sparse-View CT Reconstruction (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 17.,10
6309,TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms (vol10),,Feature Mapping:,"The feature mapping module is designed for projecting the sequence data back to the sinogram. Concretely, ) and then fed into a linear projection layer to reduce the channel dimension from C to C s . Through these operations, the residual sinogram S R ∈ R C s ×H s ×W s of the same dimension as S L , is obtained. Finally, following the spirit of residual learning, S R is directly added to the input S L to produce the output of SE-Former, i.e., the predicted denoised sinogram",10
6311,TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms (vol10),,Spatial-Spectral Transformer Layer (SSTL):,As shown in Fig. ,10
6312,TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms (vol10),,Window-based Spatial Multi-Head Self-Attention (W-SMSA):,"Denoting the input feature embedding of certain W-SMSA as e in ∈ R C I ×H I ×W I , where H I ,W I and C I represent the height, width and channel dimension, respectively. As depicted in Fig. ",10
6313,TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms (vol10),,Global Frequency Parser (GFP):,"After passing the W-SMSA, the feature e spa are already spatially representative, but still lack accurate spectral representations in the frequency domain. Hence, we propose a GFP module to rectify the high-frequency component in the frequency domain. As illustrated in Fig.  The parameterized attentive map A can adaptively adjust the frequency components of the frequency domain and compel the network to restore the high-frequency part to resemble that of the supervised signal, i.e., the corresponding real SPET images (ground truth), in the training process. Finally, we reverse e spe back to the image domain by adopting 2D IDFT, thus obtaining the optimized feature e spa = DFT (e spe ). In this manner, more high-frequency details are preserved for generating shaper constructions.",10
6317,TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms (vol10),,Evaluation on Clinical Diagnosis:,"To further prove the clinical value of our method, we further conduct an Alzheimer's disease diagnosis experiment as the downstream task. Specifically, a multi-layer CNN is firstly trained by real SPET images to distinguish between NC and MCI subjects with 90% accuracy. Then, we evaluate the PET images reconstructed by different methods on the trained classification model. Our insight is that, if the model can discriminate between NC and MCI subjects from the reconstructed images more accurately, the quality of the reconstructed images and SPET images (whose quality is preferred in clinical diagnosis) are closer. As shown in Fig.  Ablation Study: To verify the effectiveness of the key components of our TriDo-Former, we conduct the ablation studies with the following variants: (1) replacing SE-Former and SSR-Former with DnCNN ",10
6320,MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI (vol10),,Contribution:,"Our method (MoCoSR) alleviates the need for semantic knowledge and manual paired-annotation of individual structures and the requirement for acquiring multiple image stacks from different orientations, e.g.,  There are several methodological contributions of our work: (1) First, to account for non-isotropic voxel sizes of abdominal images, we reconstruct spatial resolution from corrupted bowel MR images by enforcing cycle consistency. (2) Second, volumes are corrected by incorporating latent features in the LR domain. The complementary spatial information from unpaired quality images is exploited via cycle regularisation to provide an explicit constraint. Third, we conduct extensive evaluations on 200 subjects from a UK Crohn's disease study, and a public abdominal MRI dataset with realistic respiratory motion. (3) Experimental evaluation and analysis show that our MoCoSR is able to generate high-quality MR images and performs favourably against other, alter-native methods. Furthermore, we explore confidence in the generated data and improvements to the diagnostic process. (4) Experiments with existing models for predicting the degree of small bowel inflammation in Crohn's disease patients show that MoCoSR can retain diagnostically relevant features and maintain the original HR feature distribution for downstream image analysis tasks. Related Work: MRI SR. For learning-based MRI super-resolution,  Automated Evaluation of IBD. In the field of machine learning and gastrointestinal disease, ",10
6323,MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI (vol10),,Setting:,"We compare with interpolation of bicubic and bilinear techniques, rapid and accurate MR image SR (RAISR-MR) with hashing-based learning ",10
6324,MoCoSR: Respiratory Motion Correction and Super-Resolution for 3D Abdominal MRI (vol10),,Results:,"The quantitative results are presented in Table  Complete results including LR degraded image, SR image reconstructed by MRESR, CMRSR, and MoCoSR, are shown in Table  Discussion: According to the sensitivity analysis and comparison results, our MoCoSR method shows superior results compared to the forward adversarial reconstruction algorithms and encoder-decoder structures. Combining multiscale image information in the feature space of different resolution image domains yields better results than inter-domain integration. The cycle consistency network splits the different resolution spaces and latent space, which facilitates the flexibility of the neural network to customize the MC according to the specific purpose and ensures consistency of the corrected data with the unpaired data. Furthermore, although these methods can obtain acceptable SSIM and PSNR, the key features used by the classifier for downstream tasks are potentially lost during the reconstructions. Conversely, the reconstruction result will implicitly cause domain shift. This leads to a distribution shift in the samples, which makes the disease prediction biased as shown in Fig. ",10
6346,FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 24.,10
6351,ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising (vol10),,Patch-Wise Non-contrastive Module.,"To better learn anatomical representations, we introduce a patch-wise non-contrastive module, also shown in Fig.  We then select the top-4 positive patches {f g } j∈P (i) based on s(i, j), where P (i) is a set of selected patches (i.e., |P (i) | = 4). To obtain patch-level features g (i) ∈ R 512 for each patch f (i) g and its positive neighbors, we aggregate their features using global average pooling (GAP) in the patch dimension. For the local representation of f (i) g , we select positive patches as same as P (i) , i.e., {f (j) g } j∈P (i) . Formally, From the patch-level features, the online network outputs a projection z g (i) = p g (g (i) ) and a prediction q (z g (i) ) while target network outputs the target projection z g (i) = p g (g (i) ). The projection and prediction are both multilayer perceptron (MLP). Finally, we compute the global MSE loss between the normalized prediction and target projection  where N g pos is the indices set of positive samples in the patch-level embedding. Pixel-Wise Contrastive Module. In this module, we aim to improve anatomical consistency between the denoised CT and NDCT using a local InfoNCE loss  neg |)×256 . The local InfoNCE loss in the pixel level is defined as where N l pos is the indices set of positive samples in the pixel level. v and v (j) l ∈ R 256 are the query, positive, and negative sample in z (i) l , respectively.",10
6356,ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising (vol10),,Acknowledgements,. This work was supported in part by ,10
6357,ASCON: Anatomy-Aware Supervised Contrastive Learning Framework for Low-Dose CT Denoising (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 34.,10
6363,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration (vol10),,3D Pseudo Brain MRI.,"To evaluate the performance of atlas-based registration, it is essential to have the correct mapping of pathological regions to healthy brain regions. To create such a mapping, we created a pseudo dataset by utilizing images from the OASIS-1 and BraTS2020. From the resulting t1 sequences, a pseudo dataset of 300 images was randomly selected for further analysis. Appendix B provides a detailed process for creating the pseudo dataset. Real Data with Landmarks. BraTS-Reg 2022  Atlas-Based Registration. After creating the pseudo dataset, we warped brain MR images without tumors to the atlas and used the resulting deformation field as the gold standard for evaluation. We then evaluated the mean deformation error (MDE)  Longitudinal Registration. To perform the longitudinal registration task, we registered each pre-operative scan to the corresponding follow-up scan of the same patient and measured the mean target registration error (TRE) of the paired landmarks using the resulting deformation field. For this purpose, we leveraged SegNet, trained on BraTS2020, to segment the tumor of BraT-SReg2022 and separated the landmarks into two regions: near tumor and far from tumor. Figure  To quantitatively evaluate the segmentation capability of our proposed framework, we compared its performance with other unsupervised segmentation techniques methods, including unsupervised clustering toolbox AUCseg  Ablation Study. We compared the performance of the InpNet trained with histogram matching (HM) and the SegNet trained with ground truth masks (Supervised). The results, shown in Table ",10
6365,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 51.,10
6374,Importance Weighted Variational Cardiac MRI Registration Using Transformer and Implicit Prior (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_55.,10
6382,Unsupervised 3D Registration Through Optimization-Guided Cyclical Self-training (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_64.,10
6384,Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis (vol10),,Fig. 1. Overview of our proposed two-stage Make-A-Volume framework.,"A latent diffusion model is used to predict the noises added to the image and synthesize independent slices from Gaussian noises. We insert volumetric layers and quickly finetune the model, which extends the slice-wise model to be a volume-wise model and enables synthesizing volumetric data from Gaussian noises.",10
6395,Fast Reconstruction for Deep Learning PET Head Motion Correction (vol10),,Method,"Val. loss Test Set Total loss Translation (mm) Rotation ( Quantitative Evaluation. For quantitative comparisons of motion tracking, we compare MSE loss in the validation set and test set. We calculate MSE for the 6 parameter rigid motion as well as the translation and rotational components separately. To verify feasibility of traditional intensity-based registration method on FRIs, we use BIS with a multi-resolution hierarchical representation (3 levels) and minimize the sum of squared differences (SSD) similarity metric (Fig.  (ii) DL-HMC with FRI as input (DL-HMC FRI); (iii) proposed network with PCI as input (Proposed PCI); (iv) proposed network with FRI as input (Proposed FRI); and (v) proposed method with FRI but without the data augmentation module (Proposed w/o DA); Results demonstrate that the proposed network with FRI input provides the best motion tracking performance in both validation and testing data. We also observes that using FRI yields a lower loss for the proposed network, indicating that high image quality enhanced the motion correction performance. For testing translation results, Proposed PCI outperforms Proposed FRI and has similar total motion loss, which indicates that the proposed network can still estimate motion on testing subjects even with noisy input. Figure  Qualitative Reconstruction Evaluation. After inference, the 6 rigid degrees of freedom transformation estimated from the network were used to reconstruct the PET images using Motion-compensation OSEM List-mode Algorithm for Resolution-Recovery Reconstruction (MOLAR) algorithm  Based on the tracer distribution of 18 F-PEB, we selected some frames from reconstructed images to illustrate the proposed FRI motion correction performance. In general, the Proposed FRI results in qualitatively enhanced anatomical interpretation of PET images. In Fig.  In addition, brain region of interest (ROI) analyses were also performed for quantitative use. Each subject's MR image was segmented into 74 regions using FreeSurfer software ",10
6397,Fast Reconstruction for Deep Learning PET Head Motion Correction (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 67.,10
6407,Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_23.,10
6439,An Unsupervised Multispectral Image Registration Network for Skin Diseases (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 68.,10
6454,Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_3.,10
6464,Learning Deep Intensity Field for Extremely Sparse-View CBCT Reconstruction (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 2.,10
6471,Dual Arbitrary Scale Super-Resolution for Multi-contrast MRI (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_27.,10
6482,Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography (vol10),,Assumptions Equation,"signal based on the tissue displacement  Linear Elasticity. Physical models of MRE typically assume there is harmonic motion and a linear, isotropic stress-strain relation. Then tissue displacement is a complex vector field u : Ω → C 3 defined on spatial domain Ω ⊂ R 3 , and shear elasticity is characterized by the shear modulus, a complex scalar field μ : The first equation of motion translates into the general form PDE shown in Table ",10
6484,Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography (vol10),,Incorporating Anatomical Information.,"Prior work has demonstrated that tissue elasticity can be accurately predicted from anatomical MRI  We designed our models based on the SIREN architecture, which uses sine activation functions to better represent high spatial frequencies ",10
6490,Physics-Informed Neural Networks for Tissue Elasticity Reconstruction in Magnetic Resonance Elastography (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 32.,10
6502,CycleSTTN: A Learning-Based Temporal Model for Specular Augmentation in Endoscopy (vol10),,Acknowledgments,". This research was funded in part, by the ",10
6513,An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 5.,10
6521,Inverse Consistency by Construction for Multistep Deep Registration (vol10),,OASIS Brain MRI.,We use the OASIS-1 ,10
6531,Geometric Ultrasound Localization Microscopy (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_21.,10
6541,Transformer-Based Dual-Domain Network for Few-View Dedicated Cardiac SPECT Image Reconstructions (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 16.,10
6550,MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images (vol10),,1-⋃ ⋃,"Fig.  To alleviate these issues, in this paper, we propose a model-driven equivariant proximal network, called MEPNet, which is naturally constructed based on the CT imaging geometry constraint for this specific SVMAR task, and takes into account the inherent prior structure underlying the CT scanning procedure. Concretely, we first propose a dual-domain reconstruction model and then correspondingly construct an unrolling network framework based on a derived optimization algorithm. Furthermore, motivated by the fact that the same organ can be imaged at different angles making the reconstruction task equivariant to rotation ",10
6556,MEPNet: A Model-Driven Equivariant Proximal Network for Joint Sparse-View Reconstruction and Metal Artifact Reduction in CT Images (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_11.,10
6560,Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint (vol10),,Compute Relaxation Times for Voxels with More Than One Tissue:,"In real practice there exist many voxels with more than one tissue kind present in them, and is the reason for discontinuities present in Fig.  After approximating T 1,l and T 1,h for all voxels (and empirically choosing the T R h and θ h ) the r is computed using Eq. ( ",10
6565,Estimation of 3T MR Images from 1.5T Images Regularized with Physics Based Constraint (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 13.,10
6572,Accurate Multi-contrast MRI Super-Resolution via a Dual Cross-Attention Transformer Network (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 30.,10
6575,Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras (vol10),,Search Space Regularization.,"In what follows we propose an algorithm that estimates target's indicator χ from data y, i.e. solves the inverse FDOT problem. To reduce the ill-posedness of the inverse problem (  The first regularization represents an assumption that the correct χ is a binary vector. Since binary constraints are not convex, we adopt a more relaxed condition on χ referred to as the box constraints: 0 ≤ χ ≤ 1. The second regularization describes the piece-wise constant structure of the indicator χ, and is referred to as the piece-wise total variation (PTV). It is obtained by extending the notion of total variation which has been successfully applied in optical tomography. To this end, assume m(j), n(j), and j ∈ I are indices corresponding to the j-th pair of neighboring nodes. Let the domain Ω be split into N ptv non-overlapping subdomains, e.g., cuboids, and the index I is correspondingly split into non-overlapping sub-indices I i , i = 1, . . . , N ptv of nodes pairs that belong to Ω i . PTV is obtained as a sum of total variations computed using sub-indices I i : and is also written in a matrix form assuming matrix V encodes subtraction across node pairs across all sub-indices. The third regularization aims to reduce a null space of the inverse problem in the boundary layer of a thickness , reflecting the assumption that the target is under the surface. It is referred to as the boundary regularization and is defined as W χ = 0 where W selects components of χ that belong to the boundary layer. Finally, the fourth regularization referred to as the minimum volume regularization requires that χ has at least m 0 non-zero components: Optimization Framework. In this subsection we present an Incremental Fluorescent Target Reconstruction (IFTR) scheme solving the inverse problem (1)-  x as the unique solution of linear excitation equation: then (ii) fix the obtained φ n x and compute χ n+1 as the unique solution of one of the 3 convex optimization problems:",10
6576,Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras (vol10),,Variant I. This variant relies upon direct inversion of the emission equation matrix S -1,"m to find χ n+1 : Variant II. This variant imposes the emission equation as an inequality constraint: Here, depending on the value of p we take x 1 = x 2 or x 2 = 1 T x and E m is a parameter defining emission equation constraint tolerance. Variant III. This variant uses the emission equation as a term of the loss function: We note that all the three variants depend on parameter p = 1, 2 which defines the type of optimization problem that should be solved: i) if p = 1 we get conic optimization problems of the loss function in the form • 2 which would be treated as conic constraints); ii) if p = 2 we get quadratic optimization problems. To get a good initial guess for χ 0 we borrow from the Born approximation which suggests that excitation field φ x can be approximated by the background excitation obtained by solving excitation equation with no ICG, i.e., χ 0 = 0. Iterating this splitting method for n = 0, 1, 2, . . . we obtain a sequence of updates χ n that converge into a vicinity of the true χ provided data is ""representative enough"". We conclude the presentation of IFTR scheme with stopping criteria of the iterative process. For this we use a standard Dice coefficient d(•, •) and a binary projector b(•): the scheme stops once the following condition is met",10
6580,Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras (vol10),,A Continuous formulation of the FDOT problem,"Near-infrared photon propagation in tissue like media is described by the following coupled system of PDEs: Here (  3 μ ax/mi + μ ax/mf + μ sx/m , k x/m = μ ax/mi + μ ax/mf  The system (  where γ = 2.5156 -dimensionless constant depending on the optical reflective index mismatch at the boundary.",10
6581,Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 49.,10
6589,Computationally Efficient 3D MRI Reconstruction with Adaptive MLP (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 19.,10
6593,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration (vol10),,Architecture and Training.,"We use the same feed-forward 3D CNN to process all data modalities. The proposed model is composed of residual blocks  Augmentation on the training data is used to make the model as robust as possible while leaving the target similarity unchanged. In particular, we apply the same random rotation to both patches, randomly change the sign and apply random linear transformation on the intensity values. We train our model for 35 epochs using the L2 loss and batch size of 256. The training converges to an average patch-wise L2 error of 0.0076 on the training set and 0.0083 on the validation set. The total training time on an NVIDIA RTX4090 GPU is 5 h, and inference on a 256 3 volume takes 70 ms. We make the training code and preprocessed data openly available online",10
6599,DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 72.,10
6614,3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions (vol10),,Class-Specific Conditional Features.,"A distinctive feature of the tooth reconstruction task is that teeth with the same number share properties such as surface and root shapes. Hence, we propose to use tooth class information in combination with a segmented tooth patch from PX. The tooth class is processed by a learnable embedding layer which outputs a class embedding vector. Next, we create a square patch of the tooth using the segmentation output as follows. A binary mask of the segmented tooth is generated by applying thresholding to the segmentation output. A tooth patch is created by cropping out the tooth region from the input PX, i.e., the binary mask is applied (bitwise AND) to the input PX to obtain the patch. The segmented tooth patch is subsequently encoded using a pre-trained ResNet18 model  Our approach differs from previous approaches, such as Occupancy Networks  Conditional eXcitation. To effectively inject 2D observations into the reconstruction network, we propose Conditional eXcitation (CX) inspired by Squeezeand-Excitation Network (SENet)  where c is the condition vector, σ is a gating function, W is a learnable weight matrix, α is a hyperparameter for the excitation result, and F ext is the excitation function. We use sigmoid function for σ, and component-wise multiplication for the excitation, F ext (e, x) = e ⊗ x. The CX module is depicted in Fig. ",10
6617,3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions (vol10),,Acknowledgements,. This work was supported by the ,10
6618,3D Teeth Reconstruction from Panoramic Radiographs Using Neural Implicit Functions (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 36.,10
6626,Progressively Coupling Network for Brain MRI Registration in Few-Shot Situation (vol10),,OASIS IXI,"Joint training (JTrain) introduces semi-supervised loss for registration. And the coupling feature connection (Couple) improves the performance of registration by guiding the deformation field in advance to map semantic features. Position embedding (PEmbedding) assists the field estimator in understanding the correlation between voxels. In addition, we further analyse the factors that affect the performance of segmentation (see Table ",10
6631,S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences (vol10),,Deformable Correspondence Estimation.,"PDMs require correspondences between samples to model the statistical distribution of the organ. Inspired by methods for geometric shape correspondence, we propose to estimate a functional mapping T to correspond high-level semantics from two input shapes, X i and X j . The LBO extends the Laplace operator to Riemannian manifolds, capturing intrinsic characteristics of the shape independent of its position and orientation in Euclidean space. It can be efficiently computed on a surface mesh using, for example, the cotangent weight discretization scheme  Here, A T φ (Di) ∈ R m×m denotes the transformed descriptor, written in the basis of the LBO eigenfunctions of shape X i and C ij ∈ R m×m represents the optimal functional mapping from the descriptor space of X i to the one of X j . Inspired by existing works on shape correspondence ",10
6632,S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences (vol10),,Training and Inference.,"During training, two shapes are sampled from the dataset, and the pipeline is optimized with Eq. 1. We increase model robustness by augmenting with rotations and small surface deformations. The point cloud is sub-sampled in each training iteration using farthest point sampling with random initialization. During inference, our model predicts pairwise correspondences. To accumulate these over an entire dataset of N shapes, we choose a template shape X T = arg min Xi N j=0 1 i =j L (C ij , C ji ) as the instance with the lowest average loss to all other shapes in the dataset. As in  Statistical Shape Modeling. We use the PDM  then s ∼ N X, S , which is the desired distribution of the model. For the above, the points must be in correspondence across the samples. We thus use the correspondences generated in Sect. 3 to construct the PDM.",10
6635,S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences (vol10),,Experiment 3: Thyroid Pseudo-label Generalization,"To further highlight the proposed methods' robustness to network-generated segmentation labels, we additionally measure the reconstruction ability of SSMs created from pseudo-labels on manually annotated thyroid labels under the Chamfer distance (Ours: 1.05 ± 0.10 mm, Shapeworks: 1.84 ± 0.40 mm). Notably, the proposed PDM on pseudo-labels generalizes better than the SSM built on few manual labels (1.25 ± 0.11 mm; see Table ",10
6637,S3M: Scalable Statistical Shape Modeling Through Unsupervised Correspondences (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 44.,10
6642,Self-supervised MRI Reconstruction with Unrolled Diffusion Models (vol10),,Self-Supervised Training:,"For self-supervised learning, we adopt a k-space masking strategy for diffusion models  where • 1 denotes the L1-norm, F denotes 2D Fourier Transform, C are coil sensitivities, x us is the image derived from undersampled acquisitions, and M l is the random sub-mask within the main undersampling mask M. Here x t recon is the output of the unrolled denoiser network (R θ ) at time instant t ∈ {T, T -1, ..., 0} where M p is the sub-mask of the remaining points in M after excluding M l . Inference: To speed up image sampling, inference starts with zero-filled Fourier reconstruction of the undersampled acquisitions as opposed to a pure noise sample. Conditional diffusion sampling is then performed with the trained diffusion model that iterates through cross-attention transformers for denoising and data-consistency projections. For gradual denoising, we introduce a descending random noise onto the undersampled data within data-consistency layers. Accordingly, the reverse diffusion step at time-index t is given as where low ∼ N (0, 0.1I) and z ∼ N (0, I). Unrolled Denoising Network R θ (.): SSDiffRecon deploys an unrolled physics-guided denoiser in the diffusion process instead of UNET as is used in  where β u,v j ∈ R 3×3 is the convolution kernel for the u th input channel and the v th output channel, and m is the channel index. Then, the output of modulated convolution goes into the cross-attention transformer where the attention map att t j is calculated using local latent variables w t l at time index t as follows where Q j (.), K j (.), V j (.) are queries, keys and values, respectively where each function represents a dense layer with input inside the parenthesis, and P.E. is the positional encoding. Then, x t output,j is normalized to zero-mean unit variance and scaled with a learned projection of the attention maps att t j as follows x t output,j = α j (att j ) where α j (.) is the learned scale parameter. After repeating the sequence of crossattention layer twice, lastly the data-consistency is performed. To perform dataconsistency the number of channels in x t output,j is decreased to 2 with an additional convolution layer. Then, 2-channel images are converted, where channels represent real and imaginary components, to complex and data-consistency is applied as follows where F -1 represents the inverse 2D Fourier transform and x t us = x us during training. Then, using another extra convolution, the number of feature maps are increased to n again for the next denoising block. Implementation Details : Adam optimizer is used for self-supervised training with β = (0.9, 0.999) and learning rate 0.002. Default noise schedule paramaters are taken from ",10
6648,Self-supervised MRI Reconstruction with Unrolled Diffusion Models (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 47.,10
6655,FSDiffReg: Feature-Wise and Score-Wise Diffusion-Guided Unsupervised Deformable Image Registration for Cardiac Images (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 62.,10
6658,Solving Low-Dose CT Reconstruction via GAN with Local Coherence (vol10),,Generative Adversarial Network.,"Traditional generative adversarial network  Local Coherence. As mentioned in Sect. 1, optical flow can capture the temporal coherence of object movements, which plays a crucial role in many videorelated tasks. More specifically, the optical flow refers to the instantaneous velocity of pixels of moving objects on consecutive frames over a short period of time  Based on these assumptions, the brightness of optical flow can be described by the following equation: where v = (v w , v h ) represents the optical flow of the position (w, h) in the image. ∇I = (∇I w , ∇I h ) denotes spatial gradients of image brightness, and ∇I t denotes the temporal partial derivative of the corresponding region. Following the Eq. ( ",10
6662,Solving Low-Dose CT Reconstruction via GAN with Local Coherence (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 50.,10
6673,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 46.,10
6677,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction (vol10),,Algorithm 1. Learned Descent Algorithm for PET image reconstruction,"Input: Image initialization x0, ρ, γ ∈ (0, 1), ε0, σ, τ > 0, maximum number of iteration I, total phase numbers K and measured Sinogram y 1:",10
6712,X-Ray to CT Rigid Registration Using Scene Coordinate Regression (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 74.,10
6739,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model (vol10),,Deterministic DDIM Sampling.,"In order to obtain a latent representation z T capable of reconstructing a given noisy sample into a high-resolution image, we employ deterministic DDIM sampling  where α 1:T ∈ (0, 1] T is a time-dependent decreasing sequence, zt- represents the ""predicted x 0 "", and √ 1α t-1 • θ (z t , C, t) can be understood as the ""direction pointing to x t ""  Corruption Function f . We assume a corruption function f known a-priori that is applied on the HR image x obtained from the generative model, and compute the loss function based on the corrupted image f • x and the given LR input image I. In clinical practice, a prevalent method for acquiring MR images is prioritizing high in-plane resolution while sacrificing through-plane resolution to expedite the acquisition process and reduce motion artifacts ",10
6740,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model (vol10),,InverseSR(LDM):,"In the case of high sparsity MRI SR, we optimize the noise latent code z * T and its associated conditional variables C * to restore the HR image from the given LR input image I using the optimization method: where DDIM(z T , C, T ) represents T deterministic DDIM sampling steps on the latent z 0 in Eq. 2. We follow the brain LDM model to use the perceptual loss L perc and the L1 pixelwise loss. The loss function is computed on the corrupted image generated from the generative model and the given LR input. A detailed pseudocode description of this method can be found in Algorithm 1.",10
6741,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model (vol10),,InverseSR(Decoder):,"For low sparsity MRI SR, we directly find the optimal latent code z * T using the decoder D:",10
6743,InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model (vol10),,Implementation:,"Conditional variables are all initialized to 0.5. Voxels in all input volumes are normalized to [0,1]. When sampling the pre-trained brain LDM with the DDIM sampler, we run T = 46 timesteps due to computational limitations on our hardware. For InverseSR(LDM), z T is initialized with random gaussian noise. For InverseSR(Decoder), we compute the mean latent code z0 as z0 = S i=1 1 S DDIM(z i T , C, T ) by first sampling S = 10, 000 z i T samples from N (0, I), then passing them through the DDIM model. N = 600 gradient descent steps are used for InverseSR(LDM) to guarantee converging (Algorithm 1, line 5). 600 optimization steps are also utilized in InverseSR(Decoder). We use the Adam optimizer with α = 0.07, β 1 = 0.9 and β 2 = 0.999.",10
6748,LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion (vol10),,Angiectasia Erosion,"Foreign body Angiectasia Erosion Foreign body Many traditional algorithms (e.g., intensity transformation  Recently, denoising diffusion probabilistic model (DDPM)  -We design a Low-Light image enhancement framework for Capsule endoscopy (LLCaps ",10
6751,LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion (vol10),,Overall Network,"Architecture. An overview of our framework can be found in Fig.  The shallow output F OP M ∈ R 3×W ×H shall further be propagated through the reverse diffusion process and achieve the final enhanced image Y ∈ R 3×W ×H . The whole network is constructed in an end-to-end mode and optimized by Charbonnier loss  in which Y and Y * denote the input and ground truth images, respectively.",10
6755,LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 4.,10
6765,Global k-Space Interpolation for Dynamic MRI Reconstruction Using Masked Image Modeling (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_22.,10
6774,Nonuniformly Spaced Control Points Based on Variational Cardiac Image Registration (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 60.,10
6782,Alias-Free Co-modulated Network for Cross-Modality Synthesis and Super-Resolution of MR Images (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5_7.,10
6790,Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 25.,10
6798,LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline (vol10),,Supplementary Information,The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43999-5 48.,10
