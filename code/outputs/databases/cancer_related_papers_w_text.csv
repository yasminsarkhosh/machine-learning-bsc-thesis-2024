title,header_no,header_title,text,volume
Anatomy-Driven Pathology Detection on Chest X-rays,1.0,Introduction,"chest radiographs (chest x-rays) represent the most widely utilized type of
medical imaging examination globally and hold immense significance in the
detection of prevalent thoracic diseases, including pneumonia and lung cancer,
making them a crucial tool in clinical care [10,15]. pathology detection and
localization -for brevity we will use the term pathology detection throughout
this workenables the automatic interpretation of medical scans such as chest
x-rays by predicting bounding boxes for detected pathologies. unlike
classification, which only predicts the presence of pathologies, it provides a
high level of explainability supporting radiologists in making informed
decisions. however, while image classification labels can be automatically
extracted from electronic health records or radiology reports [7,20], this is
typically not possible for bounding boxes, thus limiting the availability of
large datasets for pathology detection. additionally, manually annotating
pathology bounding boxes is a time-consuming task, further exacerbating the
issue. the resulting scarcity of large, publicly available datasets with
pathology bounding boxes limits the use of supervised methods for pathology
detection, such that current approaches typically follow weakly supervised
object detection approaches, where only classification labels are required for
training. however, as these methods are not guided by any form of bounding
boxes, their performance is limited.we, therefore, propose a novel approach
towards pathology detection that uses anatomical region bounding boxes, solely
defined on anatomical structures, as proxies for pathology bounding boxes. these
region boxes are easier to annotate -the physiological shape of a healthy
subject's thorax can be learned relatively easily by medical students -and
generalize better than those of pathologies, such that huge labeled datasets are
available [21]. in summary:-we propose anatomy-driven pathology detection
(adpd), a pathology detection approach for chest x-rays, trained with pathology
classification labels together with anatomical region bounding boxes as proxies
for pathologies. -we study two training approaches: using localized
(anatomy-level) pathology labels for our model loc-adpd and using image-level
labels with multiple instance learning (mil) for our model mil-adpd. -we train
our models on the chest imagenome [21] dataset and evaluate on nih chestx-ray 8
[20], where we found that our loc-adpd model outperforms both, weakly supervised
methods and fully supervised detection with a small training set, while our
mil-adpd model is competitive with supervised detection and slightly outperforms
weakly supervised approaches.",1
Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET,2.4,Dataset,"the dataset is composed of 23 oncological patients with different tumor types.
dpet data was acquired on a biograph vision quadra for 65 min, over 62 frames.
the exposure duration of the frames were 2 × 10 s, 30 × 2 s, 4 × 10 s, 8 × 30 s,
4 × 60 s, 5 × 120 s and 9 × 300 s. the pet volumes were reconstructed with an
isotropic voxel size of 1.6 mm. the dataset included the label maps of 7 organs
(bones, lungs, heart, liver, kidneys, spleen, aorta) and one image-derived input
function a(t) [bq/ml] from the descending aorta per patient. further details on
the dataset are presented elsewhere [16].the pet frames and the label map were
resampled to an isotropic voxel size of 2.5 mm. then, the dataset was split
patient-wise into training, validation, and test set, with 10, 4, and 9 patients
respectively. details on the dataset split are available in the supplementary
material (table 1). the training set consisted of 750 slices and the validation
consisted of 300. in both cases, 75 axial slices per patient were extracted in a
pre-defined patient-specific range from the lungs to the bladder (included) and
were cropped to size 112 × 112 pixels.",1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,1.0,Introduction,"deep learning techniques have greatly improved medical image segmentation by
automatically extracting specific tissue or substance location information,
which facilitates accurate disease diagnosis and assessment. however, most deep
learning approaches for segmentation require fully or partially labeled training
datasets, which can be time-consuming and expensive to annotate. to address this
issue, recent research has focused on developing segmentation frameworks that
require little or no segmentation labels.to meet this need, many researchers
have devoted their efforts to weakly-supervised semantic segmentation (wsss)
[21], which utilizes weak supervision, such as image-level classification
labels. recent wsss methods can be broadly categorized into two types [4]:
class-activation-mapping-based (cam-based) [9,13,16,19,20,22], and
multiple-instance-learning-based (mil-based) [15] methods.the literature has not
adequately addressed the issue of low-resolution class-activation maps (cams),
especially for medical images. some existing methods, such as dilated residual
networks [24] and u-net segmentation architecture [3,7,17], have attempted to
tackle this issue, but still require many upsampling operations, which the
results become blurry. meanwhile, layercam [9] has proposed a hierarchical
solution that extracts activation maps from multiple convolution layers using
grad-cam [16] and aggregates them with equal weights. although this approach
successfully enhances the resolution of the segmentation mask, it lacks
flexibility and may not be optimal.in this paper, we propose an attentive
multiple-exit cam (ame-cam) for brain tumor segmentation in magnetic resonance
imaging (mri). different from recent cam methods, ame-cam uses a classification
model with multipleexit training strategy applied to optimize the internal
outputs. activation maps from the outputs of internal classifiers, which have
different resolutions, are then aggregated using an attention model. the model
learns the pixel-wise weighted sum of the activation maps by a novel contrastive
learning method.our proposed method has the following contributions:-to tackle
the issues in existing cams, we propose to use multiple-exit classification
networks to accurately capture all the internal activation maps of different
resolutions. -we propose an attentive feature aggregation to learn the
pixel-wise weighted sum of the internal activation maps. -we demonstrate the
superiority of ame-cam over state-of-the-art cam methods in extracting
segmentation results from classification networks on the 2021 brain tumor
segmentation challenge (brats 2021) [1,2,14]. -for reproducibility, we have
released our code at https://github.com/windstormer/ame-cam overall, our
proposed method can help overcome the challenges of expensive and time-consuming
segmentation labeling in medical imaging, and has the potential to improve the
accuracy of disease diagnosis and assessment.",1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,3.1,Dataset,"we evaluate our method on the brain tumor segmentation challenge (brats) dataset
[1,2,14], which contains 2,000 cases, each of which includes four 3d volumes
from four different mri modalities: t1, post-contrast enhanced t1 (t1-ce), t2,
and t2 fluid attenuated inversion recovery (t2-flair), as well as a
corresponding segmentation ground-truth mask. the official data split divides
these cases by the ratio of 8:1:1 for training, validation, and testing (5,802
positive and 1,073 negative images). in order to evaluate the performance, we
use the validation set as our test set and report statistics on it. we
preprocess the data by slicing each volume along the z-axis to form a total of
193,905 2d images, following the approach of kang et al. [10] and dey and hong
[6]. we use the ground-truth segmentation masks only in the final evaluation,
not in the training process.",1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,4.1,Quantitative and Qualitative Comparison with State-of-the-Art,"in this section, we compare the segmentation performance of the proposed ame-cam
with five state-of-the-art weakly-supervised segmentation methods, namely
grad-cam [16], scorecam [19], lfi-cam [13], layercam [9], and swin-mil [15]. we
also compare with an unsupervised approach c&f [5], the supervised version of
c&f, and the supervised optimized u-net [8] to show the comparison with
non-cam-based methods. we acknowledge that the results from fully supervised and
unsupervised methods are not directly comparable to the weakly supervised cam
methods. nonetheless, these methods serve as interesting references for the
potential performance ceiling and floor of all the cam methods. quantitatively,
grad-cam and scorecam result in low dice scores, demonstrating that they have
difficulty extracting the activation of medical images. lfi-cam and layercam
improve the dice score in all modalities, except lfi-cam in t1-ce and t2-flair.
finally, the proposed ame-cam achieves optimal performance in all modalities of
the brats dataset.compared to the unsupervised baseline (ul), c&f is unable to
separate the tumor and the surrounding tissue due to low contrast, resulting in
low dice scores in all experiments. with pixel-wise labels, the dice of
supervised c&f improves significantly. without any pixel-wise label, the
proposed ame-cam outperforms supervised c&f in all modalities.the fully
supervised (fsl) optimized u-net achieves the highest dice score and iou score
in all experiments. however, even under different levels of supervision, there
is still a performance gap between the weakly supervised cam methods and the
fully supervised state-of-the-art. this indicates that there is still potential
room for wsss methods to improve in the future.qualitatively, fig. 2 shows the
visualization of the cam and segmentation results from all six cam-based
approaches under four different modalities from the brats dataset. grad-cam
(fig. 2(c)) results in large false activation region, where the segmentation
mask is totally meaningless. scorecam eliminates false activation corresponding
to air. lfi-cam focus on the exact tumor area only in the t1 and t2 mri (row 1
and 3). swin-mil can hardly capture the tumor region of the mri image, where the
activation is noisy. among all, only layercam and the proposed ame-cam
successfully focus on the exact tumor area, but ame-cam reduces the
under-estimation of the tumor area. this is attributed to the benefit provided
by aggregating activation maps from different resolutions.",1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,0.0,Effect of Different Aggregation Approaches:,"in table 2, we conducted an ablation study to investigate the impact of using
different aggregation approaches after extracting activations from the
multiple-exit network. we aim to demonstrate the superiority of the proposed
attention-based aggregation approach for segmenting tumor regions in t1 mri of
the brats dataset. note that we only report the results for t1 mri in the brats
dataset. please refer to the supplementary material for the full set of
experiments.as a baseline, we first conducted the average of four activation
maps generated by the multiple-level activation extraction (avg. me). we then
applied c 2 am [22], a state-of-the-art cam-based refinement approach, to refine
the result of the baseline, which we call ""avg. me+c 2 am"". however, we observed
that c 2 am tended to segment the brain region instead of the tumor region due
to the larger contrast between the brain tissue and the air than that between
the tumor region and its surrounding tissue. any incorrect activation of c 2 am
also led to inferior results, resulting in a degradation of the average dice
score from 0.617 to 0.484. in contrast, the proposed attention-based approach
provided a significant weighting solution that led to optimal performance in all
cases.table 3. ablation study for using single-exit from m1, m2, m3 or m4 of
fig. 1 and the multiple-exit using results from m2 and m3 and using all exits
(ame-cam). the experiments are done on the t1-ce mri of brats dataset. the dice
score, iou, and the hd95 are reported in the form of mean ± std.",1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,0.0,Selected Exit,"dice effect of single-exit and multiple-exit: table 3 summarizes the performance
of using single-exit from m 1 , m 2 , m 3 , or m 4 of fig. 1 and the
multipleexit using results from m 2 and m 3 , and using all exits (ame-cam) on
t1-ce mri in the brats dataset.the comparisons show that the activation map
obtained from the shallow layer m 1 and the deepest layer m 4 result in low dice
scores, around 0.15. this is because the network is not deep enough to learn the
tumor region in the shallow layer, and the resolution of the activation map
obtained from the deepest layer is too low to contain sufficient information to
make a clear boundary for the tumor. results of the internal classifiers from
the middle of the network (m 2 and m 3 ) achieve the highest dice score and iou,
both of which are around 0.5.to evaluate whether using results from all internal
classifiers leads to the highest performance, we further apply the proposed
method to the two internal classifiers with the highest dice scores, i.e., m 2
and m 3 , called m 2 + m 3 . compared with using all internal classifiers (m 1
to m 4 ), m 2 + m 3 results in 18.6% and 22.1% lower dice and iou, respectively.
in conclusion, our ame-cam still achieves the optimal performance among all the
experiments of single-exit and multiple-exit.other ablation studies are
presented in the supplementary material due to space limitations.",1
AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,5.0,Conclusion,"in this work, we propose a brain tumor segmentation method for mri images using
only class labels, based on an attentive multiple-exit class activation mapping
(ame-cam). our approach extracts activation maps from different exits of the
network to capture information from multiple resolutions. we then use an
attention model to hierarchically aggregate these activation maps, learning
pixel-wise weighted sums.experimental results on the four modalities of the 2021
brats dataset demonstrate the superiority of our approach compared with other
cam-based weakly-supervised segmentation methods. specifically, ame-cam achieves
the highest dice score for all patients in all datasets and modalities. these
results indicate the effectiveness of our proposed approach in accurately
segmenting brain tumors from mri images using only class labels.",1
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,3.1,Datasets,"three histo . it corresponds to absent fibrosis (f0), mild fibrosis (f1),
significant fibrosis (f2), severe fibrosis (f3) and cirrhosis (f4). this score
is then binarized to indicate the absence or presence of advanced fibrosis [14]:
f0/f1/f2 (n = 28) vs. f3/f4 (n = 78). d 2 histo . this is the public lihc
dataset from the cancer genome atlas [9], which presents a histological score,
the ishak score, designated as y 2 histo , that differs from the metavir score
present in d 1 histo . this score is also distributed through five labels: no
fibrosis, portal fibrosis, fibrous speta, nodular formation and incomplete
cirrhosis and established cirrhosis. similarly to the metavir score in d 1 histo
, we also binarize the ishak score, as proposed in [16,20], which results in two
cohorts of 34 healthy and 15 pathological patients.in all datasets, we select
the slices based on the liver segmentation of the patients. to gain in
precision, we keep the top 70% most central slices with respect to liver
segmentation maps obtained manually in d radio , and automatically for d 1 histo
and d 2 histo using a u-net architecture pretrained on d radio [18]. for the
latter pretraining dataset, it presents an average slice spacing of 3.23 mm with
a standard deviation of 1.29 mm. for the x and y axis, the dimension is 0.79 mm
per voxel on average, with a standard deviation of 0.10 mm.",1
Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification,5.0,Conclusion,"in this work, we proposed a novel kernel-based contrastive learning method that
leverages both continuous and discrete meta-data for pretraining. we tested it
on a challenging clinical application, cirrhosis prediction, using three
different datasets, including the lihc public dataset. to the best of our
knowledge, this is the first time that a pretraining strategy combining
different kinds of meta-data has been proposed for such application. our results
were compared to other stateof-the-art cl methods well-adapted for cirrhosis
prediction. the pretraining methods were also compared visually, using a 2d
projection of the representation vectors onto the first two pca modes. results
showed that our method has an organization in the representation space that is
in line with the proposed theory, which may explain its higher performances in
the experiments. as future work, it would be interesting to adapt our kernel
method to non-contrastive methods, such as simsiam [7], byol [10] or barlow
twins [25], that need smaller batch sizes and have shown greater performances in
computer vision tasks. in terms of application, our method could be easily
translated to other medical problems, such as pancreas cancer prediction using
the presence of intrapancreatic fat, diabetes mellitus or obesity as discrete
meta-labels.",1
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,1.0,Introduction,"deep neural networks (dnns) have been successfully applied to various supervised
3d biomedical image analysis tasks, such as classification [11], segmentation
[7], and registration [35]. acquiring volumetric annotations manually to
supervise deep learning models is costly and labor intensive. for example, the
supervised training of 3d dnns for segmentation requires the manual labeling of
every voxel of the structures of interest for the entire training set.
additionally, the diversity of existing biomedical 3d volumetric image types
(e.g. mri, ct, electron tomography) and different tasks associated with them
precludes image annotations for all existing problems in practice. furthermore,
experts may focus on annotating objects they are already aware of, thereby
restricting the possibility of new structural discoveries in large datasets
using deep learning. we hypothesize that the nested hierarchical structure
intrinsic to many 3d biomedical images [13] might be useful for unsupervised
segmentation. as a step in this direction, our goal in this work is to develop a
computational approach for unsupervised structure discovery.recently,
unsupervised part discovery in 2d natural images has gained significant
attention [6,8,15]. these methods are based on the finding that intermediate
activations of deep imagenet-pre-trained classification models capture
semantically meaningful conceptual regions [8]. these regions are robust to pose
and viewpoint variations and help high-level image understanding by providing
local object representations, leading to more explainable recognition [15].
however, a naive application of part discovery methods to 3d volumetric
segmentation is not feasible, due to the lack of good feature extractors for 3d
biomedical images [5] and imagenet-pretrained networks operate only on 2d
images.we hypothesize that deep generative models are good feature extractors
for unsupervised structure discovery for the following reasons. first, these
models do not require expert labels as they are trained in a self-supervised
way. second, the ability to generate high-quality images suggests that these
models capture semantically meaningful information. third, generative
representation learning has been successfully applied to global and dense
prediction tasks in 2d images [9] and has shown improvements in label efficiency
and generalization [19].besides creating stunning image generation results,
diffusion-based generative models [12] are applied to other downstream tasks.
several works use pretrained diffusion models for 2d label-efficient semantic
segmentation of natural images [1,4]. in 2d medical imaging, diffusion models
are used for self-supervised vessel segmentation [18], anomaly detection
[27,29,31,34], denoising [14], and improving supervised segmentation models
[32,33]. in 3d medical imaging, diffusion models are used for ct and mr image
synthesis [10,33]. inspired by the success of unsupervised part discovery
methods in 2d images and the effective abilities of diffusion models for many
downstream tasks we hypothesize that feature representations of generative
diffusion models discover intrinsic hierarchical structures in 3d biomedical
images. our work explores this hypothesis. our contributions are:1) we pretrain
3d diffusion models, use them as feature extractors (fig. 1), and design losses
(fig. 2) for unsupervised 3d structure discovery. 2) we show that features from
different stages of ladder-like u-net-based diffusion models capture different
hierarchy levels in 3d biomedical volumes. 3) our approach outperforms previous
3d unsupervised discovery methods on challenging synthetic datasets and on a
real-world brain tumor segmentation (brats'19) dataset.",1
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,4.1,Datasets,"to compare with state-of-the-art unsupervised 3d segmentation methods we follow
[13] and evaluate our method on challenging biologically inspired 3d synthetic
datasets and a real-world brain tumor segmentation (brats'19) dataset.the
synthetic dataset of [13], consists of 120 volumes (80-20-20 split) of size 50 ×
50 × 50. inspired by cryo-electron tomography images, it contains a three-level
structure, representing a biological cell, vesicles and mitochondria, as well as
protein aggregates. the intensities and locations of the objects are randomized
without destroying the hierarchy. the regular variant of the dataset contains
cubical and spherical objects, while the irregular variant contains more complex
shapes. pink noise of magnitude m = 0.25 which is commonly seen in biological
data [30] is applied to the volume. figure 3 shows sample slices of both
variants.the brats'19 dataset [2,3,21] is an established benchmark for 3d tumor
segmentation of brain mris. volumes are co-registered to the same template,
interpolated to (1 mm) 3 resolution and brain-extracted. following [13], images
are cropped to volumes of size 200 × 200 × 155. as in [13], flair images and
corresponding whole tumor (wt) annotations are used for unsupervised
segmentation evaluation with the same split of 259 high grade glioma training
examples into 180 train, 39 validation, and 40 test samples. the official
brats'19 validation and test sets are not used as their segmentation masks are
not available.",1
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,4.3,Results,"we compare our method with state-of-the-art unsupervised 3d structure discovery
approaches including clustering using 3d feature learning [23], a 3d
convolutional autoencoder [24], and self-supervised hyperbolic representations
[13].for the synthetic datasets, we used k = 2 (background and cell) for level
1, k = 4 (background, cell, vesicle, mitochondria) for level 2, and k = 8
(background, cell, vesicle, mitochondria, and 4 small protein aggregates) for
level 3 predictions. the evaluation metric is the average dice score on the
annotated test labels. as the label order may differ we use the hungarian
algorithm to match the predicted masks with the ground truth segmentations.
table 1 shows the results for the regular and irregular variants of the
cryo-et-inspired synthetic dataset. our models outperform all previous
unsupervised work at all hierarchy levels. for some levels, our models even
outperform semi-supervised methods (c ¸içek et al. [7] used 2% of annotated
data, zhao et al. [36] used one annotated volume). we found that simple
unsupervised denoising (bm4d [20]) followed by k-means clustering provides a
good baseline, although vanilla kmeans clustering on voxel intensities does not
perform well due to noise. results in fig. 3 demonstrate that our proposed
unsupervised method indeed discovers the hierarchical structure of different
levels. we also show in table 2 that features from early decoder stages of the
u-net-based diffusion models better discover larger objects in the hierarchy,
features at intermediate stages better capture intermediate objects, and
features at later stages better find smaller objects.for the brain tumor
segmentation (brats'19) dataset, we use the whole tumor (wt) segmentation mask
for evaluation, which is detectable based on the flair images alone. we train
segmentation models with k = 3 parts (background, brain, tumor). the evaluation
metric, as in the brats'19 challenge [21], is dice score and the 95th percentile
of the symmetric hausdorff distance, which quantifies the surface distance of
the predicted segmentation from the manual tumor segmentation in millimeters.
table 3 shows that our model outperforms all prior unsupervised methods for both
evaluation metrics. as an approximate upper bound we show for reference the
reported results of the 1st place solution [17] on brats'19 which is based on
supervised training on the full train set and evaluated on the brats'19 test
set. the qualitative results in fig. 4 show that our model can detect tumors of
different sizes. our predictions look smoother and do not capture fine details
of tumor segmentations.we perform ablation studies on the brats'19 dataset
(table 3: below the line). measuring the impact of each loss, we see that the
smallest performance drop is due to a deactivated invariance loss (λ inv = 0)
while deactivating the visual consistency (λ v = 0) and feature consistency (λ f
= 0) losses results in larger, but similar performance drops. however, to
achieve best performance all three components are necessary. we also perform
k-means clustering on intensities and features. we observe that using our deep
network model dramatically improves performance, although our losses are similar
to k-means clustering.this might be due to the fact that predictive modeling
involves learning from a distribution of images and a model may therefore
extract useful knowledge from a collection of images. to evaluate the
significance of the diffusion features, we replaced our diffusion feature
extractor with a 3d resnet from med3d [5] trained on 23 medical datasets. we use
the ""layer1 2 conv2"" features as they showed the best performance. although
performance does not drop significantly when med3d features are used with our
losses, med3d features do not produce good results when directly used for
k-means clustering.",1
Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features,5.0,Conclusion,"in this work, we showed that features from 3d generative diffusion models using
a ladder-like u-net-based architecture can discover intrinsic 3d structures in
biomedical images. we trained predictive unsupervised segmentation models using
losses that encourage the decomposition of biomedical volumes into nested
subvolumes aligned with their hierarchical structures. our method outperforms
existing unsupervised segmentation approaches and discovers meaningful
hierarchical concepts on challenging biologically-inspired synthetic datasets
and on the brats brain tumor dataset. while we tested our approach for
unsupervised image segmentation it is conceivable that it could also be useful
in semisupervised settings and that could be applied to data types other than
images.",1
S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation,1.0,Introduction,"colorectal cancer is a leading cause of cancer-related deaths worldwide [1].
early detection and efficient diagnosis of polyps, which are precursors to
colorectal cancer, is crucial for effective treatment. recently, deep learning
has emerged as a powerful tool in medical image analysis, prompting extensive
research into its potential for polyp segmentation. the effectiveness of deep
learning models in medical applications is usually based on large,
well-annotated datasets, which in turn necessitates a time-consuming and
expertise-driven annotation process. this has prompted the emergence of
approaches for annotation-efficient weakly-supervised learning in the medical
domain with limited annotations like points [8], bounding boxes [12], and
scribbles [15]. compared with other sparse labeling methods, scribbles allow the
annotator to annotate arbitrary shapes, making them more flexible than points or
boxes [13]. besides, scribbles provide a more robust supervision signal, which
can be prone to noise and outliers [5]. hence, this work investigates the
feasibility of conducting polyp segmentation using scribble annotation as
supervision. the effectiveness of medical applications during in-site deployment
depends on their ability to generalize to unseen data and remain robust against
data corruption. improving these factors is crucial to enhance the accuracy and
reliability of medical diagnoses in real-world scenarios [22,27,28]. therefore,
we comprehensively evaluate our approach on multiple datasets from various
medical sites to showcase its viability and effectiveness across different
contexts.dual-branch learning has been widely adopted in annotation-efficient
learning to encourage mutual consistency through co-teaching. while existing
approaches are typically designed for learning in the spatial domain
[21,25,29,30], a novel spatial-spectral dual-branch structure is introduced to
efficiently leverage domain-specific complementary knowledge with synergistic
mutual teaching. furthermore, the outputs from the spatial-spectral branches are
aggregated to produce mixed pseudo labels as supplementary supervision.
different from previous methods, which generally adopt the handcrafted fusion
strategies [15], we design to aggregate the outputs from spatial-spectral dual
branches with an entropy-guided adaptive mixing ratio for each pixel.
consequently, our incorporated tactic of pseudo-label fusion aptly assesses the
pixel-level ambiguity emerging from both spatial and frequency domains based on
their entropy maps, thereby allocating substantially assured categorical labels
to individual pixels and facilitating effective pseudo label ensemble learning.
contributions. overall, the contributions of this work are threefold: first, we
devise a spatial-spectral dual-branch structure to leverage cross-space
knowledge and foster collaborative mutual teaching. to our best knowledge, this
is the first attempt to explore the complementary relations of the
spatial-spectral dual branch in boosting weakly-supervised medical image
analysis. second, we introduce the pixel-level entropy-guided fusion strategy to
generate mixed pseudo labels with reduced noise and increased confidence, thus
enhancing ensemble learning. lastly, our proposed hybrid loss optimization,
comprising scribblessupervised loss, mutual training loss with domain-specific
pseudo labels, and ensemble learning loss with fused-domain pseudo labels,
facilitates obtaining a generalizable and robust model for polyp image
segmentation. an extensive assessment of our approach through the examination of
four publicly accessible datasets establishes its superiority and clinical
significance.",1
Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy,1.0,Introduction,"endoscopy is an important medical procedure with many applications, from routine
screening to detection of early signs of cancer and minimally invasive
treatment. automatic analysis and understanding of these videos raises many
opportunities for novel assistive and automatization tasks on endoscopy
procedures. obtaining 3d models from the intracorporeal scenes captured in
endoscopies is an essential step to enable these novel tasks and build
applications, for example, for improved monitoring of existing patients or
augmented reality during training or real explorations.3d reconstruction
strategies have been studied for long, and one crucial step in these strategies
is feature detection and matching which serves as input for structure from
motion (sfm) pipelines. endoscopic images are a challenging case for feature
detection and matching, due to several well known challenges for these tasks,
such as lack of texture, or the presence of frequent artifacts, like specular
reflections. these problems are accentuated when all the elements in the scene
are deformable, as it is the case in most endoscopy scenarios, and in particular
in the real use case studied in our work, the lower gastrointestinal tract
explored with colonoscopies. existing 3d reconstruction pipelines are able to
build small 3d models out of short clips from real and complete recordings [1].
one of the current bottle-necks to obtain better 3d models is the lack of more
abundant and higher quality correspondences in real data.this work introduces
superpoint-e, a new model to extract interest points from endoscopic images. we
build on the well known superpoint architecture [5], a seminal work that
delivers state-of-the-art results when coupled with downstream tasks1 . our main
contribution is a novel supervision strategy to train the model. we propose to
automatically generate reliable training data from video sequences by tracking
feature points from existing detection methods, which do not require training.
we select good features with the colmap sfm pipeline [21], generating training
examples with feature points that can be tracked across several images according
to colmap result. when used to train superpoint, our approach yields a
self-supervised method outperforming current ones.",1
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,1.0,Introduction,"semantic segmentation plays a vital role in pathological image analysis. it can
help people conduct cell counting, cell morphology analysis, and tissue
analysis, which reduces human labor [19]. however, data acquisition for medical
images poses unique challenges due to privacy concerns and the high cost of
manual annotation. moreover, pathological images from different tissues or
cancer types often show significant domain shifts, which hamper the
generalization of models trained on one dataset to others. due to the
abovementioned challenges, some researchers have proposed various white-box
domain adaptation methods to address these issues.recently, [8,16] propose to
use generative adversarial networks to align the distributions of source and
target domains and generate source-domain lookalike outputs for target images.
source-free domain adaptation methods have been also widely investigated due to
the privacy protection. [3,5,14] explore how to implicitly align target domain
data with the model trained on the source domain without accessing the source
domain data. there are also many studies on multi-source white-box domain
adaptation. ahmed et al. [1] propose a novel algorithm which automatically
identifies the optimal blend of source models to generate the target model by
optimizing a specifically designed unsupervised loss. li et al. [13] extend the
above work to semantic segmentation and proposed a method named model-invariant
feature learning, which takes full advantage of the diverse characteristics of
the source-domain models.nonetheless, several recent investigations have
demonstrated that the domain adaptation methods for source-free white-box models
still present a privacy risk due to the potential leakage of model parameters
[4]. such privacy breaches may detrimental to the privacy protection policies of
hospitals. moreover, the target domain uses the same neural network as the
source domain, which is not desirable for low-resource target users like
hospitals [15]. we thus present a more challenging task of relying solely on
black-box models from vendors to avoid parameter leakage. in clinical
applications, various vendors can offer output interfaces for different
pathological images. while black-box models are proficient in specific domains,
their performances greatly degrade when the target domain is updated with new
pathology slices. therefore, how to leverage the existing knowledge of black-box
models to effectively train new models for the target domain without accessing
the source domain data remains a critical challenge.in this paper, we present a
novel source-free domain adaptation framework for cross-tissue cell segmentation
without accessing both source domain data and model parameters, which can
seamlessly integrate heterogeneous models from different source domains into any
cell segmentation network with high generality. to the best of our knowledge,
this is the first study on the exploration of multi-source black-box domain
adaptation for cross-tissue cell segmentation. in this setting, conventional
multi-source ensemble methods are not applicable due to the unavailability of
model parameters, and simply aggregating the black-box outputs would introduce a
considerable amount of noise, which can be detrimental to the training of the
target domain model. therefore, we develop two strategies within this new
framework to address this issue. firstly, we propose a pixel-level multi-source
domain weighting method, which reduces source domain noise by knowledge
weighting. this method effectively addresses two significant challenges
encountered in the analysis of cellular images, namely, the uncertainty in
source domain output and the ambiguity in cell boundary semantics. secondly, we
also take into account the structured information from cells to images, which
may be overlooked during distillation, and design an adaptive knowledge voting
strategy. this strategy enables us to ignore low-confidence regions, similar to
cutout [6], but with selective masking of pixels, which effectively balances the
trade-off between exploiting similarities and preserving differences of
different domains. as a result, we refer to the labels generated through the
voting strategy as pseudo-cutout labels.",1
Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation,3.0,Experiments,"dataset and setting: we collect four pathology image datasets to validate our
proposed approach. firstly, we acquire 50 images from a cohort of patients with
triple negative breast cancer (tnbc), which is released by naylor et al [18].
hou et al. [10] publish a dataset of nucleus segmentation containing 5,060
segmented slides from 10 tcga cancer types. in this work, we use 98 images from
invasive carcinoma of the breast (brca). we have also included 463 images of
kidney renal clear cell carcinoma (kirc) in our dataset, which are made publicly
available by irshad et al [11]. awan et al. [2] publicly release a dataset
containing tissue slide images and associated clinical data on colorectal cancer
(crc), from which we randomly select 200 patches for our study. in our
experiments, we transfer knowledge from three black-box models trained on
different source domains to a new target domain model (e.g.,from crc, tnbc, kirc
to brca). the backbone network for the student model and source domain black-box
predictors employ the widely adopted residual u-net [12], which is commonly used
for medical image segmentation. for each source domain network, we conduct
full-supervision training on the corresponding source domain data and directly
evaluate its performance on target domain data. the upper performance metrics
(source-only upper) are shown in the table 1. to ensure the reliability of the
results, we use the same data for training, validation, and testing, which
account for 80%, 10%, and 10% of the original data respectively. for the target
domain network, we use unsupervised and semi-supervised as our task settings
respectively. in semi-supervised domain adaptation, we only use 10% of the
target domain data as labeled data.experimental results: to validate our method,
we compare it with the following approaches: (1) cellsegssda [8], an adversarial
learning based semisupervised domain adaptation approach. (2) us-msma [13], a
multi-source model domain aggregation network. (3) sfda-dpl [5], a source-free
unsupervised domain adaptation approach. (4) bbuda [17], an unsupervised
black-box model domain adaptation framework. a point worth noting is that most
of the methods we compared with are white-box methods, which means they can
obtain more information from the source domain than us. for single-source domain
adaptation approach, cellsegssda and sfda-dpl, we employ two strategies to
ensure the fairness of the experiments: (1) single-source, i.e. performing
adaptation on each single source, where we select the best results to display in
the table 1; (2) source-combined, i.e. all source domains are combined into a
traditional single source. the table 1 and fig. 2 demonstrate that our proposed
method exhibits superior performance, even when compared to these white-box
methods, surpassing them in various evaluation metrics and visualization
results.in addition, the experimental results also show that simply combining
multiple source data into a traditional single source will result in performance
degradation in some cases, which also proves the importance of studying
multi-source domain adaptation methods.ablation study: to evaluate the impact of
our proposed methods of weighted logits(wl), pseudo-cutout label(pcl) and
maximize mutual information(mmi) on the model performance, we conduct an
ablation study. we compare the baseline model with the models that added these
three methods separately. we chose crc, kirc and brca as our source domains, and
tnbc as our target domain. the results of these experiments, presented in the
table 2, show that our proposed modules are indeed useful.",1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,1.0,Introduction,"in recent years, deep learning (dl) methods have demonstrated remarkable
performance in detecting and localizing tumors on ultrasound images [2,27].
compared with conventional image processing methods, dl methods provide an
accurate feature extraction capability on ultrasound images, despite their low
resolution and noise disturbance, leading to superior segmentation accuracy
[2,5,14]. however, there are some limitations in developing a dl model in a
source domain and deploying it in an unseen target domain. the primary
limitation is that dl models require a large number of training samples to
achieve accurate predictions [8,24]. yet, acquiring large training datasets and
their corresponding labels, especially from a cohort of patients, can be costly
or even infeasible, which poses a significant challenge in developing a dl model
with high performance [7]. second, even when large-scale datasets are available
through collaborative research from multiple sites, dl models trained on such
datasets may yield sub-optimal solutions due to domain gaps caused by
differences in images acquired from different sites [20]. third, due to the
small number of datasets from each domain, the images for each individual domain
may not capture representative features, limiting the ability of dl models to
generalize across domains [3].domain adaptation (da) has been extensively
studied to alleviate the aforementioned limitations, the goal of which is to
reduce the domain gap caused by the diversity of datasets from different domains
[12,20,26,29,33]. example solutions include transfer learning-and style
transfer-based methods. nonetheless, unlike natural images, generating labels
can be a challenging task, making it difficult to apply general da methods; thus
bridging domain gaps by da methods remains limited [26,33]. this is due to
sensitive privacy issues in patients' data, particularly in collaborative
research, which restricts access to labels from different domains. as a result,
conventional da methods cannot be easily applied [10]. more recently,
unsupervised domain adaptation (uda) has been introduced to address this issue
[16,33], aiming to generate semi-predictions (pseudo-labels) in target domains
first, followed by producing accurate predictions using the pseudo-labels. one
critical limitation of pseudo-label-based uda is the possibility of error
accumulation due to mispredicted pseudo-labels. this can lead to significant
degradation of the performance of dl models, as errors can compound and become
more pronounced over time [17,25].to alleviate the problem of pseudo-label-based
uda, in this work, we propose an advanced uda framework based on self-supervised
da with a test-time finetuning network. test-time adaptation methods have been
developed [4,11,13,23] to improve the learning of knowledge in target domains.
the distinctive feature of our test-time self-supervised da is that it enables
the dl network (i) to learn knowledge about the features of target domains by
fine-tuning the network itself during the test-time phase, rather than
generating pseudo-labels and then (ii) to provide precise predictions on images
in target domains, by using the fine-tuned network. specifically, we adopt
self-supervised learning and verify the model via thorough mathematical
analysis. our framework was tested on the task of breast cancer segmentation in
ultrasound images, but it could also be applied to other lesion segmentation
tasks.to summarize, our contributions are three-fold:• we design a
self-supervised da framework that includes a parameter search method and provide
a mathematical justification for it. with our framework, we are able to identify
the best-performing parameters that result in improved performance in da tasks.
• our framework is effective at preserving privacy, since it carries out da
using only pre-trained network parameters, without transferring any patient
data. • we applied our framework to the task of segmenting breast cancer from
ultrasound imaging data, demonstrating its superior performance over competing
uda methods.our results indicate that our framework is effective in improving
the accuracy of breast cancer segmentation from ultrasound images, which could
have potential implications for improving the diagnosis and treatment of breast
cancer. sample batches of (t, ?) ∼ t",1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,3.1,Experimental Set-Ups,"to evaluate the segmentation performance of our ttft framework, we used three
different ultrasound databases: bus [32], busi [1], and buv [18], which are
considered to be different domains. all three databases contain ultrasound
imaging data and segmentation masks for breast cancer, with the masks labeled as
0 (background) and 1 (lesion) using a one-hot encoding. the bus database
consists of 163 images along with corresponding labels. the busi database
contains 780 images, with 133 images belonging to the normal class and having
labels containing only 0 values. the buv database originally consists of
ultrasound videos, providing a total of 21,702 frames. while the database also
provides labels for the detection task, we processed these labels as
segmentation masks using a region growing method [15]. we employed different
deep-learning models for evaluation. specifically, u-net [22] and fusionnet [21]
were employed as our baseline models, since u-net is a widely used basic model
for segmentation, and fusionnet contains advanced residual modules, compared
with u-net. ours i and ours ii were based on u-net and fusionnet as the baseline
network, respectively. additionally, mib-net [28], which is a state-of-the-art
model for breast cancer segmentation using ultrasound images, was employed for
comparison. furthermore, cbst [33] and ct-net [16] were employed as the
comparison models for uda methods. as the evaluation metrics, dice coefficient
(d. coef), prauc, which is an area under a precision-recall curve, and cohen
kappa (κ) were employed [30]. our experimental set-ups included: (i) individual
databases were used to assess the baseline segmentation performance (appendix);
(ii) the domain adaptive segmentation performance was assessed using the three
databases, where two databases were regarded as the source domain, and the
remaining database was regarded as the target domain; and (iii) the ablation
study was carried out to evaluate the proposed network architecture along with
the randomized re-initialization method.",1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,3.2,Comparison Analysis,"since all compared dl models show similar d. coef, only uda performance is
comparable as a control in our experiments. in this experiment, two databases
were used for training, and the remaining database was used for testing. for
instance, bus in fig. 3 illustrates the bu s database was used for testing, and
the other two databases of busi and buv were used for training. figs. 3 and4
show quantitative results, and fig. 5 shows the sample segmentation results.
unlike the experiment using the individual database, u-net, fusionnet, and
mib-net showed significantly inferior scores due to domain gaps. in contrast,
uda methods of cbst and ct-net showed superior scores, compared with others, and
the scores were not strongly reduced, compared with the experiment with the
single database. note that, our ttft framework achieved the best performance
compared with other dl models. additionally, ours ii, based on fusionnet, showed
the best scores, potentially due to the advanced residual connection module.
furthermore, as illustrated in fig 4, our framework provides superior precision
scores in a long range of (0, 0.7), indicating that our frameworks estimated
unnecessary mispredictions but precise predictions on cancer.",1
Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning,4.0,Discussion and Conclusion,"in this work, we proposed a dl-based segmentation framework for multi-domain
breast cancer segmentation on ultrasound images. due to the low resolution of
ultrasound images, manual segmentation of breast cancer is challenging even for
expert clinicians, resulting in a sparse number of labeled data. to address this
issue, we introduced a novel self-supervised da network for breast cancer
segmentation in ultrasound images. in particular, we proposed a test-time
finetuning network to learn domain-specific knowledge via knowledge distillation
by self-supervised learning. since uda is susceptible to error accumulation due
to imprecise pseudo-labels, which can lead to degraded performance, we employed
a self-supervised learning-based pretext task. specifically, we utilized an
autoencoder-based network architecture to generate synthetic images that matched
the input images. moreover, we introduced a randomized re-initialization module
that injects randomness into network parameters to reposition the network from
the local minimum in the source domain to a local minimum that is better suited
for the target domain. this approach enabled our framework to efficiently
fine-tune the network in the target domain and achieve better segmentation
performance. experimental results, carried out with three ultrasound databases
from different domains, demonstrated the superior segmentation performance of
our framework over other competing methods. additionally, our framework is
well-suited to a scenario in which access to source domain data is limited, due
to data privacy protocols. it is worth noting that we used vanilla u-net [22]
and fusionnet [21] as baseline models to evaluate the basic performance of our
ttft framework. however, the use of more advanced baseline models could lead to
even better segmentation performance, which is a subject for our future work.
moreover, our proposed framework is not limited to breast cancer segmentation on
ultrasound images acquired from different domains. it can also be applied to
other disease groups or imaging modalities such as mri or ct.",1
PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model,1.0,Introduction,"positron emission tomography (pet) is a sensitive nuclear imaging technique, and
plays an essential role in early disease diagnosis, such as cancers and
alzheimer's disease [8]. however, acquiring high-quality pet images requires
injecting a sufficient dose (standard dose) of radionuclides into the human
body, which poses unacceptable radiation hazards for pregnant women and infants
even following the as low as reasonably achievable (alara) principle [19]. to
reduce the radiation hazards, besides upgrading imaging hardware, designing
advanced pet enhancement algorithms for improving the quality of low-dose pet
(lpet) images to standard-dose pet (spet) images is a promising alternative.in
recent years, many enhancement algorithms have been proposed to improve pet
image quality. among the earliest are filtering-based methods such as non-local
mean (nlm) filter [1], block-matching 3d filter [4], bilateral filter [7], and
guided filter [22], which are quite robust but tend to over-smooth images and
suppress the high-frequency details. subsequently, with the development of deep
learning, the end-to-end pet enhancement networks [9,14,21] were proposed and
achieved significant performance improvement. but these supervised methods
relied heavily on the paired lpet and spet data that are rare in actual clinic
due to radiation exposure and involuntary motions (e.g., respiratory and muscle
relaxation). consequently, unsupervised pet enhancement methods such as deep
image prior [3], noise2noise [12,20], and their variants [17] were developed to
overcome this limitation. however, these methods still require lpet to train
models, which contradicts with the fact that only spet scans are conducted in
clinic.fortunately, the recent glowing diffusion model [6] provides us with the
idea for proposing a clinically-applicable pet enhancement approach, whose
training only relies on spet data. generally, the diffusion model consists of
two reversible processes, where the forward diffusion adds noise to a clean
image until it becomes pure noise, while the reverse process removes noise from
pure noise until the clean image is recovered. by combining the mechanics of
diffusion model with the observation that the main differences between lpet and
spet are manifested as levels of noises in the image [11], we can view lpet and
spet as results at different stages in an integrated diffusion process.
therefore, when a diffusion model (trained only on spet) can recover noisy
samples to spet, this model can also recover lpet to spet. however, extending
the diffusion model developed for 2d photographic images to pet enhancement
still faces two problems: a) three-dimensionsal (3d) pet images will
dramatically increase the computational cost of diffusion model; b) pet is the
detail-sensitive images and may be introduced/lost some details during the
procedure of adding/removing noise, which will affect the downstream
diagnosis.taking all into consideration, we propose the spet-only unsupervised
pet enhancement (upete) framework based on the latent diffusion model.
specifically, upete has an encoder-<diffusion model>-decoder structure that
first uses the encoder to compress input the lpet/spet images into latent
representations, then uses the latent diffusion model to learn/estimate the
distribution of spet latent representations, and finally uses the decoder to
recover spet images from the estimated spet latent representations. the keys of
our upete include 1) compressing the 3d pet images into a lower dimensional
space for reducing the computational cost of diffusion model, 2) adopting the
poisson noise, which is the dominant noise in pet imaging [20], to replace the
gaussian noise in the diffusion process for avoiding the introduction of details
that are not existing in pet images, and 3) designing ct-guided cross-attention
to incorporate additional ct images into the inverse process for helping the
recovery of structural details in pet.our work had three main
features/contributions: i) proposing a clinicallyapplicable unsupervised pet
enhancement framework, ii) designing three targeted strategies for improving the
diffusion model, including pet image compression, poisson diffusion, and
ct-guided cross-attention, and iii) achieving better performance than
state-of-the-art methods on the collected pet datasets.",1
3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images,2.0,Related Work,"learning from weak annotations. weak annotations have been used in deep learning
segmentation to reduce the annotation effort through cheaper, less accurate, or
sparser labeling [20]. bai et al. [1] learn to perform aortic image segmentation
by sparsely annotating only a subset of the input slices. multiple instance
learning approaches bin pixels together by only providing labels at the bin
level. jia et al. [12] use this approach to segment cancer on histopathology
images successfully. annotating 2d projections for 3d data is another approach
to using weak segmentation labels, which has garnered popularity recently in the
medical domain. bayat et al. [2] propose to learn the spine posture from 2d
radiographs, while zhou et al. [22] use multi-planar mips for multi-organ
segmentation of the abdomen. kozinski et al. [13] propose to segment vessel
centerlines using as few as 2-3 annotated mips. chen et al. [4] train a vessel
segmentation model from unsupervised 2d labels transferred from a publicly
available dataset, however, there is still a gap to be closed between
unsupervised and supervised model performance. our work uses weak annotations in
the form of annotations of 2d mips for the task of peripancreatic vessel
segmentation, where we attempt to reduce the annotation cost to a minimum by
only annotating a single projection per training input without sacrificing
performance.incorporating depth information. depth is one of the properties of
the 3d world. loss of depth information occurs whenever 3d data is projected
onto a lower dimensional space. in natural images, depth loss is inherent
through image acquisition, therefore attempts to recover or model depth have
been employed for 3d natural data. for instance, fu et al. [9] use neural
implicit fields to semantically segment images by transferring labels from 3d
primitives to 2d images. lawin et al. [14] propose to segment 3d point clouds by
projecting them onto 2d and training a 2d segmentation network. at inference
time, the predicted 2d segmentation labels are remapped back to the original 3d
space using the depth information. in the medical domain, depth information has
been used in volume rendering techniques [7] to aid with visualization, but it
has so far not been employed when working with 2d projections of 3d volumes to
recover information loss. we propose to do the conceptually opposite approach
from lawin et al. [14], by projecting 3d volumes onto 2d to facilitate and
reduce annotation. we use depth information to map the 2d annotations to the
original 3d space at annotation time and generate partial 3d segmentation
volumes, which we incorporate in training as an additional loss term.",1
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy,3.1,Results,"we perform experiments with two templates: sphere and medoid. we compare the
performance of flowssm [15] with mesh2ssm with the template feedback loop. for
mesh2ssm template, we use 256 points uniformly spread across the surface of the
sample. mesh2ssm and flowssm do not have a equivalent latent space for
comparison of the shape models, hence, we consider the deformed mesh vertices of
flowssm as correspondences and perform pca analysis. figure 3 shows the top
three pca modes of variations identified by mesh2ssm and flowssm. similar to the
observations made box-bump dataset, flowssm is affected by the choice of the
template, and the modes of variation differ as the template changes. on the
other hand, pdm predicted by mesh2ssm identifies the same primary modes
consistently. pancreatic cancer mainly presents itself on the head of the
structure [20] and for the decath dataset, we can see the first mode identifies
the change in the shape of the head. we evaluate the models based on
compactness, generalization, and specificity. compactness measures the ability
of the model to reconstruct new shape instances with fewer parameters using pca
explained variance. generalization measures the average surface distance between
all test shapes and their reconstructions, and specificity measures the distance
between randomly generated pca samples. figure 4.a shows the metrics for the
pancreas dataset. mesh2ssm outperforms flowssm in all three metrics, despite
using only 256 correspondence points compared to flowssm's ∼2000 vertices.
mesh2ssm correspondence generation module efficiently parameterizes the surface
of the pancreas with a minimum number of parameters. mesh2ssm template, shown in
fig. 4.b, becomes more detailed as optimization continues, regardless of the
starting template. the model can learn correct deformations in the
correspondence generation module and identify the correct mean shape in the
latent space of sp-vae in the analysis module. using the analysis module of
mesh2ssm, we visualized the top three modes of variation identified by sorting
the latent dimensions of sp-vae based on the standard deviations of the latent
embeddings of the training dataset. variations are generated by perturbing the
latent representation of a sample in three directions, resulting in non-linear
modes such as changes in the size and shape of the pancreas head and narrowing
of the neck and body. this is shown in fig. 4.c for meshssm model with medoid
starting template. the distance metrics for the reconstructions of the testing
samples were also computed. the results of the metrics are summarized in table
1. the calculation involved the l 1 chamfer loss between the predicted points
(correspondences in the case of mesh2ssm and the deformed mesh vertices in the
case of flowssm) and the original mesh vertices. additionally, the surface to
surface distance of the mesh reconstructions (using the correspondences in
mesh2ssm and deformed meshes in flowssm) was included. for the pancreas dataset
with the medoid as the initial template, mesh2ssm with the template feedback
produced more precise models.",1
Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation,1.0,Introduction,"medical image segmentation is a critical task in computer-aided diagnosis and
treatment planning. it involves the delineation of anatomical structures or
pathological regions in medical images, such as magnetic resonance imaging (mri)
or computed tomography (ct) scans. accurate and efficient segmentation is
essential for various medical applications, including tumor detection, surgical
planning, and monitoring disease progression. however, manual medical imaging
annotation is time-consuming and expensive because it requires the domain
knowledge from medical experts. therefore, there is a growing interest in
developing semi-supervised learning that leverages both labeled and unlabeled
data to improve the performance of image segmentation models [16,27].existing
semi-supervised segmentation methods exploit smoothness assumption, e.g., the
data samples that are closer to each other are more likely to to have the same
label. in other words, the smoothness assumption encourages the model to
generate invariant outputs under small perturbations. we have seen such
perturbations being be added to natural input images at data-level
[4,9,14,19,21], feature-level [6,17,23,25], and model-level [8,11,12,24,28].
among them, virtual adversarial training (vat) [14] is a well-known one which
promotes the smoothness of the local output distribution using adversarial
examples. the adversarial examples are near decision boundaries generated by
adding adversarial perturbations to natural inputs. however, vat can only create
one adversarial sample in a run, which is often insufficient to completely
explore the space of possible perturbations (see sect. 2.1). in addition, the
adversarial examples of vat can also lie together and lose diversity that
significantly reduces the quality of adversarial examples [15,20]. mixup
regularization [29] is a data augmentation method used in deep learning to
improve model generalization. the idea behind mixup is to create new training
examples by linearly interpolating between pairs of existing examples and their
corresponding labels, which has been adopted in [2,3,19] to semi-supervised
learning. the work [5] suggests that mixup improves the smoothness of the neural
function by bounding the lipschitz constant of the gradient function of the
neural networks. however, we show that mixing between more informative samples
(e.g., adversarial examples near decision boundaries) can lead to a better
performance enhancement compared to mixing natural samples (see sect. 3.3).in
this paper, we propose a novel cross-adversarial local distribution
regularization for semi-supervised medical image segmentation for smoothness
assumption enhancement1 . our contributions are summarized as follows: 1) to
overcome the vat's drawback, we formulate an adversarial local distribution
(ald) with dice loss function that covers all possible adversarial examples
within a ball constraint. 2) to enhance smoothness assumption, we propose a
novel cross-adversarial local distribution regularization (cross-ald) to
encourage the smoothness assumption, which is a random mixing between two alds.",1
Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.4,Dataset and Quantitative Metrics,"we use publicly available data collected from a breast phantom (model 059, cirs:
tissue simulation & phantom technology, norfolk, va) using an alpinion e-cube
r12 research us machine (bothell, wa, usa). the center frequency was 8 mhz and
the sampling frequency was 40 mhz. the young's modulus of the experimental
phantom was 20 kpa and contains several inclusions with young's modulus of
higher than 40 kpa. this data is available online at http://code.sonography.ai
in [16].in vivo data was collected at johns hopkins hospital from patients with
liver cancer during open-surgical rf thermal ablation by a research antares
siemens system using a vf 10-5 linear array with the sampling frequency of 40
mhz and the center frequency of 6.67 mhz. the institutional review board
approved the study with the consent of the patients. we selected 600 rf frame
pairs of this dataset for the training of the networks.two well-known metrics of
contrast to noise ratio (cnr) and strain ratio (sr) are utilized to evaluate the
compared methods. two regions of interest (roi) are selected to compute these
metrics and they can be defined as [10]:where the subscript t and b denote the
target and background rois. the sr is only sensitive to the mean (s x ), while
cnr depends on both the mean and the standard deviation (σ x ) of rois. for
stiff inclusions as the target, higher cnr correlates with better target
visibility, and lower sr translates to a higher difference between the target
and background strains.",1
SLPD: Slide-Level Prototypical Distillation for WSIs,1.0,Introduction,"in computational histopathology, visual representation extraction is a
fundamental problem [14], serving as a cornerstone of the (downstream)
task-specific learning on whole slide pathological images (wsis). our community
has witnessed the progress of the de facto representation learning paradigm from
the supervised imagenet pre-training to self-supervised learning (ssl) [15,36].
numerous pathological applications benefit from ssl, including classification of
glioma [7], breast carcinoma [1], and non-small-cell lung carcinoma [25],
mutation prediction [32], microsatellite instability prediction [31], and
survival prediction from wsis [2,16]. among them, pioneering works [12,22,27]
directly apply the ssl algorithms developed for natural images (e.g., simclr
[10], cpc [30] and moco [11]) to wsi analysis tasks, and the improved
performance proves the effectiveness of ssl. however, wsi is quite different
from natural images in that it exhibits a hierarchical structure with giga-pixel
resolution. following works turn to designing pathological-specific tasks to
explore the inherent characteristics of wsis for representation learning, e.g.,
resolution-aware tasks [18,34,37] and color-aware tasks [2,38]. since the
pretext tasks encourage to mine the pathologically relevant patterns, the
learned representations are expected to be more suitable for wsi analysis.
nevertheless, these works only consider learning the representations at the
patch level, i.e, the cellular organization, but neglecting macro-scale
morphological features, e.g., tissue phenotypes and intra-tumoral heterogeneity.
as a result, there is still a gap between the pre-trained representations and
downstream tasks, as the latter is mainly at the slide level, e.g., subtyping,
grading and staging.more recently, some works propose to close the gap via
directly learning slidelevel representations in pre-training. for instance, hipt
[8], a milestone work, introduces hierarchical pre-training (dino [6]) for the
patch-level (256 × 256) and region-level (4096 × 4096) in a two-stage manner,
achieving superior performance on slide-level tasks. ss-camil [13] uses
efficientnet-b0 for image compression in the first stage and then derives
multi-task learning on the compressed wsis, which assumes the primary site
information, e.g., the organ type, is always available and can be used as pseudo
labels. ss-mil [35] also proposes a two-stage pre-training framework for wsis
using contrastive learning (simclr [10]), where the differently subsampled bags1
from the same wsi are positive pairs in the second stage. a similar idea can be
found in giga-ssl [20] with delicate patch-and wsi-level augmentations. the
aforementioned methods share the same two-stage pre-training paradigm, i.e.,
patch-to-region/slide. thus broader context information is preserved to close
the gap between pretext and downstream tasks. however, they are essentially
instance discrimination where only the self-invariance of region/slide is
considered, leaving the intraand inter-slide semantic structures unexplored.in
this paper, we propose to encode the intra-and inter-slide semantic structures
by modeling the mutual-region/slide relations, which is called slpd: slide-level
prototypical distillation for wsis. specifically, we perform the slide-level
clustering for the 4096 × 4096 regions within each wsi to yield the prototypes,
which characterize the medically representative patterns of the tumor (e.g.,
morphological phenotypes). in order to learn this intra-slide semantic
structure, we encourage the region representations to be closer to the assigned
prototypes. by representing each slide with its prototypes, we further select
semantically simi- lar slides by the set-to-set distance of prototypes. then, we
learn the inter-slide semantic structure by building correspondences between
region representations and cross-slide prototypes. we conduct experiments on two
benchmarks, nsclc subtyping and brca subtyping. slpd achieves state-of-the-art
results on multiple slide-level tasks, demonstrating that representation
learning of semantic structures of slides can make a suitable proxy task for wsi
analysis. we also perform extensive ablation studies to verify the effectiveness
of crucial model components.",1
SLPD: Slide-Level Prototypical Distillation for WSIs,2.3,Slide-Level Clustering,"many histopathologic features have been established based on the morphologic
phenotypes of the tumor, such as tumor invasion, anaplasia, necrosis and
mitoses, which are then used for cancer diagnosis, prognosis and the estimation
of response-to-treatment in patients [3,9]. to obtain meaningful representations
of slides, we aim to explore and maintain such histopathologic features in the
latent space. clustering can reveal the representative patterns in the data and
has achieved success in the area of unsupervised representation learning
[4,5,24,26].to characterize the histopathologic features underlying the slides,
a straightforward practice is the global clustering, i.e., clustering the region
embeddings from all the wsis, as shown in the left of fig. 1(d). however, the
obtained clustering centers, i.e., the prototypes, are inclined to represent the
visual bias related to staining or scanning procedure rather than medically
relevant features [33]. meanwhile, this clustering strategy ignores the
hierarchical structure ""region→wsi→whole dataset"" underlying the data, where the
id of the wsi can be served as an extra learning signal. therefore, we first
consider the slidelevel clustering that clusters the embeddings within each wsi,
which is shown in the right of fig. 1(d). specifically, we conduct k-means
algorithm before the start of each epoch over l n region embeddings {z l n } ln
l=1 of w n to obtain m prototypes {c m n ∈ r d } m m=1 . similar operations are
applied across other slides, and then we acquire n groups of prototypes {{c m n
} m m=1 } n n=1 . each group of prototypes is expected to encode the semantic
structure (e.g., the combination of histopathologic features) of the wsi.",1
SLPD: Slide-Level Prototypical Distillation for WSIs,2.5,Inter-Slide Distillation,"tumors of different patients can exhibit morphological similarities in some
respects [17,21], so the correspondences across slides should be characterized
during learning. previous self-supervised learning methods applied to
histopathologic images only capture such correspondences with positive pairs at
the patchlevel [22,23], which overlooks the semantic structure of the wsi. we
rethink this problem from the perspective how to measure the similarity between
two slides accurately. due to the heterogeneity of the slides, comparing them
with the local crops or the averaged global features are both susceptible to
being one-sided. to address this, we bridge the slides with their semantic
structures and define the semantic similarity between two slides w i and w j
through an optimal bipartite matching between two sets of prototypes:where
cos(•, •) measures the cosine similarity between two vectors, and s m enumerates
the permutations of m elements. the optimal permutation σ * can be computed
efficiently with the hungarian algorithm [19]. with the proposed setto-set
distance, we can model the inter-slide correspondences conveniently and
accurately. specifically, for a region embedding z belonging to the slide w and
assigned to the prototype c, we first search the top-k nearest neighbors of w in
the dataset based on the semantic similarity, denoted as { ŵk } k k=1 . second,
we also obtain the matched prototype pairs {(c, ĉk )} k k=1 determined by the
optimal permutation, where ĉk is the prototype of ŵk . finally, we encourage z
to be closer to ĉk with the inter-slide distillation:the inter-slide
distillation can encode the sldie-level information complementary to that of
intra-slide distillation into the region embeddings.the overall learning
objective of the proposed slpd is defined as:where the loss scale is simply set
to α 1 = α 2 = 1. we believe the performance can be further improved by tuning
this.",1
PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation,1.0,Introduction,"nuclei segmentation in histopathology images is an important task for cancer
diagnosis and immune response prediction [1,13,18]. while several fully
supervised deep learning approaches to segment nuclei exist [2,6,8,9,19,25],
labeling s. nam and j. jeong-equal contribution.thousands of instances are
tedious and the ambiguous nature of nuclei boundaries requires high-level expert
annotators. to address this, weakly-supervised nuclei segmentation methods
[5,10,15,20,23,28] have emerged as an attractive alternative using cheap and
inexact labels e.g., center point annotations. as point labels alone do not
provide sufficient foreground information, it is common to use euclidean
distance-based voronoi diagrams and k-means clustering [7] to generate pseudo
segmentation labels for training. however, since euclidean distance-based
schemes only use distance information while ignoring color, they often fail to
capture nuclei shape information; resulting in inadequate boundary delineation
between adjacent nuclei. moreover, prior methods [17,21,22] typically assume
that point labels are located precisely at the center of the nuclei. in
real-world scenarios, point annotation locations may shift from nuclei centers
as a result of the expert labeling process, leading to a lower performance after
model training.to overcome these challenges, we propose a novel weakly
supervised instance segmentation method that effectively distinguishes adjacent
nuclei and is robust to point shifts. the proposed model consists of three
modules responsible for binary segmentation, boundary delineation, and instance
separation. to train the binary segmentation module, we generate pseudo binary
segmentation masks using geodesic distance-based voronoi labels and cluster
labels from point annotations. geodesic distance provides more precise nuclei
shape information than previous euclidean distance-based schemes. to train the
offset map module, we generate pseudo offset maps by computing the offset
distance between binary segmentation pixel predictions and the point label. the
offset information facilitates precise delineation of the boundaries between
adjacent nuclei. to make the model robust to center point shifts, we introduce
an expectation maximization (em) [4] algorithm-based process to refine point
labels. note that previous approaches [17,21,22] optimize model parameters only
using a fixed set of point labels, while we instead alternatively update model
parameters and the center point locations. this refinement process ensures that
the model maintains high performance even when the point annotation is not
exactly located at the center of the nuclei.the contributions of this paper are
as follows: (1) we propose an end-toend weakly supervised segmentation model
that simultaneously predicts binary mask, offset map, and center map to
accurately identify and segment nuclei.(2) by utilizing geodesic distance, we
produce more detailed voronoi and cluster labels that precisely delineate the
boundary between adjacent nuclei. (3) we introduce an em algorithm-based
refinement process to encourage model robustness on center-shifted point labels.
(4) ablation and evaluation studies on two public datasets demonstrate our
model's ability to outperform state-of-the-art techniques not only with ideal
labels but also with shifted labels.",1
Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,3.0,Experiments and Results,"data: we evaluate our method on t2-weighted brain mr and chest x-ray datasets to
provide direct comparisons to state-of-the-art methods over a wide range of real
anomalies. for brain mri we train on the human connectome project (hcp) dataset
[28] which consists of 1113 mri scans of healthy, young adults acquired as part
of a scientific study. to evaluate, we use the brain tumor segmentation
challenge 2017 (brats) dataset [1], containing 285 cases with either high or low
grade glioma, and the ischemic stroke lesion segmentation challenge 2015 (isles)
dataset [13], containing 28 cases with ischemic stroke lesions. the data from
both test sets was acquired as part of clinical routine. the hcp dataset was
resampled to have 1mm isotropic spacing to match the test datasets. we apply
z-score normalisation to each sample and then align the bounding box of each
brain before padding it to a size of 160 × 224 × 160. lastly, samples are
downsampled by a factor of two.for chest x-rays we use the vindr-cxr dataset
[18] including 22 different local labels. to be able to compare with the
benchmarks reported in [6] we use the same healthy subset of 4000 images for
training along with their test set (ddad ts ) of 1000 healthy and 1000 unhealthy
samples, with some minor changes outlined as follows. first note that [6]
derives vindr-cxr labels using the majority vote of the 3 annotators.
unfortunately, this means there are 52 training samples, where 1/3 of
radiologists identified an anomaly, but the majority label is counted as
healthy. the same applies to 10 samples within the healthy testing subset. to
avoid this ambiguity, we replace these samples with leftover training data that
all radiologists have labelled as healthy. we also evaluate using the true test
set (vindr ts ), where two senior radiologists have reviewed and consolidated
all labels. for preprocessing, we clip pixel intensities according to the window
centre and width attributes in each dicom file, and apply histogram
equalisation, before scaling intensities to the range [-1, 1]. finally, images
are resized to 256 × 256. • indicates that the metrics are evaluated over the
same region and at the same resolution as cradl [12]. upper right part: metrics
on vindr-cxr, presented as ap/auroc on the vindr and ddad test splits. random is
the baseline performance of a random classifier. lower part: a sensitivity
analysis of the average ap of each individual fold (mean±s.d.) alongside that of
the model ensemble, varying how many tasks we use for training versus
validation. best results are highlighted in bold. comparison to state-of-the-art
methods: validating on synthetic tasks is one of our main motivations; as such,
we use a 1/4 (train/val.) task split to compare with benchmark methods. for
brain mri, we evaluate results at the slice and voxel level, computing average
precision (ap) and area under the receiver operating characteristic curve
(auroc), as implemented in scikit learn [19]. note that the distribution shift
between training and test data (research vs. clinical scans) adds further
difficulty to this task. in spite of this, we substantially improve upon the
current state-of-the-art (table 1 upper left). in particular, we achieve a
pixel-wise ap of 76.2 and 45.9 for brats and isles datasets respectively. to
make our comparison as faithful as possible, we also re-evaluate after
post-processing our predictions to match the region and resolution used by
cradl, where we see similar improvement. qualitative examples are shown in fig.
3. note that all baseline methods use a validation set consisting of real
anomalous samples from brats and isles to select which anomaly scoring function
to use. we, however, only use synthetic validation data. this further verifies
that our method of using synthetic data to estimate generalisation works
well.for both vindr-cxr test sets we evaluate at a sample and pixel level,
although previous publications have only reported their results at a sample
level. we again show performance above the current state-of-the-art (table 1
upper right). our results are also substantially higher than previously proposed
selfsupervised methods, improving on the current state-of-the-art nsa [23] by
12.6 to achieve 78.4 image-level ap. this shows that our use of synthetic
validation data succeeds where their fixed training schedule fails.",1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,1.0,Introduction,"automated segmentation of histopathological images is crucial, as it can
quantify the tumor micro-environment, provide a basis for cancer grading and
prognosis, and improve the diagnostic efficiency of clinical doctors [6,13,19].
however, pixellevel annotation of images is time-consuming and labor-intensive,
especially for histopathology images that require specialized knowledge.
therefore, there is an urgent need to pursue weakly supervised solutions for
pixel-wise segmentation. nonetheless, weakly supervised histopathological image
segmentation presents a challenge due to the low contrast between different
tissues, intra-class variations, and inter-class similarities [4,11].
additionally, the tissue structures in histopathology images can be randomly
arranged and dispersed, which makes it difficult to identify complete tissues or
regions of interest [7].ours cam under the microscope, tumor epithelial ɵssue
may appear as solid nests, acinar structures, or papillary formaɵons. the cells
may have enlarged and irregular nuclei.",1
TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation,0.0,Tumor epithelial Ɵssue,"necrosis ɵssue tumor-associated stromanecrosis may appear as areas of pink,
amorphous material under the microscope, and may be surrounded by viable tumor
cells and stroma.tumor-associated stroma ɵssue is the connecɵve ɵssue that
surrounds and supports the tumor epithelial ɵssue.fig. 1. comparison of
activation maps extracted from cam and our method, from left to right: origin
image, ground truth, three activation maps of tumor epithelial (red), necrosis
(green), and tumor-associated stroma (orange) respectively. on the right side,
there are some examples of the related language knowledge descriptions used in
our method. it shows that cam only highlights a small portion of the target,
while our method, which incorporates external language knowledge, can encompass
a wider and more precise target tissue. (color figure online)recent studies on
weakly supervised segmentation primarily follow class activation mapping (cam)
[20], which localizes the attention regions and then generates the pseudo labels
to train the segmentation network. however, the cam generated based on the
image-level labels can only highlight the most discriminative region, but fail
to locate the complete object, leading to defective pseudo labels, as shown in
fig. 1. accordingly, many attempts have been made to enhance the quality of cam
and thus boost the performance of weakly supervised segmentation. han et al. [7]
proposed an erasure-based method that continuously expands the scope of
attention areas to obtain rich content of pseudo labels. li et al. [11] utilized
the confidence method to remove any noise that may exist in the pseudo labels
and only included the confident pixel labels for the segmentation training.
zhang et al. [18] leveraged the transformer to model the long-distance
dependencies on the whole histopathological images to improve the cam's ability
to find more complete regions. lee et al. [10] utilized the ability of an
advanced saliency detection model to assist cam in locating more precise
targets. however, these improved variants still face difficulties in capturing
the complete tissues. the primary limitation is that the symptoms and
manifestations of histopathological subtypes cannot be comprehensively described
by an abstract semantic category. as a result, the image-level label supervision
may not be sufficient to pinpoint the complete target area.to remedy the
limitations of image-level supervision, we advocate for the integration of
language knowledge into weakly supervised learning to provide reliable guidance
for the accurate localization of target structures. to this end, we propose a
text-prompting-based weakly supervised segmentation method (tpro) for accurate
histopathology tissue segmentation. the text information originates from the
task's semantic labels and external descriptions of subtype manifestations. for
each semantic label, a pre-trained medical language model is utilized to extract
the corresponding text features that are matched to each feature point in the
image spatial space. a higher similarity represents a higher possibility of this
location belonging to the corresponding semantic category. additionally, the
text representations of subtype manifestations, including tissue morphology,
color, and relationships to other tissues, are extracted by the language model
as external knowledge. the discriminative information can be explored from the
text knowledge to help identify and locate complete tissues accurately by
jointly modeling long-range dependencies between image and text. we conduct
experiments on two weakly supervised histological segmentation benchmarks,
luad-histoseg and bcss-wsss, and demonstrate the superior quality of pseudo
labels produced by our tpro model compared to other cam-based methods.our
contributions are summarized as follows: (1) to the best of our knowledge, this
is the first work that leverages language knowledge to improve the quality of
pseudo labels for weakly-supervised histopathology image segmentation. (2) the
proposed text prompting models the correlation between image representations and
text knowledge, effectively improving the quality of pseudo labels. (3) the
effectiveness of our approach has been effectively validated by two benchmarks,
setting a new state of the art.",1
VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,3.0,Experimental Setup,"materials. we trained our networks using a subset of the open-access intra
dataset1 published by yang et al. in 2020 [32]. this subset consisted of 1694
healthy vessel segments reconstructed from 2d mra images of patients. we
converted 3d meshes into a binary tree representation and used the network
extraction script from the vmtk toolkit2 to extract the centerline coordinates
of each vessel model. the centerline points were determined based on the ratio
between the sphere step and the local maximum radius, which was computed using
the advancement ratio specified by the user. the radius of the blood vessel
conduit at each centerline sample was determined using the computed
crosssections assuming a maximal circular shape (see fig. 2). to improve
computational efficiency during recursive tree traversal, we implemented an
algorithm that balances each tree by identifying a new root. we additionally
trimmed trees to a depth of ten in our experiments. this decision reflects a
balance between the computational demands of depth-first tree traversal in each
training step and the complexity of the training meshes. we excluded from our
study trees that exhibited greater depth, nodes with more than two children, or
with loops. however, non-binary trees can be converted into binary trees and it
is possible to train with deeper trees at the expense of higher computational
costs. ultimately, we were able to obtain 700 binary trees from the original
meshes using this approach.implementation details. for the centerline
extraction, we set the advancement ratio in the vmtk script to 1.05. the script
can sometimes produce multiple cross-sections at centerline bifurcations. in
those cases, we selected the sample with the lowest radius, which ensures proper
alignment with the centerline principal direction. all attributes were
normalized to a range of [0, 1]. for the mesh reconstruction we used 4
iterations of catmull-clark subdivision algorithm. the data pre-processing
pipeline and network code were implemented in python and pytorch
framework.training. in all stages, we set the batch size to 10 and used the adam
optimizer with β 1 = 0.9, β 2 = 0.999, and a learning rate of 1 × 10 -4 . we set
α = .3 and γ = .001 for eq. 1 in our experiments. to enhance computation speed,
we implemented dynamic batching [16], which groups together operations involving
input trees of dissimilar shapes and different nodes within a single input
graph. it takes approximately 12 h to train our models on a workstation equipped
with an nvidia a100 gpu, 80 gb vram, and 256 gb ram. however, the memory
footprint during training is very small (≤1 gb) due to the use of a lightweight
tree representation. this means that the amount of memory required to store and
manipulate our training data structures is minimal. during training, we ensure
that the reconstructed tree aligns with the original structure, rather than
relying solely on the classifier's predictions. we train the classifier using a
crossentropy loss that compares its predictions to the actual values from the
original tree. since the number of nodes in each class is unbalanced, we scale
the weight given to each class in the cross-entropy loss using the inverse of
each class count. during preliminary experiments, we observed that accurately
classifying nodes closer to the tree root is critical. this is because a
miss-classification of top nodes has a cascading effect on all subsequent nodes
in the tree (i.e. skip reconstructing a branch). to account for this, we
introduce a weighting scheme that for each node, assigns a weight to the
cross-entropy loss based on the number of total child nodes. the weight is
normalized by the total number of nodes in the tree.metrics. we defined a set of
metrics to evaluate our trained network's performance. by using these metrics,
we can determine how well the generated 3d models of blood vessels match the
original dataset distribution, as well as the diversity of the generated output.
the chosen metrics have been widely used in the field of blood vessel 3d
modeling, and have shown to provide reliable and accurate quantification of
blood vessels main characteristics [3,13]. we analyzed tortuosity per branch,
the vessel centerline total length, and the average radius of the tree.
tortuosity distance metric [4] is a widely used metric in the field of blood
vessel analysis, mainly because of its clinical importance. it measures the
amount of twistiness in each branch of the vessel. vessel's total length and
average radius were used in previous work to distinguish healthy vasculature
from cancerous malformations. finally, in order to measure the distance across
distributions for each metric, we compute the cosine similarity.",1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,1.0,Introduction,"transfer learning has become a standard practice in medical image analysis as
collecting and annotating data in clinical scenarios can be costly. the
pre-trained parameters endow better generalization to dnns than the models
trained from scratch [8,23]. a popular approach to enhancing model
transferability is by pretraining on domains similar to the targets
[9,21,[27][28][29]. however, utilizing specialized pre-training for all medical
applications becomes impractical due to the fig. 1. the motivation of metalr.
previous works fix transferable layers in pre-trained models to prevent them
from catastrophic forgetting. it is inflexible and labor-expensive for this
method to find the optimal scheme. metalr uses meta-learning to automatically
optimize layer-wise lr for fine-tuning.diversity between domains and tasks and
privacy concerns related to pre-training data. consequently, recent work
[2,6,14,22] has focused on improving the generalization capabilities of existing
pre-trained dnn backbones through fine-tuning techniques.previous studies have
shown that the transferability of lower layers is often higher than higher
layers that are near the model output [26]. layer-wise finetuning [23], was thus
introduced to preserve the transferable low-level knowledge by fixing lower
layers. but recent studies [7] revealed that the lower layers may also be
sensitive to small domains like medical images. given the two issues,
transferability for medical tasks becomes more complicated [24,25]. it can even
be irregular among layers for medical domains far from pre-training data [7].
given the diverse medical domains and model architectures, there is currently no
universal guideline to follow to determine whether a particular layer should be
retrained for a given target domain.to search for optimal layer combinations for
fine-tuning, manually selecting transferable layers [2,23] can be a solution,
but it requires a significant amount of human labor and computational cost. in
order to address this issue and improve the flexibility of fine-tuning
strategies, we propose controlling the fine-tuning process with layer-wise
learning rates (lrs), rather than simply manually fixing or updating the layers
(see fig. 1). our proposed algorithm, meta learning rate (metalr), is based on
meta-learning [13] and adaptively adjusts lrs for each layer according to
transfer feedback. it treats the layer-wise lrs as metaknowledge and optimizes
them to improve the model generalization. larger lrs indicate less
transferability of corresponding layers and require more updating, while smaller
lrs preserve transferable knowledge in the layers. inspired by [20], we use an
online adaptation strategy of lrs with a time complexity of o(n), instead of the
computationally-expensive bi-level o(n 2 ) meta-learning. we also enhance the
algorithm's performance and stability with a proportional hyper-lr (lr for lr)
and a validation scheme on training data batches.in summary, this work makes the
following three contributions. 1) we introduce metalr, a meta-learning-based lr
tuner that can adaptively adjust layerwise lrs based on transfer learning
feedback from various medical domains.2) we enhance metalr with a proportional
hyper-lr and a validation scheme using batched training data to improve the
algorithm's stability and efficacy. 3) extensive experiments on both lesion
detection and tumor segmentation tasks were conducted to demonstrate the
superior efficiency and performance of met-alr compared to current sota medical
fine-tuning techniques.",1
MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging,3.1,Experimental Settings,"we extensively evaluate metalr on four transfer learning tasks (as shown in
table 1). to ensure the reproducibility of the results, all pre-trained models
(uscl [9], imagenet [11], c2l [28], models genesis [29]) and target datasets
(pocus [5], busi [1], chest x-ray [17], lits [4]) are publicly available. in our
work, we consider models pre-trained on both natural and medical image datasets,
with three target modalities and three target organs, which makes our
experimental results more credible. for the lesion detection tasks, we used
resnet-18 [15] with the adam optimizer. the initial learning rate (lr) and
hyper-lr coefficient β are set to 10 -3 and 0.1, respectively. in addition, we
use 25% of the training set as the validation set for meta-learning. for the
segmentation task, we use 3d u-net [10] with the sgd optimizer. the initial lr
and hyper-lr coefficient β are set to 10 -2 and 3 × 10 -3 , respectively. the
validation set for the lits segmentation dataset comprises 23 samples from the
training set of size 111. all experiments are implemented using pytorch 1.10 on
an nvidia rtx a6000 gpu. we report the mean values and standard deviations for
each experiment with five different random seeds. for more detailed information
on the models and hyper-parameters, please refer to our supplementary material.
imagenet [11] supervised busi [1] breast us tumor detection 780 images mimic-cxr
[16] c2l [28] chest x-ray [17] lung x-ray pneumonia detection 5856 images
lidc-idri [3] models genesis [29] lits [4] liver ct liver segmentation 131
volumes",1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,1.0,Introduction,"modern microscopes allow the digitalization of conventional glass slides into
gigapixel whole-slide images (wsis) [18], facilitating their preservation and
fig. 1. overview of our proposed framework, das-mil. the features extracted at
different scales are connected (8-connectivity) by means of different graphs.
the nodes of both graphs are later fused into a third one, respecting the rule
""part of"". the contextualized features are then passed to distinct
attention-based mil modules that extract bag labels. furthermore, a knowledge
distillation mechanism encourages the agreement between the predictions
delivered by different scales.retrieval, but also introducing multiple
challenges. on the one hand, annotating wsis requires strong medical expertise,
is expensive, time-consuming, and labels are usually provided at the slide or
patient level. on the other hand, feeding modern neural networks with the entire
gigapixel image is not a feasible approach, forcing to crop data into small
patches and use them for training. this process is usually performed considering
a single resolution/scale among those provided by the wsi image.recently,
multi-instance learning (mil) emerged to cope with these limitations. mil
approaches consider the image slide as a bag composed of many patches, called
instances; afterwards, to provide a classification score for the entire bag,
they weigh the instances through attention mechanisms and aggregate them into a
single representation. it is noted that these approaches are intrinsically flat
and disregard the pyramidal information provided by the wsi [15], which have
been proven to be more effective than single-resolution [4,13,15,19]. however,
to the best of our knowledge, none of the existing proposals leverage the full
potential of the wsi pyramidal structure. indeed, the flat concatenation of
features [19] extracted at different resolutions does not consider the
substantial difference in the informative content they provide. a proficient
learning approach should instead consider the heterogeneity between global
structures and local cellular regions, thus allowing the information to flow
effectively across the image scales.to profit from the multi-resolution
structure of wsi, we propose a pyramidal graph neural network (gnn) framework
combined with (self) knowledge distillation (kd), called das-mil (distilling
across scales). a visual representation of the proposed approach is depicted in
fig. 1. distinct gnns provide contextualized features, which are fed to distinct
attention-based mil modules that compute bag-level predictions. through
knowledge distillation, we encour-age agreement across the predictions delivered
at different resolutions, while individual scale features are learned in
isolation to preserve the diversity in terms of information content. by
transferring knowledge across scales, we observe that the classifier
self-improves as information flows during training. our proposal has proven its
effectiveness on two well-known histological datasets, camelyon16 and tcga lung
cancer, obtaining state-of-the-art results on wsi classification.",1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,3.0,Method,"our approach aims to promote the information flow through the different employed
resolutions. while existing works [19,20,25] take into account interscales
interactions by mostly leveraging trivial operations (such as concatenation of
related feature representations), we instead provide a novel technique that
builds upon: i) a gnn module based on message passing, which propagates patches'
representation according to the natural structure of multi-resolutions wsi; ii)
a regulation term based on (self) knowledge distillation, which pins the most
effective resolution to further guide the training of the other one(s). in the
following, we are delving into the details of our architecture.feature
extraction. our work exploits dino, the self-supervised learning approach
proposed in [5], to provide a relevant representation of each patch. differently
from other proposals [19,20,28], it focuses solely on aligning positive pairs
during optimization (and hence avoids negative pairs), which has shown to
require a lower memory footprint during training. we hence devise an initial
stage with multiple self-supervised feature extractors f (•; θ 1 ), . . . , f m
(•; θ m ), one dedicated to each resolution: this way, we expect to promote
feature diversity across scales. after training, we freeze the weights of these
networks and use them as patch-level feature extractors. although we focus only
on two resolutions at time (i.e., m = 2) the approach can be extended to more
scales.architecture. the representations yield by dino provide a detailed
description of the local patterns in each patch; however, they retain poor
knowledge of the surrounding context. to grasp a global guess about the entire
slide, we allow patches to exchange local information. we achieve it through a
pyramidal graph neural network (pgnn) in which each node represents an
individual wsi patch seen at different scales. each node is connected to its
neighbors (8-connectivity) in the euclidean space and between scales following
the relation ""part of""1 . to perform message passing, we adopt graph attention
layers (gat) [27].in general terms, such a module takes as input multi-scale
patch-level representations x = [x 1 x 2 ], where x 1 ∈ r n1×f and x 2 ∈ r n2×f
are respectively the representations of the lower and higher scale. the input
undergoes two graph layers: while the former treats the two scales as
independent subgraphs a 1 ∈ r n1×n1 and a 2 ∈ r n2×n2 , the latter process them
jointly by considering the entire graph a (see fig. 1, left). in formal
terms:where h ≡ [h 1 h 2 ] stands for the output of the pgnn obtained by
concatenating the two scales. these new contextualized patch representations are
then fed to the attention-based mil module proposed in [19], which produces
bag-level scores y bag 1 , y bag 2 ∈ r 1×c where c equals the number of classes.
notably, such a module provides additional importance scores z 1 ∈ r n1 and z 2
∈ r n2 , which quantifies the importance of each original patch to the overall
prediction.aligning scales with (self ) knowledge distillation. we have hence
obtained two distinct sets of predictions for the two resolutions: namely, a
bag-level score (e.g., a tumor is either present or not) and a patch-level one
(e.g., which instances contribute the most to the target class). however, as
these learned metrics are inferred from different wsi zooms, a disagreement may
emerge: indeed, we have observed (see table 4) that the higher resolutions
generally yield better classification performance. in this work, we exploit such
a disparity to introduce two additional optimization objectives, which pin the
predictions out of the higher scale as teaching signal for the lower one.
further than improving the results of the lowest scale only, we expect its
benefits to propagate also to the shared message-passing module, and so to the
higher resolution.formally, the first term seeks to align bag predictions from
the two scales through (self) knowledge distillation [14,29]:where kl stands for
the kullback-leibler divergence and τ is a temperature that lets secondary
information emerge from the teaching signal.the second aligning term regards the
instance scores. it encourages the two resolutions to assign criticality scores
in a consistent manner: intuitively, if a lowresolution patch has been
considered critical, then the average score attributed to its children patches
should be likewise high. we encourage such a constraint by minimizing the
euclidean distance between the low-resolution criticality grid map z 1 and its
subsampled counterpart computed by the high-resolution branch:(2)in the equation
above, graphpooling identifies a pooling layer applied over the higher scale: to
do so, it considers the relation ""part of"" between scales and then averages the
child nodes, hence allowing the comparison at the instance level.overall
objective. to sum up, the overall optimization problem is formulated as a
mixture of two objectives: the one requiring higher conditional likelihood
w.r.t. ground truth labels y and carried out through the cross-entropy loss l ce
(•; y); the other one based on knowledge distillation:where λ is a
hyperparameter weighting the tradeoff between the teaching signals provided by
labels and the higher resolution, while β balances the contributions of the
consistency regularization introduced in eq. ( 2).",1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,0.0,TCGA Lung Dataset.,"it is available on the gdc data transfer portal and comprises two subsets of
cancer: lung adenocarcinoma (luad) and lung squamous cell carcinoma (lusc),
counting 541 and 513 wsis, respectively. the aim is to classify luad vs lusc; we
follow the split proposed by dsmil [19].",1
DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs,4.2,Model Analysis,"on the impact of knowledge distillation. to assess its merits, we conducted
several experiments varying the values of the corresponding balancing
coefficients (see table 2). as can be observed, lowering their values (even
reaching λ = 0, i.e., no distillation is performed) negatively affects the
performance. such a statement holds not only for the lower resolution (as one
could expect), but also for the higher one, thus corroborating the claims we
made in sect. 3 on the bidirectional benefits of knowledge distillation in our
multi-scale architecture. we have also performed an assessment on the
temperature τ , which controls the smoothing factor applied to teacher's
predictions (table 3). we found that the lowest the temperature, the better the
results, suggesting that the teacher scale is naturally not overconfident about
its predictions, but rather well-calibrated.single-scale vs multi-scale. the
impact of the feature extractors and gnns. table 5 proposes an investigation of
these aspects, which considers both simcrl [8] and dino, as well as the recently
proposed graph mechanism h 2 -mil [15]. in doing so, we fix the input
resolutions to 5× and 20×. we draw the following conclusions: i) when our
das-mil feature propagation layer is used, the selection of the optimal feature
extractor (i.e., simclr vs dino) has less impact on performance, as the
message-passing can compensate for possible lacks in the initial representation;
ii) das-mil appears a better features propagator w.r.t. h 2 -mil.h 2 -mil
exploits a global pooling layer (ihpool) that fulfils only the spatial structure
of patches: as a consequence, if non-tumor patches surround a tumor patch, its
contribution to the final prediction is likely to be outweighed by the ihpool
module of h 2 -mil. differently, our approach is not restricted in such a way,
as it can dynamically route the information across the hierarchical structure
(also based on the connections with the critical instance).",1
Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation,3.1,Experiment on MSD Dataset,"the medical segmentation decathlon (msd) [2] dataset is composed of ten
different datasets with various challenging characteristics, which are widely
used in the medical image analysis field. to evaluate the effectiveness of
cc-fv, we conduct extensive experiments on 5 of the msd dataset, including
task03 liver(liver and tumor segmentation), task06 lung(lung nodule
segmentation), task07 pancreas(pancreas and pancreas tumor segmentation), task09
spleen(spleen segmentation), and task10 colon(colon cancer segmentation). all of
the datasets are 3d ct images. the public part of the msd dataset is chosen for
our experiments, and each dataset is divided into a training set and a test set
at a scale of 80% and 20%. for each dataset, we use the other four datasets to
pre-train the model and fine-tune the model on this dataset to evaluate the
performance as well as the transferability using the correlation between two
ranking sequences of upstream pre-trained models. we load all the pre-trained
models' parameters except for the last convolutional layer and no parameters are
frozen during the fine-tuning process. on top of that, we follow the nnunet [11]
with the selfconfiguring method to choose the pre-processing, training, and
post-processing strategy. for fair comparisons, the baseline methods including
transrate [9], logme [27], gbc [17] and leep [15] are also implemented. for
these currently available methods, we employ the output of the layer before the
final convolution as the feature map and sample it through the same sliding
window as cc-fv to obtain different classes of features, which can be used for
the calculation. figure 2 visualizes the average dice score and the estimation
value on task 03 liver. the te results are obtained from the training set only.
u-net [20] and unetr [8] are applied in the experiment and each model is
pre-trained for 250k iterations and fine-tuned for 100k iterations with batch
size 2 on a single nvidia a100 gpu. besides, we use the model at the end of
training for inference and calculate the final dsc performance on the test set.
and we use weighted kendall's τ [27] and pearson correlation coefficient for the
correlation between the te results and fine-tuning performance. the kendall's τ
ranges from [-1, 1], and τ=1 means the rank of te results and performance are
perfectly correlated(t i s→t > t j s→t if and only if p i s→t > p j s→t ). since
model selection generally picks the top models and ignores the poor performers,
we assign a higher weight to the good models in the calculation, known as
weighted kendall's τ. the pearson coefficient also ranges from [-1, 1], and
measures how well the data can be described by a linear equation. the higher the
pearson coefficient, the higher the correlation between the variables. it is
clear that the te results of our method have a more positive correlation with
respect to dsc performance.table 1 demonstrates that our method surpasses all
the other methods. most of the existing methods are inferior to ours because
they are not designed for segmentation tasks with a serious class imbalance
problem. besides, these methods rely only on single-layer features and do not
make good use of the hierarchical structure of the model. 2. correlation between
the fine-tuning performance and transferability metrics using task03 as an
example. the vertical axis represents the average dice of the model, while the
horizontal axis represents the transferability metric results. we have
standardized the various metrics uniformly, aiming to observe a positive
relationship between higher performance and higher transferability estimations.",1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,1.0,Introduction,"medical image segmentation often relies on supervised model training [14], but
this approach has limitations. firstly, it requires costly manual
annotations.secondly, the resulting models may not generalize well to unseen
data domains. even small changes in the task may result in a significant drop in
performance, requiring re-training from scratch [18].self-supervised learning
(ssl) is a promising solution to these limitations. ssl pre-trains a model
backbone to extract informative representations from unlabeled data. then, a
simple linear or non-linear head on top of the frozen pre-trained backbone can
be trained for various downstream tasks in a supervised manner (linear or
non-linear probing). alternatively, the backbone can be finetuned for a
downstream task along with the head. pre-training the backbone in a
self-supervised manner enables scaling to larger datasets across multiple data
and task domains. in medical imaging, this is particularly useful given the
growing number of available datasets.in this work, we focus on contrastive
learning [8,12], one of the most effective approaches to ssl in computer vision.
in contrastive learning, the model is trained to produce similar vector
representations for augmented views of the same image and dissimilar
representations for different images. contrastive methods can also be used to
learn dense, i.e., patch-level or even pixel-or voxellevel representations:
pixels of augmented image views from the same region of the original image
should have similar representations, while different pixels should have
dissimilar ones [23].several works have implemented contrastive learning of
dense representations in medical imaging [2,7,25,26,29]. representations in
[7,25] do not resolve nearby voxels due to the negative sampling strategy and
the architectural reasons. this makes them unsuitable for full-resolution
segmentation, especially in linear and non-linear probing regimes. in the
current sota dense ssl methods [2,26], authors employ restorative learning in
addition to patch-level contrastive learning, in order to pre-train voxel-level
representations in full-resolution. in [29], separate global and voxel-wise
representations are learned in a contrastive manner to implement efficient dense
image retrieval.the common weakness of all the above works is that they do not
evaluate their ssl models in linear or non-linear probing setups, even though
these setups are de-facto standards for evaluation of ssl methods in natural
images [8,13,23]. moreover, fine-tuned models can deviate drastically from their
pre-trained states due to catastrophical forgetting [11], while models trained
in linear or non-linear probing regimes are more robust as they have several
orders of magnitude fewer trainable parameters.our contributions are threefold.
first, we propose vox2vec, a framework for contrastive learning of voxel-level
representations. our simple negative sampling strategy and the idea of storing
voxel-level representations in a feature pyramid form result in
high-dimensional, fine-grained, multi-scale representations suitable for the
segmentation of different organs and tumors in full resolution. second, we
employ vox2vec to pre-train a fpn architecture on a diverse collection of six
unannotated datasets, totaling over 6,500 ct images of the thorax and abdomen.
we make the pre-trained model publicly available to simplify the reproduction of
our results and to encourage practitioners to utilize this model as a starting
point for the segmentation algorithms training. finally, we compare the
pretrained model with the baselines on 22 segmentation tasks on seven ct
datasets in three setups: linear probing, non-linear probing, and fine-tuning.
we show that vox2vec performs slightly better than sota models in the
fine-tuning setup and outperforms them by a huge margin in the linear and
non-linear probing setups. to the best of our knowledge, this is the first
successful attempt to evaluate dense ssl methods in the medical imaging domain
in linear and non-linear probing regimes.",1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,4.2,Evaluation,"we evaluate our method on the beyond the cranial vault abdomen (btcv) [19] and
medical segmentation decathlon (msd) [4] datasets. the btcv dataset consists of
30 ct scans along with 13 different organ annotations. we test our method on 6
ct msd datasets, which include 9 different organ and tumor segmentation tasks. a
5 fold cross-validation is used for btcv experiments, and a 3 fold
cross-validation for msd experiments. the segmentation performance of each model
on btcv and msd datasets is evaluated by the dice score.for our method, the
pre-processing steps are the same for all datasets, as at the pre-training
stage, but in addition, intensities are clipped to (-1350, 1000) hu window and
rescaled to (0, 1).we compare our results with the current state-of-the-art
self-supervised methods [2,26] in medical imaging. the pre-trained weights for
the swinunetr encoder and transvw unet are taken from the official repositories
of corresponding papers. in these experiments, we keep the crucial pipeline
hyperparameters (e.g., spacing, clipping window, patch size) the same as in the
original works. to evaluate the pre-trained swinunetr and transvw in linear and
nonlinear probing setups, we use similar linear and non-linear head
architectures as for vox2vec-fpn (see sect. 3.4). swinunetr and transvw cost 391
gflops and 1.2 tflops, correspondingly, compared to 115 gflops of vox2vec-fpn.we
train all models for 45000 batches of size 7 (batch size for swinunetr is set to
3 due to memory constraints), using the adam optimizer with a learning rate of
0.0003. in the fine-tuning setup, we freeze the backbone for the first 15000
batches and then exponentially increase the learning rate for the backbone
parameters from 0.00003 up to 0.0003 during 1200 batches.",1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,5.0,Results,"the mean value and standard deviation of dice score across 5 folds on the btcv
dataset for all models in all evaluation setups are presented in table 1.
vox2vec-fpn performs slightly better than other models in the fine-tuning setup.
however, considering the standard deviation, all the fine-tuned models perform
on par with their counterparts trained from scratch.nevertheless, vox2vec-fpn
significantly outperforms other models in linear and non-linear regimes. on top
of that, we observe that in non-linear probing regime, it performs (within the
standard deviation) as well as the fpn trained from scratch while having x50
times fewer trainable parameters (see fig. 2). we demonstrate an example of the
excellent performance of vox2vec-fpn in both linear and non-linear probing
regimes in supplementary materials.we reproduce the key results on msd challenge
ct datasets, which contain tumor and organ segmentation tasks. table 2 shows
that in the vox2vec representation space, organ voxels can be separated from
tumor voxels with a quality comparable to the model trained from scratch. a
t-sne embedding of vox2vec representations on msd is available in the
supplementary materials.",1
vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images,6.0,Conclusion,"in this work, we present vox2vec -a self-supervised framework for voxel-wise
representation learning in medical imaging. our method expands the contrastive
learning setup to the feature pyramid architecture allowing to pre-train
effective representations in full resolution. by pre-training a fpn backbone to
extract informative representations from unlabeled data, our method scales to
large datasets across multiple task domains. we pre-train a fpn architecture on
more than 6500 ct images and test it on various segmentation tasks, including
different organs and tumors segmentation in three setups: linear probing,
nonlinear probing, and fine-tuning. our model outperformed existing methods in
all regimes. moreover, vox2vec establishes a new state-of-the-art result on the
linear and non-linear probing scenarios. still, this work has a few limitations
to consider. we plan to investigate further how the performance of vox2vec
scales with the increasing size of the pre-training dataset and the pre-trained
architecture size. another interesting research direction is exploring the
effectiveness of vox2vec with regard to domain adaptation to address the
challenges of domain shift between different medical imaging datasets obtained
from different sources. a particular interest is a lowshot scenario when only a
few examples from the target domain are available.",1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,1.0,Introduction,"deep learning has brought medical image segmentation into the era of datadriven
approaches, and has made significant progress in this field [1,2], i.e., the
segmentation accuracy has improved considerably. in spite of the huge success,
the deployment of trained segmentation models is often severely impacted by a
distribution shift between the training (or labeled) and test (or unlabeled)
data since the segmentation performance will deteriorate greatly in such
situations. domain shift is typically caused by various factors, including
differences in acquisition protocols (e.g., parameters, imaging methods,
modalities) and characteristics of data (e.g., age, gender, the severity of the
disease and so on).domain adaptation (da) has been proposed and investigated to
combat distribution shift in medical image segmentation. many researchers
proposed using adversarial learning to tackle distribution shift problems
[3][4][5][6][7]. these methods mainly use the game between the domain classifier
and the feature extractor to learn domain-invariant features. however, they
easily suffer from the balance between feature alignment and discrimination
ability of the model. some recent researchers begin to explore self-training
based da algorithms, which generate pseudo labels for the 'other' domain samples
to fulfill self-training [8][9][10][11]. while it is very difficult to ensure
the quality of pseudo labels in the 'other' domain and is also hard to build
capable models with noise labels. however, most of these methods cannot well
handle the situation when the domains are very diverse, since it is very
challenging to learn domain-invariant features when each domain contains
domain-specific knowledge. also, the domain information itself is well utilized
in the da algorithms.to tackle the aforementioned issues, we propose utilizing
prompt learning to take full advantage of domain information. prompt learning
[12,13] is a recently emergent strategy to extend the same natural language
processing (nlp) model to different tasks without re-training. prompt learning
models can autonomously tune themselves for different tasks by transferring
domain knowledge introduced through prompts, and they can usually demonstrate
better generalization ability across many downstream tasks. very few works have
attempted to apply prompt learning to the computer vision field, and have
achieved promising results. [14] introduced prompt tuning as an efficient and
effective alternative to full finetuning for large-scale transformer models.
[15] exploited prompt learning to fulfill domain generalization in image
classification tasks. the prompts in these models are generated and used in the
very early stage of the models, which prevents the smooth combination with other
domain adaptation methods.in this paper, we introduce a domain prompt learning
method (prompt-da) to tackle distribution shift in multi-target domains.
different from the recent prompt learning methods, we generate domain-specific
prompts in the encoding feature space instead of the image space. as a
consequence, it can improve the quality of the domain prompts, more importantly,
we can easily consolidate the prompt learning with the other da methods, for
instance, adversarial learning based da. in addition, we propose a specially
designed fusion module to reinforce the respective characteristics of the
encoder features and domain-specific prompts, and thus generate domain-aware
features. as a way to prove the prompt-da is compatible with other das, a very
simple adversarial learning module is jointly adopted in our method to further
enhance the model's generalization ability (we denote this model as comb-da). we
evaluate our proposed method on two multi-domain datasets: 1). the infant brain
mri dataset for cross-age segmentation; 2). the brats2018 dataset for
cross-grade tumor segmentation. experiments show our proposed method outperforms
state-of-the-art methods. moreover, ablation study demonstrates the
effectiveness of the proposed domain prompt learning and the feature fusion
module. our claim about the successful combination of prompt learning with
adversarial learning is also well-supported by experiments.",1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,3.1,Datasets,"our proposed method was evaluated using two medical image segmentation da
datasets. the first dataset, i.e., cross-age infant segmentation [20], was used
for cross-age infant brain image segmentation, while the second dataset, i.e.,
brats2018 [21], was used for hgg to lgg domain adaptation.the first dataset is
for infant brain segmentation (white matter, gray matter and cerebrospinal
fluid). to build the cross-age dataset, we take advantage 10 brain mris of
6-month-old from iseg2019 [20], and also build 3-month-old and 12-month-old
in-house datasets. in this dataset, we collect 11 brain mri for both the
3-month-old and 12-month-old infants. we take the 6-month-old data as the source
domain, the 3-month-old and 12-month-old as the target domains.the 2nd dataset
is for brain tumor segmentation (enhancing tumor, peritumoral edema and necrotic
and non-enhancing tumor core), which has 285 mri samples (210 hgg and 75 lgg).
we take hgg as the source domain and lgg as the target domain.",1
Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation,3.2,Comparison with State-of-the-Art (SOTA) Method,"we compared our method with four sota methods: adda [18], cycada [22], sifa [23]
and adr [24]. we directly use the code from the corresponding papers.for fair
comparison, we have replaced the backbone of these models with the same we used
in our approach. the quantitative comparison results of cross-age infant brain
segmentation is presented in table 1, and due to space limitations, we put the
experimental results of the brain tumor segmentation task in table 1 of
supplementary material, sec.3. as observed, our method demonstrates very good da
ability on the crossage infant segmentation task, which improves about 5.46 dice
and 4.75 dice on 12-month-old and 3-month-old datasets, respectively. when
compared to the four selected sota da methods, we also show superior transfer
performance in all the target domains. specially, we outperform other sota
methods by at least 2.83 dice and 1.04 dice on the 12-month-old and 3-month-old
tasks.when transferring to a single target domain in the brain tumor
segmentation task, our proposed da solution improves about 3.09 dice in the
target lgg domain. also, the proposed method shows considerable improvements
over adda and cycada, but very subtle improvements to the sifa and adr methods
(although adr shows a small advantage on the whole category).we also visualize
the segmentation results on a typical test sample of the infant brain dataset in
fig. 2, which once again demonstrates the advantage of our method in some
detailed regions.",1
Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,3.0,Experiments and Results,"we evaluated samix on two medical image datasets. fundus [5,14] is an optic disc
and cup segmentation task. following [21], we consider images collected from
different scanners as distinct domains. the source domain contains 400 images of
the refuge [14] training set. we took 400 images from the refuge validation set
and 159 images of rim-one [5] to form the target domain 1 & 2. we center crop
and resize the disc region to 256 × 256 as network input. camelyon [1] is a
tumor tissue binary classification task across 5 hospitals. we use the training
set of camelyon as the source domain (302, 436 images from hospitals 1 -3) and
consider the validation set (34, 904 images from hospital 4) and test set (85,
054 images from the hospital 5) as the target domains 1 and 2, respectively. all
the images are resized into 256 × 256 as network input. for all experiments, the
source domain images are split into training and validation in the ratio of 4 :
1. we randomly selected k-shot target domain images for training, while the
remaining target domain images were reserved for testing.",1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,2.0,Datasets,"gallbladder cancer detection in ultrasound images: we use the public gbc us
dataset [3] consisting of 1255 image samples from 218 patients. the dataset
contains 990 non-malignant (171 patients) and 265 malignant (47 patients) gb
images (see fig. 2 for some sample images). the dataset contains image labels as
well as bounding box annotations showing the malignant regions.note that, we use
only the image labels for training. we report results on 5-fold
cross-validation. we did the cross-validation splits at the patient level, and
all images of any patient appeared either in the train or validation split.
polyp detection in colonoscopy images: we use the publicly available kvasir-seg
[17] dataset consisting of 1000 white light colonoscopy images showing polyps
(see fig. 2). since kvasir-seg does not contain any control images, we add 600
non-polyp images randomly sampled from the polypgen [1] dataset.since the
patient information is not available with the data, we use random stratified
splitting for 5-fold cross-validation.",1
Gall Bladder Cancer Detection from US Images with only Image Level Labels,3.0,Our Method,"revisiting detr: the detr [6] architectures utilize a resnet [13] backbone to
extract 2d convolutional features, which are then flattened and added with a
positional encoding, and fed to the self-attention-based transformer encoder.
the decoder uses cross-attention between learned object queries containing
positional embedding, and encoder output to produce output embedding containing
the class and localization information. the number of object queries, and the
decoder output embeddings is set to 100 in detr. subsequently, a feed-forward
network generates predictions for object bounding boxes with their corresponding
labels and confidence scores.proposed architecture: fig. 3 gives an overview of
our method. we use a coco pre-trained class-agnostic detr as proposal generator.
the learned object queries contain the embedded positional information of the
proposal. classagnostic indicates that all object categories are considered as a
single object class, as we are only interested in the object proposals. we then
finetune a regular, class-aware detr for the wsod task. this class-aware detr is
initialized with the checkpoint of the class-agnostic detr. the learned object
queries from the class-agnostic detr is frozen and shared with the wsod detr
during finetuning to ensure that the class-aware detr attends similar locations
of the object proposals. the class-agnostic detr branch is frozen during the
finetuning phase. we finally use the mil-based instance classification with the
self-supervised instance learning over the finetuning branch. for gbc
classification, if the model generates bounding boxes for the input image, then
we predict the image to be malignant, since the only object present in the data
is the cancer.mil setup: the decoder of the fine-tuning detr generates r
d-dimensional output embeddings. each embedding corresponds to a proposal
generated by the class-agnostic detr. we pass these embeddings as input to two
branches with fc layers to obtain the matrices x c ∈ r r×nc and x r ∈ r r×nc ,
where r is the number of object queries (same as proposals) and n c is the
number of object (disease) categories. let σ(•) denote the softmax operation. we
then generate the class-wise and detection-wise softmax matrices c ∈ r r×nc and
d ∈ r r×nc , where c ij = σ((x c ) t j )i and d ij = σ(x r i )j, and x i denotes
the i-th row of x. c provides classification probabilities of each proposal, and
d provides the relative score of the proposals corresponding to each class. the
two matrices are element-wise multiplied and summed over the proposal dimension
to generate the image-level classification predictions, φ ∈ r nc :notice, φ j ∈
(0, 1) since c ij and d ij are normalized. finally, the negative loglikelihood
loss between the predicted labels, and image labels y ∈ r nc is computed as the
mil loss:the mil classifier further suffers from overfitting to the distinctive
classification features due to the mismatch of classification and detection
probabilities [24].to tackle this, we further use a self-supervised module to
improve the instances.",1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,1.0,Introduction,"precision medicine efforts are shifting cancer care standards by providing novel
personalised treatment plans with promising outcomes. patient selection for such
treatment regimes is based principally on the assessment of tissue biopsies and
the characterisation of the tumor microenvironment. this is typically performed
by experienced pathologists, who closely inspect chemically stained
histopathological whole slide images (wsis). increasingly, clinical centers are
investing in the digitisation of such tissue slides to enable both automatic
processing as well as research studies to elucidate the underlying biological
processes of cancer. the resulting images are of gigapixel size, rendering their
computational analysis challenging. to deal with this issue, multiple instance
learning (mil) schemes based on weakly supervised training are used for wsi
classification tasks. in such schemes, the wsi is typically divided into a grid
of patches, with general purpose features derived from pretrained imagenet [18]
networks extracted for each patch. these representations are subsequently pooled
together using different aggregation functions and attention-based operators for
a final slide-level prediction.state space models are designed to efficiently
model long sequences, such as the sequences of patches that arise in wsi mil. in
this paper, we present the first use of state space models for wsi mil.
extensive experiments on three publicly available datasets show the potential of
such models for the processing of gigapixel-sized images, under both weakly and
multi-task schemes. moreover, comparisons with other commonly used mil schemes
highlight their robust performance, while we demonstrate empirically the
superiority of state space models in processing the longest of wsi sequences
with respect to commonly used mil methods.",1
Structured State Space Models for Multiple Instance Learning in Digital Pathology,4.1,Data,"camelyon16 [16] is a dataset that consists of resections of lymph nodes, where
each wsi is annotated with a binary label indicating the presence of tumour
tissue in the slide, and all slides containing tumors have a pixel-level
annotation indicating the metastatic region. in multitask experiments, we use
this annotation to give each patch a label indicating local tumour presence.
there are 270 wsis in the training/validation set, and 130 wsis in the
predefined test set. in our experiments, the average patch sequence length
arising from camelyon16 is 6129 (ranging from 127 to 27444).tcga-luad is a tcga
lung adenocarcinoma dataset that contains 541 wsis along with genetic
information about each patient. we obtained genetic information for this cohort
using xena browser [7]. as a mil task, we chose the task of predicting the
patient mutation status of tp53, a tumor suppressor gene that is highly relevant
in oncology studies. the average sequence length is 10557 (ranging from 85 to
34560).tcga-rcc is a tcga dataset for three kidney cancer subtypes (denoted
kich, kirc, and kirp). it consists of 936 wsis (121 kich, 518 kirc, and 297
kirp). the average sequence length is 12234 (ranging from 319 to 62235).",1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,1.0,Introduction,"longitudinal lesion or tumor tracking is a fundamental task in treatment
monitoring workflows, and for planning of re-treatments in radiation therapy.
based on longitudinal imaging for a given patient it requires establishing which
lesions are corresponding (i.e., same lesion, observed at different timepoints),
which lesions have disappeared and which are new compared to prior scanning.
this information can be leveraged to assess treatment response, e.g., by
analyzing the evolution of size and morphology for a given tumor [1], but also
for adaptation of (re-)treatment radiotherapy plans that take into account new
tumors.in practice, the development of automatic and reliable lesion tracking
solutions is hindered by the complexity of the data (over different modalities),
the absence of large, annotated datasets, and the difficulties associated with
lesion identification (i.e., varying sizes, poses, shapes, and sparsely
distributed locations). in this work, we present a multi-scale self-supervised
learning solution for lesion tracking in longitudinal studies using the
capabilities of contrastive learning [9]. inspired by the pixel-wise contrastive
learning strategy introduced in [5], we choose to learn pixel-wise feature
representations that embed consistent anatomical information from unlabeled
(i.e., without lesion-related annotations) and unpaired (i.e., without the use
of longitudinal scans) data, overcoming barriers to data collection. to increase
the system robustness and emulate the clinician's reading strategies, we propose
to use multi-scale embeddings to enable the system to progressively refine the
fine-grained location. in addition, as imaging offers contextual information
about the human body that is naturally consistent, we design the model to
benefit from biologically-meaningful points (i.e., anatomical landmarks). the
reasoning behind this strategy is that simple data augmentation methods cannot
faithfully model inter-subject variability or possible organ deformations.
hence, we ensure the spatial coherence of the tracked lesion location using
well-defined anatomical landmarks.our proposed method brings two elements of
novelty from a technical point of view: (1) the multi-scale approach for the
anatomical embedding learning and (2) a positive sampling approach that
incorporates anatomically significant landmarks across different subjects. with
these two strategies, the goal is to ensure a high degree of robustness in the
computation of the lesion matching across different lesion sizes and varying
anatomies. furthermore, a significant focus and contribution of our research is
the experimental study at a very large scale: we (1) train a pixel-wise
self-supervised system using a very large and diverse dataset of 52,487 ct
volumes and (2) evaluate on two publicly available datasets. notably, one of the
datasets, nlst, presents challenging cases with 68% of lesions being very small
(i.e., radius < 5 mm).",1
Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision,5.0,Conclusion,"in conclusion, this paper presents an effective method for longitudinal lesion
tracking based on multi-scale self-supervised learning. the method is generic,
it does not require expert annotations or longitudinal data for training and can
generalize to different types of tumors/organs/modalities. the multi-scale
approach ensures a high degree of robustness and accuracy for small lesions.
through large-scale experiments and validation on two longitudinal datasets, we
highlight the superiority of the proposed method in comparison to
state-of-theart. we found that adopting a multi-scale approach (instead of the
global/local approach as proposed in [5]) can lead to embeddings that better
capture the anatomical location and are able to handle lesions that vary in size
or appearance at different scales. moreover, the changes proposed in this work
help to alleviate the confusion caused by left-right body symmetries (e.g., the
apices of the lungs). this effect challenged the tracking of small nodules in
the lungs using [5]. our future work aims to enhance the matching accuracy by
examining the implications of correlation magnitude, conducting robustness
studies on slight variations in tracking initialization, and implementing a more
advanced fusion strategy for the multi-scale similarity maps. in addition, we
aim to expand to more applications, e.g., treatment monitoring for brain cancer
using mri.disclaimer: the concepts and information presented in this
paper/presentation are based on research results that are not commercially
available. future commercial availability cannot be guaranteed.",1
Geometry-Invariant Abnormality Detection,1.0,Introduction,"the use of machine learning for anomaly detection in medical imaging analysis
has gained a great deal of traction over previous years. most recent approaches
have focused on improvements in performance rather than flexibility, thus
limiting approaches to specific input types -little research has been carried
out to generate models unhindered by variations in data geometries. often,
research assumes certain similarities in data acquisition parameters, from image
dimensions to voxel dimensions and fields-of-view (fov). these restrictions are
then carried forward during inference [5,25]. this strong assumption can often
be complex to maintain in the real-world and although image pre-processing steps
can mitigate some of this complexity, test error often largely increases as new
data variations arise. this can include variances in scanner quality and
resolution, in addition to the fov selected during patient scans. usually
training data, especially when acquired from differing sources, undergoes
significant preprocessing such that data showcases the same fov and has the same
input dimensions, e.g. by registering data to a population atlas. whilst making
the model design simpler, these pre-processing approaches can result in poor
generalisation in addition to adding significant pre-processing times
[11,13,26]. given this, the task of generating an anomaly detection model that
works on inputs with a varying resolution, dimension and fov is a topic of
importance and the main focus of this research.unsupervised methods have become
an increasingly prominent field for automatic anomaly detection by eliminating
the necessity of acquiring accurately labelled data [4,7] therefore relaxing the
stringent data requirements of medical imaging. this approach consists of
training generative models on healthy data, and defining anomalies as deviations
from the defined model of normality during inference. until recently, the
variational autoencoder (vae) and its variants held the state-of-the-art for the
unsupervised approach. however, novel unsupervised anomaly detectors based on
autoregressive transformers coupled with vector-quantized variational
autoencoders (vq-vae) have overcome issues associated with autoencoder-only
methods [21,22]. in [22], the authors explore the advantage of tractably
maximizing the likelihood of the normal data to model the long-range
dependencies of the training data. the work in [21] takes this method a step
further through multiple samplings from the transformer to generate a
non-parametric kernel density estimation (kde) anomaly map.even though these
methods are state-of-the-art, they have stringent data requirements, such as
having a consistent geometry of the input data, e.g., in a whole-body imaging
scenario, it is not possible to crop a region of interest and feed it to the
algorithm, as this cropped region will be wrongly detected as an anomaly. this
would happen even in the case that a scan's original fov was restricted [17].as
such, we propose a geometric-invariant approach to anomaly detection, and apply
it to cancer detection in whole-body pet via an unsupervised anomaly detection
method with minimal spatial labelling. through adapting the vq-vae transformer
approach in [21], we showcase that we can train our model on data with varying
fields of view, orientations and resolutions by adding spatial conditioning in
both the vq-vae and transformer. furthermore, we show that the performance of
our model with spatial conditioning is at least equivalent to, and sometimes
better, than a model trained on whole-body data in all testing scenarios, with
the added flexibility of a ""one model fits all data"" approach. we greatly reduce
the pre-processing requirements for generating a model (as visualised in fig.
1), demonstrating the potential use cases of our model in more flexible
environments with no compromises on performance.",1
Geometry-Invariant Abnormality Detection,4.0,Results,"the proposed model was trained on the data described in sect. 3.3, with random
crops applied while training. model and anomaly detection hyperparameter tuning
was done on our validation samples using the best dice scores. we then test our
model and baselines on 4 hold-out test sets: a low-resolution whole-body set, a
low-resolution cropped set, a high-resolution rotated set and a high-resolution
test set of pet images with varying cancers. the visual results shown in fig. 4
show outputs rotated back to the original orientation. we measure our models'
performance using the dice score, obtained by thresholding the residual/density
score maps. in addition, we calculate the area under the precision-recall curve
(auprc) as a suitable measure for segmentation performance under class
imbalance. we additionally showcase the performance of the classic vq-vae +
transformer approach trained on whole-body data only (without the proposed
spatial conditioning), as well as the proposed coordconv model trained with
varying image geometries but without the transformer spatial conditioning to
explicitly showcase the added contribution of both spatial conditionings. the
full results are presented in table 1 with visual examples shown in fig. 4. we
can observe that the addition of spatial conditioning improves performance even
against the same model without conditioning trained on whole-body data (mann
whitney u test, p < 0.01 on high resolution and p < 0.001 on cropped data for
dice and auprc). for cropped data, models trained on whole-body data fail around
cropping borders, as showcased in fig. 4. this is not the case for the models
trained on varying geometries. note that the vq-vae + transformer trained on
varying geometries still shows adequate performance, highlighting the resilience
of the transformer network to varying sequence lengths without any form of
spatial conditioning. however, by adding the transformer spatial conditioning,
we see improvements across all test sets (most significantly on cropped data and
the rotated data p < 0.001) for both evaluation metrics. for the rotated data,
we see little performance degradation in the conditioned model thanks to the
spatial conditioning. the same model without conditioning showed much lower
performance with higher false positives likely due to the model's inability to
comprehend the anatomical structures present due to the rotated orientation.",1
Geometry-Invariant Abnormality Detection,5.0,Conclusion,"detection and segmentation of anomalous regions, particularly for cancer
patients, is essential for staging, treatment and intervention planning.
generally, the variation scanners and acquisition protocols can cause failures
in models trained on data from single sources. in this study, we proposed a
system for anomaly detection that is robust to variances in geometry. not only
does the proposed model showcase strong and statistically-significant
performance improvements on varying image resolutions and fov, but also on
whole-body data. through this, we demonstrate that one can improve the
adaptability and flexibility to varying data geometries while also improving
performance. such flexibility also increases the pool of potential training
data, as they dont require the same fov. we hope this work serves as a
foundation for further exploration into geometry-invariant deep-learning methods
for medical-imaging.",1
Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,3.0,Experiments,"data. the proposed approach is evaluated using the publicly available lidc-idri
dataset consisting of 1018 clinical thoracic ct scans from patients with
non-small cell lung cancer (nsclc) [2,3]. each lung nodule with a minimum size
of 3 mm was segmented and annotated with a malignancy score ranging from
1-highly unlikely to 5-highly suspicious by one to four expert raters. nodules
were also scored according to their characteristics with respect to predefined
attributes, namely subtlety (difficulty of detection, 1-extremely subtle,
5-obvious), internal structure (1-soft tissue, 4-air ), pattern of calcification
(1popcorn, 6-absent), sphericity (1-linear, 5-round ), margin (1-poorly defined,
5sharp), lobulation (1-no lobulation, 5-marked lobulation), spiculation (1-no
spiculation, 5-marked spiculation), and texture (1-non-solid, 5-solid ). the
pylidc framework [7] is used to access and process the data. the mean attribute
annotation and the mean and standard deviation of the malignancy annotations are
calculated. the latter was used to fit a gaussian distribution, which serves as
the ground truth label for optimization. samples with a mean expert malignancy
score of 3-indeterminate or annotations from fewer than three experts were
excluded in consistency with the literature [8,9,11].experiment designs. to
ensure comparability with previous work [8,9,11], the main metric used is
within-1-accuracy, where a prediction within one score is considered correct.
five-fold stratified cross-validation was performed using 10 % of the training
data for validation and the best run of three is reported.the algorithm was
implemented using the pytorch framework version 1.13 and cuda version 11.6. a
learning rate of 0.5 was chosen for the prototype vectors and 0.02 for the other
learnable parameters. the batch size was set to 128 and the optimizer was adam
[10]. with a maximum of 1000 epochs, but stopping early if there was no
improvement in target accuracy within 100 epochs, the experiments lasted an
average of three hours on a geforce rtx 3090 graphics card. the code is publicly
available at https://github.com/xrad-ulm/proto-caps.besides pure performance,
the effect of reduced availability of attribute annotations was investigated.
this was done by using attribute information only for a randomly selected
fraction of the nodules during the training.to investigate the effect of
prototypes on the network performance, an ablation study was performed. three
networks were compared: proto-caps (proposed) including learning and applying
prototypes during inference, proto-caps w/o use where prototypes are only
learned but ignored for inference, and proto-caps w/o learn using the proposed
architecture without any prototypes.",2
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI,3.0,Experiments and Results,"we carried out two evaluation settings using the brats2018 database [1],
including cross-subset (relatively small domain shift) and cross-modality
(relatively large domain shift) tasks. the brats2018 database is a continually
evolving database [1] with a total of 285 glioblastoma or low-grade gliomas
subjects, comprising three consecutive subsets, i.e., 30 subjects from brats2013
[26], 167 subjects from tcia [3], and 88 subjects from cbica [1]. notably, these
three subsets were collected from different clinical sites, vendors, or
populations [1]. each subject has t1, t1ce, t2, and flair mri volumes with
voxel-wise labels for the tumor core (coret), the enhancing tumor (enht), and
the edema (ed). we incrementally learned coret, enht, and ed structures
throughout three consecutive stages, each following different data
distributions. we used subjectindependent 7/1/2 split for training, validation,
and testing. for a fair comparison, we adopted the resnet-based 2d nnu-net
backbone with bn as in [12] for all of the methods and all stages used in this
work.",2
ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models,1.0,Introduction,"colonoscopy is a critical tool for identifying adenomatous polyps and reducing
rectal cancer mortality. deep learning methods have shown powerful abilities in
automatic colonoscopy analysis, including polyp segmentation [5,22,26,27,29] and
polyp detection [19,24]. however, the scarcity of annotated data due to high
manual annotation costs results in poorly trained and low generalizable models.
previous methods have relied on generative adversarial networks (gans) [9,25] or
data augmentation methods [3,13,28] to enhance learning features, but these
methods yielded limited improvements in downstream tasks. recently, diffusion
models [6,15] have emerged as promising solutions to this problem, demonstrating
remarkable progress in generating multiple modalities of medical data
[4,10,12,21].",2
Synthetic Augmentation with Large-Scale Unconditional Pre-training,1.0,Introduction,"the recent advancements in medical image recognition systems have greatly
benefited from deep learning techniques [15,28]. large-scale well-annotated
datasets are one of the key components for training deep learning models to
achieve satisfactory results [3,17]. however, unlike natural images in computer
vision, the number of medical images with expert annotations is often limited by
the high labeling cost and privacy concerns. to overcome this challenge, a
natural choice is to employ data augmentation to increase the number of training
samples. although conventional augmentation techniques [23] such as flipping and
cropping can be directly applied to medical images, they merely improve the
diversity of datasets, thus leading to marginal performance gains [1]. another
group of studies employ conditional generative adversarial networks (cgans) [10]
to synthesize visually appealing medical images that closely resemble those in
the original datasets [36,37]. while existing works have proven effective in
improving the performance of downstream models to some extent, a sufficient
amount of labeled data is still required to adequately train models to generate
decentquality images. more recently, diffusion models have become popular for
natural image generation due to their impressive results and training stability
[4,13,31]. a few studies have also demonstrated the potential of diffusion
models for medical image synthesis [19,24].although annotated data is typically
hard to acquire for medical images, unannotated data is often more accessible.
to mitigate the issue existed in current cgan-based synthetic augmentation
methods [8,[36][37][38], in this work, we propose to leverage the diffusion
model with unlabeled pre-training to reduce the dependency on the amount of
labeled data (see comparisons in fig. 1). we propose a novel synthetic
augmentation method, named histodiffusion, which can be pre-trained on
large-scale unannotated datasets and adapted to smallscale annotated datasets
for augmented training. specifically, we first employ a latent diffusion model
(ldm) and train it on a collection of unlabeled datasets from multiple sources.
this large-scale pre-training enables the model to learn common yet diverse
image characteristics and generate realistic medical images. second, given a
small labeled dataset that does not exist in the pre-training datasets, the
decoder of the ldm is fine-tuned using annotations to adapt to the domain shift.
synthetic images are then generated with classifier guidance [4] in the latent
space. following the prior work [36], we select generated images based on the
confidence of target labels and feature similarity to real labeled images. we
evaluate our proposed method on a histopathology image dataset of colorectal
cancer (crc). experiment results show that when presented with limited
annotations, the classifier trained with our augmentation method outperforms the
ones trained with the prior cgan-based methods. our experimental results show
that once histodiffusion is well pre-trained using large datasets, it can be
applied to any future incoming small dataset with minimal fine-tuning and may
substantially improve the flexibility and efficacy of synthetic augmentation.",2
Synthetic Augmentation with Large-Scale Unconditional Pre-training,3.0,Experiments,"datasets. we employ three public datasets of histopathology images during the
large-scale pre-training procedure. the first one is the h&e breast cancer
dataset [2], containing 312,320 patches extracted from the hematoxylin & eosin
(h&e) stained human breast cancer tissue micro-array (tma) images [18]. each
patch has a resolution of 224 × 224. the second dataset is pannuke [9], a
pan-cancer histology dataset for nuclei instance segmentation and
classification. the pannuke dataset includes 7,901 patches of 19 types of h&e
stained tissues obtained from multiple data sources, and each patch has a
unified size of 256×256 pixels. the third dataset is tcga-brca-a2/e2 [34], a
subset derived from the tcga-brca breast cancer histology dataset [20]. the
subset consists of 482,958 patches with a resolution of 256 × 256. overall,
there are 803,179 patches used for pre-training. as for fine-tuning and
evaluation, we employ the nct-crc-he-100k dataset that contains 100,000 patches
from h&e stained histological images of human colorectal cancer (crc) and normal
tissue. the patches have been divided into 9 classes: adipose (adi), background
(back), debris (deb), lymphocytes (lym), mucus (muc), smooth muscle (mus),
nor-mal colon mucosa (norm), cancer-associated stroma (str), colorectal
adenocarcinoma epithelium (tum). the resolution of each patch is 224 × 224.to
replicate a scenario where only a small annotated dataset is available for
training, we have opted to utilize a subset of 5,000 (5%) samples for
finetuning. this subset has been carefully selected through an even sampling
without replacement from each tissue type present in the train set. it is worth
noting that the labels for these samples have been kept, which allows the
fine-tuning process to be guided by labeled data, leading to better predictions
on the specific task or domain being trained. by ensuring that the fine-tuning
process is representative of the entire dataset through even sampling from each
tissue type, we can eliminate bias towards any particular tissue type. we
evaluate the fine-tuned model on the official test set. the related data use
declaration and acknowledgment can be found in our supplementary
materials.evaluation metrics. we employ fréchet inception distance (fid) score
[12] to assess the image quality of the synthetic samples. we further compute
the accuracy, f1-score, sensitivity, and specificity of the downstream
classifiers to evaluate the performance gain from different augmentation
methods.model implementation. all the patches are resized to 256 × 256 × 3
before being passed into the models. our implementation of histodiffusion
basically follows the ldm-4 [26] architecture, where the input is downsampled by
a factor of 4, resulting in a latent representation with dimensions of 64 × 64 ×
3. we use 1000 timesteps (t = 1000) for the training of diffusion model and
sample with classifier-free guidance scale g = 1.0 and 200 ddim steps. the
latent classifier φ is constructed using the encoder architecture of the lae and
an additional attention pooling layer [25] added before the output layer.we use
the same architecture for the auxiliary image classifier ϕ. for downstream
evaluation, we implement the classifier using the vit-b/16 architecture [5] in
all experiments to ensure fair comparisons. the default hyper-parameter settings
provided in their officially released codebases are followed.comparison to
state-of-the-art. we compare our proposed histodiffusion with the current
state-of-the-art cgan-based method [36]. we employ style-gan2 [14] as the
backbone generative model for cgan-based synthesis. to ensure a fair comparison,
all images synthesized by stylegan2 and histodiffusion model are further
selected based on feature centroid distances [36]. more implementation details
of our proposed histodiffusion, stylegan2, and baseline classifier can also be
found in our supplementary materials. 3, where histodiffusion consistently
generates more realistic images matching the given class conditions than
sytlegan2, especially for classes adi and back. when augmenting the training
dataset with different numbers of images synthesized from histodiffusion and
stylegan2, one can observe that when increasing the ratio of synthesized data to
100%, the fid score of stylegan2 increases quickly and can become even worse
than the one without using image selection strategy. in contrast, histodiffusion
can keep synthesizing high-quality images until the augmentation ratio reaches
300%. regarding classification performance improvement of the baseline
classifier, the accuracy and f1 score of using his-todiffusion augmentation are
increased by up to 6.4% and 6.6%, respectively. even when not using the image
selection module to filter out the low-quality results (i.e., +random 50%), our
histodiffusion can still improve the accuracy by 1.5%. the robustness and
effectiveness of histodiffusion can be attributed to the unconditional
large-scale pre-training, our specially-designed conditional fine-tuning, and
classifier-guided generation, among others.",2
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,1.0,Introduction,"a must-have ingredient for training a deep neural network (dnn) is a large
number of labelled data that is not always available in real-world applications.
this challenge of data annotation becomes even worse for medical image
segmentation tasks that require pixel-level annotation by experts. data
augmentation (da) is a recognized approach to tackle this challenge. common da
strategies create new samples by using predefined transformations such as
rotation, translation, and colour jitter to existing data, where the performance
gains heavily relies on the choice of augmentation operations and parameters
[1].to mitigate this reliance, recent efforts have focused on learning optimal
augmentation operations for a given task and dataset [3,8,11,15]. however,
transformations learned from these methods are typically still limited to a
predefined set of simple operations such as rotation, translation, and scaling.
in the meantime, another direction of research has emerged that provides an
alternative way of learning more expressive augmentations based on
deformation-based transformations commonly used in image registration [6,12,16].
instead of pre-specifying a list of operations such as rotation and scaling [3],
these deformation-based transformations can describe more general spatial
transformations. moreover, they are perfectly suited for modelling an object's
shape changes [16] that are crucial for image segmentation tasks. it thus
provides an excellent candidate for learning shape variations of an object from
the data, and via which to enable shape-based augmentations for medical image
segmentation tasks. [12,16].however, to date, all existing approaches to
learning deformable registrationbased da assume a perfect alignment of image
pairs to learn the transformations. in other words, the deformation-based
transformations are learned globally for the image. this assumption is
restrictive and associated with several challenges. first, the learning of a
global image-level transformation requires image alignment that may be
non-trivial in many scenarios, such as the alignment of tumours that can appear
at different locations of an image, or alignment of images from different
modalities. the learning of transformations itself is also complicated by the
presence of other objects in the image and is best suited when the object of
interest is always in the same (and often centre) location in all the images,
i.e., images are globally aligned a priori [16]. second, the application of the
learned global transformations for da is also restricted to images similar (and
aligned) to those in training. it thus will be challenging to transfer the
learned shape variations to even the same objects across different locations,
orientations, or sizes in the image, let alone transferring across dataset
(e.g., to transfer the learned shape variations of an organ from one image
modality to another).intuitively, object-centric transformations and
augmentations have the potential to overcome the challenges associated with
global image-level transformations. recently, an object-centric augmentation
method termed as tumorcp [13] showed that a simple object-level augmentation,
via copy-pasting a tumour from one location to another, can yield impressive
performance gains. however, the diversity of samples generated by tumorcp is
limited to pasting tumours on different backgrounds with random distortions
without further learned shapebased augmentation.similarly, other existing works
on object-level augmentation of lesions have mostly focused on position,
orientation, and random transformations of the lesion on different backgrounds
[14,17]. to date, no existing works have considered shape-based object-centric
augmentations. enriching object-centric da with learned shape variations -a
factor critical to object segmentation -can result in more diverse samples and
thereby improve dnn training for medical image segmentation.in this paper, we
present a novel approach for learning and transferring object-centric
deformations for da in medical image segmentation tasks. as illustrated in fig.
1, this is achieved with two key elements:-a generative model of object-centric
deformations -constrained to c1 diffeomorphism for better dnn training -to
describe shape variability learned from paired patches of objects of interest.
this allows the learning to focus on the shape variations of an object
regardless of its positions and sizes in the image, thus bypassing the
requirement for image alignment. -an online augmentation strategy to sample
transformations from the generative model and to augment the objects of interest
in place without distorting the surrounding content in the image. this allows us
to add shape diversity to the objects of interest in an image regardless of
their positions or sizes, eventually facilitating transferring the learned
variations across datasets. we demonstrated the effectiveness of the presented
object-centric diffeomorphic augmentation in kidney tumour segmentation,
including using shape variations of kidney tumours learned from the same dataset
(kits [7]), as well as transferring those learned from a larger liver tumour
dataset (lits [2]). experimental results showed that it can enrich the
augmentation diversity of other techniques such as tumorcp [13], and improve
kidney tumour segmentation [7] using shape variations learned either within or
outside the same training data.",2
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.0,Methods,"we focus on da for tumour segmentation because tumours can occur at different
locations of an organ with substantially different orientations and sizes. it
thus presents a challenging scenario where global image-level deformable
transformations cannot apply. mentation approach comprises as outlined in below
we describe the two key methodological elements.",2
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.1,Object-Centric Diffeomorphism as a Generative Model,"the goal of this element is to learn to generate diffeomorphic transformation
parameters θ that describe shape variations -in the form of deformable
transformations t θ -that are present within training instances of tumour x. to
realize this, we train a generative model g(.) for θ such that, when given two
instances of tumours (x src , x tgt ), it is asked to generate θ from the
encoded latent representations z in order to deform x src through t θ (x src )
to x tgt .transformations: in order to model shape deformations between x src
and x tgt , we need highly expressive transformations to capture rich shape
variations in tumour pairs. we assume a spatial transformation t θ in the form
of pixel-wise displacement field u as t θ (x) = x + u. inspired from [4,6], we
turn to c 1 diffeomorphisms to model our transformations. c 1 diffeomorphisms
are smooth and invertible transformations that preserve differentiability up to
the first derivative, making them a suitable choice to be embedded in a neural
network for gradient-based optimization [4]. however, the set of all
diffeomorphisms is an infinitely large lie group. to overcome this issue, we
focus on a specific finitedimensional subset of the lie group that is large
enough to capture the relevant variations in the tumours. for this, we make use
of continuous piecewise-affinebased (cpab) transformation based on the
integration of cpa velocity field v θ proposed in [5]. let ω ⊂ r 2 denote the
tumour domain and let p be triangular tesselation of ω [6]. a velocity field
that maps points from ω to r 2 is said to be piecewise affine (pa) if it is
affine when restricted to each triangle of p . the set v of v θ which are zero
on the boundary of ω can be shown to be finitedimensional linear space [5]. the
dimensionality d of v is a result of how fine p is tessellated. it can be shown
that v is parameterized by θ, i.e., any instance of v is a linear combination of
d orthonormal cpa fields with weights θ [5]. a spatial transformation t θ can be
derived by integrating a velocity field v θ [5] as:where the integration can be
done via a specialized solver [5]. the solver chosen produces faster and more
accurate results than a generic ode solver. specifically, the cost for this
solver is o(c1)+o(c2 x number of integration steps), where c1 is matrix
exponential for the number of cells an image is divided into and c2 is the
dimensionality of an image. the transformations t θ thus can be described by a
generative model of θ. we also experimented with squaring and scaling layers for
integration but that resulted in texture loss when learning
transformations.generative modeling: the data generation process can be
described as:where z is the latent variable assumed to follow an isotropic
gaussian prior, p φ (θ|z) is modeled by a neural network parameterized by φ, and
p(x tgt |θ, x src ) follows the deformable transformation as described in
equation ( 1).we define variational approximations of the posterior density as q
ψ (z|x src , x tgt ), modeled by a convolutional neural network that expects two
inputs x src and x tgt . passing a tuple of x src and x tgt as the input helps
the latent representations to learn the spatial difference between two tumour
samples. alternatively, the generative model as described can be considered as a
conditional model where both the generative and inference model is conditioned
on the source tumour sample x src .variational inference: the parameters ψ and φ
are optimized by the modified evidence lower bound (elbo) of the log-likelihood
log p(x tgt |x src ):where the first term in the elbo takes the form of
similarity loss: l 2 norm on the difference between x tgt and xsrc = t θ (x src
) synthesized using the θ from g(z).the second kl term constrains our
approximated posterior q ψ (z|x src , x tgt ) to be closer to the isotropic
gaussian prior p(z), and its contribution to the overall loss is scaled by the
hyperparameter β. to further ensure that xsrc looks realistic, we discourage
g(z) from generating overly-expressive transformations by adding a
regularization term over the l 2 norm of the displacement field u with a tunable
hyperparameter λ reg . the final objective function becomes:object-centric
learning: to learn object-centric spatial transformations, x src and x tgt are
in the forms of image patches that solely contain tumours. given an image and
its corresponding tumour segmentation mask (x, y ), we first extract a bounding
box around the tumour by applying skimage.measure.regionprops from the
scikit-image package to y . we then use this bounding box to carve out the
tumour x from the image x, masking out all the regions within the bounding box
that do not belong to the tumour. all the tumour patches are then resized to the
same scale, such that tumours of different sizes can be described by the same
tesselation resolution. when pairing tumour patches, we pair each tumour with
its k nearest neighbour tumours based on their euclidean distance -this again
avoids learning overly expressive transformation when attempting to deform
between significantly different tumour shapes.",2
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,2.2,Online Augmentations with Generative Models,"the goal of this element is to sample random object-centric transformations of t
θ from g(z), to generate diverse augmentations of different instances of tumours
in place. however, if we only transform the tumour and keep the rest of the
image identical, the transformed tumour may appear unrealistic and out of
place.to ensure that the entire transformed image appears smooth, we use a
hybrid strategy to construct a deformation field for the entire image x that
combines tumour-specific deformations with an identity transform for the rest of
the image. specifically, we fill a small region around the tumour with
displacements of diminishing magnitudes, achieved by propagating the
deformations from the boundaries of the deformation fields from g(z) to their
neighbours with reduced magnitudes. repeating this process ensures that the
change at the boundaries is smooth and that the transformed region appears
naturally as part of the image.",2
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,0.0,Model:,"the encoder of g(z) consisted of five convolutional layers and three fully
connected layers, with a latent dimension of 12 for z. the decoder consisted of
five fully connected layers to output the parameters θ for t θ . we trained the
g(z) for a total of 400 epochs and a batch size of 16. we also implemented early
stopping if the validation loss does not improve for 20 epochs. we used adam
optimizer [10] with a learning rate of 1e-4. we trained separate g(z)'s from
kits and lits, respectively. we set β = 0.001 for both models but needed a high
λ reg of 0.009 for the lits model compared to 0.004 for kits model. the tumours
in the lits have higher intensity differences, which may explain why a higher
value of λ reg was needed to ensure that transformed tumours did not become
unrealistic.results: we evaluated g(z) with two criteria. first, the model needs
to be able to reconstruct x tgt by generating θ to transform x src . second, the
model needs to be able to generate diverse transformed tumour samples for a
given tumour sample. figure 2 presents visual examples of the reconstruction and
generation results achieved by g(z). it can be observed that the reconstruction
is successful in most cases, except when x src and x tgt were too different.
this was necessary to ensure that t θ (x src ) did not produce unrealistic
examples. the averaged l2-loss of transformed xsrc was 1.23 on the validation
pairs. we also visually inspected validation samples after training to make sure
that the deformed tumours were similar to the original tumours in appearance.
the generated examples of tumours from a single source, as shown in fig. 2(b),
demonstrated that the generations were diverse yet realistic.",2
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,3.2,Deformation-Based da for Kidney Tumour Segmentation,"data: we then used g(z) to generate deformation-based augmentations to increase
the size and diversity of training samples for kidney tumour segmentation on
kits. to assess the effect of augmentations on different sizes of labelled data,
we considered training using 25%, 50%, 75%, and 100% of the kits training set.
we considered two da scenarios: augment with transformations learned from kits
(within-data augmentation) versus from lits (cross-data augmentation).models:
for the base segmentation network, we adopted nnu-net [9] as it contains state
of the art (sota) pipeline for medical image segmentation on most datasets. to
make the segmentation pipeline compatible with g(z), we used the 2d segmentation
module of nnu-net. for baselines, we considered 1) default augmentations such as
rotation, scaling, and random crop in nnu-net as well as 2) tumorcp, all
modified for 2d segmentation. note that our goal is not to achieve sota results
on kits, but to test the relative efficacy of the presented da strategies in
comparison with existing object-centric da methods.results: we use sørensen-dice
coefficient (dice) to measure segmentation network performance. dice measures
the overlap between prediction and ground truth. as summarized in table 1, when
combined with tumorcp, the presented augmentations were able to generate
statistically significant (paired t-test, p ≤ 0.05) improvements in all cases
compared to tumorcp alone. this demonstrated the benefit of enriching simple
copy-and-paste da with shape variations. interestingly, cross-data transferring
of the learned augmentations (from lits) outperformed the within-data
augmentation in the majority of the cases. which we believe is because of two
factors. firstly, learning of the within-data augmentations is limited to the
percentage of the training set used for segmentation. the number of objects to
learn transformations from is thus greater in crossdata augmentation settings.
secondly, the transformations present in cross-data are completely unseen in the
segmentation training network which helps in generating more diverse samples.
note that, as the transformations are learned as variations in object shapes,
they can be transferred easily across datasets surprisingly, the improvements
achieved by the presented augmentation strategy were the most prominent when the
segmentation was trained on 50% and 75% of the kits training set. this is
contrary to the expectation that da would be most beneficial when the labelled
training set is small. this may be because smaller sample sizes do not provide
sufficient initial tumor samples for shape transformations. this may also
explain why the combination of tumorcp boosted the performance of our
augmentation strategy, as the oversampling nature of tumorcp provided more
tumour samples for the presented strategy to transform to further enrich the
training set. it is also worth noting that in contrast to prior literature,
random wrapping of objects does not come close to the learned augmentations. we
speculate that while unrealistic transformations work for whole images, they may
be problematic when only augmenting specific local objects in an image.",2
Learning Transferable Object-Centric Diffeomorphic Transformations for Data Augmentation in Medical Image Segmentation,4.0,Discussion and Conclusions,"in this work, we presented a novel diffeomorphism-based object-centric
augmentation that can be learned and used to augment the objects of interest
regardless of their position and size in an image. as demonstrated by the
experimental results, this allows us to not only introduce new variations to
unfixed objects like tumours in an image but also transfer the knowledge of
shape variations across datasets. an immediate next step will be to extend the
presented approach to learn and transfer 3d transformations for 3d segmentation
tasks, and to enrich the shape-based transformation with appearance-based
transformations. in the long term, it would be interesting to explore ways to
transfer knowledge about more general forms of variations across datasets.",2
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,1.0,Introduction,"the absence of highly accurate and noninvasive diagnostics for risk-stratifying
benign vs malignant solitary pulmonary nodules (spns) leads to increased
anxiety, costs, complications, and mortality [22,26]. the use of noninvasive
methods to discriminate malignant from benign spns is a high-priority public
health initiative [8,9]. deep learning approaches have shown promise in
classifying spns from longitudinal chest computed tomography (ct) [1,5,12,21],
but approaches that only consider imaging are fundamentally limited. multimodal
models generally outperform single modality models in disease diagnosis and
prediction [24], and this is especially true in lung cancer which is heavily
contextualized through non-imaging risk factors [6,23,30]. taken together, these
findings suggest that learning across both time and multiple modalities is
important in biomedical predictive modeling, especially spn diagnosis. however,
such an approach that scales across longitudinal multimodal data from
comprehensive representations of the clinical routine has yet to be demonstrated
[24]. related work. directly learning from routinely collected electronic health
records (ehrs) is challenging because observations within and between modalities
can be sparse and irregularly sampled. previous studies overcome these
challenges by aggregating over visits and binning time series within a
bidirectional encoder representations from transformers (bert) architecture
[2,14,20,25], limiting their scope to data collected on similar time scales,
such as icu measurements, [11,29], or leveraging graph guided transformers to
handle asynchrony [33]. self-attention [31] has become the dominant technique
for learning powerful representations of ehrs with trade-offs in
interpretability and quadratic scaling with the number of visits or bins, which
can be inefficient with data spanning multiple years. in contrast, others
address the episodic nature of ehrs by converting non-imaging variables to
continuous longitudinal curves that provide the instantaneous value of
categorical variables as intensity functions [17] or continuous variables as
latent functions [16]. operating with the hypothesis that distinct disease
mechanisms manifest independently of one another in a probabilistic manner, one
can learn a transformation that disentangles latent sources, or clinical
signatures, from these longitudinal curves. clinical signatures learned in this
way are expert-interpretable and have been well-validated to reflect known
pathophysiology across many diseases [15,18]. given that several clinical risk
factors have been shown to independently contribute to lung cancer risk, these
signatures are well poised for this predictive task. despite the wealth of
studies seeking to learn comprehensive representations of routine ehrs, these
techniques have not been combined with longitudinal imaging.",2
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,3.0,Experimental Setup,"datasets. this study used an imaging-only cohort from the nlst [28] and three
multimodal cohorts from our home institution with irb approval (table 1). for
the nlst cohort (https://cdas.cancer.gov/nlst/), we identified cases who had a
biopsy-confirmed diagnosis of lung malignancy and controls who had a positive
screening result for an spn but no lung malignancy. we randomly sampled from the
control group to obtain a 4:6 case control ratio. next, ehr-pulmonary was the
unlabeled dataset used to learn clinical signatures in an unsupervised manner.
we searched all records in our ehr archives for patients who had billing codes
from a broad set of pulmonary conditions, intending to capture pulmonary
conditions beyond just malignancy. additionally, image-ehr was a labeled dataset
with paired imaging and ehrs. we searched our institution's imaging archive for
patients with three chest cts within five years. in the ehr-image cohort,
malignant cases were labeled as those with a billing code for lung malignancy
and no cancer of any type prior. importantly, this case criteria includes
metastasis from cancer in non-lung locations. benign controls were those who did
not meet this criterion. finally, image-ehr-spn was a subset of image-ehr with
the inclusion criteria that subjects had a billing code for an spn and no cancer
of any type prior to the spn. we labeled malignant cases as those with a lung
malignancy billing code occurring within three years after any scan and only
used data collected before the lung malignancy code. all data within the
five-year period were used for controls. we removed all billing codes relating
to lung malignancy. a description of the billing codes used to define spn and
lung cancer events are provided in supplementary 1.2. training and validation.
all models were pretrained with the nlst cohort after which we froze the
convolutional embedding layer. while this was the only pretraining step for
image-only models (csimage and tdimage), the multimodal models underwent another
stage of pretraining using the image-ehr cohort with subjects from image-ehr-spn
subtracted. in this stage, we randomly selected one scan and the corresponding
clinical signature expressions for each subject and each training epoch. models
were trained until the running mean over 100 global steps of the validation loss
increased by more than 0.2. for evaluation, we performed five-fold
cross-validation with image-ehr-spn, using up to three of the most recent scans
in the longitudinal models. we report the mean auc and 95% confidence interval
from 1000 bootstrapped samples, sampling with replacement from the pooled
predictions across all test folds. a two-sided wilcoxon signed-rank test was
used to test if differences in mean auc between models were
significant.reclassification analysis. we performed a reclassification analysis
of low, medium, and high-risk tiers separated by thresholds of 0.05 and 0.65,
which are the cutoffs used to guide clinical management. given a baseline
comparison, our approach reclassifies a subject correctly if it predicts a
higher risk tier than the baseline in cases, or a lower risk tier than the
baseline in controls (fig. 2).",2
Partially Supervised Multi-organ Segmentation via Affinity-Aware Consistency Learning and Cross Site Feature Alignment,3.0,Experiments and Results,"datasets and implementation details. we use five abdominal ct datasets (malbcvwc
[1], decathlon spleen [3], kits [2], decathlon liver [3] and decathlon pancreas
[3] datasets respectively) to evaluate the effectiveness of our method
[1][2][3]. the spatial resolution of all these datasets are resampled to (1 × 1
× 3)mm 3 . we randomly split each dataset into training (60%), validation (20%)
and testing (20%). we adopt 3d u-net [18] as our backbone model. the patch size
(h, w, z) is set to (160, 160, 96). the hyper-parameters α and τ are empirically
set to 0.9, and 0.8, respectively. λ a is initialized as 0.01 and linearly
decreased to 1e-3 at 20000 iterations. we use sgd optimizer to train the model
and the initial learning rate is set to 0.01. we adopt dice similarity
coefficient (dsc) as metric to evaluate the performance of different methods. 1,
the ""baseline+acl"" setting reports the results with our proposed affinity-aware
consistency learning scheme. comparing to the baseline, it brings a 1.1%
performance gain in terms of dsc. by introducing the csfa module, the
""baseline+acl+csfa"" setting can further boost the performance by 0.5% in terms
of dsc. we further study the effectiveness of the csfa module in alleviating
crosssite data discrepancy. concretely, we measure the feature distribution
discrepancy between the fld and each pld by calculating the maximum mean
discrepancy (mmd) using gaussian kernel [19], which was designed to quantify
domain discrepancy. we conduct ""full vs partial"" mmd analysis on the following
two settings: ""ours w/csfa"" and ""ours wo/csfa"", where ""ours w/csfa"" is the
proposed framework, while ""ours wo/csfa"" setting refers to removing the csfa
module from our framework. in the mmd calculation, for each dataset, we first
generate features from the penultimate layer. then we randomly select 2000
features in each class for mmd calculation. please note, for each pld, we adopt
the pseudo labels for feature selection. detailed comparison results are
illustrated in table 2. as shown, by introducing the csfa module, the feature
distribution discrepancy in terms of mmd can be effectively alleviated across
all the ""full vs partial"" dataset pairs.comparison with the state-of-the-art
(sota) methods. we compare with four sota methods, including pann [8], pipo [7],
marginal loss [9], and dodnet [5]. for fair comparison, all the sota methods
were trained/tested on our own dataset splits. we also implemented our method
taking the nnunet as the backbone to compare with marginal loss [9] and pann
[8]. we reported the dsc values for each organ across test sets from all the
datasets. for a straightforward comparison with the sota, we also recorded the
average dsc over all the organs. detailed results are illustrated in table 3. as
shown, our method achieves the best performance. specifically, our method
outperforms the secondbest method pann [8] with a 1.2% dsc gain using the same
nnunet backbone. and our method when taking 3d-unet as the backbone also
outperforms the listed sota methods. we further conduct paired t-test to compare
the difference between ours and other sota methods, the p-values are 2e-8
(pipo), 2e-5 (dodnet), 2e-4 (marginal loss), 0.037 (pann), respectively. as all
p-values are smaller than 0.05, the differences between ours and other sota
methods are statistically significant. in practice, some organs are much harder
to be well-segmented than others due to their relatively small organ sizes.
therefore, we pay more attention to the performance on those hard organs (in our
datasets, pancreas and kidneys are deemed to be more difficult due to their
relatively small sizes). from the last column of table 3, we can see that the
segmentation performance gains of our method are more pronounced on hard organs
(on average a 1.8% dsc gain). figure 2 demonstrates the qualitative
visualization results on some hard samples. as shown in this figure, our method
can generate better segmentation results than other sota methods. besides, the
reasonable performance on segmenting kidney with tumors (row 2 in fig. 2) makes
our method promising in clinical practice.",2
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,1.0,Introduction,"globally, cancer is a leading cause of death and the burden of cancer incidence
and mortality is rapidly growing [1]. in cancer diagnosis, treatment, and
management, pathologydriven information plays a pivotal role. cancer grade is,
in particular, one of the major factors that determine the treatment options and
life expectancy. however, the current pathology workflow is sub-optimal and
low-throughput since it is, by and large, manually conducted, and the large
volume of workloads can result in dysfunction or errors in cancer grading, which
have an adversarial effect on patient care and safety [2]. therefore, there is a
high demand to automate and expedite the current pathology workflow and to
improve the overall accuracy and robustness of cancer grading.recently, many
computational tools have shown to be effective in analyzing pathology images
[3]. these are mainly built based upon deep convolutional neural networks
(dcnns). for instance, [4] used dccns for prostate cancer detection and grading,
[5] classified gliomas into three different cancer grades, and [6] utilized an
ensemble of dcnns for breast cancer classification. to further improve the
efficiency and effectiveness of dcnns in pathology image analysis, advanced
methods that are tailored to pathology images have been proposed. for example,
[7] proposed to incorporate both local and global contexts through the
aggregation learning of multiple context blocks for colorectal cancer
classification; [8] extracted and utilized multi-scale patterns for cancer
grading in prostate and colorectal tissues; [9] proposed to re-formulate cancer
classification in pathology images as both categorical and ordinal
classification problems. built based upon a shared feature extractor, a
categorical classification branch, and an ordinal classification branch, it
simultaneously conducts both categorical and ordinal learning for colorectal and
prostate cancer grading; a hybrid method that combines dccns with hand-crafted
features was developed for mitosis detection in breast cancer [10]. moreover,
attention mechanisms have been utilized for an improved pathology image
analysis. for instance, [11] proposed a two-step framework for glioma sub-type
classification in the brain, which consists of a contrastive learning framework
for robust feature extractor training and a sparse-attention block for
meaningful multiple instance feature aggregation. such attention mechanisms have
been usually utilized in a multiple instance learning framework or as
self-attention for feature representations. to the best of our knowledge,
attention mechanisms have not been used for feature representations of class
centroids.in this study, we propose a centroid-aware feature recalibration
network (cafenet) for accurate and robust cancer grading in pathology images.
cafenet is built based upon three major components: 1) a feature extractor, 2) a
centroid update (cup) module, and 3) a centroid-aware feature recalibration
(cafe) module. the feature extractor is utilized to obtain the feature
representation of pathology images. cup module obtains and updates the centroids
of class labels, i.e., cancer grades. cafe module adjusts the input embedding
vectors with respect to the class centroids (i.e., training data distribution).
assuming that the classes are well separated in the feature space, the centroid
embedding vectors can serve as reference points to represent the data
distribution of the training data. this indicates that the centroid embedding
vectors can be used to recalibrate the input embedding vectors of pathology
images. during inference, we fix the centroid embedding vectors so that the
recalibrated embedding vectors do not vary much compared to the input embedding
vectors even though the data distribution substantially changes, leading to
improved stability and robustness of the feature representation. in this manner,
the feature representations of the input pathology images are re-calibrated and
stabilized for a reliable cancer classification. the experimental results
demonstrate that cafenet achieves the state-of-the-art cancer grading
performance in colorectal cancer grading datasets. the source code of cafenet is
available at https://github.com/col in19950703/cafenet.",2
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,2.0,Methodology,"the overview of the proposed cafenet is illustrated in fig. 1. cafenet employs a
deep convolutional neural network as a feature extractor and an attention
mechanism to produce robust feature representations of pathology images and
conducts cancer grading with high accuracy. algorithm 1 depicts the detailed
algorithm of cafenet.",2
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,2.1,Centroid-Aware Feature Recalibration,"let {x i , y i } n i=1 be a set of pairs of pathology images and ground truth
labels where n is the number of pathology image-ground truth label pairs, x i ∈
r h×w×c is the i th pathology image, y i ∈ {c 1 , . . . , c m } represents the
corresponding ground truth label. h, w, and c denote the height, width, and the
number of channels, respectively. m is the cardinality of the class labels.
given x i , a deep neural network f maps x i into an embedding space, producing
an embedding vector e i ∈ r d . the embedding vector e i is fed into 1) a
centroid update (cup) module and 2) a centroid-aware feature recalibration
(cafe) module. cup module obtains and updates the centroid of the class label in
the embedding space e c ∈ r m ×d . cafe module adjusts the embedding vector e i
in regard to the embedding vectors of the class centroids and produces a
recalibrated embedding vector e r i . e i and e r i are concatenated together
and is fed into a classification layer to conduct cancer grading. from the
centroid embedding vectors e c by using a linear layer. then, attention scores
are computed via a dot product between q e and k c followed by a softmax
operation. multiplying the attention scores by v c , we obtain the recalibrated
feature representation e r for the input embedding vectors e. the process can be
formulated as follows:finally, cafe concatenates e and e r and produces them as
the output.",2
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.1,Datasets,"two publicly available colorectal cancer datasets [9] were employed to evaluate
the effectiveness of the proposed cafenet.",2
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.2,Comparative Experiments,"we conducted a series of comparative experiments to evaluate the effectiveness
of cafenet for cancer grading, in comparison to several existing methods: 1)
three dcnnbased models: resnet [13], densenet [14], efficientnet [12], 2) two
metric learningbased models: triplet loss (triplet) [15] and supervised
contrastive loss (sc) [16], 3) two transformer-based models: vision transformer
(vit) [17] and swin transformer (swin) [18], and 4) one (pathology)
domain-specific model (m mae-ce o ) [9], which demonstrates the state-of-the-art
performance on the two colorectal cancer datasets under consideration. for
triplet and sc, efficientnet was used as a backbone network. we trained cafenet
and other competing networks on c train and selected the best model usingc
validation . then, the chosen model of each network was separately applied to c
testi andc testii . the results of m mae-ce o were obtained from the original
literature.",2
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,3.4,Result and Discussions,"we evaluated the performance of colorectal cancer grading by the proposed
cafenet and other competing models using five evaluation metrics, including
accuracy (acc), precision, recall, f1-score (f1), and quadratic weighted kappa
(κ w ). table 2 demonstrates the quantitative experimental results on c testi .
the results show that cafenet was one of the best performing models along with
resnet, swin, and m mae-ce o .were the best performing models. among dcnn-based
models, resnet was superior to other dcnn-based models. metric learning was able
to improve the classification performance. effcientnet was the worst model among
them, but with the help of triplet loss (triplet) or supervised contrastive loss
(sc), the overall performance increased by ≥2.8% acc, ≥0.023 precision, ≥0.001
recall, ≥0.010 f1, and ≥0.047 κ w . among the transformer-based models, swin was
one of the best performing models, but vit showed much lower performance in all
evaluation metrics. moreover, we applied the same models to c testii to test the
generalizability of the models. we note that c testi originated from the same
set with c train and c validation and c testii was obtained from different time
periods and using a different slide scanner. table 3 depicts the quantitative
classification results on c testii . cafenet outperformed other competing models
in all evaluation metrics except triplet for recall. in a headto-head comparison
of the classification results between c testi and c testii , there was a
consistent performance drop in the proposed cafenet and other competing models.
this is ascribable to the difference between the test datasets (c testi and c
testii ) and the training and validation datasets (c train and c validation ).
in regard to such differences, it is striking that the proposed cafenet achieved
the best performance on c testii . cafenet, resnet, swin, and m mae-ce o were
the four best performing models on c testi . however, resnet, swin, and m mae-ce
o showed a higher performance drop in all evaluation metrics. cafenet had a
minimal performance drop except efficientnet. efficientnet, however, obtained
poorer performance on both c testi and c testii . these results suggest that
cafenet has the better generalizability so as to well adapt to unseen
histopathology image data.we conducted ablation experiments to investigate the
effect of the cafe module on cancer classification. the results are presented in
table 4. the exclusion of the cafe module, i.e., efficientnet, resulted in much
worse performance than cafenet. using only the recalibrated embedding vectors e
r , a substantial drop in performance was observed. these two results indicate
that the recalibrated embedding vectors complement to the input embedding
vectors e. moreover, we examined the effect of the method that merges the two
embedding vectors. using addition, instead of concatenation, there was a
consistent performance drop, indicating that concatenation is the superior
approach for combining the two embedding vectors together. in addition, we
compared the model complexity of the proposed cafenet and other competing
models. table 5 demonstrates the number of parameters, floating point operations
per second (flops), and training and inference time (in milliseconds). the
proposed cafenet was one of the models that require a relatively small number of
parameters and flops and a short amount of time during training and inference.
densenet, efficientnet, triplet, sc, and m mae-ce o contain the smaller number
of parameters than that of cafenet, but these models show either the higher
number of flops or longer time during training and/or inference. similar
observations were made for resnet, vit, and swin. these models require much
larger number of parameters and flops and longer training time. these results
confirm that the proposed cafenet is computational efficient and it does not
achieve its superior learning capability and generalizability at the expense of
the model complexity.",2
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,4.0,Conclusions,"herein, we propose an attention mechanism-based deep neural network, called
cafenet, for cancer classification in pathology images. the proposed approach
proposes to improve the feature representation of deep neural networks by
re-calibrating input embedding vectors via an attention mechanism in regard to
the centroids of cancer grades. in the experiments on colorectal cancer datasets
against several competing models, the proposed network demonstrated that it has
a better learning capability as well as a generalizability in classifying
pathology images into different cancer grades. however, the experiments were
only conducted on two public colorectal cancer datasets from a single institute.
additional experiments need to be conducted to further verify the findings of
our study. therefore, future work will focus on validating the effectiveness of
the proposed network for other types of cancers and tissues in pathology images.",2
Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,0.0,Table 1,"shows the details of the datasets. both datasets provide colorectal pathology
images with ground truth labels for cancer grading. the ground labels are benign
(bn), well-differentiated (wd) cancer, moderatelydifferentiated (md) cancer, and
poorly-differentiated (pd) cancer.",2
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,3.1,Dynamic Contrast-Enhanced Liver CT,"our xai technique was applied to explain the mtann model's decision in a liver
tumor segmentation task [20]. dynamic contrast-enhanced liver ct scans
consisting of 42 patients with 194 liver tumors in the portal venous phase from
the lits database [21] were used in this study. each slice of the ct volumes in
the dataset has a matrix size of 512 × 512 pixels, with in-plane pixel sizes of
0.60-1.00 mm and thicknesses of 0.20-0.70 mm. the dataset consists of the
original hepatic ct image with the liver mask and the ""gold-standard"" liver
tumor region manually segmented by a radiologist, as illustrated in fig. 2.
firstly, to have the same physical scale on spatial coordinates, bicubic
interpolation was applied on the original hepatic ct images together with the
corresponding liver mask and ""gold-standard"" tumor segmentation to obtain
isotropic images with a voxel size of 0.60 × 0.60 × 0.60 mm 3 . then, to unify
the image size into the same size, the isotropic image was cropped to obtain the
liver region volume of interest (voi) with an in-plane matrix size of 512 × 512.
an anisotropic diffusion filter was applied to reduce the quantum noise, which
could substantially reduce the noise while major structures such as tumors and
vessels maintained [22]. finally, a z-score normalization was applied to unify
complex histograms of tumors in different cases. the final pre-processed ct
images were used as the input images.in addition, since most liver tumors' shape
is ellipsoidal, the liver tumors can also be enhanced by the hessian-based
method and utilized in the model to improve the performance [23,24]. hence, the
model consisted of these two input channels: segmented liver ct image and its
hessian-enhanced image. also, the patches were extracted from input images from
both channels: a 5 × 5 × 5 sized patch in the same spatial position was
extracted to form a training patch with a size of 2 × 5 × 5 × 5 pixels.seven
cases and 24 cases in the dynamic contrast-enhanced ct scans dataset were used
for training and testing, respectively. 10,000 patches were randomly selected
from the liver mask region in each case, summing up to a total of 70,000
training samples for training. the number of input units in the mtann model with
one hidden layer was 250. the structure optimization process started with 80
hidden units in the hidden layer. the binary cross-entropy (bce) loss function
was used to train the model. the mtann model classified the input patches into
tumor or non-tumor classes, and the output pixels represented the probability of
being a tumor class. during the structure optimization process, the f1 score on
the training patches and the dice coefficient on the training images were also
calculated as the reference to select a suitable compact model that performed
equivalently to the original large model.as observed in the four evaluation
metric curves in fig. 3, as the number of hidden units was reduced from 80 to 9,
the performance of the model fluctuated up and down, and after it was reduced
below 9, the performance of the model dropped dramatically. therefore, we chose
a number of hidden units of 9 as the optimized structure.then, we applied the
unsupervised hierarchical clustering algorithm to the weighted function maps
from the optimized compact model with 9 hidden units. figure 4 shows that the 9
hidden units are clearly divided into 3 different groups. we denote hidden units
3, 4, and 7 as group a, hidden units 2, 6, 1, and 8 as group b, and hidden units
0 and 5 as group c. the hidden units in the same group should have a similar
function, and the function maps from each group should show the function of the
group. as illustrated in fig. 5, the low-intensity areas in the function maps of
hidden units 0 and 5 in group c match the high-intensity areas in the
hessian-enhanced input image, which means they suppress the high-intensity
areas. likewise, group a enhances the liver area, and group b suppresses the
non-tumor area. we also understood that groups a and b worked together to
enhance the tumor area, and group c suppressed the liver's boundary as well as
reduced the false enhancements inside the liver. thus, our xai method was able
to reveal the learned functions of groups of neurons in the neural network,
which we call ""functional explanations"" and define as the explanations of the
model behavior by a combination of functions. our method is a post-hoc method
that offers both instance-based and model-based functional explanations.",2
Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,4.0,Conclusion,"in this study, we proposed a novel xai approach to explain the functions and
behavior of an mtann model for semantic segmentation of liver tumors in ct. our
structure optimization algorithm refined the structure and made every hidden
unit in the model have a clear, meaningful function by removing redundant hidden
units and ""condensing"" the functions into fewer hidden units, which solved the
issue of unstable xai results with conventional xai methods. the unsupervised
hierarchical clustering algorithm in our xai approach grouped the hidden units
with a similar function into one group so as to explain their functions by
group. through the experiments, we successfully proved that the mtann model was
explainable by functions.",2
FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling,1.0,Introduction,"vision transformers (vits) are self-attention based neural networks that have
achieved state-of-the-art performance on various medical imaging tasks
[8,24,30]. since vits are capable of encoding long range dependencies between
input f. almalik and n. alkhunaizi-equal contribution sequences [16], they are
more robust against distribution shifts and are wellsuited for handling
heterogeneous distributions [5]. however, training vit models typically requires
significantly more data than traditional convolutional neural network (cnn)
models [16], which limits their application in domains such as healthcare, where
data scarcity is a challenge. one way to overcome this challenge is to train
such models in a collaborative and distributed manner, where large amounts of
data can be leveraged from different sites without the need for sharing private
data [9,11]. federated learning and split learning are two well-known approaches
for collaborative model training.federated learning (fl) enables clients to
collaboratively learn a global model by aggregating locally trained models [14].
since this can be accomplished without sharing raw data, fl mitigates risks
related to private data leakage. several aggregation rules such as fedavg [20]
and fedprox [19] have been proposed for fl. however, it has been demonstrated
that most fl algorithms are vulnerable to gradient inversion attacks [13], which
dilute their privacy guarantees. in contrast, split learning (sl) divides a deep
neural network into components with independently accessible parameters [10].
since no participant in sl can access the complete model parameters, it has been
claimed that sl offers better data confidentiality compared to fl. in
particular, the u-shaped sl configuration, where each client has its own feature
extraction head and a task-specific tail [27] can further improve client
privacy, as it circumvents the need to share the data or labels. recently, sl
frameworks have been proposed for various medical applications such as tumor
classification [3] and chext x-ray classification [23].recent studies [21,22]
have demonstrated that both fl and sl can be combined to effectively train vits.
in [22], a framework called festa was proposed for medical image classification.
the festa framework involves a hybrid vit architecture with u-shaped sl
configuration -each client has its own cnn head and a multilayer perceptron
(mlp) tail, while the shared vit body resides on a central server. this
architecture can be trained using both sl and fl in a potentially task-agnostic
fashion, leading to better performance compared to other distributed learning
methods. the work in [21] focuses on privacy and incorporates differential
privacy with mixed masked patches sent from the vit on the server to the clients
to prevent any potential data leakage.in this work, we build upon the festa
framework [22] for collaborative learning of vit. despite its success, festa
requires pretraining the vit body on a large dataset prior to its utilization in
the sl and fl training process. in the absence of pretraining, limited training
data availability (a common problem in medical imaging) leads to severe
overfitting and poor generalization. furthermore, the festa framework exploits
only the final cls token produced by the vit body and ignores all the other
intermediate features of the vit. it is well-known that intermediate features
(referred to as patch tokens) also contain discriminative information that could
be useful for the classification task [4].to overcome the above limitations, we
propose a framework called federated split learning of vision transformer with
block sampling (fesvibs). our primary novelty is the introduction of a block
sampling module, which randomly selects an intermediate transformer block for
each client in each training round, extracts intermediate features, and distills
these features into a pseudo cls token using a shared projection network. the
proposed approach has two key benefits: (i) it effectively leverages
intermediate vit features, which are completely ignored in festa, and (ii)
sampling these intermediate features from different blocks, rather than relying
solely on an individual block's features or the final cls token, serves as a
feature augmentation strategy for the network, enhancing its generalization. the
contributions of this work can be summarized as follows: i. we propose the
fesvibs framework, a novel federated and split learning framework that leverages
the features learned by intermediate vit blocks to enhance the performance of
the collaborative system. ii. we introduce block sampling at the server level,
which acts as a feature augmentation strategy for better generalization.",2
TransLiver: A Hybrid Transformer Model for Multi-phase Liver Lesion Classification,1.0,Introduction,"liver cancer is one of the most deadly cancers and has the second highest
fatality rate [17]. focal liver lesions (flls) are the most common lesions found
in liver cancer, yet flls are challenging to diagnose because they can be either
benign lesions, such as focal nodular hyperplasia (fnh), hepatic abscess (ha),
hepatic hemangioma (hh), and hepatic cyst (hc) or malignant tumors, such as
intrahepatic cholangiocarcinoma (icc), hepatic metastases (hm), and
hepatocellular carcinoma (hcc). accurate early diagnosis of flls is thus
critical to increasing the 5-year survival rate, a task that remains challenging
as of today. dynamic contrast-enhanced ct is a common technique for liver cancer
diagnosis, where four different phases of imaging, namely, non-contrast (nc),
arterial (art), portal venous (pv), and delayed (dl) provide complementary
information about the liver. different types of flls acquired in the four phases
are shown in fig. 1. with the development of deep learning, computer-aided liver
lesion diagnosis has attracted much attention [5,8,16] in recent years. romero
et al. [16] presented an end-to-end framework based on inception-v3 and
inceptionresnet-v2 to discriminate liver lesions between cysts and malignant
tumors. heker et al. [8] combined liver segmentation and classification using
transfer learning and joint learning to increase the performance of cnn. as a
manner to elevate the accuracy of cnns, frid-adar et al. [5] designed a
gan-based network to generate synthetic liver lesion images, improving the
classification performance based on cnn. it is reported in many studies [9,18]
that using multi-phase data, like most professionals do in practice, can help
the network get a more accurate result, which also acts in liver lesion
classification [15,23,24]. yasaka et al. [24] proposed multi-channel cnn to
extract features from multi-phase liver ct by concatenation. roboh et al. [15]
proposed an algorithm based on cnns to handle 3d context in liver cts and
utilized clinical context to assist the classification. xu et al. [23]
constructed a knowledge-guided framework to integrate liver lesion features from
three phases using self-attention and fused them with a cross-feature
interaction module and a cross-lesion correlation module.a single-phase lesion
annotation means the annotation of both lesion position and its class. in
hospitals, collected multi-phase cts are normally grouped by patients rather
than lesions, which makes single-phase lesion annotation insufficient for
feature fusion learning. however, the number of lesions inside a single patient
can vary from one to dozens and they can be of different types in realistic
cases. multi-phase cts are also not co-registered in most cases, therefore, it
is necessary to make sure the lesions extracted from different phases are
somehow aligned for feature fusion, which is called as multi-phase lesion
annotation. moreover, while most works have attached much importance to liver
lesion segmentation [2], its outcome is usually organized at a single-phase
level. additional effort will be needed when consolidating segmentation and
multi-phase classification.self-attention based transformers [19] have shown
strong capability in natural language processing tasks. meanwhile, vision
transformers (vit) [4] have been shown to replace cnn with a transformer encoder
in computer vision tasks and can achieve obvious advantages on large-scale
datasets. to the best of our knowledge, we find no study using vit backbone
network in liver lesion classification. the reason for this is twofold. first,
pure vit has several limitations itself [6], including ignoring local
information within each patch, extracting only single-scale features, and
lacking inductive bias. second, no complete open liver lesion classification
datasets exist. most relevant studies are based on private datasets, which tend
to be small in size and cause overfitting in learning models.in this paper, we
construct a hybrid framework with vit backbone for liver lesion classification,
transliver. we design a pre-processing unit to reduce the annotation cost, where
we obtain lesion area on multi-phase cts from annotations marked on a single
phase. to alleviate the limitations of pure transformers, we propose a
multi-stage pyramid structure and add convolutional layers to the original
transformer encoder. we use additional cross phase tokens at the last stage to
complete a multi-phase fusion, which can focus on cross-phase communication and
improve the fusion effectiveness as compared with conventional modes. while most
multi-phase liver lesion classification studies use datasets with no more than
three phases (without dl phase for its difficulty of collection) or no more than
six lesion classes, we validate the whole framework on an in-house dataset with
four phases of abdominal ct and seven classes of liver lesions. considering the
disproportion of axial lesion slice number and the relatively small scale of the
dataset, we adopt a 2-d network in classification part instead of 3-d in
pre-processing part and achieve a 90.9% accuracy.",2
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,1.0,Introduction,"semantic segmentation on histological whole slide images (wsis) allows precise
detection of tumor boundaries, thereby facilitating the assessment of metastases
[3] and other related analytical procedures [17]. however, pixel-level
annotations of gigapixel-sized wsis (e.g. 100, 000 × 100, 000 pixels) for
training a segmentation model are difficult to acquire. for instance, in the
camelyon16 breast cancer metastases dataset [10], 49.5% of wsis contain
metastases that are smaller than 1% of the tissue, requiring a high level of
expertise and long inspection time to ensure exhaustive tumor localization;
whereas other wsis have large tumor lesions and require a substantial amount of
annotation time for boundary delineation [18]. identifying potentially
informative image regions (i.e., providing useful information for model
training) allows requesting the minimum amount of annotations for model
optimization, and a decrease in annotated area reduces both localization and
delineation workloads. the challenge is to effectively select annotation regions
in order to achieve full annotation performance with the least annotated area,
resulting in high sampling efficiency.we use region-based active learning (al)
[13] to progressively identify annotation regions, based on iteratively updated
segmentation models. each region selection process consists of two steps. first,
the prediction of the most recently trained segmentation model is converted to a
priority map that reflects informativeness of each pixel. existing studies on
wsis made extensive use of informativeness measures that quantify model
uncertainty (e.g., least confidence [8], maximum entropy [5] and highest
disagreement between a set of models [19]). the enhancement of priority maps,
such as highlighting easy-to-label pixels [13], edge pixels [6] or pixels with a
low estimated segmentation quality [2], is also a popular area of research.
second, on the priority map, regions are selected according to a region
selection method. prior works have rarely looked into region selection methods;
the majority followed the standard approach [13] where a sliding window divides
the priority map into fixed-sized square regions, the selection priority of each
region is calculated as the cumulative informativeness of its constituent
pixels, and a number of regions with the highest priorities are then selected.
in some other works, only non-overlapping or sparsely overlapped regions were
considered to be candidates [8,19]. following that, some works used additional
criteria to filter the selected regions, such as finding a representative subset
[5,19]. all of these works selected square regions of a manually predefined
size, disregarding the actual shape and size of informative areas.this work
focuses on region selection methods, a topic that has been largely neglected in
literature until now, but which we show to have a great impact on al sampling
efficiency (i.e., the annotated area required to reach the full annotation
performance). we discover that the sampling efficiency of the aforementioned
standard method decreases as the al step size (i.e., the annotated area at each
al cycle, determined by the multiplication of the region size and the number of
selected regions per wsi) increases. to avoid extensive al step size tuning, we
propose an adaptive region selection method with reduced reliance on this al
hyperparameter. specifically, our method dynamically determines an annotation
region by first identifying an informative area with connected component
detection and then detecting its bounding box. we test our method using a breast
cancer metastases segmentation task on the public camelyon16 dataset and
demonstrate that determining the selected regions individually provides greater
flexibility and efficiency than selecting regions with a uniform predefined
shape and size, given the variability in histological tissue structures. results
show that our method consistently outperforms the standard method by providing a
higher sampling efficiency, while also being more robust to al step size
choices. additionally, our method is especially beneficial for settings where a
large al step size is desirable due to annotator availability or computational
restrictions.",2
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,2.3,WSI Semantic Segmentation Framework,"this section describes the breast cancer metastases segmentation task we use for
evaluating the al region selection methods. the task is performed with
patch-wise classification, where the wsi is partitioned into patches, each patch
is classified as to whether it contains metastases, and the results are
assembled.training. the patch classification model h(x, w) : r d×d -→ [0, 1]
takes as input a patch x and outputs the probability p(y = 1|x, w) of containing
metastases, where w denotes model parameters. patches are extracted from the
annotated regions at 40× magnification (0.25 µm px ) with d = 256 pixels.
following [11], a patch is labeled as positive if the center 128 × 128 pixels
area contains at least one metastasis pixel and negative otherwise. in each
training epoch, 20 patches per wsi are extracted at random positions within the
annotated area; for wsis containing annotated metastases, positive and negative
patches are extracted with equal probability. a patch with less than 1% tissue
content is discarded. data augmentation includes random flip, random rotation,
and stain augmentation [12]. inference. x i is divided into a grid of uniformly
spaced patches (40× magnification, d = 256 pixels) with a stride s. the patches
are predicted using the trained patch classification model and the results are
stitched to a probability map p i ∈ [0, 1] w i ×h i , where each pixel
represents a patch prediction. the patch extraction stride s determines the size
of p i (w i = wi s , h i = hi s ).",2
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,3.2,Implementation Details,"training schedules. we use mobilenet v2 [15] initialized with imagenet [14]
weights as the backbone of the patch classification model. it is extended with
two fully-connected layers with sizes of 512 and 2, followed by a softmax
activation layer. the model is trained for up to 500 epochs using cross-entropy
loss and the adam optimizer [7], and is stopped early if the validation loss
stagnates for 100 consecutive epochs. model selection is guided by the lowest
validation loss. the learning rate is scheduled by the one cycle policy [16]
with a maximum of 0.0005. the batch size is 32. we used fastai v1 [4] for model
training and testing. the running time of one al cycle (select-train-test) on a
single nvidia geforce rtx3080 gpu (10gb) is around 7 h.active learning setups.
since the camelyon16 dataset is fully annotated, we perform al by assuming all
wsis are unannotated and revealing the annotation of a region only after it is
selected during the al procedure. we divide the wsis in u randomly into five
stratified subsets of equal size and use them sequentially. in particular,
regions are selected from wsis in the first subset at the first al cycle, from
wsis in the second subset at the second al cycle, and so on. this is done
because wsi inference is computationally expensive due to the large patch
amount, reducing the number of predicted wsis to one fifth helps to speed up al
cycles. we use an informativeness measure that prioritizes pixels with a
predicted probability close to 0.5 (i.e., m i = 1-2|p i -0.5|), following [9].
we annotate validation wsis in the same way as the training wsis via
al.evaluations. we use the camelyon16 challenge metric free response operating
characteristic (froc) score [1] to validate the segmentation framework.to
evaluate the wsi segmentation performance directly, we use mean intersection
over union (miou). for comparison, we follow [3] to use a threshold of 0.5 to
generate the binary segmentation map and report miou (tumor), which is the
average miou over the 48 test wsis with metastases. we evaluate the model
trained at each al cycle to track performance change across the al procedure.",2
Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation,3.3,Results,"full annotation performance. to validate our segmentation framework, we first
train on the fully-annotated data (average performance of five repetitions
reported). with a patch extraction stride s = 256 pixels, our framework yields
an froc score of 0.760 that is equivalent to the challenge top 2, and an miou
(tumor) of 0.749, which is higher than the most comparable method in [3] that
achieved 0.741 with s = 128 pixels. with our framework, reducing s to 128 pixels
improves both metastases identification and segmentation (froc score: 0.779,
miou (tumor): 0.758). however, halving s results in a 4-fold increase in
inference time. this makes an al experiment, which involves multiple rounds of
wsi inference, extremely costly. therefore, we use s = 256 pixels for all
following al experiments to compromise between performance and computation
costs. because wsis without metastases do not require pixel-level annotation, we
exclude the 159 training and validation wsis without metastases from all
following al experiments. this reduction leads to a slight decrease of full
annotation performance (miou (tumor) from 0.749 to 0.722). all experiments
(except for random) use uncertainty sampling. when using region selection method
standard, the sampling efficiency advantage of uncertainty sampling over random
sampling decreases as al step size increases. a small al step size minimizes the
annotated tissue area for a certain high level of model performance, such as an
miou (tumor) of 0.7, yet requires a large number of al cycles to achieve full
annotation performance (fig. 4 (a-d)), table 1. annotated tissue area (%)
required to achieve full annotation performance. the symbol ""/"" indicates that
the full annotation performance is not achieved in the corresponding
experimental setting in fig. 4. resulting in high computation costs. a large al
step size allows for full annotation performance to be achieved in a small
number of al cycles, but at the expense of rapidly expanding the annotated
tissue area (fig. 4(e), (f), (h) and (i)). enabling selected regions to have
variable aspect ratios does not substantially improve the sampling efficiency,
with standard (non-square) outperforming standard only when the al step size is
excessively large (fig. 4(i)). however, allowing regions to be of variable size
consistently improves sampling efficiency. table 1 shows that adaptive achieves
full annotation performance with fewer al cycles than standard for small al step
sizes and less annotated tissue area for large al step sizes. as a result, when
region selection method adaptive is used, uncertainty sampling consistently
outperforms random sampling. furthermore, fig. 4(e-i)) shows that adaptive
effectively prevents the rapid expansion of annotated tissue area as al step
size increases, demonstrating greater robustness to al step size choices than
standard. this is advantageous because extensive al step size tuning to balance
the annotation and computation costs can be avoided. this behavior can also be
desirable in cases where frequent interaction with annotators is not possible or
to reduce computation costs, because the proposed method is more tolerant to a
large al step size. we note in fig. 4(h) that the full annotation performance is
not achieved with adaptive within 15 al cycles; in fig. s1 in the supplementary
materials we show that allowing for oversampling of previously selected regions
can be a solution to this problem. additionally, we visualize examples of
selected regions in fig. 5 and show that adaptive avoids two region selection
issues of standard : small, isolated informative areas are missed, and
irrelevant pixels are selected due to the region shape and size restrictions.",2
COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,1.0,Introduction,"segmentation is among the most common medical image analysis tasks and is
critical to a wide variety of clinical applications. to date, data-driven deep
learning (dl) methods have shown prominent segmentation performance when trained
on fully-annotated datasets [8]. however, data annotation is a significant
bottleneck for dataset creation. first, annotation process is tedious, laborious
and time-consuming, especially for 3d medical images where dense annotation with
voxel-level accuracy is required. second, medical images typically need to be
annotated by medical experts whose time is limited and expensive, making the
annotations even more difficult and costly to obtain. active learning (al) is a
promising solution to improve annotation efficiency by iteratively selecting the
most important data to annotate with the goal of reducing the total number of
annotated samples required. however, most deep al methods require an initial set
of labeled samples to start the active selection. when the entire data pool is
unlabeled, which samples should one select as the initial set? this problem is
known as cold-start active learning , a low-budget paradigm of al that permits
only one chance to request annotations from experts without access to any
previously annotated data.cold-start al is highly relevant to many practical
scenarios. first, cold-start al aims to study the general question of
constructing a training set for an organ that has not been labeled in public
datasets. this is a very common scenario (whenever a dataset is collected for a
new application), especially when iterative al is not an option. second, even if
iterative al is possible, a better initial set has been found to lead to
noticeable improvement for the subsequent al cycles [4,25]. third, in low-budget
scenarios, cold-start al can achieve one-shot selection of the most informative
data without several cycles of annotation. this can lead to an appealing 'less
is more' outcome by optimizing the available budget and also alleviating the
issue of having human experts on standby for traditional iterative al.despite
its importance, very little effort has been made to address the coldstart
problem, especially in medical imaging settings. the existing cold-start al
techniques are mainly based on the two principles of the traditional al
strategies: (1) uncertainty sampling [5,11,15,18], where the most uncertain
samples are selected to maximize the added value of the new annotations. (2)
diversity sampling [7,10,19,22], where samples from diverse regions of the data
distribution are selected to avoid redundancy. in the medical domain,
diversity-based cold-start strategies have been recently explored on 2d
classification/segmentation tasks [4,24,25]. the effectiveness of these
approaches on 3d medical image segmentation remains unknown, especially since 3d
models are often patch-based while 2d models can use the entire image. a recent
study on 3d medical segmentation shows the feasibility to use the uncertainty
estimated from a proxy task to rank the importance of the unlabeled data in the
cold-start scenario [14]. however, it fails to compare against the
diversity-based approaches, and the proposed proxy task is only limited to ct
images, making the effectiveness of this strategy unclear on other 3d imaging
modalities. consequently, no comprehensive cold-start al baselines currently
exist for 3d medical image segmentation, creating additional challenges for this
promising research direction.in this paper, we introduce the colossal benchmark,
the first cold-start active learning benchmark for 3d medical image segmentation
by evaluating on six popular cold-start al strategies. specifically, we aim to
answer three important open questions: (1) compared to random selection, how
effective are the uncertainty-based and diversity-based cold-start strategies
for 3d segmentation tasks? (2) what is the impact of allowing a larger budget on
the compared strategies? (3) can these strategies work better if the local roi
of the target organ is known as prior? we train and validate our models on five
3d medical image segmentation tasks from the publicly available medical
segmentation decathlon (msd) dataset [1], which covers two of the most common 3d
image modalities and the segmentation tasks for both healthy tissue and
tumor/pathology.our contributions are summarized as follows:• we offer the first
cold-start al benchmark for 3d medical image segmentation. we make our code
repository, data partitions, and baseline results publicly available to
facilitate future cold-start al research. • we explore the impact of the budget
and the extent of the 3d roi on the cold-start al strategies.",2
COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,2.1,3D Medical Image Datasets,"we use the medical segmentation decathlon (msd) collection [1] to define our
benchmark, due to its public accessibility and the standardized datasets
spanning across two common 3d image modalities, i.e., ct and mri. we select five
tasks from the collection appropriate for the 3d segmentation tasks, namely
tasks 2-heart, 3-liver, 4-hippocampus, 7-pancreas, and 9-spleen. liver and
pancreas tasks include both organ and tumor segmentation, while the other tasks
focus on organs only.",2
COLosSAL: A Benchmark for Cold-Start Active Learning for 3D Medical Image Segmentation,3.0,Experimental Results,"impact of selection strategies. in fig. 2, with a fixed budget of 5 samples
(except for heart, where 3 samples are used), we compare the uncertainty-based
and diversity-based strategies against the random selection on five different
segmentation tasks. note that the selections made by each of our evaluated al
strategies are deterministic. for random selection, we visualize the individual
dice scores (red dots) of all 15 runs as well as their mean (dashed line). hd95
results (supp. tab. 1) follow the same trends.our results explain why random
selection remains a strong competitor for 3d segmentation tasks in cold-start
scenarios, as no strategy evaluated in our benchmark consistently outperforms
the random selection average performance.however, we observe that typiclust
(shown as orange) achieves comparable or superior performance compared to random
selection across all tasks in our benchmark, whereas other approaches can
significantly under-perform on certain tasks, especially challenging ones like
the liver dataset. hence, typi-clust stands out as a more robust cold-start
selection strategy, which can achieve at least a comparable (sometimes better)
performance against the mean of random selection. we further note that typiclust
largely mitigates the risk of 'unlucky' random selection as it consistently
performs better than the low-performing random samples (red dots below the
dashed line).impact of different budgets. in fig. 3(a), we compare al strategies
under the budgets of m = 5 vs. m = 10 (3 vs. 5 for hearts). we visualize the
performance under each budget using a heatmap, where each element in the matrix
is the difference of dice scores between the evaluated strategy and the mean of
random selection under that budget. a positive value (warm color) means that the
al strategy is more effective than random selection. we observe an increasing
amount of warm elements in the higher-budget regime, indicating that most
cold-start al strategies become more effective when more budget is allowed. this
is especially true for the diversity-based strategies (three bottom rows),
suggesting that when a slightly higher budget is available, the diversity of the
selected samples is important. hd95 results (supp. tab. 1) are similar.impact of
different rois. in fig. 3(b), with a fixed budget of m = 5 volumes, we compare
the al strategies when uncertainty/diversity is extracted from the entire volume
(global) vs. a local roi (local). each element in this heatmap is the dice
difference of the al strategy between global and local; warm color means global
is better than local. the hippocampus images in msd are already cropped to the
roi, and thus are excluded from this comparison. we observe different trends
across different methods and tasks. overall, we can observe more warm elements
in the heatmap, indicating that using only the local uncertainty or diversity
for cold-start al cannot consistently outperform the global counterparts, even
with ideal roi generated from ground truth. hd95 results (supp. tab. 2) follow
the same trends.limitations. for the segmentation tasks that include tumors (
4th and 5th columns on fig. 3(a)), we find that almost no al strategy is very
effective, especially the uncertainty-based approaches. the uncertainty-based
methods heavily rely on the uncertainty estimated by the network trained on the
proxy tasks, which likely makes the uncertainty of tumors difficult to capture.
it may be nec-essary to allocate more budget or design better proxy tasks to
make cold-start al methods effective for such challenging tasks. lastly,
empirical exploration of cold-start al on iterative al is beyond the scope of
this study and merits its own dedicated study in future.",2
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,1.0,Introduction,"when using a machine learning (ml) model during intraoperative tissue
characterisation, it is vital that the surgeon is able to assess how reliable a
model's prediction is [8]. for the surgeon to trust the output predictions of
the model, the model must be able to explain itself reliably in a clinical
scenario [2]. to assess an explainability method we consider five metrics of
performance: speed, usability, generalisability, trustworthiness and ability to
localise semantic features. the explanation of a model's predictions is
trustworthy if small perturbations in the input or model parameters, results in
a similar output explanation. one form of explainability in the image
classification domain is pixel attribution (pa) mapping. pa maps aim to
highlight the ""most important"" pixels to the classification. pa maps can be used
to visually highlight whether a model is poorly extracting semantic features
[32] and/or that the model is misinformed due to spurious correlations within
the data that it was trained on [16]. to efficiently process image data, these
methods mainly rely on convolutional neural networks (cnns) and achieve
state-of-the-art (sota) performance. one of the first pa methods proposed for
cnns was class activation maps (cam) [33]. cam uses one forward pass of the
model to find the channels in the last convolutional layer that contributed most
to the prediction. one of cam's limitations is its reliance on global average
pooling (gap) [21] after the last convolutional layer as it dramatically reduces
the number of architectures that can use cam. to improve on this, grad-cam [30]
generalises to all cnn architectures which are differentiable from the output
logit layer to the chosen convolutional layer. however, grad-cam often lacks
sharpness in object localisation, as noted and improved on in grad-cam++ [6] and
smoothgrad-cam++ [24]. these extensions of grad-cam have good semantic feature
localisation but they are unable to be deployed for use in surgery [5]. both
score-cam [31] and recipro-cam [5] also generalise to all cnn architectures but
are deployable. score-cam improves on object localisation within the visual pa
map without losing the class specific capabilities of grad-cam by masking out
regions of the image and measuring the change in the output score. this is
similar to perturbation methods like rise [26], lime [28] and other perturbation
techniques [3,32]. on the other hand, recipro-cam focuses on the speed of pa map
computation whilst maintaining comparable sota performance. by utilising the
cnn's receptive field, recipro-cam generates a number of spatial masks and then
measures the effect on the output score much like score-cam.despite being
speedy, easy to deploy and able to localise semantic features, the above methods
lack trustworthiness due to the training strategy of their underlying model.
deep learning (dl) models trained with empirical risk minimisation (erm) are
overconfident in prediction [12] and vulnerable to adversarial attacks [13].
bayesian neural networks (bnns) [23] bring improved regularisation and output
uncertainty estimates. unfortunately, the non-linearity and number of variables
within nns make bayesian inference a computationally intensive task. for this
reason, variational methods [15,18] are used to approximate bayesian inference.
more recently, the variational method bayes by backprop [4] used dropout [19] to
approximate bayesian inference. dropout is a regularisation technique which has
also been noted to improve salient feature extraction. although bayes by
backprop is trustworthy, it often fails to scale to the complex architectures of
sota models. to improve on this lack of generalisability, another variational
method called monte carlo (mc) dropout [12] proposes that a model trained with
dropout is equivalent to a probabilistic deep gaussian process [7,11]. with this
assumption, an estimated output distribution is computed after a number of
forward passes with dropout have been applied. this output distribution is used
in practice to indicate risk in the model's predictions. surgeons in practice
can use this risk during diagnosis to trust the model for decision making [14].
using dropout to perturb a model is a computationally cheap method of model
averaging [19]. it is worth noting though that this method's validity as a
bayesian inference approximation was later questioned [10]. however, this does
not affect the use of this method for risk estimation. so far, model
explainability and risk estimation have mostly been used separately to assess
models' suitability for surgical applications. distdeepshap [20] computed the
uncertainty of shapley values to show uncertainty in explainability maps.
however, distdeepshap is a model-agnostic interpretability method that shows the
global effect of perturbing inputs, instead of providing an insight to the
model's learned representations. the aim of this paper is to show that the
fusion of mc dropout and pa methods leads to improved explainability.in this
paper, we propose the first approach which incorporates risk estimation into a
pa method. a classification model is trained with dropout and a pa method is
used to generate a pa map. at test time, the classification model is employed
with the dropout enabled. in this work, we propose to repeat this process for a
number of iterations creating a volume of pa maps. this volume is used to
generate a pixel-wise distribution of pa values from which we can infer risk.
more specifically, we introduce a method to generate an enhanced pa map by
estimating the expectation values of the pixel-wise distributions. in addition,
the coefficient of variation (cv) is used to estimate pixel-wise risk of this
enhanced pa map. this provides an improved explanation of the model's prediction
by clearly presenting to the surgeon which salient areas to trust in the model's
enhanced pa map. in this work, we focus on the explainability of the
classification of brain tumours using probe-based confocal laser endomicroscopy
(pcle) data. performance evaluation on pcle data shows that our improved
explainability method outperforms the sota.",2
Explainable Image Classification with Improved Trustworthiness for Tissue Characterisation,3.0,Experiments and Analysis,"dataset. the developed explainability framework has been validated on an in vivo
and ex vivo pcle dataset of meningioma, glioblastoma and metastases of an
invasive ductal carcinoma (idc). all studies on human subjects were performed
according to the requirements of the local ethic committee and in agreement with
the declaration of helsinki (no. cle-001 nr: 2014480). the cellvizio c by mauna
kea technologies, paris, france has been used in combination with the mini laser
probe cystoflex c uhd-r. the distinguishing characteristic of the meningioma is
the psammoma body with concentric circles that show various degrees of
calcification. regarding glioblastomas, the pcle images allow for the
visualization of the characteristic hypercellularity, evidence of irregular
nuclei with mitotic activities or multinuclear appearance with irregular cell
shape. when examining metastases of an idc, the tumor presents as egg-shaped
cells with uniform evenly spaced nuclei. our dataset includes 38 meningioma
videos, 24 glioblastoma and 6 idc. each pcle video represents one tumour type
and corresponds to a different patient. the data has been curated to remove
noisy images and similar frames. this resulted in a training dataset of 2500
frames per class (7500 frames in total) and a testing dataset of the same size.
the dataset is split into a training and testing subset, with the division done
on the patient level.implementation. to implement the dl models we use the
open-source framework pytorch [25] and a nvidia geforce rtx 3090 graphics card
for parallel computation. to show our method generalises we trained two
lightweight models: resnet-18 [17] with a learning rate of 0.01 and mobilenetv2
[29] with a learning rate of 0.001. both were trained using the adam-w [22]
optimiser with a weight decay of 0.01 and dropout probability 0.1. we report the
model's top-1 accuracy for resnet18 as 94.0% and for mobilenet as 86.6%. at test
time, we set t = 100 to create a fair distribution of pa maps. pa methods were
implemented with the help of torchcam [9] and reciprocam was implemented using
the authors' source code.evaluation metrics. evaluating a pa method is not a
trivial task because a pa map may not need to be inline with what a human deems
""reasonable"" [1]. segmentation scores like intersection over union (iou) may be
used with caution to compare thresholded pa maps to ground truth maps with
annotated salient regions. by doing so, we can measure how informed the model is
about a particular class. to quantify how misinformed a model is, we can
estimate at its average drop [6]:where, x = x f s ( ŷ (x). the above equation
measures the effect on the output score of the classification model if we only
include the pixels which the pa method scored highly. a minimum average drop is
desired.as average drop was found to not be sufficient on its own, the unified
method adcc [27] was introduced which is the harmonic mean of average drop,
coherency and complexity, defined as:coherency is the pearson correlation
coefficient which ensures that the remaining pixels after dropping are still
important, defined as:where, cov(., .) is the covariance and σ is the standard
deviation. a higher coherency is better. complexity is the l1 norm of the output
pa map.complexity is used to measure how cluttered a pa map is. for a good pa
map, complexity should be a minimum. as it has been shown in the literature, the
metrics in eqs. ( 3), ( 5) and ( 6), can not be used individually to evaluate a
pa method [27]. adcc combined with computation time gives us a reliable overall
metric of how a pa method is performing. performance evaluation. the proposed
method has been compared to combinations of resnet18 and mobilenetv2 with sota
pa methods. at test time, dropout is not enabled for these standard methods, it
is only enabled for our method. in table 1, we show that our method outperforms
all the compared cnn-pa method combinations on adcc. the dropout version of
scorecam is too computationally expensive and therefore is not included in our
comparison. we believe that the better performance of our method is because of
the random dropping of features taking place during dropout at test time which
helps to suppress noise in the estimated enhanced pa map. the combination of
recipro-cam with our proposed method improves performance (increases adcc) at
the expense of increasing the computational complexity. we believe that this
could be reduced using a batched implementation of recipro-cam. we attribute
slow down in smoothgradcam++ when dropout is applied during test time to the
perturbations it adds on top of the pa method. our validation study shows that
grad-cam, grad-cam++ and recipro-cam are often leading in terms of speed as
expected from the literature. in fig. 1, we can see our proposed method reduces
noise in the pa map around the salient region. the distinguishing characteristic
of the meningioma is the psammoma body which is highlighted by all the pa
methods. risk estimations from eq. ( 2) are also displayed and provide an added
visualisation for a surgeon to trust the model. as it can be seen, areas of low
cv match the areas of high pa values which verifies the trustworthiness of our
method. we believe that the proposed explainability method could be used to
support the surgeon intraoperatively in diagnosis and decision making during
tumour resection. the enhanced pa map extracted with our method highlights the
areas which were the most important to the model's prediction. when these areas
correlate with clinically relevant areas, it shows that the model has learned to
robustly classify the different tissue classes. hence, it can be trusted by the
surgeon for diagnosis.",2
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,1.0,Introduction,"humans inherently learn in an incremental manner, acquiring new concepts over
time without forgetting previous ones. in contrast, deep learning models suffer
from catastrophic forgetting [10], where learning from new data can override
previously acquired knowledge. in this context, the class-incremental continual
learning problem was formalized by rebuffi et al. [23], where new classes are
observed in different stages, restricting the model from accessing previous
data.the medical domain faces a similar problem: the ability to dynamically
extend a model to new classes is critical for multiple organ and tumor
segmentation, wherein the key obstacle lies in mitigating 'forgetting.' a
typical strategy involves retaining some previous data. for instance, liu et al.
[13] introduced a memory module to store the prototypical representation of
different organ categories. however, such methods, reliant on an account of data
and annotations, may face practical constraints as privacy regulations could
make accessing prior data and annotations difficult [9]. an alternative strategy
is to use pseudo labels generated by previously trained models on new data.
ozdemir et al. [18,19] extended the distillation loss to medical image
segmentation. a concurrent study of ours [7] mainly focused on architectural
extension, addressing the forgetting problem by freezing the encoder and decoder
and adding additional decoders when learning new classes. while these strategies
have been alleviating the forgetting problem, they led to tremendous memory
costs for model parameters.therefore, we identify two main open questions that
must be addressed when designing a multi-organ and tumor segmentation framework.
q1: can we relieve the forgetting problem without needing previous data and
annotations? q2: can we design a new model architecture that allows us to share
more parameters among different continual learning steps?to tackle the above
questions, in this paper, we propose a novel continual multi-organ and tumor
segmentation method that overcomes the forgetting problem with little memory and
computation overhead. first, inspired by knowledge distillation methods in
continual learning [11,14,15,17], we propose to generate soft pseudo annotations
for the old classes on newly-arrived data. this enables us to recall old
knowledge without saving the old data. we observe that with this simple
strategy, we are able to maintain a reasonable performance for the old classes.
second, we propose image-aware segmentation heads for each class on top of the
shared encoder and decoder. these heads allow the use of a single backbone and
easy extension to new classes while bringing little computational cost. inspired
by liu et al. [12], we adopt the text embedding generated by contrastive
language-image pre-training (clip) [22]. clip is a large-scale image-text
co-training model that is able to encode high-level visual semantics into text
embeddings. this information will be an advantage for training new classes with
the class names known in advance.we focus on organ/tumor segmentation because it
is one of the most critical tasks in medical imaging [6,21,27,28], and continual
learning in semantic segmentation is under-explored in the medical domain. we
evaluate our continual learning method using three datasets: btcv [8], lits [1]
and jhh [25] (a private dataset at johns hopkins hospital) 1 . on the public
datasets, the learning trajectory is to first segment 13 organs in the btcv
dataset, then learn to fig. 1. an overview of the proposed method. an encoder
(enc) processes the input image to extract its features, which are then reduced
to a feature vector (fimage) by a global average pooling layer. this feature
vector is subsequently concatenated with a clip embedding (ω class ), calculated
using the pre-trained clip model. through a series of multi-layer perceptron
(mlp) layers, we derive class-specific parameters of convolution kernels (θ
class ). these kernels, when applied to the decoder (dec) feature, yield the
mask for the respective class.segment liver tumors in the lits dataset. on the
private dataset, the learning trajectory is to first segment 13 organs, followed
by continual segmentation of three gastrointestinal tracts and four
cardiovascular system structures. in our study, we review and compare three
popular continual learning baselines that apply knowledge distillation to
predictions [11], features [17], and multi-scale pooled features [3],
respectively. the extensive results demonstrate that the proposed method
outperforms existing methods, achieving superior performance in both keeping the
knowledge of old classes and learning the new ones while maintaining high memory
efficiency.",2
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,0.0,Image-Aware Organ-Specific Heads:,"the vanilla swin unetr has a softmax layer as the output layer that predicts the
probabilities of each class. we propose to replace the output layer with
multiple image-aware organ-specific heads. we first use a global average pooling
(gap) layer on the last encoder features to obtain a global feature f of the
current image x. then for each organ class k, a multilayer perceptron (mlp)
module is learned to map the global image feature to a set of parameters θ k
:where e(x) denotes the encoder feature of image x. an output head for organ
class k is a sequence of convolution layers that use parameters θ k as
convolution kernel parameters. these convolution layers are applied to the
decoder features, which output the segmentation prediction for organ class
k:where e is the encoder, d is the decoder, σ is the sigmoid non-linear layer
and p (y k j = 1) denotes the predicted probability that pixel j belongs to the
organ class k. the predictions for each class are optimized by binary cross
entropy loss. the separate heads allow independent probability prediction for
newly introduced and previously learned classes, therefore minimizing the impact
of new classes on old ones during continual learning. moreover, this design
allows multi-label prediction for cases where a pixel belongs to more than one
class (e.g., a tumor on an organ).",2
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,3.0,Experiment and Result,"datasets: we empirically evaluate the proposed model under two data settings: in
one setting, both training and continual learning are conducted on the inhouse
jhh dataset. it has multiple classes annotated, which can be categorized into
three groups: the abdominal organs (in which seven classes are learned in step
1: spleen, right kidney, left kidney, gall bladder, liver, postcava, pancreas),
the gastrointestinal tract (in which three classes are learned in step 2:
stomach, colon, intestine), and other organs (in which four classes are learned
in step 3: aorta, portal vein and splenic vein, celiac truck, superior
mesenteric artery). the categorization is in accordance with totalsegmentator
[24]. in the other setting, we first train on the btcv dataset and then do
continual learning on the lits dataset. the btcv dataset contains 47 abdominal
ct images delineating 13 organs. the lits dataset contains 130 contrast-enhanced
abdominal ct scans for liver and liver tumor segmentation. we use 13 classes
(spleen, right kidney, left kidney, gall bladder, esophagus, liver, stomach,
aorta, inferior vena cava, portal vein and splenic vein, pancreas, right adrenal
gland, left adrenal gland) from btcv in step 1 learning and the live tumor from
lits in step 2 learning.",2
Continual Learning for Abdominal Multi-organ and Tumor Segmentation,4.0,Conclusion,"in this paper, we propose a method for continual multiple organ and tumor
segmentation in 3d abdominal ct images. we first empirically verified the
effectiveness of high-quality pseudo labels in retaining previous knowledge.
then, we propose a new model design that uses organ-specific heads for
segmentation, which allows easy extension to new classes and brings little
computational cost in the meantime. the segmentation heads are further
strengthened by utilizing the clip text embeddings that encode the semantics of
organ or tumor classes. numerical results on an in-house dataset and two public
datasets demonstrate that the proposed method outperforms the continual learning
baseline methods in the challenging multiple organ and tumor segmentation tasks.",2
Efficient Subclass Segmentation in Medical Images,2.0,Method,"problem definition. we start by considering a set of r coarse classes, denoted
by y c = {y 1 , ..., y r }, such as background and brain tumor, and a set of n
training images, annotated with y c , denoted by d c = {(x l , y l )|y l i ∈ y c
} n l=1 . each pixel i in image x l is assigned a superclass label y l i . to
learn a finer segmentation model, we introduce a set of fine subclasskr }, such
as background, enhancing tumor, tumor core, and whole tumor. we assume that only
a small subset of n training images have pixel-wise subclass labels z ∈ y f
denoted byour goal is to train a segmentation network f (x l ) that can
accurately predict the subclass labels for each pixel in the image x l , even
when n n . without specification, we consider r = 2 (background and foreground)
and extend the foreground class to multi subclass in this work.prior
concatenation. one direct way to leverage the superclass and subclass
annotations simultaneously is using two 1×1×1 convolution layers as superclass
and subclass classification heads for the features extracted from the network.
the superclassification and subclassification heads are individually trained by
superclass p c (x l ) labels and subclass labels p f (x l ). with enough
superclass labels, the feature maps corresponding to different superclasses
should be well separated. however, this coerces the subclassification head to
discriminate among k subclasses under the mere guidance from few subclass
annotations, making it prone to overfitting.another common method to incorporate
the information from superclass annotations into the subclassification head is
negative learning [14]. this technique penalizes the prediction of pixels being
in the wrong superclass label, effectively using the superclass labels as a
guiding principle for the subclassification head. however, in our experiments,
we found that this method may lead to lower overall performance, possibly due to
unstable training gradients resulting from the uncertainty of the subclass
labels.to make use of superclass labels without affecting the training of the
subclass classification head, we propose a simple yet effective method called
prior concatenation (pc): as shown in fig. 1 (a), we concatenate predicted
superclass logit scores s c (x l ) onto the feature maps f (x l ) and then
perform subclass segmentation. the intuition behind this operation is that by
concatenating the predicted superclass probabilities with feature maps, the
network is able to leverage the prior knowledge of the superclass distribution
and focus more on learning the fine-grained features for better discrimination
among subclasses.separate normalization. intuitively, given sufficient
superclass labels in supervised learning, the superclassification head tends to
reduce feature distance among samples within the same superclass, which
conflicts with the goal of increasing the distance between subclasses within the
same superclass. to alleviate this issue, we aim to enhance the internal
diversity of the distribution within the same superclass while preserving the
discriminative features among superclasses.to achieve this, we propose separate
normalization(sn) to separately process feature maps belonging to hierarchical
foreground and background divided by superclass labels. as a superclass and the
subclasses within share the same background, the original conflict between
classifiers is transferred to finding the optimal transformations that separate
foreground from background, enabling the network to extract class-specific
features while keeping the features inside different superclasses
well-separated.our framework is shown in fig. 1 (b). first, we use batch norm
layers [12] to perform separate affine transformations on the original feature
map. the transformed feature maps, each representing a semantic foreground and
background, are then passed through a convolution block for feature extraction
before further classification. the classification process is coherent with the
semantic meaning of each branch. namely, the foreground branch includes a
superclassifier and a subclassifier that classifies the superclass and subclass
foreground, while the background branch is dedicated solely to classify
background pixels. finally, two separate network branches are jointly supervised
by segmentation loss on superand subclass labels. the aforementioned prior
concatenation continues to take effect by concatenating predicted superclass
logits on the inputs of subclassifier.hierarchicalmix. given the scarcity of
subclass labels, we intend to maximally exploit the existent subclass
supervision to guide the segmentation of coarsely labeled samples. inspired by
guidedmix [23], which provides consistent knowledge transfer between similar
labeled and unlabeled images with pseudo labeling, we propose
hierarchicalmix(hm) to generate robust pseudo supervision. nevertheless,
guidedmix relies on image distance to select similar images and performs a
whole-image mixup, which loses focus on the semantic meaning of each region
within an image. we address this limitation by exploiting the additional
superclass information for a more targeted mixup. this information allows us to
fuse only the semantic foreground regions, realizing a more precise transfer of
foreground knowledge. a detailed pipeline of hierarchicalmix is described
below.as shown in fig. 2, for each sample (x, y) in the dataset that does not
have subclass labels, we pair it with a randomly chosen fine-labeled sample (x ,
y , z ). first, we perform an random rotation and flipping t on (x, y) and feed
both the original sample and the transformed sample tx into the segmentation
network f . an indirect segmentation of x is obtained by performing the inverse
transformation t -1 on the segmentation result of tx. a transform-invariant
pseudo subclass label map z pse is generated according to the following scheme:
pixel (i, j) in z pse is assigned a valid subclass label index (z pse ) i,j = f
(x) i,j only when f (x) i,j agrees with [t -1 f (tx)] i,j with a high confidence
τ as well as f (x) i,j and x i,j both belong to the same superclass label.next,
we adopt image mixup by cropping the bounding box of foreground pixels in x ,
resizing it to match the size of foreground in x, and linearly overlaying them
by a factor of α on x. this semantically mixed image x mix has subclass labels z
= resize(α • z ) from the fine-labeled image x . then, we pass it through the
network to obtain a segmentation result f (x mix ). this segmentation result is
supervised by the superposition of the pseudo label map z pse and subclass
labels z, with weighting factor α:the intuition behind this framework is to
simultaneously leverage the information from both unlabeled and labeled data by
incorporating a more robust supervision from transform-invariant pseudo labels.
while mixing up only the semantic foreground provides a way of exchanging
knowledge between similar foreground objects while lifting the confirmation bias
in pseudo labeling [1].",2
Efficient Subclass Segmentation in Medical Images,3.0,Experiments,"dataset and preprocessing. we conduct all experiments on two public datasets.
the first one is the acdc1 dataset [5], which contains 200 mri images with
segmentation labels for left ventricle cavity (lv), right ventricle cavity (rv),
and myocardium (myo). due to the large inter-slice spacing, we use 2d
segmentation as in [2]. we adopt the processed data and the same data division
in [16], which uses 140 scans for training, 20 scans for validation and 40 scans
for evaluation. during inference, predictions are made on each individual slice
and then assembled into a 3d volume. the second is the brats20212 dataset [3],
which consists of 1251 mpmri scans with an isotropic 1 mm 3 resolution. each
scan includes four modalities (flair, t1, t1ce, and t2), and is annotated for
necrotic tumor core (tc), peritumoral edematous/invaded tissue (pe), and the
gd-enhancing tumor (et). we randomly split the dataset into 876, 125, and 250
cases for training, validation, and testing, respectively. for both datasets,
image intensities are normalized to values in [0, 1] and the foreground
superclass is defined as the union of all foreground subclasses for both
datasets.implementation details and evaluation metrics. to augment the data
during training, we randomly cropped the images with a patch size of 256 × 256
for the acdc dataset and 96 × 96 × 96 for the brats2021 dataset. the model loss
l is set by adding the losses from cross entropy loss and dice loss. the
weighing factor α in hierarchicalmix section is chosen to be 0.5, while τ
linearly decreases from 1 to 0.4 during the training process.we trained the
model for 40,000 iterations using sgd optimizer with a 0.9 momentum and a
linearly decreasing learning rate that starts at 0.01 and ends with 0. we used a
batch size of 24 for the acdc dataset and 4 for the brats2021 dataset, where
half of the samples are labeled with subclasses and the other half only labeled
with superclasses. more details can be found in the supplementary materials. to
evaluate the segmentation performance, we used two widely-used metrics: the dice
coefficient (dsc) and 95% hausdorff distance (hd 95 ). the confidence factor τ
mentioned in hierarchicalmix starts at 1 and linearly decays to 0.4 throughout
the training process, along with a weighting factor α sampled according to the
uniform distribution on [0.5, 1].performance comparison with other methods. to
evaluate the effectiveness of our proposed method, we firstly trained two u-net
models [20] to serve as upper and lower bounds of performance. the first u-net
was trained on the complete subclass dataset {(x l , y l , z l )} n l=1 , while
the second was trained on its subset {(x l , y l , z l )} n l=1 . then, we
compared our method with the following four methods, all of which were trained
using n subclass labels and n superclass labels: modified u-net (mod): this
method adds an additional superclass classifier alongside the subclass
classifier in the u-net. negative learning (nl): this method incorporates
superclass information into the loss module by introducing a separate negative
learning loss in the original u-net. this additional loss penalizes pixels that
are not segmented as the correct superclass. cross pseudo supervision (cps) [7]:
this method simulates pseudo supervision by utilizing the segmentation results
from two models with different parameter initializations, and adapts their
original network to the modified u-net architecture. uncertainty aware mean
teacher (uamt) [26]: this method modifies the classical mean teacher
architecture [22] by adapting the teacher model to learn from only reliable
targets while ignoring the rest, and also adapts the original network to the
modified u-net architecture.table 1. mean dice score (%, left) and hd95 (mm,
right) of different methods on acdc and brats2021 datasets. sup. and sub.
separately represents the number of data with superclass and subclass
annotations in the experiments. '_' means the result of our proposal is
significantly better than the closet competitive result (p-value < 0.05). the
standard deviations of each metric are recorded in the supplementary materials.
the quantitative results presented in table 1 reveal that all methods that
utilize additional superclass annotations outperformed the baseline method,
which involved training a u-net using only limited subclass labels. however, the
methods that were specifically designed to utilize superclass information or
explore the intrinsic structure of the subclass data, such as nl, cps, and uamt,
did not consistently outperform the simple modified u-net. in fact, these
methods sometimes performed worse than the simple modified u-net, indicating the
difficulty of utilizing superclass information effectively. in contrast, our
proposed method achieved the best performance among all compared methods on both
the acdc and brats2021 datasets. specifically, our method attained an average
dice score of 87.3% for acdc and 75.4% for brats2021, outperforming the closest
competitor by 5.0% and 1.4%, respectively. ablation studies. in this study, we
performed comprehensive ablation studies to analyze the contributions of each
component and the performance of our method under different numbers of images
with subclass annotations. the performance of each component is individually
evaluated, and is listed in table 2. each component has demonstrated its
effectiveness in comparison to the naive modified u-net method. moreover, models
that incorporate more components generally outperform those with fewer
components. the effectiveness of the proposed hierarchicalmix is evident from
the comparisons made with models that use only image mixup or pseudo-labeling
for data augmentation, while the addition of separate normalization consistently
improves the model performance. furthermore, our method was competitive with a
fully supervised baseline, achieving comparable results with only 6.5% and 3.4%
subclass annotations on acdc and brats2021.",2
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,1.0,Introduction,"despite the success of artificial intelligence (ai) in aiding diagnosis, its
application to medical education remains limited. trainee physicians require
several years of experience with a diverse range of clinical cases to develop
sufficient skills and expertise. however, designing educational materials solely
based on real-world data poses several challenges. for example, although small
but significant disease characteristics (e.g., depth of cancer invasion) can
sometimes alter diagnosis and treatment, collecting pairs with and without these
characteristics is cumbersome. another major challenge is longitudinal tracking
of pathological progression over time (e.g., from the early stage of cancer to
the advanced stage), which is difficult to understand because medical images are
often snapshots. privacy is also a concern since images of educational materials
are widely distributed. therefore, medical image editing that allows users to
generate their intended disease characteristics is useful for precise medical
education [3].image editing can synthesize low-or high-level image contents
[11]. our goal is to develop high-precision medical image editing according to
the fine-grained characteristics of individual diseases, rather than at the
level of disease categories. for example, even if two diseases belong to the
same disease category of ""lung tumor,"" the impression of benign or malignant
will differ depending on fine-grained characteristics, such as whether the
margins are ""smooth"" or ""spiculated."" in this case, our approach is to edit the
tumor margins to be smooth or spiculated. these fine-grained characteristics
consist of low-to mid-level image features to distinguish the substructures of
organs and diseases, which we call anatomical elements.several types of image
editing techniques for medical imaging have been introduced, mainly using
generative adversarial networks [5] and, more recently, diffusion models [2].
nevertheless, editing specific anatomical elements remains a challenge [1,11].
latent space manipulation generates images by controlling latent feature axes
[4,14], but the editable attributes are often global rather than fine-grained.
conditional generation can precisely edit image content by using class or
segmentation labels. however, it requires manually provided labels [15] or
virtual models [18], which are labor-intensive. additionally, accurately
modeling certain fine-grained characteristics, such as the textual variations of
disease, can be a daunting task. image interpolation [17] requires actual images
with targeted content, which limits its applicability.here, we propose a novel
framework for image editing called u3-net that allows the generation of
anatomical elements with precise conditions. the core technique is
self-supervised segmentation, which aims to achieve pixel-wise clustering
without manually annotated labels [6,7]. as shown in fig. 1a,u3-net converts an
input image into a segmentation map corresponding to the anatomical elements.
once the user has completed editing, u3-net synthesizes an image in which the
targeted anatomical element has been modified. as a result, our synthesized
medical images can highlight hypothetical pathological changes and significant
clinical differences in a single image. for example, fig. 1b shows that whether
or not rectal cancer invades the muscularis propria (i.e., b-2 vs. b-3) affects
cancer staging (i.e., t1 vs. t2) as well as treatment strategy (i.e., endoscopic
resection vs. surgery). the distinction between mucinous and nonmucinous rectal
cancers (see fig. 1c) is also important to estimate the better or worse
prognosis of the disease. these synthetic images can help trainees intuitively
comprehend clinically significant findings and alleviate privacy concerns. five
expert physicians evaluated the edited images from a clinical perspective using
two datasets: a pelvic mri dataset and chest ct dataset.",2
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,4.0,Conclusion,"in this study, we propose a medical image-editing framework to edit fine-grained
anatomical elements. the self-supervised segmentation extracted low-to midlevel
content of medical images, which corresponded well to the clinically meaningful
substructures of organs and diseases. the majority of the edited images with
intended characteristics were perceived as natural medical images by several
expert physicians. our medical image editing method can be applied to medical
education, which has been overlooked as an application of ai. future challenges
include improving scalability with fewer manual operations, validating
segmentation maps from a more objective perspective, and comparing our proposed
algorithm with existing methods, such as those based on superpixels [10].data
use declaration and acknowledgment: the pelvic mri and chest ct datasets were
collected from the national cancer center hospital. the study, data use, and
data protection procedures were approved by the ethics committee of the national
cancer center, tokyo, japan (protocol number 2016-496).our implementation and
all synthesized images will be available here: https://
github.com/kaz-k/medical-image-editing.",2
Towards AI-Driven Radiology Education: A Self-supervised Segmentation-Based Framework for High-Precision Medical Image Editing,3.0,Experiments and Results,"implementation and datasets: all neural networks were implemented in python 3.8
using the pytorch library 1.10.0 [12] on an nvidia tesla a100 gpu running cuda
10.2. the encoder, decoder, and discriminator were implemented based on u-net
[13] (see supplementary information for details). the pelvic mri dataset with
rectal cancer contained 289 image series for training and 100 image series for
testing. for each image series, the min-max normalization converted the pixel
values to [-1, 1]. the chest ct dataset with lung cancer contained 500 image
series for training and 100 image series for testing. the ct values in the range
[-2048, 2048] were normalized to [-1, 1]. both were in-house datasets collected
from a single hospital. every image series comprises two-dimensional (2d)
consecutive slices, and we applied our algorithm on a per 2d slice
basis.self-supervised medical image segmentation: we began by optimizing the
hyperparameters to achieve self-supervised segmentation. appropriate
transformations were selected from six candidate functions: t 1 , random
horizontalflip, t 2 , randomaffine, t 3 , colorjitter, t 4 , randomgaussianblur,
t 5 , randomposterize, t 6 , randomgaussiannoise. because anatomical elements,
including the substructures of organs and diseases, are too detailed for human
annotators to segment, it was difficult to create ground-truth labels.
therefore, the training configuration was selected based on the consensus of two
expert radiologists with domain knowledge. by comparing different settings on
the pelvic mri training dataset (see supplementary information), the number of
segmentation classes of 10, the combination of t 1 , t 2 , and t 3 with moderate
magnitude, the weakly imposed reconstruction loss, and a certain value of the
margin parameter were considered suitable for self-supervised segmentation. in
particular, we found that reconstruction loss is essential for obtaining
segmentation maps corresponding to anatomical elements, although such a loss
term was not included in previous studies [6,7]. a similar configuration was
applied to the chest ct training dataset. the resultant segmentation maps are
shown in fig. 4ab. the anatomical substructures, including the histological
structure of the colorectal wall and subregions within the lung, corresponded
well with the segmentation maps in both the pelvic mri and chest ct testing
datasets. because our self-supervised segmentation extracts low-to mid-level
image content, a semantic object (e.g., rectum or lung cancer) typically
consists of multiple segmentation classes shared with other objects (see the
magnified images in fig. 4ab). these anatomical elements may be too detailed for
humans to annotate, demonstrating the necessity of self-supervised segmentation
for highprecision medical-image editing.",2
Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.2,Fine-Tuning Datasets,"during the fine-tuning stage, we perform extensive experiments on three datasets
with respect to different regions of the human body. abd-110 is an abdomen
dataset from [25] that contains 110 ct images from patients with various
abdominal tumors and these ct images were taken during the treatment planning
stage. we report the average dsc on 11 abdominal organs (large bowel, duodenum,
spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney,
stomach and gallbladder).thorax-85 is a thorax dataset from [5] that contains 85
thoracic ct images. we report the average dsc on 6 thoracic organs (esophagus,
trachea, spinal cord, left lung, right lung, and heart).han is from [24] and
contains 120 ct images covering the head and neck region. we report the average
dsc on 9 organs (brainstem, mandible, optical chiasm, left optical nerve, right
optical nerve, left parotid, right parotid, left submandibular gland, and right
submandibular gland).",2
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,1.0,Introduction,"the demand for precise medical data analysis has led to the widespread use of
deep learning methods in the medical field. however, accompanied by the
promulgation of data acts and the strengthening of data privacy, it has become
increasingly challenging to train models in large-scale centralized medical
datasets. as one of the solutions, federated learning provides a new way out of
the dilemma and attracts significant attention from researchers.federated
learning (fl) [1,2] is a distributed machine learning paradigm in which all
clients train a global model collaboratively while preserving their data
locally. as a crucial core of them, the aggregation algorithm plays an important
role in releasing data potential and improving global model performance. fedavg
[1], as pioneering work, was a simple and effective aggregation algorithm, which
makes the proportions of local datasets size as the aggregation weights of local
models. but in the real world, not only the numbers of datasets held by clients
is different, but also their data distribution may be diverse, which leads to
the fact that the data in the federated learning is non-independent identically
distribution (non-iid). the naive aggregation algorithms maybe have worse
performance because of the non-iid data [3][4][5][6][7][8]. in medical image
segmentation, [9] and [10] took the lead in discussing the application and
safety of federated learning in brain tumor segmentation (brats). to solve the
non-iid challenges of fl in the medical image field, feddg [11] and fedmrcm [12]
were proposed to address the domain shift issue between the source domain and
the target domain, but the sharing of latent features may cause privacy
concerns. auto-fedrl [13] and auto-fedavg [14] were proposed to deal with the
non-iid problem by using an optimization algorithm to learn super parameters and
aggregate weights. ida [15] introduced the inverse distance of local models and
the average model of all clients to handle non-iid data. the work
[16][17][18][19] proposed corresponding aggregation methods from the
perspectives of clustering, frequency domain, bayesian, and representation
similarity analysis. more than this, the first computational competition on
federated learning, federated tumor segmentation (fets) challenge1 [20] was held
to measure the performance of different aggregation algorithms on glioma
segmentation [21][22][23][24]. leon et al. [25] proposed fedcostwavg get a
notable improvement compared to fedavg by including the cost function decreased
during the last round and won the challenge. however, most of these methods
improve the performance by adding other regular terms to the aggregation method,
without considering all factors as a whole, which may limit the performance of
the global model. different from the above methods, inspired by the concept of
the law of universal gravitation in physics, in this paper, we propose a novel
aggregation strategy, fedgrav, which unifies the differences in sample size and
the discrepancies of local models among clients by defining the concept of model
affinity. specifically, we take the client sample size as the mass of the local
model, and the discrepancies among the local models as their distance, which is
quantified from the topological perspective of neural networks. last, the
formula 1 is employed to calculate the affinity and explore the internal
correlation between the local models. the proposed method promotes a more
effective aggregation of local models by unifying the difference between sample
size and local model between clients.the primary contributions of this paper can
be summarized as: (1) we propose fedgrav, a novel aggregation strategy that
unifies the difference both in sample size and local model among clients by
defining the concept of model affinity; (2) we propose model graph distance, a
new method to quantify model differences from the perspective of neural network
topology. (3) we propose an aggregation algorithm that introduces the concept of
affinity and graph into federated learning, and the aggregation weights can be
adjusted adaptively; (4) the superior performance is achieved by the proposed
method, on the public cifar-10 and fets challenge datasets.",2
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,0.0,MICCAI FeTS2021,"training data. the real-world dataset used in experiments is provided by the
fets challenge organizer, which is the training set of the whole dataset about
brain tumor segmentation. in order to evaluate the performance of fedgrav, we
partition the dataset composed of 341 data samples",2
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,3.2,Results,"experiment results on the cifar-10. we first validate the proposed method on the
cifar-10 dataset. table 1 shows the quantitative results of the state-of-the-art
fl methods in terms of the average accuracy, such as fedavg [1], fedprox [6],
fednova [29], and auto-fedavg [14]. as can be seen from the table, the proposed
fedgrav method outperforms the other competing fl aggregation methods including
auto-fedavg, a learning-based aggregation method, which indicates the potential
and superiority of fedgrav.experiment results on miccai fets2021 training
dataset. in order to verify the robustness of our method and its performance in
real-world data, we conduct the experiment on the miccai fets2021 training
dataset. we evaluate the performance of our algorithm by comparing six
indicators: the dice similarity coefficient(dsc) and hausdorff distance-95th
percentile(hd95) of whole tumor(wt), enhancing tumor(et), and tumor core(tc). as
is shown in table 2, we list the average results of fedavg,
fedcostwavg(shortened to fcw), the champion method of fets challenge 2021, and
the proposed fed-grav. different from the original fedcostwavg which changed the
activation function of networks, our re-implemented version made the network
unchanged to ensure a fair comparison. through the quantitative comparison in
table 2, we can find that the proposed method fedgrav has achieved the best
results in all indicators except the hd95 tc. moreover, compared with
fedcostwavg, fedgrav has significantly improved the evaluation of segmentation
performance, especially in the enhancing tumor segmentation.the visualization
results are shown in fig. 2. it can be seen that our fed-grav achieves better
segmentation results, even in the hard example, compared to fedcostwavg and
fedavg. the results proved that the proposed method fedgrav can explore the
correlations of local models better and achieved more excellent aggregation
performance compared with other methods.",2
FedGrav: An Adaptive Federated Aggregation Algorithm for Multi-institutional Medical Image Segmentation,4.0,Conclusion,"in this paper, we introduced fedgrav, a novel aggregation strategy inspired by
the law of universal gravitation in physics. fedgrav improves local model
aggregation by considering both the differences in sample size and discrepancies
among local models. it can adaptively adjust the aggregation weights and explore
the internal correlations of local models more effectively. we evaluated our
method on cifar-10 and real-world miccai federated tumor segmentation challenge
(fets) datasets, and the superior results demonstrated the effectiveness and
robustness of our fedgrav.",2
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,1.0,Introduction,"magnetic resonance imaging (mri) of the brain is an essential imaging modality
to accurately diagnose various neurological diseases ranging from inflammatory
t. pinetz and a. effland-are funded the german research foundation under
germany's excellence strategy -exc-2047/1 -390685813 and -exc2151 -390873048 and
r. haase is funded by a research grant (bonfor; o-194.0002.1). t. pinetz and e.
kobler-contributed equally to this work. lesions to brain tumors and metastases.
for accurate depictions of said pathologies, gadolinium-based contrast agents
(gbca) are injected intravenously to highlight brain-blood barrier dysfunctions.
however, these contrast agents are expensive and may cause nephrogenic systemic
fibrosis in patients with severely reduced kidney function [31]. moreover, [17]
reported that gadolinium accumulates inside patients with unclear health
consequences, especially after repeated application. the american college of
radiology recommends administering the lowest gbca dose to obtain the needed
clinical information [1].driven by this recommendation, several research groups
have recently published dose-reduction techniques focusing on maintaining image
quality. complementary to the development of higher relaxivity contrast agents
[28], virtual contrast [3,8] -replacing a large fraction of the gbca dose by
deep learninghas been proposed. these approaches typically acquire a
contrast-enhanced (ce) scan with a lower gbca dose along with non-ce scans,
e.g., t1w, t2w, flair, or adc. these input images are then processed by a deep
neural network (dnn) to replicate the corresponding standard-dose scan. while
promising, virtual contrast techniques have not been integrated into clinical
practice yet due to falsepositive signals or missed small lesions [3,23]. as
with all deep learning-based approaches, the availability of large datasets is
essential, which is problematic in the considered case since the additional ce
low-dose scan is not acquired in clinical routine exams. hence, there are no
public datasets to easily benchmark and compare different algorithms or evaluate
their performance. in general, the enhancement behavior of pathological tissues
at various gbca dosages has barely been researched due to a lack of data [12].in
recent years, generative models have been used to overcome data scarcity in the
computer vision and medical imaging community. frequently, generative
adversarial networks (gans) [9] are applied as state-of-the-art in image
generation [30] or semantic translation/interpolation [5,18,21]. in a nutshell,
the gan framework trains two competing dnns -the generator and the
discriminator. the generator learns a non-linear transformation of a predefined
noise distribution to fit the distribution of a target dataset, while the
discriminator provides feedback by simultaneously approximating a distance or
divergence between the generated and the target distribution. the choice of this
distance leads to the well-known different gan algorithms, e.g., wasserstein
gans [4,10], least squares gans [24], or non-saturating gans [9]. however, lucic
et al. [22] showed that this choice has only a minor impact on the
performance.learning conditional distributions between images can be
accomplished by additionally feeding a condition (additional scans, dose level,
etc.) into both the generator and discriminator. in particular, for
image-to-image translation tasks, these conditional gans have been successfully
applied using paired [14,25,27] and unpaired training data [35]. within these
methods, an additional content (cycle) loss typically penalizes pixel-wise
deviations (e.g., 1 ) from a corresponding reference to enforce structural
similarity, whereas a local adversarial loss (discriminator with local receptive
field) controls textural similarity. in addition, embeddings have been used to
inject metadata [7,18]. to study the gbca accumulation behavior, we collected
453 ce scans with non-standard gbca doses in the set of {10%, 20%, 33%} along
with the corresponding standard-dose (0.1 mmol/kg) scan after applying the
remaining contrast agent. using this dataset, we aim at the semantic
interpolation of the gbca signal at various fractional dose levels. to this end,
we use gans to learn the contrast enhancement behavior from the dataset
collective and thereby enable the synthesis of contrast signals at various dose
levels for individual cases. further, to minimize the smoothing effect [19] of
typical content losses (e.g., 1 or perceptual [16]), we develop a
noise-preserving content loss function based on the wasserstein distance between
paired image patches calculated using a sinkhornstyle algorithm. this novel loss
enables a faithful generation of noise, which is important for the
identification of enhancing pathologies and their usability as additional
training data.with this in mind, the contributions of this work are as
follows:-synthesis of gbca behavior at various doses using conditional gans,
-loss enabling interpolation of dose levels present in training data,
-noise-preserving content loss function to generate realistic synthetic images.",2
Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,3.0,Numerical Results,"in this section, we evaluate the proposed conditional gan approach with a
particular focus on different content loss distance functions. all synthesis
models were trained on 250 samples acquired on 1.5t and 3t philips achieva
scanners and evaluated on 193 test cases, all collected at site 1 . further
details of the dataset, model and training can be found in the supplementary. in
our experiments, we observed that the choice of the content loss distance
function c (ŷ, y) strongly influences the performance. thus, we consider the
different cases:following johnsen et al. [16], h(x) is the vgg-16 model [32] up
to relu3_3.a qualitative comparison of the different distance functions c is
visualized in fig. 3. the first column depicts synthesized images using the 1
-norm as the distance function. these images depict a plausible contrast signal,
however, suffer from unrealistic smooth homogeneous regions. an improvement
thereof is shown by the perceptual content loss (vgg). the np-loss leads to a
further improvement not only in the contrast signal behavior but also in the
realism of the noise texture, cf. zoom regions in the lower corners.to highlight
the generalization capabilities, we depict in the bottom row of fig. 3 a sample
from site 2 , which was acquired using a philips ingenia scanner. moreover, the
gbca gadoterate was used, while our training data only consists of scans using
the gbca gadobutrol. nevertheless, all models present realistically synthesized
ld images. comparing the zooms of the ld images, we observe that our np-loss
leads to a better synthesis of noise and thereby to fig. 3. qualitative
comparison of synthesized images using different loss functions to the
corresponding reference xld. while the 1 loss yields smooth low-dose images, the
noise pattern is preserved to some extent using the vgg loss; our loss helps to
further retain the noise characteristics. more realistic ld images. in the 1 and
vgg columns, the noise is not faithfully synthesized, thus it is visually easy
to spot the enhancing pathological regions.for completeness, a quantitative
ablation of the considered distance functions on the test images of site 1 is
shown in table 1. although neither maximizing psnr nor ssim [33] is our
objective, we observe on-par performances of the perceptual (vgg) and our
proposed content loss (np) with the standard 1 distance function. using the sd
image, we define ce pixels as those pixels at which the intensity increases by
at least 10% compared to the native scan. an example of these ce regions is
illustrated in the supplementary. thus, the mean absolute error for ce pixels
(mae ce ) quantifies the enhancement behavior. further, we estimate the standard
deviation of the non-ce pixels and report the mae to the ground truth standard
deviation (mae σ ). as shown in table 1, our loss outperforms the other content
losses to a large extent on both metrics, proving its effectiveness for faithful
contrast enhancement and noise generation. further statistical analyses are
presented in the supplementary.table 1. quantitative comparison of the low-dose
synthesis methods. the central columns present metrics evaluated on the
synthesized low-dose images, whereas the right columns evaluate the effect of
purely synthesized data for training the standarddose prediction model [26].
note, that the psnr/ssim of the standard dose prediction model was always
evaluated on real ld images. the definitions of the mean absolute error on the
contrast enhancement (maece) and on the noise standard deviation (maeσ) are in
sect. next, we evaluate the effect of synthesized ld images on the performance
of a virtual contrast model (vcm). in particular, we consider the
state-of-the-art 2.5d u-net model [11,23,26], which predicts an sd image given a
corresponding native and ld image, see supplementary for further details. the
columns on the right of table 1 list the average psnr and ssim score on the real
33% ld subset of our test data from site 1 . the bottom row depicts the
performance if just real 33% ld images are used for training the vcm as an upper
bound. in contrast, the other entries on the right list the performance if only
synthesized ld images are used for training. both metrics show that the samples
synthesized using our np-loss model are superior to both 1 and vgg.to determine
the effectiveness of the ld synthesis models at different settings, we acquired
160 data samples from 1.5t and 3t philips ingenia scanners at site 2 . this site
used the gbca gadoterate, which has a lower relaxivity compared to gadobutrol
used at site 1 [15]. for 80 samples real ld images were acquired, which are used
for testing. using the vcm solely trained on the real 33% ld data of site 1
yields an average psnr and mae ce on the test samples of site 2 of 40.04 and
0.092, respectively. extending the training data for the vcm by synthesized ld
images from our model with np-loss, we get a significantly improvemed (p <
0.001) psnr score of 40.37 and mae ce of 0.075.finally, fig. 4 visualizes
synthesized ld images on the brats dataset [6] along with the associated vcm
outputs. comparing the predicted sd images xsd using 10% and 33% synthesized ld
images xld , we observe that the weakly enhancing tumor at the bottom zoom is
not preserved in the case of 10%, enabling evaluation of dose reduction methods
on known pathological regions. [6] along with the native (left) and real sd
image (right). we also included non-fractional dosage levels (17% and 47%) to
showcase the wide applicability of our algorithm. top: the tumor is well
contrasted in all xsd even for 10%. bottom: the subtle enhancement of the tumor
cannot be recovered from the 10% ld image.",2
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,1.0,Introduction,"ultrasound imaging is a very effective technique for breast lesion diagnosis,
which has high sensitivity. automatically detecting breast lesions is a
challenging problem with a potential to aid in improving the efficiency of
radiologists in ultrasound-based breast cancer diagnosis [18,21]. some of the
challenges associated with automatic breast lesion detection include blurry
boundaries and changeable sizes of breast lesions.most existing breast lesion
detection methods can be categorized into imagebased [10,11,16,17,19] and
video-based [1,9] breast lesion detection approaches. image-based breast lesion
detection approaches perform detection in each frame independently. compared to
image-based breast lesion detection approaches, methods based on videos are
capable of utilizing temporal information for improved detection performance.
for instance, chen et al. [1] exploited temporal coherence for semi-supervised
video-based breast lesion detection. recently, lin et al. [9] proposed a feature
aggregation network, termed as cva-net, that executes intra-video and
inter-video fusions at both video and clip levels based on attention blocks.
although the recent cva-net aggregates clip and video level features, we
distinguish two key issues that hamper its performance. first, the
self-attention based cross-frame feature fusion is a global-level operation and
it operates once before the encoder-decoder, thereby ignoring the useful local
information and in turn missing an effective deep feature fusion. second,
cva-net only performs one-frame prediction based on multiple frame inputs, which
is very time-consuming.to address the aforementioned issues, we propose a
spatial-temporal deformable attention based network, named stnet, for detecting
the breast lesions in ultrasound videos. within our stnet, we introduce a
spatial-temporal deformable attention module to fuse multi-scale
spatial-temporal information among different frames, and further integrate it
into each layer of the encoder and decoder. in this way, different from the
recent cva-net, our proposed stnet performs both deep and local feature fusion.
in addition, we introduce multiframe prediction with encoder feature shuffle
operation that shares the backbone and encoder features, and only perform
multi-frame prediction in the decoder. this enables us to significantly
accelerate the detection speed of the proposed approach. we conduct extensive
experiments on a public breast lesion ultrasound video dataset, named bluvd-186
[9]. the experimental results validate the efficacy of our proposed stnet that
has a superior detection performance. for example, our proposed stnet achieves a
map of 40.0% with an absolute gain of 3.9% in terms of detection accuracy, while
operating at two times faster, compared to the recent cva-net [9].",2
A Spatial-Temporal Deformable Attention Based Framework for Breast Lesion Detection in Videos,3.1,Dataset and Implementation Details,"dataset. we conduct the experiments on the public bluvd-186 dataset [9],
comprising 186 videos including 112 malignant and 74 benign cases. the dataset
has totally 25,458 ultrasound frames, where the number of frames in a video
ranges from 28 to 413. the videos encompass a comprehensive tumor scan, from its
initial appearance to its largest section and eventual disappearance. all videos
were captured using philips tis l9-3 and logiq-e9. the grounding-truths in a
frame, including breast lesion bounding-boxes and corresponding categories, are
labeled by two pathologists, which have eight years of professional background
in the field of breast pathology. we adopt the same dataset splits as in table
1. state-of-the-art quantitative comparison of our approach with existing
methods in literature on the bluvd-186 dataset. our approach achieves a superior
performance on three different metrics. compared to the recent cva-net [9], our
approach obtains a gain of 3.9% in terms of overall ap. we show the best results
in bold.",2
DeDA: Deep Directed Accumulator,3.3,Discussions,"medical images often require processing of a primary target or region of
interest (roi), such as rims, left ventricles, or tumors. these rois frequently
exhibit distinct geometric structures [26] or possess specific spatial
relationships [25] with their surroundings. capturing these characteristics
poses a challenge for modern neural networks, especially given limited and
imbalanced training data. while differentiable grid sampling [12] can tackle
some of these issues within a certain scope, another major class involving
transformations (e.g. hough transform [3]) that necessitate directed
accumulation is overlooked. our proposed deda bridges this gap, enabling the use
of image transformations with directed accumulation within a neural network.
this allows for the parametrization of geometric shapes and the modeling of
spatial correlations in a differentiable manner.while the study focuses on rim+
lesion identification, the proposed deda can be extended to other applications.
these include the utilization of polar transformation for skin lesion
recognition/segmentation, symmetric circular transformation for cardiac image
registration [23], parabola transformation for curvilinear structure
segmentation [21], and high-dimensional bilateral filtering [5].",2
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,1.0,Introduction,"deep learning techniques have achieved unprecedented success in the field of
medical image classification, but this is largely due to large amount of
annotated data [5,18,20]. however, obtaining large amounts of high-quality
annotated data is usually expensive and time-consuming, especially in the field
of pathology image processing [5,[12][13][14]18]. therefore, a very important
issue is how to obtain the highest model performance with a limited annotation
budget. the unlabeled sample pool contains k target categories (red-boxed
images) and l non-target categories (blue-boxed images). existing al methods
cannot accurately distinguish whether the samples are from the target classes or
not, thus querying a large number of non-target samples and wasting the
annotation budget, while our method can accurately query samples from the target
categories. (color figure online)active learning (al) is an effective approach
to address this issue from a data selection perspective, which selects the most
informative samples from an unlabeled sample pool for experts to label and
improves the performance of the trained model with reduced labeling cost
[1,2,9,10,16,17,19]. however, existing al methods usually work under the
closed-set assumption, i.e., all classes existing in the unlabeled sample pool
need to be classified by the target model, which does not meet the needs of some
real-world scenarios [11]. figure 1 shows an al scenario for pathology image
classification in an open world, which is very common in clinical practice. in
this scenario, the whole slide images (wsis) are cut into many small patches
that compose the unlabeled sample pool, where each patch may belong to tumor,
lymph, normal tissue, fat, stroma, debris, background, and many other
categories. however, it is not necessary to perform finegrained annotation and
classification for all categories in clinical applications. for example, in the
cell classification task, only patches of tumor, lymphatic and normal cells need
to be labeled and classified by the target model. since the nontarget patches
are not necessary for training the classifier, labeling them would waste a large
amount of budget. we call this scenario in which the unlabeled pool consists of
both target class and non-target class samples open-set al problem. most
existing al algorithms can only work in the closed-set setting. even worse, in
the open-set setting, they even query more non-target samples because these
samples tend to have greater uncertainty compared to the target class samples
[11]. therefore, for real-world open-set pathology image classification
scenarios, an al method that can accurately query the most informative samples
from the target classes is urgently needed.recently, ning et al. [11] proposed
the first al algorithm for open-set annotation in the field of natural images.
they first trained a network to detect target class samples using a small number
of initially labeled samples, and then modeled the maximum activation value
(mav) distribution of each sample using a gaussian mixture model [15] (gmm) to
actively select the most deterministic target class samples for labeling.
although promising performance is achieved, their detection of target class
samples is based on the activation layer values of the detection network which
has limited accuracy and high uncertainty with small initial training samples.in
this paper, we propose a novel al framework under an open-set scenario, and
denote it as openal, which cannot only query as many target class samples as
possible but also query the most informative samples from the target classes.
openal adopts an iterative query paradigm and uses a two-stage sample selection
strategy in each query. in the first stage, we do not rely on a detection
network to select target class samples and instead, we propose a feature-based
target sample selection strategy. specifically, we first train a feature
extractor using all samples in a self-supervised learning manner, and map all
samples to the feature space. there are three types of samples in the feature
space, the unlabeled samples, the target class samples labeled in previous
iterations, and the non-target class samples queried in previous iterations but
not being labeled. then we select the unlabeled samples that are close to the
target class samples and far from the non-target class samples to form a
candidate set. in the second stage, we select the most informative samples from
the candidate set by utilizing a model-based informative sample selection
strategy. in this stage, we measure the uncertainty of all unlabeled samples in
the candidate set using the classifier trained with the target class samples
labeled in previous iterations, and select the samples with the highest model
uncertainty as the final selected samples in this round of query. after the
second stage, the queried samples are sent for annotation, which includes
distinguishing target and non-target class samples and giving a fine-grained
label to every target class sample. after that, we train the classifier again
using all the fine-grained labeled target class samples.we conducted two
experiments with different matching ratios (ratio of the number of target class
samples to the total number of samples) on a public 9-class colorectal cancer
pathology image dataset. the experimental results demonstrate that openal can
significantly improve the query quality of target class samples and obtain
higher performance with equivalent labeling cost compared with the current
state-of-the-art al methods. to the best of our knowledge, this is the first
open-set al work in the field of pathology image analysis.",2
OpenAL: An Efficient Deep Active Learning Framework for Open-Set Pathology Image Classification,3.1,"Dataset, Settings, Metrics and Competitors","to validate the effectiveness of openal, we conducted two experiments with
different matching ratios (the ratio of the number of samples in the target
class to the total number of samples) on a 9-class public colorectal cancer
pathology image classification dataset (nct-crc-he-100k) [6]. the dataset
contains a total of 100,000 patches of pathology images with fine-grained
labeling, with nine categories including adipose (adi 10%), background (back
11%), debris (deb 11%), lymphocytes (lym 12%), mucus (muc 9%), smooth muscle
(mus 14%), normal colon mucosa (norm 9%), cancer-associated stroma (str 10%),
and colorectal adenocarcinoma epithelium (tum, 14%). to construct the openset
datasets, we selected three classes, tum, lym and norm, as the target classes
and the remaining classes as the non-target classes. we selected these target
classes to simulate a possible scenario for pathological cell classification in
clinical practice. technically, target classes can be randomly chosen. in the
two experiments, we set the matching ratio to 33% (3 target classes, 6
non-target classes), and 42% (3 target classes, 4 non-target classes),
respectively.metrics. following [11], we use three metrics, precision, recall
and accuracy to compare the performance of each al method. we use precision and
recall to measure the performance of different methods in target class sample
selection.as defined in eq. 5, precision is the proportion of the target class
samples among the total samples queried in each query and recall is the ratio of
the number of the queried target class samples to the number of all the target
class samples in the unlabeled sample pool.where k m denotes the number of
target class samples queried in the mth query, l m denotes the number of
non-target class samples queried in the mth query, and n target denotes the
number of target class samples in the original unlabeled sample pool. obviously,
the higher the precision and recall are, the more target class samples are
queried, and the more effective the trained target class classifier will be. we
measure the final performance of each al method using the accuracy of the final
classifier on the test set of target class samples.competitors. we compare the
proposed openal to random sampling and five al methods, lfosa [11], uncertainty
[7,8], certainty [7,8], coreset [17] and ra [20], of which only lfosa [11] is
designed for open-set al. for all al methods, we randomly selected 1% of the
samples to label and used them as the initial labeled set for model
initialization. it is worth noting that the initial labeled samples contain
target class samples as well as non-target class samples, but the non-target
class samples are not fine-grained labeled. after each query round, we train a
resnet18 model of 100 epochs, using sgd as the optimizer with momentum of 0.9,
weight decay of 5e-4, initial learning rate of 0.01, and batchsize of 128. the
annotation budget for each query is 5% of all samples, and the length of the
candidate set is twice the budget for each query. for each method, we ran four
experiments and recorded the average results for four randomly selected seeds.",2
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,1.0,Introduction,"people perceive the world with signals from different modalities, which often
carry complementary information about varying aspects of an object or event of
interest. therefore, collecting and utilizing multimodal information is crucial
for artificial intelligence to understand the world around us. data collected
from various sensors (e.g., microphones, cameras, motion controllers) are used
to identify human activity [4]. moreover, multimodal medical images obtained
from different scanning protocols (e.g., computed tomography, magnetic resonance
imaging) are employed for disease diagnosis [12]. satisfactory performances have
been achieved with these multimodal data.in practical application, however,
modality missing is a common scenario. wirelessly connected sensors may
occasionally disconnect and temporarily be unable to send any data [3]. medical
images may be missing due to artifacts and diverse patient conditions [11]. in
these unexpected situations, any combinatorial subset of available modalities
can be given as input. to handle this, one intuitive solution is to train a
dedicated model on all possible subsets of available modalities [6,14,23].
however, these methods are ineffective and timeconsuming. another way is to
predict missing modalities and perform with the completed modalities [20]. but,
these approaches also require additional prediction networks for each missing
situation, and the quality of the recovered data directly affects the
performance, especially when there are only a few available modalities.
recently, fusing the available modalities into a shared representation received
wide attention. however, it is particularly challenging due to the varying
number of input modalities, which results in the n-to-one fusion
problem.currently, existing fusion strategies to tackle this challenge can be
broadly grouped into three categories: the arithmetic strategy, the selection
strategy and the convolution strategy. as shown in fig. 1(a), in the arithmetic
strategy, feature representations of available modalities are merged by an
arithmetic function, such as averaging, computing the first and second moments
or other designed formulas [10,13,17]. for the selection strategy, as shown in
fig. 1(b), each value of fused representation is selected from the values at the
corresponding position of the inputs. the selection rule can be defined as max,
min or probabilitybased [2,8,19]. although the above two fusion strategies are
easily scalable to various data missing situations, their fusion operation is
hard-coded. all available modalities contribute equally and their latent
correlations are neglected. unlike hard-coding the fusion operation, in the
convolution strategy, the convolutional fusion network automatically learns how
to fuse these feature representations, which is beneficial to exploiting the
correlation between multiple modalities. however, as shown in fig. 1(c), this
fusion strategy needs a constant number of data to meet the requirements of the
input channels in the convolutional network. therefore, it has to simulate
missing data by crudely zero-padding or replacing it with similar modalities,
which inevitably introduces a bias in computation and causes performance
degradation [5,18,25].transformer has achieved success in the field of computer
vision, demonstrating that self-attention mechanism has the ability to capture
the latent correlation of image tokens. however, no work has explored the
effectiveness of self-attention mechanism on the n-to-one fusion, where n is
variable during training, rather than fixed. furthermore, the calculation of
self-attention does not require a fixed number of tokens as input, which
represents a potential for handling missing data. therefore, we propose a
self-attention based fusion block (sfusion) to tackle the problems of the above
fusion strategies. as shown in fig. 1(d), sfusion can handle any number of input
data instead of fixing its number. in addition, sfusion is a learning-based
fusion strategy that consists of two components: the correlation extraction (ce)
module and the modal attention (ma) module. in the ce module, feature
representations extracted from available modalities are projected as tokens and
fed into the self-attention layers to learn multimodal correlations. based on
these correlations, a modal softmax function is proposed to generate weight maps
in the ma module. finally, it builds a shared feature representation by fusing
the varying inputs with the weight maps.the contributions of this work are:-we
propose sfusion, which is a data-dependent fusion strategy without impersonating
missing modalities. it can learn the latent correlations between different
modalities and builds a shared representation adaptively. -the sfusion is not
limited to specific deep learning architectures. it takes inputs from any kind
of upstream processing model and serves as the input of the downstream decision
model, which enables applying the sfusion to various backbone networks for
different tasks. -we provide qualitative and quantitative performance
evaluations on activity recognition with the shl [22] dataset and brain tumor
segmentation with the brats2020 [1] dataset. the results show the superiority of
sfusion over competing fusion strategies.",2
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,3.1,Datasets,"shl2019. the shl (sussex-huawei locomotion) challenge 2019 [22] dataset provides
data from seven sensors of a smartphone to recognize eight modes of locomotion
and transportation (activities), including still, walking, run, bike, car, bus,
train, and subway. the sensor data are collected from smartphones of a person
with four locations, including the bag, trousers front pocket, breast pocket and
hand. each location is called ""bag"", ""hips"", ""torso"", and ""hand"", respectively.
data acquired from the locations except the ""hand"" are given in the train
subset, while the validation subset provides the data of all four locations. in
the test subset, only unlabeled ""hand"" location data are available.brats2020.
the brats2020 [1] dataset provide four modality scans: t1ce, t1, t2, flair for
brain tumor segmentation. it contains 369 subjects. to better represent the
clinical application tasks, there are three mutually inclusive tumor regions:
the enhancing tumor (et), the tumor core (tc), and the whole tumor (wt) [1]. we
select 70% data as training data, while 10% and 20% as validation and test data
respectively. to prevent overfitting, two data augmentation techniques (randomly
flip the axes and rotate with a random angle in [-10 • , 10 • ]) are applied
during training. we apply z-score normalization [15] to the volumes individually
and randomly crop 128×128×128 patches as inputs to the networks.",2
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,0.0,GFF.,"in the experiments on brain tumor segmentation, we compare sfusion with a gated
feature fusion block (gff) [5], which belongs to the convolution strategy (shown
in fig. 1(c)). as shown in fig. 4 (b), a feature disentanglement architecture is
employed. multimodal medical images are decomposed into the modality-invariant
content and the modality-specific appearance code by encoders e c and e a ,
respectively. the content codes (e.g., c 2 and c 3 , shown in fig. 4 (b)) of
missing modalities are simulated with zero values. then, all content codes are
fused into a shared representation c s by gff. given c s , the tumor
segmentation results are generated by the decoder d s . for a fair comparison,
we adopt the same encoders (e c i and e a i ) and decoders (d s and d r i ) as
used in [5]. we obtain the performance of our fusion strategy by replacing gff
with sfusion and removing the zero-padding operation. the training max_epoch is
set to 200. following [5] setting, the batch size is set to 1. adam [16] is
utilized with a learning rate of 1 × 10 -4 and progressively multiplies it by (1
-epoch / max_epoch) 0.9 . losses of l kl , l rec and l seg are employed as [5].
during training, to simulate real missing modalities scenarios, each training
patient's data is fixed to one of 15 possible missing cases. for a comprehensive
evaluation, we test the performance of all 15 cases for each test patient.our
implementations are on an nvidia rtx 3090(24g) with pytorch 1.8.1.",2
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,3.3,Results,"activity recognition. we compare sfusion with the embracenet [9] on shl2019. as
shown in table 1, we also compare the results of other fusion methods, which use
the same processing (p) model and decision (d) model as [9]. (1) in the early
fusion method, the data of seven sensors are concatenated along their c
dimension. the prediction results are obtained by inputting the concatenation
into a network of p and d in series. (2) for the intermediate fusion approach,
the embracenet is replaced with the concatenation of feature representations
along their r f dimension. (3) in the late fusion method, an independent network
of p and d in series is trained for each sensor, and then the decision is made
from the averaged softmax outputs. (4) in the confidence fusion model, the
embracenet is replaced with the confidence calculation and fusion layers in [7].
the results of different fusion methods on the validation data are presented in
table 1. our proposed sfusion outperforms the embracenet in all four smartphone
locations and improves the overall accuracy from 65.22% to 67.47%.brain tumor
segmentation. the quantitative segmentation results are shown in table 2.
compared with gff, the network integrated with sfusion achieves better average
performance over the 15 possible combinations in all three tasks. in particular,
sfusion outperforms gff for all the possible combinations in tc segmentation.
overall, sfusion achieves better dice scores in most situations (13,15,13
situations for wt, tc and et segmentation, respectively). in addition, we
conduct the statistical significance analysis. the number of situations with
significant improvement are 6, 10 and 8 for wt, tc and et, respectively. it is
provided by a wilcoxon test (p-values < 0.05). besides, we find no significant
drop in performance caused by sfusion. in addition, we compare the sf_fdgf
(where gff is replaced by sfusion) with current state-of-the-art methods. table
4 presents the average dice of 15 situations. for a fair comparison, we conduct
experiments on brats2018, adopt the same data partition as [24], and cite the
results in [24]. sf_fdgf achieves the best performance and verifies the
effectiveness of the sfusion.ablation experiments. the correlation extraction
(ce) module and the modal attention (ma) module are two key components in
sfusion. we evaluate the sfusion without ce and ma, respectively. sfusion
without ce denotes that feature representations are directly fed into the ma
module (fig. 2). sfusion without ma means that we directly add the calculated
feature representations (i ) up to get the fusion result. as shown in table 1,
we can find that sfusion without ce performs worse than other methods. compared
with embracenet, the improvement of sfusion without ma is inconspicuous. as
shown in table . 3, we present the averaged performance over the 15 possible
combinations on brats2020. it shows that both the ce and ma module lead to
performance improvement across all the tumor regions. therefore, ablation
experiments on two different tasks show that both ce and ma play an important
role in sfusion.",2
SFusion: Self-attention Based N-to-One Multimodal Fusion Block,4.0,Conclusion,"in this paper, we propose a self-attention based n-to-one fusion block sfusion
to tackle the problem of multimodal missing modalities fusion. as a
data-dependent fusion strategy, sfusion can automatically learn the latent
correlations between different modalities and builds a shared feature
representation. the entire fusion process is based on available data without
simulating missing modalities. in addition, sfusion has compatibility with any
kind of upstream processing model and downstream decision model, making it
universally applicable to different tasks. we show that it can be integrated
into existing backbone networks by replacing their fusion operation or block to
improve activity recognition and achieve brain tumor segmentation performance.
in particular, by integrating with sfusion, sf_fdgf achieves the
state-of-the-art performance. in the future, we will explore other tasks related
to variable multimodal fusion with sfusion.",2
VISA-FSS: A Volume-Informed Self Supervised Approach for Few-Shot 3D Segmentation,1.0,Introduction,"automated image segmentation is a fundamental task in many medical imaging
applications, such as diagnosis [24], treatment planning [6], radiation therapy
planning, and tumor resection surgeries [7,12]. in the current literature,
numerous fully-supervised deep learning (dl) methods have become dominant in the
medical image segmentation task [5,16,18]. they can achieve their full potential
when trained on large amounts of fully annotated data, which is often
unavailable in the medical domain. medical data annotation requires expert
knowledge, and exhaustive labor, especially for volumetric images [17].
moreover, supervised dl-based methods are not sufficiently generalizable to
previously unseen classes.to address these limitations, few-shot segmentation
(fss) methods have been proposed [21][22][23]25], that segment an unseen class
based on just a few annotated samples. the main fss approaches use the idea of
meta-learning [9,11,13] and apply supervised learning to train a few-shot model.
however, to avoid overfitting and improve the generalization capability of fss
models, they rely on a large number of related tasks or classes. this can be
challenging as it may require a large amount of annotated data, which may not
always be available. although some works on fss techniques focus on training
with fewer data [4,20,26], they require re-training before applying to unseen
classes. to eliminate the need for annotated data during training and
re-training on unseen classes, some recent works have proposed self-supervised
fss methods for 3d medical images which use superpixel-based pseudo-labels as
supervision during training [8,19]. these methods design their self-supervised
tasks (support-query pairs) by applying a predefined transformation (e.g.,
geometric and intensity transformation) on a support image (i.e., a random slice
of a volume) to synthetically form a query one. thus, these methods do not take
into account intra-volume information and context that may be important for the
accurate segmentation of volumetric images during inference.we propose a novel
volume-informed self-supervised approach for few-shot 3d segmentation
(visa-fss). generally, visa-fss aims to exploit information beyond 2d image
slices by learning inter-slice information and continuous shape changes that
intrinsically exists among consecutive slices within a 3d image. to this end, we
introduce a novel type of self-supervised tasks (see sect. 2.2) that builds more
varied and realistic self-supervised fss tasks during training. besides of
generating synthetic queries (like [19] by applying geometric or intensity
transformation on the support images), we also utilize consecutive slices within
a 3d volume as support and query images. this novel type of task generation (in
addition to diversifying the tasks) allows us to present a 2.5d loss function
that enforces mask continuity between the prediction of adjacent queries. in
addition, to provide pseudo-labels for consecutive slices, we propose the
superpixel propagation strategy (spps). it propagates the superpixel of a
support slice into query ones by using flow field vectors that exist between
adjacent slices within a 3d image. we then introduce a novel strategy for
volumetric segmentation during inference that also exploits inter-slice
information within query volumes. it propagates a segmentation mask among
consecutive slices using the few-shot segmenter trained by visa-fss.
comprehensive experiments demonstrate the superiority of our method against
state-of-the-art fss approaches.",2
ECL: Class-Enhancement Contrastive Learning for Long-Tailed Skin Lesion Classification,1.0,Introduction,"skin cancer is one of the most common cancers all over the world. serious skin
diseases such as melanoma can be life-threatening, making early detection and
treatment essential [3]. as computer-aided diagnosis matures, recent advances
with deep learning techniques such as cnns have significantly improved the
performance of skin lesion classification [7,8]. however, as data-hungry
approaches, deep learning models require large balanced and high-quality
datasets to meet the in scl, head classes are overtreated leading to
optimization concentrating on head classes. by contrast, ecl utilizes the
proxies to enhance the learning of tail classes and treats all classes equally
according to balanced contrastive theory [24]. moreover, the enriched relations
in samples and proxies are helped for better representations.accuracy and
robustness requirements in applications, which is hard to suffice due to the
long-tailed occurrence of diseases in the real-world. long-tailed problem is
usually caused by differences in incidence rate and difficulties in data
collection. some diseases are common while others are rare, making it difficult
to collect balanced data [13]. this will cause the head classes to account for
the majority of the samples and the tail classes only have small portions. thus,
existing public skin datasets usually suffer from imbalanced problems which then
results in class bias of classifier, for example, poor model performance
especially on tail lesion types.to tackle the challenge of learning unbiased
classifiers with imbalanced data, many previous works focus on three main ideas,
including re-sampling data [1,18], re-weighting loss [2,15,22] and re-balancing
training strategies [10,23]. resampling methods over-sample tail classes or
under-sample head classes, reweighting methods adjust the weights of losses on
class-level or instance-level, and re-balancing methods decouple the
representation learning and classifier learning into two stages or assign the
weights between features from different sampling branches [21]. despite the
great results achieved, these methods either manually interfere with the
original data distribution or improve the accuracy of minority classes at the
cost of reducing that of majority classes [12,13].recently, contrastive learning
(cl) methods pose great potential for representation learning when trained on
imbalanced data [4,14]. among them, supervised contrastive learning (scl) [11]
aggregates semantically similar samples and separates different classes by
training in pairs, leading to impressive success in long-tailed classification
of both natural and medical images [16]. however, there still remain some
defects: (1) current scl-based methods utilize the information of minority
classes insufficiently. since tail classes are sampled with low probability,
each training mini-batch inherits the long-tail distribution, making parameter
updates less dependent on tail classes. (2) scl loss focuses more on optimizing
the head classes with much larger gradients than tail classes, which means tail
classes are all pushed farther away from heads [24]. (3) most methods only
consider the impact of sample size (""imbalanced data"") on the classification
accuracy of skin diseases, while ignoring the diagnostic difficulty of the
diseases themselves (""imbalanced diagnosis difficulty"").to address the above
issues, we propose a class-enhancement contrastive learning (ecl) method for
skin lesion classification, differences between scl and ecl are illustrated in
fig. 1. for sufficiently utilizing the tail data information, we attempt to
address the solution from a proxy-based perspective. a proxy can be regarded as
the representative of a specific class set as learnable parameters. we propose a
novel hybrid-proxy model to generate proxies for enhancing different classes
with a reversed imbalanced strategy, i.e., the fewer samples in a class, the
more proxies the class has. these learnable proxies are optimized with a cycle
update strategy that captures original data distribution to mitigate the quality
degradation caused by the lack of minority samples in a mini-batch. furthermore,
we propose a balanced-hybrid-proxy loss, besides introducing balanced
contrastive learning (bcl) [24]. the new loss treats all classes equally and
utilizes sample-to-sample, proxy-to-sample and proxy-to-proxy relations to
improve representation learning. moreover, we design a balanced-weighted
crossentropy loss which follows a curriculum learning schedule by considering
both imbalanced data and diagnosis difficulty.our contributions can be
summarized as follows: (1) we propose an ecl framework for long-tailed skin
lesion classification. information of classes are enhanced by the designed
hybrid-proxy model with a cycle update strategy. (2) we present a
balanced-hybrid-proxy loss to balance the optimization of each class and
leverage relations among samples and proxies. (3) a new balancedweighted
cross-entropy loss is designed for an unbiased classifier, which considers both
""imbalanced data"" and ""imbalanced diagnosis difficulty"". (4) experimental
results demonstrate that the proposed framework outperforms other
state-of-theart methods on two imbalanced dermoscopic image datasets and the
ablation study shows the effectiveness of each element.",2
SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation,3.1,Experimental Settings,"datasets and pre-trained model. we conducted experiments on automating liver
tumor segmentation in contrast-enhanced ct scans, a crucial task in liver cancer
diagnosis and surgical planning [1]. although there are publicly available liver
tumor datasets [1,24], they only contain major tumor types and differ in image
characteristics and label distribution from our hospital's data. deploying a
model trained from public data to our hospital directly will be
problematic.collecting large-scale data from our hospital and training a new
model will be expensive. therefore, we can use the model trained from them as a
starting point and use slpt to adapt it to our hospital with minimum cost. we
collected a dataset from our in-house hospital comprising 941 ct scans with
eight categories: hepatocellular carcinoma, cholangioma, metastasis,
hepatoblastoma, hemangioma, focal nodular hyperplasia, cyst, and others. it
covers both major and rare tumor types. our objective is to segment all types of
lesions accurately. we utilized a pre-trained model for liver segmentation using
supervised learning on two public datasets [24] with no data overlap with our
downstream task. the nnunet [12] was used to preprocess and sample the data into
24 × 256 × 256 patches for training. to evaluate the performance, we employed a
5-fold crossvalidation (752 for selection, 189 for test).metrics. we evaluated
lesion segmentation performance using pixel-wise and lesion-wise metrics. for
pixel-wise evaluation, we used the dice per case, a commonly used metric [1].
for lesion-wise evaluation, we first do connected component analysis to
predicted and ground truth masks to extract lesion instances, and then compute
precision and recall per case [20]. a predicted lesion is regarded as a tp if
its overlap with ground truth is higher than 0.2 in dice. competing approaches.
in the prompt tuning experiment, we compared our method with three types of
tuning: full parameter update (fine-tuning, learn-from-scratch), partial
parameter update (head-tuning, encoder-tuning, decoder-tuning), and prompt
update (spm [17]). in the unsupervised diversity selection experiment, we
compared our method with random sampling. in the supervised uncertainty
selection experiment, we compared our method with random sampling, diversity
sampling (coreset [22], corecgn [6]), and uncertainty sampling (entropy, mc
dropout [10], ensemble [4], uncertaingcn [6], ent-gn [26]). unlike ensemble, our
method was on multi-prompt-based heads. furthermore, unlike ent-gn, which
computed the entropy-based gradient from a single prediction, we calculated a
stable entropy from the muti-prompt-based mean predictions and solely considered
the prompt gradient.training setup. we conducted the experiments using the
pytorch framework on a single nvidia tesla v100 gpu. the nnunet [12] framework
was used for 3d lesion segmentation with training 500 epochs at an initial
learning rate of 0.01. we integrated 13 fpus behind each upsampling or
downsampling of nnunet, adding only 2.7m parameters. during training, we set k =
3 and employed diverse data augmentation techniques such as scale, elastic,
rotation, and mirror. three sets of tl parameters is (α 1,2,3 = 0.5,0.7,0.3, β
1,2,3 = 0.5,0.3,0.7). to ensure fairness and eliminate model ensemble effects,
we only used the model's prediction with k = 1 during testing. we used fixed
random seeds and 5-fold cross-validation for all segmentation experiments.",2
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,1.0,Introduction,"recent years have witnessed the remarkable success of deep learning in medical
image segmentation. however, although the performance of deep learning models
even surpasses the accuracy of human exports on some segmentation tasks, two
challenges still persist. (1) different segmentation tasks are usually tackled
separately by specialized networks (see fig. 1(a)), leading to distributed
research efforts. (2) most segmentation tasks face the limitation of a small
labeled dataset, especially for 3d segmentation tasks, since pixel-wise 3d image
annotation is labor-intensive, time-consuming, and susceptible to operator bias.
train one model on n datasets using task-specific prompts. we use purple to
highlight where to add the task-related information.several strategies have been
attempted to address both challenges. first, multi-head networks (see fig. 1(b))
were designed for multiple segmentation tasks [4,7,21]. a typical example is
med3d [4], which contains a shared encoder and multiple task-specific decoders.
although they benefit from the encoder parameter-sharing scheme and the rich
information provided by multiple training datasets, multi-head networks are
less-suitable for multi-task co-training, due to the structural redundancy
caused by the requirement of preparing a separate decoder for each task. the
second strategy is the multi-class model, which formulates multiple segmentation
tasks into a multi-class problem and performs it simultaneously. to achieve
this, the clip-driven universal model [16] (see fig. 1(c)) introduces the text
embedding of all labels as external knowledge, obtained by feeding medical
prompts to clip [5]. however, clip has limited ability to generalize in medical
scenarios due to the differences between natural and medical texts. it is
concluded that the discriminative ability of text prompts is weak in different
tasks, and it is difficult to help learn task-specific semantic information. the
third strategy is dynamic convolution. dodnet [29] and its variants [6,17,25]
present universal models, which can perform different segmentation tasks based
on using task encoding and a controller to generate dynamic convolutions (see
fig. 1(d)). the limitations of these models are two-fold. (1) different tasks
are encoded as one-hot vectors, which are mutually orthogonal, ignoring the
correlations among tasks. (2) the task-related information (i.e., dynamic
convolution parameters) is introduced at the end of the decoder. it may be too
late for the model to be 'aware' of the ongoing task, making it difficult to
decode complex targets.in this paper, we propose a prompt-driven universal
segmentation model (uniseg) to segment multiple organs, tumors, and vertebrae on
3d medical images with diverse modalities and domains. uniseg contains a vision
encoder, a fusion and selection (fuse) module, and a prompt-driven decoder. the
fuse module is devised to generate the task-specific prompt, which enables the
model to be 'aware' of the ongoing task (see fig. 1(e)). specifically, since
prompt learning has a proven ability to represent both task-specific and
task-invariant knowledge [24], a learnable universal prompt is designed to
describe the correlations among tasks. then, the universal prompt and the
features extracted by the vision encoder are fed to the fuse module to generate
task prompts for all tasks. the task-specific prompt is selected according to
the ongoing task. moreover, to introduce the prompt information to the model
early, we move the task-specific prompt from the end of the decoder to the start
of the decoder (see fig. 2). thanks to both designs, we can use a single decoder
and a segmentation head to predict various targets under the supervision of the
corresponding ground truths. we collected 3237 volumetric data with three
modalities (ct, mr, and pet) and various targets (eight organs, vertebrae, and
tumors) from 11 datasets as the upstream dataset. on this dataset, we evaluated
our uniseg model against other universal models, such as dodnet and the
clip-driven universal model. we also compared uniseg to seven advanced
single-task models, such as cotr [26], nnformer [30], and nnunet [12], which are
trained independently on each dataset. furthermore, to verify its generalization
ability on downstream tasks, we applied the trained uniseg to two downstream
datasets and compared it to other pre-trained models, such as mg [31], desd
[28], and unimiss [27]. our results indicate that uniseg outperforms all
competing methods on 11 upstream tasks and two downstream tasks.our
contributions are three-fold: (1) we design a universal prompt to describe the
correlations among different tasks and use it to generate task prompts for all
tasks. (2) we utilize the task-related prompt information as the input of the
decoder, facilitating the training of the whole decoder, instead of just the
last few layers. (3) the proposed uniseg can be trained on and applied to
various 3d medical image tasks with diverse modalities and domains, providing a
highquality pre-trained 3d medical image segmentation model for the community.",3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,3.1,Datasets and Evaluation Metric,"datasets. for this study, we collected 11 medical image segmentation datasets as
the upstream dataset to train our uniseg and single-task models. the liver and
kidney datasets are from lits [3] and kits [11], respectively. the hepatic
vessel (hepav), pancreas, colon, lung, and spleen datasets are from medical
segmentation decathlon (msd) [1]. verse20 [19], prostate [18], brats21 [2], and
autopet [8] datasets have annotations of the vertebrae, prostate, brain tumors,
and whole-body tumors, respectively. we used the binary version of the verse20
dataset, where all foreground classes are regarded as one class. moreover, we
dropped the samples without tumors in the autopet dataset. meanwhile, we use
btcv [14] and vs datasets [20] as downstream datasets to verify the ability of
uniseg to generalize to other medical image segmentation tasks. btcv contains
the annotations of 13 abdominal organs, including the spleen (sp), right kidney
(rki), left kidney (lki), gallbladder (gb), esophagus (es), liver (li), stomach
(st), aorta(ao), inferior vena cava (ivc), portal vein and splenic vein (psv),
pancreas (pa), right adrenal gland (rag), and left adrenal gland (lag). the vs
dataset contains the annotations of the vestibular schwannoma. more details are
shown in table 1.evaluation metric. the dice similarity coefficient (dice) that
measures the overlap region of the segmentation prediction and ground truth is
employed to evaluate the segmentation performance.",3
UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner,4.0,Conclusion,"this study proposes a universal model called uniseg (a single model) to perform
multiple organs, tumors, and vertebrae segmentation on images with multiple
modalities and domains. to solve two limitations existing in preview universal
models, we design the universal prompt to describe correlations among all tasks
and make the model 'aware' of the ongoing task early, boosting the training of
the whole decoder instead of just the last few layers. thanks to both designs,
our uniseg achieves superior performance on 11 upstream datasets and two
downstream datasets, setting a new record. in our future work, we plan to design
a universal model that can effectively process multiple dimensional data.",3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,1.0,Introduction,"histopathological image analysis is an important step towards cancer diagnosis.
however, shortage of pathologists worldwide along with the complexity of
histopathological data make this task time consuming and challenging. therefore,
developing automatic and accurate histopathological image analysis methods that
leverage recent progress in deep learning has received significant attention in
recent years. in this work, we investigate the problem of diagnosing colorectal
cancer, which is one of the most common reason for cancer deaths around the
world and particularly in europe and america [23].existing deep learning-based
colorectal tissue classification methods [18,21,22] typically require large
amounts of annotated histopathological training data for all tissue types to be
categorized. however, obtaining large amount of training data is challenging,
especially for rare cancer tissues. to this end, it is desirable to develop a
few-shot colorectal tissue classification method, which can learn from seen
tissue classes having sufficient training data, and be able to transfer this
knowledge to unseen (novel) tissue classes having only a few exemplar training
images.while generative adversarial networks (gans) [6] have been utilized to
synthesize images, they typically need to be trained using large amount of real
images of the respective classes, which is not feasible in aforementioned
few-shot setting. therefore, we propose a few-shot (fs) image generation
approach for generating high-quality and diverse colorectal tissue images of
novel classes using limited exemplars. moreover, we demonstrate the
applicability of these generated images for the challenging problem of fs
colorectal tissue classification.",3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,2.0,Related Work,"the ability of generative models [6,15] to fit to a variety of data
distributions has enabled great strides of advancement in tasks, such as image
generation [3,12,13,19], and so on. despite their success, these generative
models typically require large amount of data to train and avoid overfitting. in
contrast, fewshot (fs) image generation approaches [2,4,7,9,16] strive to
generate natural images from disjoint novel categories from the same domain as
in the training. existing fs natural image generation approaches can be broadly
divided into three categories based on transformation [1], optimization [4,16]
and fusion [7,9,10]. the transformation-based approach learns to perform
generalized data augmentations to generate intra-class images from a single
conditional image. on the other hand, optimization-based approaches typically
utilize meta-learning techniques to adapt to a different image generation task
by optimizing on a few reference images from the novel domain. different from
these two paradigms that are better suited for simple image generation task,
fusion-based approaches first aggregate latent features of reference images and
then employ a decoder to generate same class images from these aggregated
features.our approach: while the aforementioned works explore fs generation in
natural images, to the best of our knowledge, we are the first to investigate fs
generation in colorectal tissue images. in this work, we look into multi-class
colorectal tissue analysis problem, with low and high-grade tumors included in
the set. the corresponding dataset [14] used in this study is widely employed
for multi-class texture classification in colorectal cancer histology and
comprises eight types of tissue: tumor epithelium, simple stroma, complex
stroma, immune cells, debris, normal mucosal glands, adipose tissue and
background (no tissue). generating colorectal tissue images of these diverse
categories is a challenging task, especially in the fs setting. generating
realistic and diverse tissue images require ensuring both global and local
texture consistency (patterns). our xm-gan densely aggregates features [5,20]
from all relevant local regions of the reference tissue images at a
global-receptive field along with a controllable mechanism for modulating the
tissue image features by utilizing meta-weights computed from the input
reference tissue image features. as a result, this leads to high-quality yet
diverse colorectal tissue image generation in fs setting.",3
Cross-Modulated Few-Shot Image Generation for Colorectal Tissue Classification,4.0,Experiments,"we conduct experiments on human colorectal cancer dataset [14]. the dataset
consist of 8 categories of colorectal tissues, tumor, stroma, lymph, complex,
debris, mucosa, adipose, and empty with 625 per categories. to enable few-shot
setting, we split the 8 categories into 5 seen (for training) and 3 unseen
categories (for evaluation) with 40 images per category. we evaluate our
approach using two metrics: frèchet inception distance (fid) [8] and learned
perceptual image patch similarity (lpips) [24]. our encoder f e and decoder f d
both have five convolutional blocks with batch normalization and leaky-relu
activation, as in [7]. the input and generated image size is 128 × 128. the
linear transformation ψ(•) is implemented as a 1 × 1 convolution with input and
output channels set to d. the weights η p and η cl are set to 50 and 1. we set k
= 3 in all the experiments, unless specified otherwise. our xm-gan is trained
with a batchsize of 8 using the adam optimizer and a fixed learning rate of 10
-4 .",3
Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.1,Materials,"we used the public covid-19 segmentation benchmark [15] to verify the proposed
uci. it is collected from two public resources [5,8] on chest ct images
available on the cancer imaging archive (tcia) [4]. all ct images were acquired
without intravenous contrast enhancement from patients with positive reverse
transcription polymerase chain reaction (rt-pcr) for sars-cov-2. in total, we
used 199 ct images including 149 training images and 50 test images. we also
used two chest x-ray-based classification datasets including chestx-ray14 [18]
and chestxr [1] to assist the uci training. the chestx-ray14 dataset comprises
112,120 x-ray images showing positive cases from 30,805 patients, encompassing
14 disease image labels pertaining to thoracic and lung ailments. an image may
contain multiple or no labels. the chestxr dataset consists of 21,390 samples,
with each sample classified as healthy, pneumonia, or covid-19.",3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,1.0,Introduction,"instances with complex shapes arise in many biomedical domains, and their
morphology carries critical information. for example, the structure of gland
tissues in microscopy images is essential in accessing the pathological stages
for cancer diagnosis and treatment. these instances, however, are usually
closely in touch with each other and have non-convex structures with parts of
varying widths (fig. 1a), posing significant challenges for existing
segmentation methods.in the biomedical domain, most methods [3,4,13,14,22] first
learns intermediate representations and then convert them into masks with
standard segmentation algorithms like connected-component labeling and watershed
transform. these representations are not only efficient to predict in one model
forward pass but also able to capture object geometry (i.e., precise instance
boundary), which are hard for top-down methods using low-resolution features for
mask generation. however, existing representations have several restrictions.
for example, boundary map is usually learned as a pixel-wise binary
classification task, which makes the model conduct relatively local predictions
and consequently become vulnerable to small errors that break the connectivity
between adjacent instances (fig. 1b). to improve the boundary map, deep
watershed transform (dwt) [1] predicts the euclidean distance transform (dt) of
each pixel to the instance boundary. this representation is more aware of the
structure for convex objects, as the energy value for centers is significantly
different from pixels close to the boundary. however, for objects with
non-convex morphology, the boundary-based distance transform produces multiple
local optima in the energy landscape (fig. 1c), which tends to break the
intra-instance connectivity when applying thresholding and results in
over-segmentation.to preserve the connectivity of instances while keeping the
precise instance boundary, in this paper, we propose a novel representation
named skeleton-aware distance transform (sdt). our sdt incorporate object
skeleton, a concise and connectivity-preserving representation of object
structure, into the traditional boundary-based distance transform (dt) (fig.
1d). in quantitative evaluations, we show that our proposed sdt achieves leading
performance on histopathology image segmentation for instances with various
sizes and complex structures. specifically, under the hausdorff distance for
evaluating shape similarity, our approach improves the previous state-of-the-art
method by relatively 10.6%.",3
Structure-Preserving Instance Segmentation via Skeleton-Aware Distance Transform,3.1,Histopathology Instance Segmentation,"accurate instance segmentation of gland tissues in histopathology images is
essential for clinical analysis, especially cancer diagnosis. the diversity of
object appearance, size, and shape makes the task challenging.dataset and
evaluation metric. we use the gland segmentation challenge dataset [17] that
contains colored light microscopy images of tissues with a wide range of
histological levels from benign to malignant. there are 85 and 80 images in the
training and test set, respectively, with ground truth annotations provided by
pathologists. according to the challenge protocol, the test set is further
divided into two splits with 60 images of normal and 20 images of abnormal
tissues for evaluation. three evaluation criteria used in the challenge include
instance-level f1 score, dice index, and hausdorff distance, which measure the
performance of object detection, segmentation, and shape similarity,
respectively. for the instance-level f1 score, an iou threshold of 0.5 is used
to decide the correctness of a prediction.",3
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4.1,Data Sets and Implementation Details,"we adopted large-scale data sets task07 pancreas from medical segmentation
decathlon (msd) [1] as used in [12] for architecture searching, and kits'19
[13,14] to validate the searched architectures. both are very challenging
applications involving various fields of view of ct volumes and different types
of pathology. the pancreas data set has 3-class segmentation labels (background,
pancreas and tumor) for 282 ct volumes. we adopt entire labeled set for nas with
the same data split as [12]: 114 volumes for model training, 114 volumes for
architecture search, and 54 volumes for model validation. a different 4 : 1 data
split is used for experiments of training from scratch. the kits'19 data set has
3-class segmentation labels (background, kidney and tumor) for 210 ct volumes,
and an additional standalone 90 test volumes (with hidden ground truth) for the
public leaderboard. we train the searched models with 5-fold data split, and
verify the model performance on the test set using the public leaderboard. all
data sets are re-sampled into the isotropic voxel spacing 1.0 mm for both images
and labels. for both ct data sets, the voxel intensities of the images are
normalized to the range [0, 1] according to the 5 th and 95 th percentile of
overall foreground intensities.a combination of dice loss and cross-entropy loss
is adopted to minimize both global and pixel-wise distance between ground truth
and predictions. the nas is conducted through conventional bi-level optimization
following implementation 1 . we use the same definition of search space with 4
resolution scales and 12 levels. for cell level searching, we only use three
different operations: skip-connection, convolution and transformer. for model
training, we use a large input patch shape 160 3 , and the batch size at each
gpu is 2. the training settings (like data augmentation, optimizer, etc.) are
very similar to the model searching. we keep a constant learning rate 1e -3 to
train the model from scratch for 40, 000 iterations. our experiments are
conducted using monai and trained on eight nvidia v100 gpus with 32 gb of
memory. for searching or training, the time cost is ∼ 15 hours including
training and validation on-the-fly (similar as [12]).",3
DAST: Differentiable Architecture Search with Transformer for 3D Medical Image Segmentation,4.3,KiTS'19 Experiments,"to verify the effectiveness and generalization of our searched architectures
from dast, we validate the searched architecture (from pancreas data set) on
this challenging task. metrics for kidneys and tumors are the average dice score
per case. finally, we evaluate our single-fold model as well as the ensemble
from 5 cross-validation models on the public test leaderboard2 .table 1. kits'19
challenge test-set performance evaluation for kidney and tumor segmentation in
terms of the average dice score per case. the evaluation results of our method
are copied directly from the public leaderboard. based on the results from the
public leaderboard in table . 1, our single-fold model and ensemble of five
models achieve excellent performance compared to all other entries in the
challenge shown in the table . 1. the nnu-net [16] is the best among all other
entries, but the method utilized 20 u-net models with training strategies to
achieve the ensemble result for the challenge. some other entries rely on
cascaded models, which use more complex and intensive training mechanisms. on
the contrary, dast shows great simplicity when transferring a searched
architecture to a new task. it is important to point out that the performance of
our models is not only the best of all entries with publications, but also the
best of all public entries (around 2, 000) on the test leaderboard.",3
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,1.0,Introduction,"the success of deep neural networks heavily relies on the availability of large
and diverse annotated datasets across a range of computer vision tasks. to learn
a strong data representation for robust and performant medical image
segmentation, huge datasets with either many thousands of annotated data
structures or less specific self-supervised pretraining objectives with
unlabeled data are needed [29,33]. the annotation of 3d medical images is a
difficult and laborious task. thus, depending on the task, only a bare minimum
of images and target structures is usually annotated. this results in a
situation where a zoo of partially labeled datasets is available to the
community. recent efforts have resulted in a large dataset of >1000 ct images
with >100 annotated classes each, thus providing more than 100,000 manual
annotations which can be used for pre-training [30]. focusing on such a dataset
prevents leveraging the potentially precious additional information of the above
mentioned other datasets that are only partially annotated. integrating
information across different datasets potentially yields a higher variety in
image acquisition protocols, more anatomical target structures or details about
them as well as information on different kinds of pathologies. consequently,
recent advances in the field allowed utilizing partially labeled datasets to
train one integrated model [21]. early approaches handled annotations that are
present in one dataset but missing in another by considering them as background
[5,27] and penalizing overlapping predictions by taking advantage of the fact
that organs are mutually exclusive [7,28]. some other methods only predicted one
structure of interest for each forward pass by incorporating the class
information at different stages of the network [4,22,31]. chen et al. trained
one network with a shared encoder and separate decoders for each dataset to
generate a generalized encoder for transfer learning [2]. however, most
approaches are primarily geared towards multi-organ segmentation as they do not
support overlapping target structures, like vessels or cancer classes within an
organ [6,8,12,23]. so far, all previous methods do not convincingly leverage
cross-dataset synergies. as liu et al. pointed out, one common caveat is that
many methods force the resulting model to average between distinct annotation
protocol characteristics [22] by combining labels from different datasets for
the same target structure (visualized in fig. 1 b)). hence, they all fail to
reach segmentation performance on par with cutting-edge single dataset
segmentation methods. to this end, we introduce multitalent (multi dataset
learning and pre-training), a new, flexible, multi-dataset training method: 1)
multitalent can handle classes that are absent in one dataset but annotated in
another during training. 2) it retains different annotation protocol
characteristics for the same target structure and 3) allows for overlapping
target structures with different level of detail such as liver, liver vessel and
liver tumor. overall, mul-titalent can include all kinds of new datasets
irrespective of their annotated target structures.multitalent can be used in two
scenarios: first, in a combined multi-dataset (md) training to generate one
foundation segmentation model that is able to predict all classes that are
present in any of the utilized partially annotated datasets, and second, for
pre-training to leverage the learned representation of this foundation model for
a new task. in experiments with a large collection of abdominal ct datasets, the
proposed model outperformed state-of-the-art segmentation networks that were
trained on each dataset individually as well as all previous methods that
incorporated multiple datasets for training. interestingly, the benefits of
multitalent are particularly notable for more difficult classes and pathologies.
in comparison to an ensemble of single dataset solutions, multitalent comes with
shorter training and inference times. additionally, at the example of three
challenging datasets, we demonstrate that fine-tuning multitalent yields higher
segmentation performance than training from scratch or initializing the model
parameters using unsupervised pretraining strategies [29,33]. it also surpasses
supervised pretrained and fine-tuned state-of-the art models on most tasks,
despite requiring orders of magnitude less annotations during pre-training.",3
MultiTalent: A Multi-dataset Approach to Medical Image Segmentation,3.0,Results,"multi-dataset training results are presented in fig. 2. in general, the
convolutional architectures clearly outperform the transformer-inspired
swinunetr. multitalent improves the performance of the purely convolutional
architectures (u-net and resenc u-net) and outperforms the corresponding
baseline models that were trained on each dataset individually. since a simple
average over all classes would introduce a biased perception due to the highly
varying numbers of images and classes, we additionally report an average over
all datasets. for example, dataset 7 consists of only 30 training images but has
13 classes, whereas 4 in the appendix provides all results for all classes.
averaged over all datasets, the multitalent gains 1.26 dice points for the
resenc u-net architecture and 1.05 dice points for the u-net architecture.
compared to the default nnu-net, configured without manual intervention for each
dataset, the improvements are 1.56 and 0.84 dice points. additionally, in fig. 2
we analyzed two subgroups of classes. the first group includes all ""difficult""
classes for which the default nnu-net has a dice smaller than 75 (labeled by a
""d"" in table 4 in the appendix). the second group includes all cancer classes
because of their clinical relevance. both class groups, but especially the
cancer classes, experience notable performance improvements from multitalent.
for the official btcv test set in table 1, multitalent outperforms all related
work that have also incorporated multiple datasets during training, proving that
multitalent is substantially superior to related approaches. the advantages of
multitalent include not only better segmentation results, but also considerable
time savings for training and inference due to the simultaneous prediction of
all classes. the training is 6.5 times faster and the inference is around 13
times faster than an ensemble of models trained on 13 datasets.transfer learning
results are found in table 2, which compares the finetuned 5-fold
cross-validation results of different pre-training strategies for three
different models on three datasets. the multitalent pre-training is highly
beneficial for the convolutional models and outperforms all unsupervised
baselines. although multitalent was trained with a substantially lower amount of
manually annotated structures ( ˜3600 vs. ˜10 5 annotations), it also exceeds
the supervised pre-training baseline. especially for the small multi-organ
dataset, which only has 30 training images (btcv), and for the kidney tumor
(kits19), the multi-talent pre-training boosts the segmentation results. in
general, the results show that supervised pre-training can be beneficial for the
swinunetr as well, but pre-training on the large totalsegmentator dataset works
better than the md pre-training. for the amos dataset, no pre-training scheme
has a substantial impact on the performance. we suspect that it is a result of
the dataset being saturated due to its large number of training cases. the
resenc u-net pretrained with multitalent, sets a new state-of-the-art on the
btcv leaderboard1 (table 1). allows including any publicly available datasets
(e.g. amos and totalsegmentator). this paves the way towards holistic whole body
segmentation model that is even capable of handling pathologies.",3
Co-assistant Networks for Label Correction,3.1,Experimental Settings,"the used datasets are breakhis [13], isic [3], and nihcc [16]. breakhis consists
of 7,909 breast cancer histopathological images including 2,480 benigns and
5,429 malignants. we randomly select 5,537 images for training and 2,372 ones
for testing. isic has 12,000 digital skin images where 6,000 are normal and
6,000 are with melanoma. we randomly choose 9,600 samples for training and the
remaining ones for testing. nihcc has 10,280 frontal-view x-ray images, where
5,110 are normal and 5,170 are with lung diseases. we randomly select 8,574
images for training and the rest of images for testing. in particular, the
random selection in our experiments guarantees that three datasets (i.e., the
training set, the testing set, and the whole set) have the same ratio for each
class. moreover, we assume that all labels in the used raw datasets are clean,
so we add corrupted labels with different noise rates = {0, 0.2, 0.4} into these
datasests, where = 0 means that all labels in the training set are clean.we
compare our proposed method with six popular methods, including one fundamental
baseline (i.e., cross-entropy (ce)), three robustness-based methods (i.e.,
co-teaching (ct) [6], nested co-teaching (nct) [2] and self-paced resistance
learning (sprl) [12]), and two label correction methods (i.e., co-correcting
(cc) [9] and self-ensemble label correction (selc) [11]). for fairness, in our
experiments, we adopt the same neural network for all comparison methods based
on their public codes and default parameter settings. we evaluate the
effectiveness of all methods in terms of four evaluation metrics, i.e.,
classification accuracy (acc), specificity (spe), sensitivity (sen) and area
under the roc curve (auc).",3
Pre-trained Diffusion Models for Plug-and-Play Medical Image Enhancement,1.0,Introduction,"computed tomography (ct) and magnetic resonance (mr) are two widely used imaging
techniques in clinical practice. ct imaging uses x-rays to produce detailed,
cross-sectional images of the body, which is particularly useful for imaging
bones and detecting certain types of cancers with fast imaging speed. however,
ct imaging has relatively high radiation doses that can pose a risk of radiation
exposure to patients. low-dose ct techniques have been developed to address this
concern by using lower doses of radiation, but the image quality is degraded
with increased noise, which may compromise diagnostic accuracy [9].mr imaging,
on the other hand, uses a strong magnetic field and radio waves to create
detailed images of the body's internal structures, which can produce
high-contrast images for soft tissues and does not involve ionizing radiation.
this makes mr imaging safer for patients, particularly for those who require
frequent or repeated scans. however, mr imaging typically has a lower resolution
than ct [18], which limits its ability to visualize small structures or
abnormalities.motivated by the aforementioned, there is a pressing need to
improve the quality of low-dose ct images and low-resolution mr images to ensure
that they provide the necessary diagnostic information. numerous algorithms have
been developed for ct and mr image enhancement, with deep learning-based methods
emerging as a prominent trend [5,14], such as using the conditional generative
adversarial network for ct image denoising [32] and convolutional neural network
for mr image super-resolution (sr) [4].these algorithms are capable of improving
image quality, but they have two significant limitations. first, paired images
are required for training, e.g., low-dose and full-dose ct images;
low-resolution and high-resolution mr images). however, acquiring such paired
data is challenging in real clinical scenarios. although it is possible to
simulate low-quality images from high-quality images, the models derived from
such data may have limited generalization ability when applied to real data
[9,14]. second, customized models are required for each task. for example, for
mr super-resolution tasks with different degradation levels (i.e., 4x and 8x
downsampling), one may need to train a customized model for each degradation
level and the trained model cannot generalize to other degradation levels.
addressing these limitations is crucial for widespread adoption in clinical
practice.recently, pre-trained diffusion models [8,11,21] have shown great
promise in the context of unsupervised natural image reconstruction [6,7,12,28].
however, their applicability to medical images has not been fully explored due
to the absence of publicly available pre-trained diffusion models tailored for
the medical imaging community. the training of diffusion models requires a
significant amount of computational resources and training images. for example,
openai's improved diffusion models [21] took 1600-16000 a100 hours to be trained
on the imagenet dataset with one million images, which is prohibitively
expensive. several studies have used diffusion models for low-dose ct denoising
[30] and mr image reconstruction [22,31], but they still rely on paired
images.in this paper, we aim at addressing the limitations of existing image
enhancement methods and the scarcity of pre-trained diffusion models for medical
images. specifically, we provide two well-trained diffusion models on full-dose
ct images and high-resolution heart mr images, suitable for a range of
applications including image generation, denoising, and super-resolution.
motivated by the existing plug-and-play image restoration methods [26,34,35] and
denoising diffusion restoration and null-space models (ddnm) [12,28], we further
introduce a paradigm for plug-and-play ct and mr image denoising and
super-resolution as shown in fig. 1. notably, it eliminates the need for paired
data, enabling greater scalability and wider applicability than existing
paired-image dependent methods. moreover, it eliminates the need to train a
customized model for each task. our method does not need additional training on
specific tasks and can directly use the single pre-trained diffusion model on
multiple medical image enhancement tasks. the pre-trained diffusion models and
pytorch code of the present method are publicly available at
https://github.com/bowang-lab/ dpm-medimgenhance.",3
Robust T-Loss for Medical Image Segmentation,3.1,Datasets,"the isic 2017 dataset [5] is a well-known public benchmark of dermoscopy images
for skin cancer detection. it contains 2000 training and 600 test images with
corresponding lesion boundary masks. the images are annotated with lesion type,
diagnosis, and anatomical location metadata. the dataset also includes a list of
lesion attributes, such as size, shape, and color. we resized the images to 256
× 256 pixels for our experiments.shenzhen [4,13,25] is a public dataset
containing 566 frontal chest radiographs with corresponding lung segmentation
masks for tuberculosis detection. since there is not a predefined split for
shenzhen as in isic, to ensure representative training and testing sets, we
stratified the images by their tuberculosis and normal lung labels, with 70% of
the data for training and the remaining 30% for testing. resulting in 296
training images and 170 test images. all images were resized to 256 × 256
pixels.without a public benchmark with real noisy and clean segmentation masks,
we artificially inject additional mask noise in these two datasets to test the
model's robustness to low annotation quality. this simulates the real risk of
errors due to factors like annotator fatigue and difficulty in annotating
certain images. in particular, we follow [15], randomly sample a portion of the
training data with probability α ∈ {0.3, 0.5, 0.7}, and apply morphological
transformations with noise levels controlled by β ∈ {0.5, 0.7}1 . the
morphological transformations included erosion, dilation, and affine
transformations, which respectively reduced, enlarged, and displaced the
annotated area.",3
Multi-Head Multi-Loss Model Calibration,1.0,Introduction and Related Work,"when training supervised computer vision models, we typically focus on improving
their predictive performance, yet equally important for safety-critical tasks is
their ability to express meaningful uncertainties about their own predictions
[4]. in the context of machine learning, we often distinguish two types of
uncertainties: epistemic and aleatoric [13]. briefly speaking, epistemic
uncertainty arises from imperfect knowledge of the model about the problem it is
trained to solve, whereas aleatoric uncertainty describes ignorance regarding
the data used for learning and making predictions. for example, if a classifier
has learned to predict the presence of cancerous tissue on a colon
histopathology, and it is tasked with making a prediction on a breast biopsy it
may display epistemic uncertainty, as it was never trained for this problem
[21]. nonetheless, if we ask the model about a colon biopsy with ambiguous
visual content, i.e. a hard-todiagnose image, then it could express aleatoric
uncertainty, as it may not know how to solve the problem, but the ambiguity
comes from the data. this distinction between epistemic and aleatoric is often
blurry, because the presence of one of them does not imply the absence of the
other [12]. also, under strong epistemic uncertainty, aleatoric uncertainty
estimates can become unreliable [31].producing good uncertainty estimates can be
useful, e.g. to identify test samples where the model predicts with little
confidence and which should be reviewed [1]. a straightforward way to report
uncertainty estimates is by interpreting the output of a model (maximum of its
softmax probabilities) as its predictive confidence. when this confidence aligns
with the actual accuracy we say that the model is calibrated [8]. model
calibration has been studied for a long time, with roots going back to the
weather forecasting field [3]. initially applied mostly for binary
classification systems [7], the realization that modern neural networks tend to
predict over-confidently [10] has led to a surge of interest in recent years
[8]. broadly speaking, one can attempt to promote calibration during training,
by means of a post-processing stage, or by model ensembling.training-time
calibration. popular training-time approaches consist of reducing the predictive
entropy by means of regularization [11], e.g. label smoothing [25] or mixup
[30], or loss functions that smooth predictions [26]. these techniques often
rely on correctly tuning a hyper-parameter controlling the trade-off between
discrimination ability and confidence, and can easily achieve better calibration
at the expense of decreasing predictive performance [22]. examples of medical
image analysis works adopting this approach are difference between confidence
and accuracy regularization [20] for medical image diagnosis, or
spatially-varying and margin-based label smoothing [14,27], which extend and
improve label smoothing for biomedical image segmentation tasks.post-hoc
calibration. post-hoc calibration techniques like temperature scaling [10] and
its variants [6,15] have been proposed to correct over or underconfident
predictions by applying simple monotone mappings (fitted on a heldout subset of
the training data) on the output probabilities of the model. their greatest
shortcoming is the dependence on the i.i.d. assumption implicitly made when
using validation data to learn the mapping: these approaches suffer to
generalize to unseen data [28]. other than that, these techniques can be
combined with training-time methods and return compounded performance
improvements.model ensembling. a third approach to improve calibration is to
aggregate the output of several models, which are trained beforehand so that
they have some diversity in their predictions [5]. in deep learning, model
ensembles are considered to be the most successful method to generate meaningful
uncertainty estimates [16]. an obvious weakness of deep ensembles is the
requirement of training and then keeping for inference purposes a set of models,
which results in a computational overhead that can be considerable for larger
architectures. examples of applying ensembling in medical image computing
include [17,24].in this work we achieve model calibration by means of multi-head
models trained with diverse loss functions. in this sense, our approach is
closest to some recent works on multi-output architectures like [21], where a
multi-branch cnn is trained on histopathological data, enforcing specialization
of the different heads by backpropagating gradients through branches with the
lowest loss. compared to our approach, ensuring correct gradient flow to avoid
dead heads requires ad-hoc computational tricks [21]; in addition, no analysis
on model calibration on in-domain data or aleatoric uncertainty was developed,
focusing instead on anomaly detection. our main contribution is a multi-head
model that i) exploits multi-loss diversity to achieve greater confidence
calibration than other learning-based methods, while ii) avoiding the use of
training data to learn post-processing mappings as most post-hoc calibration
methods do, and iii) sidesteping the computation overhead of deep ensembles.",3
Guiding the Guidance: A Comparative Analysis of User Guidance Signals for Interactive Segmentation of Volumetric Images,2.2,Model Backbone and Datasets,"we use the deepedit [11] model with a u-net backbone [15] and simulate a fixed
number of clicks n during training and evaluation. for each volume, n clicks are
iteratively sampled from over-and undersegmented predictions of the model as in
[16] and represented as foreground and background guidance signals. we
implemented our experiments with monai label [23] and will release our code.we
trained and evaluated all of our models on the openly available autopet [1] and
msd spleen [2] datasets. msd spleen [2] contains 41 ct volumes with voxel size
0.79×0.79×5.00mm 3 and average resolution of 512×512×89 voxels with dense
annotations of the spleen. autopet [1] consists of 1014 pet/ct volumes with
annotated tumor lesions of melanoma, lung cancer, or lymphoma. we discard the
513 tumor-free patients, leaving us with 501 volumes. we also only use pet data
for our experiments. the pet volumes have a voxel size of 2.0 × 2.0 × 2.0mm 3
and an average resolution of 400 × 400 × 352 voxels.",3
Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection,0.0,Drawbacks of Transformers:,"recent research has revealed that traditional self-attention mechanisms, while
effective in addressing local feature discrepancies, have a tendency to overlook
important high-frequency information such as texture and edge details [21]. this
is especially problematic for tasks like tumor detection, cancer-type
identification through radiomics analysis, as well as treatment response
assessment, where abnormalities often manifest in texture. moreover,
self-attention mechanisms have a quadratic computational complexity and may
produce redundant features [18].our contributions: ➊ we propose
laplacian-former, a novel approach that includes new efficient attention
(ef-att) consisting of two sub-attention mechanisms: efficient attention and
frequency attention. the efficient attention mechanism reduces the complexity of
self-attention to linear while producing the same output. the frequency
attention mechanism is modeled using a laplacian pyramid to emphasize each
frequency information's contribution selectively. then, a parametric frequency
attention fusion strategy to balance the importance of shape and texture
features by recalibrating the frequency features. these two attention mechanisms
work in parallel. ➋ we also introduce a novel efficient enhancement multi-scale
bridge that effectively transfers spatial information from the encoder to the
decoder while preserving the fundamental features. ➌ our method not only
alleviates the problem of the traditional self-attention mechanism mentioned
above, but also it surpasses all its counterparts in terms of different
evaluation metrics for the tasks of medical image segmentation.",3
Understanding Silent Failures in Medical Image Classification,2.0,Methods,"benchmark for silent failure prevention under distribution shifts. we follow the
spirit of recent robustness benchmarks, where existing datasets have been
enhanced by various distribution shifts to evaluate methods under a wide range
of failure sources and thus simulate real-world application [19,27]. to our
knowledge, no such comprehensive benchmark currently exists in the biomedical
domain. specifically, we introduce corruptions of various intensity levels to
the images in four datasets in the form of brightness, motion blur, elastic
transformations and gaussian noise. we further simulate acquisition shifts and
manifestation shifts by splitting the data into ""source domain"" (development
data) and ""target domain"" (deployment data) according to sub-class information
from the meta-data such as lesion subtypes or clinical sites. dermoscopy
dataset: we combine data from isic 2020 [26], derma 7 point [17], ph2 [24] and
ham10000 [30] and map all lesion sub-types to the super-classes ""benign"" or
""malignant"". we emulate two acquisition shifts by defining either images from
the memorial sloan kettering cancer center (mskcc) or hospital clinic barcelona
(hcb) as the target domain and the remaining images as the source domain.
further, a manifestation shift is designed by defining the lesion subtypes
""keratosis-like"" (benign) and ""actinic keratosis"" (malignant) as the target
domain. chest x-ray dataset: we pool the data from chexpert [14], nih14 [31] and
mimic [16], while only retaining the classes common to all three. next, we
emulate two acquisition shifts by defining either the nih14 or the chexpert data
as the target domain. fc-microscopy dataset: the rxrx1 dataset [28] represents
the fluorescence cell microscopy domain. since the images were acquired in 51
deviating acquisition steps, we define 10 of these batches as target-domain to
emulate an acquisition shift. lung nodule ct dataset: we create a simple 2d
binary nodule classification task based on the 3d lidc-idri data [1] by
selecting the slice with the largest annotation per nodule (±two slices
resulting in 5 slices per nodule). average malignancy ratings (four raters per
nodule, scores between 1 and 5) > 2 are considered malignant and all others as
benign. we emulate two manifestation shifts by defining nodules with high
spiculation (rating > 2), and low texture (rating < 3) as target domains.the
datasets consist only of publicly available data, our benchmark provides scripts
to automatically generate the combined datasets and distribution shifts.the
sf-visuals tool: visualizing silent failures. the proposed tool is based on
three simple operations, that enable effective and intuitive analysis of silent
failures in datasets across various csfs: 1) interactive scatter plots: see
example in fig. 1b. we first reduce the dimensionality of the classifier's
latent space to 50 using principal component analysis and use t-sne to obtain
the final 3-dimensional embedding. interactive functionality includes coloring
dots via pre-defined schemes such as classes, distribution shifts, classifier
confusion matrix, or csf confusion matrix. the associated images are displayed
upon selection of a dot to establish a direct visual link between input space
and embedding. 2) concept cluster plots: see examples in fig. 1c. to abstract
away from individual points in the scatter plot, concepts of interest, such as
classes or distribution shifts can be defined and visualized to identify
conceptual commonalities and differences in the data as perceived by the model.
therefore, k-means clustering is applied to the 3-dimensional embedding. nine
clusters are identified per concept and the resulting plots show the
closest-to-center image per cluster as a visual representation of the concept.
3) silent failure visualization: see examples in fig. 2. we sort all failures by
the classifier confidence and by default show the images associated with the
top-two most confident failures. for corruption shifts, we further allow
investigating the predictions on a fixed input image over varying intensity
levels.based on these visualizations, the functionality of sf-visuals is
three-fold: 1) visual analysis of the dataset including distribution shifts. 2)
visual analysis of the general behavior of various csfs on a given task 3)
visual analysis of individual silent failures in the dataset for various csfs.",3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,1.0,Introduction,"detecting out-of-distribution (ood) samples is crucial in real-world
applications of machine learning, especially in medical imaging analysis where
misdiagnosis can pose significant risks [7]. recently, deep neural networks,
particularly resnets [9] and u-nets [15], have been widely used in various
medical imaging applications such as classification and segmentation tasks,
achieving state-ofthe-art performance. however, due to the typical
overconfidence seen in neural networks [8,18], deep learning with uncertainty
estimation is becoming increasingly important in ood detection.deep
learning-based ood detection methods with uncertainty estimation, such as
evidential deep learning (edl) [10,17] and its variants [2,[11][12][13]24], have
shown their superiority in terms of computational performance, efficiency, and
extensibility. however, most of these methods consider identifying outliers that
significantly differ from training samples(e.g. natural images collected from
imagenet [5]) as ood samples [1]. these approaches overlook the inherent near
ood problem in medical images, in which instances belong to categories or
classes that are not present in the training set [21] due to the differences in
morbidities. failing to detect such near ood samples poses a high risk in
medical application, as it can lead to inaccurate diagnoses and treatments. some
recent works have been proposed for near ood detection based on density models
[20], preprocessing [14], and outlier exposure [16]. nevertheless, all of these
approaches are susceptible to the quality of the training set, which cannot
always be guaranteed in clinical applications.to address this limitation, we
propose an evidence reconciled neural network (ernn), which aims to reliably
detect those samples that are similar to the training data but still with
different distributions (near ood), while maintain accuracy for in-distribution
(id) classification. concretely, we introduce a module named evidence reconcile
block (erb) based on evidence offset. this module cancels out the conflict
evidences obtained from the evidential head, maximizes the uncertainty of
derived opinions, thus minimizes the error of uncertainty calibration in ood
detection. with the proposed method, the decision boundary of the model is
restricted, the capability of medical outlier detection is improved and the risk
of misdiagnosis in medical images is mitigated. extensive experiments on both
isic2019 dataset and in-house pancreas tumor dataset demonstrate that the
proposed ernn significantly improves the reliability and accuracy of ood
detection for clinical applications. code for ernn can be found at
https://github.com/kelladoe/ernn.",3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,3.1,Experimental Setup,"datasets. we conduct experiments on isic 2019 dataset [3,4,19] and an inhouse
dataset. isic 2019 consists of skin lesion images in jpeg format, which are
categorized into nv (12875), mel (4522), bcc (3323), bkl (2624), ak (867), scc
(628), df (239) and vasc (253), with a long-tailed distribution of classes. in
line with the settings presented in [14,16], we define df and vasc, for which
samples are relatively scarce as the near-ood classes. the in-house pancreas
tumor dataset collected from a cooperative hospital is composed of eight
classes: pdac (302), ipmn (71), net (43), scn (37), asc (33), cp (6), mcn (3),
and panin (1). for each sequence, ct slices with the largest tumor area are
picked for experiment. similarly, pdac, ipmn and net are chosen as id classes,
while the remaining classes are reserved as ood categories.implementations and
evaluation metrics. to ensure fairness, we used pretrained resnet34 [9] as
backbone for all methods. during our training process, the images were first
resized to 224 × 224 pixels and normalized, then horizontal and vertical flips
were applied for augmentation. the training was performed using one geforce rtx
3090 with a batch size of 256 for 100 epochs using the adamw optimizer with an
initial learning rate of 1e-4 along with exponential decay. note that we
employed five-fold cross-validation on all methods, without using any additional
ood samples during training. furthermore, we selected the precision(pre),
recall(rec), and f1-score(f1) as the evaluation metrics for id samples, and used
the area under receiver operator characteristic (auroc) as ood evaluation
metric, in line with the work of [6].",3
Evidence Reconciled Neural Network for Out-of-Distribution Detection in Medical Images,3.3,Ablation Study,"in this section, we conduct a detailed ablation study to clearly demonstrate the
effectiveness of our major technical components, which consist of evaluation of
evidential head, evaluation of the proposed evidence reconcile block on both
isic 2019 dataset and our in-house pancreas tumor dataset. since the evidence
reconcile block is based on the evidential head, thus there are four
combinations, but only three experimental results were obtained. as shown in
table 2, it is clear that a network with an evidential head can improve the ood
detection capability by 6% and 1% on isic dataset and in-house pancreas tumor
dataset respectively. furthermore, introducing erb further improves the ood
detection performance of ernn by 1% on isic dataset. and on the more challenging
inhouse dataset, which has more similarities in samples, the proposed method
improves the auroc by 2.3%, demonstrating the effectiveness and robustness of
our model on more challenging tasks.",3
Scale-Aware Test-Time Click Adaptation for Pulmonary Nodule and Mass Segmentation,1.0,Introduction,"lung cancer is the main cause of cancer death worldwide [18]. pulmonary nodules
and masses are both features present in computed tomography images that aid in
the diagnosis of lung cancer. the primary difference is that a nodule is smaller
than 30 mm in diameter, while a mass is larger than 30 mm [22]. early detection
of these features is crucial to aid physicians in making a diagnosis of z. li
and j. yang-equal contributions. visualization on results of four large-scale
mass segmentation given by nnu-net baseline [7]. compared with the ground-truth
segmentation, the recall rate for these four samples is 46.29%, 58.34%, 79.51%,
and 68.51%, respectively. this is significantly lower than the mean value of
81.68%. (b): statistics of the number of nodules at different scales in three
datasets. the range of nodule diameter corresponding to micro, small, medium,
and mass is (0, 10], (10,20], (20,30], [30, ∞), respectively. (c) : the
distribution of recall rate with respect to the nodule size. existing methods
have low recall rates for the segmentation of large scale nodules and
masses.benign or malignant tumors [27] and determining follow-up treatment.
lesion segmentation can be utilized to evaluate two important factors: the
volume of the lesion and its growth rate [5,6,8,12]. furthermore, obtaining
accurate information regarding the nodule can assist in determining the
appropriate resection method and surgical margin required to preserve as much
lung function as possible. [14,17].segmenting nodules is a tedious task that
requires significant human labor. computer aided diagnosis (cad) systems can
significantly reduce such heavy workloads. the accuracy of the existing nodule
detection model reaches 96.1% [9] accuracy. however, the accuracy of the 3d
nodule segmentation model is prone to significantly decline in the application,
regardless of whether its structure is based on cnn or transformer [2]. as shown
in fig. 1(a-c), the recall rate of the large-scale nodule and mass is usually
lower than the average level. the main reason is that the lesion scale in the
two public datasets are relatively small, which matches the fact few patients
have very large nodule or mass. this makes the pulmonary nodule and mass
segmentation task resemble a long-tail problem rather than a mere large scale
span problem. this leads to unsatisfactory results when segmenting large lesions
that require more accurate delineation [26].several studies have proposed
solutions to tackle the large scale span challenges at both the input and
feature level. for instance, some approaches adopt multi-scale inputs [4], where
the input images are resized to different resolu-tion ratios. some other methods
leverage multi-scale feature maps to capture information from different scales,
such as cross-scale feature fusion [19] or using multi-scale convolutional
filters [3]. furthermore, the attention mechanisms [23] has also been utilized
to emphasize the features that are more relevant for segmentation. though these
methods have achieved impressive performance, they still struggle to accurately
segment the extremely imbalanced multi-scale lesions.recently, some click-based
lesion segmentation methods [19][20][21] introduce the click at the input or
feature level and modify the network accordingly, resulting in higher accuracy
results. yet, the click input does not provide the scale information of lesions
for the network.in this paper, we propose a scale-aware test-time click
adaptation (sattca) method, which simply utilizes easily obtainable lesion click
(i.e., the center detected nodule) to adjust the parameters of the network
normalization layers [24] during testing. note that we do not need to exploit
any data from the training set. specifically, we expand the click into an
ellipsoid mask, which supervises the test-time adaptation. this helps to improve
the segmentation performance of large-scale nodules and masses. additionally, we
also propose a multi-scale input encoder to further address the problem of
imbalanced lesion scales. experimental results on two public datasets and one
in-house dataset demonstrate that the proposed method outperforms existing
methods with different backbones.",3
WeakPolyp: You only Look Bounding Box for Polyp Segmentation,1.0,Introduction,"colorectal cancer (crc) has become a major threat to health worldwide. since
most crcs originate from colorectal polyps, early screening for polyps is
necessary. given its significance, automatic polyp segmentation models
[5,8,16,18] have been designed to aid in screening. for example, acsnet [21],
hrenet [14], ldnet [20] and ccbanet [11] propose to use convolutional neural
networks to extract multi-scale contexts for robust predictions. lodnet [2],
pranet [5], and msnet [23] aim to improve the model's discrimination of polyp
boundaries. sanet [19] eliminates the distribution gap between the training set
and the testing set, thus improving the model generalization. recently, tganet
[15] introduces text embeddings to enhance the model's discrimination.
furthermore, transfuse [22], ppformer [1], and polyp-pvt [3] introduce the
transformer [4] backbone to extract global contexts, achieving a significant
performance gain.all above models are fully supervised and require pixel-level
annotations. however, pixel-by-pixel labeling is time-consuming and expensive,
which hampers practical clinical usage. besides, many polyps do not have
welldefined boundaries. pixel-level labeling inevitably introduces subjective
noise. to address the above limitations, a generalized polyp segmentation model
is urgently needed. in this paper, we achieve this goal by a weakly supervised
polyp segmentation model (named weakpolyp) that only uses coarse bounding box
annotations. figure 1(a) shows the differences between our weakpolyp and fully
supervised models. compared with fully supervised ones, weakpolyp requires only
a bounding box for each polyp, thus dramatically reducing the labeling cost.
more meaningfully, weakpolyp can take existing large-scale polyp detection
datasets to assist the polyp segmentation task. finally, weakpolyp does not
require the labeling for polyp boundaries, avoiding the subjective noise at
source. all these advantages make weakpolyp more clinically practical.however,
bounding box annotations are much coarser than pixel-level ones, which can not
describe the shape of polyps. simply adopting these box annotations as
supervision introduces too much background noise, thereby leading to suboptimal
models. as a solution, boxpolyp [18] only supervises the pixels with high
certainty. however, it requires a fully supervised model to predict the
uncertainty map. unlike boxpolyp, our weakpolyp completely follows the weakly
supervised form that requires no additional models or annotations. surprisingly,
just by redesigning the supervision loss without any changes to the model
structure, weakpolyp achieves comparable performance to its fully supervised
counterpart. figure 1(b) visualizes some predicted results by
weakpolyp.weakpolyp is mainly enabled by two novel components: mask-to-box (m2b)
transformation and scale consistency (sc) loss. in practice, m2b is applied to
transform the predicted mask into a box-like mask by projection and
backprojection. then, this transformed mask is supervised by the bounding box
annotation. this indirect supervision avoids the misleading of box-shape bias of
annotations. however, many regions in the predicted mask are lost in the
projection and therefore get no supervision. to fully explore these regions, we
propose the sc loss to provide a pixel-level self-supervision while requiring no
annotations at all. specifically, the sc loss explicitly reduces the distance
between predictions of the same image at different scales. by forcing feature
alignment, it inhibits the excessive diversity of predictions, thus improving
the model generalization.in summary, our contributions are three-fold: (1) we
build the weakpolyp model completely based on bounding box annotations, which
largely reduces the labeling cost and achieves a comparable performance to full
supervision. (2) we propose the m2b transformation to mitigate the mismatch
between the prediction and the supervision, and design the sc loss to improve
the robustness of the model against the variability of the predictions. (3) our
proposed weakpolyp is a plug-and-play option, which can boost the performances
of polyp segmentation models under different backbones.",3
Shifting More Attention to Breast Lesion Segmentation in Ultrasound Videos,1.0,Introduction,"axillary lymph node (aln) metastasis is a severe complication of cancer that can
have devastating consequences, including significant morbidity and mortality.
early detection and timely treatment are crucial for improving outcomes and
reducing the risk of recurrence. in breast cancer diagnosis, accurately
segmenting breast lesions in ultrasound (us) videos is an essential step for
computer-aided diagnosis systems, as well as breast cancer diagnosis and
treatment. however, this task is challenging due to several factors, including
blurry lesion boundaries, inhomogeneous distributions, diverse motion patterns,
and dynamic changes in lesion sizes over time [12]. the work presented in [10]
proposed the first pixel-wise annotated benchmark dataset for breast lesion
segmentation in us videos, but it has some limitations. although their efforts
were commendable, this dataset is private and contains only 63 videos with 4,619
annotated frames. the small dataset size increases the risk of overfitting and
limits the generalizability capability. in this work, we collected a
larger-scale us video breast lesion segmentation dataset with 572 videos and
34,300 annotated frames, of which 222 videos contain aln metastasis, covering a
wide range of realistic clinical scenarios. please refer to table 1 for a
detailed comparison between our dataset and existing datasets.although the
existing benchmark method dpstt [10] has shown promising results for breast
lesion segmentation in us videos, it only uses the ultrasound image to read
memory for learning temporal features. however, ultrasound images suffer from
speckle noise, weak boundaries, and low image quality. thus, there is still
considerable room for improvement in ultrasound video breast lesion
segmentation. to address this, we propose a novel network called frequency and
localization feature aggregation network (fla-net) to improve breast lesion
segmentation in ultrasound videos. our fla-net learns frequency-based temporal
features and then uses them to predict auxiliary breast lesion location maps to
assist the segmentation of breast lesions in video frames. additionally, we
devise a contrastive loss to enhance the breast lesion location similarity of
video frames within the same ultrasound video and to prohibit location
similarity of different ultrasound videos. the experimental results
unequivocally showcase that our network surpasses state-of-the-art techniques in
the realm of both breast lesion segmentation in us videos and two video polyp
segmentation benchmark datasets (fig. 1).",3
ACC-UNet: A Completely Convolutional UNet Model for the 2020s,3.4,Comparative Qualitative Results on the Five Datasets,"in addition to, achieving higher dice scores, apparently, acc-unet generated
better qualitative results. figure 3 presents a qualitative comparison of
acc-unet with the other models. each row of the figure comprises one example
from each of the datasets and the segmentation predicted by acc-unet and the
ground truth mask are presented in the rightmost two columns. for the 1 st
example from the isic-18 dataset, our model did not oversegment but rather
followed the lesion boundary. in the 2 nd example from cvc-clinicdb, our model
managed to distinguish the finger from the polyp almost perfectly. next in the 3
rd example from busi, our prediction filtered out the apparent nodule region on
the left, which was predicted as a false positive tumor by all the other models.
similarly, in the 4 th sample from the covid dataset, we were capable to model
the gaps in the consolidation of the left lung visually better, which in turn
resulted in 2.9% higher dice score than the 2 nd best method. again, in the
final example from the glas dataset, we not only successfully predicted the
gland at the bottom right corner but also identified the glands at the top left
individually, which were mostly missed or merged by the other models,
respectively.",3
FocalUNETR: A Focal Transformer for Boundary-Aware Prostate Segmentation Using CT Images,1.0,Introduction,"prostate cancer is a leading cause of cancer-related deaths in adult males, as
reported in studies, such as [17]. a common treatment option for prostate cancer
is external beam radiation therapy (ebrt) [4], where ct scanning is a
cost-effective tool for the treatment planning process compared with the more
expensive magnetic resonance imaging (mri). as a result, precise prostate
segmentation in ct images becomes a crucial step, as it helps to ensure that the
radiation doses are delivered effectively to the tumor tissues while minimizing
harm to the surrounding healthy tissues.due to the relatively low spatial
resolution and soft tissue contrast in ct images compared to mri images, manual
prostate segmentation in ct images can be time-consuming and may result in
significant variations between operators [10]. several automated segmentation
methods have been proposed to alleviate these issues, especially the fully
convolutional networks (fcn) based u-net [19] (an encoder-decoder architecture
with skip connections to preserve details and extract local visual features) and
its variants [14,23,26]. despite good progress, these methods often have
limitations in capturing long-range relationships and global context information
[2] due to the inherent bias of convolutional operations. researchers naturally
turn to vit [5], powered with self-attention (sa), for more possibilities:
transunet first [2] adapts vit to medical image segmentation tasks by connecting
several layers of the transformer module (multi-head sa) to the fcn-based
encoder for better capturing the global context information from the high-level
feature maps. transfuse [25] and medt [21] use a combined fcn and transformer
architecture with two branches to capture global dependency and low-level
spatial details more effectively. swin-unet [1] is the first u-shaped network
based purely on more efficient swin transformers [12] and outperforms models
with fcn-based methods. unetr [6] and siwnunetr [20] are transformer
architectures extended for 3d inputs.in spite of the improved performance for
the aforementioned vit-based networks, these methods utilize the standard or
shifted-window-based sa, which is the fine-grained local sa and may overlook the
local and global interactions [18,24]. as reported by [20], even pre-trained
with a massive amount of medical data using self-supervised learning, the
performance of prostate segmentation task using high-resolution and better soft
tissue contrast mri images has not been completely satisfactory, not to mention
the lower-quality ct images. additionally, the unclear boundary of the prostate
in ct images derived from the low soft tissue contrast is not properly addressed
[7,22].recently, focal transformer [24] is proposed for general computer vision
tasks, in which focal self-attention is leveraged to incorporate both
fine-grained local and coarse-grained global interactions. each token attends
its closest surrounding tokens with fine granularity, and the tokens far away
with coarse granularity; thus, focal sa can capture both short-and long-range
visual dependencies efficiently and effectively. inspired by this work, we
propose the focalunetr (focal u-net transformers), a novel focal transformer
architecture for ctbased medical image segmentation (fig. 1a). even though prior
works such as psi-net [15] incorporates additional decoders to enhance boundary
detection and distance map estimation, they either lack the capacity for
effective global context capture through fcn-based techniques or overlook the
significance of considering the randomness of the boundary, particularly in poor
soft tissue contrast ct images for prostate segmentation. in contrast, our
approach utilizes a multi-task learning strategy that leverages a gaussian
kernel over the boundary of the ground truth segmentation mask [11] as an
auxiliary boundary-aware contour regression task (fig. 1b). this serves as a
regularization term for the main task of generating the segmentation mask. and
the auxiliary task enhances the model's generalizability by addressing the
challenge of unclear boundaries in low-contrast ct images. in this paper, we
make several new contributions. first, we develop a novel focal transformer
model (focalunetr) for ct-based prostate segmentation, which makes use of focal
sa to hierarchically learn the feature maps accounting for both short-and
long-range visual dependencies efficiently and effectively. second, we also
address the challenge of unclear boundaries specific to ct images by
incorporating an auxiliary task of contour regression. third, our methodology
advances state-of-the-art performance via extensive experiments on both
realworld and benchmark datasets.",3
Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,5.0,Discussion and Conclusion,"the results reported before reveal that approaching the problem of segmentation
uncertainty prediction via a regression task, where the uncertainty is expressed
in terms of landmark location, is globally better than via pixel-based
segmentation methods. it also shows that our method (n 1 , n 2 and sn 2 ) is
better than the commonly-used mc-dropout. it can also be said that our method is
more interpretable as is detailed in sect. 2.2 and shown in fig. 3.the choice of
distribution has an impact when considering the shape of the predicted contour.
for instance, structures such as the left ventricle and the myocardium wall in
the ultrasound datasets have large components of their contour oriented along
the vertical direction which allows the univariate and bivariate models to
perform as well, if not better, than the asymmetric model. however, the lungs
and heart in chest x-rays have contours in more directions and therefore the
uncertainty is better modeled with the asymmetric model.furthermore, it has been
demonstrated that skewed uncertainty is more prevalent when tissue separation is
clear, for instance, along the septum border (camus) and along the lung contours
(jsrt). the contrast between the left ventricle and myocardium in the images of
the private cardiac us dataset is small, which explains why the simpler
univariate and bivariate models perform well. this is why on very noisy and
poorly contrasted data, the univariate or the bivariate model might be
preferable to using the asymmetric model.while our method works well on the
tasks presented, it is worth noting that it may not be applicable to all
segmentation problems like tumour segmentation. nevertheless, our approach is
broad enough to cover many applications, especially related to segmentation that
is later used for downstream tasks such as clinical metric estimation. future
work will look to expand this method to more general distributions, including
bi-modal distributions, and combine the aleatoric and epistemic uncertainty to
obtain the full predictive uncertainty.",3
DHC: Dual-Debiased Heterogeneous Co-training Framework for Class-Imbalanced Semi-supervised Medical Image Segmentation,1.0,Introduction,"the shortage of labeled data is a significant challenge in medical image
segmentation, as acquiring large amounts of labeled data is expensive and
requires specialized knowledge. this shortage limits the performance of existing
segmentation models. to address this issue, researchers have proposed various
semi-supervised learning (ssl) techniques that incorporate both labeled and
unlabeled data to train models for both natural [2,4,12,13,15,16] and medical
images [10,11,14,[18][19][20][21]. however, most of these methods do not
consider the class imbalance issue, which is common in medical image datasets.
for example, multi-organ segmentation from ct scans requires to segment
esophagus, right adrenal gland, left adrenal gland, etc., where the class ratio
is quite imbalanced; see fig 1(a). as for liver tumor segmentation from ct
scans, usually the ratio for liver and tumor is larger than 16:1.recently, some
researchers proposed class-imbalanced semi-supervised methods [1,10] and
demonstrated substantial advances in medical image segmentation tasks.
concretely, basak et al. [1] introduced a robust class-wise sampling strategy to
address the learning bias by maintaining performance indicators on the fly and
using fuzzy fusion to dynamically obtain the class-wise sampling rates. however,
the proposed indicators can not model the difficulty well, and the benefits may
be overestimated due to the non-representative datasets used (fig. 1(a)). lin et
al. [10] proposed cld to address the data bias by weighting the overall loss
function based on the voxel number of each class. however, this method fails due
to the easily over-fitted cps (cross pseudo supervision) [4] baseline, ignoring
unlabeled data in weight estimation and the fixed class-aware weights.in this
work, we explore the importance of heterogeneity in solving the over-fitting
problem of cps (fig. 2) and propose a novel dhc (dual-debiased heterogeneous
co-training) framework with two distinct dynamic weighting strategies leveraging
both labeled and unlabeled data, to tackle the class imbalance issues and
drawbacks of the cps baseline model. the key idea of heterogeneous co-training
is that individual learners in an ensemble model should be both accurate and
diverse, as stated in the error-ambiguity decomposition [8].to achieve this, we
propose distdw (distribution-aware debiased weighting) and diffdw (diff
iculty-aware debiased weighting) strategies to guide the two sub-models to
tackle different biases, leading to heterogeneous learning directions.
specifically, distdw solves the data bias by calculating the imbalance ratio
with the unlabeled data and forcing the model to focus on extreme minority
classes through careful function design. then, after observing the inconsistency
between the imbalance degrees and the performances (see fig. 1(b)), diffdw is
designed to solve the learning bias. we use the labeled samples and the
corresponding labels to measure the learning difficulty from learning speed and
dice value aspects and slow down the speeds of the easier classes by setting
smaller weights. distdw and diffdw are diverse and have complementary properties
(fig. 1(c)), which satisfies the design ethos of a heterogeneous framework.the
key contributions of our work can be summarized as follows: 1) we first state
the homogeneity issue of cps and improve it with a novel dual-debiased
heterogeneous co-training framework targeting the class imbalance issue; 2) we
propose two novel weighting strategies, distdw and diffdw, which effectively
solve two critical issues of ssl: data and learning biases; 3) we introduce two
public datasets, synapse [9] and amos [7], as new benchmarks for
class-imbalanced semi-supervised medical image segmentation. these datasets
include sufficient classes and significant imbalance ratios (> 500 : 1), making
them ideal for evaluating the effectiveness of class-imbalance-targeted
algorithm designs.",3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,1.0,Introduction,"primary liver cancer is one of the most common and deadly cancer diseases in the
world, and liver resection is a highly effective treatment [11,14]. the couinaud
segmentation [7] based on ct images divides the liver into eight functionally
independent regions, which intuitively display the positional relationship
between couinaud segments and intrahepatic lesions, and helps surgeons for make
surgical planning [3,13]. in clinics, couinaud segments obtained from manual
annotation are tedious and time-consuming, based on the vasculature used as
rough guide (fig. 1). thus, designing an automatic method to accurately segment
couinaud segments from ct images is greatly demanded and has attracted
tremendous research attention.however, automatic and accurate couinaud
segmentation from ct images is a challenging task. since it is defined based on
the anatomical structure of live vessels, even no intensity contrast (fig.
1.(b)) can be observed between different couinaud segments, and the uncertainty
of boundary (fig. 1.(d)) often greatly affect the segmentation performance.
previous works [4,8,15,19] mainly rely on handcrafted features or atlas-based
models, and often fail to robustly handle those regions with limited features,
such as the boundary between adjacent couinaud segments. recently, with the
advancement of deep learning [5,10,18], many cnn-based algorithms perform
supervised training through pixel-level couinaud annotations to automatically
obtain segmentation results [1,9,21]. unfortunately, the cnn models treat all
voxel-wise features in the ct image equally, cannot effectively capture key
anatomical regions useful for couinaud segmentation. in addition, all these
methods deal with the 3d voxels of the liver directly without considering the
spatial relationship of the different couinaud segments, even if this
relationship is very important in couinaud segmentation. it can supplement the
cnn-based method and improve the segmentation performance in regions without
intensity contrast.in this paper, to tackle the aforementioned challenges, we
propose a pointvoxel fusion framework that represents the liver ct in continuous
points to better learn the spatial structure, while performing the convolutions
in voxels to obtain the complementary semantic information of the couinaud
segments. specifically, the liver mask and vessel attention maps are first
extracted from the ct images, which allows us to randomly sample points embedded
with vessel structure prior in the liver space and voxelize them into a voxel
grid. subsequently, points and voxels pass through two branches to extract
features. the point-based branch extracts the fine-grained feature of
independent points and explores spatial topological relations. the voxel-based
branch is composed of a series of convolutions to learn semantic features,
followed by de-voxelization to convert them back to points. through the
operation of voxelization and devoxelization at different resolutions, the
features extracted by these two branches can achieve multi-scale fusion on
point-based representation, and finally output the couinaud segment category of
each point. extensive experiments on two publicly available datasets named
3dircadb [20] and lits [2] demonstrate that our proposed framework achieves
state-of-the-art (sota) performance, outperforming cutting-edge methods
quantitatively and qualitatively.",3
Anatomical-Aware Point-Voxel Network for Couinaud Segmentation in Liver CT,3.1,Datasets and Evaluation Metrics,"we evaluated the proposed framework on two publicly available datasets, 3dircadb
[20] and lits [2]. the 3dircadb dataset [20] contains 20 ct images with spacing
ranging from 0.56 mm to 0.87 mm, and slice thickness ranging from 1 mm to 4 mm
with liver and liver vessel segmentation labels. the lits dataset [2] consists
of 200 ct images, with a spacing of 0.56 mm to 1.0 mm and slice thickness of
0.45 mm to 6.0 mm, and has liver and liver tumour labels, but without vessels.
we annotated the 20 subjects of the 3dircadb dataset [20] with the couinaud
segments and randomly divided 10 subjects for training and another 10 subjects
for testing. for lits dataset [2], we observed the vessel structure on ct
images, annotated the couinaud segments of 131 subjects, and randomly selected
66 subjects for training and 65 for testing.we have used three widely used
metrics, i.e., accuracy (acc, in %), dice similarity metric (dice, in %), and
average surface distance (asd, in mm) to evaluate the performance of the
couinaud segmentation.",3
HENet: Hierarchical Enhancement Network for Pulmonary Vessel Segmentation in Non-contrast CT Images,1.0,Introduction,"segmentation of the pulmonary vessels is the foundation for the clinical
diagnosis of pulmonary vascular diseases such as pulmonary embolism (pe),
pulmonary hypertension (ph) and lung cancer [9]. accurate vascular quantitative
analysis is crucial for physicians to study and apply in treatment planning, as
well as making surgical plans. although contrast-enhanced ct images have better
contrast for pulmonary vessels compared to non-contrast ct images, the
acquisition of contrast-enhanced ct images needs to inject a certain amount of
contrast agent to the patients. some patients have concerns about the possible
risk of contrast media [2]. at the same time, non-contrast ct is the most widely
used imaging modality for visualizing, diagnosing, and treating various lung
diseases.in the literature, several conventional methods [5,16] have been
proposed for the segmentation of pulmonary vessels in contrast-enhanced ct
images. most of these methods employed manual features to segment peripheral
intrapulmonary vessels. in recent years, deep learning-based methods have
emerged as promising approaches to solving challenging medical image analysis
problems and have demonstrated exciting performance in segmenting various
biological structures [10,11,15,17]. however, for vessel segmentation, the
widely used models, such as u-net and its variants, limit their segmentation
accuracy on low-contrast small vessels due to the loss of detailed information
caused by the multiple downsampling operations. accordingly, zhou et al. [17]
proposed a nested structure unet++ to redesign the skip connections for
aggregating multi-scale features and improve the segmentation quality of
varying-size objects. also, some recent methods combine convolutional neural
networks (cnns) with transformer or non-local block to address this issue
[3,6,13,18]. wang et al. [13] replaced the original skip connections with
transformer blocks to better merge the multi-scale contextual information. for
this task, cui et al. [1] also proposed an orthogonal fused u-net++ for
pulmonary peripheral vessel segmentation. however, all these methods ignored the
significant variability in hu values of pulmonary vessels at different
regions.to summarize, there exist several challenges for pulmonary vessel
segmentation in non-contrast ct images: (1) the contrast between pulmonary
vessels and background voxels is extremely low (fig. 1(c)); (2) pulmonary
vessels have a complex structure and significant variability in vessel
appearance, with different scales in different areas. the central extrapulmonary
vessels near the heart have a large irregular ball-like shape, while the shape
of the intrapulmonary vessels is delicate and tubular-like (fig. 1(a) and (b)).
vessels become thinner as they get closer to the peripheral lung; (3) hu values
of vessels in different regions vary significantly, ranging from -850 hu to 100
hu. normally, central extrapulmonary vessels have higher hu values than
peripheral intrapulmonary vessels. thus, we set different ranges of hu values to
better visualize the vessels in fig. 1(d) and(e).to address the above
challenges, we propose a h ierarchical e nhancement n etwork (henet) for
pulmonary vessel segmentation in non-contrast ct images by enhancing the
representation of vessels at both image-and feature-level. for the input ct
images, we propose an auto contrast enhancement (ace) module to automatically
adjust the range of hu values in different areas of ct images. it mimics the
radiologist in setting the window level (wl) and window width (ww) to better
enhance vessels from surrounding voxels, as shown in fig. 1(d) and (e). also, we
propose a cross-scale non-local block (csnb) to replace the skip connections in
vanilla u-net [11] structure for the aggregation of multi-scale feature maps. it
helps to form local-to-global information connections to enhance vessel
information at the feature-level, and address the complex scale variations of
pulmonary vessels.",3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.1,Datasets,"qubiq is a recent challenge held at miccai 2020 and 2021, specifically designed
to evaluate the inter-rater variability in medical imaging. following [23,41],
we use qubiq 2020, which contains 7 segmentation tasks in 4 different ct and mr
datasets: prostate (55 cases, 2 tasks, 6 raters), brain growth (39 cases, 1
task, 7 raters), brain tumor (32 cases, 3 tasks, 3 raters), and kidney (24
cases, 1 task, 3 raters). for each dataset, we calculate the average dice score
between each rater and the majority votes in table 1. in some datasets, such as
brain tumor t2, the inter-rater disagreement can be quite substantial. in line
with [23], we resize all images to 256 × 256. lits contains 201 high-quality ct
scans of liver tumors. out of these, 131 cases are designated for training and
70 for testing. as the ground-truth labels for the test set are not publicly
accessible, we only use the training set. following [36], all images are resized
to 512×512 and the hu values of ct images are windowed to the range of [-60,
140]. kits includes 210 annotated ct scans of kidney tumors from different
patients. in accordance with [36], all images are resized to 512 × 512 and the
hu values of ct images are windowed to the range of [-200, 300].",3
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.4,Results on QUBIQ,"in table 2, we compare different training methods on qubiq using unet-resnet50.
this comparison includes both hard labels, obtained through (i) majority votes
[25] and (ii) random sampling each rater's annotation [22], as well as soft
labels derived from (i) averaging across all annotations [12,25,41] and (ii)
label smoothing [43].in the literature [12,25,41], annotations are usually
averaged with uniform weights. we additionally consider weighting each rater's
annotation by its dice score with respect to the majority votes, so that a rater
who deviates far from the majority votes receives a low weight. note that for
all methods, the dice score and ece are computed with respect to the majority
votes, while bdice is calculated as illustrated in sect. 3.3.generally, models
trained with soft labels exhibit improved accuracy and calibration. in
particular, averaging annotations with uniform weights obtains the highest
bdice, while a weighted average achieves the highest dice score. it is worth
noting that the weighted average significantly outperforms the majority votes in
terms of the dice score which is evaluated based on the majority votes
themselves. we hypothesize that this is because soft labels contain extra
interrater information, which can ease the network optimization at those
ambiguous regions. overall, we find the weighted average outperforms other
methods, with the exception of brain tumor t2, where there is a high degree of
disagreement among raters.we compare our method with state-of-the-art (sota)
methods using unet-resnet50 in table 3. in our method, we average annotations
with uniform weights for brain tumor t2 and with each rater's dice score for all
other datasets. our method, which simply averages annotations to produce soft
labels obtains superior results compared to methods that adopt complex
architectures or training techniques.",3
Semi-supervised Domain Adaptive Medical Image Segmentation Through Consistency Regularized Disentangled Contrastive Learning,3.0,Experiments and Results,"datasets: we evaluate our work on two different da tasks to evaluate its
generalizability: (1) polyp segmentation from colonoscopy images in kvasir-seg
[11] and cvc-endoscene still [20], and (2) brain tumor segmentation in mri
images from brats2018 [16]. kvasir and cvc contain 1000 and 912 images
respectively and were split into 4 : 1 training-testing sets following [10].
brats consists of brain mris from 285 patients with t1, t2, t1ce, and flair
scans. the data was split into 4 : 1 train-test ratio, following [14].
source→target: we perform experiments on cv c → kvasir and kvasir → cv c for
polyp segmentation, and t 2 → {t 1, t 1ce, f lair} for tumor segmentation. the
ssda accesses 10 -50% and 1 -5 labels from the target domain for the two tasks,
respectively. for uda, only s is used for l sup , whereas t 1 ∪ t 2 is used for
l reg . implementation details: implementation is done in a pytorch environment
using a tesla v100 gpu with 32gb ram. we use u-net [17] backbone for the
encoder-decoder structure, and the projection heads g s and g c are shallow fc
layers. the model is trained for 300 epochs for pre-training and 500 epochs for
fine-tuning using an adam optimizer with a batch size of 4 and a learning rate
of 1e -4. λ1, λ2, λ3, and t h are set to 0.75, 0.75, 0.5, 0.6, respectively by
validation, τ, α are set to 0.07, 0.999 following [9]. augmentations include
random rotation and translation. metrics: segmentation performance is evaluated
using dice similarity score (dsc) and hausdorff distance (hd).",4
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,1.0,Introduction,"medical image segmentation plays a crucial role in enabling better diagnosis,
surgical planning, and image-guided surgery [8]. the inherent ambiguity and high
uncertainty of medical images pose significant challenges [5] for accurate
segmentation, attributed to factors such as unclear tumor boundaries in brain
magnetic resonance imaging (mri) images and multiple plausible annotations of
lung nodule in computed tomography (ct) images. existing medical image
segmentation methods typically provide a single, deterministic, most likely
hypothesis mask, which may lead to misdiagnosis or sub-optimal treatment.
therefore, providing accurate and diverse segmentation masks as valuable
references [17] for radiologists is crucial in clinical practice.recently,
diffusion models [10] have shown strong capacities in various visual generation
tasks [21,22]. however, how to better deal with discrete segmentation tasks
needs further consideration. although many researches [1,26] have combined
diffusion model with segmentation tasks, all these methods do not take full
account of the discrete characteristic of segmentation task and still use
gaussian noise as their diffusion kernel.to achieve accurate and diverse
segmentation masks, we propose a novel conditional bernoulli diff usion model
for medical image segmentation (berdiff). instead of using the gaussian noise,
we first propose to use the bernoulli noise as the diffusion kernel to enhance
the capacity of the diffusion model for segmentation, resulting in more accurate
segmentation masks. moreover, by leveraging the stochastic nature of the
diffusion model, our berdiff randomly samples the initial bernoulli noise and
intermediate latent variables multiple times to produce a range of diverse
segmentation masks, highlighting salient regions of interest (roi) that can
serve as a valuable reference for radiologists. in addition, our berdiff can
efficiently sample sub-sequences from the overall trajectory of the reverse
diffusion based on the rationale behind the denoising diffusion implicit models
(ddim) [25], thereby speeding up the segmentation process.the contributions of
this work are summarized as follows. 1) instead of using the gaussian noise, we
propose a novel conditional diffusion model based on the bernoulli noise for
discrete binary segmentation tasks, achieving accurate and diverse medical image
segmentation masks. 2) our berdiff can efficiently sample sub-sequences from the
overall trajectory of the reverse diffusion, thereby speeding up the
segmentation process. 3) experimental results on lidc-idri and brats 2021
datasets demonstrate that our berdiff outperforms other state-of-the-art
methods.",4
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,3.1,Experimental Setup,"dataset and preprocessing. the data used in this experiment are obtained from
lidc-idri [2,7] and brats 2021 [4] datasets. lidc-idri contains 1,018 lung ct
scans with plausible segmentation masks annotated by four radiologists. we adopt
a standard preprocessing pipeline for lung ct scans and the trainvalidation-test
partition as in previous work [5,15,23]. brats 2021 consists of four different
sequence (t1, t2, flair, t1ce) mri images for each patient. all 3d scans are
sliced into axial slices and discarded the bottom 80 and top 26 slices. note
that we treat the original four types of brain tumors as one type following
previous work [25], converting the multi-target segmentation problem into
binary. our training set includes 55,174 2d images scanned from 1,126 patients,
and the test set comprises 3,991 2d images scanned from 125 patients. finally,
the sizes of images from lidc-idri and brast 2021 are resized to a resolution of
128 × 128 and 224 × 224, respectively. implementation details. we implement all
the methods with the pytorch library and train the models on nvidia v100 gpus.
all the networks are trained using the adamw [19] optimizer with a mini-batch
size of 32. the initial learning rate is set to 1 × 10 -4 for brats 2021 and 5 ×
10 -5 for lidc-idri. the bernoulli noise estimation u-net network in fig. 1 of
our berdiff is the same as previous diffusion-based models [20]. we employ a
linear noise schedule for t = 1000 timesteps for all the diffusion models. and
we use the sub-sequence sampling strategy of ddim to accelerate the segmentation
process. during minibatch training of lidc-idri, our berdiff learns diverse
expertise by randomly sampling one from four annotated segmentation masks for
each image. four metrics are used for performance evaluation, including
generalized energy distance (ged), hungarian-matched intersection over union
(hm-iou), soft-dice and dice coefficient. we compute ged using varying numbers
of segmentation samples (1, 4, 8, and 16), hm-iou and soft-dice using 16
samples.",4
BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,3.2,Ablation Study,"we start by conducting ablation experiments to demonstrate the effectiveness of
different losses and estimation targets, as shown in table 1. all experiments
are trained for 21,000 training iterations on lidc-idri. we first explore the
selection of losses in the top three rows. we find that the combination of kl
divergence and bce loss can achieve the best performance. then, we explore the
selection of estimation targets in the bottom two rows. we observe that
estimating bernoulli noise, instead of directly estimating the ground-truth
mask, is the u-net has the same architecture as the noise estimation network in
our berdiff and previous diffusion-based models. fig. 2. diverse segmentation
masks and the corresponding saliency mask of two lung nodules randomly selected
in lidc-idri. y i 0 and y i gt refer to the i-th generated and ground-truth
segmentation masks, respectively. saliency mask is the mean of diverse
segmentation masks. more suitable for our binary segmentation task. all of these
findings are consistent with previous works [3,10].here, we conduct ablation
experiments on our berdiff with gaussian or bernoulli noise, and the results are
shown in table 2. for discrete segmentation tasks, we find that using bernoulli
noise can produce favorable results when training iterations are limited (e.g.
21,000 iterations) and even outperform using gaussian noise when training
iterations are sufficient (e.g. 86,500 iterations). we also provide a more
detailed performance comparison between bernoulliand gaussian-based diffusion
models over training iterations in fig. s3. 3, and find that our berdiff
performs well for discrete segmentation tasks. probabilistic u-net (prob.u-net),
hierarchical prob.u-net (hprob.u-net), and joint prob.u-net (jpro.u-net) use
conditional variational autoencoder (cvae) to accomplish segmentation tasks.
calibrated adversarial refinement (car) employs generative adversarial networks
(gan) to refine segmentation. pixelseg is based on autoregressive models, while
segdiff and medsegdiff are diffusion-based models. there are also methods that
attempt to model multi-annotators explicitly [13,18,27]. we have the following
three observations: 1) diffusion-based methods demonstrate significant
superiority over traditional approaches based on vae, gan, and autoregression
models for discrete segmentation tasks; 2) our berdiff outperforms other
diffusion-based models that use gaussian noise as the diffusion kernel; and 3)
our berdiff also outperforms the methods that explicitly model the annotator,
striking a good balance between diversity and accuracy. at the same time, we
present comparison segmentation results in fig. 2. compared to other models, our
berdiff can effectively learn diverse expertise, resulting in more diverse and
accurate segmentation masks. especially for small nodules that can create
ambiguity, such as the lung nodule on the left, our berdiff approach produces
segmentation masks that are more in line with the ground-truth masks.results on
brats 2021. here, we present the quantitative and qualitative results of brats
2021 in table 4 and fig. 3, respectively. we conducted a comparative analysis of
our berdiff with other models such as nnunet, transformer-based models like
transu-net and swin unetr, as well as diffusion-based methods like segdiff.
first, we find that diffusion-based methods show superior performance compared
to traditional u-net and transformer-based approaches. besides, the high
performance achieved by u-net, which shares the same architecture as our noise
estimation network, highlights the effectiveness of the backbone design in
diffusion-based models. moreover, our proposed berdiff surpasses other
diffusion-based models that use gaussian noise as the diffusion kernel. finally,
from fig. 3, we find that our berdiff segments more accurately on parts that are
difficult to recognize by the human eye, such as the tumor in the 3rd row. at
the same time, we can also generate diverse plausible segmentation masks to
produce a saliency segmentation mask. we note that some of these masks may be
false positives, as shown in the 1st row, but they can be filtered out due to
low saliency. please refer to figs. s1 ands2 for more examples of diverse
segmentation masks generated by our berdiff.",4
Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models,4.0,Experiments,"we conducted a series of experiments to evaluate the performance of our proposed
method for multi-annotator prediction. our experiments were carried out on
datasets of the qubiq benchmark1 . we compared the performance of our proposed
method with several state-of-the-art methods. datasets. the quantification of
uncertainties in biomedical image quantification challenge (qubiq), is a
recently available challenge dataset specifically for the evaluation of
inter-rater variability. qubiq comprises four different segmentation datasets
with ct and mri modalities, including brain growth (one task, mri, seven raters,
34 cases for training and 5 cases for testing), brain tumor (one task, mri,
three raters, 28 cases for training and 4 cases for testing), prostate (two
subtasks, mri, six raters, 33 cases for training and 15 cases for testing), and
kidney (one task, ct, three raters, 20 cases for training and 4 cases for
testing). following [13], the evaluation is performed using the soft dice
coefficient with five threshold levels, set as (0.1, 0.3, 0.5, 0.7, 0.9).
implementation details. the number of diffusion steps in previous works was 1000
[9] and even 4000 [19]. the literature suggests that more is better [22]. in our
experiments, we employ 100 diffusion steps, to reduce inference time.the adamw
[18] optimizer is used in all our experiments. based on the intuition that the
more rrdb blocks, the better the results, we used as many blocks as we could fit
on the gpu without overly reducing batch size.following [13], for all datasets
of the qubiq benchmark the input image resolution, as well as the test image
resolution, was 256 × 256. the experiments were performed with a batch size of
four images and eight rrdb blocks. the network depth was seven, and the number
of channels in each depth was [l, l, l, 2l, 2l, 4l, 4l], with l = 128. the
augmentations used were: random scaling by a factor sampled uniformly in the
range [0.9, 1.1], a rotation between 0 and 15 • , translation between [0, 0.1]
in both axes, and horizontal and vertical flips, each applied with a probability
of 0.5.results. we compare our method with fcn [17], mcd [6], fpm [27], daf
[25], mv-unet [13], ls-unet [12], mh-unet [7], and mrnet [13].we also compare
with models that we train ourselves, using public code amis [20], and dmise
[26]. the first is trained in a scenario where each annotator is a different
sample (""no annotator"" variant of our ablation results below), and the second is
trained on the consensus setting, similar to our method. as can be seen in table
1, our method outperforms all other methods across all datasets of qubiq
benchmark.ablation study. we evaluate alternative training variants as an
ablation study in table 2. the ""annotator"" variant, in which our model learns to
produce each annotator binary segmentation map and then averages all the results
to obtain the required soft-label map, achieves lower scores compared to the
""consensus"" variant, which is our full method. the ""no annotator"" variant, where
images were paired with random annotators without utilizing the annotator ids,
achieves a slightly lower average score compared to the ""annotator"" variant. we
also note that our ""no annotator"" variant outperforms the analog amis model in
four out of five datasets, indicating that our architecture is somewhat
preferable. in a third variant, our model learns to predict the soft-label map
that denotes the fraction of annotators that mark each image location directly.
since this results in fewer generated images, we generate c times as many images
per test sample. the score of this variant is also much lower than that of our
method.next, we study the effect of the number of generated images on
performance. the results can be seen in fig. 3. in general, increasing the
number of generated instances tends to improve performance. however, the number
of runs required to reach optimal performance varies between classes. for
example, for the brain and the prostate 1 datasets, optimal performance is
achieved using 5 generated images, while on prostate 2 the optimal performance
is achieved using 25 gen- erated images. figure 4 depicts samples from multiple
datasets and presents the progression as the number of generated images
increases. as can be seen, as the number of generated images increases, the
outcome becomes more and more similar to the target segmentation.",4
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,1.0,Introduction,"semantic segmentation aims to segment objects in an image by classifying each
pixel into an object class. training a deep neural network (dnn) for such a task
is known to be data-hungry, as labeling dense pixel-level annotations requires
laborious and expensive human efforts in practice [23,32]. furthermore, semantic
segmentation in medical imaging suffers from privacy and data sharing issues
[13,35] and a lack of experts to secure accurate and clinically meaningful
regions of interest (rois). this data shortage problem causes overfitting for
training dnns, resulting in the networks being biased by outliers and ignorant
of unseen data.to alleviate the sample size and overfitting issues, diverse data
augmentations have been recently developed. for example, cutmix [31] and cutout
[4] augment images by dropping random-sized image patches or replacing the
removed regions with a patch from another image. random erase [33] extracts
noise from a uniform distribution and injects it into patches. geometric
transformations such as elastic transformation [26] warp images and deform the
original shape of objects. alternatively, feature perturbation methods augment
data by perturbing data in feature space [7,22] and logit space [9].although
these augmentation approaches have been successful for natural images, their
usage for medical image semantic segmentation is quite restricted as objects in
medical images contain non-rigid morphological characteristics that should be
sensitively preserved. for example, basalioma (e.g., pigmented basal cell
carcinoma) may look similar to malignant melanoma or mole in terms of color and
texture [6,20], and early-stage colon polyps are mostly small and
indistinguishable from background entrail surfaces [14]. in these cases, the
underlying clinical features of target rois (e.g., polyp, tumor and cancer) can
be distorted if regional colors and textures are modified with blur-based
augmentations or geometric transformations. also, cut-and-paste and crop-based
methods carry risks of dropping or distorting key objects such that expensive
pixel-level annotations could not be properly used. considering the rois are
usually small and underrepresented compared to the backgrounds, the loss of
information may cause a fatal class imbalance problem in semantic segmentation
tasks.in these regards, we tackle these issues with a novel augmentation method
without distorting the semantics of objects in image space. this can be achieved
by slightly but effectively perturbing target objects with adversarial noises at
the object level. we first augment hard samples with adversarial attacks [18]
that deceive a network and defend against such attacks with anti-adversaries.
specifically, multi-step adversarial noises are injected into rois to maximize
loss and induce false predictions. conversely, anti-adversaries are obtained
with antiadversarial perturbations that minimize a loss which eventually become
easier samples to predict. we impose consistency regularization between these
contrasting samples by evaluating their prediction ambiguities via supervised
losses with true labels. with this regularization, the easier samples provide
adaptive guidance to the misclassified data such that the difficult (but
object-relevant) pixels can be gradually integrated into the correct prediction.
from active learning perspective [12,19], as vague samples near the decision
boundary are augmented and trained, improvement on a downstream prediction task
is highly expected.we summarize our main contributions as follows: 1) we propose
a novel online data augmentation method for semantic segmentation by imposing
objectspecific consistency regularization between anti-adversarial and
adversarial data.",4
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,0.0,Baselines.,"along with conventional augmentation methods (i.e., random horizontal and
vertical flipping denoted as 'basic' in table 1), recent methods such as cutmix
[31], cutout [4], elastic transform [26], random erase [33], drop-block [7],
gaussian noise training (gnt) [22], logit uncertainty (lu) [9] and tumor
copy-paste (tumorcp) [30] were used as baselines. their hyperparameters were
adopted from the original papers. the basic augmentation was used in all methods
including ours by default. for the training, we used k augmented images with the
given images for all baselines as in ours for a fair comparison.",4
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,4.2,Comparison with Existing Methods,"as shown in table 1, our method outperforms all baselines for all settings by at
most 10.06%p and 5.98%p miou margin on kvasir-seg and etis, respectively.
moreover, in fig. 3, our method with u-net on kvasir-seg surpasses the baselines
by ∼8.2%p and ∼7.2%p in precision and recall, respectively. note that, all
baselines showed improvements in most cases. however, our method performed
better even compared with the tumorcp which uses seven different augmentations
methods together for tumor segmentation. this is because our method preserves
the semantics of the key rois with small but effective noises unlike geometric
transformations [26,30], drop and cut-and-paste-based methods [4,7,30,31,33].
also, as we augment uncertain samples that deliberately deceive a network as in
active learning [12,16], our method is able to sensitively include the
challenging (but roi-relevant) features into prediction, unlike existing
noise-based methods that extract noises from known distributions [9,22,30].",4
Anti-adversarial Consistency Regularization for Data Augmentation: Applications to Robust Medical Image Segmentation,5.0,Conclusion,"we present a novel data augmentation method for semantic segmentation using a
flexible anti-adversarial consistency regularization. in particular, our method
is tailored for medical images that contain small and underrepresented key
objects such as a polyp and tumor. with object-level perturbations, our method
effectively expands discriminative regions on challenging samples while
preserving the morphological characteristics of key objects. extensive
experiments with various backbones and datasets confirm the effectiveness of our
method.",4
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,1.0,Introduction,"head and neck (han) cancer is a prevalent type of cancer [3] with a yearly
incidence of above 1 million cases and prevalence of above 4 million cases
worldwide, accounting for around 5% of all cancer sites [17]. radiotherapy (rt)
is a standard treatment modality for han cancer, which aims to deliver high
doses of radiation to cancerous cells while sparing nearby healthy
organs-at-risk (oars) [21]. to optimize radiation dose distribution, accurate
three-dimensional (3d) segmentation of target volumes and oars is required.
computed tomography (ct) is the primary imaging modality used for rt planning
due to its ability to provide information about electron density, however, its
low image contrast for soft tissues, including tumors, makes accurate
segmentation of soft tissue oars challenging. therefore, the integration of
complementary imaging modalities, such as magnetic resonance (mr), has been
strongly recommended in clinical practice to enhance the segmentation of several
soft tissue oars in the han region [1]. this naturally poses a question of
whether automatic oar segmentation can benefit from the mr image modality. our
study therefore aims to evaluate the impact of mr integration on the quality and
robustness of automatic oar segmentation in the han region, therefore
contributing to the growing body of research on multimodal methods for medical
image analysis.",4
Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,5.0,Conclusions,"in this study, we introduced mfm, a fusion module that aligns fms from an
auxiliary modality (e.g. mr) to fms from the primary modality (e.g. ct). the
proposed mfm is versatile, as it can be applied to any multimodal segmentation
network. however, it has to be noted that it is not symmetrical, and therefore
requires the user to specify the primary modality, which is typically the same
as the primary modality used in manual delineation (i.e. in our case ct). we
evaluated the performance of mfm combined with the nnu-net backbone for
segmentation of oars in the han region, an important task in rt cancer treatment
planning. the obtained results indicate that the performance of mfm is similar
to other state-of-the-art methods, but it outperforms other multimodal methods
in scenarios with one missing modality.",4
Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,1.0,Introduction,"integrating multi-modality medical images for tumor segmentation is crucial for
comprehensive diagnosis and surgical planning. in the clinic, the consistent
information and complementary information in multi-modality medical images
provide the basis for tumor diagnosis. for instance, the consistent anatomical
structure information offers the location feature for tumor tracking [22], while
the complementary information such as differences in lesion area among
multimodality medical images provides the texture feature for tumor
characterization. multi-modality machine learning aims to process and relate
information from multiple modalities [4]. but it is still tricky to integrate
multi-modality medical images due to the complexity of medical images.existing
methods for multi-modality medical image integration can be categorized into
three groups: (1) input-based integration methods that concatenate
multi-modality images at the beginning of the framework to fuse them directly [
19,21], (2) feature-based fusion methods that incorporate a fusion module to
merge feature maps [16,23], and (3) decision-based fusion methods that use
weighted averaging to balance the weights of different modalities [11,15].
essentially, these methods differ in their approach to modifying the number of
channels, adding additional convolutional layers with a softmax layer for
attention, or incorporating fixed modality-specific weights. however, there is
no mechanism to evaluate the reliability of information from multi-modality
medical images. since the anatomical information of different modality medical
images varies, the reliability provided by different modalities may also differ.
therefore, it remains challenging to consider the reliability of different
modality medical images when combining multi-modality medical image
information.dempster-shafer theory (dst) [18], also known as evidence theory, is
a powerful tool for modeling information, combining evidence, and making
decisions by integrating uncertain information from various sources or knowledge
[10].some studies have attempted to apply dst to medical image processing [8].
however, using evidence theory alone does not enable us to weigh the different
anatomical information from multi-modality medical images. to explore the
reliability of different sources when using evidence theory, the work by mercier
et al. [13] proposed a contextual discounting mechanism to assign weights to
different sources. furthermore, for medical image segmentation, denoising
diffusion probabilistic models (ddpm [7]) have shown remarkable performance
[9,20]. inspired by these studies, if ddpm can parse the reliability of
multi-modality medical images to weigh the different anatomy information from
them, it will provide a significant approach for tumor segmentation.in this
paper, we propose an evidence-identified ddpm (ei-ddpm) with contextual
discounting for tumor segmentation via integrating multi-modality medical
images. our basic assumption is that we can learn the segmentation feature on
single modality medical images using ddpm and parse the reliability of different
modalities medical images by evidence theory with a contextual discounting
mechanism. specifically, the ei-ddpm first utilizes parallel conditional ddpm to
learn the segmentation feature from a single modality image. next, the
evidence-identified layer (eil) preliminarily integrates multi-modality images
by comprehensively using the multi-modality uncertain information. lastly, the
contextual discounting operator (cdo) performs the final integration of
multimodality images by parsing the reliability of information from
multi-modality medical images. the contributions of this work are:-our ei-ddpm
achieves tumor segmentation by using ddpm under the guidance of evidence theory.
it provides a solution to integrate multi-modality medical images when deploying
the ddpm algorithm. -the proposed eil and cdo apply contextual discounting
guided dst to parse the reliability of information from different modalities of
medical images. this allows for the integration of multi-modality medical images
with learned weights corresponding to their reliability. -we conducted extensive
experiments using the brats 2021 [12] dataset for brain tumor segmentation and a
liver mri dataset for liver tumor segmentation. experimental results demonstrate
the superiority of ei-ddpm over other state-of-the-art (sota) methods.",4
Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,2.0,Method,"the ei-ddpm achieves tumor segmentation by parsing the reliability of
multimodality medical images. specifically, as shown in fig. 1, the ei-ddpm is
fed with multi-modality medical images into the parallel ddpm path and performs
the conditional sampling process to learn the segmentation feature from the
single modality image (sect. 2.1). next, the eil preliminary integrates
multimodality images by embedding the segmentation features from multi-modality
images into the combination rule of dst (sect. 2.2). lastly, the cdo integrates
multi-modality medical images for tumor segmentation by contextual discounting
mechanism (sect. 2.3).",4
Learning Reliability of Multi modality Medical Images for Tumor Segmentation via Evidence Identified Denoising Diffusion Probabilistic Models,4.0,Conclusion,"in this paper, we proposed a novel ddpm-based framework for tumor segmentation
under the condition of multi-modality medical images. the eil and cdo enable our
ei-ddpm to capture the reliability of different modality medical images with
respect to different tumor regions. it provides a way of deploying contextual
discounted dst to parse the reliability of multi-modality medical images.
extensive experiments prove the superiority of ei-ddpm for tumor segmentation on
multi-modality medical images, which has great potential to aid in clinical
diagnosis. the weakness of ei-ddpm is that it takes around 13 s to predict one
segmentation image. in future work, we will focus on improving sampling steps in
parallel ddpm paths to speed up ei-ddpm.",4
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,1.0,Introduction,"breast cancer is the leading cause of cancer-related fatalities among women.
currently, it holds the highest incidence rate of cancer among women in the
u.s., and in 2022 it accounted for 31% of all newly diagnosed cancer cases [1].
due to the high incidence rate, early breast cancer detection is essential for
reducing mortality rates and expanding treatment options. bus imaging is an
effective screening option because it is cost-effective, nonradioactive, and
noninvasive. however, bus image analysis is also challenging due to the large
variations in tumor shape and appearance, speckle noise, low contrast, weak
boundaries, and occurrence of artifacts.in the past decade, deep learning-based
approaches achieved remarkable advancements in bus tumor classification [2,3].
the progress has been driven by the capability of cnn-based models to learn
hierarchies of structured image representations as semantics. to extract deep
context features, cnns apply a series of convolutional and downsampling layers,
frequently organized into blocks with residual connections. nevertheless, one
disadvantage of such architectural choice is that the feature representations in
the deeper layers become increasingly abstract, leading to a loss of spatial and
contextual information. the intrinsic locality of convolutional operations
hinders the ability of cnns to model longrange dependencies while preserving
spatial information in images effectively.vision transformer (vit) [5] and its
variants recently demonstrated superior performance in image classification
tasks. these models convert input images into smaller patches and utilize the
self-attention mechanism to model the relationships between the patches.
self-attention enables vits to capture long-range dependencies and model complex
relationships between different regions of the image. however, the effectiveness
of vit-based approaches heavily relies on access to large datasets for learning
meaningful representations of input images. this is primarily because the
architectural design of vits does not rely on the same inductive biases in
feature extraction which allow cnns to learn spatially invariant
features.accordingly, numerous prior studies introduced modifications to the
original vit network specifically designed for bus image classification
[13,14,23]. in addition, several works proposed network architectures that
combined transformers and cnns [4,15,16]. for instance, mo et al. [15] proposed
a hybrid cnn-transformer incorporating bus anatomical priors. qu et al. [16]
employed squeeze and excitation blocks to enhance the feature extraction
capacity in a hybrid cnn-based vgg16 network and vit. similarly, iqbal et al.
[4] designed two hybrid cnn-transformer networks intended either for
classification or segmentation of multi-modal breast cancer images. despite the
promising results of such hybrid approaches, effectively capturing the local
patterns and global long-range dependencies in bus images remains challenging
[4,5,24].multitask learning leverages shared information across related tasks by
jointly training the model. it constrains models to learn representations that
are relevant to all tasks rather than learning task-specific details. moreover,
multitask learning acts as a regularizer by introducing inductive bias and
prevents overfitting [25] (particularly with vits), and with that, can mitigate
the challenges posed by small bus dataset sizes. in [3], the authors
demonstrated that multitask learning outperforms single-task learning approaches
for bus classification.in this study, we introduce a hybrid multitask approach,
hybrid-mt-estan, which encompasses tumor classification as a primary task and
tumor segmentation as a secondary task. hybrid-mt-estan combines the advantages
of cnns and transformers in a framework incorporating anatomical tissue
information in bus images. specifically, we designed a novel attention block
named anatomy-aware attention (aaa), which modifies the attention block of swin
transformer by considering the breast anatomy. the anatomy of the human breast
is categorized into four primary layers: the skin, premammary (subcutaneous
fat), mammary, and retromammary layers, where each layer has a distinct texture
and generates different echo patterns. the primary layers in bus images are
arranged in a vertical stack, with similar echo patterns appearing horizontally
across the images. the kernels in the introduced aaa attention blocks are
organized in rows and columns to capture the anatomical structure of the breast
tissue. in the published literature, the closest approach to ours is the work by
iqbal et al. [4], in which the authors used hybrid single-task cnn-transformer
networks for either classification or segmentation of bus images. conversely,
hybrid-mt-estan employs a multitask approach and introduces novel architectural
design. the main contributions of this work are summarized as: [17], which
employs row-column-wise kernels to learn and fuse context information in bus
images at different context scales (see fig. 2). specifically, each mt-estan
block is composed of two parallel branches consisting of four square
convolutional kernels and two consecutive row-column-wise kernels. these
specialized convolutional kernels effectively extract contextual information of
small tumors in bus images. refer to [17,22], and [3] for the implementation
details of estan and mt-estan. the source codes of these works are available at
http://busbench.midalab.net. f l = w-msa(ln(f l-1 )) + f l-1 (1)where f l and f
l are the output features of the mlp module and the (s)w-msa module for block l,
respectively; in the proposed anatomy-aware attention (aaa) block, we redesigned
the swin blocks to enhance their ability to model both global and local features
by adding an attention block based on the breast anatomy (see fig. 3). the
additional layers are defined byconcretely, we first reconstruct the i-th
feature map (y i ) by merging (m ) all patches, and afterward, we applied
average pooling (avg-p) and max pooling (max-p) layers with size (2, 2). the
outputs of (avg-p) and (max-p) layers are concatenated and up-sampled (u ) with
size (2, 2) and stride (2, 2). rowcolumn-wise kernels (a) with size (9 , 1) and
(1 , 9) are then employed to adapt to the anatomy of the breast, and finally a
sigmoid function (σ) is applied to the output of (a) multiplied by the input
feature map (y i ).",4
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,2.3,Segmentation and Classification Branches/Tasks,"the segmentation branch in fig. 1 outputs dense mask predictions of bus tumors.
it consists of four up blocks, each with three convolutional layers and one
upsampling layer (with size (2, 2) and stride (2, 2)). the settings of the
convolutional layers are adopted from [3]. in addition, the blocks receive four
skip connections from the mt-estan encoder, i.e., there is a skip connection
from each mt-estan block 1 to 4. the classification branch consists of three
dense layers, a dropout layer (50%), and the final dense layer that predicts the
tumor class into benign or malignant.",4
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,2.4,Loss Function,"we applied a multitask loss function (l mt ) that aggregates two terms: a focal
loss l f ocal for the classification task and dice loss l dice for the
segmentation task. therefore, the composite loss function is l mt = w 1 • l f
ocal + l dice , where the weight coefficient w 1 is set to apply greater
importance to the classification task as the primary task. since in medical
image diagnosis achieving high sensitivity places emphasis on the detection of
malignant lesions, we employed the focal loss for the classification task to
trade off between sensitivity and specificity. because malignant tumors are more
challenging to detect due to greater differences in margin, shape, and
appearance in bus images, focal loss forces the model to focus more on difficult
predictions. specifically, focal loss adds a factor (1 -p i ) γ to the
cross-entropy loss where γ is a focusing parameter, resulting inin the
formulation, α is a weighting coefficient, n denotes the number of image
samples, t i is the target label of the i th training sample, and p i denotes
the prediction. the segmentation loss is calculated using the commonly-employed
dice loss (l dice ) function.",4
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,3.1,Datasets,"we evaluated the performance of hybrid-mt-estan using four public datasets, hmss
[9], busi [10], busis [20], and dataset b [6]. we combined all four datasets to
build a large and diverse dataset with a total of 3,320 b-mode bus images, of
which 1,664 contain benign tumors and 1,656 have malignant tumors. table 1 shows
the detailed information for each dataset. hmss dataset does not provide the
segmentation ground-truth masks, and for this study we arranged with a group of
experienced radiologists to prepare the masks for hmss. refer to the original
publications of the datasets for more details.",4
Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network,4.0,Conclusion,"in this paper, we introduced the hybrid-mt-estan, a multitask learning approach
for bus image analysis that alleviates the lack of global contextual
infor-mation in the low-level layers of cnn-based approaches. hybrid-mt-estan
concurrently performs bus tumor classification and segmentation, with a hybrid
architecture that employs cnn-based and swin transformer layers. the proposed
approach exploits multi-scale local patterns and global long-range dependencies
provided by mt-esta and aaa transformer blocks for learning feature
representations, resulting in improved generalization. experimental validation
demonstrated significant performance improvement by hybrid-mt-estan in
comparison to current state-of-the-art models for bus classification.",4
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,1.0,Introduction,"medical image segmentation is a core step for quantitative and precision
medicine. in the past decade, convolutional neural networks (cnns) became the
sota method to achieve accurate and fast medical image segmentation [10,12,21].
nn-unet [12], which is based on unet [21], has achieved top performances on over
20 medical segmentation challenges. parallel to manually created networks such
as nn-unet, dints [10], a cnn designed by automated neural network search, also
achieved top performances in medical segmentation decathlon (msd) [1]
challenges. the convolution operation in cnn provides a strong inductive bias
which is translational equivalent and efficient in capturing local features like
boundary and texture. however, this inductive bias limits the representation
power of cnn models which means a potentially lower performance ceiling on more
challenging tasks [7]. additionally, cnn has a local receptive field and are not
able to capture long-range dependencies unlike transformers. recently, vision
transformers have been proposed, which adopt the transformers in natural
language processing by splitting images into patches (tokens) [6], and use
self-attention to learn features. the self-attention mechanism enables learning
longrange dependencies between far-away tokens. this is intriguing and numerous
works have been proposed to incorporate transformer attentions into medical
image segmentation [2,3,9,23,24,30,32,35]. among them, swinunetr [23] has
achieved the new top performance in the msd challenge and beyond the cranial
vault (btcv) segmentation challenge by pretraining on large datasets. it has a
u-shaped structure where the encoder is a swin-transformer [16].although
transformers have achieved certain success in medical imaging, the lack of
inductive bias makes them harder to be trained and requires much more training
data to avoid overfitting. the self-attentions are good at learning complicated
relational interactions for high-level concepts [5] but are also observed to be
ignoring local feature details [5]. unlike natural image segmentation
benchmarks, e.g. ade20k [34], where the challenge is in learning complex
relationships and scene understanding from a large amount of labeled training
images, many medical image segmentation networks need to be extremely focused on
local boundary details while less in need of highlevel relationships. moreover,
the number of training data is also limited. hence in real clinical studies and
challenges, cnns can still achieve better results than transformers. for
example, the top solutions in the last year miccai challenges hector [19], flare
[11], instance [15,22] and amos [13] are all cnn based. besides lacking
inductive bias and enough training data, one extra reason could be that
transformers are computationally much expensive and harder to tune. more
improvements and empirical evidence are needed before we say transformers are
ready to replace cnns for medical image segmentation.in this paper, we try to
develop a new ""to-go"" transformer for 3d medical image segmentation, which is
expected to exhibit strong performance under different data situations and does
not require extensive hyperparameter tuning. swinunetr reaches top performances
on several large benchmarks, making itself the current sota, but without
effective pretraining and excessive tuning, its performance on new datasets and
challenges is not as high-performing as expected.a straightforward direction to
improve transformers is to combine the merits of both convolutions and
self-attentions. many methods have been proposed and most of them fall into two
directions: 1) a new self-attention scheme to have convolutionlike properties
[5,7,16,25,26,29]. swin-transformer [16] is a typical work in the first
direction. it uses a local window instead of the whole image to perform
self-attention. although the basic operation is still self-attention, the local
window and relative position embedding give self-attention a conv-like local
receptive field and less computation cost. another line in 1) is changing the
self-attention operation directly. coatnet [5] unifies convolution and
self-attention with relative attention, while convit [7] uses gated positional
self-attention which is equipped with a soft convolutional inductive bias. works
in the second direction 2) employs both convolution and self-attention in the
network [3,4,8,20,27,28,30,31,33,35]. for the works in this direction, we
sum-marize them into three major categories as shown in fig. 1: 2.a) dual branch
feature fusion. mobileformer [4], conformer [20], and transfuse [33] use a cnn
branch and a transformer branch in parallel to fuse the features, thus the local
details and global features are learned separately and fused altogether.
however, this doubles the computation cost. another line of works 2.b) focuses
on the bottleneck design. the low-level features are extracted by convolution
blocks and the bottleneck is the transformer, like the transunet [3], cotr [30]
and transbts [27]. the third direction 2.c) is a new block containing both
convolution and self-attention. moat [31] removes the mlp in self-attention and
uses a mobile convolution block at the front. the moat block is then used as the
basic block in building the network. cvt [28] uses convolution as the embedding
layer for key, value, and query. nnformer [35] replaces the patch merging with
convolution with stride. although those works showed strong performances, which
works best and can be the ""to go"" transformer for 3d medical image segmentation
is still unknown. for this purpose, we design the swinunetr-v2, which improves
the current sota swi-nunetr by introducing stage-wise convolutions into the
backbone. our network belongs to the second category, which employs convolution
and self-attention directly. at each resolution level, we add a residual
convolution (resconv) block at the beginning, and the output is then used as
input to the swin transformer blocks (contains a swin block and a shifted window
swin block). moat [31] and cvt [28] add convolution before self-attention as a
micro-level building block, and nnformer has a similar design that uses
convolution with stride to replace the patch merging layer for downsampling.
differently, our work only adds a resconv block at the beginning of each stage,
which is a macro-network level design. it is used to regularize the features for
the following transformers. although simple, we found it surprisingly effective
for 3d medical image segmentation. the network is evaluated extensively on a
variety of benchmarks and achieved top performances on the word [17], flare2021
[18], msd prostate, msd lung cancer, and msd pancreas cancer datasets [1].
compared to the original swin-unetr which needs extensive recipe tuning on a new
dataset, we utilized the same training recipe with minimum changes across all
benchmarks, showcasing the straightforward applicability of swinunetr-v2 to
reach state-of-the-art without extensive hyperparameter tuning or pretraining.
we also experimented with four design variations inspired by existing works to
justify the swinunetr-v2 design.",4
SwinUNETR-V2: Stronger Swin Transformers with Stagewise Convolutions for 3D Medical Image Segmentation,3.0,Experiments,"we use extensive experiments to show its effectiveness and justify its design
for 3d medical image segmentation. to make fair comparisons with baselines, we
did not use any pre-trained weights.datasets. the network is validated on five
datasets of different sizes, targets and modalities:1) the word dataset [17] the
challenge comes from segmenting small tumors from large full 3d ct images. the
pancreas dataset contains 281 3d ct scans with annotated pancreas and tumors (or
cysts). the challenge is from the large label imbalances between the background,
pancreas, and tumor structures. for all three msd tasks, we perform 5-fold
crossvalidation with 70%/10%/20% train, validation, and test splits. these 20%
test data will not overlap with other folds and cover all data by 5 folds.",4
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,1.0,Introduction,"breast cancer is the most common cause of cancer-related deaths among women all
around the world [8]. early diagnosis and treatment is beneficial to improve the
survival rate and prognosis of breast cancer patients. mammography,
ultrasonography, and magnetic resonance imaging (mri) are routine imaging
modalities for breast examinations [15]. recent clinical studies have proven
that dynamic contrast-enhanced (dce)-mri has the capability to reflect tumor
morphology, texture, and kinetic heterogeneity [14], and is with the highest
sensitivity for breast cancer screening and diagnosis among current clinical
imaging modalities [17]. the basis for dce-mri is a dynamic t1-weighted contrast
enhanced sequence (fig. 1). t1-weighted acquisition depicts enhancing
abnormalities after contrast material administration, that is, the cancer
screening is performed by using the post-contrast images. radiologists will
analyze features such as texture, morphology, and then make the treatment plan
or prognosis assessment. computer-aided feature quantification and diagnosis
algorithms have recently been exploited to facilitate radiologists analyze
breast dce-mri [12,22], in which automatic cancer segmentation is the very first
and important step.to better support the radiologists with breast cancer
diagnosis, various segmentation algorithms have been developed [20]. early
studies focused on image processing based approaches by conducting graph-cut
segmentation [29] or analyzing low-level hand-crafted features [1,11,19]. these
methods may encounter the issue of high computational complexity when analyzing
volumetric data, and most of them require manual interactions. recently,
deep-learning-based methods have been applied to analyze breast mri. zhang et
al. [28] proposed a mask-guided hierarchical learning framework for breast tumor
segmentation via convolutional neural networks (cnns), in which breast masks
were also required to train one of cnns. this framework achieved a mean dice
value of 72% on 48 testing t1-weighted scans. li et al. [16] developed a
multi-stream fusion mechanism to analyze t1/t2-weighted scans, and obtained a
dice result of 77% on 313 subjects. gao et al. [7] proposed a 2d cnn
architecture with designed attention modules, and got a dice result of 81% on 87
testing samples. zhou et al. [30] employed a 3d affinity learning based
multi-branch ensemble network for the segmentation refinement and generated 78%
dice on 90 testing subjects. wang et al. [24] integrated a combined 2d and 3d
cnn and a contextual pyramid into u-net to obtain a dice result of 76% on 90
subjects. wang et al. [25] proposed a tumor-sensitive synthesis module to reduce
false segmentation and obtained 78% dice value. to reduce the huge annotation
burden for the segmentation task, zeng et al. [27] presented a semi-supervised
strategy to segment the manually cropped dce-mri scans, and attained a dice
value of 78%.although [27] has been proposed to alleviate the annotation effort,
to acquire the voxel-level segmentation masks is still time-consuming and
laborious, see fig. 1(c). weakly-supervised learning strategies such as extreme
points [5,21], bounding box [6] and scribbles [4] can be promising solutions.
roth et al. [21] utilized extreme points to generate scribbles to supervise the
training of the segmentation network. based on [21], dorent et al. [5]
introduced a regularized loss [4] derived from a conditional random field (crf)
formulation to encourage the prediction consistency over homogeneous regions. du
et al. [6] employed bounding boxes to train the segmentation network for organs.
however, the geometric prior used in [6] can not be an appropriate strategy for
the segmentation of lesions with various shapes. to our knowledge, currently
only one weakly-supervised work [18] has been proposed for breast mass
segmentation in dce-mri. this method employed three partial annotation methods
including single-slice, orthogonal-slice (i.e., 3 slices) and interval-slice (∼6
slices) to alleviate the annotation cost, and then constrained segmentation by
estimated volume using the partial annotation. the method obtained a dice value
of 83% using the interval-slice annotation, on a testing dataset containing only
28 patients.in this study, we propose a simple yet effective weakly-supervised
strategy, by using extreme points as annotations (see fig. 1(d)) to segment
breast cancer. specifically, we attempt to optimize the segmentation network via
the conventional trainfine-tuneretrain process. the initial training is
supervised by a contrastive loss to pull close positive voxels in feature space.
the fine-tune is conducted by using a similarity-aware propagation learning
(simple) strategy to update the pseudo-masks for the subsequent retrain. we
evaluate our method on a collected dce-mri dataset containing 206 subjects.
experimental results show our method achieves competitive performance compared
with fully supervision, demonstrating the efficacy of the proposed simple
strategy.",4
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,2.0,Method,"the proposed simple strategy and the trainfine-tuneretrain procedure is
illustrated in fig. 2. the extreme points are defined as the left-, right-,
anterior-, posterior-, inferior-, and superior-most points of the cancerous
region in 3d. the initial pseudo-masks are generated according to the extreme
points by using the random walker algorithm. the segmentation network is firstly
trained based on the initial pseudo-masks. then simple is employed to fine-tune
the network and update the pseudo-masks. at last, the network is retrained from
random initialization using the updated pseudo-masks.",4
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,2.1,Generate Initial Pseudo-masks,"we use the extreme points to generate pseudo-masks based on random walker
algorithm [9]. to improve the performance of random walker, according to [21],
we first generate scribbles by searching the shortest path on gradient magnitude
map between each extreme point pair via the dijkstra algorithm [3]. after
generating the scribbles, we propose to dilate them to increase foreground seeds
for random walker. voxels outside the bounding box (note that once we have the
six extreme points, we have the 3d bounding box of the cancer) are expected to
be the background seeds. next, the random walker algorithm is used to produce a
foreground probability map y :where ω is the spatial domain. to further increase
the area of foreground, the voxel at location k is considered as new foreground
seed if y (k) is greater than 0.8 and new background seed if y (k) is less than
0.1. then we run the random walker algorithm repeatedly. after seven times
iterations, we set foreground in the same way via the last output probability
map. voxels outside the bounding box are considered as background. the rest of
voxels remain unlabeled. this is the way initial pseudo-masks y init : ω ⊂ r 3 →
{0, 1, 2} generated, where 0, 1 and 2 represent negative, positive and
unlabeled.",4
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,3.0,Experiments,"dataset. we evaluated our method on an in-house breast dce-mri dataset collected
from the cancer center of sun yat-sen university. in total, we collected 206
dce-mri scans with biopsy-proven breast cancers. all mri scans were examined
with 1.5t mri scanner. the dce-mri sequences (tr/te = 4.43 ms/1.50 ms, and flip
angle = 10 • ) using gadolinium-based contrast agent were performed with the
t1-weighted gradient echo technique, and injected 0.2 ml/kg intravenously at 2.0
ml/s followed by 20 ml saline. the dce-mri volumes have two kinds of resolution,
0.379×0.379×1.700 mm 3 and 0.511×0.511×1.000 mm 3 .all cancerous regions and
extreme points were manually annotated by an experienced radiologist via
itk-snap [26] and further confirmed by another radiologist. we randomly divided
the dataset into 21 scans for training and the remaining scans for testing 1 .
before training, we resampled all volumes into the same target spacing
0.600×0.600×1.000 mm 3 and normalized all volumes as zero mean and unit
variance.implementation details. the framework was implemented in pytorch, using
a nvidia geforce gtx 1080 ti with 11gb of memory. we employed 3d unet [2] as our
network backbone.• train: the network was trained by stochastic gradient descent
(sgd) for 200 epochs, with an initial learning rate η = 0.01. the ploy learning
policy was used to adjust the learning rate, (1epoch/200) 0.9 . the batch size
was 2, consisting of a random foreground patch and a random background patch
located via initial segmentation y init . such setting can help alleviate class
imbalance issue. the patch size was 128 × 128 × 96. for the contrastive loss, we
set n = 100, temperature parameter τ = 0.1. • fine-tune: we initialized the
network with the trained weights. we trained it by sgd for 100 iterations, with
η = 0.0001. the ploy learning policy was also used. for the simple strategy, we
set n = 100, λ = 0.96, α = 0.96, w = 0.1. quantitative and qualitative analysis.
we first verified the efficacy of our simple in the training stage. figure 3
illustrates the pseudo-masks at different training stages. it is obvious that
our simple effectively updated the pseudomasks to make them approaching the
ground-truths. therefore, such fune-tuned pseudo-masks could be used to retrain
the network for better performance. 1 reports the quantitative dice, jaccard,
average surface distance (asd), and hausdorff distance (95hd) results of
different methods. we compared our method with an end-to-end approach [4] that
proposed to optimize network via crf-regularized loss l crf . although our l ctr
supervised method outcompeted l crf [4], the networks trained only using the
initial pseudo-masks could not achieve enough high accuracy (dice values<70%).
in contrast, the proposed simple largely boosted the performance of the
basically trained networks, by +14.74% dice and +15.16% jaccard (v.s. l crf ),
+11.81% dice and +12.65% jaccard (v.s. l ctr ). table 1 also shows the
comparison results of three general weakly-supervised strategies, including
entropy minimization [10], mean teacher [23], and bounding box [13]. our method
consistently outperformed these strategies with respect to all evaluation
metrics. furthermore, our method achieved competitive dice results compared with
fully supervision, which again proves the efficacy of the proposed simple
strategy. note that the average annotation time for extreme points and full
masks were 31 s and 95 s per scan, respectively. figure 5 visualizes the 3d
distance map between the segmented surface and ground-truth. it can be observed
that our simple consistently enhanced the segmentation.",4
SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,4.0,Conclusion,"we introduce a simple yet effective weakly-supervised learning method for breast
cancer segmentation in dce-mri. the primary attribute is to fully exploit the
simple trainfine-tuneretrain process to optimize the segmentation network via
only extreme point annotations. this is achieved by employing a similarityaware
propagation learning (simple) strategy to update the pseudo-masks. experimental
results demonstrate the efficacy of the proposed simple strategy for
weakly-supervised segmentation.",4
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,1.0,Introduction,"accurate and robust classification and segmentation of the medical image are
powerful tools to inform diagnostic schemes. in clinical practice, the
image-level classification and pixel-wise segmentation tasks are not independent
[8,27]. joint classification and segmentation can not only provide clinicians
with results for both tasks simultaneously, but also extract valuable
information and improve performance. however, improving the reliability and
interpretability of medical image analysis is still reaching.considering the
close correlation between the classification and segmentation, many researchers
[6,8,20,22,24,27,28] proposed to collaboratively analyze the two tasks with the
help of sharing model parameters or task interacting. most of the methods are
based on sharing model parameters, which improves the performance by fully
utilizing the supervision from multiple tasks [8,27]. for example, thomas et al.
[20] combined whole image classification and segmentation of skin cancer using a
shared encoder. task interacting is also a widely used method [12,24,28] as it
can introduce the high-level features and results produced by one task to
benignly guide another. however, there has been relatively little research on
introducing reliability into joint classification and segmentation. the
reliability and interpretability of the model are particularly important for
clinical tasks, a single result of the most likely hypothesis without any clues
about how to make the decision might lead to misdiagnoses and sub-optimal
treatment [10,22]. one potential way of improving reliability is to introduce
uncertainty for the medical image analysis model.the current uncertainty
estimation method can roughly include the dropoutbased [11], ensemble-based
[4,18,19], deterministic-based methods [21] and evidential deep learning
[5,16,23,30,31]. all of these methods are widely utilized in classification and
segmentation applications for medical image analysis. abdar et al. [1] employed
three uncertainty quantification methods (monte carlo dropout, ensemble mc
dropout, and deep ensemble) simultaneously to deal with uncertainty estimation
during skin cancer image classification. zou et al. [31] proposed tbrats based
on evidential deep learning to generate robust segmentation results for brain
tumor and reliable uncertainty estimations. unlike the aforementioned methods,
which only focus on uncertainty in either medical image classification or
segmentation. furthermore, none of the existing methods have considered how
pixel-wise and image-level uncertainty can help improve performance and
reliability in mutual learning.based on the analysis presented above, we design
a novel uncertaintyinformed mutual learning (uml) network for medical image
analysis in this study. our uml not only enhances the image-level and pixel-wise
reliability of medical image classification and segmentation, but also leverages
mutual learning under uncertainty to improve performance. specifically, we adopt
evidential deep learning [16,31] to simultaneously estimate the uncertainty of
both to estimate image-level and pixel-wise uncertainty. we introduce an
uncertainty navigator for segmentation (un) to generate preliminary segmentation
results, taking into account the uncertainty of mutual learning features. we
also propose an uncer- tainty instructor for classification (ui) to screen
reliable masks for classification based on the preliminary segmentation results.
our uml represents pioneering work in introducing reliability and
interpretability to joint classification and segmentation, which has the
potential to the development of more trusted medical analysis tools1 .",4
Uncertainty-Informed Mutual Learning for Joint Medical Image Classification and Segmentation,3.0,Experiments,"dataset and implementation. we evaluate the our uml network on two datasets
refuge [14] and ispy-1 [13]. refuge contains two tasks, classification of
glaucoma and segmentation of optic disc/cup in fundus images. the overall 1200
images were equally divided for training, validation, and testing. all images
are uniformly adjusted to 256 × 256 px. the tasks of ispy-1 are the pcr
prediction and the breast tumor segmentation. a total of 157 patients who suffer
the breast cancer are considered -43 achieve pcr and 114 non-pcr.for each case,
we cut out the slices in the 3d image and totally got 1,570 2d images, which are
randomly divided into the train, validation, and test datasets with 1,230, 170,
and 170 slices, respectively. we implement the proposed method via pytorch and
train it on nvidia geforce rtx 2080ti. the adam optimizer is adopted to update
the overall parameters with an initial learning rate 0.0001 for 100 epochs. the
scale of the regularizer is set as 1 × 10 -5 . we choose vgg-16 and res2net as
the encoders for classification and segmentation, separately.compared methods
and metrics. we compared our method with singletask methods and multi-task
methods. (1) single-task methods: (a) ec [17], (b) tbrats [31] and (c) transunet
[2]. evidential deep learning for classification (ec) first proposed to
parameterize classification probabilities as dirichlet distributions to explain
evidence. tbrats then extended ec to medical image segmentation. meriting both
transformers and u-net, transunet is a strong model for medical image
segmentation. (2) multi-task methods: (d) bcs [25] and (e) dsi [28]. the
baseline of the joint classification and segmentation framework (bcs) is a
simple but useful way to share model parameters, which utilize two different
encoders and decoders for learning respectively. the deep synergistic
interaction network (dsi) has demonstrated superior performance in joint task.
we adopt overall accuracy (acc) and f1 score (f1) as the evaluation criteria for
the classification task. dice score (di) and average symmetric surface distance
(assd) are chosen for the segmentation task. comparison under noisy data. to
further valid the reliability of our model, we introduce gaussian noise with
various levels of standard deviations (σ) to the input medical images. the
comparison results are shown in table 2. as can be observed that, the accuracy
of classification and segmentation significantly decreases after adding noise to
the raw data. however, benefiting from the uncertainty-informed guiding, our uml
consistently deliver impressive results. in fig. 3, we show the output of our
model under the noise. it is obvious that both the image-level uncertainty and
the pixel-wise uncertainty respond reasonably well to noise. these experimental
results can verify the reliability and interpre of the uncertainty guided
interaction between the classification and segmentation in the proposed uml. the
results of more qualitative comparisons can be found in the supplementary
material.ablation study. as illustrated in table 3, both of the proposed un and
ui play important roles in trusted mutual learning. the baseline method is
bcs.md represents the mutual feature decoder. it is clear that the performance
of classification and segmentation is significantly improved when we introduce
supervision of mutual features. as we thought, the introduction of un and ui
takes the reliability of the model to a higher level.",4
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,1.0,Introduction,"medical image segmentation is always a critical task as it can be used for
disease diagnosis, treatment planning, and anomaly monitoring. weakly supervised
semantic segmentation attracts significant attention from medical image
community since it greatly reduces the cost of dense pixel-wise labeling to get
segmentation mask. in wsss, the training labels are usually easier and faster to
obtain, like image-level tags, bounding boxes, scribbles, or point annotations.
this work only focuses on wsss with image-level tags, like whether a tumor
presents or not. in this field, previous wsss works [10,14] are dominated by
class activation map (cam) [25] and its variants [4,18,21], which was firstly
introduced as a tool to visualize saliency maps when making a class
prediction.meanwhile, denoising diffusion models [6,9] demonstrate superior
performance in image synthesis than other generative models. also, there are
several works exploring the application of diffusion models to semantic
segmentation in natural images [2,8]and medical images [15,[22][23][24]. to the
best of our knowledge, wolleb et al. [22] is the only work that introduces
diffusion models to pixel-wise anomaly detection with only classification
labels. they achieve this by utilizing an external classifier trained with
image-level annotations to guide the reverse markov chain. by passing the
gradient of the classifier, the diffusion model gradually removes the anomaly
areas during the denoising process and then obtains the anomaly map by comparing
the reconstructed and original images however, this approach is based on the
hypothesis that the classifier can accurately locate the target objects and that
the background is not changed when removing the noise. this assumption does not
always hold, especially when the distribution of positive images is diverse, and
the reconstruction error can also be accumulated after hundreds of steps. as
shown in fig. 1, the reconstructed images guided by the gradient of non-kidney
not only remove the kidney area but also change the content in the background.
another limitation of this method is the long inference time required for a
single image, as hundreds of iterations are needed to restore the image to its
original noise level. in contrast, cam approaches need only one inference to get
the saliency maps. therefore, there is ample room for improvement in using
diffusion models for wsss task. in this work, we propose a novel wsss framework
with conditional diffusion models (cdm) as we observe that the predicted noises
on different condition show difference. instead of completely removing the
noises from images, we calculate the derivative of the predicted noise after a
few stages with respect to conditions so that the related objects are
highlighted in the gradient map with less background misidentified. as the
output of diffusion model is not differentiable with respect to the discrete
condition input, we adopt the finite difference method, i.e., perturbing the
condition embedding by a small amplitude and logging the change of the output
with ddim [19] generative process. in addition, our method does not require the
full reverse denoising process for the noised images and may only need one or a
few iterations. thus the inference time of our method is comparable to that of
cam-based approaches. we evaluate our methods on two different tasks, brain
tumor segmentation and kidney segmentation, and provide the quantitative results
of both cam based and diffusion model based methods as comparison. our approach
achieves state-of-the-art performance on both datasets, demonstrating the
effectiveness of the proposed framework. we also conduct extensive ablation
studies to analyze the impact of various components in our framework and provide
reasoning for each design.",4
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,2.1,Training Conditional Denoising Diffusion Models,"suppose that we have a sample x 0 from distribution d(x|y), and y is the
condition. the condition y can be various, like different modality [20],
inpainting [3] and low resolution images [17]. in this work, y ∈ {y 0 , y 1 }
indicates the binary classification label, like brain ct scans without tumor vs.
with tumor. we then gradually add guassian noise to the original image sample
with different level t ∈ {0, 1, ..., t } aswith fixed variance {β 1 , β 2 , ...,
β t }, x t can be explicitly expressed by x 0 ,where α t := 1β t , ᾱt := t s=1 α
s . then a conditional u-net [16] θ (x, t, y) is trained to approximate the
reverse denoising process,the variance μ σ can be learnable parameters or a
fixed set of scalars, and both settings achieve comparable results in [9]. as
for the mean, after reparameterization with x t = √ ᾱt x 0 + √ 1ᾱt for ∼ n (0,
i), the loss function can be simplified as:as for how to infuse binary condition
y in the u-net, we follow the strategy in [6], using a embedding projection
function e = f (y), f ∈ r → r n , with n being the embedding dimension. then the
condition embedding is added to feature maps in different blocks. after training
the denoising model, tashiro et al. [3] proved that the network can yield the
desired conditional distribution d(x|y) given condition y.algorithm 1.
generation of wsss prediction mask using differentiate conditional model with
ddim sampling input: input image x with label y 1 , noise level q, inference
stage r, noise predictor θ , τ output: prediction mask of label y 1 for all t
from 1 to q do) end for return a",4
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,3.0,Experiments and Results,"brain tumor segmentation. brats (brain tumor segmentation challenge) [1]
contains 2,000 cases of 3d brain scans, each of which includes four different
mri modalities as well as tumor segmentation ground truth. in this work, we only
use the flair channel and treat all types of tumor as one single class. we
divide the official training set into 9:1 for training and validation purpose,
and evaluate our method on the official validation set. for preprocessing, we
slice each volume into 2d images, following the setting in [5]. then the total
number of slices in training set is 193,905, and we report metrics on the 5802
positive samples in the test set kidney segmentation. this task is conducted on
dataset from isbi 2019 chaos challenge [12], which contains 20 volumes of
t2-spir mr abdominal scans. chaos provides pixel-wise annotation for several
organs, but we focus on the kidney. we split the 20 volumes into four folds for
cross-validation, and then decompose 3d volumes to 2d slices in every fold. in
the test stage, we remove slices with area of interest taking up less than 5% of
the total area in the slice, in order to avoid the influence of extreme cases on
the average results.only classification labels are used during training the
diffusion models, and segmentation masks are used for evaluation in the test
stage. for both datasets, we repeat the evaluation protocols for four times and
report the average metrics and their standard deviation on test set.",4
Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation,3.2,Results,"comparison with state of the arts. we benchmark our methods against previous
wsss works on two datasets in table 1 & 2, in terms of dice score, mean
intersection over union (miou), and hausdorff distance (hd95). for cam based
methods, we include the classical gradcam [18] and gradcam++ [4], as well as two
more recent methods, scorecam [21] and layercam [11]. the implementation of
these cam approaches is based on the repository [7]. for diffusion based
methods, we include the only diffusion model for medical image segmentation in
the wsss literature, namely cg-diff [22]. we follow the default setting in [22],
setting noise level q = 400 and gradient scale s = 100. we also present the
results under the fully supervised learning setting, which is the upper bond of
all wsss methods (fig. 2).from the results, we can make several key
observations. firstly, our proposed method, even without classifier guidance,
outperform all other wsss methods including the classifier guided diffusion
model cg-diff on both datasets for all three metrics. when classifier guidance
is provided, the improvement gets even bigger, and cg-cdm can beat other methods
regarding segmentation accuracy. secondly, all wsss methods have performance
drop on kidney dataset compared with brats dataset. this demonstrates that the
kidney segmentation task is a more challenging task for wsss than brain tumor
task, which may be caused by the small training size and diverse appearance
across slices in the chaos dataset. time efficiency. regarding inference time
for different methods, as shown in table 1, both cdm and cg-cdm are much faster
than cg-diff. the default noise level q is set as 400 for all diffusion model
approaches, and our methods run 10 iterations during the denoising steps. for
all cam-based approaches, we add augmentation smooth and eigen smooth suggested
in [7] to reduce noise in the prediction mask. this post-processing greatly
increases the inference time. without the two smooth methods, the inference time
for gradcam is 0.031 s, but the segmentation accuracy is significantly degraded.
therefore, considering both inference time and performance, our method is a
better option than cam for wsss.ablation studies. there are several important
hyperparameters in our framework, noise level q, number of iterations r, moving
weight τ , and gradient scale s. the default setting is cg-cdm on brats dataset
with q = 400, r = 10, τ = 0.95, and s = 10. we evaluate the influence of one
hyperparameter at a time by keeping other parameters at their default values. as
illustrated in fig. 3, a few observations can be made: (1) either too large or
too small noise level can negatively influence the performance. when q is small,
most spatial information is still kept in x t and the predicted noise by
diffusion model contains no semantic knowledge. when q is large, most of the
spatial information is lost and the predicted noise can be distracted from
original structure. meanwhile, larger number of iterations can lightly improve
the dice score at the beginning. when r gets too high, the error in the
background is also accumulated after too many iterations. (2) we try different τ
in the range (0, 1.0). small τ leads to more noises in the background when
calculating the difference in different conditions. on the other hand, as τ gets
close to 1, the difference between x t-1 and x t-1 becomes minor, and the
gradient map mainly comes from the guidance of the classifier, making
localization not so accurate. thus, τ = 0.95 becomes the optimal choice for this
task. (3) as for gradient scale, fig. 3 shows that before s = 100, larger
gradient scale can boost the cdm, because at this time, the gradient from the
classifier is at the same magnitude as the difference caused by the changed
condition embedding. when the guidance of the classifier becomes dominant, the
dice score gets lower as the background is distorted by too large gradients.",4
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,1.0,Introduction,"glioma is one of the most common malignant brain tumors with varying degrees of
invasiveness [1]. brain tumor semantic segmentation of gliomas based on 3d
spatially aligned magnetic resonance imaging (samm-bts) is crucial for accurate
diagnosis and treatment planning. unfortunately, radiologists suffer from
spending several hours manually performing the samm-bts task in clinical
practice, resulting in low diagnostic efficiency. in addition, manual
delineation requires doctors to have high professionalism. therefore, it is
necessary to design an efficient and accurate glioma lesion segmentation
algorithm to effectively alleviate this problem and relieve doctors' workload
and improve radiotherapy quality.with the rise of deep learning, researchers
have begun to study deep learning-based image analysis methods [2,37].
specifically, many convolutional neural network-based (cnn-based) models have
achieved promising results [3][4][5][6][7][8]. compared with natural images,
medical image segmentation often requires higher accuracy to make subsequent
treatment plans for patients. u-net reaches an outstanding performance on
medical image segmentation by combining the features from shallow and deep
layers using skipconnection [9][10][11]. based on u-net, brugger. et al. [12]
proposed a partially reversible u-net to reduce memory consumption while
maintaining acceptable segmentation results. pei et al. [13] explored the
efficiency of residual learning and designed a 3d resunet for multi-modal brain
tumor segmentation. however, due to the lack of global understanding of images
for convolution operation, cnn-based methods struggle to model the dependencies
between distant features and make full use of the contextual information [14].
but for semantic segmentation tasks whose results need to be predicted at
pixel-level or voxel-level, both local spatial details and global dependencies
are extremely important.in recent years, models based on the self-attention
mechanism, such as transformer, have received widespread attention due to their
excellent performance in natural language processing (nlp) [15]. compared with
convolution operation, the self-attention mechanism is not restricted by local
receptive fields and can capture long-range dependencies. many works
[16][17][18][19] have applied transformers to computer vision tasks and achieved
favorable results. for classification tasks, vision transformer (vit) [19] was a
groundbreaking innovation that first introduced pure transformer layers directly
across domains. and for semantic segmentation tasks, many methods, such as setr
[20] and segformer [21], use vit as the direct backbone network and combine it
with a taskspecific segmentation head for prediction results, reaching excellent
performance on some 2d natural image datasets. for 3d medical image
segmentation, vision transformer has also been preferred by researchers. a lot
of robust variants based on transformer have been designed to endow u-net with
the ability to capture contextual information in long-distance dependencies,
further improving the semantic segmentation results of medical images
[22][23][24][25][26][27]. wang et al. [25] proposed a novel framework named
transbts that embeds the transformer in the bottleneck part of a 3d u-net
structure. peiris et al. [26] introduced a 3d swin-transformer [28] to
segmentation tasks and first incorporated the attention mechanism into
skip-connection.while transformer-based models have shown effectiveness in
capturing long-range dependencies, designing a transformer architecture that
performs well on the samm-bts task remains challenging. first, modeling
relationships between 3d voxel sequences is much more difficult than 2d pixel
sequences. when applying 2d models, 3d images need to be sliced along one
dimension. however, the data in each slice is related to three views, discarding
any of them may lead to the loss of local information, which may cause the
degradation of performance [29]. second, most existing mri segmentation methods
still have difficulty capturing global interaction information while effectively
encoding local information. moreover, current methods just stack modalities and
pass them through a network, which treats each modality equally along the
channel dimension and may ignore the contribution of different modalities. to
address the above limitations, we propose a novel encoder-decoder model, namely
dbtrans, for multi-modal medical image segmentation. in the encoder, two types
of window-based attention mechanisms, i.e., shifted window-based multi-head self
attention (shifted-w-msa) and shuffle window-based multi-head cross attention
(shuffle-w-mca), are introduced and applied in parallel to dual-branch encoder
layers, while in the decoder, in addition to shifted-w-msa mechanism, shifted
window-based multi-head cross attention (shifted-w-mca) is designed for the
dual-branch decoder layers. these mechanisms in the dual-branch architecture
greatly enhance the ability of both local and global feature extraction.
notably, dbtrans is designed for 3d medical images, avoiding the information
loss caused by data slicing.the contributions of our proposed method can be
described as follows: 1) based on transformer, we construct dual-branch encoder
and decoder layers that assemble two attention mechanisms, being able to model
close-window and distant-window dependencies without any extra computational
cost. 2) in addition to the traditional skipconnection structure, in the
dual-branch decoder, we also establish an extra path to facilitate the decoding
process. we design a shifted-w-mca-based global branch to build a bridge between
the decoder and encoder, maintaining affluent information of the segmentation
target during the decoding process. 3) for the multi-modal data adopt in the
task of samm-bts, we improve the channel attention mechanism in se-net by
applying se-weights to features from both branches in the encoder and decoder
layers. by this means, we implicitly consider the importance of multiple mri
modalities and two window-based attention branches, thereby strengthening the
fusion effect of the multi-modal information from a global perspective.",4
DBTrans: A Dual-Branch Vision Transformer for Multi-Modal Brain Tumor Segmentation,3.0,Experiments and Results,"datasets. we use the multimodal brain tumor segmentation challenge (brats 2021
[33,34,38]) as the benchmark training set, validation set, and testing set. we
divide the 1251 scans provided into 834, 208, and 209 (in a ratio of 2:1:1),
respectively for training and testing. the ground truth labels of gbm
segmentation necrotic/active tumor and edema are used to train the model. the
brats 2021 dataset reflects real clinical diagnostic species and has four
spatially aligned mri modality data, namely t1, t1ce, t2, and flair, which are
obtained from different devices or according to different imaging protocols. the
dataset contains three distinct sub-regions of brain tumors, namely peritumoral
edema, enhancing tumor, and tumor core. the data augmentation includes random
flipping, intensity scaling and intensity shifting on each axis with
probabilities set to 0.5, 0.1 and 0.1, respectively.comparative experiments. to
evaluate the effectiveness of the proposed dbtrans, we compare it with the
state-of-the-art brain tumor segmentation methods including six
transformer-based networks swin-unet [27], transbts [25], unetr [22], nnformer
[23], vt-unet-b [26], nestedformer [36] as well as the most basic cnn network 3d
u-net [31] as the baseline. during inference, for any size of 3d images, we
utilize the overlapping sliding windows technique to generate multi-class
prediction results and take average values for the voxels in the overlapping
region. the evaluation strategy adopted in this work is consistent with that of
vt-unet [26]. for other methods, we used the corresponding hyperparameter
configuration mentioned in the original papers and reported the average metrics
over 3 runs. ablation study. to further verify the contribution of each module,
we establish the ablation models based on the modules introduced above. note
that, dp-e represents the dual-branch encoder layer, while db-d represents the
dual-branch decoder layer. when the dual-branch fusion is not included, we do
not split the input, and simply fuse the features from the two branches using a
convolution layer. in all, there are 5 models included in this ablation study:
2, after applying our proposed dual-branch encoder and decoder layers to the
baseline model, the average dice score notably increased by 2.13. subsequently,
applying the dual-branch fusion module also prominently contributes to the
performance of the model by an improvement of 0.83 on the dice score. notably,
our dual-branch designs achieve higher performance while also reducing the
number of parameters required. this is because we split the original feature
embedding into two parts, thus the channel dimensions of features in two
branches are halved.",4
HartleyMHA: Self-attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation,3.1,Data and Experimental Setups,"the dataset of brats'19 with 335 cases of gliomas was used, each with four
modalities of t1, post-contrast t1, t2, and t2-flair images with 240 × 240 × 155
voxels [3]. there is also an official validation dataset of 125 cases in the
same format without given annotations. models were trained with images
downsampled by different factors (1, 2, 3, and 4) to study the robustness to
image resolution. in training, we split the training dataset (335 cases) into
90% for training and 10% for validation. in testing, each model was tested on
the official validation dataset (125 cases) with 240 × 240 × 155 voxels
regardless of the downsampling factor. the predictions were uploaded to the
cbica image processing portal3 for the results statistics of the ""whole tumor""
(wt), ""tumor core"" (tc), and ""enhancing tumor"" (et) regions [3]. we compare our
proposed hnoseg and hartleymha models with three other models:1. v-net-ds [26]:
a v-net with deep supervision representing the commonlyused encoding-decoding
architectures. 2. utnet [7]: a u-net enhanced by the transformer's attention
mechanism. 3. fno [17]: original fno without shared parameters, residual
connections, and deep supervision. the same hyperparameters as hnoseg were used.
the learnable resampling approach in sect. 2.4 was applied to all models. note
that our goal is not competing for the best accuracy but studying the robustness
to image resolution. although only the results of a dataset are shown because of
the page limit, the characteristics of the proposed models can be demonstrated
through this challenging multi-modal brain tumor segmentation problem.",4
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,1.0,Introduction,"medical image segmentation (mis) is a crucial component in medical image
analysis, which aims to partition an image into distinct regions (or segments)
that are semantically related and/or visually similar. this process is essential
for clinicians to, among others, perform qualitative and quantitative
assessments of various anatomical structures or pathological conditions and
perform imageguided treatments or treatment planning [2]. vision transformers
(vits), with their inherent ability to model long-range dependencies, have
recently been considered a promising technique to tackle mis. they process
images as sequences of patches, with each patch having a global view of the
entire image. this enables a vit to achieve improved segmentation performance
compared to traditional convolutional neural networks (cnns) on plenty of
segmentation tasks [16]. however, due to the lack of inductive biases, such as
weight sharing and locality, vits are more data-hungry than cnns, i.e., require
more data to train [31]. meanwhile, it is common to have access to multiple,
diverse, yet small-sized datasets (100 s to 1000 ss of images per dataset) for
the same mis task, e.g., ph2 [25] and isic 2018 [11] in dermatology, lits [6]
and chaos [18] in liver ct, or oasis [24] and adni [17] in brain mri. as each
dataset alone is too small to properly train a vit, the challenge becomes how to
effectively leverage the different datasets. various strategies have been
proposed to address vits' data-hunger (table 1), mainly: adding inductive bias
by constructing a hybrid network that fuses a cnn with a vit [39], imitating
cnns' shifted filters and convolutional operations [7], or enhancing spatial
information learning [22]; sharing knowledge by transferring knowledge from a
cnn [31] or pertaining vits on multiple related tasks and then fine-tuning on a
down-stream task [37]; increasing data via augmentation [34]; and non-supervised
pre-training [8]. nevertheless, one notable limitation in these approaches is
that they are not universal, i.e., they rely on separate training for each
dataset rather than incorporate valuable knowledge from related domains. as a
result, they can incur additional training, inference, and memory costs, which
is especially challenging when dealing with multiple small datasets in the
context of mis tasks. multi-domain learning, which trains a single universal
model to tackle all the datasets simultaneously, has been found promising for
reducing computational demands while still leveraging information from multiple
domains [1,21]. to the best of our knowledge, multi-domain universal models have
not yet been investigated for alleviating vits' data-hunger.given the
inter-domain heterogeneity resulting from variations in imaging protocols,
scanner manufacturers, etc. [4,21], directly mixing all the datasets for
training, i.e., joint training, may improve a model's performance on one dataset
while degrading performance on other datasets with non-negligible unrelated
domain-specific information, a phenomenon referred to as negative knowledge
transfer (nkt) [1,38]. a common strategy to mitigate nkt in computer vision is
to introduce adapters aiding the model to adapt to different domains, i.e.,
multi-domain adaptive training (mat), such as domain-specific mechanisms
[21,26,32], and squeeze-excitation layers [28,35] (table 1). however, those mat
techniques are built based on cnn rather than vit or are scalable, i.e., the
models' size at the inference time increases linearly with the number of
domains.to address vits' data-hunger, in this work, we propose mdvit, a novel
fixedsize multi-domain vit trained to adaptively aggregate valuable knowledge
from multiple datasets (domains) for improved segmentation. in particular, we
introduce a domain adapter that adapts the model to different domains to
mitigate negative knowledge transfer caused by inter-domain heterogeneity.
besides, for better representation learning across domains, we propose a novel
mutual knowledge distillation approach that transfers knowledge between a
universal network (spanning all the domains) and additional domain-specific
network branches.we summarize our contributions as follows: (1) to the best of
our knowledge, we are the first to introduce multi-domain learning to alleviate
vits' data-hunger when facing limited samples per dataset. (2) we propose a
multi-domain vit, mdvit, for medical image segmentation with a novel domain
adapter to counteract negative knowledge transfer and with mutual knowledge
distillation to enhance representation learning. (3) the experiments on 4 skin
lesion segmentation datasets show that our multi-domain adaptive training
outperforms separate and joint training (st and jt), especially a 10.16%
improvement in iou on the skin cancer detection dataset compared to st and that
mdvit outperforms state-of-the-art data-efficient vits and multi-domain learning
strategies.",4
MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets,0.0,Datasets and Evaluation Metrics:,"we study 4 skin lesion segmentation datasets collected from varied sources: isic
2018 (isic) [11], dermofit image library (dmf) [3], skin cancer detection (scd)
[14], and ph2 [25], which contain 2594, 1300, 206, and 200 samples,
respectively. to facilitate a fairer performance comparison across datasets, as
in [4], we only use the 1212 images from dmf that exhibited similar lesion
conditions as those in other datasets. we perform 5-fold cross-validation and
utilize dice and iou metrics for evaluation as [33].implementation details: we
conduct 3 training paradigms: separate (st), joint (jt), and multi-domain
adaptive training (mat), described in sect. 1, to train all the models from
scratch on the skin datasets. images are resized to 256 × 256 and then augmented
through random scaling, shifting, rotation, flipping, gaussian noise, and
brightness and contrast changes. the encoding transformer blocks' channel
dimensions are [64, 128, 320, 512] (fig. 1-a). we use two transformer layers in
each transformer block and set the number of heads in mhsa to 8. the hidden
dimensions of the cnn bridge and auxiliary peers are 1024 and 512. we deploy
models on a single titan v gpu and train them for 200 epochs with the adamw [23]
optimizer, a batch size of 16, ensuring 4 samples from each dataset, and an
initial learning rate of 1×10 -4 , which changes through a linear decay
scheduler whose step size is 50 and decay factor γ = 0.5.",4
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,1.0,Introduction,"deep learning methods have demonstrated their tremendous potential when it comes
to medical image segmentation. however, the success of most existing
architectures relies on the availability of pixel-level annotations, which are
difficult to produce [1]. furthermore, these methods are known to be
inadequately equipped for distribution shifts. therefore, cross-modality
generalization is needed when one imaging modality has insufficient training
data. for instance, conditions such as vestibular schwannoma, where new hrt2
sequences are set to replace cet1 for diagnosis to mitigate the use of contrast
agents, is a sample use case [2]. recently billot et al. [3] proposed a domain
randomisation strategy to segment images from a wide range of target contrasts
without any fine-tuning. the method demonstrated great generalization capability
for brain parcellation, but the model performance when exposed to tumors and
pathologies was not quantified. this challenge could also be addressed through
unsupervised domain-adaptive approaches, which transfer the knowledge available
in the ""source"" modality s from pixel-level labels to the ""target"" imaging
modality t lacking annotations [4].several generative models attempt to
generalize to a target modality by performing unsupervised domain adaptation
through image-to-image translation and image reconstruction. in [5], by learning
to translate between ct and mr cardiac images, the proposed method jointly
disentangles the domain specific and domain invariant features between each
modality and trains a segmenter from the domain invariant features. other
methods [6][7][8][9][10][11][12] also integrate this translation approach, but
the segmenter is trained in an end-to-end manner on the synthetic target images
generated from the source modality using a cycle-gan [13] model. these methods
perform well but do not explicitly use the unannotated target modality data to
further improve the segmentation.in this paper, we propose m-genseg, a novel
training strategy for crossmodality domain adaptation, as illustrated in fig. 1.
this work leverages and extends genseg [14], a generative method that uses
image-level ""diseased"" or ""healthy"" labels for semi-supervised segmentation.
given these labels, the model imposes an image-to-image translation objective
between the image domain presenting tumor lesions and the domain corresponding
to an absence of lesions. therefore, like in low-rank atlas based methods
[15][16][17] the model is taught to find and remove a lesion, which acts as a
guide for the segmentation. we incorporate cross-modality image segmentation
with an image-to-image translation objective between source and target
modalities. we hypothesize both objectives are complementary since genseg helps
localizing the tumors on unannotated target images, while modality translation
enables fine-tuning the segmenter on the target modality by displaying annotated
pseudo-target images. we evaluate m-genseg on a modified version of the brats
2020 dataset, in which each type of sequence (t1, t2, t1ce and flair) is
considered as a distinct modality. we demonstrate that our model can better
generalize than other state-of-the-art methods to the target modality.
healthy-diseased translation. we propose to integrate image-level supervision to
the cross-modality segmentation task with genseg, a model that introduces
translation between domains with a presence (p) or absence (a) of tumor lesions.
leveraging this framework has a two-fold advantage here. indeed, (i) training a
genseg module on the source modality makes the model aware of the tumor
appearances in the source images even with limited source pixel-level
annotations. this helps to preserve tumor structures during the generation of
pseudo-target samples (see sect. 2.1). furthermore, (ii) training a second
genseg module on the target modality allows to further close the domain gap by
extending the segmentation objective to unannotated target data.",4
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,2.1,M-GenSeg: Semi-supervised Segmentation,"in order to disentangle the information common to a and p, and the information
specific to p, we split the latent representation of each image into a common
code c and a unique code u. essentially, the common code contains information
inherent to both domains, which represents organs and other structures, while
the unique code stores features like tumor shapes and location. in the two
fol-lowing paragraphs, we explain p→a and a→p translations for source images.
the same process is applied for target images by replacing s notation with t
.presence to absence translation. given an image s p of modality s in the
presence domain p, we use an encoder e s to compute the latent representation [c
s p , u s p ]. a common decoder g s com takes as input the common code c s p and
generates a healthy version s pa of that image by removing the apparent tumor
region. simultaneously, both common and unique codes are used by a residual
decoder g s res to output a residual image δ s pp , which corresponds to the
additive change necessary to shift the generated healthy image back to the
presence domain. in other words, the residual is the disentangled tumor that can
be added to the generated healthy image to create a reconstruction s pp of the
initial diseased image: like approaches in [18][19][20] we therefore generate
diseased samples from healthy ones for data augmentation. however, m-genseg aims
primarily at tackling cross-modality lesion segmentation tasks, which is not
addressed in these studies. furthermore, note that these methods are limited to
data augmentation and do not incorporate any unannotated diseased samples when
training the segmentation network, as achieved by our model with the p→a
translation.modality translation. our objective is to learn to segment tumor
lesions in a target modality by reusing potentially scarce image annotations in
a source modality. note that for each modality m ∈ {s, t }, m-genseg holds a
segmentation decoder g m seg that shares most of its weights with the residual
decoder g m res , but has its own set of normalization parameters and a
supplementary classifying layer. thus, through the absence and presence
translations, these segmenters have already learned how to disentangle the tumor
from the background. however, supervised training on a few example annotations
is still required to learn how to transform the resulting residual
representation into appropriate segmentation maps. while this is a fairly
straightforward task for the source modality using pixel-level annotations,
achieving this for the target modality is more complex, justifying the second
unsupervised translation objective between source and target modalities. based
on the cyclegan [13] approach, modality translations are performed via two
distinct generators that share their encoder with the genseg task. more
precisely, combined with the encoder e s a decoder g t enables performing s→t
modality translation, while the encoder e t and a second decoder g s perform the
t→s modality translation. to maintain the anatomical information, we ensure
cycle-consistency by reconstructing the initial images after mapping them back
to their original modality. we note for the t→s→t cycle. note that to perform
the domain adaptation, training the model to segment only the pseudo-target
images generated by the s→t modality generator would suffice (in addition to the
diseased/healthy target translation). however, training the segmentation on
diseased source images also imposes additional constraints on encoder e s ,
ensuring the preservation of tumor structures. this constraint proves beneficial
for the translation decoder g t as it generates pseudo-target tumoral samples
that are more reliable. segmentation is therefore trained on both diseased
source images s p and their corresponding synthetic target images s t p , when
provided with annotations y s . to such an extent, two segmentation masks are
predicted ŷs = g s seg • e s (s p ) and ŷst = g t seg • e t (s t p ).",4
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,2.3,Implementation Details,"training and hyper-parameters. all models are implemented using pytorch and are
trained on one nvidia a100 gpu with 40 gb memory. we used a batch size of 15, an
amsgrad optimizer (β 1 = 0.5 and β 2 = 0.999) and a learning rate of 10 -4 . our
models were trained for 300 epochs and weights of the segmentation model with
the highest validation dice score were saved for evaluation. the same on-the-fly
data augmentation as in [14] was applied for all runs. each training experiment
was repeated three times with a different random seed for weight initialization.
the performance reported is the mean of all test dice scores, with standard
deviation, across the three runs. the following parameters yielded both great
modality and absence/presence translations: λ mod adv = 3, λ mod cyc = 20, λ gen
adv = 6, λ gen rec = 20 and λ gen lat = 2. note that optimal λ seg varies
depending on the fraction of pixel-level annotations provided to the network for
training.architecture. one distinct encoder, common decoder,
residual/segmentation decoder, and modality translation decoder are used for
each modality. the architecture used for encoders, decoders and discriminators
is the same as in [14]. however, in order to give insight on the model's
behaviour and properly choose the semantic information relevant for each
objective, we introduced attention gates [22] in the skip connections. figure 2a
shows the attention maps generated for each type of decoder. as expected,
residual decoders focus towards tumor areas. more interestingly, in order not to
disturb the process of healthy image generation, common decoders avoid lesion
locations. finally, modality translators tend to focus on salient details of the
brain tissue, which facilitates contrast redefinition needed for accurate
translation.",4
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,3.1,Datasets,"experiments were performed on the brats 2020 challenge dataset [23][24][25],
adapted for the cross-modality tumor segmentation problem where images are known
to be diseased or healthy. amongst the 369 brain volumes available in brats, 37
were allocated each for validation and test steps, while the 295 left were used
for training. we split the 3d brain volumes into 2 hemispheres and extracted 2d
axial slices. any slices with at least 1% tumor by brain surface area were
considered diseased. those that didn't show any tumor lesion were labelled as
healthy images. datasets were then assembled from each distinct pair of the four
mri contrasts available (t1, t2, t1ce and flair). to constitute unpaired
training data, we used only one modality (source or target) per training volume.
all the images are provided with healthy/diseased weak labels, distinct from the
pixel-level annotations that we provide only to a subset of the data. note that
the interest for cross-sequence segmentation is limited if multi-parametric
acquisitions are performed as is the case in brats. however, this modified
version of the dataset provides an excellent study case for the evaluation of
any modality adaptation method for tumor segmentation.",4
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,3.2,Model Evaluation,"domain adaptation. we compared m-genseg with accsegnet [10] and attent [6], two
high performance models for domain-adaptative medical image segmentation. to
that extent, we performed domain-adaptation experiments with source and target
modalities drawn from t1, t2, flair and t1ce. we used available github code for
the two baselines and performed fine-tuning on our data. for each possible
source/target pair, pixel-level annotations were only retained for the source
modality. we show in fig. 2b several presence to absence translations and
segmentation examples on different target modality images. although no
pixel-level annotations were provided for the target modality, tumors were well
disentangled from the brain, resulting in a successful presence to absence
translation, as well as segmentation. note that for hypo-intense lesions (t1 and
t1ce), m-genseg still manages to convert complex residuals into consistent
segmentation maps. we plot in fig. 3 the dice performance on the target modality
for (i) supervised segmentation on source data without domain adaptation, (ii)
domain adaptation methods and (iii) uagan [26], a model designed for unpaired
multi-modal datasets, trained on all source and target data. over all modality
pairs our model shows an absolute dice score increase of 0.04 and 0.08,
respectively, compared to accsegnet and attent. annotation deficit. m-genseg
introduces the ability to train with limited pixel-level annotations available
in the source modality. we show in fig. 4 the dice scores for models trained
when only 1%, 10%, 40%, or 70% of the source t1 modality and 0% of the t2 target
modality annotations were available. while performance is severely dropping at
1% of annotations for the baselines, our model shows in comparison only a slight
decrease. we thus claim that m-genseg can yield robust performance even when a
small fraction of the source images is annotated.reaching supervised
performance. we report that, when the target modality is completely unannotated,
m-genseg reaches 90% of uagan's performance (vs 81% and 85% for attent and
accsegnet). further experiments showed that with a fully annotated source
modality, it is sufficient to annotate 25% of the target modality to reach 99%
of the performance of fully-supervised uagan (e.g. m-genseg: 0.861 ± 0.004 vs
uagan: 0.872 ± 0.003 for t1 → t2 experiment). thus, the annotation burden could
be reduced with m-genseg.",4
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,4.0,Conclusion,"we propose m-genseg, a new framework for unpaired cross-modality tumor
segmentation. we show that m-genseg is an annotation-efficient framework that
greatly reduces the performance gap due to domain shift in cross-modality tumor
segmentation. we claim that healthy tissues, if adequately incorporated to the
training process of neural networks like in m-genseg, can help to better
delineate tumor lesions in segmentation tasks. however, top performing methods
on brats are 3d models. thus, future work will explore the use of full 3d images
rather than 2d slices, along with more optimal architectures. our code is
available: https://github.com/maloadba/mgenseg_2d.",4
M-GenSeg: Domain Adaptation for Target Modality Tumor Segmentation with Annotation-Efficient Supervision,0.0,0,"p ) and s pp = s pa + δ s pp(1) absence to presence translation. concomitantly,
a similar path is implemented for images in the healthy domain. given an image s
a of modality s in domain a, we generate a translated version in domain p. to do
so, a synthetic tumor δ s ap is generated by sampling a code from the normal
distribution n (0, i) a , u s a ] as follows:",4
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,1.0,Introduction,"simultaneous multi-index quantification (i.e., max diameter (md), center point
coordinates (x o , y o ), and area), segmentation, and uncertainty prediction of
liver tumor have essential significance for the prognosis and treatment of
patients [6,16]. in clinical settings, segmentation and quantitation are
manually performed by the clinicians through visually analyzing the
contrast-enhanced mri images (cemri) [9,10,18]. however, as shown in fig. 1(b),
contrast-enhanced fig. 1. our method integrates segmentation and quantification
of liver tumor using multi-modality ncmri, which has the advantages of avoiding
contrast agent injection, mutual promotion of multi-task, and reliability and
stability. mri (cemri) has the drawbacks of being toxic, expensive, and
time-consuming due to the need for contrast agents (ca) to be injected [2,4].
moreover, manually annotating medical images is a laborious and tedious process
that requires human expertise, making it manpower-intensive, subjective, and
prone to variation [14]. therefore, it is desirable to provide a reliable and
stable tool for simultaneous segmentation, quantification, and uncertainty
analysis, without requiring the use of contrast agents, as shown in fig.
1(a).recently, an increasing number of works have been attempted on liver tumor
segmentation or quantification [25,26,28,30]. as shown in fig. 1(c), the work
[26] attempted to use the t2fs for liver tumor segmentation, while it ignored
the complementary information between multi-modality ncmri of t2fs and dwi. in
particular, there is evidence that diffusion-weighted imaging (dwi) helps to
improve the detection sensitivity of focal lesions as these lesions typically
have higher cell density and microstructure heterogeneity [20]. the study in
[25,30] attempted to quantify the multi-index of liver tumor, however, the
approach is limited to using multi-phase cemri that requires the injection of
ca. in addition, all these works are limited to a single task and ignore the
constraints and mutual promotion between multi-tasks. available evidence
suggests that uncertainty information regarding segmentation results is
important as it guides clinical decisions and helps understand the reliability
of the provided segmentation. however, current research on liver tumors tends to
overlook this vital task.to the best of our knowledge, although many works focus
on the simultaneous quantization, segmentation, and uncertainty in medical
images (i.e., heart [3,5,11,27], kidney [17], polyp [13]). no attempt has been
made to automatically liver tumor multi-task via integrating multi-modality
ncmri due to the following challenges: (1) the lack of an effective
multi-modality mri fusion mechanism. because the imaging characteristics between
t2fs and dwi have significant differences (i.e., t2fs is good at anatomy
structure information while dwi is good at location information of lesions
[29]). (2) the lack of strategy for capturing the accurate boundary information
of liver tumors. due to the lack of contrast agent injection, the boundary of
the lesion may appear blurred or even invisible in a single ncmri, making it
challenging to accurately capture tumor boundaries [29]. (3) the lack of an
associated multi-task framework. because segmentation and uncertainty involve
pixel-level classification, whereas quantification tasks involve image-level
regression [11]. this makes it challenging to integrate and optimize the
complementary information between multi-tasks.in this study, we propose an
edge-aware multi-task network (eamtnet) that integrates the multi-index
quantification (i.e., center point, max-diameter (md), and area), segmentation,
and uncertainty. our basic assumption is that the model should capture the
long-range dependency of features between multimodality and enhance the boundary
information for quantification, segmentation, and uncertainty of liver tumors.
the two parallel cnn encoders first extract local feature maps of multi-modality
ncmri. meanwhile, to enhance the weight of tumor boundary information, the sobel
filters are employed to extract edge maps that are fed into edge-aware feature
aggregation (eafa) as prior knowledge. then, the eafa module is designed to
select and fuse the information of multi-modality, making our eamtnet edge-aware
by capturing the long-range dependency of features maps and edge maps. lastly,
the proposed method estimates segmentation, uncertainty prediction, and
multi-index quantification simultaneously by combining multi-task and cross-task
joint loss.the contributions of this work mainly include: (1) for the first
time, multiindex quantification, segmentation, and uncertainty of the liver
tumor on multimodality ncmri are achieved simultaneously, providing a
time-saving, reliable, and stable clinical tool. (2) the edge information
extracted by the sobel filter enhances the weight of the tumor boundary by
connecting the local feature as prior knowledge. (3) the novel eafa module makes
our eamtnet edge-aware by capturing the long-range dependency of features maps
and edge maps for feature fusion. the source code will be available on the
author's website.",4
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.0,Method,"the eamtnet employs an innovative approach for simultaneous tumor multiindex
quantification, segmentation, and uncertainty prediction on multimodality ncmri.
as shown in fig. 2, the eamtnet inputs multi-modality ncmri of t2fs and dwi for
capturing the feature and outputs the multiindex quantification, segmentation,
and uncertainty. specifically, the proposed approach mainly consists of three
steps: 1) the cnn encoders for capturing feature maps and the sobel filters for
extracting edge maps (sect. 2.1); 2) the edge-aware feature aggregation (eafa)
for multi-modality feature selection and fusion via capturing the long-distance
dependence (sect. 2.2); and 3) multi-task prediction module (sect. 2.3).",4
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.1,CNN Encoder for Feature Extraction,"in step 1 of fig. 2, the multi-modality ncmri (i.e., χ i t 2 ∈ r h×w , χ i dw i
∈ r h×w ) are fed into two parallel encoders and the sobel filter to extract the
feature maps (i.e., g i t 2 ∈ r h×w ×n , g i dw i ∈ r h×w ×n ) and the
corresponding edge maps (i.e., edge i t 2 ∈ r h×w , edge i dw i ∈ r h×w )
respectively. specifically, eamtnet employs unet as the backbone for
segmentation because the cnn encoder has excellent capabilities in low-range
semantic information extraction [15]. the two parallel cnn encoders have the
same architecture where each encoder contains three shallow convolutional
network blocks to capture features of adjacent slices. each conv block consists
of a convolutional layer, batch normalization, relu, and non-overlapping
subsampling. at the same time, eamt-net utilizes the boundary information
extracted by the sobel filter [19] as prior knowledge to enhance the weight of
tumor edge information to increase the awareness of the boundary.",4
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,2.3,Multi-task Prediction,"in step 3 of fig. 2, the eamtnet outputs the multi-modality quantification ŷq
(i.e., md, x o , y o and area), segmentation result ŷs and uncertainty map ûi .
specifically, for the quantification path, ŷq is directly obtained by performing
a linear layer to the feature f from eafa. for the segmentation and uncertainty
path, the output feature f from eafa is first reshaped into a 2d feature map f
out . then, to scale up to higher-resolution images, a 1 × 1 convolution layer
is employed to change the channel number of f out for feeding into the decoder.
after upsampling by the cnn decoder, eamtnet predicts the segmentation result ŷs
with h × w and uncertainty map ûi with h × w . the cnn decoder contains three
shallow deconv blocks, which consist of deconv layer, batch normalization, and
relu. inspired by [24], we select the entropy map as our uncertainty measure.
given the prediction probability after softmax, the entropy map is computed as
follows:where z i is the probability of pixel x belonging to category i. when a
pixel has high entropy, it means that the network is uncertain about its
classification. therefore, pixels with high entropy are more likely to be
misclassified. in other words, its entropy will decrease when the network is
confident in a pixel's label.under the constraints of uncertainty, the eamtnet
can effectively rectify the errors in tumor segmentation because the uncertainty
estimation can avoid overconfidence and erroneous quantification [23]. moreover,
the eamtnet novelly make represent different tasks in a unified framework,
leading to beneficial interactions. thus, the quantification performance is
improved through backpropagation by the joint loss function l multi-task . the
function comprises segmentation loss l seg and quantification loss l qua , where
the loss function l seg is utilized for optimizing tumor segmentation, and l qua
is utilized for optimization of multi-index quantification. it can be defined
as:where ŷs represents the prediction, and y i represents the ground truth
label. the sum is performed on s pixels, ŷi task represents the predicted
multi-index value, and y i task represents the ground truth of multi-index
value, task ∈ {md, x, y , area}.",4
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,3.0,Experimental Results and Discussion,"for the first time, eamtnet has achieved high performance with the dice
similarity coefficient (dsc) up to 90.01 ± 1.23%, and the mean absolute error
(mae) of the md, x o , y o and area are down to 2.72 ± 0.58 mm,1.87±0.76 mm,
2.14 ± 0.93 mm and 15.76 ± 8.02 cm 2 , respectively.dataset and configuration.
an axial dataset includes 250 distinct subjects, each underwent initial standard
clinical liver mri protocol examinations with corresponding pre-contrast images
(t2fs [4mm]) and dwi [4mm]) was collected. the ground truth was reviewed by two
abdominal radiologists with 10 and 22 years of experience in liver imaging,
respectively. if any interpretations demonstrated discrepancies between the
reviewers, they would re-evaluate the examinations together and reach a
consensus. to align the paired images of t2 and dwi produced at different times.
we set the t2 as the target image and the dwi as the source image to perform the
pre-processing of non-rigid registration between t2 and dwi by using the demons
non-rigid registration method. it has been widely used in the field of medical
image registration since it was proposed by thirion [21]. we perform the demons
non-rigid registration on an open-source toolbox dirart using matlab
2017b.inspired by the work [22], we set the scaling factor d k to 64 in equation
(1). all experiments were assessed with a 5-fold cross-validation test. to
quantitatively evaluate the segmentation results, we calculated the dice
coefficient scores (dsc) metric that measures the overlapping between the
segmentation prediction and ground truth [12]. to quantitatively evaluate the
quantification results, we calculated the mean absolute error (mae). our eamtnet
was implemented using ubuntu 18.04 platform, python v3.6, pytorch v0.4.0, and
running on two nvidia gtx 3090ti gpus.accurate segmentation. the segmentation
performance of eamtnet has been validated and compared with three
state-of-the-art (sota) segmentation methods (transunet [1], unet [15], and
unet++ [31]). furthermore, to ensure consistency in input modality, the channel
number of the first convolution layer in the three comparison methods is set to
2. the visual examples of liver tumors are shown in fig. 3, it is evident that
our proposed eamtnet outperforms the three sota methods. some quantitative
analysis results are shown in table 1 and table 2, our network achieves high
performance with the dsc of 90.01 ± 1.23% (5.39% higher than the second-best).
the results demonstrate that edge-aware, multi-modality fusion, and uncertainty
prediction are essential for segmentation.ablation study. to verify the
contributions of edge-aware feature aggregation (eafa) and uncertainty, we
performed ablation study and compared and performance of different networks.
first, we removed the eafa and used concatenate, meaning we removed fusion
multi-modality (no-eafa). then, we removed the uncertainty task
(no-uncertainty). the quantitative analysis results of these ablation studies
are shown in table 1. our method exhibits high performance in both segmentation
and quantification, indicating that each component of the eamtnet plays a vital
role in liver tumor segmentation and quantification.performance comparison with
state-of-the-art. the eamtnet has been validated and compared with three sota
segmentation methods and two sota quantification methods (i.e., resnet-50 [7]
and densenet [8]). furthermore, the channel number of the first convolution
layer in the two quantification comparison methods is set to 2 to ensure the
consistency of input modalities. the visual segmentation results are shown in
fig. 3. moreover, the quantitative results (as shown in table 2) corresponding
to the visualization results (i.e., fig. 3) obtained from the existing
experiments further demonstrate that our method outperforms the three sota
methods. specifically, compared with the second-best approach, the dsc is
boosted from 84.62 ± 1.45% to 90.01 ± 1.23%. the quantitative analysis results
are shown in table 3. it is evident that our method outperforms the two sota
methods with a large margin in all metrics, owing to the proposed multi-modality
fusing and multi-task association.",4
Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,4.0,Conclusion,"in this paper, we have proposed an eamtnet for the simultaneous segmentation and
multi-index quantification of liver tumors on multi-modality ncmri. the new eafa
enhances edge awareness by utilizing boundary information as prior knowledge
while capturing the long-range dependency of features to improve feature
selection and fusion. additionally, multi-task leverages the prediction
discrepancy to estimate uncertainty, thereby improving segmentation and
quantification performance. extensive experiments have demonstrated the proposed
model outperforms the sota methods in terms of dsc and mae, with great potential
to be a diagnostic tool for doctors.",4
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,1.0,Introduction,"automatic detection of brain tumors from magnetic resonance imaging (mri) is
complex, tedious, and time-consuming because there are a lot of missed,
misinterpreted, and misleading tumor-like lesions in the images of the brain
tumors [8]. most of the current work focuses on brain tumor classification and
segmentation from mri and detection tasks are less explored [1,13,22]. while
existing studies showed that various convolutional neural networks (cnns) are
efficient for brain tumor detection, the performance of using you only look once
(yolo) networks is scarcely investigated [12,20,[23][24][25]27].with the rapid
development of cnns, the accuracies of different visual tasks are constantly
improved. however, the increasingly complex network architecture in cnn-based
models, such as resnet [6], densenet [9], inception [28], etc. renders the
inference speed slower. though many advanced cnns deliver higher accuracy, the
complicated multi-branch designs (e.g., residual-addition in resnet and
branch-concatenation in inception) make the models difficult to implement and
customize, slowing down the inference and reducing memory utilization. the
depth-wise separable convolutions used in mobilenets [7] also reduce the upper
limit of the gpu inference speed. in addition, 3 × 3 regular convolution is
highly optimized by some modern computing libraries. consequently, vgg [26] is
still heavily used for real-world applications in both research and
industries.repvgg [2] is an extension of vgg via reparametrization to accelerate
inference time. repvgg uses a multi-branch topological architecture during the
training phase, which is then reparameterized to a simplified single-branch
architecture during the inference phase. in terms of the optimization strategy
of network training, reparameterization was introduced in yolov6 [16], yolov7
[31], and yolov6 v3.0 [17]. yolov6 and yolov6 v3.0 employ reparameterization
from repvgg. repconv, a repvgg without an identity connection, is converted from
repvgg during inference time in yolov6, yolov6 v3.0, and yolov7 (named repconvn
in yolov7). due to the removal of identity connections in repconv, direct access
to resnet or the concatenation in densenet can provide more diversity of
gradients for different feature maps. grouped convolutions, which use a group of
convolutions with multiple kernels per layer, like repvgg, can also
significantly reduce the computational complexity of the model, but there is no
information communication between groups, which limits the ability of feature
extraction of the convolution operator. in order to overcome the disadvantage of
grouped convolutions, shufflenet v1 [34] and v2 [21] introduced the channel
shuffle operation to facilitate information flows across different feature
channels. in addition, when comparing spatial pyramid pooling & cross stage
partial network plus convbnsilu (sppcspc) in yolov7 with spatial pyramid pooling
fast (sppf) in yolov5 [10] and yolov8 [11], it is found that more convolution
layers in sppcspc architecture slow down the computation of the network.
nevertheless, spp [4,5]",4
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.1,Dataset Details,"to evaluate the proposed rcs-yolo model, we used the brain tumor detection 2020
dataset (br35h) [3], with a total of 701 images in the 'train' and 'val' two
folders, 500 images of which are the 'train' folder were selected as the
training set, while the other 201 images in the 'val' folder as the testing set.
for the input size of 640×640 image, the actual corresponding size is 44×32. the
small object is defined as the object whose pixel size is less than 32 × 32
defined by the ms coco dataset [18], so there are no small objects in the brain
tumor medical image data sets, and the scale change of the target boxes is
smooth, almost square. the label boxes of the brain images were normalized (see
supplementary material sect. 1).",4
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,3.4,Results,"to highlight the accuracy and rapidity of the proposed model for the detection
of brain tumor medical image data set, table 1 shows the performance comparison
between our proposed detector and other state-of-the-art object detectors. the
time duration of fps includes data preprocessing, forward model inference, and
post-processing. the long border of the input images is set as 640 pixels. the
short border adaptively scales without distortion, whilst keeping the grey
filling with 32 times the pixels of the short border.it can be seen that
rcs-yolo with the advantages of incorporating the rcs-osa module performs well.
compared with yolov7, the flops of the object detectors of this paper decrease
by 8.8g, and the inference speed improves by 43.4 fps. in terms of detection
rate, precision improves by 0.024; ap 50 increases by 0.01; ap 50:95 by 0.006.
also, rcs-yolo is faster and more accurate than yolov6-l v3.0 and yolov8l.
although the ap 50:95 of rcs-yolo equals that of yolov8l, it doesn't obscure the
essential advantage of rcs-yolo. the results clearly show the superior
performance and efficiency of our method, compared to the state-of-the-art for
brain tumor detection. as shown in supplementary material fig. 2, brain tumor
regions are accurately detected from mri by using the proposed method.",4
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,4.0,Conclusion,"we developed an rcs-yolo network for fast and accurate medical object detection,
by leveraging the reparameterized convolution operator rcs based on channel
shuffle in the yolo architecture. we designed an efficient one-shot aggregation
module rcs-osa based on rcs, which serves as a computational unit in the
backbone and neck of a new yolo network. evaluation of the brain mri dataset
shows superior performance for brain tumor detection in terms of both speed and
precision, as compared to yolov6, yolov7, and yolov8 models.",4
RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection,0.0,0,"yolo-based model for fast brain tumor detection. evaluation on a publicly
available brain tumor detection annotated dataset shows superior detection
accuracy and speed compared to other state-of-the-art yolo architectures.",4
Certification of Deep Learning Models for Medical Image Segmentation,1.0,Introduction,"for the past decade, deep neural networks have dominated the computer vision
community and provided near human performance on many different tasks, including
classification [18], segmentation [24], and image generation [16]. given these
impressive results, convolutional neural networks are now used on a daily basis
in fields like healthcare, self-driving cars, and robotics, to cite a few. in
medical imaging, convolutional neural networks are particularly used to segment
organs or regions of interest on different modalities such as x-rays, ct scans,
mris, or ultrasound [36]. indeed, segmentation techniques and variations of 2d
and 3d u-nets are currently the state-of-the-art to identify and isolate tumors,
blood vessels, organs, or other structures within an image and provide crucial
help to physicians for medical diagnosis, screening, and prognosis
[32].nowadays, segmentation models are gaining widespread adoption in modern
clinical practice and are being used with increasing frequency, making the
results of these models critical for many patients. however, it is now commonly
known that neural networks can be vulnerable to adversarial attacks [17,34],
i.e., small input perturbations invisible to humans crafted specifically such
that the network performs errors. over the past few years, a large body of work
has devised empirical defenses against adversarial attacks for classification
tasks [3,17,25], as well as segmentation tasks [37], including applications on
medical imaging [27]. although state-of-the-art empirical defenses provide
significant robustness, these defenses do not guarantee theoretical robustness
and stronger attacks can be crafted to break them [5]. recently, certified
defenses, for classification [2,11,26] and segmentation [15,23], have been
proposed to guarantee the accuracy and reliability of neural networks. however,
certified defenses for segmentation in the context of medical imaging are still
lacking, even if models are getting market approvals (e.g., fda, ce) and are
already adopted in clinical practice.in this paper, we provide the first method
for certified robustness in the context of segmentation for medical imaging. we
leverage the randomized smoothing strategy [11,15], and the recent work on
diffusion models [7] to achieve stateof-the-art certified robustness for
segmentation models. randomized smoothing consists in convolving the neural
network with a gaussian distribution (i.e., by adding noise to the input) in
order to obtain a smooth segmentation model. from the smoothness properties of
the segmentation model, we can derive a robustness guarantee and compute a
certified dice score. we go even further by using diffusion models to first
denoise the perturbed input and boost the certified robustness. by extension, we
show that current diffusion models, trained on 'classical images' generalize
well to medical datasets for denoising tasks. extensive experiments on five
public medical datasets of chest x-rays [21,31], skin lesions [10], and
colonoscopies [6], and different popular segmentation models, prove the
potential of our method. we hope that this study will provide the first step
towards robustness guarantees for medical image segmentation.",4
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,1.0,Introduction,"prostate segmentation from magnetic resonance imaging (mri) is a crucial step
for diagnosis and treatment planning of prostate cancer. recently, deep
learningbased approaches have greatly improved the accuracy and efficiency of
automatic prostate mri segmentation [7,8]. yet, their success usually requires a
large amount of labeled medical data, which is expensive and expertise-demanding
in practice. in this regard, semi-supervised learning (ssl) has emerged as an
attractive option as it can leverage both limited labeled data and abundant
unlabeled data [3,[9][10][11]15,16,[21][22][23][24][25][26]28]. nevertheless,
the effectiveness of ssl is heavily dependent on the quantity and quality of the
unlabeled data.regarding quantity , the abundance of unlabeled data serves as a
way to regularize the model and alleviate overfitting to the limited labeled
data. unfortunately, such ""abundance"" may be unobtainable in practice, i.e., the
local unlabeled pool is also limited due to restricted image collection
capabilities or scarce patient samples. as a specific case shown in table 1,
there are only limited prostate scans available per center. taking c1 as a case
study, if the amount of local unlabeled data is limited, existing ssl methods
may still suffer from inferior performance when generalizing to unseen test data
(fig. 1). to efficiently enrich the unlabeled pool, seeking support from other
centers is a viable solution, as illustrated in fig. 1. yet, due to differences
in imaging protocols and variations in patient demographics, this solution
usually introduces data heterogeneity, lead-ing to a quality problem. such
heterogeneity may impede the performance of ssl which typically assumes that the
distributions of labeled data and unlabeled data are independent and identically
distributed (i.i.d.) [16]. thus, proper mechanisms are called for this practical
but challenging ssl scenario.here, we define this new ssl scenario as multi-site
semi-supervised learning (ms-ssl), allowing to enrich the unlabeled pool with
multi-site heterogeneous images. being an under-explored scenario, few efforts
have been made. to our best knowledge, the most relevant work is ahdc [2].
however, it only deals with additional unlabeled data from a specific source
rather than multiple arbitrary sources. thus, it intuitively utilizes
image-level mapping to minimize dual-distribution discrepancy. yet, their
adversarial min-max optimization often leads to instability and it is difficult
to align multiple external sources with the local source using a single image
mapping network.in this work, we propose a more generalized framework called
categorylevel regularized unlabeled-to-labeled (cu2l) learning, as depicted in
fig. 2, to achieve robust ms-ssl for prostate mri segmentation. specifically,
cu2l is built upon the teacher-student architecture with customized learning
strategies for local and external unlabeled data: (i) recognizing the importance
of supervised learning in data distribution fitting (which leads to the failure
of cps [3] in ms-ssl as elaborated in sec. 3), the local unlabeled data is
involved into pseudolabel supervised-like learning to reinforce fitting of the
local data distribution; (ii) considering that intra-class variance hinders
effective ms-ssl, we introduce a non-parametric unlabeled-to-labeled learning
scheme, which takes advantage of the scarce expert labels to explicitly
constrain the prototype-propagated predictions, to help the model exploit
discriminative and domain-insensitive features from heterogeneous multi-site
data to support the local center. yet, observing that such scheme is challenging
when significant shifts and various distributions are present, we further
propose category-level regularization, which advocates prototype alignment, to
regularize the distribution of intra-class features from arbitrary external data
to be closer to the local distribution; (iii) based on the fact that
perturbations (e.g., gaussian noises [15]) can be regarded as a simulation of
heterogeneity, perturbed stability learning is incorporated to enhance the
robustness of the model. our method is evaluated on prostate mri data from six
different clinical centers and shows promising performance on tackling ms-ssl
compared to other semi-supervised methods.",4
Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,3.0,Experiments and Results,"materials. we utilize prostate t2-weighted mr images from six different clinical
centers (c1-6) [1,4,5] to perform a retrospective evaluation. rizes the
characteristics of the six data sources, following [7,8], where [7,8] also
reveal the severity of inter-center heterogeneity here through extensive
experiments. the heterogeneity comes from the differences in scanners, field
strengths, coil types, disease and in-plane/through-plane resolution. compared
to c1 and c2, scans from c3 to c6 are taken from patients with prostate cancer,
either for detection or staging purposes, which can cause inherent semantic
differences in the prostate region to further aggravate heterogeneity. following
[7,8], we crop each scan to preserve the slices with the prostate region only
and then resize and normalize it to 384 × 384 px in the axial plane with zero
mean and unit variance. we take c1 or c2 as the local target center and randomly
divide their 30 scans into 18, 3, and 9 samples as training, validation, and
test sets, respectively.implementation and evaluation metrics. the framework is
implemented on pytorch using an nvidia geforce rtx 3090 gpu. considering the
large variance in slice thickness among different centers, we adopt the 2d
architecture. specifically, 2d u-net [12] is adopted as our backbone. consists
of the cross-entropy loss and the k-regional dice loss [6]. the maximum
consistency weight w max is set to 0.1 [20,26]. t max is set to 20,000. k is
empirically set to 2. the network is trained using the sgd optimizer and the
learning rate is initialized as 0.01 and decayed by multiplication with (1.0t/t
max ) 0.9 . data augmentation is applied, including random flip and rotation. we
adopt the dice similarity coefficient (dsc) and jaccard as the evaluation
metrics and the results are the average over three runs with different
seeds.comparison study. table 2 presents the quantitative results with either c1
or c2 as the local target center, wherein only 6 or 8 local scans are annotated.
besides the supervised-only baselines, we include recent top-performing ssl
methods [2,3,11,14,15,17,20,25,26] for comparison. all methods are implemented
with the same backbone and training protocols to ensure fairness. as observed,
compared to the supervised-only baselines, our cu2l with {6, 8} local labeled
scans achieves {19.15%, 17.42%} and {9.1%, 6.44%} dsc improvements in {c1, c2},
showing its effectiveness in leveraging multi-site unlabeled data. despite the
violation of the assumption of i.i.d. data, existing ssl methods can still
benefit from the external unlabeled data to some extent compared to the results
using local data only as shown in fig. 1, revealing that the quantity of
unlabeled data has a significant impact. however, due to the lack of proper
mechanisms for learning from heterogeneous data, limited improvement can be
achieved by them, especially for cps [3] and fixmatch [14] in c2. particularly,
cps relies on cross-modal pseudo labeling which exploits all the unlabeled data
in a supervised-like fashion. we attribute its degradation to the fact that
supervised learning is crucial for distribution fitting, which supports our
motivation of performing pseudo-label learning on local unlabeled data only. as
a result, its models struggle to determine which distribution to prioritize.
meanwhile, the most relevant ahdc [2] is mediocre in ms-ssl, mainly due to the
instability of adversarial training and the difficulty of aligning multiple
distributions to the local distribution via a single image-mapping network. in
contrast, with specialized mechanisms for simultaneously learning informative
representations from multi-site data and handling heterogeneity, our cu2l
obtains the best performance over the recent ssl methods. figure 3(a) further
shows that the predictions of our method fit more accurately with the ground
truth.ablation study. to evaluate the effectiveness of each component, we
conduct an ablation study under the setting with 6 local labeled scans, as shown
in fig. 2(b). firstly, when we remove l u p l (cu2l-1), the performance drops by
{5.69% (c1), 3.05%(c2)} in dsc, showing that reinforcing confirmation on local
distribution is critical. cu2l-2 represents the removal of both l u2l and l cr ,
and it can be observed that such an unlabeled-to-labeled learning approach
combined with class-level regularization is crucial for exploring multi-site
data. if we remove l cr which accompanies with l u2l (cu2l-3), the performance
degrades, which justifies the necessity of this regularization to reduce the
difficulty of unlabeled-to-labeled learning process. cu2l-4 denotes the removal
of l u sta . as observed, such a typical stability loss [15] can further improve
the performance by introducing hand-crafted noises to enhance the robustness to
real-world heterogeneity.",4
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,1.0,Introduction,"segmenting the prostate anatomy and detecting tumors is essential for both
diagnostic and treatment planning purposes. hence, the task of developing domain
generalisable prostate mri segmentation models is essential for the safe
translation of these models into clinical practice. deep learning models are
susceptible to textural shifts and artefacts which is often seen in mri due to
variations in the complex acquisition protocols across multiple sites [12].the
most common approach to tackle domain shifts is with data augmentation
[16,33,35] and adversarial training [11,30]. however, this increases training
time and we propose to tackle the problem head on by learning shape only
embedding features to build a shape dictionary using vector quantisation [31]
which can be sampled to compose the segmentation output. we therefore
hypothesise by limiting the search space to a set of shape components, we can
improve generalisability of a segmentation model. we also propose to correctly
sample and compose shape components with local and global topological
constraints by tracking topological features as we compose the shape components
in an ordered manner. this is achieved using a branch of algebraic topology
called cellular sheaf theory [8,19]. we hypothesise this approach will produce
more anatomically meaningful segmentation maps and improve tumour
localisation.the contributions of this paper are summarized as follows: 1. this
work considers shape compositionality to enhance the generalisability of deep
learning models to segment the prostate on mri. 2. we use cellular sheaves to
aid compositionality for segmentation as well as improve tumour localisation.",4
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,3.0,Related Work,"there have been various deep learning based architectures developed for prostate
tumour segmentation [3,15,18]. there is however no work looking at developing
models which generalise well to target domains after training on one source
domain known as single domain generalisation (sdg). effective data augmentation
techniques, such as cutout [16], mixup [34] and bigaug [35] offer a
straightforward approach to enhance the generalisability of segmentation models
across different domains. recent methods have utilized adversarial techniques,
such as advbias [11], which trains the model to generate bias field deformations
and enhance its robustness.randconv [33] incorporates a randomized convolution
layer to learn textural invariant features. self-supervised strategies such as
jigen [9] can also improve generalisability. the principle of compositionality
has been integrated into neural networks for tasks such as image classification
[23], generation [2] and more recently, segmentation [26,28] to improve
generalisability. the utilization of persistent homology in deep learning-based
segmentation is restricted to either generating topologically accurate
segmentations in the output space [21] or as a subsequent processing step [14].
the novel approach of topological auto-encoders [27] marks the first instance of
incorporating persistent homology to maintain the topological structure of the
data manifold within the latent representation. cellular sheaves were used to
provide a topological insight into the poor performance of graph neural networks
in the heterophilic setting [7]. recently, cellular sheaves were used as a
method of detecting patch based merging relations in binary images [20].
finally, [4] recently proposed using sheaf theory to construct a shape space
which allows one to precisely define how to glue shapes together in this shape
space.",4
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,0.0,Illustration:,"we illustrate our methodology of using cellular sheaves with a simple example in
fig. 2. here, we show y as perfectly matching the ground truth label (not
one-hot encoded) divided into 2 patches. y is a topological space with the
subspaces,each element in p is associated with a subspace in v such that the
inclusion relationship is satisfied. therefore, in fig. 2a, p defines that y 1
and y 2 associated with (1, 0) and (0, 1) respectively are glued together to
form y = y 1 ∪ y 2 which maps with (1, 1). a cellular sheaf f over p is created
by assigning a vector space to p ∈ p by deriving a persistence diagram, d for
each element in v associated with p as shown in fig. 2b. the arrows in fig. 2b
are inclusion maps defined as ρ .,. . persistence diagrams are computed from the
sequence of nested cubical complexes of each subspace in v. the persistence
diagrams in fig. 2b are formed by overlapping the persistence diagrams for each
class segmentation. note, persistence diagrams contain infinite points in the
form (τ, τ ) (diagonal line in persistence diagrams) which always allows a
bijection between two persistence diagrams. the main advantage of our approach
is that in addition to ensuring correct local topology (patch level) and global
topology (image level), we also force our network to produce topologically
accurate patches correctly merged together in a topology preserving manner which
matches the ground truth. for example in fig. 2b, y 2 contains 3 connected
components glued onto y 1 containing 2 connected components to form y , which
also has 3 connected components. this means an extra connected component is
added by y 2 due to tumour which therefore improves patch-wise tumour
localisation. it also indicates the other 2 connected components in y 2 are
merged into the 2 connected components in y 1 to form 2 larger connected
components (peripheral and transitional zone) in y . hence, the same 2 vectors
present in both f(1, 0) and f(0, 1) representing the peripheral and transitional
zone are also in f (1,1). this is also known as a local section in f.",4
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,5.2,Results,"in the task of anatomical segmentation, the first two columns of table 1 show
the results for the domain shift from runmc in the decathlon dataset to
bmc.here, we demonstrate that our method improves segmentation performance in
all evaluation metrics compared to the baseline, nn-unet and the other sdg
methods. similar findings are noted for the domain shift from the internal
dataset to the runmc data in the prostatex2 dataset (second two columns of table
1).in table 2, we note our method significantly improves tumour segmentation and
localisation performance. we visualise our findings with an example in fig. 3,
where there is improved localisation of the tumour and the correct number of
tumour components enforced by our topological loss. this significantly reduces
the false positive rate highlighted in table 2. also, note the more anatomically
plausible zonal segmentations. however, our method is restricted by the number
of low dimensional shape components in the shape dictionary used to compose the
high dimensional segmentation output. therefore, our approach can fail to
segment the finer details of prostate tumours due to its high shape variability
which leads to coarser but better localised tumour segmentations.",4
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,6.0,Conclusion,"in conclusion, we propose shape compositionality as a way to improve the
generalisability of segmentation models for prostate mri. we devise a method to
learn texture invariant and shape equivariant features used to create a
dictionary of shape components. we use cellular sheaf theory to help model the
composition of sampled shape components from this dictionary in order to produce
more anatomically meaningful segmentations and improve tumour localisation.",4
A Sheaf Theoretic Perspective for Robust Prostate Segmentation,0.0,Comparison:,"we compare our method with the nnunet [22] and several approaches to tackle sdg
segmentation namely, randconv [33], advbias [11], jigen [9] and bigaug [35]
applied to the baseline model. we also compare to a compositionality driven
segmentation method called the vmfnet [26].training: in all our experiments, the
models were trained using adam optimization with a learning rate of 0.0001 and
weight decay of 0.05. training was run for up to 500 epochs on three nvidia rtx
2080 gpus. the performance of the models was evaluated using the dice score,
betti error [21] and hausdorff distance. we evaluate tumour localisation by
determining a true positive if the tumour segmentation overlaps by a minimum of
one pixel with the ground truth.in our ablation studies, the minimum number of
shape components required in d for the zonal and zonal + tumour segmentation
experiments was 64 and 192 respectively before segmentation performance dropped.
see supplementary material for ablation experiments analysing each component of
our framework.",4
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,1.0,Introduction,"transformers [7,21,30] have seen wide-scale adoption in medical image
segmentation as either components of hybrid architectures [2,3,8,9,31,33] or
standalone techniques [15,25,34] for state-of-the-art performance. the ability
to learn longrange spatial dependencies is one of the major advantages of the
transformer architecture in visual tasks. however, transformers are plagued by
the necessity of large annotated datasets to maximize performance benefits owing
to their limited inductive bias. while such datasets are common to natural
images (imagenet-1k [6], imagenet-21k [26]), medical image datasets usually
suffer from the lack of abundant high quality annotations [19]. to retain the
inherent inductive bias of convolutions while taking advantage of architectural
improvements of transformers, the convnext [22] was recently introduced to
re-establish the competitive performance of convolutional networks for natural
images. the con-vnext architecture uses an inverted bottleneck mirroring that of
transformers, composed of a depthwise layer, an expansion layer and a
contraction layer (sect. 2.1), in addition to large depthwise kernels to
replicate their scalability and long-range representation learning. the authors
paired large kernel con-vnext networks with enormous datasets to outperform
erstwhile state-of-the-art transformer-based networks. in contrast, the vggnet
[28] approach of stacking small kernels continues to be the predominant
technique for designing convnets in medical image segmentation. out-of-the-box
data-efficient solutions such as nnunet [13], using variants of a standard unet
[5], have still remained effective across a wide range of tasks.the convnext
architecture marries the scalability and long-range spatial representation
learning capabilities of vision [7] and swin transformers [21] with the inherent
inductive bias of convnets. additionally, the inverted bottleneck design allows
us to scale width (increase channels) while not being affected by kernel sizes.
effective usage in medical image segmentation would allow benefits from -1)
learning long-range spatial dependencies via large kernels, 2) less intuitively,
simultaneously scaling multiple network levels. to achieve this would require
techniques to combat the tendency of large networks to overfit on limited
training data. despite this, there have been recent attempts to introduce large
kernel techniques to the medical vision domain. in [18], a large kernel 3d-unet
[5] was used by decomposing the kernel into depthwise and depthwise dilated
kernels for improved performance in organ and brain tumor segmentationexploring
kernel scaling, while using constant number of layers and channels. the convnext
architecture itself was utilized in 3d-ux-net [17], where the transformer of
swinunetr [8] was replaced with convnext blocks for high performance on multiple
segmentation tasks. however, 3d-ux-net only uses these blocks partially in a
standard convolutional encoder, limiting their possible benefits.in this work,
we maximize the potential of a convnext design while uniquely addressing
challenges of limited datasets in medical image segmentation. we present the
first fully convnext 3d segmentation network, mednext, which is a scalable
encoder-decoder network, and make the following contributions: mednext achieves
state-of-the-art performance against baselines consisting of transformer-based,
convolutional and large kernel networks. we show performance benefits on 4 tasks
of varying modality (ct, mri) and sizes (ranging from 30 to 1251 samples),
encompassing segmentation of organs and tumors. we propose mednext as a strong
and modernized alternative to standard convnets for building deep networks for
medical image segmentation.",4
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,3.2,Datasets,"we use 4 popular tasks, encompassing organ as well as tumor segmentation tasks,
to comprehensively demonstrate the benefits of the mednext architecture -1)
beyond-the-cranial-vault (btcv) abdominal ct organ segmentation [16], 2) amos22
abdominal ct organ segmentation [14] 3) kidney tumor segmentation challenge 2019
dataset (kits19) [11], 4) brain tumor segmentation challenge 2021 (brats21) [1].
btcv, amos22 and kits19 datasets contain 30, 200 and 210 ct volumes with 13, 15
and 2 classes respectively, while the brats21 dataset contains 1251 mri volumes
with 3 classes. this diversity shows the effectiveness of our methods across
imaging modalities and training set sizes.",4
MedNeXt: Transformer-Driven Scaling of ConvNets for Medical Image Segmentation,4.2,Performance Comparison to Baselines,"there are 2 levels at which mednext successfully overcomes existing baselines -5
fold cv and public testset performance. in 5-fold cv scores in table 2,
med-next, with 3 × 3 × 3 kernels, takes advantage of depth and width scaling to
provide state-of-the-art segmentation performance against every baseline on all
4 datasets with no additional training data. mednext-l outperforms or is
competitive with smaller variants despite task heterogeneity (brain and kidney
tumors, organs), modality (ct, mri) and training set size (btcv: 18 samples vs
brats21: 1000 samples), establishing itself as a powerful alternative to
established methods such as nnunet. with upkern and 5 × 5 × 5 kernels, mednext
takes advantage of full compound scaling to improve further on its own small
kernel networks, comprehensively on organ segmentation (btcv, amos22) and in a
more limited fashion on tumor segmentation (kits19, brats21).furthermore, in
leaderboard scores on official testsets (fig. 1c), 5-fold ensembles for
mednext-l (kernel: 5 × 5 × 5) and nnunet, its strongest competitor are compared
-1) btcv: mednext beats nnunet and, to the best of our knowledge, is one of the
leading methods with only supervised training and no extra training data (dsc:
88.76, hd95: 15.34), 2) amos22: mednext not only surpasses nnunet, but is also
rank 1 (date: 09.03.23) currently on the leaderboard (dsc: 91.77, nsd: 84.00),
3) kits19: mednext exceeds nnunet performance (dsc: 91.02), 4) brats21: mednext
surpasses nnunet in both volumetric and surface accuracy (dsc: 88.01, hd95:
10.69). mednext attributes its performance solely to its architecture without
leveraging techniques like transfer learning (3d-ux-net) or repeated 5-fold
ensembling (unetr, swinunetr), thus establishing itself as the state-of-the-art
for medical image segmentation.",4
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,1.0,Introduction,"fluorodeoxyglucose positron emission tomography (pet) is widely recognized as an
essential tool in oncology [10], playing an important role in the stag-ing,
monitoring, and follow-up radiotherapy (rt) planning [2,19]. delineation of
region of interest (roi) is a crucial step in rt planning. it enables the
extraction of semi-quantitative metrics such as standardized uptake values
(suvs), which normalize pixel intensities based on patient weight and
radiotracer dose [20]. manual delineation is a time-consuming and laborious task
that is prone to poor reproducibility in medical imaging, and this is
particularly true for pet, due to its low signal-to-noise ratio and limited
spatial resolution [10]. in addition, manual delineation depends heavily on the
expert's prior knowledge, which often leads to large inter-observer and
intra-observer variations [8]. therefore, there is an urgent need for developing
accurate automatic segmentation algorithms in pet images which will reduce
expert workload, speed up rt planning while reducing intra-observer
variability.in the last decade, cnns have demonstrated remarkable achievements
in medical image segmentation tasks. this is primarily due to their ability to
learn informative hierarchical features directly from data. however, as
illustrated in [9,23], it is rather difficult for cnns to recognize the object
boundary precisely due to the information loss in the successive downsampling
layers. despite the headway made in using cnns, their applications have been
restricted to the generation of pixel-wise segmentation maps instead of smooth
contour. although cnns may yield satisfactory segmentation results, low values
of the loss function may not always indicate a meaningful segmentation. for
instance, a noisy result can create incorrect background contours and blurry
object boundaries near the edge pixels [6]. to address this, a kernel
smoothing-based probability contour (kspc) approach was proposed in our previous
work [22]. instead of a pixel-wise analysis, we assume that the true suvs come
from a smooth underlying spatial process that can be modelled by kernel
estimates. the kspc provides a surface over images that naturally produces
contour-based results rather than pixel-wise results, thus mimicking experts'
hand segmentation. however, the performance of kspc depends heavily on the
tuning parameters of bandwidth and threshold in the model, and it lacks
information from other patients.beyond tumour delineation, another important use
of functional images, such as pet images is their use for designing imrt dose
painting (dp). in particular, dose painting uses functional images to paint
optimised dose prescriptions based on the spatially varying radiation
sensitivities of tumours, thus enhancing the efficacy of tumour control [14,18].
one of the popular dp strategies is dose painting by contours (dpbc), which
assigns a homogeneous boost dose to the subregions defined by suv thresholds.
however, there is an urgent need to develop image segmentation approaches that
reproducibly and accurately identify the high recurrent-risk contours [18]. our
previously proposed kspc provides a clear framework to calculate the probability
contours of the suv values and can readily be used to define an objective
strategy for segmenting tumours into subregions based on metabolic activities,
which in turn can be used to design the imrt dp strategy.to address both tumour
delineation and corresponding dose painting challenges, we propose to combine
the expressiveness of deep cnns with the versa-tility of kspc in a unified
framework, which we call kspc-net. in the proposed kspc-net, a cnn is employed
to learn directly from the data to produce the pixel-wise bandwidth feature map
and initial segmentation map, which are used to define the tuning parameters in
the kspc module. our framework is completely automatic and differentiable. more
specifically, we use the classic unet [17] as the cnn backbone and evaluate our
kspc-net on the publicly available miccai hecktor (head and neck tumor
segmentation) challenge 2021 dataset. our proposed kspc-net yields superior
results in terms of both dice similarity scores and hausdorff distance compared
to state-of-art models. moreover, it can produce contour-based segmentation
results which provide a more accurate delineation of object edges and provide
probability contours as a byproduct, which can readily be used for dp planning.",4
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,2.1,Kernel Smoothing Based Probability Contour,"kernel-based method and follow up approach of modal clustering [13,16] have been
used to cluster high-dimensional random variables and natural-scene image
segmentation. in this work, we propose to model the pixel-specific suv as a
discretized version of the underlying unknown smooth process of ""metabolic
activity"". the smooth process can then be estimated as kernel smoothed surface
of the suvs over the domain of the entire slice. in particular, let y = (y 1 , y
2 , ..., y n ) denote n pixel's suv in a 2d pet image sequentially, and x i = (x
i1 , x i2 ), i = 1, ..., n denote position vector with x i1 and x i2 being the
position in 2d respectively. note that x i ∈ r d and d = 2 in our case. we
assume that for each position vector x, the suv represents the frequency of x
appearing in the corresponding grid. the suv surface can therefore be modelled
as kernel density estimate (kde) [3,15] of an estimated point x, which is
defined generally aswhere k is a kernel function and h is a symmetric, positive
definite, d×d matrix of smoothing tuning parameters, called bandwidth which
controls the orientation and amount of smoothing via the scaled kernelon the
other hand, since x t is counted y i times at the same position, eq. 1 can be
further simplified asa scaled kernel is positioned so that its mode coincides
with each data point x i which is expressed mathematically as k h (x-x i ). in
this paper, we have used a guassian kernel which is denoted as:which is a normal
distribution with mean x i and variance-covariance matrix h. therefore, we can
interpret f in eq. ( 2) as the probability mass of the data point x which is
estimated by smoothing the suvs of the local neighbourhood using the gaussian
kernel. the resulting surface built by the kde process can be visualized in fig.
1(c). by placing a threshold plane, a contour-based segmentation map can
naturally be obtained. note that one can obtain a pixel-based segmentation map,
by thresholding the surface at the observed grid points. after delineating the
gross tumour volume, a follow-up application of the kernel smoothed surface is
to construct probability contours. mathematically, a 100 ω% region of a density
f is defined as the level set l(f ω ) = {f (x) ≥ f ω } with its corresponding
contour level f ω such that p(x∈ l(f ω ) = 1ω, where x is a random variable and
l(f ω ) has a minimal hypervolume [11]. in other words, for any ω ∈ (0, 1), the
100 ω% contour refers to the region with the smallest area which encompasses 100
ω% of the probability mass of the density function [11]. in practice, f ω can be
estimated using the following result.result. the estimated probability contour
level f ω can be computed as the ω-th quantile of fω of f (x 1 ; h), ..., f (x n
; h) (proof in supplementary materials).the primary advantage of utilizing
probability contours is their ability to assign a clear probabilistic
interpretation on the defined contours, which are scale-invariant [5]. this
provides a robust definition of probability under the perturbation of the input
data. in addition, these contours can be mapped to the imrt dose painting
contours, thus providing an alternative prescription strategy for imrt. examples
of the application of probability contours will be demonstrated and explained in
sect. 4.2.",4
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,2.2,The KsPC-Net Architecture,"in the kspc module, the model performance heavily depends on the bandwidth
matrix h and it is often assumed that each kernel shares the same scalar
bandwidth parameter. however, one may want to use different amounts of smoothing
in the kernel at different grid positions. the commonly used approach for
bandwidth selection is cross-validation [4], which is rather time-consuming even
in the simpler scalar situation. in this paper, we instead use the classic
2d-unet [17] as our cnn backbone to compute the pixel-level bandwidth feature
map, which informs the kspc bandwidth. additionally, we obtain the optimal
threshold for constructing the kspc contour from the initial segmentation map.
as shown in fig. 2 the proposed kspc-net integrates the kspc approach with a cnn
backbone (unet) in an end-to-end differentiable manner. first, the initial
segmentation map and pixel-level bandwidth parameter map h(x i1 , x i2 ) of kspc
are learned from data by the cnn backbone. then the kspc module obtains the
quantile threshold value for each image by identifying the quantile
corresponding to the minimum suv of the tumour class in the initial segmentation
map. the next step involves transmitting the bandwidth map, quantile threshold,
and raw image to kspc module to generate the segmentation map and its
corresponding probability contours. the resulting output from kspc is then
compared to experts' labels using a dice similarity loss function, referred to
kspc loss. additionally, the initial unet segmentation can produce another loss
function, called cnn loss, which serves as an auxiliary supervision for the cnn
backbone. the final loss can then be constructed as the weighted sum of cnn loss
and kspc loss. by minimizing the final loss, the error can be backpropagated
through the entire kspc architecture to guide the weights updating the cnn
backbone.",4
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,3.1,Dataset,"the dataset is from the hecktor challenge in miccai 2021 (head and neck tumor
segmentation challenge). the hecktor training dataset consists of 224 patients
diagnosed with oropharyngeal cancer [1]. for each patient, fdg-pet input images
and corresponding labels in binary description (0 s and 1 s) for the primary
gross tumour volume are provided and co-registered to a size of 144 × 144 × 144
using bounding box information encompassing the tumour. five-fold
cross-validation is used to generalize the performance of models.",4
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,4.1,Results on HECKTOR 2021 Dataset,"to evaluate the performance of our kspc-net, we compared it with the results of
5-fold cross-validation against three widely-used models namely, the standard 2d
unet, the 2d residual unet and the 3d unet. additionally, we compare our
performance against newly developed approaches msa-net [7] and ccut-net [21]
which were reported in the hecktor 2021 challenges [1]. to quantify the
performance, we report several metrics including dice similarity scores,
precision, recall, and hausdorff distance. table 1 shows the quantitative
comparison of different approaches on hecktor dataset. it is worth mentioning
that since our kspc-net is in a 2d unet structure, the hausdorff distance here
was calculated on slice averages to use a uniform metric across all 2d and 3d
segmentation models. however, the results of 2d hausdorff distances of msa-net
and ccut-net are not available and therefore they are omitted in the table of
comparison. the results clearly demonstrate that the proposed kspc-net is
effective in segmenting h&n tumours, achieving a mean dice score of 0.768. this
represents a substantial improvement over alternative approaches, including
2d-unet (0.740), 3d u-net (0.764), residual-unet (0.680), msa-net (0.757) and
ccut-net (0.750). while we acknowledge that there was no statistically
significant improvement compared to other sota models, it is important to note
that our main goal is to showcase the ability to obtain probability contours as
a natural byproduct while preserving state-of-the-art accuracy levels. on the
other hand, in comparison to the baseline 2d-unet model, kspc-net yields a
higher recall (0.911) with a significant improvement (4.35%), indicating that
kspc-net generates fewer false negatives (fn). although the precision of
kspc-net is slightly lower than the best-performing method (3d unet), it
achieves a relatively high value of 0.793. in addition, the proposed kspc-net
achieves the best performance on hausdorff distance among the three commonly
used unet models (2d-unet, res-unet and 3d-unet), which indicates that kspc-net
exhibits a stronger capacity for accurately localizing the boundaries of
objects. this is consistent with the mechanisms of kspc, which leverages
neighbouring weights to yield outputs with enhanced smoothness.",4
Deep Probability Contour Framework for Tumour Segmentation and Dose Painting in PET Images,5.0,Conclusion,"in this paper, we present a novel network, kspc-net, for the segmentation in 2d
pet images, which integrates kspc into the unet architecture in an end-toend
differential manner. the kspc-net utilizes the benefits of kspc to deliver both
contour-based and grid-based segmentation outcomes, leading to improved
precision in the segmentation of contours. promising performance was achieved by
our proposed kspc-net compared to the state-of-the-art approaches on the miccai
2021 challenge dataset (hecktor). it is worth mentioning that the architecture
of our kspc-net is not limited to head & neck cancer type and can be broadcast
to different cancer types. additionally, a byproduct application of our kspc-net
is to construct probability contours, which enables probabilistic interpretation
of contours. the subregions created by probability contours allow for a strategy
planning for the assigned dose boosts, which is a necessity for the treatment
planning of radiation therapy for cancers.",4
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,1.0,Introduction,"extracting brain tumors from medical image scans plays an important role in
further analysis and clinical diagnosis. typically, a brain tumor includes
peritumoral edema, enhancing tumor, and non-enhancing tumor core. since
different modalities present different clarity of brain tumor components, we
often use multi-modal image scans, such as t1, t1c, t2, and flair, in the task
of brain tumor segmentation [12]. works have been done to handle brain tumor
segmentation using image scans collected from all four modalities [11,15].
however, in practice, we face the challenge of collecting all modalities at the
same time, with often one or more missing. therefore, in this paper, we consider
the problem of segmenting brain tumors with missing image modalities.current
image segmentation methods for handling missing modalities can be divided into
three categories, including: 1) brute-force methods: designing individual
segmentation networks for each possible modality combination [18], 2) completion
methods: synthesizing the missing modalities to complete all modalities required
for conventional image segmentation methods [16], and 3) fusionbased methods:
mapping images from different modalities into the same feature space for fusion
and then segmenting brain tumors based on the fused features [10]. methods in
the first category have good segmentation performance; however, they are
resource intensive and often require more training time. the performance of
methods in the second category is limited by the synthesis quality of the
missing modality. the third category often has one single network to take care
of different scenarios of missing modalities, which is the most commonly used
one in practice.to handle various numbers of modal inputs, hemis [5] projects
the image features of different modalities into the same feature space, by
computing the mean and variance of the feature maps extracted from different
modalities as the fused features. to improve the representation of feature
fusion, hved [3] treats the input of each modality as a gaussian distribution,
and fuses feature maps from different modalities through a gaussian mixture
model. robustseg [1], on the other hand, decomposes the modality features into
modality-invariant content code and modality-specific appearance code, for more
accurate fusion and segmentation. considering the different clarity of brain
tumor regions observed in different modalities, rfnet [2] introduces an
attention mechanism to model the relations of modalities and tumor regions
adaptively. based on graph structure and attention mechanism, mfi [21] is
proposed to learn adaptive complementary information between modalities in
different missing situations.due to the complexity of current models, we tend to
develop a simple model, which adopts a simple average fusion and attention
mechanism. these two techniques are demonstrated to be effective in handling
missing modalities and multimodal fusion [17]. inspired by maml [20], we propose
a model called a2fseg (average and adaptive fusion segmentation network, see
fig. 1), which has two fusion steps, i.e., an average fusion and an
attention-based adaptive fusion, to integrate features from different modalities
for segmentation. although our fusion idea is quite simple, a2fseg achieves
state-of-the-art (sota) performance in the incomplete multimodal brain tumor
image segmentation task on the brats2020 dataset. our contributions in this
paper are summarized below:-we propose a simple multi-modal fusion network,
a2fseg, for brain tumor segmentation, which is general and can be extended to
any number of modalities for incomplete image segmentation. -we conduct
experiments on the brats 2020 dataset and achieve the sota segmentation
performance, having a mean dice core of 89.79% for the whole tumor, 82.72% for
the tumor core, and 66.71% for the enhancing tumor.",4
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,2.0,Method,"figure 1 presents the network architecture of our a2fseg. it consists of four
modality-specific sub-networks to extract features from each modality, an
average fusion module to simply fuse features from available modalities at the
first stage, and an adaptive fusion module based on an attention mechanism to
adaptively fuse those features again at the second stage.modality-specific
feature extraction (msfe) module. before fusion, we first extract features for
every single modality, using the nnunet model [7] as shown in fig. 1. in
particular, this msfe model takes a 3d image scan from a specific modality m,
i.e., i m ∈ r h×w ×d and m ∈ {t1, t2, t1c, flair}, and outputs the corresponding
image featureshere, the number of channels is c = 32; h f , w f , and d f are
the height, width, and depth of feature maps f m , which share the same size as
the input image. for every single modality, each msfe module is supervised by
the image segmentation mask to fasten its convergence and provide a good feature
extraction for fusion later. all four msfes have the same architecture but with
different weights.average fusion module. to aggregate image features from
different modalities and handle the possibility of missing one or more
modalities, we use the average of the available features from different
modalities as the first fusion result. that is, we obtain a fused average
featurehere, n m is the number of available modalities. for example, as shown in
fig. 1, if only the first two modalities are available at an iteration, then n m
= 2, and we will take the average of these two modalities, ignoring those
missing ones.adaptive fusion module. since each modality contributes differently
to the final tumor segmentation, similar to maml [20], we adopt the attention
mechanism to measure the voxel-level contributions of each modality to the final
segmentation. as shown in fig. 1, to generate the attention map for a specific
modality m, we take the concatenation of its feature extracted by the msfe
module f m and the mean feature after the average fusion f, which is passed
through a convolutional layer to generate the initial attention weights:here, f
m is a convolutional layer for this specific modality m, and θ m represents the
parameters of this layer, and σ is a sigmoid function. that is, we have an
individual convolution layer f m for each modality to generate different
weights.due to the possibility of missing modalities, we will have different
numbers of feature maps for fusion. to address this issue, we normalize the
different attention weights by using a softmax function:that is, we only
consider feature maps from those available modalities but normalize their
contribution to the final fusion result, so that, the fused one has a consistent
value range, no matter how many modalities are missing. then, we perform
voxel-wise multiplication of the attention weight with the corresponding modal
feature maps. as a result, the adaptively fused feature maps f is calculated by
the weighted sum of each modal feature:here, ⊗ indicates the voxel-wise
multiplication.loss function. we have multiple segmentation heads, which are
distributed in each module of a2fseg. for each segmentation head, we use the
combination of the cross-entropy and the soft dice score as the basic loss
function, which is defined aswhere ŷ and y represent the segmentation prediction
and the ground truth, respectively. based on this basic one, we have the overall
loss function defined aswhere the first term is the basic segmentation loss for
each modality m after feature extraction; the second term is the loss for the
segmentation output of the average fusion module; and the last term is the
segmentation loss for the final output from the adaptive fusion module. 3
experiments",4
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.1,Dataset,"our experiments are conducted on brats2020, which contains 369 multicontrast mri
scans with four modalities: t1, t1c, t2, and flair. these images went through a
sequence of preprocessing steps, including co-registration to the same
anatomical template, resampling to the same resolution (1 mm 3 ), and
skullstripping. the segmentation masks have three labels, including the whole
tumor (abbreviated as complete), tumor core (abbreviated as core), and enhancing
tumor (abbreviated as enhancing). these annotations are manually provided by one
to four radiologists according to the same annotation protocol.",4
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.3,Experimental Results and Comparison to Baseline Methods,"to evaluate the performance of our model, we compare it with four recent models,
hemis [5], u-hved [3], mmformer [19], and mfi [21]. the dataset is randomly
split into 70% for training, 10% for validation, and 20% for testing, and all
methods are evaluated on the same dataset and data splitting. we use the dice
score as the metric. as shown in table 1, our method achieves the best result.
for example, our method outperforms the current sota method mfi [21] in most
missing-modality cases, including all cases for the whole/complete tumor, 8 out
of 15 cases for the tumor core, 12 out of 15 cases for the enhancing tumor.
compared to mfi, for the whole tumor, tumor core, and enhancing tumor regions,
we improve the average dice scores by 0.99%, 0.41%, and 0.77%, respectively.
although the design of our model is quite simple, these results demonstrate its
effectiveness for the incomplete multimodel segmentation task of brain tumors.
figure 2 visualizes the segmentation results of samples from the brats2020
dataset. with only one flair image available, the segmentation results of the
tumor core and enhancing tumor are poor, because little information on these two
regions is observed in the flair image. with an additional t1c image, the
segmentation results of these two regions are significantly improved and quite
close to the ground truth. although adding t1 and t2 images does not greatly
improve the segmentation of the tumor core and the enhancing tumor, the boundary
of the whole tumor is refined with their help.figure 3 visualizes the
contribution to each tumor region from each modality. the numbers are the mean
values of the attention maps computed for images in the test set. overall, in
our model, each modality has its contribution to the final segmentation, and no
one dominates the result. this is because we have supervision on the
segmentation branch of each modality, so that, each modality has the ability to
segment each region to some extent. however, we still observe that flair and t2
modalities have relatively larger contributions to the segmentation of all tumor
regions, followed by t1c and then t1. this is probably because the whole tumor
area is much clear in flair and t2 compared to the other two modalities. each
modality shows its preference when segmenting different regions. flair and t2
are more useful for extracting the peritumoral edema (ed) than the enhancing
tumor (et) and the non-enhancing tumor and necrosis (ncr/net); while t1c and t1
are on the opposite and more helpful for extracting et and ncr/net.",4
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,3.4,Ablation Study,"in this part, we investigate the effectiveness of the average fusion module and
the adaptive fusion module, which are two important components of our method.
firstly, we set a baseline model without any modal interaction, that is, with
the average fusion module only. then, we add the adaptive fusion module to the
baseline model. table 2 reports this ablation study. with only adding the
average fusion module, our method already obtains comparable performance with
the current sota method mfi. by adding the adaptive fusion module, the dice
scores of the three regions further increase by 0.50%, 0.72%, and 0.71%,
respectively. this shows that both the average fusion module and the adaptive
fusion module are effective in this brain tumor segmentation task.",4
A2FSeg: Adaptive Multi-modal Fusion Network for Medical Image Segmentation,4.0,Discussion and Conclusion,"in this paper, we propose an average and adaptive fusion segmentation network
(a2fseg) for the incomplete multi-model brain tumor segmentation task. the
essential components of our a2fseg network are the two stages of feature fusion,
including an average fusion and an adaptive fusion. compare to existing
complicated models, our model is much simpler and more effective, which is
demonstrated by the best performance on the brats 2020 brain tumor segmentation
task. the experimental results demonstrate the effectiveness of two techniques,
i.e., the average fusion and the attention-based adaptive one, for incomplete
modal segmentation tasks.our study brings up the question of whether having
complicated models is necessary. if there is no huge gap between different
modalities, like in our case where all four modalities are images, the image
feature maps are similar and a simple fusion like ours can work. otherwise, we
perhaps need an adaptor or an alignment strategy to fuse different types of
features, such as images and audio.also, we observe that a good feature
extractor is essential for improving the segmentation results. in this paper, we
only explore a reduced unet for feature extraction. in future work, we will
explore other feature extractors, such as vision transformer (vit) or other
pre-trained visual foundation models [4,6,14]. recently, the segment anything
model (sam) [9] demonstrates its general ability to extract different regions of
interest, which is promising to be adopted as a good starting point for brain
tumor segmentation. besides, our model is general for multi-modal segmentation
and we will apply it to other multi-model segmentation tasks to evaluate its
generalization on other applications.",4
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,1.0,Introduction,"diffuse glioma is a common malignant tumor with highly variable prognosis across
individuals. to improve survival outcomes, many pre-operative survival
prediction methods have been proposed with success. based on the prediction
results, personalized treatment can be achieved. for instance, isensee et al.
[1] proposed a random forest model [2], which adopts the radiomics features [3]
of the brain tumor images, to predict the overall survival (os) time of diffuse
glioma patients. nie et al. [4] developed a multi-channel 3d convolutional
neural network (cnn) [5] to learn features from multimodal mr brain images and
classify os time as long or short using a support vector machine (svm) model
[6]. in [7], an end-to-end cnn-based method was presented that uses multimodal
mr brain images and clinical features such as karnofsky performance score [8] to
predict os time. in [9], an imaging phenotype and genotype based survival
prediction method (pgsp) was proposed, which integrates tumor genotype
information to enhance prediction accuracy.despite the promising results of
existing pre-operative survival prediction methods, they often overlook clinical
knowledge that could aid in improving the prediction accuracy. notably, tumor
types have been found to be strongly correlated with the prognosis of diffuse
glioma [10]. unfortunately, tumor type information is unavailable before
craniotomy. to address this limitation, we propose a new pre-operative survival
prediction method that integrates a tumor subtyping network into the survival
prediction backbone. the subtyping network is responsible for learning
tumor-type-related features from pre-operative multimodal mr brain images.
concerning the inherent issue of imbalanced tumor types in the training data
collected in clinic, a novel ordinal manifold mixup based feature augmentation
is presented and applied in the training stage of the tumor subtyping network.
unlike the original manifold mixup [11], which ignores the feature distribution
of different classes, in the proposed ordinal manifold mixup, feature
distribution of different tumor types is encouraged to be in the order of risk
grade, and the augmented features are produced between neighboring risk grades.
in this way, inconsistency between the augmented features and the corresponding
labels can be effectively reduced.our method is evaluated using pre-operative
multimodal mr brain images of 1726 diffuse glioma patients collected from
cooperation hospitals and a public dataset brats2019 [12] containing multimodal
mr brain images of 210 patients. our method achieves the highest prediction
accuracy of all state-of-the-art methods under evaluation. in addition, ablation
study further confirms the effectiveness of the proposed tumor subtyping network
and the ordinal manifold mixup.",4
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.0,Methods,"diffuse glioma can be classified into three histological types: the
oligodendroglioma, the astrocytoma, and the glioblastoma [10]. the median
survival times (in months) are 119 (oligodendroglioma), 36 (astrocytoma), and 8
(glioblastoma) [13]. so the tumor types have strong correlation with the
prognosis of diffused glioma. based on this observation, we propose a new
pre-operative survival prediction method (see fig. 1). our network is composed
of two parts: the survival prediction backbone and the tumor subtyping network.
the survival prediction backbone is a deep cox proportional hazard model [14]
which takes the multimodal mr brain images of diffuse glioma patients as inputs
and predicts the corresponding risks. the tumor subtyping network is a
classification network, which classifies the patient tumor types and feeds the
learned tumor-type-related features to the backbone to enhance the survival
prediction performance.the tumor subtyping network is trained independently
before being integrated into the backbone. to solve the inherent issue of
imbalanced tumor type in the training data collected in clinic, a novel ordinal
manifold mixup based feature augmentation is applied in the training of the
tumor subtyping network. it is worth noting that the ground truth of tumor
types, which is determined after craniotomy, is available in the training data,
while for the testing data, tumor types are not required, because
tumor-type-related features can be learned from the pre-operative multimodal mr
brain images.",4
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.1,The Survival Prediction Backbone,"the architecture of the survival prediction backbone, depicted in fig. 1 top,
consists of an encoder e cox with four resblocks [15], a global average pooling
layer (gap), and three fully connected (fc) layers. assume that d = {x 1 , ...,
x n } is the dataset containing pre-operative multimodal mr brain images of
diffuse glioma patients, and n is the number of patients. the backbone is
responsible for deriving features from x i to predict the risk of the patient.
moreover, after the gap of the backbone, the learned featurefrom the tumor
subtyping network (discussed later), and m is the vector dimension which is set
to 128. as f type i has strong correlation with prognosis, the performance of
the backbone can be improved. in addition, information of patient age and tumor
position is also used. to encode the tumor position, the brain is divided into 3
× 3 × 3 blocks, and the tumor position is represented by 27 binary values (0 or
1) with each value for one block. if a block contains tumors, then the
corresponding binary value is 1, otherwise is 0. the backbone is based on the
deep cox proportional hazard model, and the loss function is defined as:where h
θ (x i ) represents the risk of the i-th patient predicted by the backbone, θ
stands for the parameters of the backbone, x i is the input multimodal mr brain
images of the i-th patient, r(t i ) is the risk group at time t i , which
contains all patients who are still alive before time t i , t i is the observed
time (time of death happened) of x i , and δ i = 0/1 for censored/non-censored
patient.",4
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.2,The Tumor Subtyping Network,"the tumor subtyping network has almost the same structure as the backbone.it is
responsible for learning tumor-type-related features from each input
preoperative multimodal mr brain image x i and classifying the tumor into
oligodendroglioma, astrocytoma, or glioblastoma. the cross entropy is adopted as
the loss function of the tumor subtyping network, which is defined as:where y k
i and p k i are the ground truth (0 or 1) and the prediction (probability) of
the k-th tumor type (k = 1, 2, 3) of the i-th patient, respectively. the learned
tumor-type-related feature f type i ∈ r m is fed to the survival prediction
backbone and concatenated with f cox i learned in the backbone to predict the
risk. in the in-house dataset, the proportions of the three tumor types are
20.9% (oligodendroglioma), 28.7% (astrocytoma), and 50.4% (glioblastoma), which
is consistent with the statistical report in [13]. to solve the imbalance issue
of tumor types in the training of the tumor subtyping network, a novel ordinal
manifold mixup based feature augmentation is presented.",4
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,2.3,The Ordinal Manifold Mixup,"in the original manifold mixup [11], features and the corresponding labels are
augmented using linear interpolation on two randomly selected features (e.g., f
i and f j ). specifically, the augmented feature fi∼j and label ȳi∼j is defined
as:where y k i and y k j stand for the labels of the k-th tumor type of the i-th
and j-th patients, respectively, and λ ∈ [0, 1] is a weighting factor. for
binary classification, the original manifold mixup can effectively enhance the
network performance, however, for the classification of more than two classes,
e.g., tumor types, there exists a big issue.as shown in fig. 2 left, assume that
f i and f j are features of oligodendroglioma (green) and astrocytoma (yellow)
learned in the tumor subtyping network, respectively. the augmented feature fi∼j
(red) produced from the linear interpolation between f i and f j has the
corresponding label ȳi∼j with high probabilities for the tumor types of
oligodendroglioma and astrocytoma. however, since these is no constraint imposed
on the feature distribution of different tumor types, fi∼j could fall into the
distribution of glioblastoma (blue) as shown in fig. 2 left. in this case, fi∼j
and ȳi∼j are inconsistent, which could influence the training and degrade the
performance of the tumor subtyping network. as aforementioned, the survival time
of patients with different tumor types varies largely (oligodendroglioma >
astrocytoma > glioblastoma), so the tumor types can be regarded as risk grade,
which are ordered rather than categorical. based on this assertion and inspired
by [16], we impose an ordinal constraint on the tumor-type-related features to
make the feature distribution of different tumor types in the order of risk
grade. in this way, the manifold mixup strategy can be applied between each two
neighboring tumor types to produce augmented features with consistent labels
that reflect reasonable risk (see fig. 2",4
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,0.0,right).,"normally, the feature distribution of each tumor type is assumed to be
independent normal distribution, so their joint distribution is given by:where f
k , k = 1, 2, 3 represents the feature set of oligodendroglioma, astrocytoma,
and glioblastoma, respectively, μ k and σ 2 k are mean and variance of f k . to
impose the ordinal constraint, we define the desired feature distribution of
each tumor type as n (μ 1 , σ21 ) for k = 1, and n (μ k-1 + δ k , σ2 k ) for k =
2 and 3. in this way, the feature distribution of each tumor type depends on its
predecessor, and the mean feature of each tumor type μk (except μ1 ) is equal to
the mean feature of its predecessor μk-1 shifted by δ k . note that δ k is set
to be larger than 3 × σk to ensure the desired ordering [16]. in this way, the
conditional distribution under the ordinal constraint is defined as:which can be
represented as:where μ1 and σk , k = 1, 2, 3 can be learned by the tumor
subtyping network. finally, the ordinal loss, which is in the form of kl
divergence, is defined as:in our method, μ k and σ 2 k are calculated bywhere φ
θ and g are the encoder and gap of the tumor subtyping network, respectively, θ
is the parameter set of the encoder,stands for the subset containing the
pre-operative multimodal mr brain images of the patients with the k-th tumor
type, n k is the patient number in d k . so we impose the ordinal loss l kl to
the features after the gap of the tumor subtyping network as shown in fig. 1.
since the ordinal constraint, feature distribution of different tumor types
learned in the subtyping network is encouraged to be in the ordered of risk
grade, and features can be augmented between neighboring tumor type. in this
way, inconsistency between the resulting augmented features and labels can be
effectively reduced.the tumor subtyping network is first trained before being
integrated into the survival prediction backbone. in the training stage of the
tumor subtyping network, each input batch contains pre-operative multimodal mr
brain images of n patients and can be divided into k = 3 subsets according to
their corresponding tumor types, i.e., d k , k = 1, 2, 3. with the ordinal
constrained feature distribution, high consistent features can be augmented
between neighboring tumor types. based on the original and augmented features,
the performance of the tumor subtyping network can be enhanced.once the tumor
subtyping network has been trained, it is then integrated into the survival
prediction backbone, which is trained under the constraint of the cox
proportional hazard loss l cox .",4
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,3.0,Results,"in our experiment, both in-house and public datasets are used to evaluate our
method. specifically, the in-house dataset collected in cooperation hospitals
contains pre-operative multimodal mr images, including t1, t1 contrast enhanced
(t1c), t2, and flair, of 1726 patients (age 49.7 ± 13.1) with confirmed diffuse
glioma types. the patient number of each tumor type is 361 (oligodendroglioma),
495 (astrocytoma), and 870 (glioblastoma), respectively. in the 1726 patients,
743 have the corresponding overall survival time (dead, non-censored), and 983
patients have the last visiting time (alive, censored). besides the inhouse
dataset, a public dataset brats2019, including pre-operative multimodal mr
images of 210 non-censored patients (age 61.4 ± 12.2), is adopted as the
external independent testing dataset. all images of the in-house and brats2019
datasets go through the same pre-processing stage, including image normalization
and affine transformation to mni152 [17]. based on the tumor mask of each image,
tumor bounding boxes can be calculated. according to the bounding boxes of all
1936 patients, the size of input 3d image patch is set to 96 × 96 × 64 voxels,
which can cover the entire tumor of every patient.besides our method, four
state-of-the-art methods, including random forest based method (rf) [18], deep
convolutional survival model (deepconvsurv) [19], multi-channel survival
prediction method (mcsp) [20], and imaging phenotype and genotype based survival
prediction method (pgsp) [9], are evaluated. it is worth noting that in the rf
method, 100 decision trees and 390 handcrafted radiomics features are used. the
output of rf, mcsp, and pgsp is the overall survival (os) times in days, while
for deepconvsurv and our method, the output is the risk (deep cox proportional
hazard models). concordance index (c-index) is adopted to quantify the
prediction accuracy:where d = {x 1 , ..., x n } is the dataset containing all
patients, t i and t j are ground truth of survival times of the i-th and j-th
patients, r i and r j are the days predicted by rf, mcsp, and pgsp or risks
predicted by the deep cox proportional hazard models (i.e., deepconvsurv and our
method), 1 x<y = 1 if x < y, else 0, and δ i = 0 or 1 when the i-th patient is
censored or non-censored. as rf, mcsp, and pgsp cannot use the censored data in
the in-house dataset, 80% of the non-censored data (594 patients) are randomly
selected as the training data, and the rest 20% non-censored data (149 patients)
are for testing. while deepconvsurv and our method are deep cox models, both
censored and non-censored patients can be utilized. so besides the 80%
non-censored patients, all censored data (983 patients) are also included in the
training data.table 1 shows the evaluation results of the in-house and the
external independent (brats2019) testing datasets using all methods under
evaluation. our method achieves the highest c-index of all the methods under
evaluation. moreover, comparing with deepconvsurv, our method can improve the
prediction accuracy up to 10% (in-house) and 8% (brats2019).",4
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,3.1,Ablation Study of Survival Prediction,"to show the effect of the tumor subtyping network and the ordinal manifold mixup
in survival prediction, our method without the tumor subtyping network
(baseline-1) and our method with the tumor subtyping network (using original
manifold mixup instead, baseline-2) are evaluated. for the in-house dataset, the
resulting c-indices are 0.744 (baseline-1) and 0.735 (baseline-2). so our method
make the improvement of c-index more than 8% comparing with baseline-2. for the
external independent testing dataset brats2019, the resulting c-indices are
0.738 (baseline-1) and 0.714 (baseline-2), and our method still has more than 6%
improvement comparing with baseline-2.figure 3 shows the distributions of
tumor-type-related features (after the gap) of the in-house testing data in
baseline-2, and our method. principal component analysis [21] is used to project
features to a 2d plane. since the ordinal constraint, the feature distribution
of different tumor types is in the order of risk grade using our method, which
cannot be observed in baseline-2.",4
Pre-operative Survival Prediction of Diffuse Glioma Patients with Joint Tumor Subtyping,4.0,Conclusions,"we proposed a new method for pre-operative survival prediction of diffuse glioma
patients, where a tumor subtyping network is integrated into the prediction
backbone. based on the tumor subtyping network, tumor type information, which
are only available after craniotomy, can be derived from the pre-operative
multimodal mr images to boost the survival prediction performance. moreover, a
novel ordinal manifold mixup was presented, where ordinal constraint is imposed
to make feature distribution of different tumor types in the order of risk
grade, and feature augmentation only takes place between neighboring tumor
types. in this way, inconsistency between the augmented features and
corresponding labels can be effectively reduced. both in-house and public
datasets containing 1936 patients were used in the experiment. our method
outperformed the state-of-the-art methods in terms of the concordance-index.",4
Medical Boundary Diffusion Model for Skin Lesion Segmentation,1.0,Introduction,"segmentation of skin lesions from dermoscopy images is a critical task in
disease diagnosis and treatment planning of skin cancers [17]. manual lesion
segmentation is time-consuming and prone to inter-and intra-observer
variability. to improve the efficiency and accuracy of clinical workflows,
numerous automated skin lesion segmentation models have been developed over the
years [1,2,7,10,18,19,21]. these models have focused on enhancing feature
representations using various techniques such as multi-scale feature fusion
[10], attention mechanisms [1,7], self-attention mechanisms [18,19], and
boundary-aware attention [2,18,19], resulting in significant improvements in
skin lesion segmentation performance. despite these advances, the segmentation
of skin lesions with ambiguous boundaries, particularly at extremely challenging
scales, remains a bottleneck issue that needs to be addressed. in such cases,
even state-of-the-art segmentation models struggle to achieve accurate and
consistent results.",4
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,1.0,Introduction,"accurate tumor segmentation from medical images is essential for quantitative
assessment of cancer progression and preoperative treatment planning [3]. tumor
tissues usually present different features in different imaging modalities. for
example, computed tomography (ct) and positron emission tomography (pet) are
beneficial to represent morphological and metabolic information of tumors,
respectively. in clinical practice, multimodal registered images, such as pet-ct
images and magnetic resonance (mr) images with different sequences, are often
utilized to delineate tumors to improve accuracy. however, manual delineation is
time-consuming and error-prone, with a low inter-professional agreement [12].
these have prompted the demand for intelligent applications that can
automatically segment tumors from multimodal images to optimize clinical
procedures.recently, multimodal tumor segmentation has attracted the interest of
many researchers. with the emergence of multimodal datasets (e.g., brats [25]
and hecktor [1]), various deep-learning-based multimodal image segmentation
methods have been proposed [3,10,13,27,29,31]. overall, large efforts have been
made on effectively fusing image features of different modalities to improve
segmentation accuracy. according to the way of feature fusion, the existing
methods can be roughly divided into three categories [15,36]: input-level
fusion, decisionlevel fusion, and layer-level fusion. as a typical approach,
input-level fusion [8,20,26,31,34] refers to concatenating multimodal images in
the channel dimension as network input during the data processing or
augmentation stage. this approach is suitable for most existing end-to-end
models [6,32], such as u-net [28] and u-net++ [37]. however, the shallow fusion
entangles the low-level features from different modalities, preventing the
effective extraction of high-level semantics and resulting in limited
performance gains. in contrast, [35] and [21] propose a solution based on
decision-level fusion. the core idea is to train an independent segmentation
network for each data modality and fuse the results in a specific way. these
approaches can bring much extra computation at the same time, as the number of
networks is positively correlated with the number of modalities. as a compromise
alternative, layer-level fusion methods such as hyperdense-net [10] advocate the
cross-fusion of the multimodal features in the middle layer of the network.in
addition to the progress on the fusion of multimodal features, improving the
model representation ability is also an effective way to boost segmentation
performance. in the past few years, transformer structure [11,24,30], centered
on the multi-head attention mechanism, has been introduced to multimodal image
segmentation tasks. extensive studies [2,4,14,16] have shown that the
transformer can effectively model global context to enhance semantic
representations and facilitate pixel-level prediction. wang et al. [31] proposed
transbts, a form of input-level fusion with a u-like structure, to segment brain
tumors from multimodal mr images. transbts employs the transformer as a
bottleneck layer to wrap the features generated by the encoder, outperforming
the traditional end-to-end models. saeed et al. [29] adopted a similar structure
in which the transformer serves as the encoder rather than a wrapper, also
achieving promising performance. other works like [9] and [33], which combine
the transformer with the multimodal feature fusion approaches mentioned above,
further demonstrate the potential of this idea for multimodal tumor
segmentation.although remarkable performance has been accomplished with these
efforts, there still exist several challenges to be resolved. most existing
methods are either limited to specific modality numbers due to the design of
asymmetric connections or suffer from large computational complexity because of
the huge amount of model parameters. therefore, how to improve model ability
while ensuring computational efficiency is the main focus of this paper.to this
end, we propose an efficient multimodal tumor segmentation solution named hybrid
densely connected network (h-denseformer). first, our method leverages
transformer to enhance the global contextual information of different
modalities. second, h-denseformer integrates a transformer-based multi-path
parallel embedding (mpe) module, which can extract and fuse multimodal image
features as a complement to naive input-level fusion structure. specifically,
mpe assigns an independent encoding path to each modality, then merges the
semantic features of all paths and feeds them to the encoder of the segmentation
network. this decouples the feature representations of different modalities
while relaxing the input constraint on the specific number of modalities.
finally, we design a lightweight, densely connected transformer (dct) module to
replace the standard transformer to ensure performance and computational
efficiency. extensive experimental results on two publicly available datasets
demonstrate the effectiveness of our proposed method. as the auxiliary extractor
of multimodal fusion features, while the latter is used to generate predictions.
specifically, given a multimodal image input x 3d ∈ r c×h×w ×d or x 2d ∈ r c×h×w
with a spatial resolution of h × w , the depth dimension of d (number of slices)
and c channels (number of modalities), we first utilize mpe to extract and fuse
multimodal image features. then, the obtained features are progressively
upsampled and delivered to the encoder of the segmentation network to enhance
the semantic representation. finally, the segmentation network generates
multi-scale outputs, which are used to calculate deep supervision loss as the
optimization target.",4
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,2.4,Segmentation Backbone Network,"the h-denseformer adopts a u-shaped encoder-decoder structure as its backbone.
as shown in fig. 1, the encoder extracts features and reduces their resolution
progressively. to preserve more details, we set the maximum downsampling factor
to 8. the multi-level multimodal features from mpe are fused in a bitwise
addition way to enrich the semantic information. the decoder is used to restore
the resolution of the features, consisting of deconvolutional and convolutional
layers with skip connections to the encoder. in particular, we employ deep
supervision (ds) loss to improve convergence, which means that the multiscale
output of the decoder is involved in the final loss computation. deep
supervision loss. during training, the decoder has four outputs; for example,
the i-th output of 2d h-denseformer is, where i ∈ [0, 1, 2, 3], and c = 2 (tumor
and background) represents the number of segmentation classes. to mitigate the
pixel imbalance problem, we use a combined loss of focal loss [23] and dice loss
as the optimization target, defined as follows:where n refers to the total
number of pixels, p t and q t denote the predicted probability and ground truth
of the t-th pixel, respectively, and r = 2 is the modulation factor. thus, ds
loss can be calculated as follows:where g i represents the ground truth after
resizing and has the same size as o i . α is a weighting factor to control the
proportion of loss corresponding to the output at different scales. this
approach can improve the convergence speed and performance of the network.",4
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3.1,Dataset and Metrics,"to validate the effectiveness of our proposed method, we performed extensive
experiments on hecktor21 [1] and pi-cai221 . hecktor21 is a dualmodality dataset
for head and neck tumor segmentation, containing 224 pet-ct image pairs. each
pet-ct pair is registered and cropped to a fixed size of (144,144,144). pi-cai22
provides multimodal mr images of 220 patients with prostate cancer, including
t2-weighted imaging (t2w), high b-value diffusion-weighted imaging (dwi), and
apparent diffusion coefficient (adc) maps. after standard resampling and center
cropping, all images have a size of (24,384,384). we randomly select 180 samples
for each dataset as the training set and the rest as the independent test set
(44 cases for hecktor21 and 40 cases for pi-cai22). specifically, the training
set is further randomly divided into five folds for cross-validation. for
quantitative analysis, we use the dice similarity coefficient (dsc), the jaccard
index (ji), and the 95% hausdorff distance (hd95) as evaluation metrics for
segmentation performance. a better segmentation will have a smaller hd95 and
larger values for dsc and ji. we also conduct holistic t-tests of the overall
performance for our method and all baseline models with the two-tailed p < 0.05.",4
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,3.2,Implementation Details,"we use pytorch to implement our proposed method and the baselines. for a fair
comparison, all models are trained from scratch using two nvidia a100 gpus and
all comparison methods are implemented with open-source codes, following their
original configurations. in particular, we evaluate the 3d and 2d h-denseformer
on hecktor21 and pi-cai22, respectively. during the training phase, the adam
optimizer is employed to minimize the loss with an initial learning rate of 10
-3 and a weight decay of 10 -4 . we use the polylr strategy [19] to control the
learning rate change. we also use an early stopping strategy with a tolerance of
30 epochs to find the best model within 100 epochs. online data augmentation,
including random rotation and flipping, is performed to alleviate the
overfitting problem. table 2 compares the performance and computational
complexity of our proposed method with the existing state-of-the-art methods on
the independent test sets. for hecktor21, 3d h-denseformer achieves a dsc of
73.9%, hd95 of 8.1mm, and ji of 62.5%, which is a significant improvement (p <
0.01) over 3d u-net [7], unetr [16], and transbts [31]. it is worth noting that
the performance of hybrid models such as unetr is not as good as expected, even
worse than 3d u-net, perhaps due to the small size of the dataset. moreover,
compared to the champion solution of hecktor20 proposed by iantsen et al. [18],
our method has higher accuracy and about 10 and 5 times lower amount of network
parameters and computational cost, respectively. for pi-cai22, the 2d variant of
h-denseformer also outperforms existing methods (p < 0.05), achieving a dsc of
49.9%, hd95 of 35.9 mm, and ji of 37.1%. overall, h-denseformer reaches an
effective balance of performance and computational cost compared to existing
cnns and hybrid structures. for qualitative analysis, we show a visual
comparison of the different methods. it is evident from fig. 2 that our approach
can describe tumor contours more accurately while providing better segmentation
accuracy for small-volume targets. these results further demonstrate the
effectiveness of our proposed method in multimodal tumor segmentation tasks.",4
H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation,4.0,Conclusion,"in this paper, we proposed an efficient hybrid model (h-denseformer) that
combines transformer and cnn for multimodal tumor segmentation. concretely, a
multi-path parallel embedding module and a densely connected transformer block
were developed and integrated to balance accuracy and computational complexity.
extensive experimental results demonstrated the effectiveness and superiority of
our proposed h-denseformer. in future work, we will extend our method to more
tasks and explore more efficient multimodal feature fusion methods to further
improve computational efficiency and segmentation performance.",4
TransNuSeg: A Lightweight Multi-task Transformer for Nuclei Segmentation,1.0,Introduction,"accurate cancer diagnosis, grading, and treatment decisions from medical images
heavily rely on the analysis of underlying complex nuclei structures [7]. yet,
due to the numerous nuclei contained in a digitized whole-slide image (wsi), or
even in an image patch of deep learning input, dense annotation of nuclei
contouring is extremely time-consuming and labor-expensive [11]. consequently,
automated nuclei segmentation approaches have emerged to satisfy a broad range
of computer-aided diagnostic systems, where the deep learning methods,
particularly the convolutional neural networks [5,12,14,19,21] have received
notable attention due to their simplicity and generalization ability.in the
literature work, the sole-decoder design in these unet variants (fig. 1(a)) is
susceptible to failures in splitting densely clustered nuclei when precise edge
information is absent. hence, deep contour-aware neural network (dcan) [3] with
bi-decoder structure achieves improved instance segmentation performance by
adopting multi-task learning, in which one decoder learns to segment the nuclei
and the other recognizes edges as described in fig. 1(b). similarly, cia-net
[20] extends dcan with an extra information aggregator to fuse the features from
two decoders for more precise segmentation. much recently, ca 2.5 -net [6] shows
identifying the clustered edges in a multiple-task learning manner can achieve
higher performance, and thereby proposes an extra output path to learn the
segmentation of clustered edges explicitly. a significant drawback of the
aforementioned multi-decoder networks is the ignorance of the prediction
consistency between branches, resulting in sub-optimal performance and missing
correlations between the learned branches. specifically, a prediction mismatch
between the nuclei and edge branches is observed in previous work [8], implying
a direction for performance improvement. to narrow this gap, we propose a
consistency distillation between the branches, as shown by the dashed line in
fig. 1(c). furthermore, to resolve the cost of involving more decoders, we
propose an attention sharing scheme, along with an efficient token mlp
bottleneck [16], which can both reduce the number of parameters.additionally,
existing methods are cnn-based, and their intrinsic convolution operation fails
to capture global spatial information or the correlation amongst nuclei [18],
which domain experts rely heavily on for accurate nuclei allocation. it suggests
the presence of long-range correlation in practical nuclei segmentation tasks.
inspired by the capability in long-range global context capturing by
transformers [17], we make the first attempt to construct a tri-decoder based
transformer model to segment nuclei. in short, our major contributions are
three-fold: (1) we propose a novel multi-task framework for nuclei segmentation,
namely transnuseg, as the first attempt at a fully swin-transformer driven
architecture for nuclei segmentation. (2) to alleviate the prediction
inconsistency between branches, we propose a novel self distillation loss that
regulates the consistency between the nuclei decoder and normal edge decoder.
(3) we propose an innovative attention sharing scheme that shares attention
heads amongst all decoders. by leveraging the high correlation between tasks, it
can communicate the learned features efficiently across decoders and sharply
reduce the number of parameters. furthermore, the incorporation of a
light-weighted mlp bottleneck leads to a sharp reduction of parameters at no
cost of performance decline. fig. 2. the overall framework of the proposed
transnuseg of three output branches to separate the nuclei, normal edges, and
cluster edges, respectively. in the novel design, a pre-defined proportion of
the attention heads are shared between the decoders via the proposed sharing
scheme, which considerably reduces the number of parameters and enables more
efficient information communication.",4
Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,1.0,Introduction,"multi-modal learning has become a popular research area in computer vision and
medical image analysis, with modalities spanning across various media types,
including texts, audio, images, videos and multiple sensor data. this approach
has been utilised in robot control [15,17], visual question answering [12] and
audio-visual speech recognition [10], as well as in the medical field to improve
diagnostic system performance [7,18]. for instance, magnetic resonance imaging
(mri) is a common tool for brain tumour detection that relies on multiple
modalities (flair, t1, t1 contrast-enhanced known as t1c, and t2) rather than a
single type of mri images. however, most existing multi-modal methods require
complete modalities during training and testing, which limits their
applicability in real-world scenarios, where subsets of modalities may be
missing during training and testing.the missing modality issue is a significant
challenge in the multi-modal domain, and it has motivated the community to
develop approaches that attempt to address this problem. havaei et al. [8]
developed hemis, a model that handles missing modalities using statistical
features as embeddings for the model decoding process. taking one step ahead,
dorent et al. [6] proposed an extension to hemis via a multi-modal variational
auto-encoder (mvae) to make predictions based on learned statistical features.
in fact, variational auto-encoder (vae) has been adopted to generate data from
other modalities in the image or feature domains [3,11]. yin et al. [20] aimed
to learn a unified subspace for incomplete and unlabelled multi-view data. chen
et al. [4] proposed a feature disentanglement and gated fusion framework to
separate modality-robust and modalitysensitive features. ding et al. [5]
proposed an rfm module to fuse the modal features based on the sensitivity of
each modality to different tumor regions and a segmentation-based regularizer to
address the imbalanced training problem. zhang et al. [22] proposed an ma module
to ensure that modality-specific models are interconnected and calibrated with
attention weights for adaptive information exchange. recently, zhang et al. [21]
introduced a vision transformer architecture, mmformer, that fuses features from
all modalities into a set of comprehensive features. there are several existing
works [9,16,19] proposed to approximate the features from full modalities when
one or more modalities are absent. but none work performs cross-modal knowledge
distillation. from an other point of view, wang et al. [19] introduced a
dedicated training strategy that separately trains a series of models
specifically for each missing situation, which requires significantly more
computation resources compared with a nondedicated training strategy. an
interesting fact about multi-modal problems is that there is always one modality
that contributes much more than other modalities for a certain task. for
instance, for brain tumour segmentation, it is known from domain knowledge that
t1c scans clearly display the enhanced tumour, but not edema [4]. if the
knowledge of these modalities can be successfully preserved, the model can
produce promising results even when these best performing modalities are not
available. however, the aforementioned methods neglect the contribution biases
of different modalities and failed to consider keeping that knowledge.aiming at
this issue, we propose the non-dedicated training model1 learnable cross-modal
knowledge distillation (lckd) for tackling the missing modality issue. lckd is
able to handle missing modalities in both training and testing by automatically
identifying important modalities and distilling knowledge from them to learn the
parameters that are beneficial for all tasks while training for other modalities
(e.g., there are four modalities and three tasks for the three types of tumours
in brats2018). our main contributions are:-we propose the learnable cross-modal
knowledge distillation (lckd) model to address missing modality problem in
multi-modal learning. it is a simple yet effective model designed from the
viewpoint of distilling crossmodal knowledge to maximise the performance for all
tasks; -the lckd approach is designed to automatically identify the important
modalities per task, which helps the cross-modal knowledge distillation process.
it also can handle missing modality during both training and testing.the
experiments are conducted on the brain tumour segmentation benchmark brats2018
[1,14], showing that our lckd model achieves state-of-theart performance. in
comparison to recently proposed competing methods on brats2018, our model
demonstrates better performance in segmentation dice score by 3.61% for
enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour, on average.",4
Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,2.1,Overall Architecture,"let us represent the n -modality data with m l = {x∈ x denotes the l th data
sample and the superscript (i) indexes the modality. to simplify the notation,
we omit the subscript l when that information is clear from the context. the
label for each set m is represented by y ∈ y, where y represents the
ground-truth annotation space. the framework of lckd is shown in fig.
1.multi-modal segmentation is composed not only of multiple modalities, but also
of multiple tasks, such as the three types of tumours in brats2018 dataset that
represent the three tasks. take one of the tasks for example. our model
undergoes an external teacher election procedure prior to processing all
modalities {x (i) } n i=1 ∈ m in order to select the modalities that exhibit
promising performance as teachers. this is illustrated in fig. 1, where one of
the modalities, x (2) , is selected as a teacher, {x (1) , x (3) , ..., x (n ) }
are the students, and x (n) (with n = 2) is assumed to be absent. subsequently,
the modalities are encoded to output features {f (i) } n i=1 , individually. for
the modalities that are available, namely x (1) , ..., x (n-1) , x (n+1) , ...,
x (n ) , knowledge distillation is carried out between each pair of teacher and
student modalities. however, for the absent i=1 } are processed by the encoder
to produce the features {f (i) n i=1 }, which are concatenated and used by the
decoder to produce the segmentation. the teacher is elected using a validation
process that selects the top-performing modalities as teachers. cross-modal
distillation is performed by approximating the available students' features to
the available teachers' features. features from missing modalities are generated
by averaging the other modalities' features. modality x (n) , its features f (n)
are produced through a missing modality feature generation process from the
available features f (1) , ..., f (n-1) , f (n+1) , ..., f (n ) .in the next
sections, we explain each module of the proposed learnable crossmodal knowledge
distillation model training and testing with full and missing modalities.",4
Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,2.2,Teacher Election Procedure,"usually, one of the modalities is more useful than others for a certain task,
e.g. for brain tumour segmentation, t1c scan clearly displays the enhanced
tumour, but it does not clearly show edema [4]. following knowledge distillation
(kd) [9], we propose to transfer the knowledge from modalities with promising
performance (known as teachers) to other modalities (known as students). the
teacher election procedure is further introduced to automatically elect proper
teachers for different tasks.more specifically, in the teacher election
procedure, a validation process is applied: for each task k (for k ∈ {1, ...,
k}), the modality with the best performance is selected as the teacher t (k) .
formally, we have:where i indexes different modalities, f (•; θ) is the lckd
segmentation model parameterised by θ, including the encoder and decoder
parameters {θ enc , θ dec } ∈ θ, and d(•, •) is the function to calculate the
dice score. based on the elected teachers for different tasks, a list of unique
teachers (i.e., repetitions are not allowed in the list, so for brats, {t1c,
t1c, flair} would be reduced to {t1c, flair}) are generated with: t = φ(t (1) ,
t (2) , ..., t (k) , ..., t (k) ), (2) where φ is the function that returns the
unique elements from a given list, and t ⊆ {1, ..., n } is the teacher set.",4
Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,2.5,Training and Testing,"all features encoded from eq. 3 or generated from eq. 5 are then concatenated to
be fed into the decoder parameterised by θ dec for predictingwhere ỹ ∈ y is the
prediction of the task.the training of the whole model is achieved by minimising
the following objective function: tot (d, θ) = task (d, θ enc , θ dec ) + α ckd
(d; θ enc ), (7) where task (d, θ enc , θ dec ) is the objective function for
the whole task (e.g., cross-entropy and dice losses are adopted for brain tumour
segmentation), and α is the trade-off factor between the task objective and
cross-modal kd objective.testing is based on taking all image modalities
available in the input to produce the features from eq. 3, and generating the
features from the missing modalities with eq. 5, which are then provided to the
decoder to predict the segmentation with eq. 6.",4
Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,3.1,Data and Implementation Details,"our model and competing methods are evaluated on the brats2018 segmentation
challenge dataset [1,14]. the task involves segmentation of three subregions of
brain tumours, namely enhancing tumour (et), tumour core (tc), and whole tumour
(wt). the dataset consists of 3d multi-modal brain mris, including flair, t1, t1
contrast-enhanced (t1c), and t2, with ground-truth annotations. the dataset
comprises 285 cases for training, and 66 cases for evaluation. the ground-truth
annotations for the training set are publicly available, while the validation
set annotations are hidden 2 .3d unet architecture (with 3d convolution and
normalisation) is adopted as our backbone network, where the ckd process occurs
at the bottom stage of the unet structure. to optimise our model, we adopt a
stochastic gradient descent optimiser with nesterov momentum [2] set to 0.99. l1
loss is adopted for ckd (.) in eq. 4. batch-size is set to 2. the learning rate
is initially set to 10 -2 and gradually decreased via the cosine annealing [13]
strategy. we trained the lckd model for 115,000 iterations and use 20% of the
training data as the validation task for teacher election. to simulate
modality-missing situations with non-dedicated training of models, we randomly
dropped 0 to 3 modalities for each iteration. our training time is 70.12 h and
testing time is 6.43 s per case on one nvidia 3090 gpu. 19795 mib gpu memory is
used for model training with batch-size 2 and 3789 mib gpu memory is consumed
for model testing with batch-size 1.",4
Learnable Cross-modal Knowledge Distillation for Multi-modal Learning with Missing Modality,3.2,Overall Performance,"table 1 shows the overall performance on all 15 possible combinations of missing
modalities for three sub-regions of brain tumours. our models are compared with
several strong baseline models: u-hemis (abbreviated as hmis in the figure) [8],
u-hved (hved) [6], robust-mseg (rseg) [4] and mmformer (mmfm) [21]. we can
clearly observe that with t1c, the model performs considerably better than other
modalities for et. similarly, t1c for tc and flair for wt contribute the most,
which confirm our motivation.the lckd model significantly outperforms (as shown
by the one-tailed paired t-test for each task between models in the last row of
table 1) u-hemis, u-hved, robust-mseg and mmformer in terms of the segmentation
dice for enhancing tumour and whole tumour on all 15 combinations and the tumour
core on 14 out of 15. it is observed that, on average, the proposed lckd model
improves the state-of-the-art performance by 3.61% for enhancing tumour, 5.99%
for tumour core, and 3.76% for whole tumour in terms of the segmentation dice
score. especially in some combinations without the best modality, e.g. et/tc
without t1c and wt without flair, lckd has a 6.15% improvement with only flair
and 10.69% with only t1 over the second best model for et segmentation; 10.8%
and 10.03% improvement with only flair and t1 for tc; 8.96% and 5.01%
improvement with only t1 and t1c for wt, respectively. these results demonstrate
that useful knowledge of the best modality has been successfully distilled into
the model by lckd for multimodal learning with missing modalities.",4
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,1.0,Introduction,"gliomas are the most commonly seen central nervous system malignancies with
aggressive growth and low survival rates [19]. accurate multi-class segmentation
of gliomas in multimodal magnetic resonance imaging (mri) plays an indispensable
role in quantitative analysis, treatment planning, and monitoring of progression
and treatment. although deep learning-based methods have achieved
state-of-the-art performance in automated brain tumor segmentation [6][7][8]14],
their performance often drops when tasked with segmenting out-of-distribution
samples and poor-quality artifactual images. however, segmentations of desired
quality are required to reliably drive treatment decisions and facilitate
clinical management of gliomas. therefore, tools for automated quality control
(qc) are essential for the clinical translation of automated segmentation
methods. such tools can enable a streamlined clinical workflow by identifying
catastrophic segmentation failures, informing clinical experts where the
segmentations need to be refined, and providing a quantitative measure of
quality that can be taken into account in downstream analyses.most previous
studies of segmentation qc only provide subject-level quality assessment by
either directly predicting segmentation-quality metrics or their surrogates.
specifically, wang et al. [18] leveraged a variational autoencoder to learn the
latent representation of good-quality image-segmentation pairs in the context of
cardiac mri segmentation. during the inference, an iterative search scheme was
performed in the latent space to find a surrogate segmentation. this
segmentation is assumed to be a good proxy of the (unknown) ground-truth
segmentation of the query image, and can thus be compared to the at-hand
predicted segmentation to estimate its quality. another approach that takes
advantage of the pairs of images and ground-truth segmentation is the reverse
classification accuracy (rca) framework [13,17]. in this framework, the test
image is registered to a preselected reference dataset with known ground-truth
segmentation. the quality of a query segmentation is assessed by warping the
query image to the reference dataset. however, these methods primarily targeted
qc of cardiac mri segmentation, which involves a single imaging modality and a
single tissue type with a welch-characterized location and appearance. in
contrast, brain tumor segmentation involves the delineation of heterogeneous
tumor regions, which are manifested through intensity changes relative to the
surrounding healthy tissue across multiple modalities. importantly, there is
significant variability in brain tumor appearances, including multifocal masses
and complex shapes with heterogeneous textures. consequently, adapting
approaches for automated qc of cardiac segmentation to brain tumor segmentation
is challenging. additionally, iterative search or registration during inference
makes the existing methods computationally expensive and time-consuming, which
limits their applicability in large-scale segmentation qc.multiple studies have
also explored regression-based methods to directly predict segmentation-quality
metrics, e.g., dice similarity coefficient (dsc). for example, kohlberger et al.
[10] used support vector machine (svm) with handcrafted features to detect
cardiac mri segmentation failures. robinson et al. [12] proposed a convolutional
neural network (cnn) to automatically extract features from segmentations
generated by a series of random forest segmenters to predict dsc for cardiac mri
segmentation. kofler et al. [9] proposed a cnn to predict holistic ratings of
segmentations, which were annotated by neuroradiologists, with the goal of
better emulating how human experts. though these regression-based methods are
advantageous for fast inference, they do not provide voxel-level localization of
segmentation failures, which can be crucial for both auditing purposes and
guiding manual refinements.in summary, while numerous efforts have been devoted
to segmentation qc, most works were in the context of cardiac mri segmentation
with few works tackling segmentation qc of brain tumors, which have more complex
and heterogeneous appearances than the heart. furthermore, most of the existing
methods do not localize segmentation errors, which is meaningful for both
auditing purposes and guiding manual refinement. to address these challenges, we
propose a novel framework for joint subject-level and voxel-level prediction of
segmentation quality from multimodal mri. the contribution of this work is
four-fold. first, we proposed a predictive model (qcresunet) that simultaneously
predicts dsc and localizes segmentation errors at the voxel level. second, we
devised a datageneration approach, called seggen, that generates a wide range of
segmentations of varying quality, ensuring unbiased model training and testing.
third, our end-to-end predictive model yields fast inference. fourth, the
proposed method achieved a good performance in predicting subject-level
segmentation quality and identifying voxel-level segmentation failures.",4
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,2.0,Method,"given four imaging modalities denoted as [x 1 , x 2 , x 3 , x 4 ] and a
predicted multiclass brain tumor segmentation mask (s pred ), the goal of our
approach is to automatically assess the tumor segmentation quality by
simultaneously predicting dsc and identifying segmentation errors as a binary
mask (s err ). toward this end, we proposed a 3d encoder-decoder architecture
termed qcresunet (see fig. 1(a)) for simultaneously predicting dsc and
localizing segmentation errors. qcresunet has two parts trained in an end-to-end
fashion: i) a resnet-34 [4] encoder for dsc prediction; and ii) a decoder
architecture for segmentation error map prediction (i.e., the difference between
predicted segmentation and ground-truth segmentation).the resnet-34 encoder
enables the extraction of semantically rich features that are useful for
characterizing the quality of the segmentation. we maintained the main structure
of the vanilla 2d resnet-34 [4] but made the following modifications, which were
necessary to account for the 3d nature of the input data (see fig. 1(b)). first,
all the 2d convolutional layers and pooling layers in the vanilla resnet were
changed to 3d. second, the batch normalization [5] was replaced by instance
normalization [16] to accommodate the small batch size in 3d model training.
third, spatial dropout [15] with a probability of 0.3 was added to each residual
block to prevent overfitting.the building block of the decoder consisted of an
upsampling by a factor of two, which was implemented by a nearest neighbor
interpolation in the feature map, followed by two convolutional blocks that
halve the number of feature maps. each convolutional block comprised a 3 × 3 × 3
convolutional layer followed by an instance normalization layer and a leaky relu
activation [11] (see fig. 1(c)). the output of each decoder block was
concatenated with features from the corresponding encoder level to facilitate
information flow from the encoder to the decoder. compared to the encoder, we
used a shallower decoder with fewer parameters to prevent overfitting and reduce
computational complexity.the objective function for training qcresunet consists
of two parts. the first part corresponds to the dsc regression task. it consists
of a mean absolute error (mae) loss (l mae ) term that penalizes differences
between ground truth (dsc gt ) and predicted dsc (dsc pred ):where n denotes the
number of samples in a batch. the second part of the objective function
corresponds to the segmentation error prediction. it consists of a dice loss [3]
and a binary cross-entropy loss, given by:where s errgt , s err pred denote the
binary ground-truth segmentation error map and the predicted error segmentation
map from the sigmoid output of the decoder, respectively. the dice loss and
cross-entropy loss were averaged across the number of pixels i in a batch. the
two parts are combined using a weight parameter λ to balance the different loss
components:3 experimentsfor this study, pre-operative multimodal mri scans of
varying grades of glioma were obtained from the 2021 brain tumor segmentation
(brats) challenge [1] training dataset (n = 1251). for each subject, four
modalities viz. pre-contrast t1-weighted (t1), t2-weighted (t2), post-contrast
t1-weighted (t1c), and fluid attenuated inversion recovery (flair) are included
in the dataset. it also included expert-annotated multi-class tumor segmentation
masks comprising enhancing tumor (et), necrotic tumor core (ncr), and edema (ed)
classes. all data were already registered to a standard anatomical atlas and
skull-stripped. the skull-stripped scans were then z-scored to zero mean and
unit variance. all the data was first cropped to non-zero value regions, and
then zero-padded to a size of 160 × 192 × 160 to be fed into the network.",4
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,3.1,Data Generation,"the initial dataset was expanded by producing segmentation results at different
levels of quality to provide an unbiased estimation of segmentation quality. to
this end, we adopted a three-step approach. first, a nnunet framework [6] was
adopted and trained five times separately using different modalities as input
(i.e., t1-only, t1c-only, t2-only, flair-only, and all four modalities). as only
certain tissue-types are captured in each modality (e.g., enhancing tumor is
captured well in t1c but not in flair), this allowed us to generate
segmentations of a wide range of qualities. nnunet was selected for this purpose
due to its wide success in brain tumor segmentation tasks. second, to further
enrich our dataset with segmentations of diverse quality, we sampled
segmentations along the training routines at different iterations. a small
learning rate (1 × 10 -6 ) was chosen in training all the models to slower their
convergence in order to sample segmentations gradually sweeping from poor
quality to high quality. third, we devised a method called seggen that applied
image transformations, including random rotation (angle = [-15 • , 15 • ]),
random scaling (scale = [0.85, 1.25]), random translation (moves = [-20, 20]),
and random elastic deformation (displacement = [0, 20]), to the ground-truth
segmentations with a probability of 0.5, resulting in three segmentations for
each subject.the original brats 2021 training dataset was split into training (n
= 800), validation (n = 200), and testing (n = 251) sets. after applying the
three-step approach, it resulted in 48000, 12000, and 15060 samples for the
three sets, respectively. however, this generated dataset suffered from
imbalance (fig. 2(a), (b), and(c)) because the cnn models could segment most of
the cases correctly. training using such an imbalanced dataset is prone to
producing biased models that do not generalize well. to mitigate this issue, we
proposed a resampling strategy during the training to make the dsc more
uniformly distributed. specifically, we used the quantile transform to map the
distribution of a variable to a target distribution by randomly smoothing out
the samples unrelated to the target distribution. using the quantile transform,
the data generator first transformed the distribution of the generated dsc to a
uniform distribution. next, the generated samples closest to the transformed
uniform distribution in terms of euclidean distance were chosen to form the
resampled dataset. after applying our proposed resampling strategy, the dsc in
the training and validation set approached a uniform distribution (fig. 2(a),
(b), and (c)). the total number of samples before and after resampling remained
the same with repeating samples. we kept the resampling stochastic at each
iteration during training to make all the generated samples seen by the model.
the generated testing set was also resampled to perform an unbiased estimation
of the quality at different levels resulting in 4895 samples.in addition to the
segmentations generated by the nnunet framework and the seggen method, we also
generated out-of-distribution segmentation samples for the testing set to
validate the generalizability of our proposed model. for this purpose, five
models were trained on the training set using the deepmedic framework [8] with
different input modalities (i.e., t1-only, t1c-only, t2-only, flair-only, and
all four modalities). this resulted in 251 × 5 = 1255 out-ofdistribution samples
in the testing set.",4
QCResUNet: Joint Subject-Level and Voxel-Level Prediction of Segmentation Quality,5.0,Conclusion,"in this work, we proposed a novel cnn architecture called qcresunet to perform
automatic brain tumor segmentation qc in multimodal mri scans. qcre-sunet
simultaneously provides subject-level segmentation-quality prediction and
localizes segmentation failures at the voxel level. it achieved superior dsc
prediction performance compared to all baselines. in addition, the ability to
localize segmentation errors has the potential to guide the refinement of
predicted segmentations in a clinical setting. this can significantly expedite
clinical workflows, thus improving the overall clinical management of gliomas.",4
EGE-UNet: An Efficient Group Enhanced UNet for Skin Lesion Segmentation,1.0,Introduction,"malignant melanoma is one of the most rapidly growing cancers in the world. as
estimated by the american cancer society, there were approximately 100,350 new
cases and over 6,500 deaths in 2020 [14]. thus, an automated skin lesion
segmentation system is imperative, as it can assist medical professionals in
swiftly identifying lesion areas and facilitating subsequent treatment
processes. to enhance the segmentation performance, recent studies tend to
employ modules with larger parameter and computational complexity, such as
incorporating self-attention mechanisms of vision transformer (vit) [7]. for
example, swin-unet [4], based on the swin transformer [11], leverages the
feature extraction ability of self-attention mechanisms to improve segmentation
performance. b) respectively show the visualization of comparative experimental
results on the isic2017 and isic2018 datasets. the x-axis represents the number
of parameters (lower is better), while y-axis represents miou (higher is
better). the color depth represents computational complexity (gflops, lighter is
better). (color figure online)transunet [5] has pioneered a serial fusion of cnn
and vit for medical image segmentation. transfuse [26] employs a dual-path
structure, utilizing cnn and vit to capture local and global information,
respectively. utnetv2 [8] utilizes a hybrid hierarchical architecture, efficient
bidirectional attention, and semantic maps to achieve global multi-scale feature
fusion, combining the strengths of cnn and vit. transbts [23] introduces
self-attention into brain tumor segmentation tasks and uses it to aggregate
high-level information.prior works have enhanced performance by introducing
intricate modules, but neglected the constraint of computational resources in
real medical settings. hence, there is an urgent need to design a low-parameter
and low-computational load model for segmentation tasks in mobile healthcare.
recently, unext [22] has combined unet [18] and mlp [21] to develop a
lightweight model that attains superior performance, while diminishing parameter
and computation. furthermore, malunet [19] has reduced the model size by
declining the number of model channels and introducing multiple attention
modules, resulting in better performance for skin lesion segmentation than
unext. however, while malunet greatly reduces the number of parameter and
computation, its segmentation performance is still lower than some large models,
such as trans-fuse. therefore, in this study, we propose ege-unet, a lightweight
skin lesion segmentation model that achieves state-of-the-art while
significantly reducing parameter and computation costs. additionally, to our
best knowledge, this is the first work to reduce parameter to approximately
50kb.to be specific, ege-unet leverages two key modules: the group multi-axis
hadamard product attention module (ghpa) and group aggregation bridge module
(gab). on the one hand, recent models based on vit [7] have shown promise, owing
to the multi-head self-attention mechanism (mhsa). mhsa divides the input into
multiple heads and calculates self-attention in each head, which allows the
model to obtain information from diverse perspectives, integrate different
knowledge, and improve performance. nonetheless, the quadratic complexity of
mhsa enormously increases the model's size. therefore, we present the hadamard
product attention mechanism (hpa) with linear complexity. hpa employs a
learnable weight and performs a hadamard product operation with the input to
obtain the output. subsequently, inspired by the multi-head mode in mhsa, we
propose ghpa, which divides the input into different groups and performs hpa in
each group. however, it is worth noting that we perform hpa on different axes in
different groups, which helps to further obtain information from diverse
perspectives. on the other hand, for gab, since the size and shape of
segmentation targets in medical images are inconsistent, it is essential to
obtain multi-scale information [19]. therefore, gab integrates high-level and
low-level features with different sizes based on group aggregation, and
additionally introduce mask information to assist feature fusion. via combining
the above two modules with unet, we propose ege-unet, which achieves excellent
segmentation performance with extremely low parameter and computation. unlike
previous approaches that focus solely on improving performance, our model also
prioritizes usability in real-world environments. a clear comparison of ege-unet
with others is shown in fig. 1.in summary, our contributions are threefold: (1)
ghpa and gab are proposed, with the former efficiently acquiring and integrating
multi-perspective information and the latter accepting features at different
scales, along with an auxiliary mask for efficient multi-scale feature fusion.
(2) we propose ege-unet, an extremely lightweight model designed for skin lesion
segmentation.(3) we conduct extensive experiments, which demonstrate the
effectiveness of our methods in achieving state-of-the-art performance with
significantly lower resource requirements.",4
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,1.0,Introduction,"dynamic contrast-enhanced magnetic resonance imaging (dce-mri) revealing tumor
hemodynamics information is often applied to early diagnosis and treatment of
breast cancer [1]. in particular, automatically and accurately segmenting tumor
regions in dce-mri is vital for computer-aided diagnosis (cad) and various
clinical tasks such as surgical planning. for the sake of promoting segmentation
performance, recent methods utilize the dynamic mr sequence and exploit its
temporal correlations to acquire powerful representations [2][3][4]. more
recently, a handful of approaches take advantage of hemodynamic knowledge and
time intensity curve (tic) to improve segmentation accuracy [5,6]. however, the
aforementioned methods require the complete dce-mri sequences and overlook the
difficulty in assessing complete temporal sequences and the missing time point
problem, especially post-contrast phase, due to the privacy protection and
patient conditions. hence, these breast cancer segmentation models cannot be
deployed directly in clinical practice.recently, denoising diffusion
probabilistic model (ddpm) [7,8] has produced a tremendous impact on image
generation field due to its impressive performance. diffusion model is composed
of a forward diffusion process that add noise to images, along with a reverse
generation process that generates realistic images from the noisy input [8].
based on this, several methods investigate the potential of ddpm for natural
image segmentation [9] and medical image segmentation [10][11][12].
specifically, baranchuk et al. [9] explores the intermediate activations from
the networks that perform the markov step of the reverse diffusion process and
find these activations can capture semantic information for segmentation.
however, the applicability of ddpm to medical image segmentation are still
limited. in addition, existing ddpm-based segmentation networks are generic and
are not optimized for specific applications. in particular, a core question for
dce-mri segmentation is how to optimally exploit hemodynamic priors.based on the
above observations, we innovatively consider the underlying relation between
hemodynamic response function (hrf) and denoising diffusion process (ddp). as
shown in fig. 1, during hrf process, only tumor lesions are enhanced and other
non-tumor regions remain unchanged. by designing a network architecture to
effectively transmute pre-contrast images into post-contrast images, the network
should acquire hemodynamic inherent in hrf that can be used to improve
segmentation performance. inspired by the fact that ddpm generates images from
noisy input provided by the parameterized gaussian process, this work aims to
exploit implicit hemodynamic information by a diffusion process that predict
post-contrast images from noisy pre-contrast images. specifically, given the
pre-contrast and post-contrast images, the latent kinetic code is learned using
a score function of ddpm, which contains sufficient hemodynamic characteristics
to facilitate segmentation performance.once the diffusion module is pretrained,
the latent kinetic code can be easily generated with only pre-contrast images,
which is fed into a segmentation module to annotate cancers. to verify the
effectiveness of the latent kinetic code, the sm adopts a simple u-net-like
structure, with an encoder to simultaneously conduct semantic feature encoding
and kinetic code fusion, along with a decoder to obtain voxel-level
classification. in this manner, our latent kinetic code can be interpreted to
provide tic information and hemodynamic characteristics for accurate cancer
segmentation.we verify the effectiveness of our proposed diffusion kinetic model
(dkm) on dce-mri-based breast cancer segmentation using breast-mri-nact-pilot
dataset [13]. compared to the existing state-of-the-art approaches with complete
sequences, our method yields higher segmentation performance even with
precontrast images. in summary, the main contributions of this work are listed
as follows:• we propose a diffusion kinetic model that implicitly exploits
hemodynamic priors in dce-mri and effectively generates high-quality
segmentation maps only requiring pre-contrast images. • we first consider the
underlying relation between hemodynamic response function and denoising
diffusion process and provide a ddpm-based solution to capture a latent kinetic
code for hemodynamic knowledge. • compared to the existing approaches with
complete sequences, the proposed method yields higher cancer segmentation
performance even with pre-contrast images.",4
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2.2,Segmentation Module,"once pretrained, the dm outputs multi-scale latent kinetic code f dm from
intermediate layers, which is fed into the sm to guide cancer segmentation. as
shown in fig. 2(b), the sm consists of four kinetic blocks and four up blocks.
each kinetic block is composed of a fusion layer, two convolutional layers, two
batch normalization layers, two relu activation functions, a max pooling layer
and a residual addition. specifically, to obtain sufficient expressive power to
transform the learned kinetic code into higher-level features, at least one
learnable linear transformation is required. to this end, a linear
transformation, parametrized by a weight matrix w , is applied to the latent
code f dm , followed by a batch normalization, relu activation layer and
concatenation, which can be represented as follows:where * represents 1 × 1
based convolution operation, w is the weight matrix, bn represents batch
normalization, φ represents relu activation function and c is concatenation
operation. in this way, the hemodynamic knowledge can be incorporated into the
sm to capture more expressive representations to improve segmentation
performance.",4
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,2.3,Model Training,"to maintain training stability, the proposed dkm adopts a two-step training
procedure for cancer annotation. in the first step, the dm is trained to
transform pre-contrast images into post-contrast images for a latent space where
hemodynamic priors are exploited. in particular, the diffusion loss for the
reverse diffusion process can be formulated as follows:where θ represents the
denoising model that employs an u-net structure, x 0 and x k are the
pre-contrast and post-contrast images, respectively, is gaussian distribution
data ∼ n (0, i), and t is a timestep. for a second step, we train the sm that
integrates the previously learned latent kinetic code to provide tumor
hemodynamic information for voxel-level prediction. considering the varying
sizes, shapes and appearances of tumors that results from intratumor
heterogeneity and results in difficulties of accurate cancer annotation, we
design the segmentation loss as follows:where l ssim is used to evaluate tumor
structural characteristics, s and g represents segmentation map and ground
truth, respectively; μ s is the mean of s and μ g is the mean of g; ϕ s
represents the variance of s and ϕ g represents the variance of g; c 1 and c 2
denote the constant to hold training stable [15], and ϕ sg is the covariance
between s and g. the λ is set as 0.5 empirically. following [16],where k 1 is
set as 0.01, k 2 is set as 0.03 and l is set as the range of voxel values.",4
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,3.0,Experiments,"dataset: to demonstrate the effectiveness of our proposed dkm, we evaluate our
method on 4d dce-mri breast cancer segmentation using the breast-mri-nact-pilot
dataset [13], which contains a total of 64 patients with the contrastenhanced
mri protocol: a pre-contrast scan, followed by 2 consecutive postcontrast time
points (as shown in fig. 3). each mr volume consists of 60 slices and the size
of each slice is 256 × 256. regarding preprocessing, we conduct zeromean
unit-variance intensity normalization for the whole volume. we divided the
original dataset into training (70%) and test set (30%) based on the scans.
ground truth segmentations of the data are provided in the dataset for tumor
annotation. no data augmentation techniques are used to ensure fairness.",4
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,0.0,Competing Methods and Evaluation Metrics:,"to comprehensively evaluate the proposed method, we compare it with 3d
segmentation methods, including dual attention net (danet) [17], multiresunet
[18] and multi-task learning network (mtln) [19], and 4d segmentation methods,
including lnet [20], 3d patch u-net [21], and hybridnet [5]. all approaches are
evaluated using 1) dice similarity coefficient (dsc) and 2) jaccard index
(ji).implementation details: we implement our proposed framework with pytorch
using two nvidia rtx 2080ti gpus to accelerate model training.following ddpm
[8], we set 128, 256, 256, 256 channels for each stage in the dm and set the
noise level from 10 -4 to 10 -2 using a linear schedule with t = 1000.once the
dm is trained, we extract intermediate feature maps from four resolutions for
further segmentation task. similar to dm, the sm also consists of four
resolution blocks. however, unlike channel settings of dm, we set 128, 256, 512,
1024 channels for each stage in the sm to capture expressive and sufficient
semantic information. the sm is optimized by adam with a learning rate 2 × 10 -5
and a weight decay 10 -6 . the model is trained for 500 epochs with the batch
size to 1. no data augmentation techniques are used to ensure
fairness.comparison with sota methods: the quantitative comparison of the
proposed method to recent state-of-the-art methdos is reported in table 1.
experimental results demonstrate that the proposed method comprehensively other
models with less scans (i.e., pre-contrast) in testing. we attribute it to the
ability of diffusion module to exploit hemodynamic priors to guide the
segmentation task. specifically, in comparison with 3d segmentation models (e.g.
mtln), our method yields higher segmentation scores. the possible reason is that
our method is able to exploit the time intensity curve, which contains richer
information compared to post-contrast scan. besides, we can observe that our
method achieves improvements when compared to 4d segmentation models using
complete sequence. our method outperform the hybridnet by 7.1% and 7.0% in dsc
and ji, respectively. it probably due to two aspects: 1) the hemodynamic
knowledge is implicitly exploited by diffusion module from pre-contrast images,
which is useful for cancer segmentation.2) the intermediate activations from
diffusion models effectively capture the semantic information and are excellent
pixel-level representations for the segmentation problem [9]. thus, combining
the intermediate features can further promote the segmentation performance. in a
word, the proposed framework can produce accurate prediction masks only
requiring pre-contrast images. this is useful when post-contrast data is
limited.ablation study: to explore the effectiveness of the latent kinetic code,
we first conduct ablation studies to select the optimal setting. we denote the
intermediate features extracted from each stage in the dm as f 1 , f 2 , f 3 ,
and f 4 , respectively, where f i represents the feature map of i-th stage.
table 2 reports the segmentation performance with different incorporations of
intermediate kinetic codes. it can be observed that the latent kinetic code is
able to guide the network training for better segmentation results.
specifically, we note that the incorporation of f 3 and f 4 achieves the highest
scores among these combinations, and outperforms the integration of all features
by 2.0% and 2.6% in dsc and ji, respectively. we attribute it to the denoising
diffusion model that receives the noisy input, leading to the noise of shallow
features. in contrast, the deep features capture essential characteristics to
reveal the structural information and hemodynamic changes of tumors. figure 4
shows visual comparison of segmentation performance. the above results reveal
that incorporation of kinetic code comfortably outperform the baseline without
hemodynamic information.",4
Diffusion Kinetic Model for Breast Cancer Segmentation in Incomplete DCE-MRI,4.0,Conclusion,"we propose a diffusion kinetic model by exploiting hemodynamic priors in dce-mri
to effectively generate high-quality segmentation results only requiring
precontrast images. our models learns the hemodynamic response function based on
the denoising diffusion process and estimates the latent kinetic code to guide
the segmentation task. experiments demonstrate that our proposed framework has
the potential to be a promising tool in clinical applications to annotate
cancers.",4
Morphology-Inspired Unsupervised Gland Segmentation via Selective Semantic Grouping,1.0,Introduction,"accurate gland segmentation from whole slide images (wsis) plays a crucial role
in the diagnosis and prognosis of cancer, as the morphological features of
glands can provide valuable information regarding tumor aggressiveness [11].
with the prior uss methods in medical image research [2] and natural image
research [6] vs. our mssg. green and orange regions denote the glands and the
background respectively. (color figure online) emergence of deep learning (dl),
there has been a growing interest in developing dl-based methods for
semantic-level [9,12,36] and instance-level [5,13,25,27,32,35] gland
segmentation. however, such methods typically rely on large-scale annotated
image datasets, which usually require significant effort and expertise from
pathologists and can be prohibitively expensive [28].to reduce the annotation
cost, developing annotation-efficient methods for semantic-level gland
segmentation has attracted much attention [10,18,23,37]. recently, some
researchers have explored weakly supervised semantic segmentation methods which
use weak annotations (e.g., bound box [37] and patch tag [18]) instead of
pixel-level annotations to train a gland segmentation network. however, these
weak annotations are still laborious and require expert knowledge [37]. to
address this issue, previous works have exploited conventional clustering
[8,22,23] and metric learning [10,29] to design annotation-free methods for
gland segmentation. however, the performance of these methods can vary widely,
especially in cases of malignancy. this paper focuses on unsupervised gland
segmentation, where no annotations are required during training and
inference.one potential solution is to adopt unsupervised semantic segmentation
(uss) methods which have been successfully applied to medical image research and
natural image research. on the one hand, existing uss methods have shown
promising results in various medical modalities, e.g., magnetic resonance images
[19],x-ray images [1,15] and dermoscopic images [2]. however, directly utilizing
these methods to segment glands could lead to over-segment results where a gland
is segmented into many fractions rather than being considered as one target (see
fig. 1(b)). this is because these methods are usually designed to be extremely
sensitive to color [2], while gland images present a unique challenge due to
their highly dense and complex tissues with intricate color distribution [18].
on the other hand, prior uss methods for natural images can be broadly
categorized into coarse-to-fine-grained [4,14,16,21,31] and end-to-end (e2e)
cluster-ing [3,6,17]. the former ones typically rely on pre-generated coarse
masks (e.g., super-pixel proposals [16], salience masks [31], and self-attention
maps [4,14,21]) as prior, which is not always feasible on gland images. the e2e
clustering methods, however, produce under-segment results on gland images by
confusing many gland regions with the background; see fig. 1(b). this is due to
the fact that e2e clustering relies on the inherent connections between pixels
of the same class [33], and essentially, grouping similar pixels and separate
dissimilar ones. nevertheless, the glands are composed of different parts (gland
border and interior epithelial tissues, see fig. 1(a)) with significant
variations in appearance. gland borders typically consist of dark-colored cells,
whereas the interior epithelial tissues contain cells with various color
distributions that may closely resemble those non-glandular tissues in the
background. as such, the e2e clustering methods tend to blindly cluster pixels
with similar properties and confuse many gland regions with the background,
leading to under-segment results.to tackle the above challenges, our solution is
to incorporate an empirical cue about gland morphology as additional knowledge
to guide gland segmentation. the cue can be described as: each gland is
comprised of a border region with high gray levels that surrounds the interior
epithelial tissues. to this end, we propose a novel morphology-inspired method
via selective semantic grouping, abbreviated as mssg. to begin, we leverage the
empirical cue to selectively mine out proposals for the two gland sub-regions
with variant appearances. then, considering that our segmentation target is the
gland, we employ a morphology-aware semantic grouping module to summarize the
semantic information about glands by explicitly grouping the semantics of the
sub-region proposals. in this way, we not only prioritize and dedicate extra
attention to the target gland regions, thus avoiding under-segmentation; but
also exploit the valuable morphology information hidden in the empirical cue,
and force the segmentation network to recognize entire glands despite the
excessive variance among the sub-regions, thus preventing over-segmentation.
ultimately, our method produces well-delineated and complete predictions; see
fig. 1(b).our contributions are as follows: (1) we identify the major challenge
encountered by prior unsupervised semantic segmentation (uss) methods when
dealing with gland images, and propose a novel mssg for unsupervised gland
segmentation. (2) we propose to leverage an empirical cue to select gland
sub-regions and explicitly group their semantics into a complete gland region,
thus avoiding over-segmentation and under-segmentation in the segmentation
results. (3) we validate the efficacy of our mssg on two public glandular
datasets (i.e., the glas dataset [27] and the crag dataset [13]), and the
experiment results demonstrate the effectiveness of our mssg in unsupervised
gland segmentation.",4
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,1.0,Introduction,"accurate segmentation of brain tumors from mri images is of great significance
as it enables more accurate assessment of tumor morphology, size, location, and
distribution range, thereby providing clinicians with a reliable basis for
diagnosis and treatment [16]. physicians manually delineate the tumor regions
based on the varying signal intensities between diseased and normal tissues.
this signal disparity constitutes the edge information in the images, making it
essential for accurate tumor segmentation.cnn-based networks, such as unet [2],
segresnet [15], and nnunet [8], have made significant progress in the field of
medical image segmentation, including brain tumor segmentation. with the
emergence of transformer [19], which is capable of modeling long-range
dependencies that cnns struggle with, a number of cnn-transformer hybrid
networks have been proposed, such as transbts [21], unetr [7], swin-unetr [6]
and nestedformer [23], leading to further improvements in brain tumor
segmentation. however, the performance of existing brain tumor segmentation
methods are still unsatisfactory, especially for the segmentation of edges
between tumor lesion and normal tissues.considerable advancement has been
achieved in the field of natural image segmentation by focusing on the edge
information [3,11,18,25], and this idea is also being applied to medical image
segmentation. some methods utilize the distance-dependent objective functions to
generate more accurate edge predictions. karimi et al. [9] design a
hausdorff-based metric loss function to minimize hausdorff distance (hd), which
is used to measure the edge distance between two point sets. other methods
[1,12,20,22] involve post-processing uncertain regions to more accurately
segment pixels near edges. for example, bat [20] considers global context to
coarsely locate lesion area and paying special attention to the ambiguous area
to specify the exact edges of the skin cancers. similarly, xie et al. [22] use
the confidence map to evaluate the uncertainty of each pixel to enhance the
segmentation of the ambiguous edges of ultrasound images. however, the methods
mentioned above are not suitable for brain tumor segmentation for two main
reasons. (1) efficiency. for instance, karimi et al. [9] require the calculation
of the hd at each iteration, which is both time-consuming and computationally
demanding. moreover, processing every slice of large volumes of mri images at
the pixel-level is impractical. (2) task complexity. unlike many other medical
image segmentation tasks that involve the segmentation of a single roi, brain
tumor segmentation requires the simultaneous segmentation of three regions: the
whole tumor (wt), the tumor core (tc), and the enhancing tumor (et) regions.
therefore, in addition to focusing on the edge between the tumor lesion and
normal tissue to segment the wt, it is also necessary to consider the edges
within the tumor in order to segment the tc and et regions.in this paper, we
propose an edge-oriented transformer (eoformer), for efficient and accurate
brain tumor segmentation. we design a cnn-transformer based encoder for more
effective feature representation, called efficient hybrid encoder (ehe).
specifically, the input image is first processed by the cnn blocks to extract
low-level local features. then, the extracted features are fed into the
transformer blocks to create long-range dependencies, resulting in the formation
of high-level semantic features. in addition, to provide more accurate edge
predictions, we design two edge sharpening modules in the decoder, called
edgeoriented sobel (eos) and laplacian (eol) modules. by implicitly embedding
sobel and laplacian filters into the convolution layers to extract 1st-order and
2nd-order differential features, the two modules could enhance the edge
information contained in the feature maps. in order to reduce the computational
and memory complexity of the model, we replace the vanilla attention module with
our extended efficient attention module [17]. to simplify the model architecture
and reduce inference time, we also introduce the re-parameterization technique
[4,5]. our model has been evaluated on both the publicly brats 2020 dataset and
a private medulloblastoma segmentation dataset. the results demonstrate that
eoformer clearly outperforms the state-of-the-art methods with limited model
parameters and lower flops (see more in supplementary material). (2) a decoder
which incorporates edge-oriented modules to enhance the edge information in
features.",4
EoFormer: Edge-Oriented Transformer for Brain Tumor Segmentation,4.0,Conclusion,"in this paper, we propose the eoformer, a novel approach for brain tumor
segmentation. our method comprises the efficient hybrid encoder and the
edgeoriented transformer decoder. the encoder effectively extracts features from
images by striking a balance between cnn and transformer architectures. the
decoder integrates the sobel and laplacian edge detection filters into our
edgeoriented modules that enhance the extraction capability of edge and texture
information. besides, we introduce the efficient attention mechanism and the
re-parameterization technology to improve the model efficiency. our eoformer
outperforms other state-of-the-art methods on both brats 2020 and medseg. our
model is computationally efficient and can be readily applied to other 3d
medical image segmentation tasks.",4
EdgeMixup: Embarrassingly Simple Data Alteration to Improve Lyme Disease Lesion Segmentation and Diagnosis Fairness,1.0,Introduction,"medical image analysis has greatly benefited from advances in ai [1] yet some
improvements still remain to be addressed, importantly in areas that allow both
algorithmic performance and fairness [2], and in certain medical applications
that promise to significantly lessen morbidity and mortality. early detection of
skin lesions is such an endeavor as it can aid in identifying infectious
diseases with cutaneous manifestations. lyme disease is an example of that with
a potentially diagnostic skin lesion [3]-which is caused by the bacterium
borrelia burgdorferi and leads to nearly 476,000 cases per annum during
2010-2018 [4]. the earliest and most treatable phase of lyme disease is
manifested via a red concentric lesion at the site of a tick bite, called
erythema migrans (em) [5]. while the em pattern may appear simple to recognize,
its diagnosis can be challenging for those with or without a medical background
alike, as only 20% of united states patients have the stereotypical bull's eye
lesion [6]. when skin lesions are atypical they can be mistaken for other
diseases such as tinea corporis (tc) or herpes zoster (hz), two other diseases
acting as confusers for lyme, considered herein. this has increased interest in
medical applications of deep learning (dl), and using deep convolutional neural
networks (cnns), to assist clinicians in timely and accurate diagnosis of
conditions including lyme disease, tc and hz [7][8][9].one important diagnosis
task is to segment lyme lesion, particularly the em pattern, from benign skins.
such dl-assisted segmentation not only helps clinicians in pre-screening
patients but also improves downstream tasks such as lesion classification.
however, while lyme disease lesion segmentation is intuitively simple, it is
challenging due to the following reasons. first, there lacks of a well-segmented
dataset with manual labels on lyme disease. on one hand, some datasets-such as
ham10000 [10] and isbi challenges [11]-have manual annotated segmentations for
diseases like melanoma, but they do not have lyme disease lesions. on the other
hand, some datasets-such as groh et al. [12]-have lyme disease and skin tone and
classification labels, but not segmentation.second, the segmentation of lyme
lesion is itself challenging due to the nature of em pattern. specifically, a
typical lyme lesion exhibits a bull's eye pattern with one central redness and
one outer circle, which is different from darkness lesion in cancer-related skin
disease like melanoma. furthermore, clinical data collected for training is
usually imbalanced in some properties, e.g., more samples with light skins
compared with dark skins. therefore, existing skin disease segmentation [13] as
well as existing general segmentation works, such as u-net [14], polar training
[15], vit-adapter [16], and mfsnet [17], usually suffer from relatively low
performance and reduced fairness [2,18,19].in this paper, we present the first
lyme disease dataset that contains labeled segmentation and skin tones. our lyme
disease dataset contains two parts: (i) a classification dataset, composed of
more than 3,000 diseased skin images that are either obtained from public
resources or clinicians with patient-informed consent, and (ii) a segmentation
dataset containing 185 samples that are manually annotated for three
regions-i.e., background, skin (light vs. dark), and lesionconducted under
clinician supervision and institutional review boards (irb) approval. our
dataset with manual labels is available at this url [20].secondly, we design a
simple yet novel data preprocessing and alternation method, called edgemixup, to
improve lyme disease segmentation and diagnosis fairness on samples with
different skin-tones. the key insight is to alter a skin image with a linear
combination of the source image and a detected lesion boundary so that the
lesion structure is preserved while minimizing skin tone information. such an
improvement is an iterative process that gradually improves lesion edge
detection and segmentation fairness until convergence. then, the detected,
converged edge in the first step also helps classification of lyme diseases via
mixup with improved fairness. our source code is available at this url [20].we
evaluate edgemixup for skin disease segmentation and classification tasks. our
results show that edgemixup is able to increase segmentation utility and improve
fairness. we also show that the improved segmentation further improves
classification fairness as well as joint fairness-utility metrics compared to
existing debiasing methods, e.g., ad [21] and st-debias [22].",4
Factor Space and Spectrum for Medical Hyperspectral Image Segmentation,1.0,Introduction,"medical hyperspectral imaging (mhsi) is an emerging imaging modality which
acquires two-dimensional medical images across a wide range of electromagnetic
spectrum. it brings opportunities for disease diagnosis, and computational
pathology [16]. typically, an mhsi is presented as a hypercube, with hundreds of
narrow and contiguous spectral bands in spectral dimension, and thousands of
pixels in spatial dimension (fig. 1(a)). due to the success of 2-dimensional
(2d) deep neural network in natural images, the simplest way to classify/segment
an mhsi is to treat its two spatial dimensions as input spatial dimension, and
treat its spectral dimension as input channel dimension [25] (fig. 1(c)).
dimensionality reduction [12] and recurrent approaches [1] are usually adopted
to aggregate spectral information before feeding the hsi into 2d networks (fig.
1(d)). these methods are not suitable for high spatial resolution mhsi, and they
may bring noises in spatial features while reducing spectral dimension. the 2d
networks are computationally efficient, usually much faster than 3d networks.
but, they mix spectral information after the first convolutional layer, making
the interband correlations of mhsis underutilized. building a 3d network usually
suffers from high computational complexity, but it is the most straightforward
way to learn interpixel and interband correlations of mhsis [23] (fig. 1(e)).
since spatiospectral orientations are not equally likely, there is no need to
treat space and spectrum symmetrically, as is implicit in 3d networks. we might
instead design a dual-stream strategy to ""factor"" the architecture. a few hsi
classification backbones try to design dual-stream architectures that treat
spatial structures and spectral intensities separately [2,20,30] (fig. 1(f)).
but, these methods simply adopt convolutional or mlp layers to extract spectral
features. spectr [28], learning spectral and spatial features alternatively,
utilizes transformer to capture the global spectral feature. they overlook the
low rankness in the spectral domain, which contains discriminative information
for differentiating targets from the background.high spatiospectral dimensions
make it difficult to perform a thorough analysis of mhsi. in mhsis, there exist
two types of correlation. one is a spectral correlation in adjacent pixels. as
shown in fig. 1(b), the intensity values vs. spectral bands for the local
positive (cancer) area and negative (normal) area are highly correlated. the
other is spatial correlation between adjacent bands. figure 1(b) plots the
spatial similarity among all bands, and shows large cosine similarity scores
among nearby bands (error band of line chart in the light color area) and small
scores between bands in a long distance. the correlation implies spectral
redundancy when representing spatial features, and spatial redundancy when
learning spectral features. the low-rank structure in mhsis holds significant
discriminatory and characterizing information [11]. exploring mhsi's low-rank
prior can promote the segmentation performance.in this paper, we consider
treating spatiospectral dimensions separately and propose an effective and
efficient dual-stream strategy to ""factor"" the architecture, by exploiting the
correlation information of mhsis. our dual-stream strategy is designed based on
2d cnns with u-shaped [16] architecture. for the spatial feature extraction
stream, inspired from spatial redundancy between adjacent bands, we group
adjacent bands into a spectral agent. different spectral agents are fed into a
2d cnn backbone as a batch. for the spectral feature extraction stream, inspired
by the low-rank prior on the spectral space, we propose a matrix
factorization-based method to capture global spectral information. to remove the
redundancy in the spatiospectral features and promote the capability of
representing the low-rank prior of mhsi, we further design lowrank decomposition
modules, and employ the canonical-polyadic decomposition method [9,32]. our
space and spectrum factorization strategy is plug-and-play. the effectiveness of
the proposed strategy is compared and verified by plugging in different 2d
architectures. we also show that with our proposed strategy, u-net model using
resnet-34 can achieve state-of-the-art mhsi segmentation with 3-13 faster than
other 3d architectures.",4
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,1.0,Introduction,"the periodic acquisition and analysis of volumetric ct and mri scans of oncology
patients is essential for the evaluation of the disease status, the selection of
the treatment, and the response to treatment. currently, scans are acquired
every 2-12 months according to the patient's characteristics, disease stage, and
treatment regime. the scan interpretation consists of identifying lesions
(primary tumors, metastases) in the affected organs and characterizing their
changes over time. lesion changes include changes in the size of existing
lesions, the appearance of new lesions, the disappearance of existing lesions,
and complex lesion changes, e.g., the formation of conglomerate lesions. as
treatments improve and patients live longer, the number of scans in longitudinal
studies increases and their interpretation is more challenging and
time-consuming.radiological follow-up requires the quantitative analysis of
lesions and patterns of lesion changes in subsequent scans. it differs from
diagnostic reading since the goal is to find and quantify the differences
between the scans, rather than to find abnormalities in a single scan. in
current practice, quantification of lesion changes is partial and approximate.
the recist 1.1 guidelines call for finding new lesions (if any), identifying up
to the five largest lesions in each scan in the ct slice where they appear
largest, manually measuring their diameters, and comparing their difference [1].
while volumetric measures of individual lesions and of all lesions (tumor
burden) have long been established as more accurate and reliable than partial
linear measurements, they are not used clinically because they require manual
lesion delineation and lesion matching in unregistered scans, which is usually
time-consuming and subject to variability [2].in a previous paper, we presented
an automatic pipeline for the detection and quantification of lesion changes in
pairs of ct liver scans [3]. this paper describes a graph-based lesion tracking
method for the comprehensive analysis of lesion changes and their patterns at
the lesion level. the tasks are formalized as graph-theoretic problems (fig. 1).
complex lesion changes include merged lesions, which occurs when at least two
lesions grow and merge into one (possible disease progression), split lesions,
which occurs when a lesion shrinks and cleaves into several parts (possible
response to treatment) and conglomeration of lesions, which occurs when clusters
of lesions coalesce. while some of these lesion changes have been observed [4],
they have been poorly studied. comprehensive quantitative analysis of lesion
changes and patterns is of clinical importance, since response to treatment may
vary among lesions, so the analysis of a few lesions may not be
representative.the novelties of this paper are: 1) identification and
formalization of longitudinal lesion matching and patterns of lesion changes in
ct in a graph-theoretic framework; 2) new classification and detection of
changes of individual lesions and lesion patterns based on the properties of the
lesion changes graph and its connected components; 3) a simultaneous lesion
matching method with more than two scans; 4) graph-based methods for the
detection of changes in individual lesions and patterns of lesion changes.
experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18
patients) datasets show that our method yields high classification accuracy.to
the best of our knowledge, ours is the first method to perform longitudinal
lesion matching and lesion changes pattern detection. only a few papers address
lesion matching in pairs of ct/mri scans [5][6][7][8][9][10][11][12][13] -none
performs simultaneous matching of all lesions in more than two scans. also, very
few methods [3,14] handle matching of split/merged lesions. although many
methods exist for object tracking in optical images and videos [15][16][17],
they are unsuited for analyzing lesion changes since they assume many
consecutive 2d images where objects have very similar appearance and undergo
small changes between images. overlap-based methods pair two lesions in
registered scans when their segmentations overlap, with a reported accuracy of
66-98% [3,[5][6][7][8][9][10][11]18]. these methods assume that organs and
lesions undergo minor changes, are very sensitive to registration errors, and
cannot handle complex lesion changes. similarity-based methods pair two lesions
with similar features, e.g., intensity, shape, location [13][14][15][16] with an
84-96% accuracy on the deeplesion dataset [14]. they are susceptible to major
changes in the lesion appearance and do not handle complex lesion changes.
split-andmerge matching methods are used for cell tracking in fluorescence
microscopy [19]. they are limited to 2d images, assume registration between
images, and do not handle conglomerate changes.",5
Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,3.0,Experimental Results,"we evaluated our method with two studies on retrospectively collected patient
datasets that were manually annotated by an expert radiologist.dataset: lung and
liver ct studies were retrospectively obtained from two medical centers
(hadassah univ hosp jerusalem israel) during the routine clinical examination of
patients with metastatic disease. each patient study consists of at least 3
scans.dlung consists of 83 chest ct scans from 19 patients with a mean 4.4 ± 2.0
scans/patient, a mean time interval between consecutive scans of 125.9 ± 81.3
days, and voxel sizes of 0.6-1.0 × 0.6-1.0 × 1.0-3.0 mm 3 . dliver consists of
77 abdominal cect scans from 18 patients with a mean 4.3 ± 2.0 scans/patient, a
mean time interval between consecutive scans of 109.7 ± 93.5 days, and voxel
sizes of 0.6-1.0 × 0.6-1.0 × 0.8-5.0 mm 3 .lesions in both datasets were
annotated by an expert radiologist, yielding a total of 1,178 lung and 800 liver
lesions, with a mean of 14.2 ± 19.1 and 10.4 ± 7.9 lesions/scan (lesions with
<20 voxels were excluded). ground-truth lesion matching graphs and lesion
changes labeling were produced by running the method on the datasets and then
having the radiologist review and correct the resulting node labels and
edges.study 1: lesion changes labeling, lesion matching, evaluation of patterns
of lesion changes. we ran our method on the dlungs and dliver lesion
segmentations. the settings of the parameters were: dilation distance d = 1 mm,
overlap percentage p = 10%, number of iterations r = 5 and 7, and centroid
maximum distance δ = 17 and 23 mm for the lungs and liver lesions,
respectively.we compared the computed and ground-truth lesion changes graphs
with two metrics: 1) lesion changes classification accuracy, which is the % of
correct computed labels from the ground truth labels; 2) lesion matching
precision and recall based on the presence/absence of computed vs. ground truth
edges. the precision and recall definitions were adapted so that wrong or missed
non-consecutive edges are counted as true positive when there is a path between
their vertices in either the ground-truth or the computed graph. table 1
summarizes the results. the distribution of lesion changes labels for dlungs
(1,178 lesions) is unique 785 (67%), new 215 (18%), lone 109 (9%), disappeared
51 (4%), merged 12 (1%), split 6 (1%), complex 0 (0%) with class accuracy ≥ 96%
for all except split (66%). for dliver (800 lesions) it is unique 450 (56%), new
185 (23%), lone 45 (6%), disappeared 77 (10%), merged 27 (3%), split 18 (2%),
complex 1 (0.05%) with class accuracy ≥ 81% for all except disappeared (71%) and
split (67%).for the patterns of lesion changes, we compared the computed and
ground truth patterns of lesion changes. the accuracy is the % of identical
connected components in each category. table 1 summarizes the results. note that
the split_p, merged_p and complex_p patterns jointly account for 3% and 8% of
the cases. these patterns are hard to detect manually but their correct
classification and tracking are crucial for the proper application of the recist
1.1 follow-up protocol [1]. study 2: detection of missed lesions in the ground
truth. the expert radiologist was asked to examine non-consecutive edges and
lesions labeled as lone in the lesion changes graph and determine if lesions
were unseen or undetected (actual or presumed false negative) in the skipped or
contiguous scans (fig. 1d). for each non-consecutive edge connecting lesions v i
j , v k l , he analyzed the corresponding region in the skipped scans s j at t j
∈ ]t i , t k [ for possible missed lesions. for the dlungs dataset, 25 visible
and 5 faintly visible or surmised to be present unmarked lesions were found for
27 nonconsecutive edges. for the dliver dataset, 20 visible and 21 faintly
visible or surmised to be present unmarked lesions were found for 25
non-consecutive edges.after reviewing the 42 and 37 lesions labeled as lone in
dlungs and dliver with > 5mm diameter, the radiologist determined that 1 and 8
of them had been wrongly identified as a cancerous lesion. moreover, he found
that 14 and 16 lesions initially labeled as lone, had been wrongly classified:
for these lesions he found 15 and 21 previously unmarked matching lesions in the
next or previous scans. in total, 45 and 62 missing lesions were added to the
ground truth dlungs and dliver datasets, respectively. these hard-to-find
ground-truth false negatives (3.7%, 7.2% of all lesions) may change the
radiological interpretation and the disease status. see the supplemental
material for examples of these scenarios.",5
Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,3.0,Experimental Results and Conclusion,"experimental settings: the parameters α and λ 1 are are set to 1, while β and λ
2 are searched form {0.5, 1, 2, 5} and {0.1, 0.5, 1}, respectively. we use adam
optimizer and the learning rate is 0.001. to evaluate the performance of
classification, we employ a support vector machine as the basic classifier,
where the kernel is set as the radial basis function (rbf) and parameter c is
set to 1. we average the 3-fold cross-validation results.results and discussion:
we compare the classification accuracy of the proposed reasfs with several sota
baselines, including two filter methods: maximal information coefficient (mic)
[5], gini [23], and four sparse coding-based methods: multi-task feature
learning via 2,1 norm [6,12], discriminative feature selection via 2,0 norm
[21], feature selection via 1,2 norm [10] and exclusive 2,1 [9]. the proposed
reasfs is expected to have better robustness and flexibility. it can be seen
from fig. 2 that the sparse coding-based methods achieve better performance than
filter methods under most conditions, where ""0%"" represents no noise
contamination. the highest accuracy of our reasfs demonstrates the effectiveness
and flexibility of the proposed gcie 2,1 . generally speaking, the probability
of samples being contaminated by random noise is equal. therefore, we randomly
select features from the training set and replace the selected features with
pulse noise. the number of noisy attributes is denoted by the ratio between the
numbers of selected features and total features, such as 15% and 30%. the
classification performances of the npsle dataset contaminated by attribute noise
are shown in fig. 3(a) and fig. 3(b), where one clearly perceives that our
reasfs achieves the highest accuracy under all conditions. besides, it is
unreasonable to apply the same level of sparse regularization to noise features
and uncontaminated features, and our gcie 2,1 can adaptively increase the sparse
level of noise features to remove redundant information, and vice versa. for
label noise, we randomly select samples from the training set and replace
classification labels of the selected samples with opposite values, i.e., 0 → 1
and 1 → 0. the results are shown in fig. 3(c) and fig. 3(d), where the proposed
reasfs is superior to other baselines. it can be seen from fig. 3 that our
reasfs achieves the highest accuracy in different noisy environments, which
demonstrates the robustness of generalized correntropic loss. for non-invasive
biomarkers, our method shows that some metabolic features contribute greatly to
the early diagnosis of npsle, i.e., naag, mi/cr+pcr, and glu+gln/cr+pcr in rpcg;
cr+pcr, naa+naag, naa+naag/cr+pcr, mi/cr+pcr and glu+gln in lpcg; naa, naag, and
cho+pch in ldt; pcr, cr+pcr, cho+pch, cho+pch/cr+pcr and glu+gln/cr+pcr in rln;
mi/cr+pcr, cho+pch and cho+pch/cr+pcr in lln; naa+naag/cr+pcr and cho+pch in ri;
cho+pch/cr+pcr and glu+gln/cr+pcr in rpwm; and pcr, naag and naa+naag/cr+pcr in
lpwm. moreover, we use isometric feature mapping (isomap) [19] to analyze these
metabolic features and find that this feature subset is essentially a
low-dimensional manifold. meanwhile, by combining the proposed reasfs and
isomap, we can achieve 99% accuracy in the early diagnosis of npsle. in
metabolite analysis, some studies have shown that the decrease in naa
concentration is related to chronic inflammation, damage, and tumors in the
brain [18]. in the normal white matter area, different degrees of npsle disease
is accompanied by different degrees of naa decline, but structural mri is not
abnormal, suggesting that naa may indicate the progress of npsle. we also found
that glu+gln/cr+pcr in ri decreased, which indicates that the excitatory
neurotransmitter glu in the brain of patients with npsle may have lower
activity. to sum up, the proposed method provides a shortcut for revealing the
pathological mechanism of npsle and early detection.",5
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,1.0,Introduction,"lung cancer is currently the foremost cause of cancer-related mortalities
globally, with non-small cell lung cancer (nsclc) being responsible for 85% of
reported cases [25]. within nsclc, squamous cell carcinoma (scc) and
adenocarcinoma (adc) are recognized as the two principal histological subtypes.
since scc and adc differ in the effectiveness of chemotherapy and the risk of
complications, accurate identification of different subtypes is crucial for
clinical treatment options [15]. although pathological diagnosis via lung biopsy
can provide a reliable result of subtype identification, it is highly invasive
with potential clinical implications [19]. therefore, non-invasive methods
utilizing computed tomography (ct) images have garnered significant attention
over the last decade [15,16].recently, several deep-learning methods have been
put forward to differentiate between the nsclc histological subtypes using ct
images [4,11,13,22]. chaunzwa et al. [4] and marentakis et al. [13] both employ
a convolutional neural network (cnn) model with axial view ct images to classify
the tumor histology into scc and adc. albeit the good performance, the above 2d
cnn-based models only take ct images from a single view as the input, limiting
their ability to describe rich spatial properties of ct volumes [20]. multi-view
deep learning, a 2.5d method, represents a promising solution to this issue, as
it focuses on obtaining a unified joint representation from different views of
lung nodules to capture abundant spatial information [16,20]. for example, wu et
al. [22] aggregate features from axial, coronal, and sagittal view ct images via
a multi-view fusion model. similarly, li et al. [11] also extract patches from
three orthogonal views of a lung nodule and present a multi-view resnet for
feature fusion and classification. by integrating multi-view representations,
these methods efficiently preserve the spatial information of ct volumes while
significantly reducing the required computational resource compared to 3d cnns
[9,20,23].despite the promising results of previous multi-view methods, they
still confront a severe challenge for accurate nsclc histological subtype
prediction. in fact, due to the limitation of scan time and hardware capacity in
clinical practice, different views of ct volumes are anisotropic in terms of
in-plane and inter-plane resolution [21]. additionally, images from certain
views may inevitably contain some unique background information, e.g., the spine
in the sagittal view [17]. such anisotropy and background dissimilarity both
reveal the existence of significant variations between different views, which
lead to markedly various representations in feature space. consequently, the
discrepancies of distinct views will hamper the fusion of multi-view
information, limiting further improvements in the classification performance.to
overcome the challenge mentioned above, we propose a novel cross-aligned
representation learning (carl) method for the multi-view histologic subtype
classification of nsclc. carl offers a holistic and disentangled perspective of
multi-view ct images by generating both view-invariant and -specific
representations. specifically, carl incorporates a cross-view representation
alignment learning network which targets the reduction of multi-view
discrepancies by obtaining discriminative view-invariant representations. a
shared encoder with a novel discriminability-enforcing similarity constraint is
utilized to map all representations learned from multi-view ct images to a
common subspace, enabling cross-view representation alignment. such aligned
projections help to capture view-invariant features of cross-view ct images and
meanwhile make full use of the discriminative information obtained from each
view. additionally, carl learns view-specific representations as well which
complement the view-invariant ones, providing a comprehensive picture of the ct
volume data for histological subtype prediction. we validate our approach by
using a publicly available nsclc dataset from the cancer imaging archive (tcia).
detailed experimental results demonstrate the effectiveness of carl in reducing
multi-view discrepancies and improving nsclc histological subtype classification
performance. our contributions can be summarized as follows:-a novel
cross-aligned representation learning method called carl is proposed for nsclc
histological subtype classification. to reduce the discrepancies of multiview ct
images, carl incorporates a cross-view representation alignment learning network
for discriminative view-invariant representations. -we employ a view-specific
representation learning network to learn view-specific representations as a
complement to the view-invariant representations. -we conduct experiments on a
publicly available dataset and achieve superior performance compared to the most
advanced methods currently available.",5
CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,3.1,Dataset,"our dataset nsclc-tcia for lung cancer histological subtype classification is
sourced from two online resources of the cancer imaging archive (tcia) [5]:
nsclc radiomics [1] and nsclc radiogenomics [2]. exclusion criteria involves
patients diagnosed with large cell carcinoma or not otherwise specified, along
with cases that have contouring inaccuracies or lacked tumor delineation [9,13].
finally, a total of 325 available cases (146 adc cases and 179 scc cases) are
used for our study. we evaluate the performance of nsclc classification in
five-fold cross validation on the nsclc-tcia dataset, and measure accuracy
(acc), sensitivity (sen), specificity (spe), and the area under the receiver
operating characteristic (roc) curve (auc) as evaluation metrics. we also
conduct analysis including standard deviations and 95% ci, and delong
statistical test for further auc comparison.for preprocessing, given that the ct
data from nsclc-tcia has an in-plane resolution of 1 mm × 1 mm and a slice
thickness of 0.7-3.0 mm, we resample the ct images using trilinear interpolation
to a common resolution of 1mm × 1mm × 1mm. then one 128 × 128 pixel slice is
cropped from each view as input based on the center of the tumor. finally
following [7], we clip the intensities of the input patches to the interval
(-1000, 400 hounsfield unit) and normalize them to the range of [0, 1].",5
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,1.0,Introduction,"gastroscopic lesion detection (gld) plays a key role in computer-assisted
diagnostic procedures. although deep neural network-based object detectors
achieve tremendous success within the domain of natural images, directly
training generic object detectors on gld datasets performs below expectations
for two reasons: 1) the scale of labeled data in gld datasets is limited in
comparison to natural images due to the annotation costs. though gastroscopic
images are abundant, those containing lesions are rare, which necessitates
extensive image review for lesion annotation. 2) the characteristic of
gastroscopic images exhibits distinct differences from the natural images
[18,19,21] and is often of high similarity in global but high diversity in
local. specifically, each type of lesion may have diverse appearances though
gastroscopic images look quite similar. some appearances of lesions are quite
rare and can only be observed in a few patients. generic self-supervised
backbone pre-training or semi-supervised detector training methods can solve the
first challenge for natural images but its effectiveness is undermined for
gastroscopic images due to the second challenge.self-supervised backbone
pre-training methods enhance object detection performance by learning
high-quality feature representations from massive unlabelled data for the
backbone. the mainstream self-supervised backbone pretraining methods adopt
self-supervised contrast learning [3,4,7,9,10] or masked fig. 1. pipeline of
self-and semi-supervised learning (ssl) for gld. ssl consists of a hybrid
self-supervised learning (hsl) method and a prototype-based pseudo-label
generation (ppg) method. hsl combines patch reconstruction with dense
contrastive learning. ppg generates pseudo-labels for potential lesions based on
the similarity to the prototype feature vectors.image modeling [8,15].
self-supervised contrastive learning methods [3,4,7,9] can learn discriminative
global feature representations, and [10] can further learn discriminative local
feature representations by extending contrastive learning to dense paradigm.
however, these methods usually cannot grasp enough local detailed information.
on the other hand, masked image modeling is expert in extracting local detailed
information but is weak in preserving the discriminability of feature
representation. therefore, both types of methods have their own weakness for gld
tasks.semi-supervised object detection methods [12,14,16,17,20,22,23] first use
detectors trained with labeled data to generate pseudo-labels for unlabeled data
and then enhance object detection performance by regarding these unlabeled data
with pseudo-labels as labeled data to train the detector. current pseudolabel
generation methods rely on the objectiveness score threshold to generate
pseudo-labels, which makes them perform below expectations on gld, because the
characteristic of gastroscopic lesions makes it difficult to set a suitable
threshold to discover potential lesions meanwhile avoiding introducing much
noise.the motivation of this paper is to explore how to enhance gld performance
using massive unlabeled gastroscopic images to overcome the labeled data
shortage problem. the main challenge for this goal is the characteristic of
gastroscopic lesions. intuitively, such a challenge requires local feature
representations to contain enough detailed information, meanwhile preserving
discriminability. enlightened by this, we propose the self-and semi-supervised
learning (ssl) framework tailored to address challenges in daily clinical
practice and use massive unlabeled data to enhance gld performance. ssl
overcomes the challenges of gld by leveraging a large volume of unlabeled
gastroscopic images using self-supervised learning for improved feature
representations and semi-supervised learning to discover and utilize potential
lesions to enhance performance. specifically, it consists of a hybrid
self-supervised learning (hsl) method for self-supervised backbone pre-training
and a prototype-based pseudo-label generation (ppg) method for semi-supervised
detector training. the hsl combines the dense contrastive learning [10] with the
patch reconstruction to inherit the advantages of discriminative feature
learning and grasp the detailed information that is important for gld tasks. the
ppg generates pseudo-labels based on the similarity to the prototype feature
vectors (formulated from the feature vectors in its memory module) to discover
potential lesions from unlabeled data, and avoid introducing much noise at the
same time. moreover, we propose the first large-scale gld datasets (lgldd),
which contains 10,083 gastroscopic images with 12,292 well-annotated lesion
bounding boxes of four categories of lesions (polyp, ulcer, cancer, and
sub-mucosal tumor). we evaluate ssl with multiple detectors on lgldd and ssl
brings significant improvement compared with baseline methods (centernet [6]:
+2.7ap, faster rcnn [13]: +2.0ap). in summary, our contributions include:-a
self-and semi-supervise learning (ssl) framework to leverage massive unlabeled
data to enhance gld performance. -a large-scale gastroscopic lesion detection
datasets (lgldd) -experiments on lgldd demonstrate that ssl can bring
significant enhancement compared with baseline methods.",5
Self- and Semi-supervised Learning for Gastroscopic Lesion Detection,3.0,Datasets,"we contribute the first large-scale gstroscopic lesion detection datasets
(lgldd) in the literature. collection : lgmdd collects about 1m+ gastroscopic
images from 2 hospitals of about 500 patients and their diagnosis reports. after
consulting some senior doctors and surveying gastroscopic diagnosis papers [1],
we select to annotate 4-category lesions: polyp(pol), ulcer(ulc), cancer(can)
and sub-mucosal tumor(smt). we invite 10 senior doctors to annotate them from
the unlabeled endoscopic images. to preserve the annotation quality, doctors can
refer to the diagnosis reports, and each lesion is annotated by a doctor and
checked by another. finally, they annotates 12,292 lesion boxes in 10,083 images
after going through about 120,000 images. the polyp, ulcer, cancer, and
sub-mucosal tumor numbers are 7,779, 2,171, 1,164and 1,178, respectively. the
train/val split of lgmdd is 8,076/2,007. the other data serves as unlabeled
data.evaluation metrics : we use standard object detection metrics to evaluate
the gld performance, which computes the average precision (ap) under multiple
intersection-of-union (iou) thresholds and then evaluate the performance using
the mean of aps (map) and the ap of some specific iou threshold. for map, we
follow the popular object detection datasets coco [11] and calculate the mean of
11 aps of iou from 0.5 to 0.95 with stepsize 0.05 (map @[.5:.05:.95]).we also
report ap under some specific iou threshold (ap 50 for .5, ap 75 for .75) and ap
of different scale lesions (ap s , ap m , ap l ) like coco [11].",5
Revisiting Feature Propagation and Aggregation in Polyp Segmentation,1.0,Introduction,"colorectal cancer is a life-threatening disease that results in the loss of
millions of lives each year. in order to improve survival rates, it is essential
to identify colorectal polyps early. hence, regular bowel screenings are
recommended, where endoscopy is the gold standard. however, the accuracy of
endoscopic screening can heavily rely on the individual skill and expertise of
the domain experts involved, which are prone to incorrect diagnoses and missed
cases. to reduce the workload on physicians and enhance diagnostic accuracy,
computer vision technologies, such as deep neural networks, are involved to
assist in the presegmentation of endoscopic images. the unet-like model uses
skip connections that transmit only single-stage features. in contrast, our
approach utilizes fpe to propagate features from all stages, incorporating a
gate mechanism to regulate the flow of valuable information.deep learning-based
image segmentation methods have gained popularity in recent years, dominated by
unet [11] in the field of medical image segmentation. unet's success has led to
the development of several other methods that use a similar encoder-decoder
architecture to tackle polyp segmentation, including resunet++ [7], pranet [3],
caranet [10] and uacanet [8]. however, these methods are prone to inefficient
feature fusion at the decoder due to the transmission of multi-stage features
without filtering out irrelevant information.to address these limitations, we
propose a novel feature enhancement network for polyp segmentation that employs
feature propagation enhancement (fpe) modules to transmit multi-scale features
from all stages to the decoder. figure 1 illustrates a semantic comparison of
our feature propagation scheme with the unet-like model. while the existing
unet-like models use skip connections to propagate a single-scale feature, our
method utilizes fpe to propagate multi-scale features from all stages in
encoder. more importantly, this research highlights the usage of fpe can
effectively replace skip connections by providing more comprehensive multi-scale
characteristics from full stages in encoder. to further address the issue of
high-level semantics being overwhelmed in the progressive feature fusion
process, we also integrate a feature aggregation enhancement (fae) module that
aggregates the outputs of fpe from previous stages at decoder. moreover, we
introduce gate mechanisms in both fpe and fae to filter out redundant
information, prioritizing informative features for efficient feature fusion.
finally, we propose a multi-scale aggregation (msa) module appended to the
output of the encoder to capture multi-scale features and provide the decoder
with rich multi-scale semantic information. the msa incorporates a cross-stage
multi-scale feature aggregation scheme to facilitate the aggregation of
multi-scale features. overall, our proposed method improves upon existing
unet-like encoder-decoder architectures by addressing the limitations in feature
propagation and feature aggregation, leading to improved polyp segmentation
performance.our major contributions to accurate polyp segmentation are
summarized as follows.(1) the method addresses the limitations of the unet-like
encoder-decoder architecture by introducing three modules: feature propagation
enhancement (fpe), feature aggregation enhancement (fae), and multi-scale
aggregation (msa). ( 2) fpe transmits all encoder-extracted feature maps to the
decoder, and fae combines the output of the last stage at the decoder and
multiple outputs from fpe. msa aggregates multi-scale high-level features from
fpes to provide rich multi-scale information. (3) the proposed method achieves
state-of-the-art results in five polyp segmentation datasets and outperforms the
previous cutting-edge approach by a large margin (3%) on cvc-colondb and etis
datasets.",5
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",1.0,Introduction,"in the uk, approximately 11,500 patients are diagnosed with rectal cancer each
year [19]. a common form of treatment for such patients is neoadjuvant therapy,
including chemotherapy and radiotherapy, which can be given to patients with
locally advanced rectal cancer to shrink the tumour prior to surgery. recent
evidence suggests that 10-20% of patients will have a complete pathological
response to neoadjuvant therapy and can therefore avoid surgery altogether
[2,5]. however, one third of patients do not benefit from radiotherapy treatment
prior to surgery [8], hence it is important to determine how a patient will
respond to radiotherapy with a personalized approach in order to avoid
overtreatment.histology-based digital biomarkers enable the possibility to
predict a patient's response to therapy. the consensus molecular subtypes (cms)
classification system derived from gene expressions [9] has been developed to
provide biological insight into metastatic colorectal cancer. it has been shown
that these four cms classes can be predicted directly from the standard
haematoxylin and eosin (h&e) stained slide images using deep learning [18].
various studies have investigated the link between cms and patient outcomes,
suggesting that patients with tumour classified as cms4, which features stromal
invasion [9] and shows significantly higher stroma content [15], have worse
survival rates compared to the other cms classes [5]. increased stromal content
has independently been shown to be a predictor for increased risk of recurrence
in early rectal cancer [10], and tumour immune infiltrate evaluated with
immunoscore is a useful prognostic marker [3]. the spatial organisation of the
cancerous tissue has been identified as a biomarker for aggressiveness or
recurrence [12], and qi et al. [15] found that the features they developed
representing spatial organisation reflected characteristics of the four cms
classes. interactions between the epithelial tissue (cellular tissue lining) and
other prevalent tissue types in the tumour microenvironment are also indicators
of prognosis [15], since progression of colorectal cancer is dependent on both
the epithelial and stromal tissues [20]. other work has looked at predicting
chemoradiotherapy response in rectal cancer patients from h&e images using
different approaches, but without providing contextual interpretations
[19,22].as opposed to predicting response to radiotherapy alone, we aim to
analyse this prediction in the context of the overall tissue architecture and
the tumour biology as captured by cms. input to our model is a standard h&e
whole slide image (wsi) which is split into smaller patches to overcome the
memory limitations of existing gpus. to achieve our goal we need to capture the
heterogeneity at the slide level, which is why applying full or semi-supervised
approaches on individual tiles followed by a slide aggregation method is not
suitable. instead, we build on recent graph neural network (gnn) approaches that
allow us to model the entire wsi as a graph. as local cell communities form the
nodes of such a graph it can effectively model the micro-anatomy of the tissue.
at the same time it is possible to make predictions at the node-, graph-, and
slide-level. related work. to predict the grading of colorectal cancer (crc),
both cellbased and patch-based graphs have been used in separate works [16,23],
setting the nodes of the graph as either cell nuclei or square patches, defining
the node features as either handcrafted or learned features, and then applying a
gnn for outcome prediction. another patch-based gnn approach to predicting
genetic mutations in crc from h&e slides found their model trained on colon
cancer generalised well to rectal cancer. for other cancers, the slidegraph
pipeline clusters nuclei for the graph nodes, and provides node-level
predictions to make their model more interpretable [13]. other approaches to
setting the graph nodes include using subgraphs to represent regions [14], and
creating superpatches by combining patches [11]. edges between the nodes are
usually defined by a spatial distance metric, which helps model the spatial
organisation of the tissue. common choices for gnns include a graph isomorphism
network (gin) with jumping connectivity [7,13,14], as we use in this
research.our methodology proposes a novel and disease relevant approach to a
more interpretable model that effectively supports a diagnostic task.
pathologists and oncologists can use this information to inspect the validity of
the prediction result and interrogate key aspects of the spatial biology that is
critical for patient management. ultimately, this type of information that is
not available today will help to characterise interactions between the tumour
and the host tissue and therefore help to support choice of therapy. the
developed framework combines self-supervised training of a vision transformer
(vit) to extract morphological features, a superpixel algorithm for determining
nodes of a graph, and a gnn for predictions. we achieve 0.82 auc predicting
complete response to radiotherapy using deep learning on wsis for crc patients,
whilst providing novel interpretability of the results.",5
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",2.0,Methods,"in this section we present the patch-level feature extraction, provide the
detail of the superpixel segmentation of the wsi, and illustrate the resulting
graph presentation. a gnn with three branches for our output predictions is used
to simultaneously make the three different predictions as shown in fig.
1.pipeline. for computational reasons, all images are split into patches of size
256 × 256 pixels. in order to have a common feature set all the way up to the
last layer of the gnn, individual patches should be represented by morphological
features that are label-agnostic. this last layer of the gnn then splits into
three branches to predict response to radiotherapy, the cms4 subtype
classification for crc, and epithelial tissue regions. this way we can guarantee
the common latent features and derivation across branches, maintaining the
contextual importance of each branch. the dino framework [4] uses a
self-distillation training approach, using data augmentation to locally crop the
patches and train with a local-global student-teacher approach. we use the dino
framework to train a vit in a self-supervised manner on our h&e slides [6],
representing each patch with 384 features. we use only the training set to train
this model, and use the image patches at 20x magnification. we extract
patch-level features from each wsi using selfsupervised dino training with a vit
model [4]. the slic superpixel algorithm segments the entire slide into smaller
regions [1]. we calculate the mean patch features for these superpixel regions,
and use the superpixel features and centers as our graph nodes, applying
delaunay triangulation to generate the edges of the graph. a gnn consisting of
ginconv layers is trained on these fixed graphs, and the final layer splits into
three separate mlp branches to provide predictions of three different outcomes,
complete response (cr) to radiotherapy (rt), cms4 classification, and epithelial
tissue. an example output is visualized in fig. 2.to find the nodes of the wsi
graphs, we apply the slic superpixel algorithm [1] on the wsis at 5x
magnification to segment the tissue to capture cellular neighbourhoods that are
roughly between 80-100 µm 2 /pixels in size. it can be seen that the superpixel
boundaries consistently align with the boundaries of tissue compartments.the
superpixels centers are used as the nodes of the graph, and the node features
are the weighted mean of the corresponding patch features which overlap with the
superpixel region. the edges of the graph are determined by nearest neighbours
from delaunay triangulation, as in slidegraph [13].building on the ideas
introduced by slidegraph [13] we use ginconv layers [21], adding tempering to
avoid overfitting, and replace their logistic regression scaler with a simple
sigmoid function. we add three branches to the final layer of the gnn, in the
form of three separate multilayer perceptrons (mlps). two of these mlps return a
graph-level prediction, for the response to rt and cms4 predictions, and the
final branch returns node-level predictions, predicting whether each node is
epithelial tissue or not. our loss function is defined aswhere bce is the binary
cross entropy loss, ŷrt ∈ r is the slide-level prediction of response to
radiotherapy, ŷcms4 ∈ r is the slide-level prediction of cms4, ŷepi ∈ r ni are
the node-level predictions of epithelial tissue and n i is the number of nodes
in the i th wsi graph.for each prediction branch, we can visualize the
individual node predictions from the wsi graph, overlaid on the wsi itself, to
get an idea of how the node predictions vary across the different tissue
regions. each graph-level prediction is derived from the corresponding branch
node predictions, by applying pooling and dropout.data. we train and validate
our methods on two retrospective rectal cancer datasets, grampian and aristotle.
both cohorts received standard chemoradiotherapy of pelvic irradiation
(45-50.4gy in 25 fractions over 5 weeks) with capecitabine 900mg/m 2 . the
pre-treatment biopsy slides were all sectioned and stained in the same
laboratory, and scanned at 20x magnification (0.5 µm 2 /pixel) on an aperio
scanner. pathological complete response, which we use as a target outcome here,
was derived from histopathological assessment from posttreatment resections.the
cms labels for this data are derived from three different transcriptomic
versions (single cohort, combined cohort correcting batch effects and combined
cohort including 2036 cases run with the same platform), in order to generate
robust classifications. in all cases the cms call was calculated using the
cmsclassifier random forest and single sample predictor [9]. final cms calls are
based on matching calls between the three transcriptomic versions. despite our
efforts to minimise the noise from rna sequencing, we still expect a certain
level of noise in our ground truth data, which we discuss in the results
section.the epithelial labels for each graph node are calculated from epithelial
masks for each wsi. these epithelial segmentation masks were generated at 10x
magnification (1 µm 2 /pixel) with a u-net [17] which was trained and validated
on 666 full tissue sections belonging to 362 patients from the focus cohort
[18]. the ground truth annotations for the training of this model were generated
by vk.for consistency the tumour regions were marked up by an expert
pathologist. we use these masks in our analysis to filter out background and
irrelevant tissue from the images. grampian and aristotle are used in both
training and validation, with a 70/30% training-validation split, keeping any
wsis from a single patient in the same dataset. we predict complete response to
radiotherapy against all other responses, such as partial response and no
response. the datasets are unbalanced, since in grampian only 61/244 slides have
complete response, and in aristotle only 24/121 slides have complete response.
they are even more unbalanced for cms4, since only 28/244 slides in grampian and
17/121 slides in aristotle are labelled with cms4. we address this imbalance in
the supplementary materials. there are 365 slides total in our dataset, from 249
patients.",5
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",0.0,Response branch,"response results. despite the noise in our reference data used for training, our
model achieves good performance in terms of mean auc scores on all three
prediction branches of our model, predicting complete response to radiotherapy
(rt) with 0.819 auc, cms4 with 0.819 auc and epithelial tissue at the node level
with 0.760 auc across folds. further metrics are provided in table 1. the
prediction performance of the model could be improved by utilising a larger
training dataset and performing more exhaustive parameter searches, however the
current performance of the model is sufficient to demonstrate the impact of this
approach.the predicted response to radiotherapy can now be viewed in the context
of disease biology as captured by cms4. for example, the model demonstrates that
cms4 patients are less likely to respond to radiotherapy. in addition, it is now
possible to view the spatial distribution of cms4 active regions in the tissue
architecture context as shown in fig. 2. additional samples are presented in the
supplementary materials. an example of our proposed prediction maps on two
slides can be seen in fig. 2, with further slides in the supplementary
materials. a pathologist reviewing these maps assesses that the observed
patterns fit the known interplay of response to therapy, cms4 activation, and
the spatial localisation of these signals. in the top slide, we observe high
cms4 activation in stromal rich regions, and interestingly also high cms4
activation in the bottom center, dissociating from the response to rt activation
map. this could be explained by the lymphocyte content, supported by the higher
epithelial map activations in the same location. expert pathologists highlight a
similar pattern in certain regions of the maps for the bottom slide. different
from the slide above, the cms4 and response to rt maps have some overlap with
moderate activations here, encouraging discovery into tumour-host interactions.
ultimately, a pathologist confirmed that these maps support an interpretable and
trustworthy prediction in the context of response to radiotherapy. while we
cannot present a more extensive interpretation of these results due to space
limitations, these examples already indicate that the proposed approach enables
a level of analysis that has not been possible before. we find that predicting
these outcomes individually in a single branch model, particularly with response
to radiotherapy, can result in slightly higher auc scores, but we consciously
make this trade-off in order to provide better interpretability of the model
predictions. the focus of this research is not to achieve the best possible
metrics, but to develop robust methods which can add context and explanation to
clinical black box deep learning model predictions, with the view to ease
clinical translation of such models.to explore the effects of the noisy cms4
ground truth labels, we remove from our dataset any wsis classified as
'unmatched' for the cms call, which for the main results of this paper we
defined as 'not cms4'. removing this data and rerunning our analysis improved
our predictions for cms4 by +0.06 auc, and reduced our response to radiotherapy
and epithelial predictions by -0.02 and -0.01 respectively. the results can be
found in the supplementary materials. these small changes indicate that the
noise in our data does not degrade the performance of our classifier,
reinforcing it as a robust and accurate model.",5
"Joint Prediction of Response to Therapy, Molecular Traits, and Spatial Organisation in Colorectal Cancer Biopsies",4.0,Conclusion,"by setting the prediction of response to therapy in context with disease biology
and spatial organisation of the tissue we are providing a novel approach for
enhancing the interpretablity of complex prediction tasks. these results do not
only enhance the interpretability, they also provide new ways to utilise large
retrospective clinical trial cohorts for which no additional molecular data is
available. extending the amount of training data and improving model training
will improve model performance, which is already impressive.we argue that this
work also advances the state of the art in feature representation and analysis.
our prediction maps derive from the same graph model, and hence they share
underlying graph features. the prediction branches only diverge at the final
stage of translating these graph features into outcome predictions for our three
clinically relevant outcomes. importantly, this level of visualisation is not
only accessible to pathologists, this joint prediction model also enhances the
communication between pathologists and oncologists which is critical for patient
management. by cross-referencing these prediction maps with our prior
understanding of cancer biology, this approach can help to establish trust in
the prediction model and also help to identify potential failure cases.this work
relies on access to well annotated clinical trial samples which will limit our
ability to include more data for training and testing. in future, we plan to use
these methods to help better characterise tumour-stromal interactions of the
tissue. we also plan to use a denser graph with less connectivity to be able to
better predict the heterogeneous epithelial tissue.the aristotle trial was
funded by cancer research uk (cruk/08/032). the funders played no role in the
analyses performed or the results presented. financial support: rw -epsrc center
for doctoral training in health data science (ep/s02428x/1), oxford cruk cancer
centre; vhk -promedica foundation (f-87701-41-01) and swiss national science
foundation (p2skp3_168322/1, p2skp3_168322/2); tsm -s:cort (see above); jr, ks
-oxford nihr national oxford biomedical research centre and the pathlake
consortium (innovateuk). the computational aspects of this research were funded
from the nihr oxford brc with additional support from the wellcome trust core
award grant number 203141/z/16/z. the views expressed are those of the author(s)
and not necessarily those of the nhs, the nihr or the department of health.",5
Automatic Bleeding Risk Rating System of Gastric Varices,1.0,Introduction,"esophagogastric varices are one of the common manifestations in patients with
liver cirrhosis and portal hypertension and occur in about 50 percent of
patients with liver cirrhosis [3,6]. the occurrence of esophagogastric variceal
bleeding is the most serious adverse event in patients with cirrhosis, with a
6-week acute bleeding mortality rate as high as 15%-20% percent [14]. it is
crucial to identify high-risk patients and offer prophylactic treatment at the
appropriate time. regular endoscopy examinations have been proven an effective
clinical approach to promptly detect esophagogastric varices with a high risk of
bleeding [7]. different from the grading of esophageal varices (ev) that is
relatively complete [1], the bleeding risk grading of gastric varices (gv)
involves complex variables including the diameter, shapes, colors, and
locations. several rating systems have been proposed to describe gv based on the
anatomical area. sarin et al. [16] described and divided gv into 2 groups
according to their locations and extensions. hashizume et al. [10] published a
more detailed examination describing the form, location, and color. although the
existing rating systems tried to identify the risk from different perspectives,
they still lack clear quantification standard and heavily rely on the
endoscopists' subjective judgment. this may cause inconsistency or even
misdiagnosis due to the variant experience of endoscopists in different
hospitals. therefore, we aim to build an automatic gv bleeding risk rating
method that can learn a stable and robust standard from multiple experienced
endoscopists.recent works have proven the effectiveness and superiority of deep
learning (dl) technologies in handling esophagogastroduodenoscopy (egd) tasks,
such as the detection of gastric cancer and neoplasia [4]. it is even
demonstrated that ai can detect neoplasia in barrett's esophagus at a higher
accuracy than endoscopists [8]. intuitively we may regard the gv bleeding risk
rating as an image classification task and apply typical classification
architectures (e.g., resnet [12]) or state-of-the-art gastric lesion
classification methods to it. however, they may raise poor performance due to
the large intra-class variation between gv with the same bleeding risk and small
inter-class variation between gv and normal tissue or gv with different bleeding
risks. first, the gv area may look like regular stomach rugae as it is caused by
the blood vessels bulging and crumpling up the stomach (see fig. 1). also, since
the gv images are taken from different distances and angles, the number of
pixels of the gv area may not reflect its actual size. consequently, the model
may fail to focus on the important gv areas for prediction as shown in fig. 3.
to encourage the model to learn more robust representations, we constructively
introduce segmentation into the classification framework. with the segmentation
information, we further propose a region-constraint module (rcm) and a
cross-region attention module (cram) for better feature localization and
utilization. specifically, in rcm, we utilize the segmentation results to
constrain the cam heatmaps of the feature maps extracted by the classification
backbone, avoiding the model making predictions based on incorrect areas. in
cram, the varices features are extracted using the segmentation results and
combined with an attention mechanism to learn the intra-class correlation and
cross-region correlation between the target area and the context.to learn from
experienced endoscopists, gv datasets with bleeding risks annotation is needed.
while most works and public datasets focus on colonoscopy [13,15] and esophagus
[5,9], with a lack of study on gastroscopy images. in the public dataset of
endocv challenge [2], the majority are colonoscopies while only few are
gastroscopy images. in this work, we collect a gv bleeding risks rating dataset
(gvbleed) that contains 1678 gastroscopy images from 411 patients with different
levels of gv bleeding risks. three senior clinical endoscopists are invited to
grade the bleeding risk of the retrospective data in three levels and annotated
the corresponding segmentation masks of gv areas.in sum, the contributions of
this paper are: 1) a novel gv bleeding risk rating framework that constructively
introduces segmentation to enhance the robustness of representation learning; 2)
a region-constraint module for better feature localization and a cross-region
attention module to learn the correlation of target gv with its context; 3) a gv
bleeding risk rating dataset (gvbleed) with high-quality annotation from
multiple experienced endoscopists. baseline methods have been evaluated on the
newly collected gvbleed dataset. experimental results demonstrate the
effectiveness of our proposed framework and modules, where we improve the
accuracy by nearly 5% compared to the baseline model.",5
Improving Outcome Prediction of Pulmonary Embolism by De-biased Multi-modality Model,2.0,Bias in Survival Prediction,"this section describes the detail of how we identify the varying degrees of bias
in multimodal information and illustrates bias using the relative difference in
survival outcomes. we will first introduce our pulmonary embolism multimodal
datasets, including survival and race labels. then, we evaluate the baseline
survival learning framework without de-biasing in the various racial
groups.dataset. the pulmonary embolism dataset used in this study from 918
patients (163 deceased, median age 64 years, range 13-99 years, 52% female),
including 3978 ctpa images and 918 clinical reports, which were identified via
retrospective review across three institutions. the clinical reports from
physicians that provided crucial information are anonymized and divided into
four parts: medical history, clinical diagnosis, observations and radiologist's
opinion. for each patient, the race labels, survival time-to-event labels and
pesi variables are collected from clinical data, and the 11 pesi variables are
used to calculate the pesi scores, which include age, sex, comorbid illnesses
(cancer, heart failure, chronic lung disease), pulse, systolic blood pressure,
respiratory rate, temperature, altered mental status, and arterial oxygen
saturation at the time of diagnosis [2].diverse bias of multimodal survival
prediction model. we designed a deep survival prediction (sp) baseline framework
for multimodal data as shown in fig. 1, which compares the impact of different
population distributions. the frameworks without de-basing are evaluated for
risk prediction in the test set by performing survival prediction on ctpa
images, clinical reports, and clinical variables, respectively. first, we use
two large-scale data-trained models as backbones to respectively extract
features from preprocessed images and cleaned clinical reports. a
state-of-the-art pe detecting model, penet [11] is used as the backbone model
for analyzing imaging risk and extracting information from multiple slices of
volumetric ctpa scans to locate the pe. the feature with the highest pe
probability from a patient's multiple ctpas is considered as the most pe-related
visual representation. next, the gatortron [29] model is employed to recognize
clinical concepts and identify medical relations for getting accurate patient
information from pe clinical reports. the extracted features from the backbones
and pesi variables are represented as f m , m ∈ [img, text, var]. the survival
prediction baseline framework, built upon the backbones, consists of three
multi-layer perceptron (mlp) modules named imaging-based, text-based and
variable-based sp modules. to encode survival features z m sur from image, text
and pesi variables, these modules are trained to distinguish critical disease
from non-critical disease with cox partial log-likelihood loss (coxphloss) [13].
the framework also consists of a cox proportional hazard (coxph) model [7] that
is trained to predict patient ranking using a multimodal combination of risk
predictions from the above three sp modules. these coxph models calculate the
corresponding time-to-event evaluation and predict the fusion of patients' risk
as the survival outcome. we evaluate the performance of each module with
concordance probability (c-index), which measures the accuracy of prediction in
terms of ranking the order of survival times [8]. for reference, the c-index of
pesi scores is additionally provided for comparative analysis.in table 1
(baseline), we computed the c-index between the predicted risk of each model and
time-to-event labels. when debiasing is not performed, significant differences
exist among the different modalities, with the image modality exhibiting the
most pronounced deviation, followed by text and pesi variables. the biased
performance of the imaging-based module is likely caused by the richness of
redundant information in images, which includes implicit features such as body
structure and posture that reflect the distribution of different races. this
redundancy leads to model overfitting on race, compromising the fairness of risk
prediction across different races. besides, clinical data in the form of text
reports and pesi variables objectively reflect the patient's physiological
information and the physician's diagnosis, exhibiting smaller race biases in
correlation with survival across different races. moreover, the multimodal
fusion strategy is found to be effective, yielding more relevant survival
outcomes than the clinical gold standard pesi scores.",5
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,1.0,Introduction,"breast cancer impacts women globally [15] and mammographic screening for women
over a certain age has been shown to reduce mortality [7,10,23]. however,
studies suggest that mammography alone has limited sensitivity [22]. to mitigate
this, supplemental screening like mri or a tailored screening interval have been
explored to add to the screening protocol [1,13]. however, these imaging
techniques are expensive and add additional burdens for the patient. recently,
several studies [8,32,33] revealed the potential of artificial intelligence (ai)
to develop a better risk assessment model to identify women who may benefit from
supplemental screening or a personalized screening interval and these may lead
to improved screening outcomes.in clinical practice, breast density and
traditional statistical methods for predicting breast cancer risks such as the
gail [14] and the tyrer-cuzick models [27] have been used to estimate an
individual's risk of developing breast cancer. however, these models do not
perform well enough to be utilized in practical screening settings [3] and
require the collection of data that is not always available. recently, deep
neural network based models that predict a patient's risk score directly from
mammograms have shown promising results [3,8,9,20,33]. these models do not
require additional patient information and have been shown to outperform
traditional statistical models.when prior mammograms are available, radiologists
compare prior exams to the current mammogram to aid in the detection of breast
cancer. several studies have shown that utilizing past mammograms can improve
the classification performance of radiologists in the classification of benign
and malignant masses [11,25,26,29], especially for the detection of subtle
abnormalities [25]. more recently, deep learning models trained on both prior
and current mammograms have shown improved performance in breast cancer
classification tasks [24]. integrating prior mammograms into deep learning
models for breast cancer risk prediction can provide a more comprehensive
evaluation of a patient's breast health.in this paper, we introduce a deep
neural network that makes use of prior mammograms, to assess a patient's risk of
developing breast cancer, dubbed prime+ (prior mammogram enabled risk
prediction). we hypothesize that mammographic parenchymal pattern changes
between current and prior allow the model to better assess a patient's risk. our
method is based on a transformer model that uses attention [30], similar to how
radiologists would compare current and prior mammograms.the method is trained
and evaluated on a large and diverse dataset of over 9,000 patients and shown to
outperform a model based on state-of-the art risk prediction techniques for
mammography [33]. although previous models such as lrp-net and radifusion [5,34]
have utilized prior mammograms, prime+ sets itself apart by employing an
attention mechanism to extract information about the prior scan.",5
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,2.2,Architecture Overview,"we build on the current state-of-the art mirai [33] architecture, which is
trained to predict the cumulative hazard function. we use an imagenet pretrained
resnet-34 [12] as the image feature backbone. the backbone network extracts
features from the mammograms, and the fully connected layer produces the final
feature vector x. we make use of two additional fully connected layers to
calculate base hazard θ b and time-dependent hazard θ u , respectively. the
predicted cumulative hazard is obtained by adding the base hazard and
time-dependent hazard, according to:when dealing with right-censored data, we
use an indicator function δ i (t) to determine whether the information for
sample i at time t should be included in the loss calculation or not. this helps
us exclude unknown periods and only use the available information. it is defined
as follows:here, e i is a binary variable indicating whether the event of
interest occurs for sample i (i.e., e i = 1) or not (i.e., e i = 0), and c i is
the censoring time for sample i, which is the last known time when the sample
was cancer-free.we define the ground-truth h is a binary vector of length t max
, where t max is the maximum observation period. specifically, h(t) is 1 if the
patient is diagnosed with cancer within t years and 0 otherwise. we use binary
cross entropy to calculate the loss at time t for sample i:. the total loss is
defined as:here, n is the number of exams in the training set. the goal of
training the model is to minimize this loss function, which encourages the model
to make accurate predictions of the risk of developing breast cancer over time.",5
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,2.3,Incorporating Prior Mammograms,"to improve the performance of the breast cancer risk prediction model, we
incorporate information from prior mammograms taken with the same view, using a
transformer decoder structure [30]. this structure allows the current and prior
mammogram features to interact with each other, similar to how radiologists
check for changes between current and prior mammograms.during training, we
randomly select one prior mammogram, regardless of when they were taken. this
allows the model to generalize to varying time intervals. to pair each current
mammogram during inference with the most relevant prior mammogram, we first
select the prior mammogram taken at the time closest to the current time. this
approach is based on research showing that radiologists often use the closest
prior mammogram to aid in the detection of breast cancer [26].next, a shared
backbone network is used to output the current feature x curr and the prior
feature x prior . these features are then flattened and fed as input to the
transformer decoder, where multi-head attention is used to find information
related to the current feature in the prior feature. the resulting output is
concatenated and passed through a linear layer to produce the current-prior
comparison feature x cp c . the current-prior comparison feature and current
feature are concatenated to produce the final feature x * = x cp c ⊕ x curr ,
which is then used by the base hazard network and time-dependent hazard network
to predict the cumulative hazard function ĥ.",5
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.1,Dataset,"we compiled an in-house mammography dataset comprising 16,113 exams (64,452
images) from 9,113 patients across institutions from the united states, gathered
between 2010 and 2021. each mammogram includes at least one prior mammogram. the
dataset has 3,625 biopsy-proven cancer exams, 5,394 biopsyproven benign exams,
and 7,094 normal exams. mammograms were captured using hologic (72.3%) and
siemens (27.7%) devices. we partitioned the dataset by patient to create
training, validation, and test sets. the validation set contains 800 exams (198
cancer, 210 benign, 392 normal) from 400 patients, and the test set contains
1,200 exams (302 cancer, 290 benign, 608 normal) from 600 patients. all data was
de-identified according to the u.s hhs safe harbor method. therefore, the data
has no phi (protected health information) and irb (institutional review board)
approval is not required.",5
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,3.4,Results,"ablation study. to better understand the merit of the transformer decoder, we
first performed an ablation study on the architecture. our findings, summarized
in table 1, include two sets of results: one for all exams in the test set and
the other by excluding cancer exams within 180 days of cancer diagnosis which
are likely to have visible symptoms of cancer, by following a previous study
[33]. this latter set of results is particularly relevant as risk prediction
aims to predict unseen risks beyond visible cancer patterns. we also compare our
method to two other models, the state-of-the-art baseline and prime models.as
shown in the top rows in table 1, the baseline obtained a c-index of 0.68 (0.65
to 0.71). by using the transformer decoder to jointly model prior images, we
observed improved c-index from 0.70 (0.67 to 0.73) to 0.73 (0.70 to 0.76). the
c-index as well as all auc differences between the baseline and the prime+ are
all statistically significant (p < 0.05) except the 4-year auc where we had a
limited number of test cases.we observe similar performance improvements when
evaluating cases with at least 180 days to cancer diagnosis. interestingly, the
c-index as well as timedependent aucs of all three methods decreased compared to
when evaluating using all cases. the intuition behind this result is that
mammograms taken near the cancer diagnosis (<180 days) likely contain visible
signs of cancer and thus the task of risk prediction is easier. the model must
learn patterns of risk, not lastly, we empirically confirm that a transformer
decoder effectively models spatial relations between prior and current
mammograms by demonstrating consistent performance improvements of prime+ across
both short-term and longterm risk prediction settings. our results suggest that
incorporating changes in patients using prior mammograms and a transformer
decoder improves the performance of breast cancer risk prediction
models.analysis based on density. to better understand why adding prior images
improves performance, we divided our test set into subgroups to examine the
performance of the baseline model and the prime+ model on each of these groups.
mammographic breast density is one of the most important risk factor to predict
breast cancer [19,31]. women with dense breasts have a four-to six-fold higher
risk of breast cancer [2]. the addition of mammographic breast density has
improved the performance traditional breast cancer risk models [4] and can
therefore help us understand why the addition of prior images works.mammographic
breast density was determined using the breast imaging reporting and data system
(bi-rads) composition classification. bi-rads category a, b are defined as fatty
breasts and bi-rads category c, d are classified as dense breasts. to determine
the density category, we employed an internally developed density prediction
model, as most exams lack bi-rads ground truth. this model achieved an accuracy
of 0.81 on the internal density validation set.we categorized the exams into two
groups based on changes in density: ""change"" and ""no change"". density change was
defined according to whether the bi-rads category changed in the current image
as compared to the prior image. as shown in table 2, the baseline model performs
poorly for ""change"", with a c-index of 0.63 (0.49 to 0.77), especially for
long-term risk prediction, with 3-year auc of 0.56 (0.40 to 0.72). this suggests
that the baseline model has limitations in accurately predicting long-term risk
when there is a density change from the prior exam. however, prime+ is able to
predict long-term risk accurately even when a density change has occurred
(3-year auc = 0.74 (0.60 to 0.88)), by learning to refer previous exams
properly. this demonstrates the potential usefulness of incorporating past
mammogram information into breast cancer risk prediction models. thus, we
believe that incorporating prior exams is important to identify changes in
texture which are important for long term risk prediction (table 3). lastly, we
divided the exams based on the level of breast density, with a fatty group
consisting of density a and b, and a dense group consisting of density c and d.
both the baseline and prime+ performs better in fatty group than dense group. we
suspect this is because deep neural networks generally work better on low
density images given that visual cues of cancer in images with lower breast
density are more clearly visible.",5
Enhancing Breast Cancer Risk Prediction by Incorporating Prior Images,4.0,Conclusion,"in this paper, we introduce a novel breast cancer risk prediction method,
prime+, which incorporates prior mammograms with a transformer decoder to
capture changes in breast tissue over time. by doing so, we achieve high
performance for both short-term and long-term risk prediction. our extensive
experiments on a dataset of 16,113 exams show that prime+ outperformed a model
based on the state-of-the-art for breast cancer risk prediction [33]. our method
performed particularly well in cases where there was a change in breast density
from the previous exam. we believe that our method has the potential to improve
breast cancer risk prediction and ultimately contribute to earlier detection of
the disease.",5
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,1.0,Introduction,"lung cancer screening has a significant impact on the rate of mortality
associated with lung cancer. studies have proven that regular lung cancer
screening with low-dose computed tomography (ldct) can lessen the rate of lung
cancer mortality by up to 20% [1,14]. as most (e.g., 95% [13]) of the detected
nodules are benign, it is critical to accurately assess their malignancy on ct
to achieve a timely diagnosis of malignant nodules and avoid unnecessary
procedures such as biopsy for benign ones. particularly, the evaluation of
nodule (i.e., 8-30mm) malignancy is recommended in the guidelines [13].fig. 1.
in pare, a nodule is diagnosed from two levels: first parsing the contextual
information contained in the nodule itself, and then recalling the previously
learned nodules to look for related clues.one of the major challenges of lung
nodule malignancy prediction is the quality of datasets [6]. it is characterized
by a lack of standard-oftruth of labels for malignancy [16,27], and due to this
limitation, many studies use radiologists' subjective judgment on ct as labels,
such as lidc-idri [3]. recent works have focused on collecting pathologically
labeled data to develop reliable malignancy prediction models [16,17,19]. for
example, shao et al. [16] collated a pathological gold standard dataset of 990
ct scans. another issue is most of the studies focus on ldct for malignancy
prediction [10]. however, the majority of lung nodules are incidentally detected
by routine imaging other than ldct [4,15], such as noncontrast chest ct (ncct,
the most frequently performed ct exam, nearly 40% [18]).technically, current
studies on lung nodule malignancy prediction mainly focus on deep learning-based
techniques [10,12,17,23,24]. liao et al. [10] trained a 3d region proposal
network to detect suspicious nodules and then selected the top five to predict
the probability of lung cancer for the whole ct scan, instead of each nodule. to
achieve the nodule-level prediction, xie et al. [24] introduced a
knowledge-based collaborative model that hierarchically ensembles multi-view
predictions at the decision level for each nodule. liu et al. [12] extracted
both nodules' and contextual features and fused them for malignancy prediction.
shi et al. [17] effectively improved the malignancy prediction accuracy by using
a transfer learning and semi-supervised strategy. despite their advantages in
representation learning, these methods do not take into account expert
diagnostic knowledge and experience, which may lead to a bad consequence of poor
generalization. we believe a robust algorithm should be closely related to the
diagnosis experience of professionals, working like a radiologist rather than a
black box.in this paper, we suggest mimicking radiologists' diagnostic
procedures from intra-context parsing and inter-nodule recalling (see
illustrations in fig. 1),",5
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,3.1,Datasets and Implementation Details,"data collection and curation: nlst is the first large-scale ldct dataset for
low-dose ct lung cancer screening purpose [14]. there are 8,271 patients
enrolled in this study. an experienced radiologist chose the last ct scan of
each for l = 1, ..., l do 12:cross prototype attention 13:end for 15:",5
Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,0.0,17:,"j ← seg loss(m, s) + 3 i=1 cls loss(y, p i ) update loss18: end for patient, and
localized and labeled the nodules in the scan as benign or malignant based on
the rough candidate nodule location and whether the patient develops lung cancer
provided by nlst metadata. the nodules with a diameter smaller than 4mm were
excluded. the in-house cohort was retrospectively collected from 2,565 patients
at our collaborating hospital between 2019 and 2022. unlike nlst, this dataset
is noncontrast chest ct, which is used for routine clinical care. segmentation
annotation: we provide the segmentation mask for our in-house data, but not for
the nlst data considering its high cost of pixel-level labeling. the nodule mask
of each in-house data was manually annotated with the assistance of ct labeler
[20] by our radiologists, while other contextual masks such as lung, vessel, and
trachea were generated using the totalsegmentator [21].",5
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,1.0,Introduction,"screening mammography helps detect breast cancer earlier and has reduced the
breast cancer mortality rate significantly [4]. computer-aided diagnosis (cad)
software was developed to aid radiologists, but its effectiveness has been
questioned following recent large-scale clinical studies [6]. in particular, the
high [9]) to few negative cases (ddsm [8], inbreast [17]). to illustrate the
distribution shift, we train four popular dense detectors using a standard setup
that includes only annotated malignant and benign cases [1,13,16]. we utilize
optimam [7], a large dataset with a significant proportion of negatives (table
1), for training and evaluation. across all dense models, there is a large
performance drop in the clinically representative setting that includes negative
images. this means that the dense models are producing too many fps on negative
images. our model, m&m, successfully tackles this performance gap.rate of false
positive (fp) predictions of cad can cause a significant reduction in
radiologists' specificity [6]. surprisingly, recent deep learning literature
[3,5,13,16,20,21,32] focuses on improving recall without considering the need to
operate at low fp rates. as shown in fig. 1a, most works focus on reporting
recalls outside the clinically relevant region of less than 1 fp/image. to
tackle the high rate of false positives in mammography, we identify three
challenges: (1) a malignant mammogram typically contains only one malignant
finding. this is different from natural images: for example, an image in coco
contains on average 7.7 objects [11]. this calls into question the usage of
dense detectors for mammography; (2) a standard screening exam consists of two
views per breast. both views are essential in making a clinical decision because
a finding may appear suspicious in one view but not the other; (3) most
mammograms are negative: they do not contain any findings. however, excluding
negative images from training and evaluation leads to a distribution shift since
negative images are abundant in clinical practice. concretely, the false
positive rate is low for a typical evaluation data distribution but much higher
for a clinicallyrepresentative data distribution, as shown in fig. 1b.in this
work, we tackle these challenges and propose a multi-view and multiinstance
learning system, m&m. m&m is an end-to-end system that detects malignant
findings and provides breast-level classification. to achieve these goals, m&m
leverages three components: (1) sparse r-cnn to replace dense anchors with a set
of sparse proposals; (2) multi-view cross-attention to synthesize information
from two views and iteratively refine the predictions, and (3) multiinstance
learning (mil) to include negative images during training. ultimately, each
component contributes to our goal of reducing false positives.we validate m&m
through evaluation on five datasets: two in-house datasets, two public datasets
-ddsm [8] and cbis-ddsm [9], and optimam [7]. we perform ablation studies to
verify the contribution of each component of m&m. to summarize, our
contributions are:1. we show that sparsity of proposals is beneficial to the
analysis of mammograms, which have low disease prevalence (sec. with mil, m&m
improves the recall at 0.1 fp/image by 12.6% (fig. 4). furthermore, m&m can
provide breast-level classification predictions, achieving aucs of more than
0.88 on four different datasets (table 3).",5
M&M: Tackling False Positives in Mammography with a Multi-view and Multi-instance Learning Sparse Detector,3.0,Experiments,"implementation details. we use pytorch 1.10. the training settings follow sparse
r-cnn [24]. we apply random horizontal flipping and random rotation. we resize
the images' shorter edges to 2560 with the larger edges no longer than 3328. we
utilize a coco-pretrained pvt-b2-li backbone [30]. we use adamw optimizer with 5
× 10 -5 learning rate and 0.0001 weight decay. the model is trained for 9000
iterations, and the learning rate is scaled by 0.1 at the 6750 and 8250
iterations. each batch contains 16 breasts (32 images). we employ a 1:1 sampling
ratio between unannotated and annotated images.datasets. we utilize three 2d
digital mammography datasets: (1) optimam : a development dataset derived from
the optimam database [7], which is funded by cancer research uk. we split the
data into train/val/test with an 80:10:10 ratio at the patient level; (2)
inhouse-a: an evaluation dataset collected from a u.s. multi-site mammography
operator; (3) inhouse-b : an evaluation dataset collected from a u.s. academic
hospital (see [18], sec. 2.2 for more details on the inhouse datasets). we also
utilize two film mammography datasets: (4) ddsm: a dataset maintained at the
university of south florida [8]. we followed the methods by [3,5,13,16] to split
the test set; (5) cbis-ddsm: a curated subset of ddsm [9]. we only include
breasts that have one cc view and one mlo view. dataset statistics are reported
in table 1.metrics. we report average precision with intersection over union
from 0.25 to 0.75. ap mb denotes average precision on the set of annotated
malignant and benign images. ap denotes average precision when all data is
included. we report free response operating characteristic (froc) curves and
recalls at various fp/image (r@t). following [3,5,16,29], a proposal is
considered true positive if its center lies within the ground truth box. for
classification, we report the area under the receiver operating characteristic
curve (auc).detection results. gmic [23] 0.911 0.896 0.814 0.815 0.796 hct [25]
0.923 0.912 0.816 0.817 0.793 m&m (ours) 0.960 0.942 0.920 0.910 0.898resnet50
[14] 0.724 shared resnet [31] 0.735 phresnet50 [14] 0.739 cross-view transformer
[27] 0.803 * m&m (ours) 0.88323 points (pt) between excluding and including
negative images. large δ means the models are producing too many fps on negative
images. sparse r-cnn [24] generalizes significantly better with a gap of 17pt.
this shows the importance of sparsity for reducing fp. by adding both multi-view
and mil, m&m successfully reduces the δ gap to 3.5pt. with this performance gap
closed, m&m is able to achieve a high recall of 87.7% at just 0.1 fp/image.
figure 1a compares m&m with recent literature evaluated on ddsm. m&m adopts the
same ddsm splits used by [3,12,13,16,33], while [5,21,32] use other splits. m&m
(87% r@0.5) outperforms all recent sota with the same test split, including 2022
sota [33] (83% r@0.5), by at least 4%.classification results. table 3a reports
m&m's breast-level and exam-level classification results on optimam and the two
inhouse datasets. we use gmic [23] and hct [25] as baselines since they are
open-sourced classifiers developed for mammography. all three models were
trained only on optimam. for all models, the breast-level score is the average
of the cc score and mlo score, while the exam-level score is the max of the left
breast score and right breast score. both baseline models suffer large
generalization drops of approximately 3b compares m&m with recent literature
reporting on the public cbis-ddsm dataset. in particular, m&m outperforms the
cross-view transformer [27] and phresnet50 [14] by 0.08 and 0.14 breast auc,
respectively. qualitative evaluation. figure 3 presents a qualitative evaluation
of the multi-view module. with multi-view, m&m produces a tighter box on the cc
view and recovers a missed finding on the mlo view. ablation studies. figure 4
presents ablation results using the optimam validation split. on the left, we
demonstrate how each component of m&m contributes to closing the gap δ between
evaluating with and without negative images. notably, without using any extra
training samples, multi-view reasoning reduces δ to only -5.9pt (row 3). mil
allows the model to train with significantly more negative images, reducing δ to
-3.6pt (row 4). on the right of fig. 4, the froc curves show how each component
of m&m improves recall significantly at low fp/image. in particular, m&m's
recall at 0.1fp/image is 86.3%, +21.2% over vanilla sparse r-cnn.further
studies. in the appendix, we present more qualitative evaluation as well as
further ablation studies on (1) number of learnable proposals, (2) different mil
schemes, (3) backbone choices and (4) positional encoding.",5
Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection,1.0,Introduction,"the early detection of lesions in medical images is critical for the diagnosis
and treatment of various conditions, including neurological disorders. stroke is
a leading cause of death and disability, where early detection and treatment can
significantly improve patient outcomes. however, the quantification of lesion
burden is challenging and can be time-consuming and subjective when performed
manually by medical professionals [14]. while supervised learning methods
[10,11] have proven to be effective in lesion segmentation, they rely heavily on
large fig. 1. overview of phanes (see fig. 2). our method can use both expert
annotatedor unsupervised generated masks to reverse and segment anomalies
annotated datasets for training and tend to generalize poorly beyond the learned
labels [21]. on the other hand, unsupervised methods focus on detecting patterns
that significantly deviate from the norm by training only on normal data.one
widely used category of unsupervised methods is latent restoration methods. they
involve autoencoders (aes) that learn low-dimensional representations of data
and detect anomalies through inaccurate reconstructions of abnormal samples
[17]. however, developing compact and comprehensive representations of the
healthy distribution is challenging [1], as recent studies suggest aes perform
better reconstructions on out-of-distribution (ood) samples than on training
samples [23]. various techniques have been introduced to enhance representation
learning, including discretizing the latent space [15], disentangling
compounding factors [2], and variational autoencoders (vaes) that introduce a
prior into the latent distribution [26,29]. however, methods that can enforce
the reconstruction of healthy generally tend to produce blurry
reconstructions.in contrast, generative adversarial networks (gans) [8,18,24]
are capable of producing high-resolution images. new adversarial aes combine
vaes' latent representations with gans' generative abilities, achieving sota
results in image generation and outlier detection [1,5,6,19]. nevertheless,
latent methods still face difficulties in accurately reconstructing data from
their low-dimensional representations, causing false positive detections on
healthy tissues.several techniques have been proposed that make use of the
inherent spatial information in the data rather than relying on constrained
latent representations [12,25,30]. these methods are often trained on a pretext
task, such as recovering masked input content [30]. de-noising aes [12] are
trained to eliminate synthetic noise patterns, utilizing skip connections to
preserve the spatial information and achieve sota brain tumor segmentation.
however, they heavily rely on a learned noise model and may miss anomalies that
deviate from the noise distribution [1]. more recently, diffusion models [9]
apply a more complex de-noising process to detect anomalies [25]. however, the
choice and granularity of the applied noise is crucial for breaking the
structure of anomalies [25]. adapting the noise distribution to the diversity
and heterogeneity of pathology is inherently difficult, and even if achieved,
the noising process disrupts the structure of both healthy and anomalous regions
throughout the entire image.in related computer vision areas, such as industrial
inspection [3], the topperforming methods do not focus on reversing anomalies,
but rather on detecting them by using large nominal banks [7,20], or pre-trained
features from large natural imaging datasets like imagenet [4,22]. salehi et al.
[22] have employed multi-scale knowledge distillation to detect anomalies in
industrial and medical imaging. however, the application of these networks in
medical anomaly segmentation, particularly in brain mri, is limited by various
challenges specific to the medical imaging domain. they include the variability
and complexity of normal data, subtlety of anomalies, limited size of datasets,
and domain shifts.this work aims to combine the advantages of constrained latent
restoration for understanding healthy data distribution with generative
in-painting networks. unlike previous methods, our approach does not rely on a
learned noise model, but instead creates masks of probable anomalies using
latent restoration. these guide generative in-painting networks to reverse
anomalies, i.e., preserve healthy tissues and produce pseudo-healthy in-painting
in anomalous regions. we believe that our proposed method will open new avenues
for interpretable, fast, and accurate anomaly segmentation and support various
clinical-oriented downstream tasks, such as investigating progression of
disease, patient stratification and treatment planning. in summary our main
contributions are:• we investigate and measure the ability of sota methods to
reverse synthetic anomalies on real brain t1w mri data. • we propose a novel
unsupervised segmentation framework, that we call phanes, that is able to
preserve healthy regions and utilize them to generate pseudo-healthy
reconstructions on anomalous regions. • we demonstrate a significant advancement
in the challenging task of unsupervised ischemic stroke lesion segmentation.",5
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,1.0,Introduction,"breast cancer is one of the high-mortality cancers among women in the 21st
century. every year, 1.2 million women around the world suffer from breast
cancer and about 0.5 million die of it [3]. accurate identification of cancer
types will make a correct assessment of the patient's risk and improve the
chances of survival. however, the traditional analysis method is time-consuming,
as it mainly depends on the experience and skills of the doctors. therefore, it
is essential to develop computer-aided diagnosis (cadx) for assisting doctors to
realize the rapid detection and classification.due to being collected by various
devices, the resolution of histopathological images extracted may not always be
high. low-resolution (lr) images lack of lots of details, which will have an
important impact on doctors' diagnosis. considering the improvement of
histopathological images' acquisition equipment will cost lots of money while
significantly increasing patients' expense of detection. the super-resolution
(sr) algorithms that improve the resolution of lr images at a small cost can be
a practical solution to assist doctors in diagnosis. at present, most single
super-resolution methods only have fixed receptive fields [7,10,11,18]. these
models cannot capture multi-scale features and do not solve the problems caused
by lr in various magnification factors well. mrc-net [6] adopted lstm [9] and
multi-scale refined context to improve the effect of reconstructing
histopathological images. it considered the problem of multi-scale, but only
fused two scales features. this limits its performance in the scenarios with
various magnification factors. therefore, designing an appropriate feature
extraction block for sr of the histopathological images is still a challenging
task.in recent years, a series of deep learning methods have been proposed to
solve the breast cancer histopathological image classification issue by the
highresolution (hr) histopathological images. [12,21,22] improved the specific
model structure to classify breast histopathology images, which showed a
significant improvement in recognition accuracy compared with the previous works
[1,20]. ssca [24] considered the problem of multi-scale feature extraction which
utilized feature pyramid network (fpn) [15] and attention mechanism to extract
discriminative features from complex backgrounds. however, it only concatenates
multi-scale features and does not consider the problem of feature fusion. so it
is still worth to explore the potential of extraction and fusion of multi-scale
features for breast images classification.to tackle the problem of lr breast
cancer histopathological images reconstruction and diagnosis, we propose the
single histopathological image super-resolution classification network
(shisrcnet) integrating super-resolution (sr) and classification (cf) modules.
the main contributions of this paper can be described as follows:(1) in the sr
module, we design a new block called multi-features extraction block (mfeblock)
as the backbone. mfeblock adopts multi-scale receptive fields to obtain
multi-scale features. in order to better fuse multi-scale features, a new fusion
method named multi-scale selective fusion (msf) is used for multi-scale
features. these make mfeblock reconstruct lr images into sr images well.(2) the
cf module completes the task of image classification by utilizing the sr images.
like sr module, it also needs to extract multi-scale features. the difference is
that the cf module can use the method of downsampling to capture multi-scale
features. so we combine the multi-scale receptive fields (sknet) [13] with the
feature pyramid network (fpn) to achieve the feature extraction of this module.
in fpn, we design a cross-scale selective fusion block (csfblock) to fuse
features of different scales.(3) through the joint training of these two
designed modules, the superresolution and classification of low-resolution
histopathological images are integrated into our model. for improving the
performance of cf module and reducing the error caused by the reconstructed sr
images, we introduce hr images to cf module in the training stage. the
experimental results demonstrate that the effects of our method are close to
those of sota methods that take hr breast cancer histopathological images as
inputs.",5
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,3.0,Experiment,"dataset: this work uses the breast cancer histopathological image database
(breakhis)1 [20]. the images in the dataset have four magnification factors the
model is trained using the adam optimizer [25] with the learning rate set to
1x10 -3 . the learning rate is multiplied by 0.9 for every two epochs. we use
sknet-26 [13] as the backbone network in the cf module. the total training
epochs are 100.",5
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,4.1,The Results of Super-Resolution and Classification,"table 1 shows the results of the super-resolution phase. we adopt peak signal to
noise ratio (psnr) and structural similarity index (ssim) [6] to evaluate the
performance of the sr model. mrc-net and our proposed srmfenet (sr module)
achieves better metrics than the other algorithms. this proves the effectiveness
of multi-scale features extraction. compared with mrc-net, our mfeblocks can
extract and fuse multi-scale features well. and the joint training of srmfenet
and cf module improves the performance of super-resolution. figure 2
demonstrates that our model can recover more details with less blurring. we
compare our introduced cf module with five state-of-the-art breast cancer
histopathological image models and diagnosis network with mrc-net [6], as shown
in table 2. the results illustrate that the cf module reaches the best",5
SHISRCNet: Super-Resolution and Classification Network for Low-Resolution Breast Cancer Histopathology Image,0.0,Table 1 .,"fig. 2. qualitative comparison with sr methods on breast cancer
histopathological images x8 and x4.",5
Text-Guided Foundation Model Adaptation for Pathological Image Classification,2.0,Related Work,"medical image classification. deep learning for medical image classification has
long relied on training large models from scratch [1,15]. also, fine-tuning or
linear-probing the pre-trained models obtained from natural images [16][17][18]
is reasonable. however, those methods are supported by sufficient high-quality
data expensive to collect and curate [19]. in addition, task-specific models do
not generalize well with different image modalities [2]. to tackle this issue,
we emphasize the adaptation of foundation models in a data-efficient
manner.vision-language pre-training. recent work has made efforts in
pre-training vision-language models. clip [5] collects 400 million image-text
pairs from the internet and trains aligned vision and text encoders from
scratch. lit [20] trains a text encoder aligned with a fixed pre-trained vision
encoder. blip-2 [14] trains a query transformer by bootstrapping from
pre-trained encoders. react [21] fixes both pre-trained encoders and tunes extra
gated self-attention modules. however, those methods establish vision-language
alignment by pre-training on large-scale image-text pairs. instead, we combine
pre-trained unimodal models on downstream tasks and build a multi-modal
classifier with only a few data.model adaptation via prompt tuning. prompt
tuning proves to be an efficient adaptation method for both vision and language
models [22,23]. originating from natural language processing, ""prompting"" refers
to adding (manual) text instructions to model inputs, whose goal is to help the
pre-trained model better understand the current task. for instance, coop [22]
introduces learnable prompt parameters to the text branch of vision-language
models. vpt [23] demonstrates the effectiveness of prompt tuning with
pre-trained vision encoders. in this study, we adopt prompt tuning for
adaptation because it is lightweight and only modifies the input while keeping
the whole pre-trained model unchanged. however, existing prompt tuning methods
lack expert knowledge and understanding of downstream medical tasks. to address
this challenge, we leverage large language models pre-trained with biomedical
text to inject medical domain knowledge.biomedical language model utilization.
biomedical text mining promises to offer the necessary knowledge base in
medicine [9][10][11]. leveraging language models pre-trained with biomedical
text for medical language tasks is a common application. for instance, alsentzer
et al. [9] pre-train a clinical text model with biobert [10] initialization and
show a significant improvement on five clinical language tasks. however, the
potential of biomedical text information in medical imaging applications has not
been explicitly addressed. in our efforts, we emphasize the importance of
utilizing biomedical language models for adapting foundational vision models
into cancer pathological analysis.",5
Distributionally Robust Image Classifiers for Stroke Diagnosis in Accelerated MRI,3.2,Results,"we show the stroke classification auroc in fig. 3. when the k-space subsampling
fraction decreased and the signal became sparser, the performance of both vit
and cnn models trained under erm dropped significantly, from around 95% to below
80%. drl significantly improved the auroc of the ermbased vit model from 74.5%
to 83.1% when the mr images were under extreme cu perturbation, while only
slightly influenced model performance on the clean test set. for wgn, the
largest improvement brought to erm-based vit model was 16.9%. although we only
applied drl to the last layer of the cnn model, the improvement against erm was
still remarkable, up to 11.9%/4.9% for cu/wgn. with bat and pgd adversarial
training, the corresponding vit or cnn models were also improved, though when
drl was combined with bat and pgd, the model robustness can be further enhanced.
table 1 shows the maximum auroc and auprc improvement that drl can bring to
different baseline methods. for vit and cnn models, the auroc improvement w.r.t
bat/pgd defensive methods is up to 23.9%/12.2%, respectively. note that the
perturbed mri samples used to implement bat were the same as those used by drl,
which shows that drl is a more effective way to exploit the information in
adversarial samples, compared to simply adding the blurry images into the
training set. for cu perturbation, our best combined model using drl improved
the auroc/auprc of the erm model by up to 15%/12.5%, while this improvement
under wgn perturbation was up to 18.8%/36.2%. under cu perturbation, we further
show that our drl model can recognize stroke while clinicians may fail to. in
fig. 4, the stroke mri slices from the test sets are under different levels of
cu perturbations. for both erm-and drlbased vit models, we maximized the f1
score of the stroke class on the training set to calculate the optimal decision
threshold for stroke prediction, in order to balance the precision and recall.
when the k-space signal becomes more sparse, the reconstructed mri slices get
more blurry and the lesion areas become less recognizable, even for human eyes.
as a result, the erm model fails to detect stroke under high perturbation
levels. nevertheless, the drl model can tolerate more intense cu perturbation
and recognize stroke slices that may even be misclassified by clinicians, which
reveals its value in improving the diagnosis in an accelerated mri mode. we
verified the effectiveness of our approach on the actual clinical scans acquired
for clinical care and not just for research purposes, suggesting that the
methods and findings in the current study should be generalizable to routine
clinical practice conditions and potentially other types of clinical image-based
diagnosis (e.g., brain tumor) as well. in addition, our drl framework does not
necessarily need to be used in isolation, rather it can also be combined with
other performance boosting methods in accelerated mri to further improve them,
just like for bat and pgd.",5
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,1.0,Introduction,"computer-aided diagnosis utilizes machine learning techniques to conduct a
pathological diagnosis concerning biomedical imaging data collected from various
pathological modalities, such as computed tomography [19], magnetic resonance
imaging [11], ultrasound [23], and angiography [9]. with the assistance of cad
techniques, the clinicians merely need to check the possible pathological
regions narrowed down by computer-aided diagnosis method, significantly reducing
the entire diagnosis time. with the recent success of deep learning, researchers
are able to raise the reliability of cad methods and assist clinicians in
diagnosing more complex clinical tasks. however, a reliable machine
learning-based cad method usually relies on the supervision of abundant
annotated training data. yet diseased pathological data are rare and diverse,
and acquiring reliable pathological annotations are labor-intensive and
expertise-required. as a result, the difficulty of data collection restricts the
development of the supervised cad.due to the difficulty of acquiring the
abundant annotated training data, the current sota method, i.e., csm [14],
proposes a mil-based wvad manner to specifically tackle one specific disease
detection task, i.e., colorectal cancer diagnosis via colonoscopy. considering
the case of colonoscopy, the csm's anomaly detection setting is used to handle
the rare and diverse diseased pathological data by commonly assuming that only
video-level annotations are available for training. furthermore, its video
setting concerns the temporal correlation within data. the setting of such
mil-based weakly supervision prevents the need for abundant annotated training
data by assuming that merely the video-level annotations, including normal and
diseased ones, are available for training.similar to the previous mil-based wvad
methods [4,13,14], our model assumes all training snippets (consecutive video
frames) within a non-diseased video are all normal snippets, yet each diseased
video has at least one abnormal snippet. furthermore, the proposed contrastive
feature decoupling network treats disease detection as an out-of-distribution
task. precisely, our cfd learns a memory bank to learn normal features. a
snippet that failed to be well reconstructed with these normal features is
considered diseased. on the other hand, the residual of a snippet and its
reconstructed one reveals the snippet's abnormal ingredients. consequently, we
are able to decouple each snippet as normal ingredients (reconstructed parts)
and abnormal ingredients (residual parts) by leveraging the memory bank. with
the decoupled snippet-level feature ingredients, our cfd employs both the normal
and abnormal feature ingredients via a contrastive learning paradigm to
concurrently optimize video-level and snippetlevel disease scores for pursuing
more accurate detection.to assess the proposed contrastive feature decoupling
network, we conduct experiments on two datasets, i.e., polyp and panda-mil. the
main contributions are summarized as follows.-our contrastive feature decoupling
network learns a memory bank to learn normal atoms for decoupling each snippet
as normal and diseased feature ingredients as opposite contrastive learning
samples. such a feature decoupling intrinsically fits the contrastive learning
paradigm for optimizing mil objectives on bags and instances. 2 related work",5
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,2.1,Disease Detection,"with the evolution of artificial intelligence techniques in the past decades,
deep learning has shown its potential for computer-aided diagnosis of various
symptoms [6,8,14,17,22]. for example, li et al. [8] established a large-scale
attentionbased database and designed a specialized model using retinal fundus
images for detecting glaucoma. windsor et al. [17] constructed a
transformer-based model to detect spinal cancer for mri scans. more recently,
tian et al. [14] formulated polyp detection in a wvad scheme while tackling
polyp detection using colonoscopy videos to search colon polyps in the temporal
sequence. unlike previous methods of handling one specific pathological
modality, we simultaneously address disease detection across pathological
modalities of colonoscopy videos and prostate tissue biopsies using our
contrastive feature decoupling network.",5
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,0.0,PANDA-MIL.,"the prostate cancer grade assessment (panda) challenge [2] comprises over 10k
whole-slide images (wsis) of digitized hematoxylin and eosin-stained biopsies
originating from radboud university medical center and karolinska institute.
panda-mil collects the eosin-stained biopsies with region-based masks indicating
the benign (normal) and cancerous (abnormal) tissue, combined by stroma and
epithelium. to fit the mil-based wvad task, we non-overlapped partition each wsi
(bag) into patches (instances) and only keep those patches comprising tissue
over the 50% patch size. each kept patch gets its patch-level annotations from
panda, and a wsi comprising any abnormal patch is treated as an abnormal wsi. in
sum, panda-mil's training split contains 3,925 bags of bag-level annotations,
and the testing split includes 975 bags of instance-level annotations.metric. we
follow the previous methods [4,13,14] to employ the instant-level area under
curve (auc) and the average precision (ap) for a fair comparison. the larger
values of both metrics mean better disease detection performance.",5
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,4.3,Comparison Results,"table 1 shows the compared results of our cfd model against recent wvad methods
[4, 13,14,18] for tackling the disease detection task. the results in table 1
demonstrate that our cfd consistently outperforms all the other methods on two
datasets. precisely, our model achieves the new sota by 1.1% auc and 1.5% ap
improvements on the polyp dataset and 1.09% auc and 2.45% ap improvements on the
panda-mil dataset. please refer to the supplementary material for the completed
results, including more wvad methods [12,16,20,24]. figure 2 visualizes one
disease detection result of our cfd model on the panda-mil dataset. the disease
score per instance/patch predicted by our method is close to the ground-truth
annotations, in which the clear margin between cancerous and benign validates
the robust prediction of the proposed cfd model.",5
Contrastive Feature Decoupling for Weakly-Supervised Disease Detection,5.0,Conclusion,"this paper casts disease detection as a mil-based wvad task and introduces
contrastive feature decoupling (cfd) network to learn a memory bank boosted with
contrastive learning. with the learned feature atoms stored in the memory bank,
our contrastive feature decoupling is able to decouple each snippet as normal
and abnormal proxies. further, the decoupled abnormal proxies highlight the
abnormal feature ingredients for better reasoning the disease score. our feature
decoupling intrinsically fits the contrastive learning paradigm to define
opposite training samples for model optimization. besides, we introduce a new
dataset of prostate cancer detection, i.e., panda-mil, to provide a biomedical
imaging dataset concerning a different pathological modality. experiments
demonstrate that our cfd network achieves new sota performance on the polyp and
panda-mil datasets, indicating that our method effectively addresses the disease
detection task across different pathological modalities.",5
Multiple Prompt Fusion for Zero-Shot Lesion Detection Using Vision-Language Models,1.0,Introduction,"medical lesion detection plays an important role in assisting doctors with the
interpretation of medical images for disease diagnosing, cancer staging, etc.,
which can improve efficiency and reduce human errors [9,19]. current object
detection approaches are mainly based on supervised learning with abundant
well-paired image-level annotations, which heavily rely on expert-level
knowledge. as such, these supervised approaches may not be suitable for medical
lesion detection due to the laborious labeling.recently, large-scale pre-trained
vision-language models (vlms), by learning the visual concepts in the images
through the weak labels from text, have prevailed in natural object detection or
visual grounding and shown extraordinary performance. these models, such as glip
[11], x-vlm [10], and vinvl [24], can perform well in detection tasks without
supervised annotations. therefore, substituting conventional object detection
with vlms is possible and necessary. the vlms are first pre-trained to learn
universal representations via large-scale unlabelled data and can be effectively
transferred to downstream tasks. for example, a recent study [15] has
demonstrated that the pre-trained vlms can be used for zero-shot medical lesion
detection with the help of well-designed prompts.however, current existing vlms
are mostly based on a single prompt to establish textual and visual alignment.
this prompt needs refining to cover all the features of the target as much as
possible. apparently, even a well-designed prompt is not always able to combine
all expressive attributes into one sentence without semantic and syntactic
ambiguity, e.g., the prompt design for melanoma detection should include
numerous kinds of information describing attributes complementing each other,
such as shape, color, size, etc [8]. in addition, each keyword in a single
lengthy prompt cannot take effect equally as we expect, where the essential
information can be ignored. this problem motivates us to study alternative
approaches with multiple prompt fusion.in this work, instead of striving to
design a single satisfying prompt, we aim to take advantage of pre-trained vlms
in a more flexible way with the form of multiple prompts, where each prompt can
elicit respective knowledge from the model which can then be fused for better
lesion detection performance. to achieve this, we propose an ensemble guided
fusion approach derived from clustering ensemble learning [3], where we design a
step-wise clustering mechanism to gradually screen out the implausible
intermediate candidates during the grounding process, and an integration module
to obtain the final results by uniting the mutually independent candidates from
each prompt. in addition, we also examine the language syntax based prompt
fusion approach as a comparison, and explore several fusion strategies by first
grouping the prompts either with described attributes or categories and then
repeating the fusion process.we evaluate the proposed approach on a broad range
of public medical datasets across different modalities including photography
images for skin lesion detection isic 2016 [2], endoscopy images for polyp
detection cvc-300 [21], and cytology images for blood cell detection bccd. the
proposed approach exhibits extraordinary superiority compared to those with
single prompt and other common ensemble learning based methods for zero-shot
medical lesion detection. considering the practical need of lesion detection, we
further provide significantly improved fine-tuning results with a few labeled
examples.",5
Fast Non-Markovian Diffusion Model for Weakly Supervised Anomaly Detection in Brain MR Images,3.1,Dataset and Evaluation Metric,"brats 2020 [2] is a brain tumor segmentation dataset containing the mr sequences
of t1, t1gd, t2, and flair. following the preprocessing approach of [27], we
concatenate all image modalities along the channel dimension, prune the upper
and lower axial slices, and pad each slice into 256 × 256. all tumor classes are
merged into a single class in the segmentation mask. the training set contains
10,410 slices with tumors and 5,809 healthy slices. the testing set includes
1,316 images with tumors. isles 2022 [19] is an mr image dataset for stroke
lesions segmentation. it contains ischemic strokes of various sizes and from
different disease stages. we extract the axial slices from dwi sequence and
resize them into 256 × 256. the training set includes 2,707 healthy slices and
1,483 slices with lesions. the testing set contains 282 slices with lesions.
note that only the image-level binary labels are used in our training. the
evaluation metrics include dice, volumetric similarity, and hausdorff distance,
which are calculated in slice level and volume level for brats and isles,
respectively.",5
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,1.0,Introduction,"renal cancer is the most lethal malignant tumor of the urinary system, and the
incidence is steadily rising [13]. conventional b-mode ultrasound (us) is a good
screening tool but can be limited in its ability to characterize complicated
renal lesions. contrast-enhanced ultrasound (ceus) can provide information on
microcirculatory perfusion. compared with ct and mri, ceus is radiation-free,
cost-effective, and safe in patients with renal dysfunction. due to these
benefits, ceus is becoming increasingly popular in diagnosing renal lesions.
however, recognizing important diagnostic features from ceus videos to diagnose
lesions as benign or malignant is non-trivial and requires lots of experience.to
improve diagnostic efficiency and accuracy, many computational methods were
proposed to analyze renal us images and could assist radiologists in making
clinical decisions [6]. however, most of these methods only focused on
conventional b-mode images. in recent years, there has been increasing interest
in multi-modal medical image fusion [1]. directly concatenation and addition
were the most common methods, such as [3,4,12]. these simple operations might
not highlight essential information from different modalities. weight-based
fusion methods generally used an importance prediction module to learn the
weight of each modality and then performed sum, replacement, or exchange based
on the weights [7,16,17,19]. although effective, these methods did not allow
direct interaction between multi-modal information. to address this,
attention-based methods were proposed. they utilized cross-attention to
establish the feature correlation of different modalities and self-attention to
focus on global feature modeling [9,18]. nevertheless, we prove in our
experiments that these attentionbased methods may have the potential risks of
entangling features of different modalities.in practice, experienced
radiologists usually utilize dynamic information on tumors' blood supply in ceus
videos to make diagnoses [8]. previous researches have proved that temporal
information is effective in improving the performance of deep learning models.
lin et al. [11] proposed a network for breast lesion detection in us videos by
aggregating temporal features, which outperformed other image-based methods.
chen et al. [2] showed that ceus videos can provide more detailed blood supply
information of tumors allowing a more accurate breast lesion diagnosis than
static us images.in this work, we propose a novel multi-modal us video fusion
network (muvf-yolox) based on ceus videos for renal tumor diagnosis. our main
contributions are fourfold. (1) to the best of our knowledge, this is the first
deep learning-based multi-modal framework that integrates both b-mode and
ceusmode information for renal tumor diagnosis using us videos. (2) we propose
an attention-based multi-modal fusion (amf) module consisting of cross-attention
and self-attention blocks to capture modality-invariant and modality-specific
features in parallel. (3) we design an object-level temporal aggregation (ota)
module to make video-based diagnostic decisions based on the information from
multi-frames. (4) we build the first multi-modal us video datatset containing
b-mode and ceus-mode videos for renal tumor diagnosis. experimental results show
that the proposed framework outperforms single-modal, single-frame, and other
state-of-the-art methods in renal tumor diagnosis.",5
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,2.1,Overview of Framework,"the proposed muvf-yolox framework is shown in fig. 1. it can be divided into two
stages: single-frame detection stage and video-based diagnosis stage. (1) in the
single-frame detection stage, the network predicts the tumor bounding box and
category on each frame in the multi-modal ceus video clips. dual-branch backbone
is adopted to extract the features from two modalities and followed by the amf
module to fuse these features. during the diagnostic process, experienced
radiologists usually take the global features of us images into consideration
[20]. therefore, we modify the backbone of yolox from csp-darknet to
swin-transformer-tiny, which is a more suitable choice by the virtue of its
global modeling capabilities [15]. (2) in the video-based diagnosis stage, the
network automatically chooses high-confidence region features of each frame
according to the single-frame detection results and performs temporal
aggregation to output a more accurate diagnosis. the above two stages are
trained successively. we first perform a strong data augmentation to train the
network for tumor detection and classification on individual frames. after that,
the first stage model is switched to the evaluation mode and predicts the label
of each frame in the video clip. finally, we train the ota module to aggregate
the temporal information for precise diagnosis.",5
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,2.3,Video-Level Decision Generation,"in clinical practice, the dynamic changes in us videos provide useful
information for radiologists to make diagnoses. therefore, we design an ota
module that aggregates single-frame renal tumor detection results in temporal
dimension for diagnosing tumors as benign and malignant. first, we utilize a
feature selection module [14] to select high-quality features of each frame from
the cls_conv and reg_conv layers. specifically, we select the top 750 grid cells
on the prediction grid according to the confidence score. then, 30 of the top
750 grid cells are chosen by the non-maximum suppression algorithm for reducing
redundancy. the features are finally picked out from the cls_conv and reg_conv
layers guided by the positions of the top 30 grid cells. let f cls = {cls 1 ,
cls 2 , ...cls l } and f reg = {reg 1 , reg 2 , ...reg l } denote the above
obtained high-quality features from l frames. after feature selection, we
aggregate the features in the temporal dimension by time attention. f cls and f
reg are mapped into (q cls , k cls , v cls ) and (q reg , k reg ) via linear
projection. then, we utilize scaled dot-product to compute the attention weights
of v cls as:after temporal feature aggregation, f temp is fed into a multilayer
perceptron head to predict the class of tumor.3 experimental results",5
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,3.1,Materials and Implementations,"we collect a renal tumor us dataset of 179 cases from two medical centers, which
is split into the training and validation sets. we further collect 36 cases from
the two medical centers mentioned above (14 benign cases) and another center
(fujian provincial hospital, 22 malignant cases) to form the test set. each case
has a video with simultaneous imaging of b-mode and ceus-mode. some examples of
the images are shown in fig. 2. there is an obvious visual difference between
the images from the fujian provincial hospital (last column in fig. 2) and the
other two centers, which raises the complexity of the task but can better verify
our method's generalization ability. more than two radiologists with ten years
of experience manually annotate the tumor bounding box and class label at the
frame level using the pair annotation software package (https://www.aipair.
com.cn/en/, version 2.7, rayshape, shenzhen, china) [10]. each case has 40-50
labeled frames, and these frames cover the complete contrast-enhanced imaging
cycle. the number of cases and annotated frames is summarized in table 1.weights
pre-trained from imagenet are used to initialize the swin-transformer backbone.
data augmentation strategies are applied synchronously to b-mode and ceus-mode
images for all experiments, including random rotation, mosaic, mixup, and so on.
all models are trained for 150 epochs. the batch size is set to 2. we use the
sgd optimizer with a learning rate of 0.0025. the weight decay is set to 0.0005
and the momentum is set to 0.9. in the test phase, we use the weights of the
best model in validation to make predictions. all experiments are implemented in
pytorch with an nvidia rtx a6000 gpu. ap 50 and ap 75 are used to assess the
performance of single-frame detection. accuracy and f1-score are used to
evaluate the video-based tumor diagnosis.",5
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,3.2,Ablation Study,"single-frame detection. we explore the impact of different backbones in yolox
and different ways of multi-modal fusion. as shown in table 2, using
swin-transformer as the backbone in yolox achieves better performance than the
original backbone while reducing half of the parameters. the improvement may
stem from the fact that swin-transformer has a better ability to characterize
global features, which is critical in us image diagnosis. in addition, we
explore the role of cross-attention and self-attention blocks in multi-modal
tasks, as well as the optimal strategy for combining their outputs. comparing
row 5 with row 7 and row 8 in table 2, the dual-attention mechanism outperforms
the single crossattention. it indicates that we need to pay attention to both
modality-invariant and modality-specific features in our multi-modal task
through cross-attention and self-attention blocks. however, ""ca+sa"" (row 6 in
table 2) obtains inferior performance than ""ca"" (row 5 in table 2). we
conjecture that connecting the two attention modules in series leads to the
entanglement of modality-specific and modality-invariant information, which
would disrupt the model training. on the contrary, the ""ca//sa"" method,
combining two attention modules in parallel, enables the model to capture and
digest modality-specific and modality-invariant features independently. for the
same reason, we concatenate the outputs of the attention blocks rather than
summing, which further avoids confusing modality-specific and modality-invariant
information. therefore, the proposed method achieves the best performance.table
2. the results of ablation study. ""ca"" and ""sa"" denote cross-attention and
selfattention respectively. ""//"" and ""+"" mean parallel connection and series
connection. video-based diagnosis. we investigate the performance of the ota
module for renal tumor diagnosis in multi-modal videos. we generate a video clip
with l frames from annotated frames at a fixed interval forward. as shown in
table 3, gradually increasing the clip length can effectively improve the
accuracy. this suggests that the multi-frame model can provide a more
comprehensive characterization of the tumor and thus achieves better
performance. meanwhile, increasing the sampling interval tends to decrease the
performance (row 4 and row 5 in table 3). it indicates that continuous
inter-frame information is beneficial for renal tumor diagnosis.",5
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,3.3,Comparison with Other Methods,"the comparison results are shown in table 4. compared to the single-modal
models, directly concatenating multi-modal features (row 3 in table 4) improves
ap 50 and ap 75 by more than 15%. this proves that complementary information
exists among different modalities. for a fair comparison with other fusion
methods, we embed their fusion modules into our framework so that different
approaches can be validated in the same environment. cmml [19] and cen [17]
merge the multi-modal features or pick one of them by automatically generating
channel-wise weights for each modality. they score higher ap in the validation
set but lower one in the test set than ""concatenate"". this may be because the
generated weights are biased to make similar decisions to the source domain,
thereby reducing model generalization in the external data. moreover, cmf only
highlights similar features between two modalities, ignoring that each modality
contains some unique features. tmm focuses on both modality-specific and
modality-invariant information, but the chaotic confusion of the two types of
information deteriorates the model performance. therefore, both cmf [17] and tmm
[9] fail to outperform weight-based models. on the contrary, our amf module
prevents information entanglement by conducting cross-attention and
self-attention blocks in parallel. it achieves ap 50 = 82.8, ap 75 = 60.6 in the
validation set and ap 50 = 79.5, ap 75 = 39.2 in the test set, outperforming all
competing methods while demonstrating superior generalization ability.
meanwhile, the improvement of the detection performance is beneficial to our ota
module to obtain lesion features from more precise locations, thereby improving
the accuracy of benign and malignant renal tumor diagnosis.",5
MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,4.0,Conclusions,"in this paper, we create the first multi-modal ceus video dataset and propose a
novel attention-based multi-modal video fusion framework for renal tumor
diagnosis using b-mode and ceus-mode us videos. it encourages interactions
between different modalities via a weight-sharing dual-branch backbone and
automatically captures the modality-invariant and modality-specific information
by the amf module. it also utilizes a portable ota module to aggregate
information in the temporal dimension of videos, making video-level decisions.
the design of the amf module and ota module is plug-and-play and could be
applied to other multi-modal video tasks. the experimental results show that the
proposed method outperforms single-modal, single-frame, and other
stateof-the-art multi-modal approaches.",5
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,1.0,Introduction,"pathological image analysis is a vital area of research within medical image
analysis, focused on utilizing computer technology to aid doctors in diagnosing
and treating diseases by analyzing pathological tissue slide images [5].
advancements in pathological image analysis have been made in early cancer
diagnosis, tumor localization, and grading, and treatment planning [3,10].
multi-instance learning [2] is the primary analysis method used, which involves
analyzing tasks based on slide labels and patches. despite this, the clinical
pathological analysis presents certain challenges and complexities, with the
ultimate diagnosis relying on patients rather than slides.specifically, in
clinical problems of pathological image analysis, doctors usually summarize
patient-level labels based on slide labels as the diagnostic results [1,6]. for
example, for the pathological discrimination diagnosis task of intestinal
tuberculosis(itb) and crohn's desease(cd), the categories of postoperative
slides are divided into three types (normal, cd, itb), and doctors will
summarize the binary results of patients (itb or cd) based on slide-level labels
[6]. similar situations exist in other tasks, such as the classification of
breast cancer metastases in lymph nodes, where slide categories may have
different classifications, and the corresponding diagnosis of the same patient
is whether the cancer has spread to the regional lymph nodes (n-stage) [1].
therefore, as shown in fig. 1, actual pathological image analysis involves the
relationships of patches, slides, and patients, which is called a multi-level
multi-instance learning (ml-mil) problem. among them, for patients and slides,
patients are bags while slides are instances, and for slides and patches, slides
are bags while patches are instances.there are generally two methods to solve
the ml-mil problem. the first method is to directly average the prediction
values of slides or take the maximum prediction value [9]. this method is
relatively simple, but the information exchange between slides is not fully
utilized, which may lead to errors in the summary result. the second method is
to treat slide-patient as a new mil problem according to the traditional mil
thinking, where slides are regarded as instances and patient labels as bags.
although this method seems reasonable, the number of patients is usually
relatively small, and deep learning models usually require a large amount of
data for training. therefore, the insufficient number of samples at the
slide-patient level may make it difficult for the model to learn enough
information.to address the multi-level multi-instance learning (ml-mil) problem
in medical field, we propose a novel framework called patients and slides are
equal (p&sre). inspired by the iterative labeling process in medical diagnosis,
this framework treats patients and slides as instances at the same level and
uses transformers and attention mechanisms to build connections between them.
this simple yet effective method allows for interaction between patient-level
and slidelevel information to correct their respective features and improve
classification performance. our framework consists of two steps: first, at the
patch-slide level, a common mil framework is used to train a mil neural network
and obtain slide-level feature vectors; then, at the slide-patient level, we use
self-attention mechanisms to combine the slides of the same patient into
patient-level feature vectors, and treat these patient-level feature vectors
together with all slide-level feature vectors of the same patient as instances
at the same level, which are inputted into transformers for feature interaction
and prediction of patient-and slide-level labels. our method can effectively
solve the problem of difficult training due to the scarcity of samples at the
highest level in ml-mil, and can be integrated into two state-of-the-art methods
to further improve performance. we conducted rigorous experiments on two
datasets and demonstrated the effectiveness of our method. our contributions
include:1) proposing a novel general framework to address the unique
""patch-slidepatient"" ml-mil problem in the medical field. before this, no other
framework had directly tackled this specific problem, making our proposal a
ground-breaking step in the application of ml-mil in healthcare; 2) proposing a
simple yet highly effective method that leverages self-attention mechanisms and
transformer models to enhance the interaction between slide and patient
information. this innovative approach not only improves the classification
performance at the patient level but also at the slide level, showcasing its
effectiveness and versatility; 3) conducting extensive experiments on two
separate datasets. our method was seamlessly integrated with two prior
state-of-the-art methods, demonstrating its compatibility and adaptability. the
experiments resulted in improved performance, indicating that our method
enhances the efficacy of these existing approaches.",5
Patients and Slides are Equal: A Multi-level Multi-instance Learning Framework for Pathological Image Analysis,3.1,Dataset and Evaluation,"cd-itb dataset. cd-itb is a private dataset consisting of 853 slides from 163
patients, with binary patient-level labels of cd or itb in a ratio of 103:60 and
tri-class slide-level labels of cd, itb, and normal slides in a ratio of
436:121:296, respectively. on average, there were 5 slides per patient. the
slides were scanned at a magnification of 40× (0.25 µm/px), and annotations were
curated by experienced pathologists. we adopted a patient-level stratification
approach for 5-fold cross-validation, with 20% of the training set randomly
assigned as the validation set for each fold. the dataset comprises an average
of 2.3k instances per bag, with the largest bag containing over 16k
instances.camelyon17 dataset. camelyon17 [1] is a publicly dataset, and its
training set comprises 500 slides from 100 breast cancer patients with lymph
node metastases. the slides are classified into four distinct categories, namely
negative, itc, micro, and macro, in proportions of 318:36:59:87, respectively.
there were 5 slides per patient on average. the patients are divided into two
groups based on their pn stage, namely lymph node positive and lymph node
negative, in proportions of 24:76, respectively. the data folding method is the
same as the cd-itb dataset. the average number of instances per bag is
approximately 6.1k, and the largest bag contains over 23k instances.metrics. we
report class-wise weighted accuracy (acc), precision(pre), recall, and f1-score
(f1). to avoid randomness, we run all experiments five times and report the
averaged metrics.",5
Learning with Synthesized Data for Generalizable Lesion Detection in Real PET Images,1.0,Introduction,"deep neural networks have recently shown impressive performance on lesion
quantification in positron emission tomography (pet) images [6]; however, they
usually rely on a large amount of well-annotated, diverse data for model
training. this is difficult or even infeasible for some applications such as
lesion identification in neuroendocrine tumor (net) images, because nets are
rare tumors and lesion annotation in low-resolution, noisy pet images is
expensive. to address the data shortage issue, we propose to train a deep model
for lesion detection with synthesized pet images generated from list mode pet
data, which is low-cost and does not require human effort for manual data
annotation.synthesized pet images may exhibit a different data distribution from
real clinical images (see fig. 1), i.e., a domain shift, which can pose
significant challenges to model generalization. to address domain shifts, domain
adaptation requires access to target data for model training [5,29], while
domain generalization (dg) trains a model with only source data [39] and has
recently attracted increasing attention in medical imaging [1,13,15,18]. most of
current dg methods rely on multiple sources of data to learn a generalizable
model, i.e., multisource dg (mdg); however, multi-source data collection is
often difficult in real practice due to privacy concerns or budget deficits.
although single-source dg (sdg) using only one source dataset has been applied
to medical images [12,14,32], very few studies focus on sdg with pet imaging and
the current sdg methods may not be suitable for lesion identification on pet
data. for instance, many existing methods use a complicated, multi-stage model
design pipeline [10,23,30], which introduces an additional layer of algorithm
variability. this situation will become worse for pet images, which typically
have a poor signal-to-noise ratio and low spatial resolution. several other sdg
approaches [26,31,34] leverage unique characteristics of the imaging modalities,
e.g., color spectrum of histological stained images, which are not applicable to
pet data.in this paper, we propose a novel single-stage sdg framework, which
learns with human annotation-free, list mode-synthesized pet images for
generalizable lesion detection in real clinical data. compared with domain
adaptation and mdg, the proposed method, while more challenging, is quite
practical for real applications due to the relatively cheaper net data
collection and annotation. specifically, we design a new data augmentation
module, which generates out-of-domain samples from single-source data with
multi-scale random convolutions. we integrate this module into a deep lesion
detection neural network and introduce a cross-domain consistency constraint for
feature encoding between original synthesized and augmented images. furthermore,
we incorporate a novel patch-based gradient reversal mechanism into the network
and accomplish a pretext task of domain classification, which explicitly
promotes domain-invariant, generalizable representation learning. trained with a
single-source synthesized dataset, the proposed method provides superior
performance of hepatic lesion detection in multiple cross-scanner real clinical
pet image datasets, compared with the reference baseline and recent
state-of-the-art sdg methods.",5
What Do AEs Learn? Challenging Common Assumptions in Unsupervised Anomaly Detection,4.0,Pathology Detection on Chest X-rays,"in this section, we investigate whether aes can learn the healthy anatomy, i.e.,
absence of pathology, and generate pseudo-healthy reconstructions of abnormal
chest x-ray images. pathology detection algorithms are often applied to finding
hyper-intense lesions, such as tumors or multiple sclerosis on brain scans.
however, it has been shown that thresholding techniques can outperform
learningbased methods [22]. in contrast, the detection of pathology on chest
radiographs is much more difficult due to the high variability and complexity of
nominal features and the diversity and irregularity of abnormalities.datasets.
we use the covid-19 radiography database on kaggle [6,27]. we used the rsna
dataset [35], which contains 10k cxr images of normal subjects and 6k lung
opacity cases. for the detection of covid-19, we used the padchest dataset [3]
containing cxr images manually annotated by trained radiologists. we used 1.3k
healthy control images and 2.5k cases of covid-19.results. of the baselines,
only adversarially-trained aes can reconstruct pseudo-healthy images from
abnormal samples, as shown in fig. 4. however, their imperfect reconstructions
overshadow the error on pathology leading to poor anomaly detection results, as
reflected in the ssim and auroc in table 1. spatial aes and daes have a tendency
to reproduce the input and produce reconstructions of structures that are not
included in the training distribution, such as medical devices and pathologies,
despite not being trained on ood data. this can lead to false negatives. daes
can avoid the reconstruction of some pathologies and achieve good anomaly
detection results. however, pathological regions that do not conform to the
learned noise model are completely missed, as shown in fig. 4. the next three
methods (ae-d, vae, and β-vae) produce blurry reconstructions, as it can be best
seen in the lpips score. morphaeus is the only method to yield accurate
pseudo-healthy reconstructions and effectively remove anomalies, such as
pathology or implanted medical devices. this enables to precisely localize
pathologies, considerably outperforming the baselines.ablation study: importance
of morphological adaptations. we evaluate the effectiveness of individual
components of morphaeus in fig. 5. aes without perceptual loss tend to not
reconstruct small but important features such as ribs and yield false positives
on healthy tissue. interestingly, aes with perceptual loss achieve more visually
appealing reconstructions, but fail at detecting anomalies because the
pathological region is overshadowed by false positive residuals on edges and
misaligned ribs. morphometric adaptations guide the networks to learn better
representations, reduce the number of false positives, and enable the
localization of pathologies. this considerably improves the detection results to
84.8 and 82.1 for aes with and without perceptual loss, respectively. it is
important to note that the deformations are not only beneficial at the time of
inference, but also drive the learning process towards better representations.
thereby, the average pathology detection increases from 66.8 to 80.2, even if no
adaptations are made during inference, i.e., using x rec instead of x morph for
inference (see appendix for details).",5
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,1.0,Introduction,"pancreatic ductal adenocarcinoma (pdac) is one of the deadliest forms of human
cancer, with a 5-year survival rate of only 9% [16]. neoadjuvant chemotherapy
can increase the likelihood of achieving a margin-negative resection and avoid
unnecessary surgery in patients with aggressive tumor types [23]. providing
accurate and objective preoperative biomarkers is crucial for triaging patients
who are most likely to benefit from neoadjuvant chemotherapy. however, current
clinical markers such as larger tumor size and high carbohydrate antigen (ca)
19-9 level may not be sufficient to accurately tailor neoadjuvant treatment for
patients [19]. therefore, multi-phase contrast-enhanced ct has a great potential
to enable personalized prognostic prediction for pdac, leveraging its ability to
provide a wealth of texture information that can aid in the development of
accurate and effective prognostic models [2,10].previous studies have utilized
image texture analysis with hand-crafted features to predict the survival of
patients with pdacs [1], but the representational fig. 1. two examples of
spatial information between vessel (orange region) and tumor (green region). the
minimum distance, which refers to the closest distance between the superior
mesenteric artery (sma) and the pdac tumor region, is almost identical in these
two cases. we define the surface-to-surface distance based on point-to-surface
distance (weighted-average of red lines from ♦ to ) instead of point-to-point
distance (blue lines) to better capture the relationship between the tumor and
the perivascular tissue.here ♦ and are points sampled from subset vc and pc
defined in eq. power of these features may be limited. in recent years, deep
learning-based methods have shown promising results in prognosis models
[3,6,12]. however, pdacs differ significantly from the tumors in these studies.
a clinical investigation based on contrast-enhanced ct has revealed a dynamic
correlation between the internal stromal fractions of pdacs and their
surrounding vasculature [14]. therefore, focusing solely on the texture
information of the tumor itself may not be effective for the prognostic
prediction of pdac. it is necessary to incorporate tumor-vascular involvement
into the feature extraction process of the prognostic model. although some
studies have investigated tumor-vascular relationships [21,22], these methods
may not be sufficiently capable of capturing the complex dynamics between the
tumor and its environment.we propose a novel approach for measuring the relative
position relationship between the tumor and the vessel by explicitly using the
distance between them. typically, chamfer distance [7], hausdorff distance [8],
or other surfaceawareness metrics are used. however, as shown in fig. 1, these
point-to-point distances cannot differentiate the degree of tumor-vascular
invasion [18]. to address this limitation, we propose a learnable neural
distance that considers all relevant points on different surfaces and uses an
attention mechanism to compute a combined distance that is more suitable for
determining the degree of invasion. furthermore, to capture the tumor
enhancement patterns across multi-phase ct images, we are the first to combine
convolutional neural networks (cnn) and transformer [4] modules for extracting
the dynamic texture patterns of pdac and its surroundings. this approach takes
advantage of the visual transformer's adeptness in capturing long-distance
information compared to the cnn-onlybased framework in the original approach. by
incorporating texture information between pdac, pancreas, and peripancreatic
vessels, as well as the local tumor information captured by cnn, we aim to
improve the accuracy of our prognostic prediction model.in this study, we make
the following contributions: (1) we propose a novel approach for aiding survival
prediction in pdac by introducing a learnable neural distance that explicitly
evaluates the degree of vascular invasion between the tumor and its surrounding
vessels. (2) we introduce a texture-aware transformer block to enhance the
feature extraction approach, combining local and global information for
comprehensive texture information. we validate that the cross-attention is
utilized to capture cross-modality information and integrate it with in-modality
information, resulting in a more accurate and robust prognostic prediction model
for pdac. (3) through extensive evaluation and statistical analysis, we
demonstrate the effectiveness of our proposed method. the signature built from
our model remains statistically significant in multivariable analysis after
adjusting for established clinical predictors. our proposed model has the
potential to be used in combination with clinical factors for risk
stratification and treatment decisions for patients with pdac.",5
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,2.0,Methods,"as shown in fig. 2, the proposed method consists of two main components. the
first component combines the cnn and transformer to enhance the extraction of
tumor dynamic texture features. the second component proposes a neural distance
metric between pdac and important vessels to assess their involvements.",5
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,3.0,Experiments,"dataset. in this study, we used data from shengjing hospital to train our method
with 892 patients, and data from three other centers, including guangdong
provincial people's hospital, tianjin medical university and sun yatsen
university cancer center for independent testing with 178 patients. the
contrast-enhanced ct protocol included non-contrast, pancreatic, and portal
venous phases. pdac masks for 340 patients were manually labeled by a
radiologist from shengjing hospital with 18 years of experience in pancreatic
cancer, while the rest were predicted using self-learning models [11,24] and
checked by the same annotator. other vessel masks were generated using the same
semisupervised segmentation models. c-index was used as our primary evaluation
metric for survival prediction. we also reported the survival auc, which
estimates the cumulative area under the roc curve for the first 36
months.implementation details: we used nested 5-fold cross-validation and
augmented the training data by rotating volumetric tumors in the axial direction
and randomly selecting cropped regions with random shifts. we also set the
output feature dimensions to c t = 64 for the texture-aware transformer, c s =
64 for the structure extraction and k = 32 for the neural distance. the batch
size was 16 and the maximum iteration was set to 1000 epochs, and we selected
the model with the best performance on the validation set during training for
testing. we implemented our experiments using pytorch 1.11 and trained the
models on a single nvidia 32g-v100 gpu.ablation study. we first evaluated the
performance of our proposed textureaware transformer (tat) by comparing it with
the resnet18 cnn backbone and vit transformer backbone, as shown in table 1. our
model leverages the strengths of both local and global information in the
pancreas and achieved the best result. next, we compared different methods for
multi-phase stages, including lstm, early fusion (fusion), and cross-attention
(cross) in our method. cross-attention is more effective and lightweight than
lstm. moreover, we separated texture features into in-phase features and
cross-phase features, which is more reasonable than early fusion.secondly, we
evaluated each component in our proposed method, as shown in fig. 2, and
presented the results in table 1. combining the texture-aware transformer and
regular structure information improved the results from 0.630 to 0.648, as tumor
invasion strongly affects the survival of pdac patients. we also employed a
simple 4-variable regression model that used only the chamfer distance of the
tumor and the four vessels for prognostic prediction. the resulting c-index of
0.611 confirmed the correlation of the distance with the survival, which is
consistent with clinical findings [18]. explicitly adding the distance measure
further improved the results. our proposed neural distance metric outperformed
traditional surface distance metrics like chamfer distance, indicating its
suitability for distinguishing the severity of pdac.",5
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,0.0,Comparisons.,"to further evaluate the performance of our proposed model, we compared it with
recent deep prediction methods [17,21] and report the results in table 2. we
modified baseline deep learning models [12,17] and used their network
architectures to take a single pancreatic phase or all three phases as inputs.
deepct-pdac [21] is the most recent method that considers both tumor-related and
tumor-vascular relationships using 3d cnns. our proposed method, which uses the
transformer and structure-aware blocks to capture tumor enhancement patterns and
tumor-vascular involvement, demonstrated its effectiveness with better
performance in both nested 5-fold cross-validation and the multi-center
independent test set.in table 3, we used univariate and multivariate cox
proportional-hazards models to evaluate our signature and other
clinicopathologic factors in the independent test set. the proposed risk
stratification was a significant prognostic factor, along with other factors
like pathological tnm stages. after selecting significant variables (p < 0.05)
in univariate analysis, our proposed staging remained strong in multivariable
analysis after adjusting for important prognostic markers like pt and resection
margins. notably, our proposed marker remained the strongest among all
pre-operative markers, such as tumor size and ca 19-9.neoadjuvant therapy
selection. to demonstrate the added value of our signature as a tool to select
patients for neoadjuvant treatment before surgery, we plotted kaplan-meier
survival curves in fig. 3. we further stratify patients by our signature after
grouping them by tumor size and ca19-9, two clinically used preoperative
criteria for selection, and also age. our signature could significantly stratify
patients in all cases and those in the high-risk group had worse outcomes and
might be considered as potential neoadjuvant treatment candidates (e.g. 33
high-risk patients with larger tumor size and high ca19-9).",5
Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,4.0,Conclusion,"in our paper, we propose a multi-branch transformer-based framework for
predicting cancer survival. our framework includes a texture-aware transformer
that captures both local and global information about the pdac and pancreas. we
also introduce a neural distance to calculate a more reasonable distance between
pdac and vessels, which is highly correlated with pdac survival. we have
extensively evaluated and statistically analyzed our proposed method,
demonstrating its effectiveness. furthermore, our model can be combined with
established high-risk features to aid in the patient selections who might
benefit from neoadjuvant therapy before surgery.",5
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,1.0,Introduction,"existing tumor augmentation methods, including ""copy-paste"" strategy based
methods [15][16][17]19] and style-transfer based methods [5], only considered
content or style information when synthesizing new samples, which leads to a
distortion gap in content or domain space between the true image and synthetic
image, and further causes a distortion problem [14] as shown in fig. 1 (1). the
distortion problem damages the effectiveness of dcnns in feature representation
learning as proven in many studies [1,5,18]. therefore, a domain and content
simultaneously aware data augmentation method is urgently needed to eliminate
and avoid the distortion challenges during tumor generation. it remains,
however, a very challenging task because the content and domain space lack of
clear border, and the domain information always influences the distribution of
content. this is also the main reason that style transfer [7,8,10] still suffers
from spurious artifacts such as disharmonious colors and repetitive patterns,
and a large gap is still left between real artwork and synthetic style [2,3].
therefore, it's necessary to reduce the influence of the domain on content and
keep the content consistent during image generation.to overcome the above
challenges, a domain-aware and content-consistent tumor augmentation method,
named dcaug, is developed (fig. 1 experimental results on two public tumor
segmentation datasets show that dcaug improves the tumor segmentation accuracy
compared with state-of-theart tumor augmentation methods. in summary, our
contributions are as follows:-a content-aware and domain-aware tumor
augmentation method is proposed, which eliminates the distortion in content and
domain space between the true tumor image and synthetic tumor image. -our novel
dacl and cdcl disentangle the image information into two completely independent
parts: 1) domain-invariant content information; 2) individual-specific domain
information. it has the advantage of alleviating the challenge of distortion in
synthetic tumor images. -experimental results on two public tumor segmentation
datasets demonstrate that dcaug improves the diversity and quality of synthetic
tumor images.",5
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.1,Problem Definition,"formulation: given two images and the corresponding tumor labels {x a , y a },
{x b , y b }, tumor composition process can be formulated as:where,
respectively. there are two challenges need to be solved: 1) x b→a a , x a→b b ,
by adjusting the domain information of the copied tumor, making the copied tumor
have the same domain space as the target image to avoid domain distortion; 2)b ,
maintaining the domain-invariant content information consistency during tumor
copy to avoid content distortion.to achieve the above goals, a novel cross-cycle
framework (fig. 2) is designed, which consists of two generators and can
disentangle the tumor information into two solely independent parts: 1)
domain-invariant content information, 2) individual-specific domain information,
through two new learning strategies: 1) domain-aware contrastive learning
(dacl); 2) cross-domain consistency learning (cdcl). when generating new sample,
the domain-invariant content information is preserved by cdcl, while the
individual-specific domain information is adjusted by dacl based on the domain
space of target tumor image. the details are described as follows.",5
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.2,Domain-Aware Contrastive Learning for Domain Adaptation,"our domain-aware contrastive learning (dacl) strategy can adaptively adjust the
domain space of the transferred tumor and makes the domain space consistent for
domain adaptation. specifically, the input of dcaug is two combined images x b a
, x a b that consist of source images and tumor regions copied from another
image. the synthetic tumors x b→a a generated by the generator, the a as the
anchor, the positive and the negative sample, respectively. to find the domain
space of these samples for contrast, a fixed pre-trained style representation
extractor f is used to obtain domain representations for different images. thus,
dacl between the anchor, the positive, and the negative sample can be formulated
as:where d(x, y) is the l 2 distance between x and y, w i is weighting factor.
additionally, to further disentangle the individual-specific domain information,
a reversed process is designed. by utilizing the synthetic tumors x a→b b , x
b→a a , the reversed images x a→b a ,x b→a b can be construed as:the whole
reversed process receives the reversed images x a→b a , x b→a b as inputs and
tries to restore the original domain information of the synthetic tumor xa , xb
.where φ denotes the ith layer of the vgg-19 network, μ and σ represent the mean
and standard deviation of feature maps extracted by φ, respectively. in summary,
the total loss for the cross-cycle framework is) where α, β, and γ represent the
weight coefficients.",5
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,3.1,Datasets and Implementation Details,"atlas dataset [11]: the atlas dataset consists of 229 t1-weighted mr images from
220 subjects with chronic stroke lesions. these images were acquired from
different cohorts and different scanners. the chronic stroke lesions are
annotated by a group of 11 experts. the dimension of the pre-processed images is
197 × 233 × 189 with an isotropic 1mm 3 resolution. identical with the study in
[17], we selected 50 images as the test set and the rest of the cases as the
training set. kits19 dataset [4]: the kits19 consists of 210 3d abdominal ct
images with kidney tumor subtypes and segmentation of kidney and kidney tumors.
these ct images are from more than 50 institutions and scanned with different ct
scanners and acquisition protocols. in our experiment, we randomly split the
published 210 images into a training set with 168 images and a testing set with
42 images. training details: the generator in dcaug is built on the rain [12]
backbone, all of the weights in generators are shared. our dcaug is implemented
using pytorch [13] and trained end-to-end with adam [9] optimization method. in
the training phase, the learning rate is initially set to 0.0001 and decreased
by a weight decay of 1.0 × 10 -6 after each epoch. the experiments were carried
out on one nvidia rtx a4000 gpu with 16 gb memory. the weight valule of α, β,
and γ is 1.0,1.0,1.0, separately. baseline: nnunet [6] is selected as the
baseline model. the default hyperparameters and default traditional data
augmentation (tda) including rotation, scaling, mirroring, elastic deformation,
intensity perturbation are used when model training. the maximum number of
training epochs was set to 500 for the two datasets. parts of tumors generated
are shown in fig. 3. and the dice coefficients of the segmentation results on
the same test set are computed to evaluate the effectiveness of methods.",5
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,3.2,Comparison with State-of-the-Art Methods,"experimental results in table 1 and fig. 4 show that compared with other
stateof-the-art methods, including mixup [16], cutmix [15], carvemix [17],
selfmix [19], stylemix [5], nnunet combined with dcaug achieves the highest
improvement on the two datasets, which convincingly demonstrates the innovations
and contribution of dcaug in generating higher quality tumor. and it is worth
noting that cutmix (""copy-paste""method that only considers content information)
even degrades the segmentation performance, which indicates that both content
and domain information has a significant influence on the tumor segmentation.the
representative segmentation scans are shown in fig. 4. our dcaug produced better
segmentation results than the competing methods, which further proves the
effectiveness of dcaug in tumor generation. what's more, the potential of dcaug
in an extremely low-data regime is also demonstrated. we randomly select 25% and
50% of data from the training set same as training data. dcaug also assists the
baseline model to achieve higher dice coefficients, which convincingly
demonstrates the effectiveness of dcaug in generating new tumor samples.",5
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,3.3,Significant in Improving Existing Tumor Augmentation Methods,"the necessity of considering both content and domain information in the tumor
generation is also demonstrated, three representative methods, mixup
(""copy-paste""), cutmix (""copy-paste""), and stylemix (style-transfer), are
selected. the dcaug optimizes generated samples from above methods from content
and domain aspects to further improve the quality of generated samples. and the
nnuet are trained by optimized samples. from the segmentation performances
(table 2), we can notice that dcaug can further boost the quality of generated
samples produced by existing methods. specifically, the dcaug assists the mixup,
cutmix, and stylemix to obtain a 3.15%, 8.53%, and 0.60% improvement in
segmentation performance, respectively, which demonstrates that 1) it is
necessary to consider both content and domain information during samples
generation; 2) avoiding the content and domain distortion challenge can further
improve the quality of generated samples; 3) dcaug can alleviate the challenge
of distortion problem present in existing tumor augmentation methods.",5
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,4.0,Conclusion,"in this paper, our domain-aware and content-consistent tumor augmentation method
eliminated the content distortion and domain gap between the true tumor and
synthetic tumor by simultaneously focusing the content information and domain
information. specifically, dcaug can maintain the domain-invariant content
information consistency and adaptive adjust individual-specific domain
information by a new cross-cycle framework and two novel contrastive learning
strategies when generating synthetic tumor. experimental results on two tumor
segmentation tasks show that our dcaug can significantly improve the quality of
the synthetic tumors, eliminate the gaps, and has practical value in medical
imaging applications.",5
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.3,Cross-Domain Consistency Learning for Content Preservation,"cross-domain consistency learning (cdcl) strategy can preserve the
domaininvariant content information of tumor in the synthesized images x b→a a ,
x a→b b for avoiding content distortion. specifically, given the original
imagesproduced by generator, and the reconstructed images xa , xb generated by
the reversed process. the tumor can be first extracted from those imagesalthough
the domain space is various, the tumor content insideto evaluate the tumor
content inside cross-domain images, the content consistency losses, including l
a pixel (x a , xa ), l b pixel (x b , xb ), l a→b content , l b→a content , are
computed between those images for supervising the content change. the details of
content consistency loss are described in the next section.",5
DCAug: Domain-Aware and Content-Consistent Cross-Cycle Framework for Tumor Augmentation,2.4,Loss Function,"in summary, three types of losses are used to supervise the cross-cycle
framework. specifically, given the original images x a , x b and the combined
images x b a , x b a , the synthesized images x a→b b , x b→a a are produced by
the generator, and the reconstructed images xa , xb are generated by the
reversed process.the pixel-wise loss (l pixel ) computes the difference between
original images and reconstructed images at the pixel level.to disentangle the
individual-specific domain information, the higher feature representations
extracted from pre-trained networks combined with cl are used:and two content
loss l b→a content , l a→b content are employed to maintain tumor content
information during the domain adaptation:",5
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,1.0,Introduction,"gastric cancer (gc) is the third leading cause of cancer-related deaths
worldwide [19]. the five-year survival rate for gc is approximately 33% [16],
which is mainly attributed to patients being diagnosed with advanced-stage
disease harboring unresectable tumors. this is often due to the latent and
nonspecific signs and symptoms of early-stage gc. however, patients with
early-stage disease have a substantially higher five-year survival rate of
around 72% [16]. therefore, early detection of resectable/curable gastric
cancers, preferably before the onset of symptoms, presents a promising strategy
to reduce associated mortality. unfortunately, current guidelines do not
recommend any screening tests for gc [22]. while several screening tools have
been developed, such as barium-meal gastric photofluorography [5], upper
endoscopy [4,7,9], and serum pepsinogen levels [15], they are challenging to
apply to the general population due to their invasiveness, moderate
sensitivity/specificity, high cost, or side effects. therefore, there is an
urgent need for novel screening methods that are noninvasive, highly accurate,
low-cost, and ready to distribute.non-contrast ct is a commonly used imaging
protocol for various clinical purposes. it is a non-invasive, relatively
low-cost, and safe procedure that exposes patients to less radiation dose and
does not require the use of contrast injection that may cause serious side
effects (compared to multi-phase contrastenhanced ct). with recent advances in
ai, opportunistic screening of diseases using non-contrast ct during routine
clinical care performed for other clinical indications, such as lung and
colorectal cancer screening, presents an attractive approach to early detect
treatable and preventable diseases [17]. however, whether early detection of
gastric cancer using non-contrast ct scans is possible remains unknown. this is
because early-stage gastric tumors may only invade the mucosal and muscularis
layers, which are difficult to identify without the help of stomach preparation
and contrast injection. additionally, the poor contrast between the tumor and
normal stomach wall/tissues on non-contrast ct scans and various shape
alterations of gastric cancer, further exacerbates this challenge.in this paper,
we propose a novel approach for detecting gastric cancer on non-contrast ct
scans. unlike the conventional ""segmentation for classification"" methods that
directly employ segmentation networks, we developed a clusterinduced mask
transformer that performs segmentation and global classification simultaneously.
given the high variability in shape and texture of gastric cancer, we encode
these features into learnable clusters and utilize cluster analysis during
inference. by incorporating self-attention layers for global context modeling,
our model can leverage both local and global cues for accurate detection. in our
experiments, the proposed approach outperforms nnunet [8] by 0.032 in auc, 5.0%
in sensitivity, and 4.1% in specificity. these results demonstrate the potential
of our approach for opportunistic screening of gastric cancer in asymptomatic
patients using non-contrast ct scans.",5
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,2.0,Related Work,"automated cancer detection. researchers have explored automated tumor detection
techniques on endoscopic [13,14], pathological images [20], and the prediction
of cancer prognosis [12]. recent developments in deep learning have
significantly improved the segmentation of gastric tumors [11], which is
critical for their detection. however, our framework is specifically designed
for noncontrast ct scans, which is beneficial for asymptomatic patients. while
previous studies have successfully detected pancreatic [25] and esophageal [26]
cancers on non-contrast ct, identifying gastric cancer presents a unique
challenge due to its subtle texture changes, various shape alterations, and
complex background, e.g., irregular gastric wall; liquid and contents in the
stomach.",5
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0.0,Mask Transformers.,"recent studies have used transformers for natural and medical image segmentation
[21]. mask transformers [3,24,29] further enhance cnn-based backbones by
incorporating stand-alone transformer blocks, treating object queries in detr
[1] as memory-encoded queries for segmentation. cmt-deeplab [27] and
kmax-deeplab [28] have recently proposed interpreting the queries as clustering
centers and adding regulatory constraints for learning the cluster
representations of the queries. mask transformers are locally sensitive to image
textures for precise segmentation and globally aware of organtumor morphology
for recognition. their cluster representations demonstrate a remarkable balance
of intra-cluster similarity and inter-class discrepancy. therefore, mask
transformers are an ideal choice for an end-to-end joint segmentation and
classification system for detecting gastric cancer.",5
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,3.0,Methods,"problem formulation. given a non-contrast ct scan, cancer screening is a binary
classification with two classes as l = {0, 1}, where 0 stands for""normal"" and 1
for""gc"" (gastric cancer). the entire dataset is denoted by, where x i is the
i-th non-contrast ct volume, with y i being the voxel-wise label map of the same
size as x i and k channels. here, k = 3 represents the background, stomach, and
gc tumor. p i ∈ l is the class label of the image, confirmed by pathology,
radiology, or clinical records. in the testing phase, only x i is given, and our
goal is to predict a class label for x i .",5
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0.0,Knowledge Transfer from Contrast-Enhanced to Non-contrast CT.,"to address difficulties with tumor annotation on non-contrast cts, the
radiologists start by annotating a voxel-wise tumor mask on the
contrast-enhanced ct, referring to clinical and endoscopy reports as needed.
deeds [6] registration is then performed to align the contrast-enhanced ct with
the non-contrast ct and the resulting deformation field is applied to the
annotated mask. any misaligned ones are revised manually. in this manner (fig.
1d), a relatively coarse yet highly reliable tumor mask can be obtained for the
non-contrast ct image. cluster-induced classification with mask transformers.
segmentation for classification is widely used in tumor detection [25,26,32]. we
first train a unet [8,18] to segment the stomach and tumor regions using the
masks from the previous step. this unet considers local information and can only
extract stomach rois well during testing. however, local textures are inadequate
for accurate gastric tumor detection on non-contrast cts, so we need a network
of both local sensitivity to textures and global awareness of the organ-tumor
morphology. mask transformer [3,24] is a well-suited approach to boost the cnn
backbone with stand-alone transformer blocks. recent studies [27,28] suggest
interpreting object queries as cluster centers, which naturally exhibit
intra-cluster similarity and inter-class discrepancy. inspired by this, we
further develop a deep classification model on top of learnable cluster
representations.specifically, given image x ∈ r h×w ×d , annotation y ∈ r k×hw d
, and patient class p ∈ l, our model consists of three components: 1) a cnn
backbone to extract its pixel-wise features f ∈ r c×hw d (fig. 1a), 2) a
transformer module (fig. 1b), and 3) a multi-task cluster inference module (fig.
1c). the transformer module gradually updates a set of randomly initialized
object queries c ∈ r n ×c , i.e., to meaningful mask embedding vectors through
cross-attention between object queries and multi-scale pixel features,where c
and p stand for query and pixel features, q c , k p , v p represent linearly
projected query, key, and value. we adopt cluster-wise argmax from kmax-deeplab
[28] to substitute spatial-wise softmax in the original settings.we further
interpret the object queries as cluster centers from a cluster analysis
perspective. all the pixels in the convolutional feature map are assigned to
different clusters based on these centers. the assignment of clusters (a.k.a.
mask prediction) m ∈ r n ×hw d is computed as the cluster-wise softmax function
over the matrix product between the cluster centers c and pixel-wise feature
matrix f, i.e.,the final segmentation logits z ∈ r k×hw d are obtained by
aggregating the pixels within each cluster according to cluster-wise
classification, which treats pixels within a cluster as a whole. the aggregation
of pixels is achieved by z = c k m, where the cluster-wise classification c k is
represented by an mlp that projects the cluster centers c to k channels (the
number of segmentation classes).the learned cluster centers possess high-level
semantics with both intercluster discrepancy and intra-cluster similarity for
effective classification. rather than directly classifying the final feature
map, we first generate the clusterpath feature vector by taking the channel-wise
average of cluster centers c =additionally, to enhance the consistency between
the segmentation and classification outputs, we apply global max pooling to
cluster assignments r to obtain the pixel-path feature vector r ∈ r n . this
establishes a direct connection between classification features and segmentation
predictions. finally, we concatenate these two feature vectors to obtain the
final feature and project it onto the classification prediction p ∈ r 2 via a
two-layer mlp.the overall training objective is formulated as,where the
segmentation loss l seg (•, •) is a combination of dice and cross entropy
losses, and the classification loss l cls (•, •) is cross entropy loss.",5
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,4.1,Experimental Setup,"dataset and ground truth. our study analyzed a dataset of ct scans collected
from guangdong province people's hospital between years 2018 and 2020, with
2,139 patients consisting of 787 gastric cancer and 1,352 normal cases. we used
the latest patients in the second half of 2020 as a hold-out test set, resulting
in a training set of 687 gastric cancer and 1,204 normal cases, and a test set
of 100 gastric cancer and 148 normal cases. we randomly selected 20% of the
training data as an internal validation set. to further evaluate specificity in
a larger population, we collected an external test set of 903 normal cases from
shengjing hospital. cancer cases were confirmed through endoscopy (and
pathology) reports, while normal cases were confirmed by radiology reports and a
two-year follow-up. all patients underwent multi-phase cts with a median spacing
of 0.75 × 0.75 × 5.0 mm and an average size of (512, 512, 108) voxel. tumors
were annotated on the venous phase by an experienced radiologist specializing in
gastric imaging using ctlabeler [23], while the stomach was automatically
annotated using a self-learning model [31].implementation details. we resampled
each ct volume to the median spacing while normalizing it to have zero mean and
unit variance. during training, we cropped the 3d bounding box of the stomach
and added a small margin of (32,32,4). we used nnunet [8] as the backbone, with
four transformer decoders, each taking pixel features with output strides of 32,
16, 8, and 4. we set the number of object queries n to 8, with each having a
dimension of 128, and included an eight-head self-attention layer in each block.
the patch size used during training and inference is (192, 224, 40) voxel. we
followed [8] to augment data. we trained the model with radam using a learning
rate of 10 -4 and a (backbone) learning rate multiplier of 0.1 for 1000 epochs,
with a frozen backbone of the pretrained nnunet [8] for the first 50 epochs. to
enhance performance, we added deep supervision by aligning the cross-attention
map with the final segmentation map, as per kmax-deeplab [27]. the hidden layer
dimension in the two-layer mlp is 128. we also trained a standard unet [8,18] to
localize the stomach region in the entire image in the testing phase.",5
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0.0,Evaluation Metrics and Reader Study.,"for the binary classification, model performance is evaluated using area under
roc curve (auc), sensitivity (sens.), and specificity (spec.). and successful
localization of the tumors is considered when the overlap between the
segmentation mask generated by the model and the ground truth is greater than
0.01, measured by the dice score. a reader study was conducted with two
experienced radiologists, one from guangdong province people's hospital with 20
years of experience and the other from the first affiliated hospital of zhejiang
university with 9 years of experience in gastric imaging. the readers were given
248 non-contrast ct scans from the test set and asked to provide a binary
decision for each scan, indicating whether the scan showed gastric cancer. no
patient information or records were provided to the readers. readers were
informed that the dataset might contain more tumor cases than the standard
prevalence observed in screening, but the proportion of case types was not
disclosed. readers used itk-snap [30] to interpret the ct scans without any time
constraints. 1 presents a comparative analysis of our proposed method with three
baselines. the first two approaches belong to ""segmentation for classification""
(s4c) [26,32], using nnunet [8] and transunet [2]. a case is classified as
positive if the segmented tumor volume exceeds a threshold that maximizes the
sum of sensitivity and specificity on the validation set. the third baseline
(denoted as ""nnunet-joint"") integrates a cnn classification head into unet [8]
and trained end-to-end. we obtain the 95% confidence interval of auc,
sensitivity, and specificity values from 1000 bootstrap replicas of the test
dataset for statistical analysis. for statistical significance, we conduct a
delong test between two aucs (ours vs. compared method) and a permutation test
between two sensitivities or specificities (ours vs. compared method and
radiologists).",5
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0.0,AI Models Surpass Experienced Radiologists on Non-contrast CT Scans.,"as shown in fig. 2a, our ai model's roc curve is superior to that of two
experienced radiologists. the model achieves a sensitivity of 85.0% in detecting
gastric cancer, which significantly exceeds the mean performance of doctors
(73.5%) and also surpasses the best performing doctor (r2: 75.0%), while
maintaining a high specificity. a visual example is presented in fig. 2b. this
early-stage cancer (t1) is miss-detected by both radiologists, whereas
classified and localized precisely by our model.",5
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0.0,Subgroup Analysis.,"in table 2, we report the performance of patient-level detection and tumor-level
localization stratified by tumor (t) stage. we compare our model's performance
with that of both radiologists. the results show that our model performs better
in detecting early stage tumors (t1, t2) and provides more precise tumor
localization. specifically, our model detects 60.0% (6/10) t1 cancers, and 77.8%
(7/9) t2 cancers, surpassing the best performing expert (50% t1, 55.6% t2).
meanwhile, our model maintains a reliable detection rate and credible
localization accuracy for t3 and t4 tumors (2 of 34 t3 tumors missed).comparison
with established screening tools. our method surpasses or performs on par with
established screening tools [4,7,10] in terms of sensitivity for gastric cancer
detection at a similar specificity level with a relatively large testing patient
size (n = 1151 by integrating the internal and external test sets), as shown in
table 3. this finding sheds light on the opportunity to employ automated ai
systems to screen gastric cancer using non-contrast ct scans.",5
Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,5.0,Conclusion,"we propose a novel cluster-induced mask transformer for gastric cancer detection
on non-contrast ct scans. our approach outperforms strong baselines and
experienced radiologists. compared to other screening methods, such as blood
tests, endoscopy, upper-gastrointestinal series, and me-nbi, our approach is
non-invasive, cost-effective, safe, and more accurate for detecting early-stage
tumors. the robust performance of our approach demonstrates its potential for
opportunistic screening of gastric cancer in the general population.",5
Self-supervised Polyp Re-identification in Colonoscopy,1.0,Introduction,"optical colonoscopy is the standard of care screening procedure for the
prevention and early detection of colorectal cancer (crc). the primary goal of a
screening colonoscopy is polyp detection and preventive removal. it is well
known that many polyps go unnoticed during colonoscopy [22]. to deal with this
problem, computer-aided polyp detector (cade) was introduced [13][14][15][16]
and recently became commercially available [3]. the success of polyp detector
sparkled the development of new cad tools for colonoscopy, including polyp
characterization (cadx, or optical biopsy), extraction of various quality
metrics, and automatic reporting. many of those new cad applications require
aggregation of all available data on a polyp into a single unified entity. for
example, one would expect higher accuracy for cadx when it analyzes all frames
where a polyp is observed. clustering polyp detections into polyp entities is a
prerequisite for computing such quality metrics as polyp detection rate (pdr)
and polyps per colonoscopy (ppc), and for listing detected polyps in a
report.one may notice that the described task generally falls into the category
of the well known multiple object tracking (mot) problem [26,27]. while this is
true, there are a few factors specific to the colonoscopy setup: (a) due to
abrupt endoscope camera movements, targets (polyps) often go out of the field of
view, (b) because of heavy imaging conditions (liquids, debris, low
illumination) and non-rigid nature of the colon, targets may change their
appearance significantly, (c) many targets (polyps) are quite similar in
appearance. those factors limit the scope and accuracy of existing
frame-by-frame spatio-temporal tracking methods, which typically yield an
over-fragmented result. that is, the track is often lost, resulting in
relatively short tracklets (temporal sequences of same target detections in
multiple near-consecutive frames), see supplementary fig. 1.a recently published
method [2] addresses this limitation by combining spatial target proximity and
visual similarity to match a polyp detected in the current frame to ""active""
polyp tracklets dynamically maintained by the system. the tracklets are built
incrementally, by adding a single frame detection to the matched tracklet,
one-by-one. however, this approach limits itself to use of closein-time
consistent detections, and cannot handle the frequent cases where polyp gets out
of the field of view and long range association is required.in this work we
propose an alternative approach that allows polyp detections grouping over an
extended period of time (up to 10 min), relaxing the spatiotemporal proximity
limitation. it involves two steps: (i) a short-term multi-object tracking, which
forms initial, relatively short tracklets, followed by (ii) a longerterm
tracklets grouping by appearance-based polyp re-identification (reid). as the
first step can be done by any generic multiple object tracking algorithm (e.g.
we use a tracking by detection method [27]), in this paper we focus on the
second step.to avoid manual data annotation, which is extremely ineffective in
our case, we turn to self-supervision and adapt the widely used contrastive
learning approach [5] to video input and object tracking scenario.as tracklet
re-identification is a sequence-to-sequence matching problem, the standard
solution is comparing sequences element-wise and then aggregating the
per-element comparisons, e.g. by averaging or max/min pooling [21] -the
so-called late fusion technique. we, on the other hand, follow an early fusion
approach by building a joint representation for the whole sequence. we use an
advanced transformer network [23] to leverage the attention paradigm for
nonuniform weighing and ""knowledge exchange"" between tracklet frames.we
extensively test the proposed method on hundreds of colonoscopy videos and
evaluate the contribution of method components using an ablation study. finally,
we demonstrate the effectiveness of the proposed reid method for improving the
accuracy of polyp characterization (cadx).to summarize, the three main
contributions of the paper are:-an adaptation of contrastive learning to video
input for the purpose of appearance based object tracking. -an early fusion,
joint multi-view object representation for reid, based on transformer networks.
-the application of polyp reid to boost the polyp cadx performance.",5
YONA: You Only Need One Adjacent Reference-Frame for Accurate and Fast Video Polyp Detection,1.0,Introduction,"colonoscopy plays a crucial role in identifying and removing early polyps and
reducing mortality rates associated with rectal cancer. over the past few years,
the research community has devoted great effort to understanding colonoscopy
videos using either optical flow [22,23] or temporal information aggregation
[5,12,16,19] between multiple frames.however, those works are mainly designed
based on the experience of previous natural video object detection studies,
ignoring the inherent uniqueness of the colonoscopy motion patterns. thus, we
rethink the video polyp detection task and conclude three core challenges in
colonoscopy videos. 1) fast motion speed. in fig. 1(a), we show the target
motion speed [26] 1 on imagenetvid [14] (natural) and ldpolypvideo [9]
(colonoscopy) dataset. the motion speed in imagenetvid evenly distributes in
three intervals. in contrast, most targets in ldpolypvideo fall in the fast
speed zone, leading to a large variance in the adjacent foreground features,
like motion blur or occlusion, as shown in fig. 1(c). thus we conjecture that
collaborating too many frames for polyp video detection will increase the
misalignment between adjacent frames and leads to poor detection performance.
figure 1(b) shows the performance of fgfa [26] on two datasets with increasing
reference frames. the different trends of the two lines confirm our hypothesis.
2) complex background. different from the common camera-fixed videos, the
camera-moving of colonoscopy video will introduce large disturbances between
adjacent frames (e.g., specular reflection, bubbles, water, etc.), as shown in
fig. 1(d). those abnormalities disrupt the integrity of background structures
and thus affect the effect of multi-frame fusion. 3) concealed polyps. as shown
in fig. 1(e), we noticed that some polyps could be seen as concealed objects in
the colonoscopy video since such polyps have a very similar appearance to the
intestine wall. the model will be confused by such frames in inference and
result in high false-positive or false-negative predictions.to address the above
issues, we propose the yona framework, which fully exploits the reference frame
information and only needs one adjacent reference frame for accurate video polyp
detection. specifically, we propose the foreground temporal alignment (fta)
module to explicitly align the foreground channel activation patterns between
adjacent features according to their foreground similarity. in addition, we
design the background dynamic alignment (bda) module after fta that further
learns the inter-frame background spatial dynamics to better eliminate the
influence of motion speed and increase the training robustness. finally,
parallel to fta and bda, we introduce the cross-frame boxassisted contrastive
learning (cbcl) that fully utilizes the box annotations to enlarge polyp and
background discrimination in embedding space.in summary, our contributions are
in three-folds: (1) to the best of our knowledge, we are the first to
investigate the obstacles to the development of existing video polyp detectors
and conclude that two-frame collaboration is enough for video polyp detection.
(2) we propose the yona, a novel framework for video polyp detection. it
composes the foreground and background alignment modules to align the features
under the fast-moving condition. it further introduces the cross-frame
contrastive learning module to enhance the model's discrimination ability of
polyps and intestine walls. (3) extensive experiments demonstrate that our yona
achieves new state-of-the-art performance on three large-scale public video
polyp detection datasets.",5
Boosting Breast Ultrasound Video Classification by the Guidance of Keyframe Feature Centers,1.0,Introduction,"breast cancer is a life-threatening disease that has surpassed lung cancer as
leading cancer in some countries and regions [20]. breast ultrasound is the
primary screening method for diagnosing breast cancer, and accurately
distinguishing between malignant and benign breast lesions is crucial. this task
is also an essential component of computer-aided diagnosis. since each frame in
an ultrasound video can only capture a specific view of a lesion, it is
essential to aggregate information from the entire video to perform accurate
automatic lesion diagnosis. therefore, in this study, we focus on the
classification of breast ultrasound videos for detecting malignant and benign
breast lesions. [15] and static images from busi [1]. we use a 2d resnet trained
on ultrasound images to get the features.while ultrasound videos offer more
information, prior studies have primarily focused on static image classification
[2,11,27]. obtaining ultrasound video data with pathology gold standard results
poses a major challenge. sonographers typically record keyframe images during
general ultrasound examinations, not entire videos. prospective collection
requires additional efforts to track corresponding pathological results.
consequently, while there are many breast ultrasound image datasets [1,28],
breast ultrasound video datasets remain scarce, with only one relatively small
dataset [15] containing 188 videos available currently.given the difficulties in
collecting ultrasound video data, we investigate the feasibility of enhancing
the performance of ultrasound video classification using a static image dataset.
to achieve this, we first analyze the relationship between ultrasound videos and
images. the images in the ultrasound dataset are keyframes of a lesion that
exhibit the clearest appearance and most typical symptoms, making them more
discriminative for diagnosis. although ultrasound videos provide more
information, the abundance of frames may introduce redundancy or vagueness that
could disrupt classification. from the aspect of feature distribution, as shown
in fig. 1, the feature points of static images are more concentrated, while the
feature of video frames sometimes are away from the class centers. frames far
from the centers are harder to classify. therefore, it is a promising approach
to guide the video model to pay more attention to important frames close to the
class center with the assistance of static keyframe images. meanwhile, our
approach aligns with the diagnosis of ultrasound physicians, automatically
evaluates the importance of frames, and diagnoses based on the information of
key frames. additionally, our method provides interpretability through key
frames.in this paper, we propose a novel keyframe guided attention network
(kga-net) to boost ultrasound video classification. our approach leverages both
image (keyframes) and video datasets to train the network. to classify videos,
we use frame attention to predict feature weights for all frames and aggregate
them to make the final classification. the feature weights determine the
contribution of each frame for the final diagnosis. during training, we
construct category feature centers for malignant and benign examples
respectively using center loss [26] on static image inputs and use the centers
to guide the training of video frame attention. specifically, we propose
coherence loss, which promotes the frames close to the centers to have high
attention weights and decreases the weights for frames far from the centers. due
to the feature centers being generated by the larger scale image dataset, it
provides more accurate and discriminative feature centers which can guide the
video frame attention to focus on important frames, and finally leads to better
video classification.our experimental results on the public busv dataset [15]
show that our kga-net significantly outperforms other video classification
models by using an external ultrasound image dataset. additionally, we
visualized attention values guided by the coherence loss. the frames with clear
diagnostic characteristics are given higher attention values. this phenomenon
makes our method more explainable and provides a new perspective for selecting
keyframes from video.in conclusion, our contributions are as follows:1. we
analyze the relationship between ultrasound video data and image data, and
propose the coherence loss to use image feature centers to guide the training of
frame attention. 2. we propose kga-net, which adopts a static image dataset to
boost the performance of ultrasound video classification. kga-net significantly
outperforms other video baselines on the busv dataset. 3. the qualitative
analysis of the frame attention verifies the explainability of our method and
provides a new perspective for selecting keyframes.",5
Text-Guided Cross-Position Attention for Segmentation: Case of Medical Image,1.0,Introduction,"advances in deep learning have been witnessed in many research areas over the
past decade. in medical field, automatic analysis of medical image data has
actively been studied. in particular, segmentation which identify region of
interest (roi) in an automatic way is an essential medical imaging process.
thus, deep learning-based segmentation has been utilized in various medical
domains such as brain, breast cancers, and colon polyps. among the popular
architectures, variants of u-net have been widely adopted due to their effective
encoderdecoder structure, proficient at capturing the characteristics of cells
in images. recently, it has been demonstrated that the attention modules
[4,17,20] enable deep learning networks to better extract robust features, which
can be applied in medical image segmentation to learn subtle medical features
and achieve higher performance [14,16,18,21].however, as image-only training
trains a model with pixels that constitute an image, there is a limit in
extracting fine-grained information about a target object even if transfer
learning is applied through a pre-trained model. recently, to overcome this
limitation, multi-modality studies have been conducted, aiming to enhance the
expressive power of both text and image features. for instance, clip [12] used
contrastive learning based on image-text pairs to learn the similarity between
the image of an object and the text describing it, achieving significant
performance gains in a variety of computer vision problems.the trend of
text-image multi-modality-based research on image processing has extended to the
medical field. [19] proposed a semantic matching loss that learns medical
knowledge to supplement the disadvantages of clip that cannot capture uncertain
medical semantic meaning. in [2], they trained to increase the similarity
between the image and text by calculating their influence on each other as a
weighted feature. for the segmentation task, lvit [10] generated the positional
characteristics of lesions or target objects as text labels. furthermore, it
proposed a double u-shaped structure consisting of a u-shaped vit that combines
image and text information and a u-shaped cnn that produces a segmentation mask.
however, when combining medical images with non-finegrained text information,
noise can affect the outcome.in this paper, we propose a new text-guided
cross-position attention module (cp am t g ) that combines text and image. in a
medical image, a position attention module (pam) effectively learns subtle
differences among pixels. we utilized pam which calculates the influence among
pixels of an image to capture the association between text and image. to this
end, we converted the global text representation generated from the text encoder
into a form, such as an image feature map, to create keys and values. the image
feature map generated from an image encoder was used as a query. learning the
association between text and image enables us to learn positional information of
targets in an image more effectively than existing models that learned
multi-modality from medical images. cp am t g showed an excellent segmentation
performance in our comprehensive experiments on various medical images, such as
cell, chest x-ray, and magnetic resonance image (mri). in addition, by applying
the proposed technique to the automatic roi setting module for the deep
learning-based diagnosis of sacroiliac arthritis, we confirmed that the proposed
method could be effective when it is used in a practical application of
computer-aided diagnosis.our main contributions are as follows:-we devised a
text-guided cross-position attention module (cp am t g ) that efficiently
combines text information with image feature maps. -we demonstrated the effect
of cp am t g on segmentation for various types of medical images. -for a
practical computer-aided diagnosis system, we confirm the effectiveness of the
proposed method in a deep learning-based sacroiliac arthritis diagnosis system.",5
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,1.0,Introduction,"the spreading digitalisation of pathology labs has enabled the development of
deep learning (dl) tools that can assist pathologists in their daily tasks.
however, supervised dl methods require detailed annotations in whole-slide
images (wsis) which is time-consuming, expensive and prone to inter-observer
disagreements [6]. multiple instance learning (mil) alleviates the need for
detailed annotations and has seen increased adoption in recent years. mil
approaches have proven to work well in academic research on histopathology data
[1,17,29] as well as in commercial applications [26]. most mil methods for
digital pathology employ an attention mechanism as it increases the reliability
of the algorithms, which is essential for successful clinical adoption
[14].domain shift in dl occurs when the data distributions of testing and
training differs [20,34]. this remains a significant obstacle to the deployment
of dl applications in clinical practice [7]. to address this problem previous
work either use domain adaptation when data from the target domain is available
[32], or domain generalisation when the target data is unavailable [34]. domain
adaptation has been explored in the mil setting too [22,23,27]. however, it may
not be feasible to perform an explicit domain adaptation, and an already adapted
model could still experience problems with domain shifts. hence, it is important
to provide indications of the expected performance on a target dataset without
requiring annotations [5,25]. another related topic is out-of-distribution (ood)
detection [33] which aims to detect individual samples that are ood, in contrast
to our objective of estimating a difference of expected performances between
some datasets. for supervised algorithms, techniques of uncertainty estimation
have been used to measure the effect of domain shift [4,15,18] and to improve
the robustness of predictions [19,21,30]. however, the reliability of
uncertainty estimates can also be negatively affected by domain shifts [11,31].
alternatively, a drop in performance can be estimated by comparing the model's
softmax outputs [8] or some hidden features [24,28] acquired on in-domain and
domain shift datasets. although such methods have been demonstrated for
supervised algorithms, as far as we know no previous work has explored domain
shift in the specific context of mil algorithms. hence, it is not clear how well
they will work in such a scenario.in this work, we evaluate an attention-based
mil model on unseen data from a new hospital and propose a way to quantify the
domain shift severity. the model is trained to perform binary classification of
wsis from lymph nodes of breast cancer patients. we split the data from the new
hospital into several subsets to investigate clinically realistic scenarios
triggering different levels of domain shift. we show that our proposed
unsupervised metric for quantifying domain shift correlates best with the
changes in performance, in comparison to multiple baselines. the approach of
validating a mil algorithm in a new site without collecting new labels can
greatly reduce the cost and time of quality assurance efforts and ensure that
the models perform as expected in a variety of settings. the novel contributions
of our work can be summarised as:1. proposing an unsupervised metric named
fréchet domain distance (fdd) for quantifying the effects of domain shift in
attention-based mil; 2. showing how fdd can help to identify subsets of patient
cases for which mil performance is worse than reported on the in-domain test
data; 3. comparing the effectiveness of using uncertainty estimation versus
learnt representations for domain shift detection in mil.",5
Detecting Domain Shift in Multiple Instance Learning for Digital Pathology Using Fréchet Domain Distance,4.1,MIL Training,"we trained, with default settings, 10 clam models to classify wsis of breast
cancer metastases using a 10-fold cross-validation (cv) on the training data.
the test data was kept the same for all 10 models. the classification
performance is evaluated using the area under receiver operating characteristic
curve (roc-auc) and matthews correlation coefficient (mcc) [2]. following the
conclusions of [2] that mcc well represents the full confusion matrix and the
fact that in clinical practice a threshold needs to be set for a classification
decision, mcc is used as a primary metric of performance for domain shift
analysis while roc-auc is reported for completeness. whereas extremely large
variations in label prevalence could reduce the reliability of the mcc metric,
this is not the case here as label prevalence is similar (35-45%) in our test
datasets. for deep ensemble [15] we trained 4 additional clam models for each of
the 10 cv folds.",5
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,1.0,Introduction,"liver cancer is the third leading cause of cancer death world-wide in 2020 [14].
early detection and accurate diagnosis of liver tumors may improve overall
patient outcomes, in which imaging plays a key role [11]. computed tomography
(ct) is one of the most important imaging modalities for liver tumors. dynamic
contrast-enhanced (dce) ct is widely used for diagnostics, but it requires
iodine contrast injection which can cause reaction and potential risks in
patients. recently, non-contrast (nc) ct scans are gaining attention as they are
cheaper and safer to acquire, thus can be potential tools for opportunistic
tumor screening [18,20]. meanwhile, finding and diagnosing tumors in nc cts is
also extremely challenging because of the poor contrast between tumors and
normal tissues compared to those in dce cts. prior works on pancreas [18] and
esophagus [20] have shown that latest deep learning techniques can detect subtle
texture and shape changes in nc ct that even human eyes may miss. thus, we aim
to investigate the performance of liver tumor segmentation and classification in
nc cts. such an approach will be helpful to discover asymptomatic incidental
tumors [12] from routine nc ct scans indicated for general diagnostic purposes
at no additional cost and radiation exposure. after an incidental tumor is
found, the patient may undergo further imaging examination such as a multi-phase
dce ct for differential diagnosis [11], which can provide useful discriminative
information such as the vascularity of lesions and the pattern of contrast agent
enhancement [19]. liver is largest solid organ in body and is the site of many
tumor types [11]. therefore, accurate tumor type classification is important for
the decision of treatment plans and prognosis.many researchers have developed
algorithms to automatically segment [1,9,13,15,23] or classify [19,21,25] liver
tumors in ct to help radiologists improve their accuracy and efficiency. for
example, public datasets such as the liver tumor segmentation benchmark (lits)
[1] fostered a series of works aiming to segment liver tumors with improved
convolutional neural network (cnn) backbones [9,13] and lesion edge information
[15]. lits only has single-phase cts (venous phase). several studies
investigated methods to exploit multi-phase ct by methods such as hetero-phase
fusion [5] and modality-aware mutual learning [23]. there are few work
discussing liver tumor analysis in nc ct [5]. besides lesion segmentation,
cnn-based lesion classification algorithms have been studied to distinguish
common lesion types [19,21,25].in this paper, we build a comprehensive framework
to address both tumor screening and diagnosis. (1) tumor screening involves
finding tumor patients in a large pool of healthy subjects and patients. most
existing works in tumor segmentation and detection did not explicitly consider
it since their training and testing images are all tumor patients. such models
may generate false positives in real-world screening scenario when facing
diverse tumor-free images. we collect a large-scale dataset with both tumor and
non-tumor subjects, where the non-tumor subjects includes not only healthy ones,
but also patients with various diffuse liver diseases such as steatosis and
hepatitis to improve the robustness of the algorithm. (2) most works studied
liver tumor segmentation alone without differentiating tumor types, while a few
works classify liver tumors on cropped tumor patches [19,21,25]. meanwhile, we
learn tumor segmentation and classification with one network using an instance
segmentation framework [3]. we train two networks for nc and multi-phase dce
cts, respectively. (3) for evaluation, previous segmentation works typically use
pixel-level metrics such as dice coefficient. such metrics cannot reflect the
lesion-level accuracy (how many lesion instances are correctly detected and
classified) and may bias to large lesions when a patient has multiple tumors.
patient-level metrics (e.g. classifying whether a subject has malignant tumors)
are also useful for treatment recommendation in clinical practice [18,20].
therefore, we assess our algorithm thoroughly with pixel, lesion, and
patient-level metrics.algorithms for liver tumor segmentation have focused on
improving the feature extraction backbone of a fully-convolutional cnn
[9,13,15,23]. the pixelwise segmentation architectures may not be optimal for
lesion and patient-level evaluation metrics since they cannot consider a lesion
or an image holistically. recently, a series of mask transformer algorithms
[3,4,17] have emerged in the computer vision community and achieved the
state-of-the-art performance in instance segmentation tasks. in brief, they use
object queries to interact with image feature maps and with each other to
produce mask and class predictions for each instance. inspired by them, we
propose a novel end-to-end framework named pixel-lesion-patient network (plan)
for lesion segmentation and classification, as well as patient classification.
it contains three branches with bottomup cooperation: the segmentation map from
the pixel branch helps to initialize the lesion branch, which is an improved
mask transformer aiming to segment and classify each lesion; the patient branch
aggregates information from the whole image and predicts image-level labels of
each lesion type, with regularization terms to encourage consistency with the
lesion branch.we collected a large-scale multi-phase dataset containing 810
non-tumor subjects and 939 tumor patients. 4010 tumor instances of eight types
are extensively annotated based on pathological reports. on the non-contrast
tumor screening and diagnosis task, plan achieves 95.0%, 96.4%, and 0.965 in
patient-level sensitivity, specificity, and average auc for malignant and benign
patients, in contrast to 94.4%, 93.7%, and 0.889 for the widely-used nnu-net
[8]. on multi-phase dce ct, our lesion-level detection precision, recall, and
classification accuracy are 92.2%, 89.0%, 85.9%, outperforming nnu-net [8] and
mask2former [3]. we further conduct a reader study on a holdout set of 250
cases. our algorithm is on par with a senior radiologist (16 yrs experience),
showing the clinical significance of our results. our codes will be made public
upon institutional approval.",5
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,2.2,Pixel-Lesion-Patient Network (PLAN),"our goal is to segment the mask and classify the type of each tumor in a liver
ct. we also hope to make patient-level diagnoses for each ct scan. plan is
inspired by mask2former [3] with three key improvements: (1) a pixel branch is
added to provide anchor queries to the lesion branch. (2) the lesion branch is
composed of the transformer decoder in mask2former, and we improve its
segmentation loss to enhance recall of small lesions. (3) a patient branch is
attached to make dedicated image-level predictions with a proposed
lesion-patient consistency loss. our framework is shown in fig. 1.pixel branch
and anchor queries. the pixel branch is a convolutional layer after the pixel
decoder and learns to predict pixel-wise segmentation maps similar to
traditional segmentators. we do cc analysis to the predicted mask to extract
lesion instances, and then average the pixel embeddings inside each predicted
lesion to obtain a feature vector. the feature vectors are regarded as anchor
queries and work the same way as the randomly initialized queries in the lesion
branch. compared to the random queries in the original mask2former, the anchor
queries contain prior information of the lesions to be segmented, helping the
lesion branch to match with the lesion targets more easily [10].lesion branch
and foreground-enhanced sampling loss. similar to mask2former, the lesion branch
predicts a binary mask and a class label for each query, see fig. 1. mask2former
calculates its segmentation loss on k sampled pixels instead of on the whole
image, which is shown to both improve accuracy and reduce gpu memory usage [3].
however, in lesion segmentation, some tumors are very small compared to the
whole 3d image. the importance sampling strategy [3] can hardly select any
foreground pixels in such cases, so the loss only contains background pixels,
degrading the segmentation recall of small lesions. we propose a simple approach
to remedy this issue by sampling an extra n foreground pixels for each
lesion.patient branch. a patient-level diagnosis is useful for triage. for
example, diagnosing the subject as normal, benign, or malignant will result in
completely different treatments [24]. intuitively, we can also infer
patient-level labels from segmentation results by checking if there is any
lesion in the predicted mask. however, certain tumors are often related to signs
outside the tumor, e.g. hepatocellular carcinoma and cirrhosis,
cholangiocarcinoma and bile duct dilatation, etc. we equip plan with a dedicated
patient branch to aggregate such global information to make better patient-level
prediction. since one patient can have multiple liver tumors of different types,
in our problem, we give each image several hierarchical binary labels. the first
label classifies normal and tumor subjects (whether the image contains any
tumor); the second and third labels indicate the existence of respectively
benign and malignant tumors; the rest c labels suggest the existence of c
fine-grained types of tumors. we employ the dual-path transformer block [17] to
fuse multi-scale features from the pixel encoder and decoder to generate a
feature map, followed by global average pooling and a linear classification
layer to predict the c + 3 labels.a lesion-patient consistency loss is further
proposed to encourage coherence of the lesion and patient-level predictions.
inspired by multi-instance learning [6], we compute a pseudo patient-level
prediction c ∈ r c from the lesion-level predictions by max-pooling the class
probability of each class across all lesion queries (discarding the no-object
class). we also have the probability vector from the patient branch p ∈ r c
corresponding to the c fine-grained classes. then, we compute the l2 loss
between them:the overall loss of plan is listed in eq. 1, where l pixel is the
combined crossentropy (ce) and dice loss for the pixel branch as in nnu-net [8];
l lesion-class is the ce loss [3] for lesion classification in the lesion
branch; l lesion-mask is the combined ce and dice loss [3] for binary lesion
segmentation in the lesion branch with the foreground-enhanced sampling
strategy; l patient is the binary ce loss for the multi-label classification
task in the patient branch.",5
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,3.0,Experiments,"data. our dataset contains 810 normal subjects and 939 patients with liver
tumors. each normal subject has a non-contrast (nc) ct, while each patient has a
dynamic contrast-enhanced (dce) ct scan with nc, arterial, and venous phases. we
use deeds [7] to register nc and arterial phases to the venous phase, and then
invite a senior radiologist with 10 years of experience to annotate on the
multi-phase cts using ct labeler [16]. the 3d mask and the type of all liver
tumors are annotated based on pathological reports and magnetic resonance scans
if necessary. eight tumor types are considered in our study: hepatocellular
carcinoma (hcc), intrahepatic cholangiocarcinoma (icc), metastasis (meta),
hepatoblastoma (hepato), hemangioma (heman), focal nodular hyperplasia (fnh),
cyst, and others (all other tumor types). if a lesion's type cannot be
determined according to image signs [11] and pathology, it will be marked as
""unknown"" and ignored in training and evaluation. in total, 4010 tumor instances
are annotated, whose volumes range from 11 to 3.7×10 6 mm 3 . detailed
statistics and examples of the lesions are shown in the supplementary material.
we train two separate networks for nc and dce cts. in the former setting, both
normal and patient data are used and randomly split into 1149 training, 100
validation, and 500 testing. in the latter one, only patient data are used with
641 training, 100 validation, and 200 testing. another hold-out set of 150
patients and 100 normal cts are used for reader study to compare our accuracy
with two radiologists. implementation details. each ct is resampled to
0.7×0.7×5mm in spacing. we first train an nnu-net on public datasets to segment
liver and surrounding organs (gallbladder, hepatic vein, spleen, stomach, and
pancreas), and then crop the liver region to train plan. to help plan
differentiate liver tumors and other organs, we train the network to segment
both tumors and organs using the predicted organ labels. plan is built on top of
the nnu-net framework [8]. its pixel encoder is a u-net encoder, whereas its
pixel decoder is a light-weight feature pyramid network [3]. the lesion branch
incorporates three transformer decoder blocks with masked attention [3] which
use feature maps of strides 16, 8, 4 from the pixel decoder. the number of
random queries is q = 20; the embedding dimension is m = 64; the number of
sampled pixels is k = 12544 [3], foreground pixels n = 3; the loss weight is 0.1
for the no-object class while 1 for other classes in the lesion branch [3]. the
weights in eq. 1 arewe use the radam optimizer with an initial learning rate of
0.0001. each training batch contains two patches of size 256 × 256 × 24. for dce
ct, the three phases form a 3-channel image as the network input. extensive data
augmentation is applied including random cropping, scaling, flipping, elastic
deformation, and brightness adjustment [8].during training, we first pretrain
the backbone and the pixel branch for 500 epochs, and then train the whole
network for another 500 epochs.patient-level results. this paper has three major
goals: tumor screening in nc ct (classifying a subject as normal or tumor),
preliminary diagnosis in nc ct (predicting the existence of malignant and benign
tumors), and fine-grained diagnosis in dce ct (predicting the existence of 8
tumor types). among the 8 tumor types, hcc, icc, meta, and hepato are malignant;
heman, fnh, and cyst are benign. ""others"" can be either malignant or benign,
thus are excluded in the preliminary diagnosis task. the nc test set contains
198 tumor cases, 202 completely normal cases, and 100 ""hard"" non-tumor cases
which may have larger image noise, artifact, ascites, diffuse liver diseases
such as hepatitis and steatosis. these cases are used to test the robustness of
the model in real-world screening scenario with diverse tumor-free images. we
compare plan with a widely-used strong baseline, nnu-net [8]. the recent mask
transformer, mask2former [3], is also adapted to 3d for comparison. for the
baselines, patient-level labels are inferred from their predicted masks by
counting lesion pixels. as displayed in table 1, plan achieves the best accuracy
on all tasks, especially in nc preliminary diagnosis tasks, which demonstrates
the effectiveness of its dedicated patient branch that can explicitly aggregate
features from the whole image.lesion and pixel-level results. in lesion-level
evaluation, we treat a prediction as a true positive if its overlap with a
ground-truth lesion is >0.2 in dice. lesions smaller than 3 mm in radius are
ignored. as shown in table 2, the pixellevel accuracy of nnu-net and plan are
comparable, but plan's lesion-level accuracy is consistently higher than
nnu-net. in this work, we focus more on patient and lesion-level metrics.
although nc images have low contrast, they can still be used to segment and
classify lesions with ∼ 80% precision, recall, and classification accuracy. it
implies the potential of nc ct, which has been understudied in previous works.
mask2former has higher precision but lower recall in nc ct, especially for small
lesions, while plan achieves the best recall using the foreground-enhanced
sampling loss. both plan and mask2former achieve better classification accuracy,
which illustrates the mask transformer architecture is good at lesion-level
classification.",5
Liver Tumor Screening and Diagnosis in CT with Pixel-Lesion-Patient Network,0.0,Comparison with Radiologists.,"in the reader study, we invited a senior radiologist with 16 years of experience
in liver imaging, and a junior radiologist with 2 years of experience. they
first read the nc ct of all subjects and provided a diagnosis of normal, benign,
or malignant. then, they read the dce scans and provided a diagnosis of the 8
tumor types. we consider patients with only one tumor type in this study. their
reading process is without time constraint. in table 3 and fig. 2, all methods
get good specificity probably because the normal subjects are completely
healthy. our model achieves comparable accuracy with the senior radiologist but
outperforms the junior one by a large margin in sensitivity and classification
accuracy. an ablation study for our method is shown in table 4. it can be seen
that our proposed anchor queries produced by the pixel branch, fes loss, and
lesionpatient consistency loss are useful for the final performance. the
efficacy of the lesion and patient branches has been analyzed above based on the
lesion and patient-level results. due to space limit, we will show the accuracy
for each tumor type and more qualitative examples in the supplementary
material.comparison with literature. in the pixel level, we obtain dice scores
of 77.2% and 84.2% using nc and dce cts, respectively. the current state of the
art (sota) of lits [1] achieved 82.2% in dice using cts in venous phase; [23]
achieved 81.3% in dice using dce ct of two phases. in the lesion level, our
precision and recall are 80.1% and 81.9% for nc ct, 92.2% and 89.0% for dce ct,
at 20% overlap. [25] achieved 83% and 93% for dce ct. sota of lits achieved
49.7% and 46.3% at 50% overlap. [21] classified lesions into 5 classes,
achieving 84% accuracy for dce and 49% for nc ct. we classify lesions into 8
classes with 85.9% accuracy for dce and 78.5% for nc ct. in the patient level,
[5] achieved auc=0.75 in nc ct tumor screening, while our auc is 0.985. in
summary, our results are superior or comparable to existing works.",5
Self-supervised Learning for Endoscopic Video Analysis,1.0,Introduction,"endoscopic operations are minimally invasive medical procedures which allow
physicians to examine inner body organs and cavities. during an endoscopy, a
thin, flexible tube with a tiny camera is inserted into the body through a small
orifice or incision. it is used to diagnose and treat a variety of conditions,
including ulcers, polyps, tumors, and inflammation. over 250 million endoscopic
procedures are performed each year globally and 80 million in the united states,
signifying the crucial role of endoscopy in clinical research and care.a
cardinal challenge in performing endoscopy is the limited field of view which
hinders navigation and proper visual assessment, potentially leading to high
detection miss-rate, incorrect diagnosis or insufficient treatment. these
limitations have fostered the development of computer-aided systems based on
artificial intelligence (ai), resulting in unprecedented performance over a
broad range of clinical applications [10,11,17,[23][24][25]. yet the success of
such ai systems heavily relies on acquiring annotated data which requires
experts of specific knowledge, leading to an expensive, prolonged process. in
the last few years, self-supervised learning (ssl [5][6][7][8]) has been shown
to be a revolutionary strategy for unsupervised representation learning,
eliminating the need to manually annotate vast quantities of data. training
large models on sizable unlabeled data via ssl leads to powerful representations
which are effective for downstream tasks with few labels. however, research in
endoscopic video analysis has only scratched the surface of ssl which remains
largely unexplored.this study introduces masked siamese networks (msns [2]), a
prominent ssl framework, into endoscopic video analysis where we focus on
laparoscopy and colonoscopy. we first experiment solely on public datasets,
cholec80 [32] and polypsset [33], demonstrating performance on-par with the top
results reported in the literature. yet, the power of ssl lies in large data
regimes. therefore, to exploit msns to their full extent, we collect and build
two sizable unlabeled datasets for laparoscopy and colonoscopy with 7, 700
videos (>23m frames) and 14, 000 videos (>2m frames) respectively. through
extensive experiments, we find that scaling the data size necessitates scaling
the model architecture, leading to state-of-the-art performance in surgical
phase recognition of laparoscopic procedures, as well as in polyp
characterization of colonoscopic videos. furthermore, the proposed approach
exhibits robust generalization, yielding better performance with only 50% of the
annotated data, compared with standard supervised learning using the complete
labeled dataset. this shows the potential to reduce significantly the need for
expensive annotated medical data.",5
Self-supervised Learning for Endoscopic Video Analysis,2.0,Background and Related Work,"there exist a wide variety of endoscopic applications. here, we focus on
colonoscopy and laparoscopy, which combined covers over 70% of all endoscopic
procedures. specifically, our study addresses two important common tasks,
described below.cholecystectomy phase recognition. cholecystectomy is the
surgical removal of the gallbladder using small incisions and specialized
instruments. it is a common procedure performed to treat gallstones,
inflammation, or other conditions affecting the gallbladder. phase recognition
in surgical videos is an important task that aims to improve surgical workflow
and efficiency. apart from measuring quality and monitoring adverse event, this
task also serves in facilitating education, statistical analysis, and evaluating
surgical performance. furthermore, the ability to recognize phases allows
real-time monitoring and decision-making assistance during surgery, thus
improving patient safety and outcomes. ai solutions have shown remarkable
performance in recognizing surgical phases of cholecystectomy procedures
[17,18,32]; however, they typically require large labelled training datasets. as
an alternative, ssl methods have been developed [12,28,30], however, these are
early-days methods that based on heuristic, often require external information
and leads to sub-optimal performance. a recent work [27] presented an extensive
analysis of modern ssl techniques for surgical computer vision, yet on
relatively small laparoscopic datasets.optical polyp characterization.
colorectal cancer (crc) remains a critical health concern and significant
financial burden worldwide. optical colonoscopy is the standard of care
screening procedure for preventing crc through the identification and removal of
polyps [3]. according to colonoscopy guidelines, all identified polyps must be
removed and histologically evaluated regardless of their malignant nature.
optical biopsy enables practitioners to remove pre-cancerous adenoma polyps or
leave distal hyperplastic polyps in situ without the need for pathology
examination, by visually predicting histology. however, this technique is highly
dependent on operator expertise [14]. this limitation has motivated the
development of ai systems for automatic optical biopsy, allowing non-experts to
also effectively perform optical biopsy during polyp management. in recent
years, various ai systems have been developed to this end [1,19]. however,
training such automatic optical biopsy systems relies on a large body of
annotated data, while ssl has not been investigated in this context, to the best
of our knowledge.3 self-supervised learning for endoscopy ssl approaches have
produced impressive results recently [5][6][7][8], relying on two key factors:
(i) effective algorithms for unsupervised learning and (ii) training on
large-scale datasets. here, we first describe masked siamese networks [2], our
chosen ssl framework. additionally, we present our large-scale data collection
(see fig. 2). through extensive experiments in sect. 4, we show that training
msns on these substantial datasets unlocks their potential, yielding effective
representations that transfer well to public laparoscopy and colonoscopy
datasets.",5
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,1.0,Introduction,"head and neck (h&n) cancer refers to malignant tumors in h&n regions, which is
among the most common cancers worldwide [1]. survival prediction, a regression
task that models the survival outcomes of patients, is crucial for h&n cancer
patients: it provides early prognostic information to guide treatment planning
and potentially improves the overall survival outcomes of patients [2].
multi-modality imaging of positron emission tomography -computed tomography
(pet-ct) has been shown to benefit survival prediction as it offers both
anatomical (ct) and metabolic (pet) information about tumors [3,4]. therefore,
survival prediction from pet-ct images in h&n cancer has attracted wide
attention and serves as a key research area. for instance, head and neck tumor
segmentation and outcome prediction challenges (hecktor) have been held for the
last three years to facilitate the development of new algorithms for survival
prediction from pet-ct images in h&n cancer [5][6][7].traditional survival
prediction methods are usually based on radiomics [8], where handcrafted
radiomics features are extracted from pre-segmented tumor regions and then are
modeled by statistical survival models, such as the cox proportional hazard
(coxph) model [9]. in addition, deep survival models based on deep learning have
been proposed to perform end-to-end survival prediction from medical images,
where pre-segmented tumor masks are often unrequired [10]. deep survival models
usually adopt convolutional neural networks (cnns) to extract image features,
and recently visual transformers (vit) have been adopted for its capabilities to
capture long-range dependency within images [11,12]. these deep survival models
have shown the potential to outperform traditional survival prediction methods
[13]. for survival prediction in h&n cancer, deep survival models have achieved
top performance in the hecktor 2021/2022 and are regarded as state-of-the-art
[14][15][16]. nevertheless, we identified that existing deep survival models
still have two main limitations.firstly, existing deep survival models are
underdeveloped in utilizing complementary multi-modality information, such as
the metabolic and anatomical information in pet and ct images. for survival
prediction in h&n cancer, existing methods usually use single imaging modality
[17,18] or rely on early fusion (i.e., concatenating multi-modality images as
multi-channel inputs) to combine multi-modality information [11,[14][15][16]19].
in addition, late fusion has been used for survival prediction in other diseases
such as gliomas and tuberculosis [20,21], where multi-modality features were
extracted by multiple independent encoders with resultant features fused.
however, early fusion has difficulties in extracting intra-modality information
due to entangled (concatenated) images for feature extraction, while late fusion
has difficulties in extracting inter-modality information due to fully
independent feature extraction. recently, tang et al. [22] attempted to address
this limitation by proposing a multi-scale non-local attention fusion (mnaf)
block for survival prediction of glioma patients, in which multi-modality
features were fused via non-local attention mechanism [23] at multiple scales.
however, the performance of this method heavily relies on using tumor
segmentation masks as inputs, which limits its generalizability.secondly,
although deep survival models have advantages in performing end-to-end survival
prediction without requiring tumor masks, this also incurs difficulties in
extracting region-specific information, such as the prognostic information in
primary tumor (pt) and metastatic lymph node (mln) regions. to address this
limitation, recent deep survival models adopted multi-task learning for joint
tumor segmentation and survival prediction, to implicitly guide the model to
extract features related to tumor regions [11,16,[24][25][26]. however, most of
them only considered pt segmentation and ignored the prognostic information in
mln regions [11,[24][25][26]. meng et al. [16] performed survival prediction
with joint pt-mln segmentation and achieved one of the top performances in
hecktor 2022. however, this method extracted entangled features related to both
pt and mln regions, which incurs difficulties in discovering the prognostic
information in pt-/mln-only regions.in this study, we design an x-shape
merging-diverging hybrid transformer network (named xsurv, fig. 1) for survival
prediction in h&n cancer. our xsurv has a merging encoder to fuse complementary
anatomical and metabolic information in pet and ct images and has a diverging
decoder to extract region-specific prognostic information in pt and mln regions.
our technical contributions in xsurv are three folds: (i) we propose a
merging-diverging learning framework for survival prediction. this framework is
specialized in leveraging multi-modality images and extracting regionspecific
information, which potentially could be applied to many survival prediction
tasks with multi-modality imaging. (ii) we propose a hybrid parallel
cross-attention (hpca) block for multi-modality feature learning, where both
local intra-modality and global inter-modality features are learned via parallel
convolutional layers and crossattention transformers. (iii) we propose a
region-specific attention gate (rag) block for region-specific feature
extraction, which screens out the features related to lesion regions. extensive
experiments on the public dataset of hecktor 2022 [7] demonstrate that our xsurv
outperforms state-of-the-art survival prediction methods, including the
top-performing methods in hecktor 2022.",6
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.1,Dataset and Preprocessing,"we adopted the training dataset of hecktor 2022 (refer to
https://hecktor.grand-cha llenge.org/), including 488 h&n cancer patients
acquired from seven medical centers [7], while the testing dataset was excluded
as its ground-truth labels are not released. each patient underwent pretreatment
pet/ct and has clinical indicators. we present the distributions of all clinical
indicators in the supplementary materials. recurrence-free survival (rfs),
including time-to-event in days and censored-or-not status, was provided as
ground truth for survival prediction, while pt and mln annotations were provided
for segmentation. the patients from two centers (chum and chuv) were used for
testing and other patients for training, which split the data into 386/102
patients in training/testing sets. we trained and validated models using 5-fold
cross-validation within the training set and evaluated them in the testing
set.we resampled pet-ct images into isotropic voxels where 1 voxel corresponds
to 1 mm 3 . each image was cropped to 160 × 160 × 160 voxels with the tumor
located in the center. pet images were standardized using z-score normalization,
while ct images were clipped to [-1024, 1024] and then mapped to [-1, 1]. in
addition, we performed univariate and multivariate cox analyses on the clinical
indicators to screen out the prognostic indicators with significant relevance to
rfs (p < 0.05).",6
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,3.3,Experimental Settings,"we compared our xsurv to six state-of-the-art survival prediction methods,
including two traditional radiomics-based methods and four deep survival models.
the included traditional methods are coxph [9] and individual coefficient
approximation for risk estimation (icare) [34]. for traditional methods,
radiomics features were extracted from the provided ground-truth tumor regions
and selected by lasso regression. the included deep survival models are deep
multi-task logistic regression and coxph ensemble (deepmtlr-coxph) [14],
transformer-based multimodal networks for segmentation and survival prediction
(tmss) [11], deep multi-task survival model (deepmts) [24], and
radiomics-enhanced deepmts (radio-deepmts) [16]. deepmtlr-coxph, icare, and
radio-deepmts achieved top performance in hecktor 2021 and 2022. for a fair
comparison, all methods took the same preprocessed images and clinical
indicators as inputs. survival prediction and segmentation were evaluated using
concordance index (c-index) and dice similarity coefficient (dsc), which are the
standard evaluation metrics in the challenges [6,7,35].we also performed two
ablation studies on the encoder and decoder separately: (i) we replaced
hpca/hpsa blocks with conv blocks and compared different strategies to combine
pet-ct images. (ii) we removed rag blocks and compared different strategies to
extract pt/mln-related information.",6
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,4.0,Results and Discussion,"the comparison between our xsurv and the state-of-the-art methods is presented
in table 1. our xsurv achieved a higher c-index than all compared methods, which
demonstrates that our xsurv has achieved state-of-the-art performance in
survival prediction of h&n cancer. when radiomics enhancement was adopted in
xsurv and deepmts, our radio-xsurv also outperformed the radio-deepmts and
achieved the highest cindex. moreover, the segmentation results of multi-task
deep survival models (tmss, deepmts, and xsurv) are also reported in table 1.
our xsurv achieved higher dscs than tmss and deepmts, which demonstrates that
our xsurv can locate pt and mln more precisely and this infers that our xsurv
has better learning capability. we attribute these performance improvements to
the use of our proposed merging-diverging learning framework, hpca block, and
rag block, which can be evidenced by ablation studies.the ablation study on the
pet-ct merging encoder is shown in table 2. we found that using pet alone
resulted in a higher c-index than using both pet-ct with early or late fusion.
this finding is consistent with wang et al. [19]'s study, which suggests that
early and late fusion cannot effectively leverage the complementary information
in pet-ct images. as we have mentioned, early and late fusion have difficulties
in extracting intra-and inter-modality information, respectively. our encoder
first adopts conv/hpsa blocks to extract intra-modality information and then
leverages hpca blocks to discover their interactions, which achieved the highest
c-index. for pt and mln segmentation, our encoder also achieved the highest
dscs, which indicates that our encoder also can improve segmentation. in
addition, mnaf blocks [22] were compared and showed poor performance. this is
likely attributed to the fact that leveraging non-local attention at multiple
scales has corrupted local spatial information, which degraded the segmentation
performance and distracted the model from pt and mln regions. to relieve this
problem, in tang et al.'s study [22], tumor segmentation masks were fed into the
model as explicit guidance to tumor regions. however, it is intractable to have
segmentation masks at the inference stage in clinical practice.the ablation
study on the pt-mln diverging decoder is shown in table 3. we found that, even
without adopting ag, using a dual-branch decoder for pt and mln segmentation
resulted in a higher c-index than using a single-branch decoder, which
demonstrates the effectiveness of our diverging decoder design. adopting vanilla
ag [29] or rag in the dual-branch decoder further improved survival prediction.
compared to the vanilla ag, our rag contributed to a larger improvement, and
this enabled our decoder to achieve the highest c-index. in the supplementary
materials, we visualized the attention maps produced by rag blocks, where the
attention maps can precisely locate pt/mln regions and screen out
pt-/mln-related features. for pt and mln segmentation, using a single-branch
decoder for pt-or mln-only segmentation achieved the highest dscs. this is
expected as the model can leverage all its capabilities to segment only one
target. nevertheless, our decoder still achieved the second-best dscs in both pt
and mln segmentation with a small gap.",6
Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer,5.0,Conclusion,"we have outlined an x-shape merging-diverging hybrid transformer network (xsurv)
for survival prediction from pet-ct images in h&n cancer. within the xsurv, we
propose a merging-diverging learning framework, a hybrid parallel
cross-attention (hpca) block, and a region-specific attention gate (rag) block,
to learn complementary information from multi-modality images and extract
region-specific prognostic information for survival prediction. extensive
experiments have shown that the proposed framework and blocks enable our xsurv
to outperform state-of-the-art survival prediction methods on the
well-benchmarked hecktor 2022 dataset.",6
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,1.0,Introduction,"the examination of tissue and cells using microscope (referred to as histology)
has been a key component of cancer diagnosis and prognostication since more than
a hundred years ago. histological features allow visual readout of cancer
biology as they represent the overall impact of genetic changes on cells
[20].the great rise of deep learning in the past decade and our ability to
digitize histopathology slides using high-throughput slide scanners have fueled
interests in the applications of deep learning in histopathology image analysis.
the majority of the efforts, so far, focus on the deployment of these models for
diagnosis and classification [27]. as such, there is a paucity of efforts that
embark on utilizing machine learning models for patient prognostication and
survival analysis (for example, predicting risk of cancer recurrence or expected
patient survival). while prognostication and survival analysis offer invaluable
insights for patient management, biological studies and drug development
efforts, they require careful tracking of patients for a lengthy period of time;
rendering this as a task that requires a significant amount of effort and
funding.in the machine learning domain, patient prognostication can be treated
as a weakly supervised problem, which a model would predict the outcome (e.g.,
time to cancer recurrence) based on the histopathology images. their majority
have utilized multiple instance learning (mil) [8] that is a two-step learning
method. first, representation maps for a set of patches (i.e., small fields of
view), called a bag of instances, are extracted. then, a second pooling model is
applied to the feature maps for the final prediction. different mil variations
have shown superior performances in grading or subtype classification in
comparison to outcome prediction [10]. this is perhaps due to the fact that
mil-based technique do not incorporate patch locations and interactions as well
as tissue heterogeneity which can potentially have a vital role in defining
clinical outcomes [4,26].to address this issue, graph neural networks (gnn) have
recently received more attention in histology. they can model patch relations
[17] by utilizing message passing mechanism via edges connecting the nodes
(i.e., small patches in our case). however, most gnn-based models suffer from
over smoothing [22] which limits nodes' receptive fields [3]. while local
contexts mainly capture cell-cell interactions, global patterns such as immune
cell infiltration patterns and tumor invasion in normal tissue structures (e.g.,
depth of invasion through myometrium in endometrial cancer [1]) could capture
critical information about outcome [10]. hence, locally focused methods are
unable to benefit from the coarse properties of slides due to their high
dimensions which may lead to poor performance.this paper aims to investigate the
potential of extracting fine and coarse features from histopathology slides and
integrating them for risk stratification in cancer patients. therefore, the
contributions of this work can be summarized as: 1) a novel graph-based model
for predicting survival that extracts both local and global properties by
identifying morphological super-nodes; 2) introducing a fine-coarse feature
distillation module with 3 various strategies to aggregate interactions at
different scales; 3) outperforming sota approaches in both risk prediction and
patient stratification scenarios on two datasets; 4) publishing two large and
rare prostate cancer datasets containing more than 220 graphs for active
surveillance and 240 graphs for brachytherapy cases. the code and graph
embeddings are publicly available at https://github.com/pazadimo/all-in 2
related works",6
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.1,Dataset,"we utilize two prostate cancer (pca) datasets to evaluate the performance of our
proposed model. the first set (pca-as) includes 179 pca patients who were
managed with active surveillance (as). radical therapy is considered
overtreatment in these patients, so they are instead monitored with regular
serum prostate-specific antigen (psa) measurements, physical examinations,
sequential biopsies, and magnetic resonance imaging [23]. however, as may be
over-or under-utilized in low-and intermediate-risk pca due to the uncertainty
of current methods to distinguish indolent from aggressive cancers [11].
although majority of patients in our cohort are classified as low-risk based on
nccn guidelines [21], a significant subset of them experienced disease upgrade
that triggered definitive therapy (range: 6.2 to 224 months after diagnosis).the
second dataset (pca-bt) includes 105 pca patients with low to high risk disease
who went through brachytherapy. this treatment involves placing a radioactive
material inside the body to safely deliver larger dose of radiation at one time
[25]. the recorded endpoint for this set is biochemical recurrence with time to
recurrence ranging from 11.7 to 56.1 months. we also utilized the prostate
cancer grade assessment (panda) challenge dataset [7] that includes more than
10,000 pca needle biopsy slides (no outcome data) as an external dataset for
training the encoder of our model.",6
ALL-IN: A Local GLobal Graph-Based DIstillatioN Model for Representation Learning of Gigapixel Histopathology Images With Application In Cancer Risk Assessment,4.2,Experiments,"we evaluate the models' performance in two scenarios utilizing several objective
metrics. implementation details are available in supplementary material.hazard
(risk) prediction. we utilize concordance-index (c-index) that measures the
relative ordering of patients with observed events and un-censored cases
relative to censored instances [2]. using c-index, we compare the quality of
hazard ranking against multiple methods including two mil (deepset [31], amil
[14]) and graph-based (dgc [17] and patch-gcn [10]) models that were utilized
recently for histopathology risk assessment. c-index values are available in
table 1. the proposed model with all strategies outperforms baselines across all
sets and is able to achieve 0.639 and 0.600 on pca-as and pca-bt, while the
baselines, at best, obtain 0.555, and 0.572, respectively. statistical tests
(paired t-test) on c-indices also show that our model is statistically better
than all baselines in pca-as and also superior to all models, except dgc, in
pca-bt. superior performance of our mca policy implies that balanced
exploitation of fine and coarse features with shared weights may provide more
robust contextual information compared to using mixed guided information or
utilizing them independently.patient stratification. the capacity of stratifying
patients into risk groups (e.g., low and high risk) is another criterion that we
employ to assess the utility of models in clinical practice. we evaluate model
performances via kaplan-meier curve [15] (cut-off set as the ratio of patients
with recurrence within 3 years of therapy initiation for pca-bt and the ratio of
upgraded cases for pca-as), logrank test [6] (with 0.05 as significance level),
and median outcome associated with risk groups (table 1 and fig. 2). our model
stratified pca-as patients into high-and low-risk groups with median time to
progression of 36.5 and 131.7 months, respectively. moreover, pca-bt cases
assigned to high-and low-risk groups have median recurrence time of 21.86 and
35.7 months. while none of the baselines are capable of assigning patients into
risk groups with statistical significance, our distillation policies achieve
significant separation in both pca-as and pca-bt datasets; suggesting that
global histo-morphological properties improve patient stratification
performance. furthermore, our findings have significant clinical implications as
they identify, for the first time, highrisk prostate cancer patients who are
otherwise known to be low-risk based on clinico-pathological parameters. this
group should be managed differently from the rest of the low-risk prostate
cancer patients in the clinic. therefore, providing evidence of the predictive
(as opposed to prognostic) clinical information that our model provides. while a
prognostic biomarker provides information about a patient's outcome (without
specific recommendation on the next course of action), a predictive biomarker
gives insights about the effect of a therapeutic intervention and potential
actions that can be taken.ablation study. we perform ablation study (table 2) on
various components of our framework including local nodes, self-supervised
vit-based encoder, and most importantly, super-nodes in addition to fine-coarse
distillation module. although our local-only model is still showing superior
results compared to baselines, this analysis demonstrates that all modules are
essential for learning the most effective representations. we also assess the
impact of our vit on the baselines (full-results in appendix), showing that it
can, on average, improve their performance by an increase of ∼ 0.03 in c-index
for pca-as. however, the best baseline with vit still has poorer performance
compared to our model in both datasets, while the number of parameters (reported
for vit embeddings' size in table 1) in our full-model is about half of this
baseline. achieving higher c-indices in our all model versions indicates the
important role of coarse features and global context in patient risk estimation
in addition to local patterns.",6
DARC: Distribution-Aware Re-Coloring Model for Generalizable Nucleus Segmentation,3.1,Datasets,"the proposed method is evaluated on four datasets, including two h&e stained
image datasets consep [3] and cpm17 [28] and two ihc stained datasets deepliif
[29] and bc-deepliif [29,32]. consep [3] contains 28 training and 14 validation
images, whose sizes are 1000×1000 pixels. the images are extracted from 16
colorectal adenocarcinoma wsis, each of which belongs to an individual patient,
and scanned with an omnyx vl120 scanner within the department of pathology at
university hospitals coventry and warwickshire, uk. cpm17 [28] contains 32
training and 32 validation images, whose sizes are 500 × 500 pixels. the images
are selected from a set of glioblastoma multiforme, lower grade glioma, head and
neck squamous cell carcinoma, and non-small cell lung cancer whole slide tissue
images. deepliif [29] contains 575 training and 91 validation images, whose
sizes are 512 × 512 pixels. the images are extracted from the slides of lung and
bladder tissues. bc-deepliif [29,32] contains 385 training and 66 validation
ki67 stained images of breast carcinoma, whose sizes are 512 × 512 pixels.",6
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,1.0,Introduction,"accurate spatial characterization of tumor immune microenvironment is critical
for precise therapeutic stratification of cancer patients (e.g. via
immunotherapy). currently, this characterization is done manually by individual
pathologists on standard hematoxylin-and-eosin (h&e) or singleplex
immunohistochemistry (ihc) stained images. however, this results in high
interobserver variability among pathologists, primarily due to the large (> 50%)
disagreement among pathologists for immune cell phenotyping [10]. this is also a
big cause of concern for publicly available h&e/ihc cell segmentation datasets
with immune cell annotations from single pathologists. multiplex staining
resolves this issue by allowing different tumor and immune cell markers to be
stained on the same tissue section, avoiding any phenotyping guesswork from
pathologists. multiplex staining can be performed using expensive multiplex
immunofluorescence (mif) or via cheaper multiplex immunohistochemistry (mihc)
assays. mif staining (requiring expensive scanners and highly skilled lab
technicians) allows multiple markers to be stained/expressed on the same tissue
section (no co-registration needed) while also providing the utility to turn
on/off individual markers as needed. in contrast, current brightfield mihc
staining protocols relying on dab (3,3'-diaminobenzidine) alcohol-insoluble
chromogen, even though easily implementable with current clinical staining
protocols, suffer from occlusion of signal from sequential staining of
additional markers. to this effect, we introduce a new brightfield mihc staining
protocol using alcoholsoluble aminoethyl carbazole (aec) chromogen which allows
repeated stripping, restaining, and scanning of the same tissue section with
multiple markers. this requires only affine registration to align the digitized
restained images to obtain non-occluded signal intensity profiles for all the
markers, similar to mif staining/scanning.in this paper, we introduce a new
dataset that can be readily used out-ofthe-box with any artificial intelligence
(ai)/deep learning algorithms for spatial characterization of tumor immune
microenvironment and several other use cases.to date, only two denovo stained
datasets have been released publicly: bci h&e and singleplex ihc her2 dataset
[7] and deepliif singleplex ihc ki67 and mif dataset [2], both without any
immune or tumor markers. in contrast, we release the first denovo mif/mihc
stained dataset with tumor and immune markers for more accurate characterization
of tumor immune microenvironment. we also demonstrate several interesting use
cases: (1) ihc quantification of cd3/cd8 tumor-infiltrating lymphocytes (tils)
via style transfer, (2) virtual translation of cheap mihc stains to more
expensive mif stains, and (3) virtual tumor/immune cellular phenotyping on
standard hematoxylin images.",6
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.0,Dataset,"the complete staining protocols for this dataset are given in the accompanying
supplementary material. images were acquired at 20× magnification at moffitt
cancer center. the demographics and other relevant information for all eight
head-and-neck squamous cell carcinoma patients is given in table 1.",6
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,2.1,Region-of-Interest Selection and Image Registration,"after scanning the full images at low resolution, nine regions of interest
(rois) from each slide were chosen by an experienced pathologist on both mif and
mihc images: three in the tumor core (tc), three at the tumor margin (tm), and
three outside in the adjacent stroma (s) area. the size of the rois was
standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total
surface area of 0.343 mm 2 . hematoxylin-stained rois were first used to align
all the mihc marker images in the open source fiji software using affine
registration. after that, hematoxylin-and dapi-stained rois were used as
references to align mihc and mif rois again using fiji and subdivided into
512×512 patches, resulting in total of 268 co-registered mihc and mif patches
(∼33 co-registered mif/mihc images per patient).",6
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,0.0,(b) Style Transfer:,"this sub-network creates the stylized ihc image using an attention module, given
(1) the input hematoxylin and the mif marker images and (2) the style and its
corresponding marker images. for synthetically generating stylized ihc images,
we follow the approach outlined in adaattn [8]. we use a pre-trained vgg-19
network [12] as an encoder to extract multi-level feature maps and a decoder
with a symmetric structure of vgg-19. we then use both shallow and deep level
features by using adaattn modules on multiple layers of vgg. this sub-network is
used to create a stylized image using the structure of the given hematoxylin
image while transferring the overall color distribution of the style image to
the final stylized image. the generated marker image from the first sub-network
is used for a more accurate colorization of the positive cells against the blue
hematoxylin counterstain/background; not defining loss functions based on the
markers generated by the first sub-network leads to discrepancy in the final
brown dab channel synthesis.for the stylized ihc images with ground truth
cd3/cd8 marker images, we also segmented corresponding dapi images using our
interactive deep learning impartial [9] tool
https://github.com/nadeemlab/impartial and then classified the segmented masks
using the corresponding cd3/cd8 channel intensities, as shown in fig. 4. we
extracted 268 tiles of size 512×512 from this final segmented and co-registered
dataset. for the purpose of training and testing all the models, we extract four
images of size 256 × 256 from each tile due to the size of the external ihc
images, resulting in a total of 1072 images. we randomly extracted tiles from
the lyon19 challenge dataset [14] to use as style ihc images. using these
images, we created a dataset of synthetically generated ihc images from the
hematoxylin and its marker image as shown in fig. 3.we evaluated the
effectiveness of our synthetically generated dataset (stylized ihc images and
corresponding segmented/classified masks) using our generated dataset with the
nuclick training dataset (containing manually segmented cd3/cd8 cells) [6]. we
randomly selected 840 and 230 patches of size 256 × 256 from the created dataset
for training and validation, respectively. nuclick training and validation sets
[6] comprise 671 and 200 patches, respectively, of size 256 × 256 extracted from
lyon19 dataset [14]. lyon19 ihc cd3/cd8 images are taken from breast, colon, and
prostate cancer patients. we split their training set into training and
validation sets, containing 553 and 118 images, respectively, and use their
validation set for testing our trained models. we trained three models including
unet [11], fpn [5], unet++ [15] with the backbone of resnet50 for 200 epochs and
early stopping on validation score with patience of 30 epochs, using binary
cross entropy loss and adam optimizer with learning rate of 0.0001. as shown in
table 2, models trained with our synthetic training set outperform those trained
solely with nuclick data in all metrics.we also tested the trained models on
1,500 randomly selected images from the training set of the lymphocyte
assessment hackathon (lysto) [1], containing image patches of size 299 × 299
obtained at a magnification of 40× from breast, prostate, and colon cancer whole
slide images stained with cd3 and cd8 markers. only the total number of
lymphocytes in each image patch are reported in this dataset. to evaluate the
performance of trained models on this dataset, we counted the total number of
marked lymphocytes in a predicted mask and calculated the difference between the
reported number of lymphocytes in each image with the total number of
lymphocytes in the predicted mask by the model. in table 2, the average
difference value (diffcount) of lymphocyte number for the whole dataset is
reported for each model. as seen, the trained models on our dataset outperform
the models trained solely on nuclick data.",6
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,3.3,Virtual Cellular Phenotyping on Standard Hematoxylin Images,"there are several public h&e/ihc cell segmentation datasets with manual immune
cell annotations from single pathologists. these are highly problematic given
the large (> 50%) disagreement among pathologists on immune cell phenotyping
[10]. in this last use case, we infer immune and tumor markers from the standard
hematoxylin images using again the public deepliif virtual translation module
[2,3]. we train the translation task of deepliif model using the hematoxylin,
immune (cd3) and tumor (panck) markers. sample images/results taken from the
testing dataset are shown in fig. 6.",6
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,4.0,Conclusions and Future Work,"we have released the first ai-ready restained and co-registered mif and mihc
dataset for head-and-neck squamous cell carcinoma patients. this dataset can be
used for virtual phenotyping given standard clinical hematoxylin images, virtual
clinical ihc dab generation with ground truth segmentations (to train
highquality segmentation models across multiple cancer types) created from
cleaner mif images, as well as for generating standardized clean mif images from
neighboring h&e and ihc sections for registration and 3d reconstruction of
tissue specimens. in the future, we will release similar datasets for additional
cancer types as well as release for this dataset corresponding whole-cell
segmentations via impartial https://github.com/nadeemlab/impartial.",6
An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment,0.0,Data use Declaration and Acknowledgment:,"this study is not human subjects research because it was a secondary analysis of
results from biological specimens that were not collected for the purpose of the
current study and for which the samples were fully anonymized. this work was
supported by msk cancer center support grant/core grant (p30 ca008748) and by
james and esther king biomedical research grant (7jk02) and moffitt merit
society award to c. h. chung. it is also supported in part by the moffitt's
total cancer care initiative, collaborative data services, biostatistics and
bioinformatics, and tissue core facilities at the h. lee moffitt cancer center
and research institute, an nci-designated comprehensive cancer center
(p30-ca076292).",6
Detection of Basal Cell Carcinoma in Whole Slide Images,1.0,Introduction,"skin cancer, the most prevalent cancer globally, has seen increasing incidences
over recent decades [1]. it constitutes a third of all cancer diagnoses,
affecting one in five americans [2]. basal cell carcinoma (bcc), comprising 70%
of cases, has surged by 20-80% in the last 30 years, exerting a significant
healthcare strain. timely bcc diagnosis is crucial to avoid complex treatments.
although histological evaluation remains the gold standard for detection [3],
deep learning and computer vision advancements can streamline this process.
scanned traditional histology slides result in whole slide images (wsis) that
can be analyzed by deep learning models, significantly easing the histological
evaluation burden. recent advancements underscore the promise of this approach
[4][5][6]. existing skin cancer detection methods [7][8][9] typically employs
models like inception net and resnet, designed for natural images like those in
the imagenet dataset. the significant variance in pathology and natural images
can compromise these models' accuracy. neural architecture search (nas)
addresses this issue by auto-designing superior models [10][11][12][13],
exploring a vast architecture space. however, current nas methods often overlook
fairness in architecture ranking, impeding the discovery of top-performing
models.in this study, we utilized the nas approach to identify the optimal
network for skin cancer detection. to improve the efficiency and accuracy of the
search, we developed a new framework named sc-net, which focuses on identifying
highly valuable architectures. we observed that conventional nas methods often
overlook fairness ranking during the search, hindering the search for optimal
solutions. our sc-net framework addresses this by ensuring fair training and
precise ranking. the efficacy of sc-net was confirmed by our experimental
results, with our resnet50 achieving 96.2% top-1 accuracy and 96.5% auc,
outperforming baseline methods by 4.8% and 4.7% respectively.figure 1 shows the
proposed framework, which integrates two modules. module (a) extracts the region
of interest (roi) from wsi and generates patches, while module (b) uses optimal
model architecture from nas to analyze features from patches and generate
classifications.",6
Detection of Basal Cell Carcinoma in Whole Slide Images,2.0,Methods,"the proposed method (fig. 2) involves dividing the input wsi into patches for
training a supernet and the search for optimal architectures [14]. section 2.2
provides further details about the supernet. a balanced evolutionary algorithm
is then used to select the optimal structure from the search space, with the
candidate structures' performance evaluated using mini-batch patch data. we
evaluate the searched architectures on the skin cancer dataset.",6
Detection of Basal Cell Carcinoma in Whole Slide Images,3.2,Performance Evaluation,"we validated our algorithm using the curated skin cancer dataset and sc-net as a
supernet, testing both heavy and light models. we performed a search on resnet50
and mobilenetv2 models, compared against original resnet50 (ori resnet50) and
mobilenetv2 (ori mobilenetv2) models as baselines. the resulting models are
denoted as s resnet50 and s mobilenetv2.comparison with related methods. to
ensure a fair comparison on our dataset, we selected several papers in the field
of pathological image analysis, such as [9,22,23], as well as others using the
ua principle, such as [18,24].evaluation metrics. our model was evaluated on: as
shown in table 2, the s resnet50 model outperformed in all metrics, showing
4.8%, 4.5%, 5.4%, 4.7% and 4.7% improvements in accuracy, sensitivity,
specificity, f 1 score, and auc, respectively, over ori resnet50, and surpassing",6
Detection of Basal Cell Carcinoma in Whole Slide Images,4.0,Conclusion and Future Work,"in this paper, we introduce sc-net, a novel nas framework for skin cancer
detection in pathology images. by formulating sc-net as a balanced supernet, we
ensure fair ranking and treatment of all potential architectures. with scnet and
evolutionary search, we obtained optimal architectures, achieving 96.2% top-1
and 96.5% accuracy on a skin cancer dataset, improvements of 4.8% and 4.7% over
baselines. future work will apply our approach to larger datasets for
wider-scale validation.",6
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,1.0,Introduction,"pathology is widely recognized as the gold standard for disease diagnosis [15].
as the demand for intelligently pathological image analysis continues to grow,
an increasing number of researchers have paid attention to this field
[12,14,25]. however, pathological image analysis remains a challenging task due
to the complex and heterogeneous nature [19] of obtained whole slide images
(wsis), as well as their huge gigapixel-level size [20]. to address this issue,
multiple instance learning (mil) [1] is usually applied to formulate
pathological image analysis tasks into weakly supervised learning problems. in
the mil setting, the entire wsi is regarded as a bag and tiled patches are
instances. the primary challenge of mil arises from its weakly supervised
nature, i.e. only the baglevel label for the entire wsi is provided, while
labels for individual patches are usually unavailable. although mil-based
methods have shown impressive potential in solving a wide range of pathological
image analysis tasks including cancer grading and subtype diagnosis [23],
prognosis prediction [18], genotyperelated tasks such as gene mutation
prediction [4], etc., it is still an open question regarding learning an
informative and effective representation of the entire wsi for down-streaming
task based on mil architecture.current mil methods can be broadly categorized
into two types: bag-level mil and instance-level mil. bag-level mil [9,17], also
known as embeddingbased mil, involves converting patches (instances) into
low-dimensional embeddings, which are then aggregated into wsi (bag)-level
representations to conduct the analysis tasks [22]. the aggregator can take
different architectures such as an attention module [7,13], convolutional neural
network (cnn), transformer [16], or graph neural network [10,28]. instance-level
mil [2,8,24], on the other hand, focuses its learning process at the instance
level, and then obtains the bag-level prediction by simply aggregating instance
predictions. bag-level mil incorporates instance embeddings to create a bag
representation, converting the mil into a supervised learning problem.
furthermore, it can extract contextual information and correlations between
instances. nonetheless, bag-level mil needs to learn informative embeddings of
instances and adjust the contributions of these instance embeddings to generate
the bag representation simultaneously, which faces the risk of obtaining a
suboptimal model given the limited training samples in practice. the
instance-level mil, however, faces the problem of noisy labels, which is caused
by the common strategy of assigning the wsi labels to patches and the fact that
there are lots of patches irrelevant to the wsi labels [3,6].considering these
conventional mil methods usually utilize either bag-level or instance-level
supervision, leading to suboptimal performance. in this paper, we format the
instance-level mil as a noisy label learning task and propose to solve it by
designing an instance-level supervision based on the label disambiguation [21].
then we propose to combine bag-level and instance-level supervision to improve
the performance of mil. the bag-level and instance-level supervision can
corporately optimize the instance embedding learning process and welllearned
instance embeddings can facilitate the aggregation module to generate the bag
representation. the co-supervision design also makes the mil to be a multi-task
learning framework, where the bag-level supervision channel works to globally
summarise the wsi for prediction and the instance-level supervision channel can
locally identify key relevant patches. the detailed contributions can be
summarized as follows:1) we propose a novel mil method for pathological image
analysis that leverages a specially-designed residual transformer backbone and
organically integrates both transformer-based bag-level and
label-disambiguation-based instancelevel supervision for performance
enhancement. 2) we develop a label-disambiguation module that leverages
prototypes and confidence bank to tackle the weakly supervised nature of
instance-level supervision and reduce the impact of assigned noisy labels.3) the
proposed framework outperforms state-of-the-art (sota) methods on public
datasets and in a practical clinical task, demonstrating its superiors in wsi
analysis. besides, ablation studies illustrate the superiority of our
co-supervision design compared to using only one type of supervision. 2 method",6
IIB-MIL: Integrated Instance-Level and Bag-Level Multiple Instances Learning with Label Disambiguation for Pathological Image Analysis,3.1,Dataset,"we evaluate our model with three datasets. (1) luad-gm dataset: the objective is
to predict the epidermal growth factor receptor (egfr) gene mutations in
patients with lung adenocarcinoma (luad) using 723 whole slide image (wsi)
slices, where 47% of cases have egfr mutations. (2) tcga-nsclc and tcga-rcc
datasets: cancer type classification is performed using the cancer genome atlas
(tcga) dataset. the tcga-nsclc dataset comprised two subtypes, lung squamous
cell carcinoma (lusc) and lung adenocarcinoma (luad), while the tcga-rcc dataset
included three subtypes: renal chromophobe cell carcinoma (kich), renal clear
cell carcinoma (kirc), and renal papillary cell carcinoma (kirp).",6
Multi-scale Prototypical Transformer for Whole Slide Image Classification,1.0,Introduction,"histopathological images are regarded as the 'gold standard' in the diagnosis of
cancers. with the advent of the whole slide image (wsi) scanner, deep learning
has gained its reputation in the field of computational pathology [1][2][3].
however, wsis are extremely large in the size and lack of pixel-level
annotations, making it difficult to adopt the traditional supervised learning
methods for wsi classification [4].to address this issue, multiple instance
learning (mil) has been successfully applied to the wsi classification task as a
weakly supervised learning problem [5][6][7]. in this context, a wsi is
considered as a bag, and the cropped patches within the slide are the instances
in this bag. however, the lesion regions usually only account for a small
portion of the wsi, resulting in a large number of negative patches. when the
positive and negative instances in the bag are highly imbalanced, the mil models
are prone to incorrectly discriminate these positive instances when using simple
aggregation operations. to this end, several attention-based mil models, such as
abmil [8] and dsmil [9], apply variants of the attention mechanism to re-weight
instance features. thereafter, the recent works develop the transformer-based
architectures to better model long-range instance correlations via
self-attention [10][11][12][13]. however, since the average bag size of a wsi is
more than 8000 at 20 × magnification, it is computationally infeasible to use
the conventional transformer and other stacked self-attention network
architectures in mil-related tasks.recently, prototypical learning is applied in
wsi analysis to identify representative instances in the bag [14]. some works
adopt the k-means clustering on all instances in a bag to obtain k cluster
centers i.e., instance prototypes, and then use these prototypes to represent
the bags [15,16]. these clustering-based mil algorithms can significantly reduce
the redundant instances, and thereby improving the training efficiency for wsi
classification. however, it is different for k-means to specify the cluster
number as well as the initial cluster centers, and different initial values may
lead to different cluster results, thus affecting the performance of mil.
besides, affected by the feature extractor, the clustering-based mil algorithms
may ignore the most important instances that contain critical diagnostic
information. therefore, it is necessary to develop a method that can fully
exploit the potential complementary information between critical instances and
prototypes to improve representation learning of prototypes.on the other hand,
when pathologists analysis the wsis, they always observe the tissues at various
resolutions [17]. inspired by this diagnostic manner, some works use multi-scale
information of wsis to improve diagnostic accuracy. for example, li et al. [9]
adopted a pyramidal concatenation mechanism to fuse the multi-scale features of
wsis, in which the feature vectors of low-resolution patches are replicated and
concatenated with the those of their corresponding high-resolution patches; hou
et al. [18] propose a heterogeneous graph neural network to learn the
hierarchical representation of wsis from a heterogeneous graph, which is
constructed by the feature and spatial-scaling relationship of multi-resolution
patches. however, since the number of patches at each resolution is quite
different, it requires complex pre-processing to spatially align feature vectors
of patches in different resolutions. therefore, it is significant to develop an
efficient and effective patch aggregation strategy to learn multi-scale
information from wsis.in this work, we propose a multi-scale prototypical
transformer (mspt) for wsi classification. the mspt includes two key components:
a prototypical transformer (pt) and a multi-scale feature fusion module (mffm).
the specifically developed pt uses a clustering algorithm to extract instance
prototypes from the bags, and then re-calibrates these prototypes at each scale
with the self-attention mechanism in transformer [19]. mffm is designed to
effectively fuse multi-scale information of wsis, which utilizes the mlp-mixer
[20] to learn effective representations by aggregating the multi-scale
prototypes generated by the pt. the mlp-mixer adopts two types of mlp layers to
allow information communication in different dimensions of data.the
contributions of this work are summarized as follows:1) a novel prototypical
transformer (pt) is proposed to learn superior prototype representation for wsi
classification by integrating prototypical learning into the transformer
architecture. it can effectively re-calibrate the cluster prototypes as well as
reduce the computational complexity of the transformer. 2) a new multi-scale
feature fusion module (mffm) is developed based on the mlp-mixer to enhance the
information communication among phenotypes. it can effectively capture
multi-scale information in wsi to improve the performance of wsi classification.",6
Multi-scale Prototypical Transformer for Whole Slide Image Classification,3.1,Datasets,"to evaluate the effectiveness of mspt, we conducted experiments on two public
dataset, namely camelyon16 [24] and tcga-nsclc. camelyon16 is a wsi dataset for
the automated detection of metastases in lymph node tissue slides. it includes
270 training samples and 129 testing samples. after pre-processing, a total of
2.4 million patches at ×20 magnification, 0.56 million patches at ×10
magnification, and 0.16 million patches at ×5 magnification, with an average of
about 5900, 1400, and 400 patches per bag. the tcga-nsclc dataset includes two
sub-types of lung cancer, i.e., lung squamous cell carcinoma (tgca-lusc) and
lung adenocarcinoma (tcga-luad). we collected a total of 854 diagnostic slides
from the national cancer institute data portal (https:// portal.gdc.cancer.gov).
the dataset yields 4.3 million patches at 20× magnification, 1.1 million patches
at 10× magnification, and 0.30 million patches at 5× magnification with an
average of about 5000, 1200, and 350 patches per bag.",6
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,1.0,Introduction,"contrast-enhanced ultrasound (ceus) as a modality of functional imaging has the
ability to assess the intensity of vascular perfusion and haemodynamics in the
thyroid nodule, thus considered a valuable new approach in the determination of
benign vs. malignant nodules [1]. in practice, ceus video allows the dynamic
observation of microvascular perfusion through intravenous injection of contrast
agents. according to clinical experience, for thyroid nodules diagnosis, there
are two characteristic that are important when analyzing ceus video. 1) dynamic
microvessel perfusion. as shown in fig. 1(a), clinically acquired ceus records
the dynamic relative intensity changes (microvessel perfusion pattern)
throughout the whole examination [2]. 2) infiltrative expansion of microvessel.
many microvessels around nodules are constantly infiltrating and growing into
the surrounding tissue. as shown in fig. 1(b), based on the difference in lesion
size displayed by the two modalities, clinical practice shows that gray us
underestimates the size of lesions, and ceus video overestimates the size of
some lesions [3]. although the radiologist's cognition of microvascular invasive
expansion is fuzzy, they think it may promote diagnosing thyroid nodules [1].
used the spatial feature enhancement for disease diagnosis based on dynamic
ceus. furthermore, by combing the us modality, chen et al. [5] proposed a
domain-knowledge-guided temporal attention module for breast cancer diagnosis.
however, due to artifacts in ceus, sota classification methods often fail to
learn regions where thyroid nodules are prominent (as in appendix fig. a1) [6].
even the sota segmentation methods cannot accurately identify the lesion area
for blurred lesion boundaries, thus, the existing automatic diagnosis network
using ceus still requires manual labeling of pixel-level labels which will lose
key information around the tissues [7]. in particular, few studies have
developed the ceus video based diagnostic model inspired by the dynamic
microvessel perfusion, or these existing methods generally ignore the influence
of microvessel infiltrative expansion. whether the awareness of infiltrative
area information can be helpful in the improvement of diagnostic accuracy is
still unexplored.here, we propose an explanatory framework for the diagnosis of
thyroid nodules based on dynamic ceus video, which considers the dynamic
perfusion characteristics and the amplification of the lesion region caused by
microvessel infiltration. our contributions are twofolds. first, the temporal
projection attention (tpa) is proposed to complement and interact with the
semantic information of microvessel perfusion from the time dimension. second,
we adopt a group of confidence maps instead of binary masks to perceive the
infiltrative expansion area from gray us to ceus of microvessels for improving
diagnosis.",6
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2.0,Method,"the architecture of the proposed framework is shown in fig. 2. the tasks of
lesion area recognition and differential diagnosis are pixel-level and
image-level classifications, and some low-level features of these two tasks can
be shared interactively [8]. we first fed the ceus video i ∈ r c×t ×h×w into the
cross-task feature extraction (cfa) module to jointly generate the features f
iden and f cls for lesion area recognition and differential diagnosis,
respectively. after that, in the temporal-based lesions area recognition (tlar)
module, an enhanced v-net with the tpa is implemented to identify the relatively
clear lesion area which are visible on both gray us and ceus video. because
microvessel invasion expansion causes the tumor size and margin depicted by ceus
video to be larger than that of gray us, we further adopt a group of confidence
maps based on sigmoid alpha functions (saf) to aware the infiltrative area of
microvessels for improving diagnosis. finally, the confidence maps are fused
with f cls and fed into a diagnosis subnetwork based on lightweight c3d [9] to
predict the probability of benign and malignant. in the cfa, we first use the 3d
inception block to extract multi-scale features f muti . the 3d inception block
has 4 branches with cascaded 3d convolutions. multiple receptive fields are
obtained through different branches, and then group normalization and relu
activation are performed to obtain multi-scale features f muti . then, we use
the cross-task feature adaptive unit to generate the features f iden and f cls
required for lesions area recognition and thyroid nodules diagnosis via the
following formula [10]:where ω iden , ω cls are the learnable weights.",6
Thyroid Nodule Diagnosis in Dynamic Contrast-Enhanced Ultrasound via Microvessel Infiltration Awareness,2.2,Microvessel Infiltration Awareness (MIA),"we design a mia module to learn the infiltrative areas of microvessel. the
tumors and margin depicted by ceus may be larger than those depicted by gray us
because of continuous infiltrative expansion. inspired by the continuous
infiltrative expansion, a series of flexible sigmoid alpha functions (saf)
simulate the infiltrative expansion of microvessels by establishing the distance
maps from the pixel to lesion boundary. here, the distance maps [13] are denoted
as the initial probability distribution p d . then, we utilize iterative
probabilistic optimization (ipo) unit to produce a set of optimized probability
maps p = {p 1 , p2 . . . pn } to aware the microvessel infiltration for thyroid
nodules diagnosis. based on saf and ipo, ceus-based diagnosis of thyroid nodules
can make full use of the ambiguous information caused by microvessel
infiltration.",6
Label-Free Nuclei Segmentation Using Intra-Image Self Similarity,3.1,Datasets and Settings,"monuseg. multi-organ nuclei segmentation [16,17] (monuseg) is used to evaluated
our ssimnet. the monuseg dataset consists of 44 h&e stained histopathology
images with 28,846 manually annotated nuclei. with 1000 × 1000 pixel resolution,
these images were extracted from whole slide images from the the cancer genome
atlas (tcga) repository, representing 9 different organs from 44
individuals.cpm17. the cpm17 dataset [24] is also derived from tcga repository.
the training and test set each consisted of 32 images tiles selected and
extracted from a set of non-small cell lung cancer (nsclc), head and neck
squamous cell carcinoma (hnscc), glioblastoma multiforme (gbm) and lower grade
glioma (lgg) tissue images. moreover, each type cancer has 8 tiles and the size
of patch is 500 × 500 or 600 × 600.settings. we compare our ssimnet with several
current unsupervised segmentation methods. we follow the dcgn [15] to conduct
comparison experiments. we crop the image indataset into patches of 256 × 256
pixels for training. all the methods were trained for 150 epochs on monuseg and
200 epochs on cpm17 each time and experimented with an initial learning rate of
5e -5 and a decay of 0.98 per epoch. our experiment repeated ten times on
monuseg dataset and only once on cpm17 dataset for an augmented convenience.
specially for our ssimnet training, we set α = 70% for data purification and λ =
0.9 for loss in training. moreover, we fine tune the network with only five
epochs for each image on test set with optimizer parameter saved in checkpoint.",6
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,1.0,Introduction,"the tumor microenvironment (tme) is comprised of cancer, immune (e.g. b
lymphocytes, and t lymphocytes), stromal, and other cells together with
noncellular tissue components [3,5,24,30]. it is well acknowledged that tumors
evolve in close interaction with their microenvironment. quantitatively
characterizing tme has the potential to predict tumor aggressiveness and
treatment response [3,23,24,30]. different types of lymphocytes such as cd4+
(helper t cells), cd8+ (cytotoxic t cells), cd20+ (b cells), within the tme
naturally interact with tumor and stromal cells. studies [5,9] have shown that
quantifying spatial interplay of these different cell families within the tme
can provide more prognostic/predictive value compared to only measuring the
density of a single biomarker such as tumor-infiltrating lymphocytes (tils)
[3,24]. immunotherapy (io) is the standard treatment for patients with advanced
non-small cell lung cancer (nsclc) [19] but only 27-45% of patients respond to
this treatment [21]. therefore, better algorithms and improved biomarkers are
essential for identifying which cancer patients are most likely to respond to io
in advance of treatment. quantitative features that relate to the complex
spatial interplay between different types of b-and t-cells in the tme might
unlock attributes that are associated with io response. in this study, we
introduce a novel approach called triangular analysis of geographical interplay
of lymphocytes (triangil), representing a unique and interpretable way to
characterize the distribution, and higher-order interaction of various cell
families (e.g., cancerous cells, stromal cells, lymphocyte subtypes) across
digital histopathology slides. we demonstrate the efficacy of triaangil for
characterizing tme in the context of predicting 1) response to io with immune
checkpoint inhibitors (ici), 2) overall survival (os), in patients with nsclc,
and 3) providing novel insights into the spatial interplay between different
immune cell subtype. triangil source code is publicly available at
http://github.com/sarayar/triangil.",6
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,2.0,Previous Related Work and Novel Contributions,"many studies have only looked at the density of a single biomarker (e.g. tils),
to show that a high density of tils is associated with improved patient survival
and treatment response in nsclc [3,24]. other works have attempted to
characterize the spatial arrangement of cells in tme using computational
graphbased approaches. these approaches include methods that connect cells
regardless of their type (1) using global graphs (gg) such as voronoi that
connect all nuclei [2,14], or (2) using cell cluster graphs (ccg) [16] to create
multiple nuclear subgraphs based on cell-to-cell proximity to predict tumor
aggressiveness and patient outcome [16]. others have explored (3) the spatial
interplay between two different cell types [5].one example approach is spatial
architecture of til (spatil) [9] which attempted to characterize the interplay
between immune and cancer cells and has proven to be helpful in predicting the
recurrence in early-stage nsclc. all of these approaches point to overwhelming
evidence that spatial architecture of cells in tme is critical in predicting
cancer outcome. however, these approaches have not been able to exploit
higher-order interactions and dependencies between multiple cell types (> 2),
relationships that might provide additional actionable insights. the
contributions of this work include:(1) triangil is a computational framework
that characterizes the architecture and relationships of different cell types
simultaneously. instead of measuring only simple two-by-two relations between
cells, it seeks to identify triadic spatial relations (hyperedges [18,20] have
shown great capabilities in solving complex problems in the biomedical field,
these tend to be black-box in nature. a key consideration in cancer immunology
is the need for actionable insights into the spatial relationships between
different types of immune cells. not only does triangil provide predictions that
are on par or superior compared to dl approaches, but also provides a way to
glean insights into the spatial interplay of different immune cell types. these
complex interactions enhance our understanding of the tme and will help pave the
way for new therapeutic strategies that leverage these insights.",6
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.1,Dataset,"the cohort employed in this study was composed of pre-treatment tumor biopsy
specimens from patients with nsclc from five centers (two centers for training
(s t ) and three centers for independent validation (s v )). the entire analysis
was carried out using 122 patients in experiment 1 (73 in s t , and 49 in s v )
and 135 patients in experiment 2 (81 in s t , and 54 in s v ). specimens were
analyzed with a multiplexed quantitative immunofluorescence (qif) panel using
the method described in [22]. from each whole slide image, 7 representative
tiles were obtained and used to train the software inform to define background,
tumor and stromal compartments. then, individual cells were segmented based on
nuclear dapi staining and the segmentation performance was controlled by direct
visualization of samples by a trained observer. next, the software was trained
to identify cell subtypes based on marker expression (cd8, cd4, cd20, ck for
tumor epithelial cells and absence of these markers for stromal cells).",6
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,Results:,"the two top predictive triangil features were found to be the number of edges
between stroma and cd4+ cells, and the number of edges between stroma and tumor
cells with more interactions between stromal cells and both cd4+ and tumor cells
being associated with response to io. this finding is concordant with other
studies [13,17,22,27] that stromal tils were significantly associated with
improved os. therefore, triangil approach is not only predictive of treatment
response but more critically it enables biological interpretations that a dl
model might not be able to provide. in s v , this lda classifier was able to
distinguish responders from non-responders to io with au c t ri =0. design: s t
was used to construct a least absolute shrinkage and selection operator (lasso)
[28] regularized cox proportional hazards model [6] using the triangil features,
to obtain risk score for each patient. lasso features are listed in supplemental
table 4. the median risk score in s t was used as a threshold in both s t and s
v to dichotomize patients into low-risk/high-risk categories. kaplan-meier (km)
survival curves [26] were plotted and the model performance was summarized by
hazard ratio (hr), with corresponding (95% confidence intervals (ci)) using the
log-rank test, and harrell's concordance index (c-index) on s v . the c-index
evaluates the correlation between risk predictions and survival times, aiming to
maximize the discrimination between high-risk and low-risk patients [11]. os is
the time between the initiation of io to the death of the patient. the patients
were censored if the date of death was unknown.result: figure 2 presents some
triangil features in a field of view for a patient with long-term survival and
another with short-term survival. more triangular relationships, shorter
triangle edges, and smaller triangles with smaller perimeters are found in the
long-term survival case when analyzing the triadic interactions within
tumor-stroma-cd4, thereby suggesting higher relative presence and closer
interaction of these cell families. figure 3 illustrates the km plots for the
six approaches. we also calculated the concordance index (c-index) for the two
prognostic approaches in s v . the c-index for triangil and gnn methods were
0.64, and 0.63 respectively. therefore, overall triangil worked marginally
better than gnn, with much higher biological interpretability.",6
Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,5.0,Concluding Remarks,"we presented a new approach, triangular analysis of geographical interplay of
lymphocytes (triangil), to quantitatively chartacterize the spatial arrangement
and relative geographical interplay of multiple cell families across
pathological images. compared to previous spatial graph-based methods, triangil
quantifies the spatial interplay between multiple cell families, providing a
more comprehensive portrait of the tumor microenvironment. triangil was
predictive of response after io (n = 122) and also demonstrated a strong
correlation with os in nsclc patients treated with io (n = 135). triangil
outperformed other graph-and dl-based approaches, with the added benefit of
provoding interpretability with regard to the spatial interplay between cell
families. for instance, triangil yielded the insight that more interactions
between stromal cells and both cd4+ and tumor cells appears to be associated
with better response to io. although five cell families were studies in this
work, triangil is flexible and could include other cell types (e.g.,
macrophages). future work will entail larger validation studies and also
evaluation on other use cases.",6
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,1.0,Introduction,"histopathology relies on hematoxylin and eosin (h&e) stained biopsies for
microscopic inspection to identify visual evidence of diseases. hematoxylin has
a deep blue-purple color and stains acidic structures such as dna in cell
nuclei. eosin, alternatively, is red-pink and stains nonspecific proteins in the
cytoplasm and the stromal matrix. pathologists then examine highlighted tissue
characteristics to diagnose diseases, including different cancers. a correct
diagnosis, therefore, is dependent on the pathologist's training and prior
exposure to a wide variety of disease subtypes [30]. this presents a challenge,
as some disease variants are extremely rare, making visual identification
difficult. in recent years, deep learning methods have aimed to alleviate this
problem by designing discriminative frameworks that aid diagnosis [15,28].
segmentation models find applications in spatial identification of different
nuclei types [6]. however, generative modeling in histopathology is relatively
unexplored. generative models can be used to generate histopathology images with
specific characteristics, such as visual patterns identifying rare cancer
subtypes [4]. as such, generative models can be sampled to emphasize each
disease subtype equally and generate more balanced datasets, thus preventing
dataset biases getting amplified by the models [7]. generative models have the
potential to improve the pedagogy, trustworthiness, generalization, and coverage
of disease diagnosis in the field of histology by aiding both deep learning
models and human pathologists. synthetic datasets can also tackle privacy
concerns surrounding medical data sharing. additionally, conditional generation
of annotated data adds even further value to the proposition as labeling medical
images involves tremendous time, labor, and training costs. recently, denoising
diffusion probabilistic models (ddpms) [8] have achieved tremendous success in
conditional and unconditional generation of real-world images [3]. further, the
semantic diffusion model (sdm) demonstrated the use of ddpms for generating
images given semantic layout [27]. in this work, (1) we leverage recently
discovered capabilities of ddpms to design a first-of-its-kind nuclei-aware
semantic diffusion model (nasdm) that can generate realistic tissue patches
given a semantic mask comprising of multiple nuclei types, (2) we train our
framework on the lizard dataset [5] consisting of colon histology images and
achieve state-of-the-art generation capabilities, and (3) we perform extensive
ablative, qualitative, and quantitative analyses to establish the proficiency of
our framework on this tissue generation task.",6
NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models,3.1,Data Processing,"we use the lizard dataset [5] to demonstrate our framework. this dataset
consists of histology image regions of colon tissue from six different data
sources at 20× objective magnification. the images are accompanied by full
segmentation annotation for different types of nuclei, namely, epithelial cells,
connective tissue cells, lymphocytes, plasma cells, neutrophils, and
eosinophils. a generative model trained on this dataset can be used to
effectively synthesize the colonic tumor micro-environments. the dataset
contains 238 image regions, with an average size of 1055 × 934 pixels. as there
are substantial visual variations across images, we construct a representative
test set by randomly sampling a 7.5% area from each image and its corresponding
mask to be held-out for testing. the test and train image regions are further
divided into smaller image patches of 128 × 128 pixels at two different
objective magnifications: (1) at 20×, the images are directly split into 128 ×
128 pixels patches, whereas (2) at 10×, we generate 256 × 256 patches and resize
them to 128 × 128 for training. to use the data exhaustively, patching is
performed with a 50% overlap in neighboring patches. as such, at (1) 20× we
extract a total of 54,735 patches for training and 4,991 patches as a held-out
set, while at (2) 20× magnification we generate 12,409 training patches and 655
patches are held out.",6
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,1.0,Introduction,"radiotherapy, one of the mainstream treatments for cancer patients, has gained
notable advancements in past decades. for promising curative effect, a
high-quality radiotherapy plan is demanded to distribute sufficient dose of
radiation to the planning target volume (ptv) while minimizing the radiation
hazard to organs at risk (oars). to achieve this, radiotherapy plans need to be
manually adjusted by the dosimetrists in a trial-and-error manner, which is
extremely labor-intensive and time-consuming [1,2]. additionally, the quality of
treatment plans might be variable among radiologists due to their different
expertise and experience [3]. consequently, it is essential to develop a robust
methodology to automatically predict the dose distribution for cancer patients,
relieving the burden on dosimetrists and accelerating the radiotherapy
procedure.recently, the blossom of deep learning (dl) has promoted the automatic
medical image processing tasks [4][5][6], especially for dose prediction
[7][8][9][10][11][12][13][14]. for example, nguyen et al. [7] modified the
traditional 2d unet [15] to predict the dose of prostate cancer patients. wang
et al. [10] utilized a progressive refinement unet (prunet) to refine the
predictions from low resolution to high resolution. besides the above unetbased
frameworks, song et al. [11] employed the deeplabv3+ [16] to excavate contextual
information from different scales, thus obtaining accuracy improvements in the
dose prediction of rectum cancer. mahmood et al. [12] utilized a generative
adversarial network (gan)-based method to predict the dose maps of oropharyngeal
cancer. furthermore, zhan et al. [13] designed a multi-organ constraint loss to
enforce the deep model to better consider the dose requirements of different
organs. following the idea of multi-task learning, tan et al. [8] utilized
isodose line and gradient information to promote the performance of dose
prediction of rectum cancer. to ease the burden on the delineation of ptv and
oars, li et al. [17] constructed an additional segmentation task to provide the
dose prediction task with essential anatomical knowledge.although the above
methods have achieved good performance in predicting dose distribution, they
suffer from the over-smoothing problem. these dl-based dose prediction methods
always apply the l 1 or l 2 loss to guide the model optimization which
calculates a posterior mean of the joint distribution between the predictions
and the ground truth [17,18], leading to the over-smoothed predicted images
without important high-frequency details [19]. we display predicted dose maps
from multiple deep models in fig. 1. as shown, compared with the ground truth,
i.e., (5) in fig. 1, the predictions from (1) to (3) are blurred with fewer
high-frequency details, such as ray shapes. these high-frequency features formed
by ray penetration reveal the ray directions and dose attenuation with the aim
of killing the cancer cells while protecting the oars as much as possible, which
are critical for radiotherapy. consequently, exploring an automatic method to
generate high-quality predictions with rich high-frequency information is
important to improve the performance of dose prediction. currently, diffusion
model [20] has verified its remarkable potential in modeling complex image
distributions in some vision tasks [21][22][23]. unlike other dl models, the
diffusion model is trained without any extra assumption about target data
distribution, thus evading the average effect and alleviating the over-smoothing
problem [24]. figure 1 (4) provides an example in which the diffusion-based
model predicts a dose map with shaper and clearer boundaries of ray-penetrated
areas. therefore, introducing a diffusion model to the dose prediction task is a
worthwhile endeavor.in this paper, we investigate the feasibility of applying a
diffusion model to the dose prediction task and propose a diffusion-based model,
called diffdp, to automatically predict the clinically acceptable dose
distribution for rectum cancer patients. specifically, the diffdp consists of a
forward process and a reverse process. in the forward process, the model employs
a markov chain to gradually transform dose distribution maps with complex
distribution into gaussian distribution by progressively adding pre-defined
noise. then, in the reverse process, given a pure gaussian noise, the model
gradually removes the noise in multiple steps and finally outputs the predicted
dose map. in this procedure, a noise predictor is trained to predict the noise
added in the corresponding step of the forward process. to further ensure the
accuracy of the predicted dose distribution for both the ptv and oars, we design
a dl-based structure encoder to extract the anatomical information from the ct
image and the segmentation masks of the ptv and oars. such anatomical
information can indicate the structure and relative position of organs. by
incorporating the anatomical information, the noise predictor can be aware of
the dose constraints among ptv and oars, thus distributing more appropriate dose
to them and generating more accurate dose distribution maps.overall, the
contributions of this paper can be concluded as follows: (1) we propose a novel
diffusion-based model for dose prediction in cancer radiotherapy to address the
over-smoothing issue commonly encountered in existing dl-based dose prediction
methods. to the best of our knowledge, we are the first to introduce the
diffusion model for this task. (2) we introduce a structure encoder to extract
the anatomical information available in the ct images and organ segmentation
masks, and exploit the anatomical information to guide the noise predictor in
the diffusion model towards generating more precise predictions. (3) the
proposed diffdp is extensively evaluated on a clinical dataset consisting of 130
rectum cancer patients, and the results demonstrate that our approach
outperforms other state-of-the-art methods.",6
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.0,Methodology,"an overview of the proposed diffdp model is illustrated in fig. 2, containing
two markov chain processes: a forward process and a reverse process. an image
set of cancer patient is defined as {x, y}, where x ∈ r h ×w ×(2+o) represents
the structure images, ""2"" signifies the ct image and the segmentation mask of
the ptv, and o denotes the total number of segmentation mask of oars. meanwhile,
y ∈ r h ×w ×1 is the corresponding dose distribution map for x. concretely, the
forward process produces a sequence of noisy images {y 0 , y 1 , . . . , y t },
y 0 = y by gradually adding a small amount of noise to y in t steps with the
noise increased at each step and a noise predictor f is constructed to predict
the noise added to y t-1 by treating y t , anatomic information from x and
embedding of step t as input. to obtain the anatomic information, a structure
encoder g is designed to extract the crucial feature representations from the
structure images. then, in the reverse process, the model progressively deduces
the dose distribution map by iteratively denoising from y t using the
well-trained noise predictor.",6
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,3.0,Experiments and Results,"dataset and evaluations. we measure the performance of our model on an in-house
rectum cancer dataset which contains 130 patients who underwent volumetric
modulated arc therapy (vmat) treatment at west china hospital. concretely, for
every patient, the ct images, ptv segmentation, oars segmentations, and the
clinically planned dose distribution are included. additionally, there are four
oars of rectum cancer containing the bladder, femoral head r, femoral head l,
and small intestine. we randomly select 98 patients for model training, 10
patients for validation, and the remaining 22 patients for test. the thickness
of the cts is 3 mm and all the images are resized to the resolution of 256 × 256
before the training procedure.we measure the performance of our proposed model
with multiple metrics. considering dm represents the minimal absorbed dose
covering m% percentage volume of ptv, we involve d 98 , d 2 , maximum dose (d
max ), and mean dose (d mean ) as metrics. besides, the heterogeneity index (hi)
is used to quantify dose heterogeneity [26]. to quantify performance more
directly, we calculate the difference ( ) of these metrics between the ground
truth and the predicted results. more intuitively, we involve the dose volume
histogram (dvh) [27] as another essential metric of dose prediction performance.
when the dvh curves of the predictions are closer to the ground truth, we can
infer higher prediction accuracy.comparison with state-of-the-art methods. to
verify the superior accuracy of our proposed model, we select multiple
state-of-the-art (sota) models in dose prediction, containing unet (2017) [7],
gan (2018) [12], deeplabv3+ (2020) [11], c3d (2021) [9], and prunet (2022) [10],
for comparison. the quantitative comparison results are listed in table . 1
where our method outperforms the existing sotas in terms of all metrics.
specifically, compared with deeplabv3+ with the second-best accuracy in hi
(0.0448) and d 98 (0.0416), the results generated by the proposed are 0.0035 and
0.0014 lower, respectively. as for d 2 and d max , our method gains overwhelming
performance with 0.0008 and 0.0005, respectively. moreover, the paired t-test is
conducted to investigate the significance of the results. the p-values between
the proposed and other sotas are almost all less than 0.05, indicating that the
enhancement of performance is statistically meaningful.besides the quantitative
results, we also present the dvh curves derived by compared methods in fig. 3.
the results are compared on ptv as well as two oars: bladder and small
intestine. compared with other methods, the disparity between the dvh curves of
our method and the ground truth is the smallest, demonstrating the superior
performance of the proposed. furthermore, we display the visualization
comparison in fig. 4. as we can see, the proposed model achieves the best visual
quality with clearer and sharper high-frequency details (as indicated by red
arrows). furthermore, the error map of the proposed is the darkest, suggesting
the least disparity compared with the ground truth.ablation study. to study the
contributions of key components of the proposed method, we conduct the ablation
experiments by 1) removing the structure encoder from the proposed method and
concatenating the anatomical images x and noisy image y t together as the
original input for diffusion model (denoted as baseline); 2) the proposed diffdp
model. the quantitative results are given in table 2. we can clearly see the
performance for all metrics is enhanced with the structure encoder,
demonstrating its effectiveness in the proposed model.",6
DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,4.0,Conclusion,"in this paper, we introduce a novel diffusion-based dose prediction (diffdp)
model for predicting the radiotherapy dose distribution of cancer patients. the
proposed method involves a forward and a reverse process to generate accurate
prediction by progressively transferring the gaussian noise into a dose
distribution map. moreover, we propose a structure encoder to extract anatomical
information from patient anatomy images and enable the model to concentrate on
the dose constraints within several essential organs. extensive experiments on
an in-house dataset with 130 rectum cancer patients demonstrate the superiority
of our method.",6
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,1.0,Introduction,"in the past few years, the development of histopathological whole slide image
(wsi) analysis methods has dramatically contributed to the intelligent cancer
diagnosis [4,10,15]. however, due to the limitation of hardware resources, it is
difficult to directly process gigapixel wsis in an end-to-end framework. recent
studies usually divide the wsi analysis into multiple stages.generally, multiple
instance learning (mil) is one of the most popular solutions for wsi analysis
[14,17,18]. mil methods regard wsi recognition as a weakly supervised learning
problem and focus on how to effectively and efficiently aggregate
histopathological local features into a global representation. several studies
introduced attention mechanisms [9], recurrent neural networks [2] and graph
neural network [8] to enhance the capacity of mil in structural information
mining. more recently, transformer-based structures [13,19] are proposed to
aggregate long-term relationships of tissue regions, especially for large-scale
wsis. these transformer-based models achieved state-of-the-art performance in
sub-type classification, survival prediction, gene mutant prediction, etc.
however, these methods still rely on at least patient-level annotations. in the
networkbased consultation and communication platforms, there is a vast quantity
of unlabeled wsis not effectively utilized. these wsis are usually without any
annotations or definite diagnosis descriptions but are available for
unsupervised learning. in this case, self-supervised learning (ssl) is gradually
introduced into the mil-based framework and is becoming a new paradigm for wsi
analysis [1,11,16]. typically, chen et al. [5] explored and posed a new
challenge referred to as slide-level self-learning and proposed hipt, which
leveraged the hierarchical structure inherent in wsis and constructed multiple
levels of the self-supervised learning framework to learn high-resolution image
representations. this approach enables mil-based frameworks to take advantage of
abundant unlabeled wsis, further improving the accuracy and robustness of tumor
recognition.however, hipt is a hierarchical learning framework based on a greedy
training strategy. the bias and error generated in each level of the
representation model will accumulate in the final decision model. moreover, the
vit [6] backbone used in hipt is originally designed for nature sense images in
fixed sizes whose positional information is consistent. however,
histopathological wsis are scale-varying and isotropic. the positional embedding
strategy of vit will bring ambiguity into the structural modeling. to relieve
this problem, kat [19] built hierarchical masks based on local anchors to
maintain multi-scale relative distance information in the training. but these
masks are manually defined which is not trainable and lacked orientation
information. the current embedding strategy for wsi structural description is
not complete.in this paper, we propose a novel whole slide image representation
learning framework named position-aware masked autoencoder (pama), which
achieves slide-level representation learning by reconstructing the local
representations of the wsi in the patch feature space. pama can be trained
end-to-end from the local features to the wsi-level representation. moreover, we
designed a position-aware cross-attention mechanism to guarantee the correlation
of localto-global information in the wsis while saving computational resources.
the proposed approach was evaluated on a public tcga-lung dataset and an
in-house endometrial dataset and compared with 6 state-of-the-art methods. the
results have demonstrated the effectiveness of the proposed method.the
contribution of this paper can be summarized into three aspects. (1) we propose
a novel whole slide image representation learning framework named position-aware
masked autoencoder (pama). pama can make full use of abundant unlabeled wsis to
learn discriminative wsi representations. (2) we propose a position-aware
cross-attention (paca) module with a kernel reorientation (kro) strategy, which
makes the framework able to maintain the spatial integrity and semantic
enrichment of slide representation during the selfsupervised training. (3) the
experiments on two datasets show our pama can achieve competitive performance
compared with sota mil methods and ssl methods.",6
Position-Aware Masked Autoencoder for Histopathology WSI Representation Learning,0.0,end end end,"tcga-lung dataset is collected from the cancer genome atlas (tcga) data portal.
the dataset includes a total of 3,064 wsis, which consist of three categories,
namely tumor-free (normal), lung adenocarcinoma (luad), and lung squamous cancer
(lusc), endometrial dataset includes 3,654 wsis of endometrial pathology, which
includes 8 categories, namely well/moderately/low-differentiated endometrioid
adenocarcinoma, squamous differentiation carcinoma, plasmacytoid carcinoma,
clear cell carcinoma, mixed-cell adenocarcinoma, and benign tumor.each dataset
was randomly divided into training, validation and test sets according to 6:1:3
while keeping each category of data proportionally. we conducted wsi multi-type
classification experiments on the two datasets. the validation set was used to
perform an early stop. the results of the test set were reported for comparison.",6
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,1.0,Introduction,"cervical cancer is a common and severe disease that affects millions of women
globally, particularly in developing countries [9]. early diagnosis is vital for
successful treatment, which can significantly increase the cure rate [17]. in
recent years, computer-aided diagnosis (cad) methods have become an important
tool in the fight against cervical cancer, as they aim to improve the accuracy
and efficiency of diagnosis.several computer-aided cervical cancer screening
methods have been proposed for whole slide images (wsis) in the literature. most
of them are detectionbased methods, which typically contain a detection model as
well as some postprocessing modules in their frameworks. for instance, zhou et
al. [29] proposed a three-step framework for cervical thin-prep cytologic test
(tct) [12]. the first step involves training a retinanet [13] as a cell
detection network to localize suspiciously abnormal cervical cells from wsis. in
the second step, the patches centered on these detected cells are processed
through a classification model, to refine the judgment of whether they are
positive or negative. finally, the positive patches refined by the patch-level
classification are further combined to produce an overall positive/negative
diagnosis for the wsi at the sample level.some methods improve the final
classification performance by improving the detection model to identify positive
cells more reliably. cao et al. [1] improved the detection performance by
incorporating clinical knowledge and attention mechanism into their cell
detection model of attfpn. wei et al. [24] adopted the yolo [20] architecture
with a variety of convolution kernels of different sizes to accommodate diverse
cell clusters. other methods improve the classification performance by changing
the post-processing modules behind the detection model. cheng et al. [5]
proposed a progressive identification method that leveraged multi-scale visual
cues to identify abnormal cells and then an rnn [27] for sample-level
classification. zhang et al. [28] used gat [23] to model the relation of the
suspicious positive cells provided by detection, thus obtaining a global
description of the wsi and performing sample-level classification.these methods
have achieved good results through continuous improvement on the detection-based
pipeline, but there are some common drawbacks. first, they are not able to get
rid of their reliance on detection models, which means they have a high need for
expensive detection data labeling to train the detection model. cervical cancer
cell detection datasets involve labeling individual and small bounding boxes in
a large number of cells. it often requires multiple experienced pathologists to
annotate [15], which is very time-consuming and labor-intensive. second, the
widely used detection-based pipeline has not fully utilized the massive
information in wsis. a wsi is typically large (sized of about 20000 × 20000
pixels). a lot of data would be wasted if only a small part of annotated images
(e.g., corresponding to positive cells and bounding boxes) was used as training
data. finally, many existing methods focus on detecting and classifying
individual cells. the tendency to neglect effective integration of the overall
information across the entire wsi results in poor performance in sample-level
classification.to address the aforementioned issues, we propose a detection-free
pipeline in this paper, which does not rely on any detection model. instead, our
pipeline requires only sample-level diagnosis labels, which are naturally
available in clinical scenarios and thus get rid of additional image labeling.
to attain this goal, we have designed a two-stage pipeline as in fig. 1. in the
coarse-grained stage, we crop and downsample a wsi into multiple images, and
conduct sample-level classification roughly based on all resized images. the
coarse-grained classification yields attention scores, from which we perform
attention guided selection to localize these key patches from the original wsi.
then, in the fine-grained stage, we use these key patches for fine prediction of
the sample. the two stages in our pipeline adopt the same network design (i.e.,
encoder + pooling trans-former), which makes our solution friendly to develop
and to use. we also adopt contrastive learning to effectively utilize the
massive information in wsis when training the encoder for classification. as a
summary, our pipeline surpasses previous detection-based methods and achieves
state-of-the-art performance with large-scale training. our experiments show
that our method becomes more effective when increasing the data size for
training. moreover, while many pathological images are also based on wsis, our
pipeline has a high potential to extend to other pathological tasks.",6
Detection-Free Pipeline for Cervical Cancer Screening of Whole Slide Images,3.0,Experiment and Results,"dataset and experimental setup. in this study, we have collected 5384 cervical
cytopathological wsi by 20x lens, each with 20000 × 20000 pixels, from our
collaborating hospitals. among them, there 2853 negative samples, and 2531
positive samples (962 ascus, and 1569 high-level positive samples). all wsis
only have diagnosis labels at the sample level, without annotation boxes at the
cell level. and all sample labels are strictly diagnosed according to the tbs
[16] criterion by a pathologist with 35 years of clinical experience. we conduct
the experiment with initial learning rate of 1.0 × 10 -4 , batch size of 4, and
sgd optimizer [21] for 30 epochs each stage. for contrastive pre-training, we
follow the settings of mocov2 [3] and trained for 300 epochs.comparison to sota
methods. in this section, we experiment to compare our method with popular
state-of-the-art (sota) methods, which are all fully supervised and
detection-based. to the best of our knowledge, there are few good methods to
train cervical cancer classification models in weakly supervised or unsupervised
learning ways. no methods can achieve the detection-free goal either.all the
detection-based methods are evaluated in the following way. first, we label a
dataset with cell-level bounding boxes to train a detection model. the detection
dataset has 3761 images and 7623 cell-level annotations. after obtaining the
suspicious cell patches provided by the detection model, we use the subsequent
classification models used in these sota works to classify them and obtain the
final classification results. as shown in table 1 for fair five-fold
cross-validation, our method outperforms all compared detection-based methods.
while our method has a large margin with most methods in the table, the
improvement against [28] (top-ranked in current detection-based methods) is
relatively limited. on one hand, in [28], gat aggregates local patches that are
detected by retinanet. and the attention mechanism of gat is similar with the
transformer used in our pipeline to certain extent. on the other hand, the
result implies that our coarse-grained task has replaced the role of cell
detection in early works. thus, we conclude that a detection model trained with
an expensive annotated dataset is not necessary to build a cad pipeline for
cervical abnormality.ablation study. in this section, we experiment to
demonstrate the effectiveness of all the proposed parts in our pipeline. we
divide all 5384 samples into five independent parts for five-fold
cross-validation, and the results are shown in table 2. here, cg means the
classification passes only the coarse-grained stage. as can be seen, its
performance is low, in that the resized images sacrifices the resolution and
thus perform poorly for image-based classification. fg refers to classifying in
the fine-grained stage. it is worth noting that without the attention scores
provided by the coarse-grained stage, we have no way of knowing which local
images might contain suspicious positive cells. thus, we use random selection to
experiment for fg only, as exhaustively checking all local images is
computationally forbidden. as can be seen, the classification result is the
lowest because it lacks enough access to the key image content in wsis. by
combining the two stages for attention guided selection, it is effective to
improve the classification performance compared to the two previous experiments.
here, for the cases of cg, fg and cg+fg, an original transformer network without
clustering-based pooling is used. in addition, as shown in the last two rows of
the table, both pooling transformer (pt) and unsupervised pretraining (cl)
contribute to our pipeline. ultimately, we combine them together to achieve the
best performance.",6
Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images,1.0,Introduction,"colorectal cancer is a prevalent form of cancer characterized by colorectal
adenocarcinoma, which develops in the colon or rectum's inner lining and
exhibits glandular structures [5]. these glands play a critical role in protein
and carbohydrate secretion across various organ systems. histological
examinations using hematoxylin and eosin staining are commonly conducted by
pathologists to evaluate the differentiation of colorectal adenocarcinoma [15].
the extent of gland formation is a crucial factor in determining tumor grade and
differentiation. accurate segmentation of glandular instances on histological
images is essential for evaluating glandular morphology and assessing colorectal
adenocarcinoma malignancy. however, manual annotation of glandular instances is
a time-consuming and expertise-demanding process. hence, automated methods for
glandular instance segmentation hold significant value in clinical practice.
automated segmentation has been explored using deep learning techniques [21,33],
including u-net [17], fcn [13], siamese network [10,11] and their variations for
semantic segmentation [31]. there are also methods that combine information
bottleneck for detection and segmentation [23]. additionally, twostage instance
segmentation methods like mask r-cnn [7] and blendmask [3] have been utilized,
combining object detection and segmentation sub-networks. however, these methods
may face difficulties in capturing different cell shapes and distinguishing
tightly positioned gland boundaries. limitations arise from image scaling and
cropping, leading to information loss or distortion, resulting in ineffective
boundary recognition and over-/under-segmentation. to overcome these
limitations, we aim to perform gland instance segmentation to accurately
identify the target location and prevent misclassification of background
tissue.recently, diffusion model [9] has gained popularity as efficient
generative models [16]. in the task of image synthesis, diffusion model has
evolved to achieve state-of-the-art performance in terms of quality and mode
coverage compared with gan [32]. furthermore, diffusion model has been applied
to various other tasks [18]. diffusiondet [4] treats the object detection task
as a generative task on the bounding box space in images to handle projection
detection. several studies have explored the feasibility of using diffusion
model in image segmentation [26]. these methods generate segmentation maps from
noisy images and demonstrate better representation of segmentation details
compared to previous deep learning methods.in this paper, we propose a new
method for gland instance segmentation based on the diffusion model. (1) our
method utilizes a diffusion model to perform denoising and tackle the task of
gland instance segmentation in histology images. the noise boxes are generated
from gaussian noise, and the predicted ground truth (gt) boxes and segmentation
masks are performed during the diffusion process. (2) to improve segmentation,
we use instance-aware techniques to recover lost details during denoising. this
includes employing a filter and a multi-scale mask branch to create a global
mask and refine finer segmentation details. (3) to enhance object-background
differentiation, we utilize conditional encoding to augment intermediate
features with the original image encoding. this method effectively integrates
the abundant information from the original image, thereby enhancing the
distinction between the objects and the surrounding background. our proposed
method was trained and tested on the 2015 mic-cai gland segmentation (glas)
challenge dataset [20] and colorectal adenocarcinoma gland (crag) dataset [6]
(as shown in fig. 1), and the experiment results demonstrate the efficacy of the
method. to preserve multi-scale information, we introduce a mask branch that
operates on f mask . by applying convolutions with weights assigned from filters
to f mask , we obtain instance masks.",6
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,1.0,Introduction,"immunohistochemical (ihc) staining is a widely used technique in pathology for
visualizing abnormal cells that are often found in tumors. ihc chromogens
highlight the presence of certain antigens or proteins by staining their
corresponding antibodies. for instance, the her2 (human epidermal growth factor
receptor 2) biomarker is associated with aggressive breast tumor development and
is essential in forming a precise treatment plan. despite its capability to
provide highly valuable diagnostic information, the process of ihc staining is
very labor-intensive, time-consuming and requires specialized histotechnologists
and laboratory equipments [2]. such factors hinder the general availability of
ihc staining in histopathological applications.at the other end of the spectrum,
h&e (hematoxylin and eosin) staining, as the gold standard in histological
staining, highlights the tissue structures and cell morphology. in routine
diagnostics, on account of its much lower cost, an h&e-stained slide is prepared
by pathologists in order to determine whether or not to also apply the ihc
stains for a more precise assessment of the disease. therefore, it is of great
interest to have an algorithm that can automatically translate an h&e-stained
slide into one that could be considered to have been stained with ihc while
accurately predicting the target expression levels.to that end, researchers have
recently proposed to use gan-based image-to-image translation (i2it) algorithms
for transforming h&e-stained slides into ihc. despite the progress, the
outstanding challenge in training such i2it frameworks is the lack of aligned
h&e-ihc image pairs, or in other words, the inconsistencies in the h&e-ihc
groundtruth pairs. to explain, since re-staining a slice is physically
infeasible, a matching pair of h&e-ihc slices are taken from two depth-wise
consecutive cuts of the same tissue then stained and scanned separately. this
inevitably prevents pixel-perfect image correspondences due to the
slice-to-slice changes in cell morphology, staining-induced degradation (e.g.
tissue-tearing), imaging artifacts that may vary among slices (e.g. camera
out-offocus) and multi-slice registration errors. an example pair of patches is
shown in fig. 1 and another pair with significant inconsistencies is shown in
fig. 2(a)(c). in the latter, comparing the groundtruth ihc image to the input
h&e image, one can clearly see the inconsistencies -nearly the entire left half
of the tissue present in the h&e image is missing.as a result, recent advances
in h&e-to-ihc i2it have mostly avoided using the inconsistent gt pairs and
instead have imposed the cycle-consistency constraint [6,8,13]. moreover,
existing approaches have also exploited using expert annotations such as
per-cell labels [9], semantic masks [8] and patch-level labels [8,13]. as for
the prior works that directly utilize the h&e-ihc pairs for supervision, a
variant of pix2pix [4] that uses a gaussian pyramid based reconstruction loss to
accommodate the noisy gt is proposed in [7]. however, the robustness of such
approaches that punish absolute errors in the generated image to dealing with gt
inconsistencies remains unclear.in this paper, we argue that the ihc slides,
despite the disparities vis-a-vis their h&e counterparts, can still serve as
useful targets for stain translation. the work we present in this paper is based
on the important realization that even when pairs of consecutive tissue slices
do not yield images that are pixel-perfect aligned, it is highly likely that the
corresponding patches in the two stains share the same diagnostic label. for
example, if the levels of expression in a region of the her2 slide are high, the
corresponding region in the h&e slide is highly likely to contain a high density
of cancerous cells. therefore, we set our goal to meaningfully leverage such
correlations to benefit the h&e-to-ihc i2it while being resilient to any
inconsistencies.toward this goal, we propose a supervised patchwise contrastive
loss named the adaptive supervised patchnce (asp) loss. our formulation of this
loss was inspired by the recent research findings that contrastive loss benefits
model robustness under label noise [3,12]. furthermore, based on the observation
that any dissimilarity between the patch embeddings at corresponding locations
in the generated and groundtruth ihc images is indicative to the level of
inconsistency of the gt at that location, we employ an adaptive weighting scheme
in asp. by down-weighting the contrastive loss at locations with low
similarities, i.e. high inconsistencies, our proposed asp loss helps the network
learn more robustly.lastly, to support further research in virtual
ihc-restaining, we present the multi-ihc stain translation (mist) as a new
public dataset. the mist dataset contains 4k+ training and 1k testing aligned
h&e-ihc patches for each of the following ihc stains that are critical for
breast cancer diagnostics: her2, ki67, er (estrogen receptor) and pr
(progesterone receptor). we evaluated existing i2it methods and ours for
multiple ihc stains and demonstrate the superior performance achieved by our
method both qualitatively and quantitatively.",6
Adaptive Supervised PatchNCE Loss for Learning H&E-to-IHC Stain Translation with Inconsistent Groundtruth Image Pairs,3.0,Experiments,"datasets. the following datasets are used in our experiments: the breast cancer
immunohistochemical (bci) challenge dataset [7] and our own mist dataset that is
now in the public domain. the publicly available portion of bci contains 3396 h
h&e-her2 patches for training and 500 of the same for testing. note that we have
additionally normalized the brightness levels of all bci images to the same
level. due to the page limit, from the mist dataset, here we only present
detailed results on her2 and er. for mist her2 , we extracted 4642 paired
patches for training and 1000 for testing from 64 wsis. and for mist er , we
extracted 4153 patches for training, and 1000 for testing from 56 wsis. all wsis
were taken at 20× magnification. all patches are of size 1024 × 1024 and
non-overlapping. additional results on mist ki67 and mist pr are provided in the
supplementary materials.implementation details. for all of our models, we used
resnet-6blocks as the generator and a 5-layer patchgan as the discriminator. we
trained our networks with random 512 × 512 crops and a batch size of one. the
adam optimizer [5] was used with a linear decay scheduler (as shown in fig.
3(c)) and an initial learning rate of 2 × 10 -4 . the hyperparameters in eq. (
4) are set as: λ patchnce = 10.0, λ asp = 10.0 and λ gp = 10.0.evaluation
metrics. we compare the methods using both paired and unpaired evaluation
metrics. to compare a pair of images, generated and groundtruth, we use the
standard ssim (structural similarity index measure) and phv (perceptual hash
value) as described in [8]. as for the unpaired metrics, we use the fid (fréchet
inception distance) and the kid (kernel inception distance).qualitative
evaluations. in fig. 4, we compare visually the generated ihc images by our
framework. it can be observed that by using either l sp or l asp , the
pathological representations in the generated images are significantly more
accurate. and by using l asp , such representations appear to be more
consistent. quantitative evaluations. the full results comparing existing i2it
methods to ours are tabulated in tab. 1. overall, it can be observed that the
proposed framework with the asp loss consistently outperforms existing methods
across all three datasets. for those methods, fig. 5 visually illustrates the
extent of hallucinations which we believe is the reason for their poor
quantitative performance. subsequently, in tab.",6
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,1.0,Introduction,"histopathology is considered the gold standard for diagnosing and treating many
cancers [19]. the tissue slices are usually scanned into whole slide images
(wsis) and serve as important references for pathologists. unlike natural
images, wsis typically contain billions of pixels and also have a pyramid
structure, as shown in fig. 1. such gigapixel resolution and expensive
pixel-wise annotation efforts pose unique challenges to constructing effective
and accurate models for wsi analysis. to overcome these challenges, multiple
instance learning (mil) has become a popular paradigm for wsi analysis.
typically, mil-based wsi analysis methods have three steps: (1) crop the huge
wsi into numerous image patches; (2) extract instance features from the cropped
patches; and (3) aggregate instance features together to obtain slide-level
prediction results. many advanced mil models emerged in the past few years. for
instance, abmil [9] and deepattnmil [18] incorporated attention mechanisms into
the aggregation step and achieved promising results. recently, graph-transformer
architecture [17] has been proposed to learn short-range local features through
gnn and long-range global features through transformer simultaneously. such
graph-transformer architecture has also been introduced into wsi analysis
[15,20] to mine the thorough global and local correlations between different
image patches. however, current graph-transformer-based wsi analysis models only
consider the representation learning under one specific magnification, thus
ignoring the rich multi-resolution information from the wsi pyramids.different
resolution levels in the wsi pyramids contain different and complementary
information [3]. the images at a high-resolution level contain cellularlevel
information, such as the nucleus and chromatin morphology features [10]. at a
low-resolution level, tissue-related information like the extent of tumorimmune
localization can be found [1], while the whole wsi describes the entire tissue
microenvironment, such as intra-tumoral heterogeneity and tumor invasion [3].
therefore, analyzing from only a single resolution would lead to an incomplete
picture of wsis. some very recent works proposed to characterize and analyze
wsis in a pyramidal structure. h2-mil [7] formulated wsi as a hierarchical
heterogeneous graph and hipt [3] proposed an inheritable vit framework to model
wsi at different resolutions. whereas these methods only characterize local or
global correlations within the wsi pyramids and use only unidirectional
interaction between different resolutions, leading to insufficient capability to
model the rich multi-resolution information of the wsi pyramids.in this paper,
we present a novel hierarchical interaction graph-transformer framework (i.e.,
higt) to simultaneously capture both local and global information from wsi
pyramids with a novel bidirectional interaction module. specifically, we
abstract the multi-resolution wsi pyramid as a heterogeneous hierarchical graph
and devise a hierarchical interaction graph-transformer architecture to learn
both short-range and long-range correlations among different image patches
within different resolutions. considering that the information from different
resolutions is complementary and can benefit each other, we specially design a
bidirectional interaction block in our hierarchical interaction vit mod- ule to
establish communication between different resolution levels. moreover, a fusion
block is proposed to aggregate features learned from the different levels for
slide-level prediction. to reduce the tremendous computation and memory cost, we
further adopt the efficient pooling operation after the hierarchical gnn part to
reduce the number of tokens and introduce the separable self-attention mechanism
in hierarchical interaction vit modules to reduce the computation burden. the
extensive experiments with promising results on two public wsi datasets from
tcga projects, i.e., kidney carcinoma (kica) and esophageal carcinoma (esca),
validate the effectiveness and efficiency of our framework on both tumor
subtyping and staging tasks. the codes are available at https://
github.com/hku-medai/higt.",6
HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis,3.0,Experiments,"datasets and evaluation metrics. we assess the efficacy of the proposed higt
framework by testing it on two publicly available datasets (kica and esca) from
the cancer genome atlas (tcga) repository. the datasets are described below in
more detail:-kica dataset. the kica dataset consists of 371 cases of kidney
carcinoma, of which 279 are classified as early-stage and 92 as late-stage. for
the tumor typing task, 259 cases are diagnosed as kidney renal papillary cell
carcinoma, while 112 cases are diagnosed as kidney chromophobe. -esca dataset.
the esca dataset comprises 161 cases of esophageal carcinoma, with 96 cases
classified as early-stage and 65 as late-stage. for the tumor typing task, there
are 67 squamous cell carcinoma cases and 94 adenocarcinoma cases.experimental
setup. the proposed framework was implemented by pytorch [14] and pytorch
geometric [5]. all experiments were conducted on a workstation with eight nvidia
geforce rtx 3090 (24 gb) gpus. the shape of all nodes' features extracted by
kimianet is set to 1 × 1024. all methods are trained with a batch size of 8 for
50 epochs. the learning rate was set as 0.0005, with adam optimizer. the
accuracy (acc) and area under the curve (auc) are used as the evaluation metric.
all approaches were evaluated with five-fold cross-validations (5-fold cvs) from
five different initializations. comparison with state-of-the-art methods. we
first compared our proposed higt framework with two groups of state-of-the-art
wsi analysis methods: (1) non-hierarchical methods including: abmil [9], clam-sb
[12], deep-attnmil [18], ds-mil [11], la-mil [15], and (2) hierarchical methods
including: h2-mil [7], hipt [3]. for la-mil [15] method, it was introduced with
a single-scale graph-transformer architecture. for h2-mil [7] and hipt [3], they
were introduced with a hierarchical graph neural network and hierarchical
transformer architecture, respectively. the results for esca and kica datasets
are summarized in table 1 and table 2, respectively. overall, our model achieves
a content result both in auc and acc of classifying the wsi, and especially in
predicting the more complex task (i.e. staging) compared with the sota
approaches. even for the non-hierarchical graph-transformer baseline la-mil and
hierarchical transformer model hipt, our model approaches at least around 3% and
2% improvement on auc and acc in the classification of the staging of the kica
dataset. therefore we believe that our model benefits a lot from its used
modules and mechanisms.ablation analysis. we further conduct an ablation study
to demonstrate the effectiveness of the proposed components. the results are
shown in table 3. in its first row, we replace the raconv+ with the original
version of this operation. and in the second row, we replace the separable self
attention with a canonical transformer block. the third row changes the
bidirectional interaction mechanism into just one direction from region-level to
patch-level. and the last row, we remove the fusion block from our model.
finally, the ablation analysis results show that all of these modules we used
actually improved the prediction effect of the model to a certain extent.
computation cost analysis. we analyze the computation cost during the
experiments to compare the efficiency between our methods and existing
state-ofthe-art approaches. besides we visualized the model size (mb) and the
training memory allocation of gpu (gb) v.s. performance in kica's typing and
staging task plots in fig. 2. all results demonstrate that our model is able to
maintain the promising prediction result while reducing the computational cost
and model size effectively.",6
Learning Robust Classifier for Imbalanced Medical Image Dataset with Noisy Labels by Minimizing Invariant Risk,3.1,Dataset and Evaluation Metric,"we evaluated our approach on two medical image datasets with imbalanced class
distributions and noisy labels. the first dataset, ham10000 [22], is a
dermatoscopic image dataset for skin-lesion classification with 10,015 images
divided into seven categories. it contains a training set with 7,007 images, a
validation set with 1,003 images, and a testing set with 2,005 images. following
the previous noisy label settings [25], we add 20% noise to its training set by
randomly flipping labels. the second dataset, chaoyang [29], is a histopathology
image dataset manually annotated into four cancer categories by three
pathological experts, with 40% of training samples having inconsistent
annotations from the experts. to emulate imbalanced scenarios, we prune the
class sizes of the training set into an imbalanced distribution as [5].
consequently, chaoyang dataset consists of a training set with 2,181 images, a
validation set with 713 images, and a testing set with 1,426 images, where the
validation and testing sets have clean labels. the imbalanced ratios [12] of
ham10000 and chaoyang are 59 and 20, respectively. the evaluation metrics are
macro-f1, b-acc, and mcc.",6
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,1.0,Introduction,"colorectal cancer is the third most common malignant tumor, and nearly half of
all patients with colorectal cancer develop liver metastasis during the course
of the disease [6,16]. liver metastases after surgery of colorectal cancer is
the major cause of disease-related death. colorectal cancer liver metastases
(crlm) have therefore become one of the major focuses in the medical field.
patients with colorectal cancer typically undergo contrast-enhanced computed
tomography (cect) scans multiple times during follow-up visits after surgery for
early detection of crlm, generating a 5d dataset. in addition to the axial,
sagittal, and coronal planes in 3d ct scans, the data comprises
contrast-enhanced multiple phases as its 4th dimension, along with different
timestamps as its 5th dimension. radiologists heavily rely on this data to
detect the crlm in the very early stage [15].extensive existing works have
demonstrated the power of deep learning on various spatial-temporal data, and
can potentially be applied towards the problem of crlm. for example, originally
designed for natural data, several mainstream models such as e3d-lstm [12],
convlstm [11] and predrnn [13] use convolutional neural networks (cnn) to
capture spatial features and long short-term memory (lstm) to process temporal
features. some other models, such as simvp [4], replace lstms with cnns but
still have the capability of processing spatiotemporal information. these models
can be adapted for classification tasks with the use of proper classification
head.however, all these methods have only demonstrated their effectiveness
towards 3d/4d data (i.e., time-series 2d/3d images), and it is not clear how to
best extend them to work with the 5d cect data. part of the reason is due to the
lack of public availability of such data. when extending these models towards 5d
cect data, some decisions need to be made, for example: 1) what is the most
effective way to incorporate the phase information? simply concatenating
different phases together may not be the optimal choice, because the positional
information of the same ct slice in different phases would be lost.2) shall we
use uni-directional lstm or bi-direction lstm? e3d-lstm [12] shows
uni-directional lstm works well on natural videos while several other works show
bi-directional lstm is needed in certain medical image segmentation tasks
[2,7].in this paper, we investigate how state-of-art deep learning models can be
applied to the crlm prediction task using our 5d cect dataset. we evaluate the
effectiveness of bi-directional lstm and explore the possible method of
incorporating different phases in the cect dataset. specifically, we show that
the best prediction accuracy can be achieved by enhancing e3d-lstm [12] with a
bi-directional lstm and a multi-plane structure. when patients undergo cect
scans to detect crlm, typically three phases are captured: the unenhanced plain
scan phase (p), the portal venous phase (v), and the arterial phase (a). the p
phase provides the basic shape of the liver tissue, while the v and a phases
provide additional information on the liver's normal and abnormal blood vessel
patterns, respectively [10]. professional radiologists often combine the a and v
phases to determine the existence of metastases since blood in the liver is
supplied by both portal venous and arterial routes.",6
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,2.1,Dataset,"our dataset follows specific inclusion criteria:-no tumor appears on the ct
scans. that means patients have not been diagnosed as crlm when they took the
scans.-patients were previously diagnosed with colorectal cancer tnm stage i to
stage iii, and recovered from colorectal radical surgery. -patients have two or
more times of cect scans.-we already determined whether or not the patients had
liver metastases within 2 years after the surgery, and manually labeled the
dataset based on this. -no potential focal infection in the liver before the
colorectal radical surgery.-no metastases in other organs before the liver
metastases.-no other malignant tumors.our retrospective dataset includes two
cohorts from two hospitals. the first cohort consists of 201 patients and the
second cohort includes 68 patients. each scan contains three phases and 100 to
200 ct slices with a resolution of 512×512. patients may have different numbers
of ct scans, ranging from 2 to 6, depending on the number of follow-up visits.
ct images are collected with the following acquisition parameters: window width
150, window level 50, radiation dose 120 kv, slice thickness 1 mm, and slice gap
0.8 mm. all images underwent manual quality control to exclude any scans with
noticeable artifacts or blurriness and to verify the completeness of all slices.
additional statistics on our dataset are presented in table 1 and examples of
representative images are shown in fig. 1. the dataset is available upon
request.",6
MPBD-LSTM: A Predictive Model for Colorectal Liver Metastases Using Time Series Multi-phase Contrast-Enhanced CT Scans,3.1,Data Augmentation and Selection,"we selected 170 patients who underwent three or more cect scans from our
original dataset, and cropped the images to only include the liver area, as
shown in fig. 1. among these cases, we identified 49 positive cases and 121
negative cases. to handle the imbalanced training dataset, we selected and
duplicated 60% of positive cases and 20% of negative cases by applying standard
scale jittering (ssj) [5]. for data augmentation, we randomly rotated the images
from -30 • to 30 • and employed mixup [17]. we applied the same augmentation
technique consistently to all phases and timestamps of each patient's data. we
also used spline interpolated zoom (siz) [18] to uniformly select 64 slices. for
each slice, the dimension was 256 × 256 after cropping. we used the a and v
phases of cect for our crlm prediction task since the p phase is only relevant
when tumors are significantly present, which is not the case in our dataset. the
dimension of our final input is (3 × 2 × 64 × 64 × 64), representing (t × p × d
× h × w ), where t is the number of timestamps, p is the number of different
phases, d is the slice depth, h is the height, and w is the width.",6
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,1.0,Introduction,"accurate diagnosis plays an important role in achieving the best treatment
outcomes for people with cancer [1]. identification of cancer biomarkers permits
more granular classification of tumors, leading to better diagnosis, prognosis,
and treatment decisions [2,3]. for many cancers, clinically reliable genomic,
molecular, or imaging biomarkers have not been identified and biomarker
identification techniques (e.g., fluorescence in situ hybridization) have
limitations that can restrict their clinical use. on the other hand,
histological analysis of hematoxylin and eosin (h&e)-stained pathology slides is
widely used in cancer diagnosis and prognosis. however, visual examination of
h&e-stained slides is insufficient for classification of some tumors because
identifying morphological differences between molecularly defined subtypes is
beyond the limit of human detection.the introduction of digital pathology (dp)
has enabled application of machine learning approaches to extract otherwise
inaccessible diagnostic and prognostic information from h&e-stained whole slide
images (wsis) [4,5]. current deep learning approaches to wsi analysis typically
operate at three different histopathological scales: whole slidelevel,
region-level, and cell-level [4]. although cell-level analysis has the potential
to produce more detailed and explainable data, it can be limited by the
unavailability of sufficiently annotated training data. to overcome this
problem, weakly supervised and multiple instance learning (mil) based approaches
have been applied to numerous wsi classification tasks [6][7][8][9][10].
however, many of these models use embeddings derived from tiles extracted using
pretrained networks, and these often fail to capture useful information from
individual cells. here we describe a new embedding extraction method that
combines tile-level embeddings with a cell-level embedding summary. our new
method achieved better performance on wsi classification tasks and had a greater
level of explainability than models that used only tile-level embeddings.",6
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,2.0,Embedding Extraction Scheme,"transfer learning using backbones pretrained on natural images is a common
method that addresses the challenge of using data sets that largely lack
annotation. however, using backbones pretrained on natural images is not optimal
for classification of clinical images [11]. therefore, to enable the use of
large unlabeled clinical imaging data sets, as the backbone of our neural
network we used a resnet50 model [12]. the backbone was trained with the
bootstrap your own latent (byol) method [13] using four publicly available data
sets from the cancer genome atlas (tcga) and three data sets from private
vendors that included healthy and malignant tissue from a range of organs [14].",6
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,3.1,Data,"we tested our feature representation method in several classification tasks
involving wsis of h&e-stained histopathology slides. the number of slides per
class for each classification task are shown in fig. 3. for breast cancer human
epidermal growth factor receptor 2 (her2) prediction, we used data from the
herohe challenge data set [26]. to enable comparison with previous results we
used the same test data set that was used in the challenge [27]. for prediction
of estrogen receptor (er) status, we used images from the tcga-breast invasive
carcinoma (tcga-brca) data set [28] for which the er status was known.for these
two tasks we used artifact-free tiles from tumor regions detected with an
in-house tumor detection model.for breast cancer metastasis detection in lymph
node tissue, we used wsis of h&estained healthy lymph node tissue and lymph node
tissue with breast cancer metastases from the publicly available camelyon16
challenge data set [16,29]. all artifact-free tissue tiles were used.for cell of
origin (coo) prediction of activated b-cell like (abc) or germinal center b-cell
like (gcb) tumors in diffuse large b-cell lymphoma (dlbcl), we used data from
the phase 3 goya (nct01287741) and phase 2 cavalli (nct02055820) clinical
trials, hereafter referred to as ct1 and ct2, respectively. all slides were
h&e-stained and scanned using ventana dp200 scanners at 40× magnification. ct1
was used for training and testing the classifier and ct2 was used only as an
independent holdout data set. for these data sets we used artifact-free tiles
from regions annotated by expert pathologists to contain tumor tissue.",6
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,0.0,Cellular Explainability Method. The cellular average embedding is,"where e ij ∈ r 256 is the cellular embedding extracted from every detected cell
in the tile j i ∈ 1, 2, . . . , n j where n j is the number of cells in the tile
j. this can be rewritten as a weighted average of the cellular embeddingswhere w
i ∈ r 256 are the per cell attention weights that if initialized to 0 result in
the original cellular average embedding. the re-formulation does not change the
result of the forward pass since w i are not all equal. note that the weights
are not learned through training but calculated per cell at inference time to
get the per cell contribution. we computed the gradient of the output category
(of the classification method applied on top of the computed embedding) with
respect to the attention weights w i : grad i = ∂score i /∂w i and visualized
cells that received positive and negative gradients using different
colors.visual example results. examples of our cellular explainability method
applied to weakly supervised tumor detection on wsis from the camelyon16 data
set using a-mil are shown in fig. 6. cells with positive attention gradients
shifted the output towards a classification of tumor and are labeled green.
cells with negative attention gradients are labeled red. when reviewed by a
trained pathologist, cells with positive gradients had characteristics
previously associated with breast cancer tumors (e.g., larger nuclei, more
visible nucleoli, differences in size and shape). conversely, negative cells had
denser chromatin and resembled other cell types (e.g., lymphocytes). these
repeatable findings demonstrate the benefit of using cell-level embeddings and
our explainability method to gain a cell-level understanding of both correct and
incorrect slide-level model predictions (fig. 6). we also applied our
explainability method to coo prediction in dlbcl.in this case, cells with
positive attention gradients that shifted the output towards a classification of
gcb were labeled green and cells with negative attention gradients that shifted
the classification towards abc were labeled red. cells with positive attention
gradients were mostly smaller lymphoid cells with low grade morphology or were
normal lymphocytes, whereas cells with negative attention gradients were more
frequently larger lymphoid cells with high grade morphology (fig. 6).",6
Deep Cellular Embeddings: An Explainable Plug and Play Improvement for Feature Representation in Histopathology,5.0,Conclusions,"we describe a method to capture both cellular and texture feature
representations from wsis that can be plugged into any mil architecture (e.g.,
cnn or xformer-based), as well as into fully supervised models (e.g., tile
classification models). our method is more flexible than other methods (e.g.,
hierarchical image pyramid transformer) that usually capture the hierarchical
structure in wsis by aggregating features at multiple levels in a complex set of
steps to perform the final classification task. in addition, we describe a
method to explain the output of the classification model that evaluates the
contributions of histologically identifiable cells to the slide-level
classification. tilelevel embeddings result in good performance for detection of
tumor metastases in lymph nodes. however, introducing more cell-level
information, using combined embeddings, resulted in improved classification
performance. in her2 and er prediction tasks for breast cancer we demonstrate
that addition of a cell-level embedding summary to tilelevel embeddings can
boost model performance by up to 8%. finally, for coo prediction in dlbcl and
breast cancer metastasis detection in lymph nodes, we demonstrated the potential
of our explainability method to gain insights into previously unknown
associations between cellular morphology and disease biology.",6
Progressive Attention Guidance for Whole Slide Vulvovaginal Candidiasis Screening,2.2,Transformer with Skip Self-Attention (SSA),"we design a novel skip self-attention (ssa) module to fuse discriminative
features of candida from different scales. at a fine-grained level, the hyphae
and spores of candida are usually the basis for judging. yet we need to
distinguish them from easily distorting factors such as contaminants in wsis and
edges of nearby cells. at a coarse-grained level, there is the phenomenon that a
candida usually links multiple host cells and yields a string of them. thus it
is necessary to combine long-range visual cues that span several cells to derive
the decision related to candida.cnn-based methods have achieved excellent
performance in computer-aided diagnosis including cervical cancer [22]. however,
the unique shape and appearance of the candidate incur troubles for cnn-based
classifiers, whose spatial field of view can be relatively limited. in recent
years, vision transformer (vit) has been widely used in visual tasks for its
global attention mechanism [14], sensitivity to shape information in images
[19], and robustness to occlusion [12]. nevertheless, such a transformer can be
hard to train for our task, due to the large image size, huge network
parameters, and huge demand for training data. therefore, to adapt to the shape
and appearance of candida, we propose the ssa module and apply it to vit for
efficient learning. specifically, we use the pre-trained cnn-based encoder to
extract features for each cropped image. the feature maps extracted after the
first layer is considered low-level, which contains fine-grained texture
information. on the contrary, the feature maps extracted from the last layer are
high-level, which represents semantics regarding candida. to combine the low-and
high-level features, we regard the low-level features as queries (q), and the
high-level features as keys (k) and values (v). for each patch in the vit
scheme, the transformer decoder computes the attention between low-and
high-level features and combines them. the class token 'cls' is used for the
final classification. the combined feature maps can offer more representative
information so that the classifier focuses more on different scales to
long-range candida. meanwhile, the extra ssa structure is simple, which causes a
low computation burden.",6
CellGAN: Conditional Cervical Cell Synthesis for Augmenting Cytopathological Image Classification,1.0,Introduction,"cervical cancer accounts for 6.6% of the total cancer deaths in females
worldwide, making it a global threat to healthcare [6]. early cytology screening
is highly effective for the prevention and timely treatment of cervical cancer
[23].nowadays, thin-prep cytologic test (tct) [1] is widely used to screen
cervical cancers according to the bethesda system (tbs) rules [21]. typically
there are five types of cervical squamous cells under tct examinations [5],
including normal class or negative for intraepithelial malignancy (nilm),
atypical squamous cells of undetermined significance (asc-us), low-grade
squamous intraepithelial lesion (lsil), atypical squamous cells that cannot
exclude hsil (asc-h), and high-grade squamous intraepithelial lesion (hsil). the
nilm cells have no cytological abnormalities while the others are manifestations
of cervical abnormality to a different extent. by observing cellular features
(e.g., nucleus-cytoplasm ratio) and judging cell types, pathologists can provide
a diagnosis that is critical to the clinical management of cervical
abnormality.after scanning whole-slide images (wsis) from tct samples, automatic
tct screening is highly desired due to the large population versus the limited
number of pathologists. as the wsi data per sample has a huge size, the idea of
identifying abnormal cells in a hierarchical manner has been proposed and
investigated by several studies using deep learning [3,27,31]. in general, these
solutions start with the extraction of suspicious cell patches and then conduct
patch-level classification. the promising performance of cell classification at
the patch level is critical, which contributes to sample-level diagnosis after
integrating outcomes from many patches in a wsi. however, such a patchlevel
classification task requires a large number of annotated training data. and the
efforts in collecting reliably annotated data can hardly be negligible, which
requires high expertise due to the intrinsic difficulty of visually reading
wsis.to alleviate the shortage of sufficient data to supervise classification,
one may adopt traditional data augmentation techniques, which yet may bring
little improvement due to scarcely expanded data diversity [26]. thus,
synthesizing cytopathological images for cervical cells is highly desired to
effectively augment training data. existing literature on pathological image
synthesis has explored the generation of histopathological images [10,28]. in
cytopathological images, on the contrary, cervical cells can be spatially
isolated from each other, or are highly squeezed and even overlapped. the
spatial relationship of individual cells is complex, adding diversity to the
image appearance of color, morphology, texture, etc. in addition, the
differences between cell types are mainly related to nuanced cellular
attributes, thus requiring fine granularity in modulating synthesized images
toward the expected cell types. therefore, the task to synthesize realistic
cytopathological images becomes very challenging.aiming at augmenting the
performance of cervical abnormality screening, we develop a novel conditional
generative adversarial network in this paper, namely cellgan, to synthesize
cytopathological images for various cell types. we leverage fastgan [16] as the
backbone for the sake of training stability and computational efficiency. to
inject cell type for fine-grained conditioning, a non-linear mapping network
embeds the class labels to perform layer-wise feature modulation in the
generator. meanwhile, we introduce the skip-layer global context (sgc) module to
capture the long-range dependency of cells for precisely modeling their spatial
relationship. we adopt an adversarial learning scheme, where the discriminator
is modified in a projection-based way [20] for matching condi- tional data
distribution. to the best of our knowledge, our proposed cellgan is the first
generative model with the capability to synthesize realistic cytopathological
images for various cervical cell types. the experimental results validate the
visual plausibility of cellgan synthesized images, as well as demonstrate their
data augmentation effectiveness on patch-level cell classification.",6
An Anti-biased TBSRTC-Category Aware Nuclei Segmentation Framework with a Multi-label Thyroid Cytology Benchmark,1.0,Introduction,"thyroid cancer is the most common cancer of the endocrine system, accounting for
2.1% of all malignant cancers [1]. clinically, pathologists rely on the
six-category of ""the bethesda system for reporting thyroid cytopathology""
(tbsrtc) [2,3] to distinguish the cell morphology in the stained cytopathologic
sections. the emergence of computational pathology allows automatic diagnosis of
thyroid cancer, and nuclei segmentation becomes one of the most critical
diagnostic tasks [4,5], as the shapes of nuclei, whether round, oval, or
elongated, can provide valuable information for further analysis [6]. for
example, small and scattered thyroid cells with a light hue and relatively low
cell density are usually low-grade and indicative of early-stage cancer; whereas
large and dark cells with extreme-dense agglomeration are usually middle-or
late-grade [3]. correspondingly, accurate location of cell boundaries is
essential for both pathologists and computer-aided diagnosis (cad) systems to
assist decision [7].however, nuclei segmentation in thyroid cytopathology is
still challenged by the varying cellularity of images from different tbsrtc
categories [3,8]. for example, benign cells (i & ii) present high sparsity and
are difficult to be distinguished from background tissues, thus may account for
a relatively small proportion when equal images are involved in a training set
[3]. by contrast, high-grade cells (v & vi) are densely packed and severely
clustered, thus much more are presented in a training set. in this way, an
unbalanced distribution across different categories resulted, correspondingly,
the training leads to biased models with lower accuracy [9,10]. such distinct
morphological differences can be characterized by the tbsrtc category, which
thus inspires us to utilize the handy image-wise grading labels to guide the
nuclei segmentation model learning from unbalanced datasets. we also noticed
that another challenge for accurate nuclei identification is the heavy reliance
on large-scale high-quality annotations [11]. moreover, amongst multiple
annotation paradigms [12], pixellevel labeling is the most time-consuming and
laborious, whereas the image-wise diagnostic labels, i.e. tbsrtc categories, are
comparatively simpler. despite the labeling intensity, prevalent nuclei
segmentation methods, e.g., cia-net [13], ca 2.5 -net [14], and clusterseg [15],
are limited to pixel-wise annotations, where the potential benefits of
integrating accessible image-wise labels are unaware.to narrow the gap
discussed, we propose a novel tbsrtc-category-aware nuclei segmentation
framework. our contributions are three-fold. (1) we propose a cytopathology
nuclei segmentation network named tcsegnet, to provide supplementary guidance to
facilitate the learning of nuclei boundaries. innovatively, our approach can
help reduce bias in the learning process of the segmentation model with the
routine unbalanced training set. (2) we expand tcsegnet to semi-tcsegnet to
leverage image-wise labels in a semi-supervised learning manner, which
significantly reduces the reliance on annotation-intensive pixel-wise labels.
additionally, an hsv-intensity noise is designed specifically for cytopathology
images to boost the generalization ability. (3) we establish a dataset of
thyroid cytopathology image patches of 224 × 224, where 4,965 image labels are
provided following tbsrtc, and 1,473 of them are densely annotated [3] (to be on
github upon acceptance). to the best of our knowledge, it is the first
publicized thyroid cytopathology dataset of both image-wise and pixel-wise
labels. the annotated dataset well alleviates the insufficiency of an open
cytopathology dataset for computer-assisted analysis (fig. 1).",6
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,1.0,Introduction,"breast cancer (bc) is one of the most common malignant tumors in women worldwide
and it causes nearly 0.7 million deaths in 2020 [26]. the pathological process
is usually the golden standard approach for bc diagnosis, which relies on
leveraging diverse complementary information from multi-modal data. in addition
to obtaining the histological characteristics of tumors from hematoxylin and
eosin (h&e) staining images, immunohistochemical (ihc) staining images are also
widely used for pathological diagnoses, such as the human epidermal growth
factor receptor 2 (her2), the estrogen receptor (er), and the progesterone
receptor (pr) [22]. with the development of deep learning, there are a lot of
multi-modal fusion methods for cancer diagnosis [6,7,20,21].recently, with the
development of transformer, multi-modal pre-training has achieved great success
in the fields of computer vision (cv) and natural language processing (nlp).
according to the data format, there are two main multi-modal pre-training
approaches, as shown in fig. 1. one is based on isomorphic data, such as
vision-language pre-training [5] and vision-speech-text pre-training [3]. the
other is based on heterogeneous data. bachmann et al. [2] proposed multi-mae to
pre-train models with intensity images, depth images, and segmentation maps. in
the field of medical image analysis, it is widely recognized that using
multi-modal data can produce more accurate diagnoses than using single-modal
data. however, the development of multi-modal pre-training methods has been
limited due to the scarcity of paired multi-modal data. most methods focus on
chest x-ray vision-language pre-training [8,11]. to our best knowledge, there is
no work for multi-modal pre-training based on pathological heterogeneous data.in
this paper, we propose a multi-modal pre-training method based on masked
autoencoders for bc downstream tasks. our model consists of three parts, i.e.,
the modal-fusion encoder, the mixed attention, and the modal-specific decoder.
we choose paired h&e and ihc (only her2) staining images, which are cropped into
non-overlapped patches as the input of our model. we randomly mask some patches
by a ratio and feed the remaining patches into the modalfusion encoder to get
corresponding tokens. then the mixed attention module is used to take the
intra-modal and inter-modal correlation into account. finally, we use
modal-specific decoders to reconstruct the original h&e and ihc staining images
respectively. our contributions are summarized as follows:we propose a
multi-modal pre-training via masked autoencoders mmp-mae for bc diagnosis. to
our best knowledge, this is the first pre-training work based on multi-modal
pathological data. we evaluate the proposed method on two public datasets as
herohe challenge and bci challenge, which shows that our method achieves
state-of-theart performance. i=1 and {yi} λ 2 n i=1 into the modal-fusion
encoder to extract the patch tokens {fi} λ 1 n i=1 and {gi} λ 2 n i=1 . then we
use intra-modal attention and inter-modal attention to take patch correlation
into account. x and y are reconstructed by modal-specific decoders respectively.",6
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,3.1,Datasets,"acrobat challenge. the automatic registration of breast cancer tissue (acrobat)
challenge [27] provides h&e wsis and matched ihc wsis (er, pr, her2, and ki67),
which consists of 750 training cases, 100 validation cases, and 300 testing
cases. we choose paired h&e and her2 wsis for pre-training. we extract the key
points and descriptors from paired wsis using sift [18] and superpoint [10].
then the extracted key points and descriptors are matched using ransac [13] and
superglue [25]. we repeat this procedure several times on the rotated,
downsampled, or transformed moving wsi to fetch the best transformation based on
mean squared error (mse) loss between source and target wsis' descriptors. after
that, the selected transformation is optimized across different levels of wsis
by gradient descent with local normalized cross-correlation (ncc) as its cost
function. in the final phase of nonrigid registration, we use the optimized
transformation to get the initial displacement field, which is optimized across
different levels of wsis by gradient update. the loss function of which is the
weighted sum of ncc and diffusive regularization. we resize the displacement
field and apply it to the original moving wsi. after all the wsi pairs are well
registered, we convert the padded h&e image to grayscale and apply median blur
to it. next, the otsu threshold is applied to extract the foreground area, which
is cropped into non-overlapping 256 × 256 images. finally, all the chosen images
(around 0.35 million) from wsi in the same pair are saved for mmp-mae
pre-training.bci challenge. breast cancer immunohistochemical image generation
challenge [16] consists of 3896 pairs of images for training and 977 pairs for
testing, which are used to generate her2 images based on h&e images.",6
Multi-modal Pathological Pre-training via Masked Autoencoders for Breast Cancer Diagnosis,3.3,Method Comparison,"her2 staining image generation. three methods on bci datasets are compared in
our experiments, as shown in table 1. cyclegan is a representative unsupervised
method, which doesn't need paired images for training. so cyclegan focuses more
on style transformation, and it is difficult to match the cell-level information
in detail. pix2pix and pyramid pix2pix use paired data, which obtain better
results than cyclegan. pyramid pix2pix uses the multiscale constraint, which
performs better than pix2pix. our method is based on the framework of pyramid
pix2pix and we replace the generator with our pretrained encoder and a
lightweight decoder. our mmp-mae further improves the performance, which
achieves higher psnr by 1.60, and ssim by 0.007. the visualization on the
acrobat dataset also shows our model could learn the modality-related
information, as shown in fig. 5.her2 status prediction. we compare our method
with the top five methods reported in herohe challenge review [9]. most of these
methods use the multinetwork ensemble strategy and extra datasets. team macaroon
uses the came-lyon dataset [4] for tumor classification. team mitel uses bach
dataset [1] for tumor classification. team piaz and dratur both use a
multi-network ensemble strategy to improve their performances. team irisai first
segment the tumor area and then predict the her2 status. mmp-mae still achieves
competitive results by using a single pre-trained model, which is shown in table
2. our model improves and f1-score by 6%. the results show our model
pre-training has the ability to predict status from one modality.",6
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,1.0,Introduction,"automatic segmentation of tumor lesions from pathological images plays an
important role in accurate diagnosis and quantitative evaluation of cancers.
recently, deep learning has achieved remarkable performance in pathological
image segmentation when trained with a large and well-annotated dataset
[6,13,20]. however, obtaining dense annotations for pathological images is
challenging and time-consuming, due to the extremely large image size (e.g.,
10000 × 10000 pixels), scattered spatial distribution, and complex shape of
lesions.semi-supervised learning (ssl) is a potential technique to reduce the
annotation cost via learning from a limited number of labeled data along with a
large amount of unlabeled data. existing ssl methods can be roughly divided into
two categories: consistency-based [9,14,23] and pseudo label-based [2] methods.
the consistency-based methods impose consistency constraints on the predictions
of an unlabeled image under some perturbations. for example, mean teacher
(mt)-based methods [14,23] encourage consistent predictions between a teacher
and a student model with noises added to the input. xie et al. [21] introduced a
pairwise relation network to exploit semantic consistency between each pair of
images in the feature space. luo et al. [9] proposed an uncertainty rectified
pyramid consistency between multi-scale predictions. jin et al. [7] proposed to
encourage the predictions of auxiliary decoders and a main decoder to be
consistent under perturbed hierarchical features. pseudo label-based methods
typically generate pseudo labels for labeled images to supervise the network
[4]. since using a model's prediction to supervise itself may over-fit its bias,
chen et al. [2] proposed cross pseudo supervision (cps) where two networks learn
from each other's pseudo labels generated by argmax of the output prediction.
mc-net+ [19] utilized multiple decoders with different upsampling strategies to
obtain slightly different outputs, and each decoder's probability output was
sharpened to serve as pseudo labels to supervise the others. however, the pseudo
labels are not accurate and contain a lot of noise, using argmax or sharpening
operation will lead to over-confidence of potentially wrong predictions, which
limits the performance of the models. additionally, some related works advocated
the entropy-minimization methods. typical entropy minimization (em) [15] that
aims to reduce the uncertainty or entropy in a system. wu et al. [17] directly
applied entropy minimization to the segmentation results.in this work, we
propose a novel and efficient method based on cross distillation with multiple
attentions (cdma) for semi-supervised pathological image segmentation. firstly,
a multi-attention tri-branch network (mtnet) is proposed to efficiently obtain
diverse outputs for a given input. unlike mc-net+ [19] that is based on
different upsampling strategies, our mtnet uses different attention mechanisms
in three decoder branches that calibrate features in different aspects to obtain
diverse and complementary outputs. secondly, inspired by the observation that
smoothed labels are more effective for noise-robust learning found in recent
studies [10,22], we propose a cross decoder knowledge distillation (cdkd)
strategy to better leverage the diverse predictions of unlabeled images. in
cdkd, each branch serves as a teacher of the other two branches using soft label
supervision, which reduces the effect of noise for more robust learning from
inaccurate pseudo labels than argmax [2] and sharpening-based [19] pseudo
supervision in existing methods. differently from typical knowledge distillation
(kd) methods [5,24] that require a pre-trained teacher to generate soft
predictions, our method efficiently obtains the teacher and student's soft
predictions simultaneously in a single forward pass. in addition, we apply an
uncertainty minimization-based regularization to the average probability
prediction across the decoders, which not only increases the network's
confidence, but also improves the inter-decoder consistency for leveraging
labeled images.the contribution of this work is three-fold: 1) a novel framework
named cdma based on mtnet is introduced for semi-supervised pathological image
segmentation, which leverages different attention mechanisms for generating
diverse and complementary predictions for unlabeled images; 2) a cross decoder
knowledge distillation method is proposed for robust and efficient learning from
noisy pseudo labels, which is combined with an average prediction-based
uncertainty minimization to improve the model's performance; 3) experimental
results show that the proposed cdma outperforms eight state-of-the-art ssl
methods on the public digestpath dataset [3].",6
Semi-supervised Pathological Image Segmentation via Cross Distillation of Multiple Attentions,3.0,Experiments and Results,"dataset and implementation details. we used the public digestpath dataset [3]
for binary segmentation of colonoscopy tumor lesions from whole slide images
(wsi) in the experiment. the wsis were collected from four medical institutions
of ×20 magnification (0.475 μm/pixel) with an average size of 5000 × 5000. we
randomly split 130 malignant wsis into 100, 10, and 20 for training, validation
and testing, respectively. for ssl, we investigated two annotation ratios: 5%
and 10%, where only 5 and 10 wsis in the training set were taken as annotated
respectively. labeled wsis were randomly selected. for computational
feasibility, we cropped the wsis into patches with a size of 256 × 256.at
inference time for segmenting a wsi, we used a sliding window of size 256×256
with a stride of 192 × 192.the cdma framework was implemented in pytorch, and
all experiments were performed on one nvidia 2080ti gpu. mtnet was implemented
by extending deeplabv3+ [1] into a tri-branch network, where the three decoders
were equipped with ca, sa and csa blocks respectively. the encoder used a
backbone of resnet50 pre-trained on imagenet. the kernel size of conv in the sa
block is 7 × 7. sgd optimizer was used for training, with weight decay 5 × 10 -4
, momentum 0.9 and epoch number 150. the learning rate was initialized to 10 -3
and decayed by 0.1 every 50 epochs. the hyper-parameter setting was λ 1 = λ 2 =
0.1, t = 10 based on the best results on the validation set. the batch size was
16 (8 labeled and 8 unlabeled patches). for data augmentation, we adopted random
flipping, random rotation, and random gaussian noise. for inference, only the
csa branch was used due to the similar performance of the three branches after
converge and the increased inference time of their ensemble, and no
post-processing was used. dice similarity coefficient (dsc) and jaccard index
(ji) were used for quantitative evaluation. comparison with state-of-the-art
methods. our cdma was compared with eight existing ssl methods: 1) entropy
minimization (em) [15]; 2) mean teacher (mt) [14]; 3) uncertaitny-aware mean
teacher (uamt) [23]; 4) r-drop [18] that introduces a dropout-based consistency
regularization between two networks; 5) cps [2]; 6) hierarchical consistency
enforcement (hce) [7]; 7) cnn&transformer [8] that introduces cross-supervision
between cnn and transformer; 8) mc-net+ [19] that imposes mutual consistency
between multiple slightly different decoders. they were also compared with the
lower bound of supervised learning (sl) that only learns from the labeled
images. all these methods used the same backbone of deeplabv3+ [1] for a fair
comparison.quantitative evaluation of these methods is shown in table 1. in the
existing methods, mc-net+ [19] and cps [2] showed the best performance for both
of the two annotation ratios. our proposed cdma achieved a better performance
than all the existing methods, with a dsc score of 69.72% and 72.24% when the
annotation ratio was 5% and 10%, respectively. figure 2 shows a qualitative
comparison between different methods. it can be observed that our cdma yields
less mis-segmentation compared with cps [2] and mc-net+ [19]. ablation study.
for ablation study, we set the baseline as using the proposed mtnet with three
different decoders for supervised learning from labeled images only. it obtained
an average dsc of 65.02% and 68.61% under the two annotation ratios
respectively. the proposed l cdkd was compared with two variants: l cdkd
(argmax) and l cdkd (t =1) that represent using hard pseudo labels and standard
probability output obtained by softmax for cdkd respectively. table 2 shows that
our l cdkd obtained an average dsc of 68.84% and 71.49% under the two annotation
ratios respectively, and it outperformed l cdkd (argmax) and l cdkd (t =1),
demonstrating that our cdkd based on softened probability prediction is more
effective in dealing with noisy pseudo labels. by introducing our average
prediction-based uncertainty minimization l um , the dsc was further improved to
69.72% and 72.24% under the two annotation ratios respectively. in addition,
replacing our l um by applying entropy minimization to each branch respectively
(l um ) led to a dsc drop by around 0.65%. then, we compared different mtnet
variants: 1) mtnet(dual) means a dualbranch structure (removing the csa branch);
2) mtnet(csa×3) means all the three branches use csa blocks; 3) mtnet(-atten)
means no attention block is used in all the branches; and 4) mtnet(ensb) means
using an ensemble of the three branches for inference. note that all these
variants were trained with l cdkd and l um . the results in the second section
of table 2 show that using the same structures for different branches, i.e.,
mtnet(-atten) and mtnet(csa×3), had a lower performance than using different
attention blocks, and using three attention branches outperformed just using two
attention branches. it can also be found that using csa branch for inference had
a very close performance to mtnet(ensb), and it is more efficient than the
later.",6
Towards Novel Class Discovery: A Study in Novel Skin Lesions Clustering,1.0,Introduction,"automatic identification of lesions from dermoscopic images is of great
importance for the diagnosis of skin cancer [16,22]. currently, deep learning
mod-els, especially those based on deep convolution neural networks, have
achieved remarkable success in this task [17,18,22]. however, this comes at the
cost of a large amount of labeled data that needs to be collected for each
class. to alleviate the labeling burden, semi-supervised learning has been
proposed to exploit a large amount of unlabeled data to improve performance in
the case of limited labeled data [10,15,19]. however, it still requires a small
amount of labeled data for each class, which is often impossible in real
practice. for example, there are roughly more than 2000 named dermatological
diseases today, of which more than 200 are common, and new dermatological
diseases are still emerging, making it impractical to annotate data from scratch
for each new disease category [20]. however, since there is a correlation
between new and known diseases, a priori knowledge from known diseases is
expected to help automatically identify new diseases [9].one approach to address
the above problem is novel class discovery (ncd) [7,9,24], which aims to
transfer knowledge from known classes to discover new semantic classes. most ncd
methods follow a two-stage scheme: 1) a stage of fully supervised training on
known category data and 2) a stage of clustering on unknown categories [7,9,24].
for example, han et al. [9] further introduced self-supervised learning in the
first stage to learn general feature representations. they also used ranking
statistics to compute pairwise similarity for clustering. zhong et al. [24]
proposed openmix based on the mixup strategy [21] to further exploit the
information from known classes to improve the performance of unsupervised
clustering. fini et al. [7] proposed uno, which unifies multiple objective
functions into a holistic framework to achieve better interaction of information
between known and unknown classes. zhong et al. [23] used neighborhood
information in the embedding space to learn more discriminative representations.
however, most of these methods require the construction of a pairwise similarity
prediction task to perform clustering based on pairwise similarity pseudo labels
between samples. in this process, the generated pseudo labels are usually noisy,
which may affect the clustering process and cause error accumulation. in
addition, they only consider the global alignment of samples to the category
center, ignoring the local inter-sample alignment thus leading to poor
clustering performance.in this paper, we propose a new novel class discovery
framework to automatically discover novel disease categories. specifically, we
first use contrastive learning to pretrain the model based on all data from
known and unknown categories to learn a robust and general semantic feature
representation. then, we propose an uncertainty-aware multi-view
cross-pseudo-supervision strategy to perform clustering. it first uses a
self-labeling strategy to generate pseudo-labels for unknown categories, which
can be treated homogeneously with ground truth labels. the
cross-pseudo-supervision strategy is then used to force the model to maintain
consistent prediction outputs for different views of unlabeled images.in
addition, we propose to use prediction uncertainty to adaptively adjust the
contribution of the pseudo labels to mitigate the effects of noisy pseudo
labels. finally, to encourage local neighborhood alignment and further refine
the pseudo labels, we propose a local information aggregation module to
aggregate the information of the neighborhood samples to boost the clustering
performance. we conducted extensive experiments on the dermoscopy dataset isic
2019, and the experimental results show that our method outperforms other
state-of-the-art comparison algorithms by a large margin. in addition, we also
validated the effectiveness of different components through extensive ablation
experiments.",6
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,1.0,Introduction,"cancers are a group of heterogeneous diseases reflecting deep interactions
between pathological and genomics variants in tumor tissue environments [24].
different cancer genotypes are translated into pathological phenotypes that
could be assessed by pathologists [24]. high-resolution pathological images have
proven their unique benefits for improving prognostic biomarkers prediction via
exploring the tissue microenvironmental features [1,10,12,13,18,25]. meanwhile,
genomics data (e.g., mrna-sequence) display a high relevance to regulate cancer
progression [3,29]. for instance, genome-wide molecular portraits are crucial
for cancer prognostic stratification and targeted therapy [16]. despite their
importance, seldom efforts jointly exploit the multimodal value between cancer
image morphology and molecular biomarkers. in a broader context, assessing
cancer prognosis is essentially a multimodal task in association with
pathological and genomics findings. therefore, synergizing multimodal data could
deepen a crossscale understanding towards improved patient prognostication.the
major goal of multimodal data learning is to extract complementary contextual
information across modalities [4]. supervised studies [5][6][7] have allowed
multimodal data fusion among image and non-image biomarkers. for instance, the
kronecker product is able to capture the interactions between wsis and genomic
features for survival outcome prediction [5,7]. alternatively, the coattention
transformer [6] could capture the genotype-phenotype interactions for prognostic
prediction. yet these supervised approaches are limited by feature
generalizability and have a high dependency on data labeling. to alleviate label
requirement, unsupervised learning evaluates the intrinsic similarity among
multimodal representations for data fusion. for example, integrating image,
genomics, and clinical information can be achieved via a predefined unsupervised
similarity evaluation [4]. to broaden the data utility, the study [28] leverages
the pathology and genomic knowledge from the teacher model to guide the
pathology-only student model for glioma grading. from these analyses, it is
increasingly recognized that the lack of flexibility on model finetuning limits
the data utility of multimodal learning. meanwhile, the size of multimodal
medical datasets is not as large as natural vision-language datasets, which
necessitates the need for data-efficient analytics to address the training
difficulty.to tackle above challenges, we propose a pathology-and-genomics
multimodal framework (i.e., pathomics) for survival prediction (fig. 1). we
summarized our contributions as follows. (1) unsupervised multimodal data
fusion. our unsupervised pretraining exploits the intrinsic interaction between
morphological and molecular biomarkers (fig. 1a). to overcome the gap of
modality heterogeneity between images and genomics, we project the multimodal
embeddings into the same latent space by evaluating the similarity among them.
particularly, the pretrained model offers a unique means by using
similarity-guided modality fusion for extracting cross-modal patterns. (2)
flexible modality finetuning. a key contribution of our multimodal framework is
that it combines benefits from both unsupervised pretraining and supervised
finetuning data fusion (fig. 1b). as a result, the task-specific finetuning
broadens the dataset usage (fig 1b andc), which is not limited by data modality
(e.g., both singleand multi-modal data). (3) data efficiency with limited data
size. our approach could achieve comparable performance even with fewer
finetuned data (e.g., only use 50% of the finetuned data) when compared with
using the entire finetuning dataset.",6
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,2.0,Methodology,"overview. figure 1 illustrates our multimodal transformer framework. our method
includes an unsupervised multimodal data fusion pretraining and a supervised
flexible-modal finetuning. from fig. 1a, in the pretraining, our unsupervised
data fusion aims to capture the interaction pattern of image and genomics
features. overall, we formulate the objective of multimodal feature learning by
converting image patches and tabular genomics data into groupwise embeddings,
and then extracting multimodal patient-wise embeddings. more specifically, we
construct group-wise representations for both image and genomics modalities. for
image feature representation, we randomly divide image patches into groups;
meanwhile, for each type of genomics data, we construct groups of genes
depending on their clinical relevance [22]. next, as seen in fig. 1b andc, our
approach enables three types of finetuning modal modes (i.e., multimodal,
image-only, and genomics-only) towards prognostic prediction, expanding the
downstream data utility from the pretrained model. group-wise image and genomics
embedding. we define the group-wise genomics representation by referring to n =
8 major functional groups obtained from [22]. each group contains a list of
well-defined molecular features related to cancer biology, including
transcription factors, tumor suppression, cytokines and growth factors, cell
differentiation markers, homeodomain proteins, translocated cancer genes, and
protein kinases. the group-wise genomics representation is defined as g n ∈ r
1×dg , where n ∈ n , d g is the attribute dimension in each group which could be
various. to better extract high-dimensional group-wise genomics representation,
we use a self-normalizing network (snn) together with scaled exponential linear
units (selu) and alpha dropout for feature extraction to generate the group-wise
embedding g n ∈ r 1×256 for each group.for group-wise wsis representation, we
first cropped all tissue-region image tiles from the entire wsi and extracted
cnn-based (e.g., resnet50) d idimensional features for each image tile k as h k
∈ r 1×di , where d i = 1, 024, k ∈ k and k is the number of image patches. we
construct the group-wise wsis representation by randomly splitting image tile
features into n groups (i.e., the same number as genomics categories).
therefore, group-wise image representation could be defined as i n ∈ r kn×1024 ,
where n ∈ n and k n represents tile k in group n. then we apply an
attention-based refiner (abr) [17], which is able to weight the feature
embeddings in the group, together with a dimension deduction (e.g.,
fully-connected layers) to achieve the group-wise embedding. the abr and the
group-wise embedding i n ∈ r 1×256 are defined as:where w,v1 and v2 are the
learnable parameters.patient-wise multimodal feature embedding. to aggregate
patient-wise multimodal feature embedding from the group-wise representations,
as shown in fig. 1a, we propose a pathology-and-genomics multimodal model
containing two model streams, including a pathological image and a genomics data
stream.in each stream, we use the same architecture with different weights,
which is updated separately in each modality stream. in the pathological image
stream, the patient-wise image representation is aggregated by n group
representations as, where p ∈ p and p is the number of patients. similarly, the
patient-wise genomics representation is aggregated as g p ∈ r n ×256 . after
generating patient-wise representation, we utilize two transformer layers [27]
to extract feature embeddings for each modality as follows:where msa denotes
multi-head self-attention [27] (see appendix 1), l denotes the layer index of
the transformer, and h p could either be i p or g p . then, we construct global
attention poolings [17] as eq. 1 to adaptively compute a weighted sum of each
modality feature embeddings to finally construct patientwise embedding as i p
embedding ∈ r 1×256 and g p embedding ∈ r 1×256 in each modality.multimodal
fusion in pretraining and finetuning. due to the domain gap between image and
molecular feature heterogeneity, a proper design of multimodal fusion is crucial
to advance integrative analysis. in the pretraining stage, we develop an
unsupervised data fusion strategy by decreasing the mean square error (mse) loss
to map images and genomics embeddings into the same space. ideally, the image
and genomics embeddings belonging to the same patient should have a higher
relevance between each other. mse measures the average squared difference
between multimodal embeddings. in this way, the pretrained model is trained to
map the paired image and genomics embeddings to be closer in the latent space,
leading to strengthen the interaction between different modalities.in the single
modality finetuning, even if we use image-only data, the model is able to
produce genomic-related image feature embedding due to the multimodal knowledge
aggregation already obtained from the model pretraining. as a result, our
cross-modal information aggregation relaxes the modality requirement in the
finetuning stage. as shown in fig. 1b, for multimodal finetuning, we deploy a
concatenation layer to obtain the fused multimodal feature representation and
implement a risk classifier (fc layer) to achieve the final survival
stratification (see appendix 2). as for single-modality finetuning mode in fig.
1c, we simply feed i p embedding or g p embedding into risk classifier for the
final prognosis prediction. during the finetuning, we update the model
parameters using a log-likelihood loss for the discrete-time survival model
training [6](see appendix 2).",6
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,3.0,Experiments and Results,"datasets. all image and genomics data are publicly available. we collected wsis
from the cancer genome atlas colon adenocarcinoma (tcga-coad) dataset
(cc-by-3.0) [8,21] and rectum adenocarcinoma (tcga-read) dataset (cc-by-3.0)
[8,20], which contain 440 and 153 patients. we cropped each wsi into 512 × 512
non-overlapped patches. we also collected the corresponding tabular genomics
data (e.g., mrna sequence, copy number alteration, and methylation) with overall
survival (os) times and censorship statuses from cbioportal [2,14]. we removed
the samples without the corresponding genomics data or ground truth of survival
outcomes. finally, we included 426 patients of tcga-coad and 145 patients of
tcga-read.experimental settings and implementations. we implement two types of
settings that involve internal and external datasets for model pretraining and
finetuning. as shown in fig 2a, we pretrain and finetune the model on the same
dataset (i.e., internal setting). we split tcga-coad into training (80%) and
holdout testing set (20%). then, we implement four-fold cross-validation on the
training set for pretraining, finetuning, and hyperparameter-tuning. the test
set is only used for evaluating the best finetuned models from each
cross-validation split. for the external setting, we implement pretraining and
finetuning on the different datasets, as shown in fig 2b ; we use tcga-coad for
pretraining; then, we only use tcga-read for finetuning and final evaluation. we
implement a five-fold cross-validation for pretraining, and the best pretrained
models are used for finetuning. we split tcga-read into finetuning (60%),
validation (20%), and evaluation set (20%). for all experiments, we calculate
the average performance on the evaluation set across the best models.the number
of epochs for pretraining and finetuning are 25, the batch size is 1, the
optimizer is adam [19], and the learning rate is 1e-4 for pretraining and 5e-5
for finetuning. we used one 32gb tesla v100 sxm2 gpu and pytorch. the
concordance index (c-index) is used to measure the survival prediction
performance. we followed the previous studies [5][6][7] to partition the overall
survival (os) months into four non-overlapping intervals by using the quartiles
of event times of uncensored patients for discretized-survival c-index
calculation (see appendix 2). for each experiment, we reported the average
c-index among three-times repeated experiments. conceptionally, our method
shares a similar idea to multiple instance learning (mil) [9,23]. therefore, we
include two types of baseline models, including the mil-based models (deepset
[30], ab-mil [17], and transmil [26]) and mil multimodal-based models (mcat [6],
porpoise [7]). we follow the same data split and processing, as well as the
identical training hyperparameters and supervised fusion as above. notably,
there is no need for supervised finetuning for the baselines when using
tcga-coad (table 1), because the supervised pretraining is already applied to
the training set.",6
Pathology-and-Genomics Multimodal Transformer for Survival Outcome Prediction,4.0,Conclusion,"developing data-efficient multimodal learning is crucial to advance the survival
assessment of cancer patients in a variety of clinical data scenarios. we
demonstrated that the proposed pathomics framework is useful for improving the
survival prediction of colon and rectum cancer patients. importantly, our
approach opens up perspectives for exploring the key insights of intrinsic
genotypephenotype interactions in complex cancer data across modalities. our
finetuning",6
Robust Cervical Abnormal Cell Detection via Distillation from Local-Scale Consistency Refinement,1.0,Introduction,"cervical cancer is the second most common cancer among adult women. if diagnosed
early, it can be effectively treated and cured [19]. nevertheless, delayed
diagnosis of cervical cancer until an advanced stage will have a negative impact
on patient prognosis and consume medical resources. currently, early screening
of cervical cancer is recommended worldwide as an effective method to prevent
and treat cervical cancer. thin-prep cytologic test (tct) is the most common and
effective screening method for detecting cervical abnormal and premalignant
cervical lesions [5]. conventionally it is performed by visually examining the
stained cells collected through smearing on a glass slide, and generating a
diagnosis report using the descriptive diagnosis method of the bethesda system
(tbs) [15]. although tct has been widely used in clinical applications and has
significantly reduced the mortality rates caused by cervical cancer, it is still
unavailable for population-wide screening [18]. this is partly due to its
laborintensive, time-consuming, and high cost [1]. therefore, there is a high
demand for automated cervical abnormality screening to facilitate efficient and
accurate identification of cervical abnormalities.with the development of deep
learning [10], several attempts have been made to identify cervical abnormal
cells using convolutional neural networks (cnns). for example, cao et al. [2]
developed an attention feature pyramid network (attfpn) for automatic abnormal
cervical cell detection in cervical cytopathological images to assist
pathologists in making more accurate diagnoses. chen et al. [3] proposed a new
framework that decomposes tasks and compares cells for cervical lesion cell
detection. liang et al. [11] proposed to explore contextual relationships to
boost the performance of cervical abnormal cell detection. lin et al. [22]
presented an automatic cervical cell detection approach based on the
dense-cascade r-cnn. it is worth mentioning that all of the aforementioned
detection methods inevitably produce false positive results, which should be
further refined by pathologists for manual checking or classification models
established for automatic screening. to solve this problem, zhou et al. [23]
proposed a three-stage method including cell-level detection, image-level
classification, and case-level diagnosis obtained by an svm classifier. zhu et
al. [24] developed an artificial intelligence assistive diagnostic solution,
which integrated yolov3 [16] for detection, xception, and patch-based models to
boost classification.although the above-mentioned attempts can improve the
screening performance significantly, there are several issues that need to be
addressed: 1) object detection methods often require accurate annotated data to
guarantee performance with robustness and generalization. however, due to legal
limitations, the scarcity of positive samples, and especially the subjectivity
differences between cytopathologists for manual annotations [20], it is likely
to generate noisy samples that affect the performance of the detection model. 2)
conventional object detection methods intend to directly extract the feature
from the object area to locate and classify the object simultaneously. however,
in clinical practice pathologists usually examine the target cells by comparing
them to the surrounding cells to determine whether they are abnormal. therefore,
the visual feature correlations between the target cells and their surroundings
can provide valuable information to aid the screening process, which also needs
to be utilized when designing the cervical abnormal cell detection network.to
address these issues, we propose a novel method for cervical abnormal cell
detection using distillation from local-scale consistency refinement. inspired
by knowledge distillation, we construct a pre-trained patch correction network
(pcn), which is designed to exploit the supervised information from the pcn to
reduce the impact of noisy labels and utilize the contextual relationships
between cells. in our approach, we begin by utilizing retinanet [12] to locate
suspicious cells and crop the top-k suspicious cells into patches. then we feed
them into the pcn to obtain classification scores and propose a ranking loss to
refine the classifier of the detection network by correcting the score of the
detection model. in addition, we propose an roi-correlation consistency (rcc)
loss between roi features and local-scale features from the pcn, which
encourages the detector to explore the feature correlations of the suspicious
cells. our proposed method achieves improved performance during inference
without changing the detector structure.",6
StainDiff: Transfer Stain Styles of Histology Images with Denoising Diffusion Probabilistic Models and Self-ensemble,0.0,Datasets. Evaluations of StainDiff are conducted on two datasets. (1),"dataset-a: mitos-atypia 14 challenge1 . this dataset aims to measure the style
transfer performance on 284 histology frames. each slide is digitized by two
different scanners, resulting in stain style variations. for a fair comparison,
we follow the settings in previous work [26] by using 10,000 unpaired patches
randomly cropped from the first 184 slides of both scanners as the training set.
meanwhile, 500 paired patches are generated from the remaining 100 slides as the
test set, where we use pearson correlation coefficient (pc), structural
similarity index (ssim) [31] and feature similarity index for image quality
assessment (fsim) [33] as the evaluation metrics. ( 2) dataset-b: the cancer
genome atlas (tcga). this dataset evaluates the performance of stain
normalization quantified by the downstream nine-category tissue structure
classification accuracy [27]. domain a contains histology of multiple stain
styles, which are collected from 186 wsis from tcga-coad and nct-crc-he-100k
[14]; and domain b is the target style, curated from 25 wsis in crc-val-he-7k
[14].",6
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,1.0,Motivation,"whole slide imaging is capable of effectively digitizing specimen slides,
showing both the microscopic detail and the larger context, without any
significant manual effort. due to the enormous resolution of the whole slide
images (wsis), a classification based on straight-forward convolutional neural
network architectures is not feasible. multiple instance learning
[8,10,13,18,20] (mil) represents a methodology (with a high momentum indicated
by a large number of recent publications) to deal with these huge images
corresponding to single (global) labels. in the mil setting, wsis correspond to
labeled bags, whereas extracted patches correspond to unlabeled bag instances.
mil approaches typically consist of a feature extraction stage, a mil pooling
stage and a following downstream classification. state-of-the-art approaches
mainly rely on convolutional neural network architectures for feature
extraction, often in combination with attention [10,11] or self-attention [12].
for training the feature extraction stage, classical supervised and
self-supervised learning is employed [10,11]. while the majority of methods rely
on separate learning stages, also end-to-end approaches have been proposed
[3,14]. in spite of the large amount of data, the number of labeled samples in
mil (represented by the number of individual, globally labelled wsis) is often
small and/or imbalanced [6]. general data augmentation strategies, such as
rotations, flipping, stain augmentation and normalization and affine
transformations, are applicable to increase the amount of data [15]. all of
these methods are performed in the image domain.here, we consider feature-level
data augmentation directly applied to the representation extracted using a
convolutional neural network. these methods can be easily combined with
image-based augmentation and show the advantage of a high computational
efficiency (since operations are efficient and pre-computed features can be
used) [10,11]. for example, li et al. [11] proposed an augmentation strategy
based on sampling the patch-descriptors to generate several bags for an
individual wsi. in this paper, we focus on the interpolations of patch
descriptors based on the idea of zhang et al [21], which is referred to as
mixup. this method was originally proposed as data agnostic approach which also
shows good results if applied to image data [2,4,16]. variations were proposed,
to be applied to latent representations [17] as well as to balance data sets
[6]. due to the structure of mil training data, we identified several options to
perform interpolation-based data augmentation.the main contribution of this work
is a set of novel data augmentation strategies for mil, based on the
interpolation of patch descriptors. inspired by the (linear) mixup approach
[21], we investigated several ways to translate this idea to the mil setting.
beyond linear interpolation, we also defined a more flexible and novel
multilinear approach. for evaluation, a large experimental study was conducted,
including 2 histological data sets, 5 deep learning configurations for mil, 3
common data augmentation strategies and 4 mixup settings. we investigated the
classification of wsis containing thyroid cancer tissues [1,5]. to obtain an
improved understanding of reasons behind the experimental results, we also
investigate the feature distributions.",6
MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis,4.0,Discussion,"in this work, we proposed and examined novel data augmentation strategies based
on the idea of interpolations of feature vectors in the mil setting.
instance-based mil did not show any competitive scores. obviously the model
reducing each patch to a single value is not adequate for the classification of
frozen or paraffin sections from thyroid cancer tissues. the considered
dual-stream approach, including an embedding and instance-based stream,
exhibited slightly improved average scores, compared to embedding-based mil
only. in our analysis, we focused on the embedding-based configuration and on
the balanced combined approach (referred to as 2/2). with the baseline data
augmentation approaches, the maximum improvements were 0.03, and 0.02 for the
frozen, and 0.01, and 0.05 for the paraffin data set. the inter-mixup approach
did not show any systematic improvements. independently of the chosen strategy
(v1, v2), concerning the combination within or between classes, we did not
notice any positive trend. the multilinear intra-mixup method, however,
exhibited the best scores for 3 out of 4 combinations and the best overall mean
accuracy for both, the frozen and the paraffin data set. also a clear trend with
increasing scores in the case of an increasing ratio of augmented data (β) is
visible. the linear method showed a similar, but less pronounced trend.
obviously, the straightforward application of the mixup scheme (as in case of
the inter-mixup approach), is inappropriate for the considered setting. an
inhibiting factor could be a high inter-wsi variability leading to incompatible
feature vectors (which are too far away from realistic samples in the feature
space). to particularly investigate this effect, we performed 2 different
inter-mixup settings (v1 & v2), with the goal of identifying the effect of mixed
(and thereby more dissimilar) or similar classes during interpolation. the
analysis of the distance distributions between patch representations confirmed
that, the variability between wsis is clearly larger than the variability within
wsis. in addition, the results showed that the variability between classes is,
on patch-level, not clearly larger than the variability within a class.
obviously variability due to the acquisition outweigh any disease specific
variability. this could provide an explanation for the effectiveness of
intra-mixup approach compared to the (similarly) poorly performing inter-mixup
settings. we expect that stain normalization methods (but not stain
augmentation) could be utilized to align the different wsis to provide a more
appropriate basis for inter-wsi interpolation. with regard to the different data
sets, we noticed a stronger, positive effect in case of the frozen section data
set. this is supposed to be due to the clearly higher variability of the frozen
sections corresponding with a need for a higher variability in the training
data. we also noticed a stronger effect of the solely embedding-based
architecture (also showing the best overall scores). we suppose that this is due
to the fact that the additional loss of the dual-stream architecture exhibits a
valuable regularization tool to reduce the amount of needed training data. with
the proposed intra-mixup augmentation strategy, this effect diminishes, since
the amount and quality of training data is increased.to conclude, we proposed
novel data augmentation strategies based on the idea of interpolations of image
descriptors in the mil setting. based on the experimental results, the
multilinear intra-mixup setting proved to be highly effective, while the
inter-mixup method showed inferior scores compared to a state-of-the-art
baseline. we learned that there is a clear difference between combinations
within and between wsis with a noticeable effect on the final classification
accuracy. this is supposedly due to the high variability between the wsis
compared to a rather low variability within the wsis. in the future, additional
experiments will be conducted including stain normalization methods and larger
benchmark data sets to provide further insights.",6
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,1.0,Introduction,"breast cancer (bc) is the most common cancer diagnosed among females and the
second leading cause of cancer death among women after lung cancer [1]. the bc
differs greatly in clinical behavior, ranging from carcinoma in site to
aggressive metastatic disease [2,3]. thus, effective and accurate prognosis of
bc as well as stratifying cancer patients into different subgroups for
personalized cancer management has attracted more attention than ever
before.among different types of imaging biomarkers, histopathological images are
generally considered the golden standard for bc prognosis since they can confer
important cell-level information that can reflect the aggressiveness of bc [4].
recently, with the availability of digitalized whole-slide pathological images
(wsis), many computational models have been employed for the prognosis
prediction of various subtypes of bc. for instance, lu et al [5] presented a
novel approach for predicting the prognosis of er-positive bc patients by
quantifying nuclear shape and orientation from histopathological images. liu et
al [6] developed a gradient boosting algorithm to predict the disease
progression for various subtypes of bc. however, due to the high-cost of
collecting survival information from the patients, it is still a challenge to
build effective machine learning models for specific bc subtypes with limited
annotation data.to deal with the above challenges, several researchers began to
design domain adaption algorithms, which utilize the labeled data from a related
cancer subtype to help predict the patients' survival in the target domain.
specifically, alirezazadeh et al [7] presented a new representation
learning-based unsupervised domain adaption method to predict the clinical
outcome of cancer patients on the target domain. zhang et al [8] proposed a
collaborative unsupervised domain adaptation algorithm, which conducts
transferability-aware adaptation and conquers label noise in a collaborative
way. other studies include xu et al [9] developed graph neural networks for
unsupervised domain adaptation in histopathological image analysis, based on a
backbone for embedding input images into a feature space, and a graph neural
layer for propagating the supervision signals of images with labels.although
much progress has been achieved, most of the existing studies applied the
feature alignment strategy to reduce the distribution difference between source
and target domains. however, such transfer learning methods neglected to take
the interaction among different types of tissues into consideration. for
example, it is widely recognized that tumor-infiltrating lymphocytes (tils) and
its correlation with tumors reveal a similar role in the prognosis of different
brca subtypes. for instance, kurozumi et al [10] revealed that high tils
expression was correlated with negative estrogen receptor (er) expression and
high histological grade (p < 0.001). lu et al [11] utilized the tils spatial
pattern for survival analysis in different breast cancer subtypes including
er-negative, er-positive, and triple-negative. it can be expected that better
prognosis performance can be achieved if we leveraged the tils-tumor interaction
information to resolve the survival analysis task on the target domain.based on
the above considerations, in this paper, we proposed a tils-tumor interactions
guided unsupervised domain adaptation (t2uda) algorithm to predict the patients'
survival on the target bc subtype. specifically, t2uda first applied the graph
attention network (gats) to learn node embeddings and the spatial interactions
between tumor and tils patches in wsi. in order to preserve the node-level and
interaction-level similarities across different domains, we not only aligned the
embedding for different types of nodes but also designed a novel tumor-tils
interaction alignment (ttia) module to ensure that the distribution of the
interaction weights are similar in both domains. we evaluated the performance of
our method on the breast invasive carcinoma (brca) cohort derived from the
cancer genome atlas (tcga), and the experimental results indicated that t2uda
outperforms other domain adaption methods for predicting patients' clinical
outcomes.",6
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,2.0,Method,"we summarized the proposed t2uda network in fig. 1, which consists of three
parts, i.e., graph attention network-based framework, feature alignment(fa), and
tils-tumor interaction alignment(ttia). next, we will introduce each part in
detail. data pre-processing. we obtained valid patches of 512 × 512 pixels from
pathological images and segment the tils and tumor tissues using a pre-trained
u-net++ model. then we calculated the tumor and tils area ratios in each patch
and selected 300 patches with the largest ratios of each tissue type. based on
the selected patches, we constructed a graph g = (v, e) for each wsi.",6
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,0.0,Calculating TILs-Tumor Interaction via Graph Attention Networks(GATs).,"to characterize the interaction between different tils and tumor patches, we
employed gat [13], which has been proven to be useful in describing the spatial
interaction between different tissues across wsis. our gat-based framework
consisted of 3 gat layers interleaved with 3 graph pooling layers [14](shown in
fig. 1). the input of the gat layer are we calculated the attention coefficients
among different nodes, which can be formulated as:furthermore, a softmax
function was then adopted to normalize the attention coefficients e ij :where n
i represents all neighbors of node i. the new feature vector v i for node i was
calculated via a weighted sum:finally, the output features of each gat layer
were aggregated in the readout layer. we fed the generated output features from
each readout layer into the cox hazard proportional regression model for the
final prognosis predictions.feature alignment. in the proposed gat-based
transfer learning framework, the feature alignment component was employed on its
first two layers. then, for the node embeddings with different types (tils and
tumor) in both the source and target domain, we performed a mean pooling
operation to obtain their aggregated features. next, we aligned the aggregated
tumor or tils features from the two domains separately using maximum mean
discrepancy(mmd) [15].here, we adopted mmd for feature alignment due to its
ability to measure the distance between two distributions without explicit
assumptions on the data distribution, we showed the objective function of mmd in
our method as follows:where h is a hilbert space, f represents the features from
the source, f represents the feature from the target, r represents the layer
number, k ∈ {l, t } referred to tils or tumor node. in addition, n denotes the
number of source samples, while m refers to the number of target samples.
tils-tumor interaction alignment. to accurately characterize the interaction
between tils and tumors, we further analyzed the extracted interaction weights
by dividing them into 10 intervals (i.e., bins). for each interval, we
calculated the sum of all source domain interaction weights as i s k and the sum
of all target domain interaction weights as i t k , where k represents the k-th
interval. consequently, we obtained two vectors and applied softmax on each of
them for normalization that can be denoted as. in order to measure the
dissimilarity between p i and q i , the kullback-leibler (kl) divergence is
adapted on the third layer of gat, which can be formulated as:according to eq.(
5), we can ensure that the weight distributions for the til-tumor interaction
are consistent in the source and target domain, which will benefit the following
survival analysis. it is beneficial for the target domain.",6
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.0,Experiments and Results,"datasets. we conducted our experiments on the breast invasive carcinoma (brca)
dataset from the cancer genome atlas (tcga). specifically, the brca dataset
includes 661 patients with hematoxylin and eosin (he)-stained pathological
imaging and corresponding survival information. among the collected brca
patients in tcga, the number of er positive(er+) and er negative(er-) patients
are 515 and 146, respectively. we hope to investigate if the proposed t2uda
could be used to help improve the prognosis performance of (er+) or (er-) with
the aid of the survival information on its counterpart.",6
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,3.2,Result and Discussion,"in this study, we compared the performance of our proposed model with several
existing domain adaptation methods, including 1) ddc [17]: utilize the maximum
mean discrepancy (mmd) to calculate the domain difference loss between source
and target data and optimize both classification loss and disparity loss. 2)
dann [18]: an adversarial learning method that used gradient backpropagation to
extract domain-independent features. 3) mdd [19]: an adversarial training method
that combined metric learning and domain adaptation. 4) deepjdot [20]: an
unsupervised domain adaptation method based on optimal transport that
simultaneously learns features and optimizes classifiers by measuring joint
feature/label differences. 5) source only: it was trained on the source domain
and applied directly to the target domain. 6) t2uda-v1: it was a variant of
t2uda which didn't use ttia. the experimental results were presented in table
1.the results presented in table 1 revealed several key observations. first, our
proposed method outperformed feature alignment-based methods such as ddc and
deepjdot in terms of both ci and auc values. the reason lies in that these
methods only transferred the knowledge at the feature level and neglected the
inter-relationship between tils and tumors. second, our method outperformed
adversarial-based methods such as dann and mdd, as the high heterogeneity
between the target and source domains results in negative transfer through
adversarial training. instead of directly aligning regions, our proposed method
focused on similar tils-tumor interactions and aligning patches of the same
tissue.we also evaluated the contributions of the key components of our
framework and found that t2uda performed better than source only and t2uda-v1,
which shows the advantage of minimizing differences in tils-tumor interaction
weights.in addition, we also evaluated the patient stratification performance of
different methods. as shown in fig. 3, our proposed t2uda outperformed feature
alignment-based methods (such as ddc and deepjdot), adversarial-based methods
(such as dann and mdd), and t2uda-v1 in stratification performance, proving that
considering the interaction between tils and tumors as migration knowledge leads
to better prognostic results.we also examined the consistency of important edges
in each group of stratified patients based on the tils-tumor interaction weights
calculated by the gat-based framework in the source and target domains. as seen
in fig. 4(a), for both the source and target domains, the proportion of edges
that connect tils and tumor regions in the low-risk group was higher than that
in the highrisk group, showing that the interaction between tils and tumors
played a critical role in prognostic prediction in different bc subtypes.
furthermore, as shown in fig. 4(b), the weights of the edges connecting tumor
and tils regions were higher for patients in the low survival risk group in both
source and target domains. this was consistent with our knowledge that brisk
interaction between tils and tumor regions indicates a better clinical outcome
and demonstrates the transferability of this knowledge.",6
Transfer Learning-Assisted Survival Analysis of Breast Cancer Relying on the Spatial Interaction Between Tumor-Infiltrating Lymphocytes and Tumors,4.0,Conclusion,"in this paper, we presented an unsupervised domain adaptation algorithm that
leverages tils-tumor interactions to predict patients' survival in a target bc
subtype(t2uda). our results demonstrated that the relationship between tils and
tumors is transferable and can be effectively used to improve the accuracy of
survival prediction models. to the best of our knowledge, t2uda was the first
method to successfully achieve interrelationship transfer between tils and
tumors across different cancer subtypes for prognosis tasks.",6
Gene-Induced Multimodal Pre-training for Image-Omic Classification,1.0,Introduction,"pathological image-omic analysis is the cornerstone of modern medicine and
demonstrates promise in a variety of different tasks such as cancer diagnosis
and prognosis [12]. with the recent advance of digital pathology and sequencing
technologies, modern cancer screening has jointly incorporated genomics and
histology analysis of whole slide images (wsis).though deep learning techniques
have revolutionized medical imaging, designing a task-specific algorithm for
image-omic multi-modality analysis is challenging. (1) the gigapixel wsis, which
generally yield 15,000 foreground patches during pre-processing, make
attention-based backbones [6] hard to extract precise image (wsi)-level
representations. (2) learning features from genomics data which have tens of
thousands of genes make models such as transformer [16] impractical to use due
to its quadratic computation complexity. (3) image-omic feature fusion [2,3] may
fail to model high-order relevance and the inherent structural characteristics
of each modality, making the fusion less effective.specifically, to our
knowledge, most multi-modality techniques have been designed for modalities such
as chest x-ray and reports [1,17,23], ct and x-ray [18], ct and mri [21], h&e
cross-staining [22] via global feature, local feature or multi-granularity
alignment. but, none of these works considers the challenges in wsis and genes
processing. besides, vision-language models in the computer vision community
stand out for their remarkable versatility [13,14]. nevertheless, constrained by
computing resources, the most commonly used multimodal representation learning
strategy, contrastive learning, which relies on a large number of negative
samples to avoid model collapse [8], is not affordable for gigapixel wsis
analysis. a big domain gap also hampers their usage in leveraging the structural
characteristic of tumor micro-environment and genomic assay. recently, the
literature corpus has proposed some methods for accomplishing specific
image-omic tasks via kronecker product fusion [2] or co-attention mapping
between wsis and genomics data [3]. but, the kronecker product overly concerns
feature interactions between modalities while ignoring high-order relevance,
w.r.t. decision boundaries across multiple samples, which is critical to
classification tasks. as for the co-attention module, it is unidirectional and
cannot localize significant regions from genetic data with a large amount of
information.in this paper, we propose a task-specific framework dubbed
gene-induced multimodal pre-training (gimp) for image-omic classification.
concretely, we first propose a transformer-based gene encoder, group multi-head
self attention (groupmsa), to capture global structured features in gene
expression cohorts. next, we design a pre-training paradigm for wsis, masked
patch modeling (mpm), masking random patch embeddings from a fixed-length
contiguous subsequence of a wsi. we assume that one patch-level feature
embedding can be reconstructed by its adjacent patches, and this process
enhances the learning ability for pathological characteristics of different
tissues. our mpm only needs to recover the masked patch embeddings in a
fixed-length subsequence rather than processing all patches from wsis.
furthermore, to model the high-order relevance of the two modalities, we combine
cls tokens of paired image and genomic data to form unified representations and
propose a triplet learning module to differentiate patient-level positive and
negative samples in a mini-batch. it is worth mentioning that although our
unified representation fuses features from the whole gene expression cohort and
partial wsis in a mini-batch, we can still learn high-order relevance and
discriminative patient-level information between these two modalities in
pre-training thanks to the triplet learning module. in addition, note that our
proposed method is different from self-supervised pre-training. specifically, we
focus not only on superior representation learning capability, but also
category-related feature distributions, w.r.t. intra-and inter-class variation.
with the training process going on, complete information from wsis can be
integrated and the fused multimodal representations with high discrimination
will make it easier for the classifier to find the classification hyperplane.
experimental results demonstrate that our gimp achieves significant improvement
in accuracy than other image-omic competitors, and our multimodal framework
shows competitive performance even without pre-training.",6
Gene-Induced Multimodal Pre-training for Image-Omic Classification,3.1,Experimental Setup,"datasets. we verify the effectiveness of our method on the caner genome atlas
(tcga) non-small cell lung cancer (nsclc) dataset, which contains two cancer
subtypes, i.e., lung squamous cell carcinoma (lusc) and lung adenocarcinoma
(luad). after pre-processing [11], the patch number extracted from wsis at 20×
magnification varies from 485 to 148,569. we collect corresponding rna-seq fpkm
data for each patient and the length of the input genomic sequence is 60,480.
among 946 image-omic pairs, 470 of them belong to luad and 476 cases are lusc.
we randomly split the data into 567 for training, 189 for validation and 190 for
testing.implementation details. the pre-training process of all algorithms is
conducted on the training set, without any extra data augmentation. note that
our genetic encoder, groupmsa, is fully supervised pre-trained on unimodal
genetic data to accelerate convergence and it is frozen during gimp training
process. the maximum pre-training epoch for all methods is set to 100 and we
finetune the models at the last epoch. during fine-tuning, we evaluate the model
on the validation set after every epoch, and save the parameters when it
performs the best. adamw [10] is used as our optimizer and the learning rate is
10 -4 with cosine decline strategy. the maximum number of fine-tune epoch is 70.
at last, we measure the performance on the test set. training configurations are
consistent throughout the fine-tuning process to ensure fair comparisons. all
experiments are conducted on a single nvidia geforce rtx 3090.",6
Histopathology Image Classification Using Deep Manifold Contrastive Learning,2.0,Method,"the overview of our proposed model is illustrated in fig. 2. it is composed of
two stages: (1) train the feature extractor using deep manifold embedding
learning and (2) train the wsi classifier using the deep manifold embedding
extracted from the first stage. the input wsis are pre-processed to extract 256
× 256 × 3 dimensional patches from the tumor area at a 10× magnification level.
patches with less than 50% tissue coverage are excluded from the experiment.",6
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.1,Datasets,"we tested our proposed method on two different tasks: (1) intrahepatic
cholangiocarcinomas(ihccs) subtype classification and (2) liver cancer type
classification. the dataset for the former task was collected from 168 patients
with 332 wsis from seoul national university hospital. ihccs can be further
categorized into small duct type (sdt) and large duct type (ldt). using gene
mutation information as prior knowledge, we collected wsis with wild kras and
mutated idh genes for use as training samples in sdt, and wsis with mutated kras
and wild idh genes for use in ldt. the rest of the wsis were used as testing
samples. the liver cancer dataset for the latter task was composed of 323 wsis,
in which the wsis can be further classified into hepatocellular carcinomas
(hccs) (collected from pathology ai platform [1]) and ihccs. we collected 121
wsis for the training set, and the remaining wsis were used as the testing set.",6
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.2,Implementation Detail,"we used a pre-trained vgg16 with imagenet as the initial encoder, which was
further modified via deep manifold model training using the proposed manifold
and cross-entropy loss functions. the number of nearest neighbors k and the
number of sub-classes n were set to 5 and 10, respectively. in the deep manifold
embedding learning model, the learning rates were set to 1e-4 with a decay rate
of 1e-6 for the ihccs subtype classification and to 1e-5 with a decay rate of
1e-8 for the liver cancer type classification. the k-nearest neighbors graph and
the geodesic distance matrix are updated once every five training epochs, which
is empirically chosen to balance running time and accuracy. to train the mil
classifier, we set the learning rate to 1e-3 and the decay rate to 1e-6. we used
batch sizes 64 and 4 for training the deep manifold embedding learning model and
the mil classification model, respectively. the number of epochs for the deep
manifold embedding learning model was 50, while 50 and 200 epochs for the ihccs
subtype classification and liver cancer type classification, respectively. as
for the optimizer, we used stochastic gradient decay for both stages. the result
shown in the tables is the average result from 10 iterations of the mil
classification model.",6
Histopathology Image Classification Using Deep Manifold Contrastive Learning,3.3,Experimental Results,"the performance of different models from two different datasets is reported in
this section. for the baseline model, we chose the pre-trained vgg16 feature
extractor with an mil classifier, which is the same as our proposed model except
that the encoder is retrained using the proposed loss. two sota methods using
contrastive learning and clustering, pcl [9] and hcsc [5], are compared with our
method in this study. the mil classification result of the ihccs subtype
classification is shown in table 1. our proposed method outperformed the
baseline cnn by about 4% increment in accuracy, precision, recall, and f1 score.
note that our method only used 20 sub-classes but outperformed pcl (using 2300
sub-classes) by 4% and hcsc (using 112 sub-classes) by 5% in accuracy. the
result of liver cancer type classification is also shown in table 1. our method
achieved about 5% improvement in accuracy against the baseline and 1% to 2%
improvement in accuracy against the sota methods. moreover, it outperformed the
sota methods with far fewer prototypes and without complicated hierarchical
prototype assignments. to further evaluate the effect of prototypes, we
conducted an ablation study for different prototype assignment strategies as
shown in table 2. here, global prototypes imply assigning a single prototype per
class while local prototypes imply assigning multiple prototypes per class (one
per sub-class). when both are used together, it implies a hierarchical prototype
assignment where local prototypes interact with the corresponding global
prototype. as shown in this result, the model with local prototypes only
performed about 4% higher than did the model with global prototypes only.
meanwhile, the combination of both prototypes achieved a similar performance to
that of the model with local prototypes only. since the hierarchical (global +
local) assignment did not show a significant improvement but instead increased
computation, we used only local sub-class prototypes in our final experiment
setting. 1)-( 4) are the patches from sdt and ( 5)-( 8) are the patches from
ldt.since one of our contributions is the use of geodesic distance, we assessed
the efficacy of the method by comparing it with the performance using cosine
distance, as shown in table 3. to measure the performance of the
cosine-distancebased method, we simply replaced our proposed manifold loss with
nt-xent loss [3], which uses cosine distance in their feature similarity
measurement. two cosine distance experiments were conducted as follows: (1) use
only their groundtruth class without further dividing the samples into
sub-classes (i.e., global prototypes) and (2) divide the samples from each class
into 10 sub-classes by using k-means clustering (i.e., local prototypes). as
shown in table 3, using multiple local prototypes shows slightly better
performance compared to using global prototypes. by switching the nt-xent loss
with our geodesic-based manifold loss, the overall performance is increased by
about 2%. figure 3 visually compares the effect of the geodesic and cosine
distance-based losses. two scatter plots are t-sne projections of feature
vectors from the encoders trained using geodesic distance and cosine distance,
respectively. red dots represent sdt samples and blue dots represent ldt samples
from the ihccs dataset (corresponding histology thumbnail images are shown on
the right). in this example, all eight cases are correctly classified by the
method using geodesic distance while all cases are incorrectly classified by the
method using cosine distance. it is clearly shown that geodesic distance can
correctly measure the feature distance (similarity) on the manifold so that sdt
and ldt groups are located far away in the t-sne plot, whereas cosine distance
failed to separate these groups and they are located nearby in the plot.",6
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,1.0,Introduction,"prostate cancer (pca) diagnosis and grading rely on histopathology analysis of
biopsy slides [1]. however, prostate biopsies are known to have sampling error
as pca is heterogenous and commonly multifocal, meaning cancer legions can be
missed during the biopsy procedure [2]. if significant pca is detected on
biopsies and the patient has organ-confined cancer with no contraindications,
radical prostatectomy (rp) is the standard of care [3,4]. following rp, the
prostate is processed and slices are mounted onto slides for analysis. radical
prostatectomy histopathology samples are essential for validating the
biopsydetermined grade group [5,6]. analysis of whole-mount slides, meaning
slides that include slices of the entire prostate, provide more precise tumor
boundary detection, identification of various tumor foci, and increased tissue
for identifying morphological patterns not visible on biopsy due to a larger
field of view.field effect refers to the spread of genetic and epigenetic
alterations from a primary tumor site to surrounding normal tissues, leading to
the formation of secondary tumors. understanding field effect is essential for
cancer research as it provides insights into the mechanisms underlying tumor
development and progression. tumor-associated stroma, which consists of various
cell types, such as fibroblasts, smooth muscle cells, and nerve cells, is an
integral component of the tumor microenvironment that plays a critical role in
tumor development and progression. reactive stroma, a distinct phenotype of
stromal cells, arises in response to signaling pathways from cancerous cells and
is characterized by altered stromal cells and increased extracellular matrix
components [7,8]. reactive stroma is often associated with tumor-associated
stroma and is thought to be a result of field effects in prostate cancer.
altered stroma can create a pro-tumorigenic environment by producing a multitude
of chemokines, growth factors, and releasing reactive oxygen species [9,10],
which can lead to tumor development and aggressiveness [11]. therefore,
investigating the histological characterization of tumor-associated stroma is
crucial in gaining insights into the field effect and tumor progression of
prostate cancer.manual review for tumor-associated stroma is time-consuming and
lacks quantitative metrics [12,13]. several automated methods have been applied
to analyze the tumor-stroma relationship; however, most of them focus on
identifying a tumor-stroma ratio rather than finding reactive stroma tissue or
require pathologist input. machine learning algorithms have been used to
quantify the percentage of tumor to stroma in bladder cancer patients, but
required dichotomizing patients based on a threshold [14]. software has been
used to segment tumor and stroma tissue in breast cancer patient samples, but
the method required constant supervision by a pathologist [15]. similarly, akoya
biosciences inform software was used to quantify reactive stroma in pca, but
this method required substantial pathologist input to train the software [16].
fully automated deep-learning methods have been developed to identify
tumor-associated stroma in breast cancer biopsies, achieving an auc of 0.962 in
predicting invasive ductal cancer [13]. however, identifying tumor-associated
stroma in prostate biopsies and whole-mount histopathology slides remains
challenging.analyzing tumor-associated stroma in prostate cancer requires
combining whole-mount and biopsy histopathology slides. biopsy slides provide
information on the presence of pca, while whole-mount slides provide information
on the extent and distribution of pca, including more information on
tumor-associated stroma. combining the information from both modalities can
provide a more accurate understanding of the tumor microenvironment. in this
work, we explore the field effect in prostate cancer by analyzing
tumor-associated stroma in multimodal histopathological images. our main
contributions can be summarized as follows:-to the best of our knowledge, we
present the first deep-learning approach to characterize prostate
tumor-associated stroma by integrating histological image analysis from both
whole-mount and biopsy slides. our research offers a promising computational
framework for in-depth exploration of the field effect and cancer progression in
prostate cancer. -we proposed a novel approach for stroma classification with
spatial graphs modeling, which enable more accurate and efficient analysis of
tumor microenvironment in prostate cancer pathology. given the spatial nature of
cancer field effect and tumor microenvironment, our graph-based method offers
valuable insights into stroma region analysis. -we developed a comprehensive
pipeline for constructing tumor-associated stroma datasets across multiple data
sources, and employed adversarial training and neighborhood consistency
regularization techniques to learn robust multimodal-invariant image
representations.",6
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.1,Stroma Tissue Segmentation,"accurately analyzing tumor-associated stroma requires a critical pre-processing
step of segmenting stromal tissue from the background, including epithelial
tissue. this segmentation task is challenging due to the complex and
heterogeneous appearance of the stroma. to address this, we propose utilizing
the pointrend model [17], which can handle complex shapes and appearances and
produce smooth and accurate segmentations through iterative object boundary
refinement. moreover, the model's efficiency and ability to process large images
quickly make it suitable for analyzing whole-mount slides. by leveraging the
pointrend model, we can generate stromal segmentation masks for more precise
downstream analysis.",6
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.2,Stroma Classification with Spatial Patch Graphs,"to capture the spatial nature of field effect and analyze tumor-associated
stroma, modeling spatial relationships between stroma patches is essential. the
spatial relationship can reveal valuable information about the tumor
microenvironment, and neighboring stroma cells can undergo similar phenotypic
changes in response to cancer. therefore, we propose using a spatial patch graph
to capture the highorder relationship among stroma tissue regions. we construct
the stroma patch graph using a k-nearest neighbor (knn) graph and neighbor
sampling. the knn graph connects each stroma patch to its k nearest neighboring
patches. given a central stroma patch, we iteratively add neighboring patches to
construct the patch graph until we reach a specified layer number l to control
the subgraph size. this process results in a tree-like subgraph with each layer
representing a different level of spatial proximity to the central patch. the
use of neighbor sampling enables efficient processing of large images and allows
for stochastic training of the model.to predict tumor-associated binary labels
of stroma patches, we employ a message-passing approach that propagates patch
features in the spatial graph. to achieve this, we use graph convolutional
networks with attention, also known as graph attention networks (gats) [18]. the
gat uses an attention mechanism on node features to construct a weighting kernel
that determines the importance of nodes in the message-passing process. in our
case, the patch graph g is constructed using the stroma patches as vertices, and
we connect the nodes with edges based on their spatial proximity. each vertex v
i is associated with a feature vector h vi ∈ r n , which is first extracted by
resnet-50 model [19]. the gat layer is defined aswhere w ∈ r m ×n is a learnable
matrix transforming n -dimensional features to m -dimensional features. n e vi
is the neighborhood of the node v i connected by e in g. gat uses attention
mechanism to construct the weighting coefficients as:where t represents
transposition, is the concatenation operation, and ρ is leakyrelu function. the
final output of gat module is the tumor-associated probability of the input
patch. and the module was optimized using the crossentropy loss l gat in an
end-to-end fashion.",6
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.3,Neighbor Consistency Regularization for Noisy Labels,"the labeling of tumor-associated stroma can be affected by various factors,
which can result in noisy labels. one of the reasons for noisy labels is the
irregular distribution of the field effect, which makes it challenging to define
a clear boundary between the tumor-associated and normal stroma regions.
additionally, the presence of tumor heterogeneity and the varied distribution of
tumor foci can further complicate the labeling process.to address this issue, we
propose applying neighbor consistency regularization (ncr) [20] to prevent the
model from overfitting to incorrect labels. the assumption is that overfitting
happens to a lesser degree before the final classifier, and this is supported by
moit [21], which suggests that feature representations are capable of
distinguishing between noisy and clean examples during model training. based on
this assumption, ncr introduces a neighbor consistency loss to encourage similar
predictions of stroma patches that are similar in feature space. this loss
penalizes the divergence of a patch prediction from a weighted combination of
its neighbors' predictions in feature space, where the weights are determined by
their feature similarity. specifically, the loss function is designed as
follows:where d kl is the kl-divergence loss to quantify the discrepancy between
two probability distributions, t represents the temperature and nn k (v i ) is
the set of k nearest neighbors of v i in the feature space.",6
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,2.4,Adversarial Multi-modal Learning,"biopsy and whole-mount slides provide complementary multi-modal information on
the tumor microenvironment, and combining them can provide a more comprehensive
understanding of tumor-associated stroma. however, using data from multiple
modalities can introduce systematic shifts, which can impact the performance of
a deep learning model. specifically, whole-mount slides typically contain larger
tissue sections and are processed using different protocols than biopsy slides,
which can result in differences in image quality, brightness, and contrast.
these technical differences can affect the pixel intensity distributions of the
images, leading to systematic shifts in the features that the deep learning
model learns to associate with tumor-associated stroma. for instance, a model
trained on whole-mount slides only may not generalize well to biopsy slides due
to systematic shifts, hindering model performance in the clinical application
scenario.to address the above issues, we propose an adversarial multi-modal
learning (aml) module to force the feature extractor to produce
multimodal-invariant representations on multiple source images. specifically, we
incorporate a source discriminator adversarial neural network as auxiliary
classifier. the module takes the stroma embedding as an input and predicts the
source of the image (biopsy or whole-mount) using multilayer perceptron (mlp)
with cross-entropy loss function l aml . the overall loss function of the entire
model is computed as:where hyper-parameters α and β control the impact of each
loss term. all modules were concurrently optimized in an end-to-end manner. the
stroma classifier and source discriminator are trained simultaneously, aiming to
effectively classify tumor-associated stroma while impeding accurate source
prediction by the discriminator. the optimization process aims to achieve a
balance between these two goals, resulting in an embedding space that encodes as
much information as possible about tumor-associated stroma identification while
not encoding any information on the data source. by adopting the adversarial
learning strategy, our model can maintain the correlated information and shared
characteristics between two modalities, which will enhance the model's
generalization and robustness.",6
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3.1,Dataset,"in our study, we utilized three datasets for tumor-associated stroma
analysis.(1) dataset a comprises 513 tiles extracted from the whole mount slides
of 40 patients, sourced from the archives of the pathology department at
cedars-sinai medical center (irb# pro00029960). it combines two sets of tiles:
224 images from 20 patients featuring stroma, normal glands, low-grade and
highgrade cancer [22], along with 289 images from 20 patients with dense
high-grade cancer (gleason grades 4 and 5) and cribriform/non-cribriform glands
[23]. each tile measures 1200×1200 pixels and is extracted from whole slide
images captured at 20x magnification (0.5 microns per pixel). the tiles were
annotated at the pixel-level by expert pathologists to generate stroma tissue
segmentation masks and were cross-evaluated and normalized to account for stain
variability.(2) dataset b included 97 whole mount slides with an average size of
over 174,000×142,000 pixels at 40x magnification. the prostate tissue within
these slides had an average tumor area proportion of 9%, with an average tumor
area of 77 square mm. an expert pathologist annotated the tumor region
boundaries at the region-level, providing exhaustive annotations for all tumor
foci. (3) dataset c comprised 6134 negative biopsy slides obtained from 262
patients' biopsy procedures, where all samples were diagnosed as negative. these
slides are presumed to contain predominantly normal stroma tissues without
phenotypic alterations in response to cancer. dataset a was utilized for
training the stroma segmentation model. extensive data augmentation techniques,
such as image scaling and staining perturbation, were employed during the
training process. the model achieved an average test dice score of 95.57 ± 0.29
through 5-fold cross-validation. this model was then applied to generate stroma
masks for all slides in datasets b and c. to precisely isolate stroma tissues
and avoid data bleeding from epithelial tissues, we only extracted patches where
over 99.5% of the regions were identified as stroma at 40x magnification to
construct the stroma classification dataset.for positive tumor-associated stroma
patches, we sampled patches near tumor glands within annotated tumor region
boundaries, as we presumed that tumor regions represent zones in which the
greatest amount of damage has progressed. for negative stroma patches, we
calculated the tumor distance for each patch by measuring the euclidean distance
from the patch center to the nearest edge of the labeled tumor regions. negative
stroma patches were then sampled from whole mount slides with a gleason group
smaller than 3 and a tumor distance larger than 5 mm. this approach aims to
minimize the risk of mislabeling tumor-associated stroma as normal tissue.
setting a 5mm threshold accounts for the typically minimal inflammatory
responses induced by prostate cancers, particularly in lower-grade cases. to
incorporate multi-modal information, we randomly sampled negative stroma patches
from all biopsy slides in dataset c. overall, we selected over 1.1 million
stroma patches of size 256×256 pixels at 40x magnification for experiments.
during model training and testing, we performed stain normalization and standard
image augmentation methods.",6
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3.2,Model Training and Evaluation,"for constructing knn-based patch graphs, we limited the graph size by setting k
= 4 and layer number l = 3. we controlled the strength of the ncr and aml terms
by setting α = 0.25 and β = 0.5, respectively. the adam optimizer with a
learning rate of 0.0005 was used for model training. all models were implemented
using pytorch on a single tesla v100 gpu. to evaluate the model performance, we
perform 5-fold cross-validation, where all slides are stratified by source
origin and divided into 5 subsets. in each cross-validation trial, one subset
was taken as the test set while the remaining subsets constituted the training
set. we measure the prediction performance using the area under the receiver
operating characteristic (auroc), f1 score, precision, and recall. to evaluate
the effectiveness of our proposed method, we conducted an ablation study by
comparing the performance of different model variants presented in table 1.
specifically, the base model is the resnet-50 feature extractor for
tumor-associated stroma classification. each model variant included a different
combination of modules presented in method sections. we systematically add one
or more modules to the base model to evaluate their performance contribution.
the results show that the full model outperforms the base model by a large
margin with 10.04% in auroc and 10.97% in f1 score, and each module contributes
to the overall performance. compared to the base model, the addition of the gat
module resulted in a significant improvement in all metrics, suggesting spatial
information captured by the patch graph was valuable for stroma classification.
the most notable performance improvement was achieved by the aml module, with a
5.72% increase in auroc and 5.55% increase in recall. this improvement indicates
that aml helps the model better capture the multimodal-invariant features that
are associated with tumor-associated stroma while reducing the false negative
prediction by eliminating the influence of systematic shift cross modalities.
finally, the addition of the ncr module further increased the average model
performance and improved the model robustness across 5 folds. this suggests that
ncr was effective in handling noisy labels and improving model's generalization
ability.",6
Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,4.0,Results and Discussions,"in conclusion, our study introduced a deep learning approach to accurately
characterize the tumor-associated stroma in multi-modal prostate histopathology
slides. our experimental results demonstrate the feasibility of using deep
learning algorithms to identify and quantify subtle stromal alterations,
offering a promising tool for discovering new diagnostic and prognostic
biomarkers of prostate cancer. through exploring field effect in prostate
cancer, our work provides a computational system for further analysis of tumor
development and progression. future research can focus on validating our
approach on larger and more diverse datasets and expanding the method to a
patient-level prediction system, ultimately improving prostate cancer diagnosis
and treatment.",6
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,1.0,Introduction,"the ability to predict the future risk of patients with cancer can significantly
assist clinical management decisions, such as treatment and monitoring [21].
generally, pathologists need to manually assess the pathological images obtained
by whole-slide scanning systems for clinical decision-making, e.g., cancer
diagnosis and prognosis [20]. however, due to the complex morphology and
structure of human tissues and the continuum of histologic features phenotyped
across the diagnostic spectrum, it is a tedious and time-consuming task to
manually assess the whole slide image (wsi) [12]. moreover, unlike cancer
diagnosis and subtyping tasks, survival prediction is a future state prediction
task with higher difficulty. therefore, automated wsi analysis method for
survival prediction task is highly demanded yet challenging in clinical
practice.over the years, deep learning has greatly promoted the development of
computational pathology, including wsi analysis [9,17,24]. due to the huge size,
wsis are generally cropped to numerous patches with a fixed size and encoded to
patch features by a cnn encoder (e.g., imagenet pretrained resnet50 [11]) for
further analysis. the attention-dominated learning frameworks (e.g., abmil [13],
clam [18], dsmil [16], transmil [19], scl-wc [25], hipt [4], nagcn [9]) mainly
aim to find the key instances (e.g., patches and tissues) for wsi representation
and decision-making, which prefers the needle-ina-haystack tasks, e.g., cancer
diagnosis, cancer subtyping, etc. to handle cancer survival prediction, some
researchers integrated some attribute priors into the network design [5,26]. for
example, patch-gcn [5] treated the wsi as point cloud data, and the patch-level
adjacent relationship of wsi is learned by a graph convolutional network (gcn).
however, the fixed-size patches cropped from wsi mainly contain single-level
biological entities (e.g., cells), resulting in limited structural information.
deepattnmisl [26] extracted the phenotype patterns of the patient via a
clustering algorithm, which provides meaningful medical prior to guide the
aggregation of patch features. however, this cluster analysis strategy only
focuses on a single sample, which cannot describe the whole picture of the
pathological components specific to the cancer type. additionally, existing
learning frameworks often ignore the capture of contextual interactions of
pathological components (e.g., tumor, stroma, lymphocyte, etc.), which is
considered as important evidence for cancer survival prediction tasks [1,6].
therefore, wsi-based cancer survival prediction still remains a challenging
task.in summary, to better capture the prognosis-related information in wsi, two
technical key points should be fully investigated: (1) an analysis strategy to
mine more comprehensive and in-depth prior of wsis, and (2) a promising learning
network to explore the contextual interactions of pathological components. to
this end, this paper presents a novel multi-scope analysis driven learning
framework, called hierarchical graph transformer (hgt), to pertinently resolve
the above technical key points for more reliable and interpretable w. hou and y.
he-contributed equally to this work. wsi-based survival prediction. first, to
mine more comprehensive and in-depth attribute priors of wsi, we propose a
multi-scope analysis strategy consisting of in-slide superpixels and cross-slide
clustering, which can not only extract the spatial prior but also identify the
semantic prior of wsis. second, to explore the contextual interactions of
pathological components, we design a novel learning network, i.e., hgt, which
consists of a hierarchical graph convolution layer and a transformer-based
prediction head. specifically, based on the extracted spatial topology, the
hierarchical graph convolution layer in hgt progressively aggregate the
patch-level features to the tissue-level features, so as to learn the
topological features of variant microenvironments ranging from fine-grained
(e.g., cell) to coarse-grained (e.g., necrosis, epithelium, etc.). then, under
the guidance of the identified semantic prior, the tissue-level features are
further sorted and assigned to form the feature embedding of pathological
components. furthermore, the contextual interactions of pathological components
are captured with the transformer-based prediction head, leading to reliable
survival prediction and richer interpretability. extensive experiments on three
cancer cohorts (i.e., crc, tcga-lihc and tcga-kirc) demonstrates the
effectiveness and interpretability of our framework. our codes are available at
https:// github.com/baeksweety/superpixel transformer.",6
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.0,Methodology,"figure 1 illustrates the pipeline of the proposed framework. due to the huge
size, wsis are generally cropped to numerous patches with a fixed size (i.e.,
256×256) and encoded to patch features v patch ∈ r n×d in the embedding space d
by a cnn encoder (i.e., imagenet pretrained resnet50 [11]) for further analysis,
where n is the number of patches, d = 1024 is the feature dimension. the goal of
wsi-based cancer survival prediction is to learn the feature embedding of v in a
supervised manner and output the survival risk o ∈ r 1 .however, conventional
patch-level analysis cannot model complex pathological patterns (e.g., tumor
lymphocyte infiltration, immune cell composition, etc.), resulting in limited
cancer survival prediction performance. to this end, we proposed a novel
learning network, i.e., hgt, which utilized the spatial and semantic priors
mined by a multi-scope analysis strategy (i.e., in-slide superpixel and
cross-slide clustering) to represent and capture the contextual interaction of
pathological components. our framework consists two modules: a hierarchical
graph convolutional network and a transformer-based prediction head.",6
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.1,Hierarchical Graph Convolutional Network,"unlike cancer diagnosis and subtyping, cancer survival prediction is a quite
more challenging task, as it is a future event prediction task which needs to
consider complex pathological structures [20]. however, the conventional
patch-level analysis is difficult to meet this requirement. therefore, it is
essential to extract and combine higher-level topology information for better
wsi representation. in-slide superpixel. as shown in fig . 1, we first employ a
simple linear iterative clustering (slic) [2] algorithm to detect
non-overlapping homogeneous tissues of the foreground of wsi at a low
magnification, which can be served as the spatial prior to mine the hierarchical
topology of wsi. intuitively, the cropped patches and segmented tissues in a wsi
can be considered as hierarchical entities ranging from fine-grained level
(e.g., cell) to coarse-grained level (e.g., necrosis, epithelium, etc.). based
on the in-slide superpixel, the tissue adjacency matrix e tissue ∈ r m×m can be
obtained, where m denote the number of superpixels. then, the patches in each
superpixel are further connected in an 8-adjacent manner, thus generating patch
adjacency matrix e patch ∈ r n×n . the spatial assignment matrix between cropped
patches and segmented tissues is denoted aspatch graph convolutional layer.
based on the spatial topology extracted by in-slide superpixel, the patch graph
convolutional layer (patch gcl) is designed to learn the feature of the
fine-grained microenvironment (e.g., cell) through the message passing between
adjacent patches, which can be represented as:where σ(•) denotes the activation
function, such as relu. graphconv denotes the graph convolutional operation,
e.g., gcnconv [15], graphsage [10], etc.tissue graph convolutional layer. third,
based on the spatial assignment matrix a spa , the learned patch-level features
can be aggregated to the tissue-level features which contain the information of
necrosis, epithelium, etc.where [•] t denote the matrix transpose operation. the
tissue graph convolutional layer (tissue gcl) is further designed to learn the
feature of this coarse-grained microenvironment, which can be represented as:",6
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,2.2,Transformer-Based Prediction Head,"clinical studies have shown that cancer survival prediction requires considering
not only the biological morphology but also the contextual interactions of tumor
and surrounding tissues [1]. however, existing analysis frameworks for wsi often
ignore the capture of contextual interactions of pathological components (e.g.,
tumor, stroma, lymphocyte, etc.), resulting in limited performance and
interpretability. therefore, it is necessary to determine the feature embedding
of pathological components and investigate their contextual interactions for
more reliable predictions.cross-slide clustering. as shown in fig. 1, we perform
the k-means algorithm on the encoded patch features of all training wsis to
generate k pathological components p ∈ r k×d in the embedding space d. p
represents different pathological properties specific to the cancer type.
formally, the feature embedding of each tissue in space d is defined as the mean
feature embeddings of the patches within the tissue. and then, the pathological
component label of each tissue is determined as the component closest to the
euclidean distance of the tissue in space d. the semantic assignment matrix
between segmented tissues and pathological components is denoted as a sem ∈ r
m×k .transformer architecture. under the guidance of the semantic prior
identified by cross-slide clustering, the learned tissue features v tissue can
be further aggregated, forming a series meaningful component embeddings p
specific to the cancer type.then we employed a transformer [22] architecture to
mine the contextual interactions of p and output the predicted survival risk. as
shown in fig. 1, p is concatenated with an extra learnable regression token r
and attached with positional embeddings e p os , which are processed by:where p
out is the output of transformer, mhsa is the multi-headed self-attention [22],
ln is layer normalization and mlp is multilayer perceptron. finally, the
representation of the regression token at the output layer of the transformer,
i.e., [p out ] 0 , is served as the predicted survival risk o.",6
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,3.2,Comparative Results,"we compared seven state-of-the-art methods (sotas), i.e., deepsets [27], abmil
[13], deepattnmisl [26], clam [18], dsmil [16], patchgcn [5], and transmil [19].
we also compared three baselines of our method, i.e., w/o patch gcl, w/o tissue
gcl and w/o transformer. for fair comparison, same cnn extractor (i.e. imagenet
pretrained resnet50 [11]), and survival prediction loss (i.e. cox loss [26]) is
adopt for all methods. table 1 and fig. 2 show the results of ci and km-analysis
of each method, respectively. generally, most mil methods, i.e., deepsets,
abmil, dsmil, transmil mainly focus on a few key instances for prediction, but
they do not have significant advantages in cancer prognosis. furthermore, due to
the large size of crc dataset and relatively high model complexity, patch-gcn
and transmil encountered a memory overflow when processing the crc dataset,
which limits their clinical application. deepattnmisl has a certain semantic
perception ability for patch, which achieves better performance in lihc cohort.
patchgcn is capable to capture the local contextual interactions between patch,
which also achieves satisfied performance in kirc cohort. as our method has
potential to explore the contextual interactions of pathological components,
which more in line with the thinking of pathologists for cancer prognosis. our
method achieves higher ci and relatively low p-value (< 0.05) of km analysis on
both three cancer cohorts, which consistently outperform the sotas and
baselines. in addition, the feature aggregation of the lower levels (i.e., patch
and tissue) are guided by the priors, and the mhsa is only executed on
pathological components, resulting in high efficiency even on the crc dataset.
[13] 0.580 ± 0.005 0.634 ± 0.005 0.617 ± 0.094 deepattnmisl [26] 0.570 ± 0.001
0.644 ± 0.009 0.584 ± 0.019 clam [18] 0.575 ± 0.010 0.641 ± 0.002 0.635 ± 0.006
dsmil [16] 0.550 ± 0.016 0.626 ± 0.005 0.603 ± 0.022 patchgcn [",6
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,3.3,Interpretability of the Proposed Framework,"we selected the crc dataset for further interpretable analysis, as it is one of
the leading causes of mortality in industrialized countries, and its
prognosis-related factors have been widely studied [3,8]. we trained an encoded
feature based classification model (i.e., a mlp) on a open-source colorectal
cancer dataset (i.e., nct-crc-he-100k [14]), which is annotated with 9 classes,
including: adipose tissue (adi); background (back); debris (deb); lymphocytes
(lym); mucus (muc); muscle (mus); normal colon mucosa (norm); stroma (str);
tumor (tum). the trained classification model can be used to determine the
biological semantics of the pathological components extracted by our model with
a major voting rule. figure 3 shows the original image, spatial topology,
proportion and biological meaning of pathological components, and its contextual
interactions of a typical case from crc cohort. it can be seen that the
interaction between component 1 (tum) and component 9 (str) has gained the
highest attention of the network, which is consistent with the existing
knowledge [3,8]. moreover, there is also concentration of interaction in some
other interactions, which may potentially imply some new biomarkers.",6
Multi-scope Analysis Driven Hierarchical Graph Transformer for Whole Slide Image Based Cancer Survival Prediction,4.0,Conclusion,"in this paper, we propose a novel learning framework, i.e., multi-scope analysis
driven hgt, to effectively represent and capture the contextual interaction of
pathological components for improving the effectiveness and interpretability of
wsi-based cancer survival prediction. experimental results on three clinical
cancer cohorts demonstrated our model achieves better performance and richer
interpretability over the existing models. in the future, we will evaluate our
framework on more tasks and further statistically analyze the interpretability
of our model to find more pathological biomarkers related to cancer prognosis.",6
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,1.0,Introduction,"ultrasound is a widely-used imaging modality for clinical cancer screening. deep
learning has recently emerged as a promising approach for ultrasound lesion
detection. while previous works focused on lesion detection in still images [25]
and offline videos [9,11,22], this paper explores real-time ultrasound video
lesion detection. real-time lesion prompts can assist radiologists during
scanning, thus being more helpful to improve the accuracy of diagnosis. this
task requires the model to infer faster than 30 frames per second (fps) [19] and
only previous frames are available for current frame processing.previous
general-purpose detectors [1,2] report simple and obvious fps when applied to
ultrasound videos, e.g. the red box in fig. 1(a). these fps, attributable to
non-lesion anatomies, can mislead junior readers. these anatomies appear like
lesions in certain frames, but typically show negative symptoms in adjacent
frames when scanned from different positions. so experienced radiologists will
refer to corresponding regions in previous frames, denoted as temporal contexts
(tc), to help restrain fps. if tc of a lesion-like region exhibit negative
symptoms, denoted as negative temporal contexts (ntc), radiologists are less
likely to report it as a lesion [15]. although important, the utilization of ntc
remains unexplored. in natural videos, as transitions from non-objects to
objects are implausible, previous works [1,2,20] only consider inter-object
relationships. as shown in sect. 4.4, the inability to utilize ntc is a key
issue leading to the fps reported by general-purpose detectors.to address this
issue, we propose a novel ultradet model to leverage ntc. for each region of
interest (roi) r proposed by a basic detector, we extract temporal contexts from
previous frames. to compensate for inter-frame motion, we generate deformed
grids by applying inverse optical flow to the original regular roi grids,
illustrated in fig. 1. then we extract the roi features from the deformed grids
in previous frames and aggregate them into r. we call the overall process
negative temporal context aggregation (ntca). the ntca module leverages
roi-level ntc which are crucial for radiologists but ignored in previous works,
thereby effectively improving the detection performance in a reliable and
interpretable way. we plug the ntca module into a basic real-time detector to
form ultradet. experiments on cva-bus dataset [9] demonstrate that ultra-det,
with real-time inference speed, significantly outperforms previous works,
reducing about 50% fps at a recall rate of 0.90.our contributions are four-fold.
(1) we identify that the failure of generalpurpose detectors on ultrasound
videos derives from their incapability of utilizing negative temporal contexts.
(2) we propose a novel ultradet model, incorpo-rating an ntca module that
effectively leverages ntc for fp suppression. (3) we conduct extensive
experiments to demonstrate the proposed ultradet significantly outperforms the
previous state-of-the-arts. (4) we release high-quality labels of the cva-bus
dataset [9] to facilitate future research.",6
Mining Negative Temporal Contexts for False Positive Suppression in Real-Time Ultrasound Lesion Detection,5.0,Conclusion,"in this paper, we address the clinical challenge of real-time ultrasound lesion
detection. we propose a novel negative temporal context aggregation (ntca)
module, imitating radiologists' diagnosis processes to suppress fps. the ntca
module leverages negative temporal contexts that are essential for fp
suppression but ignored in previous works, thereby being more effective in
suppressing fps. we plug the ntca module into a basicdet to form the ultradet
model, which significantly improves the precision and fp rates over previous
state-ofthe-arts while achieving real-time inference speed. the ultradet has the
potential to become a real-time lesion detection application and assist
radiologists in more accurate cancer diagnosis in clinical practice.",6
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,1.0,Introduction,"whole slide scanning is increasingly used in disease diagnosis and pathological
research to visualize tissue samples. compared to traditional microscopebased
observation, whole slide scanning converts glass slides into gigapixel digital
images that can be conveniently stored and analyzed. however, the high
resolution of wsis also makes their automated classification challenging [15].
patchbased classification is a common solution to this problem [3,8,24]. it
predicts the slide-level label by first predicting the labels of small, tiled
patches in a wsi. this approach allows for the direct application of existing
image classification models, but requires additional patch-level labeling.
unfortunately, patch-level labeling by histopathology experts is expensive and
time-consuming. therefore, many weakly-supervised [8,24] and semi-supervised
[3,5] methods have been proposed to generate patch-level pseudo labels at a
lower cost. however, the lack of reliable supervision directly hinders the
performance of these methods, and serious class-imbalance problems could arise,
as tumor patches may only account for a small portion of the entire wsi [12].in
contrast, mil-based methods have become increasingly preferred due to their only
demand for slide-level labels [18]. the typical pipeline of mil methods is shown
in fig. 1, where wsis are treated as bags, and tiled patches are considered as
instances. the aim is to predict whether there are positive instances, such as
tumor patches, in a bag, and if so, the bag is considered positive as well. in
practice, a fixed imagenet pre-trained feature extractor g(•) is usually used to
convert the tiled patches in a wsi into feature maps due to limited gpu memory.
these instance features are then aggregated by a(•) into a slide-level feature
vector to be sent to the bag-level classifier f (•) for mil training. due to the
high computational cost, end-to-end training of the feature extractor and bag
classifier is prohibitive, especially for high-resolution wsis. as a result,
many methods focus solely on improving a(•) or f (•), leaving g(•) untrained on
the wsi dataset (as shown in fig. 2(b)). however, the domain shift between wsi
and natural images may lead to sub-optimal representations, so recently there
have been methods proposed to fine-tune g(•) using self-supervised techniques
[4,12,21] or weakly-supervised techniques [10,13,23] (as shown in fig. 2(c)).
nevertheless, since these two processes are still trained separately with
different supervision signals, they lack joint optimization and may still leads
to inconsistency within the entire mil pipeline. to address the challenges
mentioned above, we propose a novel mil framework called icmil, which can
iteratively couple the patch feature embedding process with the bag-level
classification process to enhance the effectiveness of mil training (as
illustrated in fig. 2(d)). unlike previous works that mainly focused on
designing sophisticated instance aggregators a(•) [12,14,20] or bag classifiers
f (•) [9,16,25], we aim to bridge the loss back-propagation process from f (•)
to g(•) to improve g(•)'s ability to perceive slide-level labels. specifically,
we propose to use the bag-level classifier f (•) to initialize an instance-level
classifier f (•), enabling f (•) to use the category knowledge learned from
bag-level features to determine each instance's category. in this regard, we
further propose a teacher-student [7] approach to effectively generate pseudo
labels and simultaneously fine-tune g(•). after fine-tuning, the domain shift
problem is alleviated in g(•), leading to better patch representations. the new
representations can be used to train a better bag-level classifier in return for
the next round of iteration.in summary, our contributions are: (1) we propose
icmil which bridges the loss propagation from the bag classifier to the patch
embedder by iteratively coupling them during training. this framework fine-tunes
the patch embedder based on the bag-level classifier, and the refined
embeddings, in turn, help train a more accurate bag-level classifier. (2) we
propose a teacher-student approach to achieve effective and robust knowledge
transfer from the bag-level classifier f (•) to the instance-level
representation embedder g(•). (3) we conduct extensive experiments on two
datasets using three different backbones and demonstrate the effectiveness of
our proposed framework.",6
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3.1,Datasets,"our experiments utilized two datasets, with the first being the publicly
available breast cancer dataset, camelyon16 [1]. this dataset consists of a
total of 399 wsis, with 159 normal and 111 metastasis wsis for the training set,
and the remaining 129 for test. although patch-level labels are officially
provided in camelyon16, they were not used in our experiments.the second dataset
is a private hepatocellular carcinoma (hcc) dataset collected from sir run run
shaw hospital, hangzhou, china. this dataset comprises a total of 1140 valid
tumor wsis scanned at 40× magnification, and the objective is to identify the
severity of each case based on the edmondson-steiner (es) grading. the ground
truth labels are binary classes of low risk and high risk, which were provided
by experienced pathologists.",6
Iteratively Coupled Multiple Instance Learning from Instance to Bag Classifier for Whole Slide Image Classification,3.3,Experimental Results,"ablation study. the results of ablation studies are presented in table 1. from
table 1(a), we can learn that as the number of icmil iteration increases, the
performance will also go up until reaching a stable point. since the number of
instances is very large in wsi datasets, we empirically recommend to choose to
run icmil one iteration for fine-tuning g(•) to achieve the balance between
performance gain and time consumption. from table 1(b), it is shown that our
teacher-student-based method outperforms the naïve ""pseudo label generation""
method for fine-tuning g(•), which demontrates the effectiveness of introducing
the learnable instance-level classifier f (•).comparison with other methods.
experimental results are presented in table 2. as shown, our icmil framework
consistently improves the performance of three different mil baselines (i.e.,
max pool, ab-mil, and dtfd-mil), demonstrating the effectiveness of bridging the
loss back-propagation from bag calssifier to embedder. it proves that a more
suitable patch embedding can greatly enhance the overall mil classification
framework. when used with the state-of-the-art mil method dtfd-mil, icmil
further increases its performance on camelyon16 by 0.5% auc, 2.1% f1, and 1.6%
acc. results on the hcc dataset also proves the effectiveness of icmil, despite
the minor difference on the relative performance of baseline methods. mean
pooling performs better on this dataset due to the large area of tumor in the
wsis (about 60% patches are tumor patches), which mitigates the impact of
average pooling on instances. also, the performance differences among different
vanilla mil methods tends to be smaller on this dataset since risk grading is a
harder task than camelyon16. in this situation, the quality of instance
representations plays a crucial role in generating more separable bag-level
representations. as a result, after applying icmil on the mil baselines, these
methods all gain great performance boost on the hcc dataset. furthermore, fig. 5
displays the instance-level and bag-level representations of camelyon16 dataset
before and after applying icmil on ab-mil backbone.the results indicate that one
iteration of g(•) fine-tuning in icmil significantly improves the instance-level
representations, leading to a better aggregated baglevel representation
naturally. besides, the bag-level representations are also more closely aligned
with the instance representations, proving that icmil can reduce the
inconsistencies between g(•) and f (•) by coupling them together for training,
resulting in a better separability.",6
Artifact Restoration in Histology Images with Diffusion Probabilistic Models,1.0,Introduction,"histology is critical for accurately diagnosing all cancers in modern medical
imaging analysis. however, the complex scanning procedure for histological
wholeslide images (wsis) digitization may result in the alteration of tissue
structures, due to improper removal, fixation, tissue processing, embedding, and
storage [11]. typically, these changes in tissue details can be caused by
various extraneous factors such as bubbles, tissue folds, uneven illumination,
pen marks, altered staining, and etc [13]. formally, the changes in tissue
structures are known as artifacts. the presence of artifacts not only makes the
analysis more challenging for pathologists but also increases the risk of
misdiagnosis for computer-aided diagnosis (cad) systems [14]. particularly, deep
learning models, which have become increasingly prevalent in histology analysis,
have shown vulnerability to the artifact, resulting in a two-times increase in
diagnosis errors [18]. [19] formulates the artifact restoration as an
image-to-image transfer problem. it leverages two pairs of the generator and
discriminator to learn the transfer between the artifact and artifactfree image
domains. (b) diffusion probabilistic model [5] (ours) formulates artifact
restoration as a regional denoising process.in real clinical practice,
rescanning the wsis that contain artifacts can partially address this issue.
however, it may require multiple attempts before obtaining a satisfactory wsi,
which can lead to a waste of time, medical resources, and deplete tissue
samples. discarding the local region with artifacts for deep learning models is
another solution, but it may result in the loss of critical contextual
information. therefore, learning-based artifact restoration approaches have
gained increasing attention. for example, cyclegan [19] formulates the artifact
restoration as an image-to-image transfer problem by learning the transfer
between the artifact and artifact-free image domains from unpaired images, as
depicted in fig. 1(a). however, existing artifact restoration solutions are
confined to generative adversarial networks (gans) [2], which are difficult to
train due to the mode collapse and are prone to suffer from unexpected stain
style mistransfer. to address these issues, we make the first attempt at a
diffusion probabilistic model for artifact restoration approach [5], as shown in
fig. 1(b). innovatively, our framework formulates the artifact restoration as a
regional denoising process, which thus can to the most extent preserve the stain
style and avoid the loss of contextual information in the non-artifact region.
furthermore, our approach is trained solely with artifact-free images, which
reduces the difficulty in data collection.the major contributions are two-fold.
(1) we make the first attempt at a denoising diffusion probabilistic model for
artifact removal, called artifusion. this approach differs from gan-based
methods that require either paired or unpaired artifacts and artifact-free
images, as our artifusion relies solely on artifact-free images, resulting in a
simplified training process. (2) to capture the local-global correlations in the
gradual regional artifact restoration process, we innovatively propose a
swin-transformer denoising architecture to replace the commonly-used u-net and a
time token scheme for optimal swin-transformer denoising. extensive evaluations
on real-world histology datasets and downstream tasks demonstrate the
superiority of our framework in artifact removal performance, which can generate
reliable restored images while preserving the stain style.",6
Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,1.0,Introduction,"gout is the most common inflammatory arthritis and musculoskeletal ultrasound
(mskus) scanning is recommended to diagnose gout due to the non-ionizing
radiation, fast imaging speed, and non-invasive characteristics of mskus [7].
however, misdiagnosis of gout can occur frequently when a patient's clinical
characteristics are atypical. traditional mskus diagnosis relies on the
experience of the radiologist which is time-consuming and labor-intensive.
although convolutional neural networks (cnns) based ultrasound classification
models have been successfully used for diseases such as thyroid nodules and
breast cancer, conspicuously absent from these successful applications is the
use of cnns for gout diagnosis from mskus images. there are significant
challenges in cnn based gout diagnosis. firstly, the gout-characteristics
contain various types including double contour sign, synovial hypertrophy,
synovial effusion, synovial dislocation and bone erosion, and these
gout-characteristics are small and difficult to localize in mskus. secondly, the
surrounding fascial tissues such as the muscle, sarcolemma and articular capsule
have similar visual traits with gout-characteristics, and we found the existing
cnn models can't accurately pay attention to the gout-characteristics that
radiologist doctors pay attention to during the diagnosis process (as shown in
fig. 1). due to these issues, sota cnn models often fail to learn the gouty
mskus features which are key factors for sonographers' decision.in medical image
analysis, recent works have attempted to inject the recorded gaze information of
clinicians into deep cnn models for helping the models to predict correctly
based on lesion area. mall et al. [9,10] modeled the visual search behavior of
radiologists for breast cancer using cnn and injected human visual attention
into cnn to detect missing cancer in mammography. wang et al. [15] demonstrated
that the eye movement of radiologists can be a new supervision form to train the
cnn model. cai et al. [3,4] developed the sononet [1] model, which integrates
eye-gaze data of sonographers and used generative adversarial networks to
address the lack of eye-gaze data. patra et al. [11] proposed the use of a
teacher-student knowledge transfer framework for us image analysis, which
combines doctor's eye-gaze data with us images as input to a large teacher
model, whose outputs and intermediate feature maps are used to condition a
student model. although these methods have led to promising results, they can be
difficult to implement due to the need to collect doctors' eye movement data for
each image, along with certain restrictions on the network structure.different
from the existing studies, we propose a novel framework to adjust the general
cnns to ""think like sonographers"" from three different levels. (1) where to
adjust: modeling sonographers' gaze map to emphasize the region that needs
adjust; (2) what to adjust: classify the instances to systemically detect
predictions made based on unreasonable/biased reasoning and adjust; (3) how to
adjust: developing a training mechanism to strike the balance between gout
prediction accuracy and attention reasonability.",6
Scribble-Based 3D Multiple Abdominal Organ Segmentation via Triple-Branch Multi-Dilated Network with Pixel- and Class-Wise Consistency,3.1,Dataset and Implementation Details,"we used the publicly available abdomen ct dataset word [17] for experiments,
which consists of 150 abdominal ct volumes from patients with rectal cancer,
prostate cancer or cervical cancer before radiotherapy. each ct volume contains
159-330 slices of 512×512 pixels, with an in-plane resolution of 0.976 × 0.976
mm and slice spacing of 2.5-3.0 mm. we aimed to segment seven organs: the liver,
spleen, left kidney, right kidney, stomach, gallbladder and pancreas. following
the default settings in [17], the dataset was split into 100 for training, 20
for validation and 30 for testing, respectively, where the scribble annotations
for foreground organs and background in the axial view of the training volumes
had been provided and were used in model training. for pre-processing, we cut
off the hounsfield unit (hu) values with a fixed window/level of 400/50 to focus
on the abdominal organs, and normalized it to [0, 1]. we used the
commonlyadopted dice similarity coefficient (dsc), 95% hausdorff distance (hd 95
) and the average surface distance (asd) for quantitative evaluation.our
framework was implemented in pytorch [19] on an nvidia 2080ti with 11 gb memory.
we employed the 3d unet [3] as the backbone network for all experiments, and
extended it with three decoders by embedding two auxiliary decoders with
different dilation rates, as detailed in sect. 2.1. to introduce perturbations,
different initializations were applied to each decoder, and random perturbations
(ratio = (0, 0.5)) were introduced in the bottleneck before the auxiliary
decoders. the stochastic gradient descent (sgd) optimizer with momentum of 0.9
and weight decay of 10 -4 was used to minimize the overall loss function
formulated in eq. 5, where α=10.0 and β=1.0 based on the best performance on the
validation set. the poly learning rate strategy [16] was used to decay learning
rate online. the batch size, patch size and maximum iterations t max were set to
1, [80, 96, 96] and 6 × 10 4 respectively. the final segmentation results were
obtained by using a sliding window strategy. for a fair comparison, we used the
primary decoder's outputs as the final results during the inference stage and
did not use any post-processing methods. note that all experiments were
conducted in the same experimental setting. the existing methods are implemented
with the help of open source codebase from [14].table 1. quantitative comparison
between our method and existing weakly supervised methods on word testing set. *
denotes p-value < 0.05 (paired t-test) when comparing with the second place
method [15]. the best values are highlighted in bold.",7
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,1.0,Introduction,"breast cancer is the most prevalent form of cancer among women and can have
serious physical and mental health consequences if left unchecked [5]. early
detection through mammography is critical for early treatment and prevention
[19]. mammograms provide images of breast tissue, which are taken from two
views: the cranio-caudal (cc) view, and the medio-lateral oblique (mlo) view
[4]. by identifying breast cancer early, patients can receive targeted treatment
before the disease progresses.deep neural networks have been widely adopted for
breast cancer diagnosis to alleviate the workload of radiologists. however,
these models often require a large number of manual annotations and lack
interpretability, which can prevent their broader applications in breast cancer
diagnosis. radiologists typically focus on areas with breast lesions during
mammogram reading [11,22], which provides valuable guidance. we propose using
real-time eye tracking information from radiologists to optimize our model. by
using gaze data to guide model training, we can improve model interpretability
and performance [24].radiologists' eye movements can be automatically and
unobtrusively recorded during the process of reading mammograms, providing a
valuable source of data without the need for manual labeling. previous studies
have incorporated radiologists' eye-gaze as a form of weak supervision, which
directs the network's attention to the regions with possible lesions [15,23].
leveraging gaze from radiologists to aid in model training not only increases
efficiency and minimizes the risk of errors linked to manual annotation, but
also can be seamlessly implemented without affecting radiologists' normal
clinical interpretation of mammograms.mammography primarily detects two types of
breast lesions: masses and microcalcifications [16]. the determination of the
benign or malignant nature of masses is largely dependent on the smoothness of
their edges [13]. the gaze data can guide the model's attention towards the
malignant masses. microcalcifications are small calcium deposits which exhibit
irregular boundaries on mammograms [9]. this feature makes them challenging to
identify, often leading to missed or false detection by models. radiologists
need to magnify mammograms to differentiate between benign scattered
calcifications and clustered calcifications, the latter of which are more likely
to be malignant and necessitate further diagnosis. leveraging gaze data can
guide the model to locate malignant calcifications.in this work, we propose a
novel diagnostic model, namely mammo-net, which integrates radiologists' gaze
data and interactive information between cc-view and mlo-view to enhance
diagnostic performance. to the best of our knowledge, this is the first work to
integrate gaze data into multi-view mammography classification. we utilize class
activation map (cam) [18] to calculate the attention maps for the model.
additionally, we apply pyramid loss to maintain consistency between
radiologists' gaze heat maps and the model's attention maps at multiple scales
of the pyramid [1]. our model is designed for singlebreast cases. mammo-net
extracts multi-view features and utilizes transformerbased attention to
mutualize information [21]. furthermore, there are differences between
multi-view mammograms of the same patient, arising from variations in breast
shape and density. capturing these multi-view shared features can be a challenge
for models. to address this issue, we develop a novel method called
bidirectional fusion learning (bfl) to extract shared features from multi-view
mammograms.our contributions can be summarized as follows:• we emphasize the
significance of low-cost gaze to provide weakly-supervised positioning and
visual interpretability for the model. additionally, we develop a pyramid loss
that adapts to the supervised process. • we propose a novel breast cancer
diagnosis model, namely mammo-net. this model employs transformer-based
attention to mutualize information and uses bfl to integrate task-related
information to make accurate predictions. • we demonstrate the effectiveness of
our approach through experiments using mammography datasets, which show the
superiority of mammo-net.2 proposed method",7
Mammo-Net: Integrating Gaze Supervision and Interactive Information in Multi-view Mammogram Classification,4.0,Conclusion and Discussion,"in this paper, we have developed a breast cancer diagnosis model to mimic the
radiologist's decision-making process. to achieve this, we integrate gaze data
as a form of weak supervision for both lesion positioning and interpretability
of the model. we also utilize transformer-based attention to mutualize
multi-view information and further develop bfl to fully fuse multi-view
information. our experimental results on mammography datasets demonstrate the
superiority of our proposed model. in future work, we intend to explore the use
of scanning path analysis as a means of obtaining insights into the
pathology-relevant regions of lesions.",7
CTFlow: Mitigating Effects of Computed Tomography Acquisition and Reconstruction with Normalizing Flows,4.0,Conclusion,"we developed ctflow, a normalizing flows approach to mitigating variations in ct
scans. we demonstrated that ctflow achieved consistent performance across image
quality metrics, yielding the best perceptual quality score. moreover, ctflow
was better than a gan-based method in maintaining consistent lung nodule
detection performance. compared to generative models, the normalizing flows
approach offers exact and efficient likelihood computation and generates diverse
outputs that are closer to the target distribution.we note several limitations
of this work. in our evaluations, we trained separate ctflow and comparison
models for each mapping (e.g., transforming a 'smooth' kernel to a 'medium'
kernel scan), allowing us to troubleshoot models more easily. a single model
conditioned on different doses and kernels would be more practical. also, ctflow
depends on tuning a variance parameter; better psnr and ssim may have been
achieved with the optimization of this parameter. finally, this study focused on
mitigating the effect of a single ct parameter, either dose (in image quality)
or kernel (in nodule detection). in the real world, multiple ct parameters
interact (dose and kernel); these more complex interactions are being
investigated as part of future work.one underexplored area of normalizing flow
is its ability to generate the full distribution of possible outputs. using this
information, we can estimate where high uncertainty exists in the model output,
providing information to downstream image processing steps, such as
segmentation, object detection, and classification. for example, chan et al.
[28] applied an approximate bayesian inference scheme based on posterior
regularization to improve uncertainty quantification on covariate-shifted data
sets, resulting in improved prognostic models for prostate cancer. investigating
how this uncertainty can be incorporated into downstream tasks, such as our lung
nodule cade algorithm, is also part of future work.",7
Probabilistic Modeling Ensemble Vision Transformer Improves Complex Polyp Segmentation,1.0,Introduction,"colorectal cancer (crc) remains a major health burden with elevated mortality
worldwide [1]. most cases of crc arise from adenomatous polyps or sessile
serrated lesions in 5 to 10 years [9]. colonoscopy is considered the gold
standard for the detection of colorectal polyps. polyp segmentation is a
fundamental task in the computer-aided detection (cade) of polyps during
colonoscopy, which is of great significance in the clinical prevention of
crc.traditional machine learning approaches in polyp segmentation primarily
focus on learning low-level features, such as texture, shape, or color
distribution [14]. in recent years, encoder-decoder based deep learning models
such as u-net [12], unet++ [22], resunet++ [6], and pranet [4] have dominated
the field. furthermore, transformer [3,17,19,20] models have also been proposed
for polyp segmentation, and achieve the state-of-the-art(sota)
performance.despite significant progress made by these binary mask supervised
models, challenges remain in accurately locating polyps, particularly in complex
clinical scenarios, due to their insensitivity to complex lesions and high
false-positive rates. more specifically, most polyps have an elliptical shape
with well-defined boundaries. however, supervised segmentation learning solely
based on binary masks may not be effective in discriminating polyps in complex
clinical scenarios. endoscopic images often contain pseudo-polyp objects with
strong boundaries, such as colon folds, blood vessels, and air bubbles, which
can result in false positives. in addition, sessile and flat polyps have
ambiguous and challenging boundaries to delineate. to address these limitations,
qadir et al. [11] proposed using gaussian masks for supervised model training.
this approach reduces false positives significantly by assigning less attention
to outer edges and prioritizing surface patterns. however, this method has
limitations in accurately segmenting polyp boundaries, which are crucial for
clinical decision-making.therefore, the primary challenge lies in enhancing
polyp segmentation performance in complex scenarios by precisely preserving the
polyp segmentation boundaries, while simultaneously maximizing the decoder's
attention on the overall pattern of the polyps.in this paper, we propose a novel
transformer-based polyp segmentation framework, petnet, which addresses the
aforementioned challenges and achieves sota performance in locating polyps with
high precision. our contributions are threefold:• we propose a novel
gaussian-probabilistic guided semantic fusion method for polyp segmentation,
which improves the decoder's global perception of polyp locations and
discrimination capability for polyps in complex scenarios.• we evaluate the
performance of petnet on five widely adopted datasets, demonstrating its
superior ability to identify polyp camouflage and small polyp scenes, achieving
state-of-the-art performance in locating polyps with high precision.
furthermore, we show that petnet can achieve a speed of about 27fps in edge
computing devices (nvidia jetson orin). • we design several polyp instance-level
evaluation metrics, considering that conventional pixel-level calculation
methods cannot explicitly and comprehensively evaluate the overall performance
of polyp segmentation algorithms.",7
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,1.0,Introduction,"breast cancer (bc) is the most common cancer in women and incidence is
increasing [14]. with the wide adoption of population-based mammography
screening programs for early detection of bc, millions of mammograms are
conducted annually worldwide [23]. developing artificial intelligence (ai) for
abnormality detection is of great significance for reducing the workload of
radiologists and facilitating early diagnosis [21]. besides using the
data-driven manner, to achieve accurate diagnosis and interpretation of the
ai-assisted system output, it is essential to consider mammogram domain
knowledge in a model-driven fashion.authenticated by the bi-rads lexicon [12],
the asymmetry of bilateral breasts is a crucial clinical factor for identifying
abnormalities. in clinical practice, radiologists typically compare the
bilateral craniocaudal (cc) and mediolateral oblique (mlo) projections and seek
the asymmetry between the right and left views. notably, the right and the left
view would not have pixel-level symmetry differences in imaging positions for
each breast and biological variations between the two views. leveraging
bilateral mammograms (bi-mg) is one of the key steps to detect asymmetrical
abnormalities, especially for subtle and non-typical abnormalities. to mimic the
process of radiologists, previous studies only extracted simple features from
the two breasts and used fusion techniques to perform the classification
[6,20,22,24,25]. besides these simple feature-fusion methods, recent studies
have demonstrated the powerful ability of transformerbased methods to fuse
information in multi-view (mv) analysis (cc and mlo view of unilateral breasts)
[1,16,26]. however, most of these studies formulate the diagnosis as an mv
analysis problem without dedicated comparisons between the two breasts.the
question of ""what the bi-mg would look like if they were symmetric?"" is often
considered when radiologists determine the symmetry of bi-mg. it can provide
valuable diagnostic information and guide the model in learning the diagnostic
process akin to that of a human radiologist. recently, two studies explored
generating healthy latent features of target mammograms by referencing
contralateral mammograms, achieving state-of-the-art (sota) classification
performance [18,19]. none of these studies is able to reconstruct a normal
pixellevel symmetric breast in the model design. image generation techniques for
generating symmetric bi-mg have not yet been investigated. visually, the
remaining parts after the elimination of asymmetrical abnormalities are the
appearance of symmetric bi-mg. a more interpretable and pristine strategy is
disentanglement learning [9,17] which utilizes synthetic images to supervise the
model in separating asymmetric anomalies from normal regions at the image
level.in this work, we present a novel end-to-end framework, disasymnet, which
consists of an asymmetric transformer-based classification (asyc) module and an
asymmetric abnormality disentanglement (asyd) module. the asyc emulates the
radiologist's analysis process of checking unilateral and comparing bi-mg for
abnormalities classifying. the asyd simulates the process of disentangling the
abnormalities and normal glands on pixel-level. additionally, we leverage a
self-adversarial learning scheme to reinforce two modules' capacity, where the
feedback from the asyc is used to guide the asyd's disentangling, and the asyd's
output is used to refine the asyc in detecting subtle abnormalities. to",7
DisAsymNet: Disentanglement of Asymmetrical Abnormality on Bilateral Mammograms Using Self-adversarial Learning,2.3,Asymmetric Synthesis for Supervised Reconstruction,"to alleviate the lack of annotation pixel-wise asymmetry annotations, in this
study, we propose a random synthesis method to supervise
disentanglement.training with synthetic artifacts is a low-cost but efficient
way to supervise the model to better reconstruct images [15,17]. in this study,
we randomly select the number n ∈ [1, 2, 3] of tumors t from a tumor set t
inserting into one or both sides of randomized selected symmetric bi-mg (x r , x
l |y asy = 0). for each tumor insertion, we randomly select a position within
the breast region. the tumors and symmetrical mammograms are combined by an
alpha blending-based method [17], which can be denoted by x|fake =the alpha
weights α k is a 2d gaussian distribution map, in which the co-variance is
determined by the size of k-th tumor t, representing the transparency of the
pixels of the tumor. some examples are shown in fig. 1. the tumor set t is
collected from real-world datasets. specifically, to maintain the rule of
weaklysupervised learning of segmentation and localization tasks, we collect the
tumors from the ddsm dataset as t and train the model on the inbreast
dataset.when training the model on other datasets, we use the tumor set
collected from the inbreast dataset. thus, the supervised reconstruction loss is
l syn = l l1 (x|real, x n |fake), where x|real is the real image before
synthesis and x n |fake is the disentangled normal image from the synthesised
image x|fake.",7
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,1.0,Introduction,"over 430,000 new cases of renal cancer were reported in 2020 in the world [1]
and this number is expected to rise [22]. when the tumor size is large (greater
than 7 cm) often the whole kidney is removed, however, when the tumor size is
small (less than 4 cm), partial nephrectomy is the preferred treatment [20] as
it could preserve kidney's function. thus, early detection of kidney tumors can
help to improve patient's prognosis. however, early-stage renal cancers are
usually asymptomatic, therefore they are often incidentally found during other
examinations [19], which includes non-contrast ct (ncct) scans.segmentation of
kidney tumors on ncct images adds challenges compared to contrast-enhanced ct
(cect) images, due to low contrast and lack of multiphase images. on cect
images, the kidney tumors have different intensity values compared to the normal
tissues. there are several works that demonstrated successful segmentation of
kidney tumors with high precision [13,21]. however, on ncct images, as shown in
fig. 1b, some tumors called isodensity tumors, have similar intensity values to
the surrounding normal tissues. to detect such tumors, one must compare the
kidney shape with tumors to the kidney shape without the tumors so that one can
recognize regions with protuberance.3d u-net [3] is the go-to network for
segmenting kidney tumors on cect images. however, convolutional neural networks
(cnns) are biased towards texture features [5]. therefore, without any
intervention, they may fail to capture the protuberance caused by isodensity
tumors on ncct images.in this work, we present a novel framework that is capable
of capturing the protuberances in the kidneys. our goal is to segment kidney
tumors including isodensity types on ncct images. to achieve this goal, we
create a synthetic dataset, which has separate annotations for normal kidneys
and protruded regions, and train a segmentation network to separate the
protruded regions from the normal kidney regions. in order to segment whole
tumors, our framework consists of three networks. the first is a base network,
which extracts kidneys and an initial tumor region masks. the second
protuberance detection network receives the kidney region mask as its input and
predicts a protruded region mask. the last fusion network receives the initial
tumor mask and the protruded region mask to predict a final tumor mask. this
proposed framework enables a better segmentation of isodensity tumors and boosts
the performance of segmentation of kidney tumors on ncct images. the
contribution of this work is summarized as follows:1. present a pioneering work
for segmentation of kidney tumors on ncct images. 2. propose a novel framework
that explicitly captures protuberances in a kidney to enable a better
segmentation of tumors including isodensity types on ncct images. this framework
can be extended to other organs (e.g. adrenal gland, liver, pancreas). 3. verify
that the proposed framework achieves a higher dice score compared to the
standard 3d u-net using a publicly available dataset.",7
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,2.0,Related Work,"the release of two public ct image datasets with kidney and tumor masks from the
2019/2021 kidney and kidney tumor segmentation challenge [8] (kits19, kits21)
attracted researchers to develop various methods for segmentation.looking at the
top 3 teams from each challenge [6,11,13,17,21], all teams utilized 3d u-net [3]
or v-net [16], which bears a similar architecture. the winner of kits19 [13]
added residual blocks [7] to 3d u-net and predicted kidney and tumor regions
directly. however, the paper notes that modifying the architecture resulted in
only slight improvement. the other 5 teams took a similar approach to nnu-net's
coarse-to-fine cascaded network [12], where it predicts from a low-resolution
image in the first stage and then predicts kidneys and tumors from a
high-resolution image in the second stage. thus, although other attempts were
made, using 3d u-net is the go-to method for predicting kidneys and tumors. in
our work, we also make use of 3d u-net, but using this network alone fails to
learn some isodensity tumors. to overcome this issue, we developed a framework
that specifically incorporates protuberances in kidneys, allowing for an
effective segmentation of tumors on ncct images.in terms of focusing on
protruded regions in kidneys, our work is close to [14,15]. [14] developed a
computer-aided diagnosis system to detect exophytic kidney tumors on ncct images
using belief propagation and manifold diffusion to search for protuberances. an
exophytic tumor is located on the outer surface of the kidney that creates a
protrusion. while this method demonstrated high sensitivity (95%), its false
positives per patient remained high (15 false positives per patient). in our
work, we will not only segment protruded tumors but also other tumors as well.
the first base network is responsible for predicting kidney and tumor region
masks. our architecture is based on 3d u-net, which has an encoder-decoder style
architecture, with few modifications. to reduce the required size of gpu memory,
we only use the encoder that has only 16 channels at the first resolution, but
instead we make the architecture deeper by having 1 strided convolution and 4
max-pooling layers. in the decoder, we replace the up-convolution layers with a
bilinear up-sampling layer and a convolution layer. in addition, by only having
a single convolution layer instead of two in the original architecture at each
resolution, we keep the decoder relatively small. throughout this paper, we
refer this architecture as our 3d u-net.the second protuberance detection
network is the same as the base network except it starts from 8 channels instead
of 16. we train this network using synthetic datasets. the details of the
dataset and training procedures are described in sect. 3.2.the last fusion
network combines the outputs from the base network and the protuberance
detection network and makes the final tumor prediction. in detail, we perform a
summation of the initial tumor mask and the protruded region mask, and then
concatenate the result with the input image. this is the input of the last
fusion network, which also has the same architecture as the base network with an
exception of having two input channels. this fusion network do not just combine
the outputs but also is responsible for removing false positives from the base
network and the protuberance detection network.our combined three network is
fully differentiable, however, to train efficiently, we train the model in 3
steps.",7
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.1,Step1: Training Base Network,"in the first step, we train the base network, which is a standard segmentation
network, to extract kidney and tumor masks from the images. we use a sigmoid
function for the last layer. and as a loss function, we use the dice loss [16]
and the cross-entropy loss equally.",7
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.2,Step2: Training Protuberance Detection Network,"in the second step, we train the protuberance detection network alone to
separate protruded regions from the normal kidney masks. here, we only use the
crossentropy loss and label smoothing with a smoothing factor of = 0.01.
synthetic dataset. to enable a segmentation of protruded regions only, a
separate annotation of each region is usually required. however, annotating such
areas is time-consuming and preparing a large number of data is challenging.
alternatively, we create a synthetic dataset that mimics a kidney with
protrusions. the synthetic dataset is created through the following steps:1.
randomly sample a kidney mask without protuberance and a tumor mask. 2. apply
random rotation and scaling to the tumor mask. 3. randomly insert the tumor mask
into the kidney mask. 4. if both of the following conditions are met, append to
the dataset.where k i is a voxel value (0 or 1) in the kidney mask and t i is a
voxel value in the tumor mask. equation 1 ensures that only up to 30% of the
kidney is covered with a tumor. equation 2ensures that not all tumors are
covered by the kidney (at least 5% of the tumor is protruded from the kidney).",7
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,3.3,Step3: End-to-End Training with Fusion Network,"in the final step, we train the complete network jointly. although our network
is fully differentiable, since there is no separate annotation for protruded
regions other from the synthetic dataset, we freeze the parameters in
protuberance detection network.the output of the protuberance detection network
will likely have more false positives than the base network since it has no
access to the input image. thus, when the output of the protuberance detection
network is concatenated with the output of the base network, the fusion network
can easily reduce the loss by ignoring the protuberance detection network's
output, which is suboptimal. to avoid this issue, we perform summation not
concatenation to avoid the model from ignoring all output from the protuberance
detection network. we then clip the value of the mask to the range of 0 and 1.
as a result, the input to the fusion network has two channels. the first channel
is the input image, and the second channel is the result of summation of the
initial tumor mask and the protruded region mask. we concatenate the input image
so that the last network can remove false positives from the predicted masks as
well as predicting the missing tumor regions from the protuberance detection
network.we use the dice loss and the cross-entropy loss as loss functions for
the fusion network. we also keep the loss functions in the base network for
predicting kidneys and tumors. the loss function for tumors in the base network
acts like an intermediate supervision. our network shares some similarities with
the stacked hourglass network [18] where the network consists of multiple u-net
like hourglass modules and has intermediate supervision at the end of each
hourglass module. by having multiple modules in this manner, the network can fix
the initial mistakes in early modules and corrects in later modules.",7
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,4.1,Datasets and Preprocessing,"we used a dataset from kits19 [8] which contains both cect and ncct images. for
cect images, there are 210 images for training and validation and, 90 images for
testing. for ncct images, there are 108 images, which are different series of
the 210 images. the ground truth masks are only available for the 210 cect
images. thus, we transfer the masks to ncct images. this is achieved by
extracting kidney masks and adjusting the height of each kidney. the ground
truth mask contains a kidney label and a kidney tumor label. cysts are not
annotated separately and included in the kidney label on this dataset. the data
can be downloaded from the cancer imaging archive (tcia) [4,9].the images were
first clipped to the intensity value range of [-90, 210] and normalized from -1
to 1. the voxel spacings were normalized to 1 mm. during the training, the
images were randomly cropped to a patch size of 128×128×128 voxels. we applied
random rotation, random scaling and random noise addition as data
augmentation.during the step2 phase of the training, where we used the synthetic
dataset, we created 10,000 masks using the method from sect. 3.2. we applied
some augmentations during training to input masks to simulate the incoming
inputs from the base network. the output of the base network is not binarized to
keep gradient from flowing, so the values are in the range [0, 1] and the edge
of kidneys are usually smooth. therefore, we applied gaussian blurring, gaussian
noise addition and intensity value shifting.",7
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,5.2,Performance on NCCT Images,"table 2 shows our experimental results and ablation studies on ncct images. the
proposed method (table 2-bottom) outperformed the baseline model (table 2-top).
the ablation studies show that adding each component (cect images and the
protuberance detection network) resulted in an increase in the performance.
while adding cect images contributed the most for the increase in tumor dice and
sensitivity, adding the protuberance detection network further pushed the
performance. however, the false positives per image (fps/image) increased from
0.283 to 0.421. the protuberance detection network cannot distinguish the
protrusions that were caused by tumors or cysts, so the output from this network
has many fps at this stage. thus, the fusion network has to eliminate cysts by
looking again the input image, however, it may have failed to eliminate some
cysts (fig. 3 second row).",7
Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,6.0,Conclusion,"in this paper, we proposed a novel framework for kidney tumor segmentation on
ncct images. to cope with isodensity tumors, which have similar intensity values
to their surrounding tissues, we created a synthetic dataset to train a network
that extracts protuberance from the kidney masks. we combined this network with
the base network and fusion network. we evaluated our method using the publicly
available kits19 dataset, and showed that the proposed method can achieve a
higher sensitivity than existing approach. our framework is not limited to
kidney tumors but can also be extended to other organs (e.g., adrenal gland,
liver, pancreas).",7
Skin Lesion Correspondence Localization in Total Body Photography,4.0,Conclusions and Limitations,"the evolution of a skin lesion is an important sign of a potentially cancerous
growth and total body photography is useful to keep track of skin lesions
longitudinally. we proposed a novel framework that leverages geometric and
texture information to effectively find lesion correspondence across tbp scans.
the framework is evaluated on a private dataset and a public dataset with
success rates that are comparable to those of the state-of-the-art method.the
proposed method assumes that the local texture enclosing the lesion and its
surroundings should be similar from scan to scan. this may not hold when the
appearance of the lesion changes dramatically (e.g. if the person acquires a
tattoo). also, the resolution of the mesh affects the precision of the positions
of landmarks and lesions. in addition, the method may not work well with
longitudinal data that has non-isometric deformation due to huge variations in
body shape, inconsistent 3d reconstruction, or a dramatic change in pose and,
therefore, topology, such as an open armpit versus a closed one.in the future,
the method needs to be evaluated on longitudinal data with longer duration and
new lesions absent in the target. in addition, an automatic method to determine
accurate landmarks is desirable. note that although we rely on the manual
selection of landmarks, the framework is still preferable over manually
annotating lesion correspondences when a subject has hundreds of lesions. as the
3d capture of the full body becomes more prevalent with better quality in tbp,
we expect that the proposed method will serve as a valuable step for the
longitudinal tracking of skin lesions.",7
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,1.0,Introduction,"data augmentation (da) is a key factor in the success of deep neural networks
(dnn) as it artificially enlarges the training set to increase their
generalization ability as well as robustness [22]. it plays a crucial role in
medical image analysis [8] where annotated datasets are only available with
limited size. dnns have already successfully supported radiologists in the
interpretation of magnetic resonance images (mri) for prostate cancer (pca)
diagnosis [3]. however, the da scheme received less attention, despite its
potential to leverage the data characteristic and address overfitting as the
root of generalization problems.state-of-the-art approaches still rely on
simplistic spatial transformations, like translation, rotation, cropping, and
scaling by globally augmenting the mri sequences [12,20]. they exclude random
elastic deformations, which can change the lesion outline but might alter the
underlying label and thus produce counterproductive examples for training [22].
however, soft tissue deformations, which are currently missing from the da
schemes, are known to significantly affect the image morphology and therefore
play a critical role in accurate diagnosis [6].both lesion and prostate shape
geometrical appearance influence the clinical assessment of prostate
imaging-reporting and data system (pi-rads) [24]. the prostate constantly
undergoes soft tissue deformation dependent on muscle contractions, respiration,
and more importantly variable filling of the adjacent organs, namely the bladder
and the rectum. among these sources, the rectum has the largest influence on the
prostate and lesion shape variability due to its large motion [4] and the fact
that the majority of the lesions are located in the adjacent peripheral prostate
zone [1]. however, only one snapshot of all these functional states is captured
within each mri examination, and almost never will be exactly the same on any
repeat or subsequent examination. ignoring these deformations in the da scheme
can potentially limit model performance.model-driven transformations attempting
to simulate organ functions -like respiration, urinary excretion,
cardiovascular-and digestion mechanics -offer a high degree of diversity while
also providing realistic transformations. currently, the finite element method
(fem) is the standard for modeling biomechanics [13]. however, their computation
is overly complex [10] and therefore does not scale to on-the-fly da [7]. recent
motion models rely on dnns using either a fem model [15] or complex training
with population-based models [18]. motion models have not been integrated into
any deep learning framework as an online data augmentation yet, thereby leaving
the high potential of inducing applicationspecific knowledge into the training
procedure unexploited.in this work we propose an anatomy-informed spatial
augmentation, which leverages information from adjacent organs to mimic typical
deformations of the prostate. due to its lightweight computational requirements,
it can be easily integrated into common da frameworks. this technique allows us
to simulate different physiological states during the training and enrich our
dataset with a wider range of organ and lesion shapes. inducing this kind of
soft tissue deformation ultimately led to improved model performance in
patient-and lesion-level pca detection on an independent test set.",7
Anatomy-Informed Data Augmentation for Enhanced Prostate Cancer Detection,2.3,Prostate MRI Data,"774 consecutive bi-parametric prostate mri examinations are included in this
study, which were acquired in-house during the clinical routine. the ethics
committee of the medical faculty heidelberg approved the study (s-164/2019) and
waived informed consent to enable analysis of a consecutive cohort. all
experiments were performed in accordance with the declaration of helsinki [2]
and relevant data privacy regulations. for every exam, pi-rads v2 [24]
interpretation was performed by a board-certified radiologist. every patient
underwent extended systematic and targeted mri trans-rectal ultrasound-fusion
transperineal biopsy. malignancy of the segmented lesions was determined from a
systematic-enhanced lesion ground-truth histopathological assessment, which has
demonstrated reliable ground-truth assessment with sensitivity comparable to
radical prostatectomy [17]. the samples were evaluated according to the
international society of urological pathology (isup) standards under the
supervision of a dedicated uropathologist. clinically significant prostate
cancer (cspca) was defined as isup grade 2 or higher. based on the biopsy
results, every cspca lesion was segmented on the t2-weighted sequences
retrospectively by multiple in-house investigators under the supervision of a
board-certified radiologist. in addition to the lesions, the rectum and the
bladder segmentations were automatically predicted by a model built upon nnu-net
[8] trained iteratively on an in-house cohort initially containing a small
portion of our cohort. multiple radiologists confirmed the quality of the
predicted segmentations.",7
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,1.0,Introduction,"achieving complete tumor resection in surgical oncology like breast conserving
surgery (bcs) is challenging as boundaries of tumors are not always visible/
palpable [10]. in bcs the surgeon removes breast cancer while attempting to
preserve as much healthy tissue as possible to prevent permanent deformation and
to enhance cosmesis. the current standard of care for evaluating surgical
success is to investigate the resection margins, which refers to the area
surrounding the excised tumor. up to 30% of surgeries result in incomplete tumor
resection and require a revision operation [10]. the intelligent knife (iknife)
is a mass spectrometry device that can address this challenge by analyzing the
biochemical signatures of resected tissue using the smoke that is released
during tissue incineration [3]. each spectrum contains the distribution of
sampled ions with respect to their mass to charge ratio (m/z). previously,
learning models have been used in combination with iknife data for ex-vivo
tissue characterization and real-time margin detection [16,17].the success of
clinical deployment of learning models heavily relies on approaches that are not
only accurate but also interpretable. therefore, it should be clear how models
reach their decisions and the confidence they have in such decision. studies
suggest that one way to improve these factors is through data centric approaches
i.e. to focus on appropriate representation of data. specifically,
representation of data as graphs has been shown to be effective for medical
diagnosis and analysis [1]. it has also been shown that graph neural networks
can accurately capture the biochemical signatures of iknife and determine the
tissue type. particularly, graph transformer networks (gtn) has have shown to
further enhance the transparency of underlying relation between the graph nodes
and decision making via attention mechanism [11].biological data, specially
those acquired intra-opertively, are heterogeneous by nature. while the use of
ex-vivo data collected under specific protocols are beneficial to develop
baseline models, intra-operative deployment of these models is challenging. for
iknife, the ex-vivo data is usually collected from homogeneous regions of
resected specimens under the guidance of a trained pathologist, versus the
intra-operative data is recorded continuously while the surgeon cutting through
tissues with different heterogeneity and pathology. therefore, beyond predictive
power and explainable decision making, intra-operative models must be able to
handle mixed and unseen pathology labels.uncertainty-aware models in
computer-assisted interventions can provide clinicians with feedback on
prediction confidence to increase their reliability during deployment. deep
ensembles [15] and bayesian networks [9] incur high runtime and computational
cost both at training and inference time and thus, less practical for real-time
computer-assisted interventions. evidential deep learning [18] is another
approach that has been proposed based on the evidence framework of
dempster-shafer theory [12]. since the evidential approach jointly generates the
network prediction and uncertainty estimation, it seems more suitable for
computationally efficient intra-operative deployment. in this paper, we propose
evidential graph transformer (egt), a combination of graph-based feature-level
attention mechanism with sample-level uncertainty estimation, to increase the
performance and interpretability of surgical margin assessment. this is done by
implementing the evidential loss and prediction functions within a graph
transformer model to output the uncertainty, intermediate attention, and model
prediction. to demonstrate the state-of-theart performance of the proposed
approach on mass spectrometry data, the model is compared with different
baselines in both cross-validation and prospective schemes on ex-vivo data.
furthermore, the performance of model is also investigated intraoperatively. in
addition to the proposed model, we present a new visualization approach to
better correlate the graph nodes with the spectral content of the data, which
improves interpretability. in addition to the ablation study on the network and
graph strictures, we also investigate the metabolic association of breast cancer
hormone receptor status.",7
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.0,Materials and Methods,"figure 1 presents the overview of the proposed approach. following data
collection and curation, each burn (spectrum) is converted to a single graph
structure. the proposed graph model learns from the biochemical signatures of
the tissue to classify cancer versus normal tissue. the uncertainty and
intermediate attentions generated by the model are visualized and explored for
their association with the biochemical mechanisms of cancer.",7
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.1,Data Curation,"ex-vivo: data is collected from fresh breast tissue samples from the patients
referred to bcs at kingston health sciences center over two years. the study is
approved by the institutional research ethics board and patients consent to be
included. peri-operatively, a pathologist guides and annotates the ex-vivo
pointburns, referred to as spectra, from normal or cancerous breast tissue
immediately after excision. in addition to spectral data, clinicopathological
details such as the status of hormone receptors is also provided
post-surgically. in total 51 cancer and 149 normal spectra are collected and
stratified into five folds (4 for cross validation and 1 prospectively) with
each patient restricted to one fold only.",7
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,0.0,Evidential Graph Transformer:,"evidential deep learning provides a welldefined theoretical framework to jointly
quantify classification prediction and uncertainty modeling by assuming the
class probability follows a dirichlet distribution [18]. we propose to modify
the loss and prediction layer of gtn, considering the same assumption, to
formulate the evidential graph transformer model. therefore, there are two
mechanisms embedded in egt: i) node-level attention calculation -via aggregation
of neighboring nodes according to their relevance to the predictions, and ii)
graph-level uncertainty estimation -via fitting the dirichlet distribution to
the predictions.in the context of surgical margin assessment, the attentions
reveal the relevant metabolic ranges to cancerous tissue, while uncertainty
helps identify and filter data with unseen pathology. specifically, the
attentions affect the predictions by selectively emphasizing the contributions
of relevant nodes, enabling the model to make more accurate predictions. on the
other hand, the spread of the outcome probabilities as modeled by the dirichlet
distribution represents the confidence in the final predictions. combining the
two provides interpretable predictions along with the uncertainty
estimation.mathematically, the dirichlet distribution is characterized by α = [α
1 , ..., α c ] where c is the number of classes in the classification task. the
parameters can be estimates as α = f (x i |θ) + 1 where f (x i |θ) is the output
of the evidential graph transformer parameterized by θ for each sample(x i ).
then, the expected probability for the c-th class p c and the total uncertainty
u for each sample (x i ) can be calculated as p c = αc s , and u = c s ,
respectively, where s = c c=1 α c . to fit the dirichlet distribution to the
output layer of our network, we use a loss function consisting of the prediction
error l p i and the evidence adjustmentwhere λ is the annealing coefficient to
balance the two terms. l p i can be crossentropy, negative log-likelihood, or
mean square error , while l e i (θ) is kl divergence to the uniform dirichlet
distribution [18].",7
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,0.0,Ex-vivo Evaluation:,"the performance of the proposed network is compared with 3 baseline models
including gtn, graph convolution network [14], and non-graph convolution
network. four-fold cross validation is used for comparison of the different
approaches, to increase the generalizability (3 folds for train/validation, test
on remaining unseen fold, report average test performance). separate ablation
studies are performed for the baseline models to fine tune their structural
parameters. all experiments are implemented using pytorch with adam optimizer,
learning rate of 10 -4 , batch size of 32, and early stopping based on
validation loss. to demonstrate the robustness of the model and ensure it is not
overfitting, we also report the performance of the ensemble model from the
4-fold cross validation study on the 5th unseen prospective test fold.clinical
relevance: hormone receptor status plays an important role in determining breast
cancer prognosis and tailoring treatment plans for patients [6]. here, we
explore the correlation of the attention maps generated by egt with the status
of her2 and pr hormones associated with each spectrum. these hormones are
involved in different types of signaling that the cell depends on [5].",7
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,3.0,Results and Discussion,"ablation study and ex-vivo evaluation: according to our ablation study, hyper
parameters of 11 attention heads, 11 hidden features per attention head, the
cross entropy loss function, and annealing coefficient of 30, result in higher
performances when compared to other configurations (370k learnable parameters).
the performance of egt in comparison with the mentioned baselines are summarized
in table 1. as can be seen, the proposed egt model with average accuracy of
94.1% outperformed all the baselines statistically significantly (maximum
p-values of 0.02 in one-tail paired wilcoxon signed-rank test). the lower
standard deviation of parameters shows the robustness of egt compared to other
baselines. the regularization term in egt loss prevents overconfident estimation
of incorrect predictions [18] that could lead to superior results, compared to
gtn, without overfitting. lastly, when compared to other state-ofthe-art
baselines with uncertainty estimation mechanisms, the proposed evidential graph
transformer network (average balanced accuracy of 91.6 ± 4.3% in table 1)
outperforms mc dropout [9], deep ensembles [15], and masksembles [7] (86.1 ±
5.7%, 88.5 ± 6.8%, and 89.2 ± 5.4% respectively [19]).the estimated
probabilities in evidence based models are directly correlated with model
confidence and therefore more interpretable. to demonstrate this, table 1.
average(standard deviation) of accuracy (acc), balanced accuracy (bac)
sensitivity (sen), specificity (spc), and the area under the curve (auc) for the
proposed evidential graph transformer in comparison with graph transformer
(gtn), graph convolution (gcn), and non-graph convolution (cnn) baselines. the
probability of cancer predictions and uncertainty scores for all test samples
are visualized in the left plot of fig. 3. as seen, the higher the uncertainty
score (bottom bar plot), the closer the estimated cancer probability is to 0.5
(top bar plot). this information can be provided during deployment to further
augment surgical decision making for uncertain data instances. this is
demonstrated in the right plot of fig. 3, where the samples with high
uncertainties are gradually disregarded. it can be seen that by not using the
network prediction for up to 10% of most uncertain test data, the auc increases
to 1. providing surgeons with not only the model decision but also a measure of
model confidence will improve their intervention decisions. for example, if the
model has low confidence in a prediction they can reinforce their decision by
other means. the result of our graph structure ablation shows the drop of
average acc to 85.6% by randomizing the edges in the graph (p-value 0.004).
dropping overlapping nodes further decreased the acc to 82.3% (p-value 0.001).
although the model still trained due to node aggregation, random graph structure
acts as noise and affects the performance. multi-level graphs were shown to
outperform other structures for masspect data [akbarifar 2021] as they preserve
the receptive field in the neighborhood of subbands (metabolites). clinical
relevance: an appropriate visualization of the attention map for samples can be
used to help with this exploration. accumulating the attentions maps from the
cancerous burns based on their hormone receptor status results in the
representative maps demonstrated in fig. 4. the polar bars in this figure show
the attention level paid to the nodes in the associated m/z subband. it can be
seen that more attention is paid to the amino acids range (100-350 m/z) in her2
positive breast cancer in comparison to her2 negative breast cancer, which is in
accordance with previous literature that has found evidence for higher glutamine
metabolism activity in her2+ [13]. we have also found that there's more
attention in this range for pr negative breast cancer in comparison pr positive,
which is in concordance with previous literature demonstrating that these
subtypes have higher glutamine metabolic activity [4,5].",7
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,0.0,Intra-operative Deployment:,"the raw intra-operative iknife data (y-axis is m/z spectral range and x-axis is
the surgery timeline) along with the temporal reference labels extracted from
surgeon's call-outs and pathology report are shown in fig. 5, top. as seen, the
iknife stream contains spectra from skin cuts, which is considered as an unseen
label for the ex-vivo models. the results of deploying the proposed models and
baselines are presented in fig. 5, bottom. when a spectrum is classified as
cancer, a red line is overlaid on the timeline. previous studies showed the
similarity between skin and breast cancer mass spectrum that can confuse the
binary models. since our proposed egt is equipped with uncertainty estimation,
this information can be used to eliminate skin spectra from being wrongly
detected as cancer. by integrating uncertainty, predictions for such burns are
flagged as uncertain so clinicians can compensate for surgical decision making
with other sources of information.",7
Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,4.0,Conclusion,"intra-operative deployment of deep learning solutions requires a measure of
interpretability as well as predictive confidence. these two factors are
particularly importance to deal with heterogeneity of tissues which represented
as mixed or unseen labels for the retrospective models. in this paper, we
propose an evidential graph transformer for margin detection in breast cancer
surgery using mass spectrometry with these benefits in mind. this structure
combines the attention mechanisms of graph transformer with predictive
uncertainty. we demonstrate the significance of this model in different
experiments. it has been shown that the proposed architecture can provide
additional insight and consequently clearer interpretation of surgical margin
characterization and clinical features like status of hormone receptors. in the
future, we plan to work on other uncertainty estimation approaches and further
investigate the graph conversion technique to be more targeted on the metabolic
pathways, rather than regular conversion.",7
Radiomics-Informed Deep Learning for Classification of Atrial Fibrillation Sub-Types from Left-Atrium CT Volumes,2.1,Feature Fusion of Locally Computed Radiomic Features with Low-Level DNN Features for Improved Local Context,"under the radiomics pipeline, a large set of features, typically more than a
thousand, is first extracted by performing calculations over the volume and roi
input x i . feature selection methodologies such as mutual information (mi),
principal component analysis (pca), or lasso regularization, are then used to
identify predictive features for classification [23]. radiomic features are
classified into shape, first-order statistical features, and texture features.
texture features are designed to capture local variations and use measures such
as gray-level co-occurrence matrices (glcm) to reflect second-order textural
distributions. conventional statistics such as entropy and correlation are then
used to summarize these measures [25], but these tend to be limited in their
ability to capture local heterogeneity, such as the varying textures on the
surface of a cancer tumor. although dnn's are more effective at capturing local
variations, they can overfit without sufficient data for training [17]. unlike
existing works that naïvely concatenate radiomic and deep features before the
classification layer [2,19], we observe that textural features selected through
radiomics feature selection algorithms are known to be predictive and can be
used as prior knowledge to improve low-level dnn features. given radiomic
feature extractor f r , the global radiomic feature, r g i ∈ r, for input x i is
represented by:our method applies feature calculations locally to cubic patches
centered around each voxel, such that features are obtained on a voxel basis and
reflect the statistics of the neighbouring region. for a cubic patch with radius
p and input x i , the local feature at location (h, w, d), denoted by r p
i,(h,w,d) , is obtained by performing r on the cubic patch in x i centered
around (h, w, d):where the input of f r is the cubic sub-volume. this process is
illustrated in fig. 2a. local features can be calculated for multiple texture
features and patch size p, which are then concatenated to obtain r l i ∈ r l×h×w
×d , where l is the total number of features used and h, w , and d are original
input dimensions. we note that only texture radiomic features are used for local
calculation since they are specifically intended to capture local context. r l i
is then concatenated with low-level dnn features, z i ∈ r c×h×w ×d , to
supplement the dnn with local radiomic features. to effectively fuse the
features, we apply a channel attention module, a, following the design in
[20]:where z i is the fused feature, ⊕ is channel concatenation, and ⊗ is
element-wise multiplication. the learned attention tensor a(r l i ⊕ z i ) has
dimensions (c + l) × 1 × 1 × 1 and is broadcasted along the volume dimension,
such that attention is applied channel-wise and spatial feature distributions
are preserved.",7
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,1.0,Introduction,"breast cancer is the most common cancer and the leading cause of cancer death in
women [18]. early detection of breast cancer allows patients to receive timely
treatment, which may have less burden and a higher probability of survival [6].
among current clinical imaging modalities, magnetic resonance imaging (mri) has
the highest sensitivity for breast cancer detection [12]. especially,
contrastenhanced mri (ce-mri) can identify tumors well and has become an
indispensable technique for detecting and defining cancer [13]. however, the use
of gadolinium-based contrast agents (gbca) requires iv-cannulation, which is a
burden to patients, time consuming and cumbersome in a screening situation.
moreover, contrast administration can lead to allergic reactions and finaly
ce-mri may be associated with nephrogenic systemic fibrosis and lead to
bioaccumulation in the brain, posing a potential risk to human health
[4,9,[14][15][16]. in 2017, the european medicines agency concluded its review
of gbca, confirming recommendations to restrict the use of certain linear gbca
used in mri body scans and to suspend the authorization of other contrast
agents, albeit macrocyclic agents can still be freely used [10].with the
development of computer technology, artificial intelligence-based methods have
shown potential in image generation and have received extensive attention. some
studies have shown that some generative models can effectively perform mutual
synthesis between mr, ct, and pet [19]. among them, synthesis of ce-mri is very
important as mentioned above, but few studies have been done by researchers in
this area due to its challenging nature. li et al. analyzed and studied the
feasibility of using t1-weighted mri and t2-weighted mri to synthesize ce-mri
based on deep learning model [11]. their results showed that the model they
developed could potentially synthesize ce-mri and outperform other cohort
models. however, mri source data of too few sequences (only t1 and t2) may not
provide enough valuable informative to effectively synthesize ce-mri. in another
study, chung et al. investigated the feasibility of using deep learning (a
simple u-net structure) to simulate contrast-enhanced breast mri of invasive
breast cancer, using source data including t1-weighted non-fatsuppressed mri,
t1-weighted fat-suppressed mri, t2-weighted fat-suppressed mri, dwi, and
apparent diffusion coefficient [5]. however, obtaining a complete mri sequence
makes the examination costly and time-consuming. on the other hand, the
information provided by multi-sequences may be redundant and may not contain the
relevant information of ce-mri. therefore, it is necessary to focus on the most
promising sequences to synthesize ce-mri.diffusion-weighted imaging (dwi) is
emerging as a key imaging technique to complement breast ce-mri [3]. dwi can
provide information on cell density and tissue microstructure based on the
diffusion of tissue water. studies have shown that dwi could be used to detect
lesions, distinguish malignant from benign breast lesions, predict patient
prognosis, etc [1,3,7,8,17]. in particular, dwi can capture the dynamic
diffusion state of water molecules to estimate the vascular distribution in
tissues, which is closely related to the contrast-enhanced regions in ce-mri.
dwi may be a valuable alternative in breast cancer detection in patients with
contraindications to gbca [3]. inspired by this, we develop a multi-sequence
fusion network based on t1-weighted mri and multi-b-value dwi to synthesize
ce-mri. our contributions are as follows:i from the perspective of method, we
innovatively proposed a multi-sequence fusion model, designed for combining
t1-weighted imaging and multi-b-value dwi to synthesize ce-mri for the first
time. ii we invented hierarchical fusion module, weighted difference module and
multi-sequence attention module to enhance the fusion at different scale, to
control the contribution of different sequence and maximising the usage of the
information within and across sequences. iii from the perspective of clinical
application, our proposed model can be used to synthesize ce-mri, which is
expected to reduce the use of gbca.",7
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.1,Patient Collection and Pre-processing,"this study was approved by institutional review board of our cancer institute
with a waiver of informed consent. we retrospectively collected 765 patients
with breast cancer presenting at our cancer institute from january 2015 to
november 2020, all patients had biopsy-proven breast cancers (all cancers
included in this study were invasive breast cancers, and ductal carcinoma in
situ had been excluded). the mris were acquired with philips ingenia all mris
were resampled to 1 mm isotropic voxels and uniformly sized, resulting in
volumes of 352 × 352 pixel images with 176 slices per mri, and subsequent
registration was performed based on advanced normalization tools (ants) [2].",7
Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,4.0,Conclusion,"we have developed a multi-sequence fusion network based on multi-b-value dwi to
synthesize ce-mri, using source data including dwis and t1-weighted
fatsuppressed mri. compared to existing methods, we avoid the challenges of
using full-sequence mri and aim to be selective on valuable source data dwi.
hierarchical fusion generation module, weighted difference module, and
multisequence attention module have all been shown to improve the performance of
synthesizing target images by addressing the problems of synthesis at different
scales, leveraging differentiable information within and across sequences. given
that current research on synthetic ce-mri is relatively sparse and challenging,
our study provides a novel approach that may be instructive for future research
based on dwis. our further work will be to conduct reader studies to verify the
clinical value of our research in downstream applications, such as helping
radiologists on detecting tumors. in addition, synthesizing dynamic
contrastenhanced mri at multiple time points will also be our future research
direction. our proposed model can potentially be used to synthesize ce-mri,
which is expected to reduce or avoid the use of gbca, thereby optimizing
logistics and minimizing potential risks to patients.",7
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,1.0,Introduction,"nasopharyngeal carcinoma (npc), also known as lymphoepithelioma, is a highly
aggressive malignancy that originates in nasopharynx [1]. npc is characterized
by a distinct geographical distribution in southeast asia, north africa, and
arctic [2]. in china, npc accounts for up to 50% of all head and neck cancers,
while in southeast asia, npc accounts for more than 70% of all head and neck
cancers [3]. radiotherapy (rt) is currently the main treatment remedy, which
needs precise tumor delineation to ensure a satisfactory rt outcome. however,
accurately delineating the npc tumor is challenging due to the highly
infiltrative nature of npc and its complex location, which is surrounded by
critical organs such as brainstem, spinal cord, temporal lobes, etc. to improve
the visibility of npc tumor for precise gross-tumor-volume (gtv) delineation,
contrastenhanced mri (ce-mri) is administrated through injection of
gadolinium-based contrast agents (gbcas) during mri scanning. despite the
superior tumor-to-normal tissue contrast of ce-mri, the use of gbcas during mri
scanning can result in a fatal systemic disease known as nephrogenic systemic
fibrosis (nsf) in patients with renal insufficiency [4]. nsf can cause severe
physical impairment, such as joint contractures of fingers, elbows, and knees,
and can progress to involve critical organs such as the heart, diaphragm,
pleura, pericardium, kidney, liver, and lung [5]. it was reported that the
incidence rate of nsf is around 4% after gbca administration in patients with
severe renal insufficiency, and the mortality rate can reach 31% [6]. currently,
there is no effective treatment for nsf, making it crucial to find a ce-mri
alternative for patients at risk of nsf.in recent years, artificial intelligence
(ai), especially deep learning, plays a gamechanging role in medical imaging
[7,8], which showed great potential to eliminate the use of the toxic gbcas
through synthesizing virtual contrast-enhanced mri (vce-mri) from
gadolinium-free sequences, such as t1-weighted (t1w) and t2-weighted (t2w) mri
[9][10][11][12]. in 2018, gong et al. [11] utilized pre-contrast and 10%
low-dose t1w mri to synthesize the vce-mri for brain disease diagnosis using a
u-shape model, they found that gadolinium dose is able to be reduced by 10-fold
by deep learning while the contrast information could be preserved. followed by
their work, kleesiek et al. [10] proposed a bayesian model to explore the
feasibility of synthesizing vce-mri from contrast-free sequences, their study
demonstrated that deep learning is highly feasible to totally eliminate the use
of gbcas. in the area of rt, li et al. [9] developed a multi-input model to
synthesize vce-mri for npc rt. in addition to the advantage of eliminating the
use of gbca, vce-mri synthesis can also speed up the clinical workflow by
eliminating the need for acquiring ce-mri scan, which saves time for both
clinical staff and patients. however, current studies mostly focus on algorithms
development while lack comprehensive clinical evaluations to demonstrate the
efficacy of the synthetic vce-mri in clinical settings.the clinical evaluation
of ai-based techniques is of paramount importance in healthcare. rigorous
clinical evaluations can establish the safety and efficacy of ai-based
techniques, identify potential biases and limitations, and facilitate the
integration of clinical expertise to ensure accurate and meaningful results
[13]. furthermore, the clinical evaluation of ai-based techniques can help
identify areas for improvement and optimization, leading to development of more
effective algorithms.to bridge this bench-to-bedside research gap, in this
study, we conducted a series of clinical evaluations to assess the effectiveness
of synthetic vce-mri in npc delineation, with a particular focus on assessment
in vce-mri image quality and primary gtv delineation. this study has two main
novelties: (i) to the best of our knowledge, this is the first clinical
evaluation study of the vce-mri technique in rt; and (ii) multiinstitutional mri
data were included in this study to obtain more reliable results. the success of
this study would fill the current knowledge gap and provide the medical
community with a clinical reference prior to clinical application of the novel
vce-mri technique in npc rt.",7
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,0.0,Image,"(ii) clarity of tumor-to-normal tissue interface. the clarity of tumor-normal
tissue interface is critical for tumor delineation, which directly affects the
delineation outcomes. oncologists were asked to use a 5-point likert scale
ranging from 1 (poor) to 5 (excellent) to evaluate the clarity of
tumor-to-normal tissue interface. paired two-tailed t-test (with a significance
level of p = 0.05) was applied to analyses if the scores obtained from real
patients and synthetic patients are significantly different. (iii) veracity of
contrast enhancement in tumor invasion risk areas. in addition to the critical
tumor-normal tissue interface, the areas surrounding the npc tumor will also be
considered during delineation. to better evaluate the veracity of contrast
enhancement in vce-mri, we selected 25 tumor invasion risk areas according to
[16], including 13 high-risk areas and 12 medium-risk areas, and asked
oncologists to determine whether these areas were at risk of being invaded
according to the contrast-enhanced tumor regions. the 13 high-risk areas
include: retropharyngeal space, parapharyngeal space, levator veli palatine
muscle, prestyloid compartment, tensor veli palatine muscle, poststyloid
compartment, nasal cavity, pterygoid process, basis of sphenoid bone, petrous
apex, prevertebral muscle, clivus, and foramen lacerum. the 12 medium-risk areas
include foramen ovale, great wing of sphenoid bone, medial pterygoid muscle,
oropharynx, cavernous sinus, sphenoidal sinus, pterygopalatine fossa, lateral
pterygoid muscle, hypoglossal canal, foramen rotundum, ethmoid sinus, and
jugular foramen. the areas considered at risk of tumor invasion were
recorded.the jaccard index (ji) [17] was utilized to quantitatively evaluate the
results of recorded risk areas from ce-mri and vce-mri. the ji could be
calculated by:where r ce and r vce represents the set of risk areas that
recorded from ce-mri and corresponding vce-mri, respectively. ji measures
similarity of two datasets, which ranges from 0% to 100%. higher ji indicates
more similar of the two sets.(iv) efficacy in primary tumor staging. a critical
rt-related application of ce-mri is tumor staging, which plays a critical role
in treatment planning and prognosis prediction [18]. to assess the efficacy of
vce-mri in npc tumor staging, oncologists were asked to determine the stage of
the primary tumor shown in ce-mri and vce-mri. the staging results from ce-mri
were taken as the ground truth and the staging accuracy of vce-mri was
calculated.primary gtv delineation. gtv delineation is the foremost prerequisite
for a successful rt treatment of npc tumor, which demands excellent precision
[19]. an accurate tumor delineation improves local control and reduce toxicity
to surrounding normal tissues, thus potentially improving patient survival [20].
to evaluate the feasibility of eliminating the use of gbca by replacing ce-mri
with vce-mri in tumor delineation, oncologists were asked to contour the primary
gtv under assistance of vce-mri. for comparison, ce-mri was also imported to
eclipse for tumor delineation but assigned as a different patient, which were
shown to oncologists in a random and blind manner.to mimic the real clinical
setting, contrast-free t1w, t2w mri and corresponding ct of each patient were
imported into the eclipse system since sometimes t1w and t2w mri will also be
referenced during tumor delineation. due to both real patients and synthetic
patients were involved in delineation, to erase the delineation memory of the
same patient, we separated the patients to two datasets, each with the same
number of patients, both two datasets with mixed real patients and synthetic
patients without overlaps (i.e., the ce-mri and vce-mri from the same patient
are not in the same dataset).when finished the first dataset delineation, there
was a one-month interval before the delineation of the second dataset. after the
delineation of all patients, the dice similarity coefficient (dsc) [21] and
hausdorff distance (hd) [22] of the gtvs delineated from real patients and
corresponding synthetic patients were calculated to evaluate the accuracy of
delineated contours.dice similarity coefficient (dsc). dsc is a broadly used
metric to compare the agreement between two segmentations [23]. it measures the
spatial overlap between two segmentations, which ranges from 0 (no spatial
overlap) to 1 (complete overlap). the dsc can be expressed as:where c ce and c
vce represent the contours delineated from real patients and synthetic patients,
respectively.",7
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,0.0,Hausdorff Distance (HD).,"even though dsc is a well-accepted segmentation comparison metric, it is easily
influenced by the size of contours. small contours typically receive lower dsc
than larger contours [24].therefore, hd was applied as a supplementary to make a
more thorough comparison. hd is a metric to measure the maximum distance between
two contours. given two contours c ce and c vce , the hd could be calculated
as:where d(x, c vce ) and d(y, c ce ) represent the distance from point x in
contour c ce to contour c vce and the distance from point y in contour c vce to
contour c ce . (i) distinguishability between ce-mri and vce-mri. the overall
judgement accuracy for the mri volumes was 53.33%, which is close to a random
guess accuracy (i.e., 50%). for institution-1, 2 real patients were judged as
synthetic and 1 synthetic patient was considered as real. for institution-2, 2
real patients were determined as synthetic and 4 synthetic patients were
determined as real. for institution-3, 2 real patients were judged as synthetic
and 3 synthetic patients were considered as real. in total, 6 real patients were
judged as synthetic and 8 synthetic patients were judged as real. (ii) clarity
of tumor-to-normal tissue interface. the overall clarity scores of
tumorto-normal tissue interface for real and synthetic patients were 3.67 with a
median of 4 and 3.47 with a median of 4, respectively. no significant difference
was observed between these two scores (p = 0.38). the average scores for real
and synthetic patients were 3.6 and 3, 3.6 and 3.8, 3.8 and 3.6 for
institution-1, institution-2, and institution-3, respectively. 5 real patients
got a higher score than synthetic patients and 3 synthetic patients obtained a
higher score than real patients. the scores of the other 7 patient pairs were
the same. (iii) veracity of contrast enhancement in tumor invasion risk areas.
the overall ji score between the recorded tumor invasion risk areas from ce-mri
and vce-mri was 74.06%. the average ji obtained from institution-1,
institution-2, and institution-3 dataset were similar with a result of 71.54%,
74.78% and 75.85%, respectively. in total, 126 risk areas were recorded from the
ce-mri for all of the evaluation patients, while 10 (7.94%) false positive high
risk invasion areas and 9 (7.14%) false negative high risk invasion areas were
recorded from vce-mri. (iv) efficacy in primary tumor staging. a t-staging
accuracy of 86.67% was obtained using vce-mri. 13 patient pairs obtained the
same staging results. for the institution-2 data, all synthetic patients
observed the same stages as real patients.",7
Clinical Evaluation of AI-Assisted Virtual Contrast Enhanced MRI in Primary Gross Tumor Volume Delineation for Radiotherapy of Nasopharyngeal Carcinoma,0.0,Table 2 .,"quality evaluation results of vce-mri: (a) distinguishability between ce-mri and
vce-mri; (b) clarity of tumor-to-normal tissue interface; (c) veracity of
contrast enhancement in risk areas; and (d) t-staging. abbreviations: inst:
institution; c.a.: center-based average; o.a.: overall average; syn: synthetic.",7
Automated CT Lung Cancer Screening Workflow Using 3D Camera,1.0,Introduction,"lung cancer is the leading cause of cancer death in the united states, and early
detection is key to improving survival rates. ct lung cancer screening is a
lowdose ct (ldct) scan of the chest that can detect lung cancer at an early
stage, when it is most treatable. however, the current workflow for performing
ct lung scans still requires an experienced technician to manually perform
pre-scanning steps, which greatly decreases the throughput of this high volume
procedure. while recent advances in human body modeling [4,5,12,13,15] have
allowed for automation of patient positioning, scout scans are still required as
they are used by automatic exposure control system in the ct scanners to compute
the dose to be delivered in order to maintain constant image quality [3].since
ldct scans are obtained in a single breath-hold and do not require any contrast
medium to be injected, the scout scan consumes a significant portion of the
scanning workflow time. it is further increased by the fact that tube rotation
has to be adjusted between the scout and actual ct scan. furthermore, any
patient movement during the time between the two scans may cause misalignment
and incorrect dose profile, which could ultimately result in a repeat of the
entire process. finally, while minimal, the radiation dose administered to the
patient is further increased by a scout scan.we introduce a novel method for
estimating patient scanning parameters from non-ionizing 3d camera images to
eliminate the need for scout scans during pre-scanning. for ldct lung cancer
screening, our framework automatically estimates the patient's lung position
(which serves as a reference point to start the scan), the patient's isocenter
(which is used to determine the table height for scanning), and an estimate of
patient's water equivalent diameter (wed) profiles along the craniocaudal
direction which is a well established method for defining size specific dose
estimate (ssde) in ct imaging [8,9,11,18]. additionally, we introduce a novel
approach for updating the estimated wed in real-time, which allows for
refinement of the scan parameters during acquisition, thus increasing accuracy.
we present a method for automatically aborting the scan if the predicted wed
deviates from real-time acquired data beyond the clinical limit. we trained our
models on a large collection of ct scans acquired from over 60, 000 patients
from over 15 sites across north america, europe and asia. the contributions of
this work can be summarized as follows:-a novel workflow for automated ct lung
cancer screening without the need for scout scan -a clinically relevant method
meeting iec 62985:2019 requirements on wed estimation. -a generative model of
patient wed trained on over 60, 000 patients.-a novel method for real-time
refinement of wed, which can be used for dose modulation",7
Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.0,Method,"water equivalent diameter (wed) is a robust patient-size descriptor [17] used
for ct dose planning. it represents the diameter of a cylinder of water having
the same averaged absorbed dose as the material contained in an axial plane at a
given craniocaudal position z [2]. the wed of a patient is thus a function
taking as input a craniocaudal coordinate and outputting the wed of the patient
at that given position. as wed is defined in an axial plane, the diameter needs
to be known on both the anterior-posterior (ap) and lateral (left-right) axes
noted respectively w ed ap (z) and w ed l (z). as our focus here is on lung
cancer screening, we define 'wed profile' to be the 1d curve obtained by
uniformly sampling the wed function along the craniocaudal axis within the lung
region.our method jointly predicts the ap and lateral wed profiles. while wed
can be derived from ct images, paired ct scans and camera images are rarely
available, making direct regression through supervised learning challenging. we
propose a semi-supervised approach to estimate wed from depth images. first, we
train a wed generative model on a large collection of ct scans. we then train an
encoder network to map the patient depth image to the wed manifold. finally, we
propose a novel method to refine the prediction using real-time scan data.",7
Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.2,Patient Preparation,"patient positioning is the first step in lung cancer screening workflow. we
first need to estimate the table position and the starting point of the scan. we
propose to estimate the table position by regressing the patient isocenter and
the starting point of the scan by estimating the location of the patient's lung
top.starting position. we define the starting position of the scan as the
location of the patient's lung top. we trained a denseunet [7] taking the camera
depth image as input and outputting a gaussian heatmap centered at the patient's
lung top location. we used 4 dense blocks of 4 convolutional layers for the
encoder and 4 dense blocks of 4 convolutional layers for the decoder. each
convolutional layer (except the last one) is followed by a batch normalization
layer and a relu activation. we trained our model on 2, 742 patients using
adaloss [14] and the adam [6] optimizer with a learning rate of 0.001 and a
batch size of 32 for 400 epochs. our model achieves a mean error of 12.74 mm and
a 95 th percentile error of 28.32 mm. to ensure the lung is fully visible in the
ct image, we added a 2 cm offset on our prediction towards the outside of the
lung. we then defined the accuracy as whether the lung is fully visible in the
ct image when using the offset prediction. we report an accuracy of 100% on our
evaluation set of 110 patients. third and fourth columns show the performance of
our model with real-time refinement every 5 cm and 2 cm respectively. ground
truth is depicted in green and our prediction is depicted in red. while the
original prediction was off towards the center of the lung, the real-time
refinement was able to correct the error.isocenter. the patient isocenter is
defined as the centerline of the patient's body. we trained a densenet [1]
taking the camera depth image as input and outputting the patient isocenter. our
model is made of 4 dense blocks of 3 convolutional layers. each convolutional
layer (except the last one) is followed by a batch normalization layer and a
relu activation. we trained our model on 2, 742 patients using adadelta [16]
with a batch size of 64 for 300 epochs. on our evaluation set, our model
outperforms the technician's estimates with a mean error of 5.42 mm and a 95 th
percentile error of 8.56 mm compared to 6.75 mm and 27.17 mm respectively.
results can be seen in fig. 2.",7
Automated CT Lung Cancer Screening Workflow Using 3D Camera,4.0,Conclusion,"we presented a novel 3d camera based approach for automating ct lung cancer
screening workflow without the need for a scout scan. our approach effectively
estimates start of scan, isocenter and water equivalent diameter from depth
images and meets the iec acceptance criteria of relative wed error. while this
approach can be used for other thorax scan protocols, it may not be applicable
to trauma (e.g. with large lung resections) and inpatient settings, as the
deviation in predicted and actual wed would likely be much higher. in future, we
plan to establish the feasibility as well as the utility of this approach for
other scan protocols and body regions.1",7
Clustering Disease Trajectories in Contrastive Feature Space for Biomarker Proposal in Age-Related Macular Degeneration,2.0,Related Work,"current amd grading systems: ophthalmologists' current understanding of
progression from early to late amd largely involves drusen, which are subretinal
lipid deposits. drusen volume increases until suddenly stagnating and
regressing, which often precedes the onset of late amd [16]. the established amd
grading system stratifies early and intermediate stages solely by the size of
drusen in a single oct image [1,6,7,10]. late amd is classified into either
choroidal neovascularisation (cnv), identified by subretinal fluid, or
geographic atrophy, signalled by progressive loss of photoreceptors and retinal
thinning. the degree of atrophy can be staged using crora (complete retinal
pigment epithelium and outer retinal atrophy), which measures the width in μm of
focal atrophy in oct [13]. grading systems derived from these biomarkers offer
limited diagnostic value and little to no prognostic capability.tracking
evolution of known biomarkers: few research efforts have aimed at quantifying
and tracking known amd biomarkers, mostly drusen, over time [16,19]. more work
has explored the disease progression of alzheimer's disease (ad), which offers a
greater array of quantitative imaging biomarkers, such as levels of tau protein
and hippocampal volume. young et al. [25] fit an event-based model that
rediscovers the order in which these biomarkers become anomalous as ad
progresses. vogel et al. [21] find four distinct spatiotemporal trajectories for
tau pathology in the brain. however, this only works if biomarkers are known a
priori and requires manual annotation of entire time series.automated discovery
of unknown biomarkers: prior work for automated biomarker discovery in amd
explores the latent feature space of encoders trained for image reconstruction
[18,23], segmentation [27] or generative adversarial networks [17]. however,
these neural networks are prone to overfit to their specific task and lose
semantic information regarding the disease. contrastive methods [3,8,26] encode
invariance to a set of image transformations, which are uncorrelated with
disease features, resulting in a more expressive feature space. however, all
aforementioned methods group single images acquired at one point in time, and in
doing so neglect temporal dynamics. the one work that tackles this challenge,
and the most related to ours, categorises the time-dependent response of cancer
cells to different drugs, measured by the changing distance in contrastive
feature space from healthy controls [5].",7
Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma,1.0,Introduction,"diffuse glioma is the most common and aggressive primary brain tumors in adults,
accounting for more deaths than any other type [7]. pathology diagnosis is the
gold standard for diffuse glioma but is usually time-consuming and highly
depends on the expertise of senior pathologists [13]. hence, automatic
algorithms based on histology whole slide images (wsis) [15], namely digital
pathology, promise to offer rapid diagnosis and aid precise treatment.recently,
deep learning has achieved success in diagnosing various tumors [2,21]. most
methods are mainly predicting histology based on wsi, less concerning molecular
markers. however, the paradigm of pathological diagnosis of glioma has shifted
to molecular pathology, reflected by the 2021 who classification of tumors of
the central nervous system [14]. the role of key molecular markers, i.e.,
isocitrate dehydrogenas (idh) mutations, co-deletion of chromosome 1p/19q and
homozygous deletion (homdel) of cyclin-dependent kinase inhibitor 2a/b (cdkn),
have been highlighted as major diagnostic markers for glioma, while histology
features that are traditionally emphasized are now considered as reference,
although still relevant in many cases. for instance, in the new pathology
scheme, glioblastoma is increasingly diagnosed according to idh mutations, while
previously its diagnosis mostly relies on histology features, including necrosis
and microvascular proliferation (nmp). 1 however, the primary approaches to
assess molecular markers include gene sequencing and immuno-staining, which are
time-consuming and expensive than histology assessment. as histology features
are closely associated with molecular alterations, algorithm predicting
molecular markers based on histology wsis is feasible and have clinical
significance. moreover, under the new paradigm of integrating molecular markers
with histological features into tumor classification, it is helpful to model the
interaction of histology and molecular makers for a more accurate diagnosis.
therefore, there is an urgent need for developing novel digital pathology
methods based on wsi to predict molecular markers and histology jointly and
modeling their interactions for final tumor classification, which could be
valuable for the clinically relevant diagnosis of diffuse glioma.this paper
proposes a deep learning model (deepmo-glioma) for glioma classification based
on wsis, aiming to reflect the molecular pathology paradigm. previous methods
are proposed to integrate histology and genomics for tumour diagnosis [3,10,20].
for instance, chen et al. [3] proposed a multimodal fusion strategy to integrate
wsis and genomics for survival prediction. xing et al. [20] devised a
self-normalizing network to encode genomics. nevertheless, most existing
approaches of tumor classification only treat molecular markers as additional
input, incapable to simultaneously predict the status of molecular markers, thus
clinically less relevant under the current clinical diagnosis scheme. to jointly
predict histology and molecular markers following clinical diagnostic pathway,
we propose a novel hierarchical multi-task multi-instance learning (hmt-mil)
framework based on vision transformer [4], with two partially weight-sharing
parts to jointly predict molecular markers and histology.moreover, multiple
molecular markers are needed for classifying cancers, due to complex tumor
biology. to reflect real-world clinical scenarios, we formulate predicting
multiple molecular markers as a multi-label classification (mlc) task. previous
mlc methods have successfully modeled the correlation among labels [12,22]. for
example, yazici et al. [22] proposed an orderless recurrent method, while li et
al. designed a label attention transformer network with graph embedding. in
medical domain, zhang et al. [25] devised a dual-pool contrastive learning for
classifying fundus and x-ray images. despite success, when applied to predicting
multiple molecular markers, most existing methods may ignore the co-occurrence
of molecular markers, which have intrinsic associations [23]. hence, we propose
a co-occurrence probability-based, label-correlation graph (cplc-graph) network
to model the co-occurrence of molecular markers, i.e., intra-omic
relationship.lastly, we focus on modeling the interaction between molecular
markers and histology. specifically, we devise a novel inter-omic interaction
strategy to model the interaction between the predictions of molecular markers
and histology, e.g., idh mutation and nmp, both of which are relevant in
diagnosing glioblastoma. particularly, we design a dynamical confidence
constraint (dcc) loss that constrains the model to focus on similar areas of
wsis for both tasks. to the best of our knowledge, this is the first attempt to
classify diffuse gliomas via modeling the interaction of histology and molecular
markers.our main contributions are: (1) we propose a multi-task multi-instance
learning framework to jointly predict molecular markers and histology and
finally classify diffuse glioma, reflecting the new paradigm of pathology
diagnosis. (2) we design a cplc-graph network to model the intra-omic
relationship of multiple molecular markers. (3) we design a dcc learning
strategy to model the inter-omic interaction between histology and molecular
markers for glioma classification.",7
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,1.0,Introduction,"esophageal cancer is a significant contributor to cancer-related deaths globally
[3,15]. one effective treatment option is radiotherapy (rt), which utilizes
high-energy radiation to target cancerous cells [4]. to ensure optimal treatment
outcomes, both the cancerous region and the adjacent organ-at-risk (oar) must be
accurately delineated, to focus the high-energy radiation solely on the
cancerous area while protecting the oars from any harm. gross tumor volume (gtv)
represents the area of the tumor that can be identified with a high degree of
certainty and is of paramount importance in clinical practice.in the clinical
setting, patients may undergo a second round of rt treatment to achieve complete
tumor control when initial treatment fails to completely eradicate cancer [16].
however, the precise delineation of the gtv is laborintensive, and is restricted
to specialized hospitals with highly skilled rt experts. the automatic
identification of the esophagus presents inherent challenges due to its
elongated soft structure and ambiguous boundaries between it and adjacent organs
[12]. moreover, the automatic delineation of the gtv in the esophagus poses a
significant difficulty, primarily attributable to the low contrast between the
esophageal gtv and the neighboring tissue, as well as the limited
datasets.recently, advances in deep learning [21] have promoted research in
automatic esophageal gtv segmentation from computed tomography (ct) [18,19].
since the task is challenging, jin et al. [9,10] improve the segmentation
accuracy by incorporating additional information from paired positron emission
tomography (pet). nevertheless, such approaches require several imaging
modalities, which can be both costly and time-consuming, while disregarding any
knowledge from previous treatment or anatomical understanding. moreover, the
correlation between the first and second courses of rt is rarely investigated,
where detailed prior tumor information naturally exists in the previous rt
planning.in this paper, we present a comprehensive study on accurate gtv
delineation for the second course rt. we proposed a novel prior anatomy and rt
information enhanced second-course esophageal gtv segmentation network (artseg).
a region-preserving attention module (ram) is designed to effectively capture
the long-range prior knowledge in the esophageal structure, while preserving
regional tumor patterns. to the best of our knowledge, we are the first to
reveal the domain gap between the first and second courses for gtv segmentation,
and explicitly leverage prior information from the first course to improve gtv
segmentation performance in the second course.the medical images are labeled
sparsely, which are isolated by different tasks [20]. meanwhile, an ideal method
for automatic esophageal gtv segmentation in the second course of rt should
consider three key aspects: 1) changes in tumor volume after the first course of
rt, 2) the proliferation of cancerous cells from a tumor to neighboring healthy
cells, and 3) the anatomical-dependent our training approach leverages
multi-center datasets containing relevant annotations, that challenges the
network to retrieve information from e1 using the features from e2. the decoder
d utilizes the prior knowledge obtained from i1 and g1 to generate the mask
prediction. our training strategy leverages three datasets that introduce prior
knowledge to the network of the following three key aspects: 1) tumor volume
variation, 2) cancer cell proliferation, and 3) reliance of gtv on esophageal
anatomy.nature of gtv on esophageal locations. to achieve this, we efficiently
exploit knowledge from multi-center datasets that are not tailored for
second-course gtv segmentation. our training strategy does not specific to any
tasks but challenges the network to retrieve information from another encoder
with augmented inputs, which enables the network to learn from the above three
aspects. extensive quantitative and qualitative experiments validate our
designs.",7
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,2.0,Network Architecture,"in the first course of rt, a ct image denoted as i 1 is utilized to manually
delineate the esophageal gtv, g 1 . during the second course of rt, a ct image i
2 of the same patient is acquired. however, i 2 is not aligned with i 1 due to
soft tissue movement and changes in tumor volume that occurred during the first
course of treatment. both images i 1/2 have the spatial shape of h × w × d.our
objective is to predict the esophageal gtv g 2 of the second course. it would be
advantageous to leverage insights from the first course, as it comprises
comprehensive information pertaining to the tumor in its preceding phase.
therefore, the input to encoder e 1 consists of the concatenation of i 1 and g 1
to encode the prior information (features f d 1 ) from the first course, while
encoder e 2 embeds both low-and high-level features f d 2 of the local pattern
of i 2 (fig. 1),where the spatial shape of, with 2 d+4 channels.
region-preserving attention module. to effectively learn the prior knowledge in
the elongated esophagus, we design a region-preserving attention module (ram),
as shown in fig. 1. the multi-head attention (mha) [17] is employed to gather
long-range informative values in f d 1 with f d 2 as queries and f d 1 as keys.
the features f d 1/2 are reshaped to hw d 2 3d × c before passed to the mha,
where c is the channel dimension. the attentive features f d a can be formulated
as:since mha perturbs the positional information, we preserve the tumor local
patterns by concatenating original features to the attentive features at the
channel dimension, followed by a 1 × 1 × 1 bottleneck convolution ξ 1×1×1 to
squeeze the channel features (named as ram), as shown in the following
equations,where the lower-level features from both encoders are fused by
concatenation.the decoder d generates a probabilistic prediction) with skip
connections (fig. 1). we utilize the 3d dice [14] loss function,",7
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.0,Training Strategy,"the network should learn from three aspects: 1) tumor volume variation: the
structural changes of the tumor from the first to the second course; 2) cancer
cell proliferation: the tumor in esophageal cancer tends to infiltrate into the
adjacent tissue; 3) reliance of gtv on esophageal anatomy: the anatomical
dependency between esophageal gtv and the position of the esophagus. medical
images are sparsely labeled which are isolated by different tasks [20], and are
often inadequate. in this study, we use a paired first-second course gtv dataset
s p , an unpaired gtv dataset s v , and a public esophagus dataset s e .in order
to fully leverage both public and private datasets, the training objective
should not be specific to any tasks. here, we denote g 1 /g 2 as prior/target
annotations respectively, which are not limited only to the gtv areas. as shown
in fig. 1, our strategy is to challenge the network to retrieve information from
augmented inputs in e 1 using the features from e 2 , which can incorporate a
wide range of datasets that are not tailored for second-course gtv segmentation.",7
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.1,Tumor Volume Variation,"the differences in tumor volume between the first and second courses following
an rt treatment can have a negative impact on the state-of-the-art (sota)
learning-based techniques, which will be discussed in sect. 4.2. to adequately
monitor changes in tumor volume and integrate information from the initial
course into the subsequent course, a paired first-second courses dataset s p =
{i 1 p , i 2 p , g 1 p ; g 2 p } is necessary for training. in s p , i 1 p and i
2 p are the first and second course ct images, while g 1 p and g 2 p are the
corresponding gtv annotations.",7
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,3.2,Cancer Cell Proliferation,"the paired dataset s p for the first and second courses is limited, whereas an
unpaired gtv dataset s v = {i v ; g v } can be easily obtained in a standard
clinical workflow with a substantial amount. s v lacks its counterpart for the
second course, in which i v /g v are the ct image and the corresponding
annotation for gtv. to address this, we apply two distinct randomized
augmentations, p 1 , p 2 , to mimic the unregistered issue of the first and
second course ct. the transformed data is feed into the encoders e 1/2 as shown
in the following equations:, p 1 (g e ), p 2 (i e ), p 2 (g e ), when i e , g e
∈ s e .(4)the esophageal tumor can proliferate with varying morphologies into
the surrounding tissues. although not paired, s v contains valuable information
about the tumor. challenging the network to query information within gtv will
enhance the capacity to retrieve pertinent information for the tumor positions.",7
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.1,Experimental Setup,"datasets. the paired first-second course dataset, s p , is collected from sun
yat-sen university cancer center (ethics approval number: b2023-107-01),
comprising paired ct scans of 69 distinct patients from south china. we
collected the gtv dataset s v from medmind technology co., ltd., which has ct
scans from 179 patients. for both s p and s v , physicians annotated the
esophageal cancer gtv in each ct. the gtv volume statistics (cm 3 , mean ± std.)
in s v is 40.60 ± 29.75, and is 83.70 ± 55.97/71.66 ± 49.36 for the first/second
course rt in s p respectively. additionally, we collect s e from segthor [12],
consisting of ct scans and esophagus annotations from 40 patients who did not
implementation details. the ct volumes from the first and second course in s p
are aligned based on the center of the lung mask [8]. the ct volumes are applied
with a windowing of [-100, 300] hu, and resampled to 128 3 , with a voxel size
of 1.2 × 1.2 × 3 mm 3 . the augmentations p 1/2 involve a combination of random
3d resized cropping, flipping, rotation in the transverse plane, and gaussian
noise. we employ the adam [11] optimizer with (β 1 , β 2 , lr) = (0.9, 0.999,
0.001) for training for 500 epoches. the network is implemented using pytorch
[2] and monai [1], and detailed configurations are in the supplementary
material. experiments are performed on an nvidia rtx 3090 gpu with 24gb
memory.performance metrics. dice score (dsc), averaged surface distance (asd)
and hausdorff distance (hsd) are used as metrics for evaluation. the wilcoxon
signed-rank test is used to compare the performance of different methods.",7
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.2,Domain Gap Between the First and Second Course,"as previously mentioned, the volume of the tumors changes after the first course
of rt. to demonstrate the presence of a domain gap between the first and second
courses, we train sota methods with datasets s train p and s v , by feeding the
data sequentially into the network. we then evaluate the models on s test p .
the results presented in table 1 indicate a performance gap between gtv
segmentation in the first and second courses, with the latter being more
challenging. notably, the paired first-second course dataset s test p pertains
to the same group of patients, thereby ensuring that any performance drop can be
attributed solely to differences in courses of rt, rather than variations across
different patients.figure 2 illustrates the reduction in the gtv area after the
initial course of rt, where the transverse plane is taken from the same location
relative to the vertebrae (yellow lines). the blue arrows indicate that the
networks failed to track these changes and produced false predictions in the
second course of rt. this suggests that deep learning-based approaches may not
rely solely on the identification of malignant tissue patterns, as doctors do,
but rather predict highrisk areas statistically. therefore, for accurate
second-course gtv segmentation, we need to explicitly propagate prior
information from the first course using dual encoders in artseg, and incorporate
learning about tumor changes.",7
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,4.3,Evaluations of Second-Course GTV Segmentation Performance,"combination of various datasets. table 2 presents the information gain derived
from multi-center datasets using quantified metrics for segmentation
performance. we first utilize a standard artseg (w/o ram) as an ablation
network. when prior information from the first course is explicitly introduced
using s p , artseg outperforms other baselines for gtv segmentation in the
second course, which reaches a dsc of 66.73%. however, in fig. 3, it can be
observed that the model failed to accurately track the gtv area along the
esophagus (orange arrows) due to the soft and elongated nature of the esophageal
tissue, which deforms easily during ct scans performed at different times.by
subsequently incorporating s e for structural esophagus prior knowledge, the dsc
improved to 69.42%. meanwhile, the esophageal tumor comprises two primary
regions, the original part located in the esophagus and the extended part that
has invaded the surrounding tissue. as shown in fig. 3, identifying the tumor
proliferation into the surrounding tissue without comprehensive knowledge of
tumor morphology can be challenging (blue arrows). to address this,
incorporating s v to comprehensively learn the tumor morphology is required.when
s v is incorporated for learning tumor proliferation, the dsc improved to
72.64%. we can observe from case 2 in fig. 3 that the network has a better
understanding of the tumor proliferation with s v , while it still fails to
track the gtv area along the esophagus as pointed by the orange arrow.
therefore, s v and s e improve the network from two distinct aspects and are
both valuable. our proposed training strategy fully exploits the datasets s p ,
s v , and s e , and further improve the dsc to 74.54% by utilizing comprehensive
knowledge of both the tumor morphology and esophageal
structures.region-preserving attention module. although introducing the
esophageal structural prior knowledge using s e can improve the performance in
dsc and asd (table 2), the increase in hsd (38.22 to 47.89 mm; 21.71 to 27.00
mm) indicates that there are outliers far from the ground truth boundaries. this
may be attributed to the convolution that cannot effectively handle the
long-range knowledge of the esophagus structure. the attention mechanism can
effectively capture the long-range relationship as shown recently in
[13].however, there is no performance gain with mha as shown in table 2, and the
hsd further increased to 27.33 mm. we attribute the drawback is due to the
location-agnostic nature of the operations in mha, where the local regional
correlations are perturbed.to tackle the aforementioned problem, we propose ram
which involves the concatenation of the original features with attention
outputs, allowing for the preservation of convolution-generated regional tumor
patterns while effectively comprehending long-range prior knowledge specific to
the esophagus. finally, our proposed artseg with ram achieves the best dsc/hsd
of 75.26%/19.75 mm, and outperforms its ablations as well as other baselines, as
shown in table 2.limitations. for the method's generalizability, analysis of
diverse imaging protocols and segmentation backbones are inadequate. besides,
artseg requires more computational resources due to its dual-encoder and
attention design.",7
Second-Course Esophageal Gross Tumor Volume Segmentation in CT with Prior Anatomical and Radiotherapy Information,5.0,Conclusion,"in this paper, we reveal the domain gap between the first and second courses of
rt for esophageal gtv segmentation. to improve the accuracy of gtv declination
in the second course, we explicitly incorporated the naturally existing prior
information from the first course. besides, to efficiently leverage prior
knowledge contained in various medical ct datasets, we train the network in an
information-querying manner. we proposed ram to capture long-range prior
knowledge in the esophageal structure, while preserving the regional tumor
patterns. our proposed artseg incorporates prior knowledge of the tumor volume
variation, cancer cell proliferation, and reliance of gtv on esophageal anatomy,
which enhances the gtv segmentation accuracy in the second course rt. our future
research includes accurate delineation for multiple targets in the second course
and knowledge transferring through the time series of multiple courses.",7
CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,1.0,Introduction,"lung cancer is one of the most fatal diseases worldwide, and early diagnosis of
the pulmonary nodule has been identified as an effective measure to prevent lung
cancer. deep learning-based methods for lung nodule classification have been
widely studied in recent years [9,12]. usually, the malignancy prediction is
often formulated as benign-malignant binary classification [9,10,19], and the
higher classification performance and explainable attention maps are impressive.
most previous works employ a learning paradigm that utilizes cross-entropy loss
between predicted probability distributions and ground-truth one-hot labels.
furthermore, inspired by ordered labels of nodule progression, researchers have
turned their attention to ordinal regression methods to evaluate the
benignunsure-malignant classification task [2,11,13,18,21], where the training
set additionally includes nodules with uncertain labels. indeed, the ordinal
regressionbased methods are able to learn ordered manifolds and to further
enhance the prediction accuracy.however, the aforementioned methods still face
challenges in distinguishing visually similar samples with adjacent rank labels.
for example, in fig. 1(a), since we conduct unimodal contrastive learning and
map the samples onto a spherical space, the false positive nodule with a
malignancy score of 2.75 has a closer distance to that with a score of 4.75, and
the false negative one should not be closer to that of score 2.5. to address
this issue, we found that the text attributes, such as ""subtlety"", ""sphericity"",
""margin"", and ""lobulation"", annotated by radiologists, can exhibit the
differences between these hard samples. therefore, we propose leveraging text
annotations to guide the learning of visual features. in practice, this also
aligns with the fact that the annotated text information represents the direct
justification for identifying lesion regions in the clinic. as shown in fig. 1,
this text information is beneficial for distinguishing visually similar pairs,
while we conduct this behavior by applying contrastive learning that pulls
semantic-closer samples and pushes away semantic-farther ones.to integrate text
annotations into the image-domain learning process, an effective text encoder
providing accurate textual features is required. fortunately, recent advances in
vision-language models, such as contrastive languageimage pre-training (clip)
[16], provide us with a powerful text encoder pre-trained with text-based
supervisions and have shown impressive results in downstream vision tasks.
nevertheless, it is ineffective to directly transfer clip to medical tasks due
to the data covariate shift. therefore, in this paper, we pro- pose clip-lung, a
framework to classify lung nodules using image-text pairs. specifically,
clip-lung constructs learnable text descriptions for each nodule from both class
and attribute perspectives. inspired by cocoop [20], we propose a channel-wise
conditional prompt (ccp) module to allow nodule descriptions to guide the
generation of informative feature maps. different from cocoop, ccp constructs
specific learnable prompts conditioned on grouped feature maps and triggers more
explainable attention maps such as grad-cam [17], whereas cocoop provides only
the common condition for all the prompt tokens. then, we design a textual
knowledge-guided contrastive learning based on obtained image features and
textual features involving classes and attributes. experimental results on
lidc-idri [1] dataset demonstrate the effectiveness of learning with textual
knowledge for improving lung nodule malignancy prediction.the contributions of
this paper are summarized as follows.1) we propose clip-lung for lung nodule
malignancy prediction, which leverages clinical textual knowledge to enhance the
image encoder and classifier. 2) we design a channel-wise conditional prompt
module to establish consistent relationships among the correlated text tokens
and feature maps. 3) we simultaneously align the image features with class and
attribute features through contrastive learning while generating more
explainable attention maps.",7
EPVT: Environment-Aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition,1.0,Introduction,"skin cancer is a serious and widespread form of cancer that requires early
detection for successful treatment. computer-aided diagnosis systems (cad) using
deep learning models have shown promise in accurate and efficient skin lesion
diagnosis. however, recent research has revealed that the success of these
models may be a result of overly relying on ""spurious cues"" in dermoscopic
images, such as rulers, gel bubbles, dark corners, and hairs [3][4][5]29], which
leads to unreliable diagnoses. when a deep learning model overfits specific
artifacts instead of learning the correct dermoscopic patterns, it may fail to
identify skin lesions in real-world environments where the artifacts are absent
or inconsistent.to alleviate the artifact bias and enhance the model's
generalization ability, we rethink the problem from the domain generalization
(dg) perspective, where a model trained within multiple different but related
domains are expected to perform well in unseen test domains. as illustrated in
fig. 1, we define the domain labels based on the types of artifacts present in
the training images, which can provide environment-aware prior knowledge
reflecting a range of noisy contexts. by doing this, we can develop a dg
algorithm to learn the generalized and robust features from diverse
domains.previous dg algorithms learning domain-invariant features from source
domains have succeeded in natural image tasks [2,17,19], but cannot directly
apply to medical images, in particular skin images, due to the vast cross-domain
diversity of skin lesions in terms of shapes, colors, textures, etc. as each
domain contains ad hoc intrinsic knowledge, learning domain-invariant features
is highly challenging. one promising way is, as suggested in some recent works
[7,24,32], exploiting multiple learnable domain experts (e.g., batch norm
statistic, auxiliary classifiers, etc.) to capture domain-specific knowledge
from different source domains individually. still, two significant challenges
remain. first, previous work only exploits some weak experts, like the batch
norm, to capture knowledge, which naturally hampers the capability of capturing
essential domain-specific knowledge. second, previous methods such as [30]
focused on learning domain knowledge independently while overlooking the rich
cross-domain information that all domain experts can contribute collectively for
the target domain prediction.to overcome the above problems, we propose an
environment-aware prompt vision transformer (epvt) for domain generalization of
skin lesion recognition. on the one hand, inspired by the emerging prompt
learning techniques that embed prompts into a model for adaptation to diverse
downstream tasks [12,26,31], we construct different prompt vectors to strengthen
the learning of domainspecific knowledge for adaptation to diverse domains.
then, the self-attention mechanism of the vision transformer (vit) [8] is
adopted to fully model the relationship between image tokens and prompt vectors.
on the other hand, to encourage cross-domain information sharing while
preserving the domain-specific knowledge of each domain prompt, we propose a
domain prompt generator based on low-rank weights updating. the prompt generator
enables multiple domain prompts to work collaboratively and benefit from each
other for generalization to unknown domains. additionally, we devise a domain
mixup strategy to resolve the problem of co-occurring artifacts in dermoscopic
images and mitigate the resulting noisy domain label assignments.our
contributions can be summarized as: (1) we resolve an artifacts-derived biasing
problem in skin cancer diagnosis using a novel environment-aware prompt
learning-based dg algorithm, epvt; (2) epvt takes advantage of a vitbased
domain-aware prompt learning and a novel domain prompt generator to improve
domain-specific and cross-domain knowledge learning simultaneously;(3) a domain
mixup strategy is devised to reduce the co-artifacts specific to dermoscopic
images; (4) extensive experiments on four out-of-distribution skin datasets and
six biased isic datasets demonstrate the outperforming generalization ability
and robustness of epvt under heterogeneous distribution shifts.",7
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,1.0,Introduction,"computer-aided diagnosis (cad) systems have achieved success in many clinical
tasks [5,6,12,17]. most cad studies were developed on regular and selected
datasets in the laboratory environment, which avoided the problems (data noise,
missing data, etc.) in the clinical scenarios [3,6,9,13,18]. in a real clinical
scenario, the clinicians generally synthesize all aspects of information, and
conduct consultations with multidisciplinary team (mdt), to accurately diagnose
and plan the treatment [9,10,13]. real-world studies have received increasing
attention [11,16], and it is challenging for the cad in the real-world scenarios
as: 1) consistent with the clinical workflow, cad needs to consider
multidisciplinary information to obtain multidimensional diagnosis; 2) due to
information collection, storage and manual evaluation, there are missing and
noisy medical data. this phenomenon is especially common in rare tumors like
pancreatic neuroendocrine neoplasms (pnens).in order to overcome above
challenges, some studies [3,9,13,18] used multilabel method because of the
following advantages: 1) the input of the model is only a single modality such
as images, which is easy to apply clinically; 2) the model learns multi-label
and multi-disciplinary knowledge, which is consistent with clinical logic; 3)
multi-label simultaneous prediction, which meets the need of clinical
multi-dimensional description of patients. for the above advantages, multi-label
technology is suitable for real-world cad. the previous multi-label cad studies
were designed based on simple parameter sharing methods [9,15,20] or graph
neural network (gnn) method [2]. the former implicitly interacts with
multi-label information, making it difficult to fully utilize the correlation
among labels; and the latter requires the use of word embeddings pre-trained on
public databases, which is not friendly to many medical domain proper nouns. the
generalizability of previous multi-label cad studies is poor due to these
disadvantages. in addition, none of the current multi-label cad studies have
considered the problem of missing labels and noisy labels.considering these
real-world challenges, we propose a multi-label model named self-feedback
transformer (sft), and validate our method on a realworld pnens dataset. the
main contributions of this work are listed: 1) a transformer multi-label model
based on self-feedback mechanism was proposed, which provided a novel method for
multi-label tasks in real-world medical application; 2) the structure is
flexibility and interactivity to meet the needs of realworld clinical
application by using four inference modes, such as expert-machine combination
mode, etc.; 3) sft has good noise resistance, and can maintain good performance
under noisy label input in expert-assisted mode.",7
Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.1,Dataset and Evaluation,"real-world pnens dataset. we validated our method on a real-world pnens dataset
from two centers. all patients with arterial phase computed tomography (ct)
images were included. the dataset contained 264 and 28 patients in center 1 and
center 2, and a senior radiologist annotated the bounding boxes for all 408 and
28 lesions. we extracted 37 labels from clinical reports, including survival,
immunohistochemical (ihc), ct findings, etc. among them, 1)recist drug response
(rs), 2)tumor shrink (ts), 3)durable clinical benefit (dcb), 4)progression-free
survival (pfs), 5)overall survival (os), 6)grade (gd), 7)somatostatin receptor
subtype 2(sstr2), 8)vascular endothelial growth factor receptor 2 (vefgr2),
9)o6-methylguanine methyltransferase (mgmt), 10)metastatic foci (mtf), and
11)surgical recurrence (rt) are main tasks, and the remaining are auxiliary
tasks. 143 and 28 lesions were segmented by radiologists, and the radiomics
features of them were extracted, of which 162 features were selected and
binarized as auxiliary tasks because of its statistically significant
correlation with the main labels. the label distribution and the overlap ratio
(jaccard index) of lesions between pairs of labels are shown in fig. 3. it is
obvious that the real-world dataset has a large number of labels with randomly
missing data, thus, we used an adjusted 5-fold cross-validation. taking a
patient as a sample, we chose the dataset from center 1 as the internal dataset,
of which the samples with most of the main labels were used as dataset 1 (219
lesions) and was split into 5 folds, and the remaining samples are randomly
divided into the training set dataset 2 (138 lesions) and the validation set
dataset 3 (51 lesions), the training set and the validation set of the
corresponding folds were added during cross-validation, respectively. all
samples in center 2 left as external test set. details of each dataset are in
the supplementary material. dataset evaluation metrics. we evaluate the
performance of our method on the 10 main tasks for internal dataset, and due to
missing labels and too few sstr2 labels, only the performance of predicting rt,
pfs, os, gd, mtf are evaluated for external dataset. we employ accuracy (acc),
sensitivity (sen), specificity (spc), f1-score (f1) and area under the receiver
operating characteristic (auc) for each task, and compute the mean value of them
(e.g. mauc).",7
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,1.0,Introduction,"breast cancer is a serious health problem with high incidence and wide
prevalence for women throughout the world [1,2]. regular screening and early
detection are crucial for effective diagnosis and treatment, and hence for
improved prognosis and survival rate [3,4]. clinical researches have shown that
ultrasound imaging is an effective tool for screening breast cancer, due to its
critical characteristics of non-invasiveness, non-radiation and inexpensiveness
[5][6][7]. in clinical diagnosis, delineating tumor regions from background is a
crucial step for quantitative analysis [8]. manual delineation always depends on
the experience of radiologists, which tends to be subjective and time-consuming
[9,10].therefore, there is a high demand for automatic and robust methods to
achieve accurate breast tumor segmentation. however, due to speckle noise and
shadows in ultrasound images, breast tumor boundaries tend to be blurry and are
difficult to be distinguished from background. furthermore, the boundary and
size of breast tumors are always variable and irregular [11,12]. these issues
pose challenges and difficulties for accurate breast tumor segmentation in
ultrasound images.various approaches based on deep learning have been developed
for tumor segmentation with promising results [13][14][15][16][17][18][19]. su
et al. [13] designed a multi-scale u-net to extract more semantic and diverse
features for medical image segmentation, using multiple convolution sequences
and convolution kernels with different receptive fields. zhou et al. [14] raised
a deeply-supervised encoder-decoder network, which is connected through a series
of nested and dense skip pathways to reduce semantic gap between feature maps.
in [15], a multi-scale selection and multi-channel fusion segmentation model was
built, which gathers global information from multiple receptive fields and
integrates multi-level features from different network positions for accurate
pancreas segmentation. oktay et al. [16] proposed an attention gate model, which
is capable of suppressing irrelevant regions while highlighting useful features
for a specific task. huang et al. [17] introduced a unet 3+ for medical image
segmentation, which incorporates low-level and high-level feature maps in
different scales and learns full-scale aggregated feature representations. liu
et al. [18] established a convolution neural network optimized by super-pixel
and support vector machine, segmenting multiple organs from ct scans to assist
physicians diagnosis. pei et al. [19] introduced channel and position attention
module into deep learning neural network to obtain contextual information for
colorectal tumors segmentation in ct scans. however, although these proposed
models have achieved satisfactory results in different medical segmentation
tasks, their performances are limited for breast tumor segmentation in
ultrasound images due to the low image contrast and blurry tissue boundary.to
address these challenges, we present, to the best of our knowledge, the first
work to adopt multi-scale features collected from large set of clinical
ultrasound images for breast tumor segmentation. the main contributions of our
work are as follows: (1) we propose a well-pruned simple but effective network
for breast tumor segmentation, which shows remarkable and solid performance on
large clinical dataset; (2) our large pretrained model is evaluated on two
additional public datasets without fine-tuning and shows extremely stabilized
improvement, indicating that our model has outstanding generalizability and good
robustness against multi-site data data.",7
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,2.0,Method,"we demonstrate the architecture of our proposed network in fig. 1. it is based
on the unet [20] backbone. in order to collect multi-scale rich information for
tumor tissues, we propose to use gan [21]-based multi-scale deep supervision. in
particular, we apply similarity constraint for each stage of the unet decoder to
obtain consistent and stable segmentation maps. instead of using dice score in
the final layer of unet, we also use dice loss on each of the decoder stages.
besides, we integrate an adversarial loss as additional constraint to penalize
the distribution dissimilarity between the predicted segmentation map and the
ground truth. in the framework of gan, we take our segmentation network as the
generator and a convolutional neural network as the discriminator. the
discriminator consists of five convolution layers with the kernel sizes of 7 ×
7, 5 × 5, 4 × 4, 4 × 4 and 4 × 4. therefore, we formulate the overall loss for
the generator, namely the segmentation network, aswhere sn represents the
segmentation network and cn represents the involved convolutional network. θ sn
and θ cn refer to the parameters in the segmentation and convolutional network,
respectively. p (n) represents the segmentation maps obtained from the n-th
stage in the segmentation network, and g refers to the corresponding ground
truth. p sn (proba) denotes the distribution of probability maps. cn θcn (sn θsn
(p (2) )) represents the probability for the input of cn coming from the
predicted maps rather than the real ones. the parameters α 1 is empirically set
as 1, α 2 , α 3 , α 4 are set as 0.1, and β 1 , β 2 , β 3 and β 4 are set as
0.05. it should be noted that, in unet, there are 4 stages and hence we employ 4
cnns for each of them without sharing their weights. meanwhile, the adversarial
loss for each of the cnn is defined as:where p cn (truth) denotes the
distribution of original samples. cn θcn (g) represents the probability for the
input of cn coming from the original dataset.in the implementation, we update
the segmentation network and all the discriminators alternatingly in each
iteration until both the generator and discriminators are converged.",7
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3.1,Dataset and Implementation Details,"we collected 10927 cases for this research from yunnan cancer hospital. each
scan is with resolution of 1 × 1 mm 2 and size of 512 × 480. the breast tumors
of each case are delineated by three experienced experts. five-fold cross
validation is performed on the dataset in all experiments to verify our proposed
network. for external validation, we further test our model on two independent
publicly-available datasets collected by stu-hospital (dataset 1) [22] and
syu-university (dataset 2) [23]. in order to comprehensively evaluate
segmentation efficiency of our model, dice similarity coefficient (dsc),
precision, recall, jaccard, and root mean squared error (rmse) are used as
evaluation metrics in this work. our proposed algorithm is conducted on pytorch,
and all experiments are performed on nvidia tesla a100 gpu. we use adam
optimizer to train the framework with an initial learning rate of 10 -4 . the
epochs to train all models are 100 and the batch size in training process is set
as 4.",7
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3.2,Comparison with State-of-the-Art Methods,"to verify the advantages of our proposed model for breast tumor segmentation in
ultrasound images, we compare our deep-supervised convolutional network with the
state-ofthe-art tumor segmentation methods, including deepres [24], msunet [13],
unet++ [14], segnet [25], attu-net [16], u 2 -net [26] and unet 3+ [17]. the
comparison experiments are carried on a large-scale clinical breast ultrasound
dataset, and the quantitative results are reported in table 1. it is obvious
that our proposed model achieves the optimal performance compared with other
segmentation models. for example, our method obtains a dsc score of 80.97%,
which is 5.81%, 9.13%, 7.77%, 4.13%, 7.51%, 2.19%, and 3.83% higher than other
seven models. these results indicate the effectiveness of the proposed model in
delineating breast tumors in ultrasound images.representative segmentation
results using different methods are provided in fig. 2. the probability maps
predicted by our model are more consistent with the ground truth, especially in
the tiny structures which are difficult to capture. this verifies the superior
ability of the proposed model in maintaining detailed edge information compared
with state-of-the-art methods.",7
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,3.3,Ablation Studies,"we demonstrate the efficacy of the deep supervision strategy using ablation
studies. four groups of frameworks (stage i, stage ii, stage iii and stage iv)
are designed, with the numerals denoting the level of deep supervision counting
from the last deconvolutional layer.we test these four frameworks on the
in-house breast ultrasound dataset, and verify their segmentation performance
using the same five evaluation criteria. the evaluation metrics from all cases
are presented by the scatter plots in fig. 3. the obtained quantitative results
are shown in table 2, where stage iv model achieves the optimal dsc, precision,
recall, and jaccard. all these results draw a unanimous conclusion on the
relationship between these four frameworks. that is, the segmentation ability of
the proposed stage iv is ameliorated from every possible perspective. moreover,
stage iv obtains minimal rmse compared with other three models (0.68 mm vs 0.84
mm, 0.82 mm, 0.75 mm), which means better matching of the predicted maps from
stage iv with the corresponding ground truth. all these comparison results
verify the superiority of deep supervision for breast tumor segmentation in
ultrasound images.",7
Developing Large Pre-trained Model for Breast Tumor Segmentation from Ultrasound Images,4.0,Conclusion,"in this paper, we have developed a large pre-trained model for breast tumor
segmentation from ultrasound images. in particular, two constraints are proposed
to exploit both image similarity and space correlation information for refining
the prediction maps. moreover, our proposed deep supervision strategy is used
for quality control at each decoding stage, optimizing prediction maps
layer-by-layer for overall performance improvement. using a large clinical
dataset, our proposed model demonstrates not only state-of-the-art segmentation
performance, but also the outstanding generalizability to new ultrasound data
from different sites. besides, our large pre-trained model is general and robust
in handling various tumor types and shadow noises in our acquired clinical
ultrasound images. this also shows the potential of directly applying our model
in real clinical applications.",7
3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers,1.0,Introduction,"mitochondria are membrane-bound organelles that generate the primary energy
required to power the cell activities, thereby crucial for metabolism.
mitochondrial dysfunction, which occurs when mitochondria are not functioning
properly has been witnessed as a major factor in numerous diseases, including
noncommunicable chronic diseases (e.g, cardiovascular and cancer), metabolic
(e.g, obesity) and neurodegenerative (e.g, alzheimer and parkinson) disorders
[23,25]. electron microscopy (em) images are typically utilized to reveal the
corresponding 3d geometry and size of mitochondria at a nanometer scale, thereby
facilitating basic biological research at finer scales. therefore, automatic
instance segmentation of mitochondria is desired, since manually segmenting from
a large amount of data is particularly laborious and demanding. however,
automatic 3d mitochondria instance segmentation is a challenging task, since
complete shape of mitochondria can be sophisticated and multiple instances can
also experience entanglement with each other resulting in unclear boundaries.
here, we look into the problem of accurate 3d mitochondria instance
segmentation.earlier works on mitochondria segmentation employ standard image
processing and machine learning methods [20,21,33]. recent approaches address
[4,15,26] this problem by leveraging either 2d or 3d deep convolutional neural
network (cnns) architectures. these existing cnn-based approaches can be roughly
categorized [36] into bottom-up [3,4,14,15,28] and top-down [12]. in case of
bottom-up mitochondria instance segmentation approaches, a binary segmentation
mask, an affinity map or a binary mask with boundary instances is computed
typically using a 3d u-net [5], followed by a post-processing step to
distinguish the different instances. on the other hand, top-down methods
typically rely on techniques such as mask r-cnn [7] for segmentation. however,
mask r-cnn based approaches struggle due to undefined bounding-box scale in em
data volume.when designing a attention-based framework for 3d mitochondria
instance segmentation, a straightforward way is to compute joint spatio-temporal
selfattention where all pairwise interactions are modelled between all
spatiotemporal tokens. however, such a joint spatio-temporal attention
computation is computation and memory intensive as the number of tokens
increases linearly with the number of input slices in the volume. in this work,
we look into an alternative way to compute spatio-temporal attention that
captures long-range global contextual relationships without significantly
increasing the computational complexity. our contributions are as follows:-we
propose a hybrid cnn-transformers based encoder-decoder framework, named
stt-unet. the focus of our design is the introduction of a split spatio-temporal
attention (sst) module that captures long-range dependencies within the cubic
volume of human and rat mitochondria samples. the sst module independently
computes spatial and temporal self-attentions in parallel, which are then later
fused through a deformable convolution. -to accurately delineate the region of
mitochondria instances from the cluttered background, we further introduce a
semantic foreground-background (fg-bg) adversarial loss during the training that
aids in learning improved instance-level features. -we conduct experiments on
three commonly used benchmarks: lucchi [20],mitoem-r [36] and mitoem-h [36]. our
stt-unet achieves state-of-the-art fig. 1. qualitative 3d instance segmentation
comparison between the recent res-unet [16] and our proposed stt-unet approach
on the example input regions from mitoem-h and mitoem-r validation sets. here,
we present the corresponding segmentation predictions of the baseline and our
approach along with the ground truth.our stt-unet approach achieves superior
segmentation performance by accurately segmenting 16% more cell instances in
these examples, compared to res-unet-r.segmentation performance on all three
datasets. on lucchi test set, our stt-unet outperforms the recent [4] with an
absolute gain of 3.0% in terms of jaccard-index coefficient. on mitoem-h val.
set, stt-unet achieves ap-75 score of 0.842 and outperforms the recent 3d
res-unet [16] by 3.0%.figure 1 shows a qualitative comparison between our
stt-unet and 3d res-unet [16] on examples from mitoem-r and mitoem-h datasets.",8
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,1.0,Introduction,"chemical exchange saturation transfer (cest) is a novel metabolic magnetic
resonance imaging (mri) method that allows to detect molecules in tissue based
on chemical exchange of their mobile protons with water protons [18]. cest works
by selectively saturating the magnetization of a specific pool of protons, such
as those in metabolites or proteins, by applying narrow-band radiofrequency (rf)
pulses at their respective larmor frequency. due to chemical exchange this
saturation state is transferred to the water pool and a decrease in the detected
water signal provides information about the concentration and exchange rate of
the underlying molecules. this procedure is repeated for several rf frequencies
to acquire the so-called cest-spectrum in each voxel. cest-mri offers several
promising contrasts that correlate with the diagnosis of diseases such as
ischemic stroke [16], brain tumors [1], and neurodegenerative diseases [2,5].
the cestspectrum contains effects of proton pools of various chemical components
in the tissue, typically isolated by a lorentzian model [14] that is derived
form the underlying physics of the bloch-mcconnell equations [8]. in this
conventional method, several lorentzian distributions are fitted to the
cest-spectrum using nonlinear least squares method [10], and the amplitude of
each fitted distribution represents a particular metabolic map. the number of
lorentzian functions utilized in this process depends on the expected number of
exchanging proton pools present in the spectrum. figure 1a shows an example of
an acquired cest-spectrum and the corresponding 5-pool lorentzian fit.
increasing the static magnetic field b 0 (e.g., with b 0 = 7t), enhances
spectral resolution, but leads to significant variations in the b 1 amplitude of
the saturating rf field across the field of view (cf. fig. 1c). this b 1
inhomogeneity is corrected by acquiring cest-spectra at various rf field
strengths (cf. fig. 1b) and then interpolating between them at fixed b 1 to
produce the b 1 -robust metabolic cest contrast maps [14]. this b 1 correction
increases the acquisition time at least twofold.hunger et al. shown that
supervised learning can be used to generate b 1robust cest maps, coining the
deepcest approach [3,4]. however, the previous work on generating the b 1
-robust cest contrasts rely on valid target data and the underlying assumptions
to generate it, and can only create cest maps at one particular b 1 level.in
this work, we developed a conditional autoencoder (cae) [13] to generate b 1
-homogeneous cest-spectra at arbitrary b 1 levels, and a physics-informed
autoencoder (piae) to fit the 5-pool lorentzian model to the b 1 corrected
cest-spectra. this inclusion of physical knowledge in the form of known
operators in neural nets (nn) is expected to reduce the absolute error margin of
the model [6,7] and to increase its interpretability. both cae and piae are
trained in an unsupervised end-to-end method that eliminates the shortcomings of
conventional lorentzian curve fitting and produces robust cest contrast at
arbitrary b 1 levels without the need for an additional acquisition scan. we
called the proposed method physics-informed conditional autoencoder (picae).",8
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,2.0,Methods,"data measurements. cest imaging was performed in seven subjects, including two
glioblastoma patients, after written informed consent was obtained to
investigate the dependence of cest effects on b 1 in brain tissue. the local
ethics committee approved the study. all volunteers were measured at three b 1
field strengths 0.72 μt, 1.0 μt, and 1.5 μt. a method as described by mennecke
et al. [9] was used to acquire cest data on a 7t whole-body mri system
(magne-tom terra, siemens healthcare gmbh, erlangen, germany). saturated images
were obtained for 54 non-equidistant frequency offsets ranging from -100 ppm to
+100 ppm. the acquisition time per b 1 level was 6:42 min. the acquisition of
the b 1 map required an additional 1:06 min.conditional autoencoder. we
developed a conditional autoencoder (cae) to solve the b 1 inhomogeneity
problem, which is essential for the generation of metabolic cest contrast maps
at 7t. the left part of fig. 2 describes the cae. the encoding network of cae
took the raw cest-spectrum and the corresponding effective b 1 value as input
and generate a latent space that was concatenated once with the same b 1 input
value and passed to the decoder that reconstruct the uncorrected b 1
cest-spectrum, and another time the latent space was concatenated with the
desired/specific effective b 1 value to reconstruct the cest-spectrum at a
specific b 1 saturation amplitude. both decoders shared the weights (cf. fig.
2). for the development of the cae networks, we used the well-known fully
concatenated (fc) layers with leaky relu activations except for the last layer
of decoder, which had a linear activation. the encoder and decoder both
consisted of 4 layers, where the layers of the encoder successively contain 128,
128, 64, 32 neurons, while the layers of the decoder successively contain 32,
64, 128, 128 neurons. the input, latent space and output layers had 55, 17 and
54 neurons respectively.physics-informed autoencoder. the lorentzian model and
its b 1dispersion can be derived from the underlying spin physics described by
the bloch-mcconnell equation system [8]. the physics-informed autoencoder (piae)
utilized fully connected nn as encoder and lorentzian distribution generator as
a decoder to perform the pixel-wise 5-pool lorentzian curve fit to the
cest-spectrum (water, amide, amine, noe, mt) [14]. the 5-pool model was
described aswhere l denotes the lorentz function. the direct saturation pool
(water) was defined as(the remaining other four pools were defined as) 2 , i ∈
amide, amine, rn oe, ssm t .(the right part of fig. 2 describes the piae. the
encoder of piae mapped the cest-spectrum to the amplitudes a i , the full width
half maximum (fwhm) τ i , and the water peak position δ ds of the 5-pool
lorentzian model. its encoder consisted of four fc layers, each with 128 neurons
with leaky relu activations. it had three so-called fc latent space layers with
linear activation for position and exponential activations for fwhm and
amplitudes of 5-pool lorentzian model. the positions of amide, rnoe, ssmt, and
amine were fixed at 3.5 ppm, -3.5 ppm, -3 ppm, and 2 ppm, respectively, and
shifted with respect to the predicted position of the water peak. the decoder of
piae consisted of a lorentzian distribution generator (cf. fig. 2). it generated
samples of the 5-pool distributions exactly at the offsets δω (i.e. between -100
ppm and 100 ppm) where the input cest-spectrum was sampled, and combined them
according to eq. 1 to generate the input cest spectrum with or without b 0
correction.bound loss. the peak positions δ i and widths τ i of the pools had to
be within certain bounds so that certain neurons in the latent space layer of
piae would not be exchanged and provide the same pool parameters for all
samples. we developed a simple cost function along the lines of the hinge loss
[12], called the bound loss. mathematically, it is defined as followsthe bound
loss increases linearly as the output of the latent space neurons of piae
exceeds or recede from the boundaries. the lower and upper limits for positions
and widths are given in table 1 of the supplementary material.training and
evaluation. four healthy volunteers formed the training and validation sets. the
test set consisted of the two tumor patients and one healthy subject. to ensure
that the outcomes were exclusively based on the cest-spectrum and not influenced
by spatial position, the training was carried out voxel-by-voxel. consequently,
there were approximately one million cestspectra for the training process. cae
was first trained with mse loss. in this step, the cae encoder was fed with the
cest-spectrum of a specific b 1 saturation amplitude, and it generated two
cest-spectra, one for the input b 1 saturation level and the other for the b 1
level injected into the latent space (cf. fig. 2). later, it was trained with a
combination of mse loss and perception loss (mse loss between the latent space
of the cest-spectra at two different b 1 levels). to incorporate perception
loss, we used two forward passes with two different b 1 cest-spectra and used
perception loss to generate a latent space that is independent of b 1 saturation
amplitude. the following equation describes the loss of the second step.piae, on
the other hand, was trained with a combination of mse loss and bound loss. the
piae loss was described as followsfor evaluation we input the uncorrected
cest-spectrum acquired at 1μt and generated corrected cest-spectra at b 1 0.5,
0.72, 1.0, 1.3, 1.5 μt. piae encoder yielded the amplitudes of 5-pool for b 1
corrected cest-spectrum. its decoder reconstructed the b 1 b 0 fitted
cest-spectrum. the b 0 correction simply refers to the shift of the position of
the water peak to 0 ppm.cest quantification. the multi-b 1 cest-spectra allow
quantification of cest effects (amide, rnoe, amine) [14,15] down to the exchange
rate and concentration. the amplitudes of the cest contrasts were expressed
according to the definition in [15] as followswhere f i , k i , and r 2i express
the concentrations, exchange rates, and relaxation rates of the pools. z ref
defines the sum of all 5 distributions at the resonance frequency of the
specific pool in b 1 b 0 corrected cest-spectrum and w 1 is the frequency of the
oscillating field.the amplitudes of cest contrasts in the lorentzian function
have the b 1 dispersion function given by the labeling efficiency α (eq. 7). the
exchange rate occurs here separately from the concentration, which allows their
quantification via the b 1 dispersion. concentration and exchange rate were
fitted as a product and denoted as z 1 (quantified maps), and k(k+r 2 ) was also
fitted with the single term z 2 using trust-region reflective least squares
[10].",8
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,3.0,Results,"the comparison of picae with the conventional method [9,14] is shown in columns
1 and 2 of fig. 3. the top image in column 3 shows the t 1 -weighted reference
image enhanced with the exogenous contrast agent gadolinium (gd-t 1 w), and the
bottom image shows the b 1 -map. the tumor shows a typical so called gadolinium
ring enhancement indicated by the arrow (a 15 ), which is also visible in the
non-invasive and gadolinium-free cest contrast maps (columns 1 and 2). the
picae-cest maps showed better visualization of this tumor feature compared to
the conventional method. the proposed method yielded at least 25% increase in
the structural similarity index (ssim) with the gd-t 1 w image for the ring
enhancement region. the contrast maps also appear less noisy and more
homogeneous over the whole brain compared to the lorentzian fit on the
interpolated-corrected b 1 cest-spectra [14]. to further evaluate the
performance of piae and cae, we b1-corrected the data using cae and fitted it
with the least squares method (cae-lorentzian fit). the comparison of the cest
maps produced by the conventional lorentzian fit, the cae-lorentzian fit, and
picae is shown in table 1 using ssim and gradient cross correlation (gcc) [11]
for the tumor ring region. both the cae-lorentzian fit and picae were better
than the conventional method. cae-lorentzian fit even outperformed picae for
rnoe metabolic map and has similar performance for amide, but it has much lower
performance for amine. the ability of picae to produce b 1 -robust cest maps at
arbitrary levels is shown in fig. 4, where different b 1 levels reveal different
features of the heterogenous tumor. quantification of chemical exchange rates
and concentration, i.e., z 1 = f•k, is shown in column 4. z 1 (quantified maps)
further improve the visualization of the ring enhancement area. column 5 shows
the z 2 maps, which are combination of the exchange rate k and the relaxation
rate r 2 . quantified maps of amide, rnoe and amine for another tumor patient is
shown in supplementary fig. 1. the accuracy of the cae to generate particular b
1 cestspectra is depicted using absolute error for acquisition at different b 1
levels (see supplementary fig. 2). the performance was lower for b 1 0.72 μt,
and 1.5 μt compared to 1 μt.",8
Physics-Informed Conditional Autoencoder Approach for Robust Metabolic CEST MRI at 7T,4.0,Discussion,"in this work, we analyzed the use of an autoencoder approach to generate b
1robust cest contrast maps at arbitrary b 1 levels, which requires multiple
acquisitions in conventional methods [14]. the proposed method reduces the
acquisition time by at least half when only two acquisitions are performed for b
1 correction. supervised learning (deepcest) can generate cest maps that are not
susceptible to b 1 inhomogeneity at a particular b 1 , which already reduces
acquisition time. however, deepcest was trained on data fitted using a
conventional pipeline [9,14] which has suboptimal b 1 correction (cf. fig. 3).
moreover, the different pools in the cest spectrum are highlighted at different
b 1 levels (cf. fig. 4). an approach that can generate a b 1 -robust
cest-spectrum at multiple b 1 levels allows quantifying the exchange rate and
concentration of the cest pools [15]. the optimal b 1 can often only be selected
at post-processing during the analysis of clinical data, as some clinically
important features appear better at certain b 1 levels (cf. fig. 4). the
proposed picae approach combines b 1 correction and lorentzian curve fitting in
a single step. the b 1 correction was performed with a cae, while the lorentzian
line fitting was performed with a piae using nn as the encoder and lorentzian
distribution generator as the decoder. this allows interpretation of the model
while overcoming the drawback of curve fitting, such as being prone to noise
(cf. fig. 3).the bound loss ensured that the positions of the pools were not
interchanged. quantification was still performed using the nonlinear least
squares fit according to eq. 7. the main reason for this was that it does not
affect the acquisition time and it is affected by the z ref .the training was
performed voxel-wise to ensure that the results are based only on the
cest-spectrum. this also results in about 1 million cest-spectra for the
training. figure 3 shows the superiority of picae over the standard method, and
fig. 4 shows that the results produced by picae are authentic because the
quantification column z 1 matches the amplitude images and follows eq. 7.
cae-lorentzian fitting showed comparable performance for amide and rnoe maps,
but significantly lower performance for amine because it was still fitted using
the least squares method, which is susceptible to noise in the input and takes
up to 5 min to evaluate, compared to picae, which takes only a few seconds. the
1 μt acquisition performs better than 0.72 μt and 1.5 μt because it was trained
for both lower and higher b 1 values compared to the other two acquisitions. the
robustness of method for cyclic consistency [17] is displayed in supplementary
fig. 2, which also shows the interpretability of the method. the results of fig.
3 and fig. 4 also show the generalization capability of picae as it was trained
without the tumor data.",8
Diffusion-Based Data Augmentation for Nuclei Image Segmentation,3.1,Implementation Details,"datasets. we conduct experiments on two datasets: monuseg [13] and kumar [14].
the monuseg dataset has 44 labeled images of size 1000 × 1000, 30 for training
and 14 for testing. the kumar dataset consists of 30 1000×1000 labeled images
from seven organs of the cancer genome atlas (tcga) database. the dataset is
splited into 16 training images and 14 testing images. paired sample synthesis.
to validate the effectiveness of the proposed augmentation method, we create 4
subsets of each training dataset with 10%, 20%, 50% and 100% nuclei instance
labels. precisely, we first crop all images of each dataset into 256 × 256
patches with stride 128, then obtain the features of all patches with pretrained
resnet50 [6] and cluster the patches into 6 classes by kmeans. patches close to
the cluster centers are selected. the encoder and decoder of the two networks
have 6 layers with channels 256, 256, 512, 512, 1024 and 1024. for the
unconditional nuclei structure synthesis network, each layer of the encoder and
decoder has 2 resblocks and last 3 layers contain attnblocks. the network is
trained using the adamw optimizer with a learning rate of 10 -4 and a batch size
of 4. for the conditional histopathology image synthesis network, each layer of
the encoder and the decoder has 2 resblocks and 2 condresblocks respectively,
and last 3 layers contain attnblocks. the network is first trained in a
fully-conditional style (drop rate = 0) and then finetuned in a classifier free
style (drop rate = 0.2). we use adamw optimizer with learning rates of 10 -4 and
2 × 10 -5 for the two training stages, respectively. the batch size is set to be
1. for the diffusion process of both steps, we set the total diffusion timestep
t to 1000 with a linear variance schedule {β 1 , ..., β t } following [8].for
monuseg dataset, we generate 512/512/512/1024 synthetic samples for
10%/20%/50%/100% labeled subsets; for kumar dataset, 256/256/256/512 synthetic
samples are generated for 10%/20%/50%/100% labeled subsets. the synthetic nuclei
structures are generate by the nuclei structure synthesis network and the
corresponding images are generated by the histopathology image synthesis network
with the classifier-free guidance scale w = 2. each follows the reverse
diffusion process with 1000 timesteps [8]. we then obtain the augmented subsets
by adding the synthetic paired images to the corresponding labeled
subsets.nuclei segmentation. the effectiveness of the proposed augmentation
method can be evaluated by comparing the segmentation performance of using the
four labeled subsets and using the corresponding augmented subsets to train a
segmentation model. we choose to train two nuclei segmentation models -hover-net
[5] and pff-net [18]. to quantify the segmentation performance, we use two
metrics: dice coefficient and aggregated jaccard index (aji) [14].",8
Deep Unsupervised Clustering for Conditional Identification of Subgroups Within a Digital Pathology Image Set,3.2,Application to a Digital Pathology Dataset,"her2 dataset. human epidermal growth factor receptor 2 (her2 or her2/neu) is a
protein involved in normal cell growth, which plays an important role in the
diagnosis and treatment of breast cancer [8]. the dataset consists of 241
patches extracted from 64 digitized slides of breast cancer tissue which were
stained with her2 antibody. each tissue slide has been digitized at three
different sites using three different whole slide imaging systems, evaluated by
7 pathologists on a 0-100 scale, and following clinical practice labeled as her2
class 1, 2, or 3 (based on mean pathologists' scores with cut-points at 33 and
66). we use a subset of this dataset consisting of 672 images (the remainder is
held out for future research). because the intended purpose is finding subgroups
in the given dataset only, a separate test set is not used. the dimensions of
the images vary from 600 to 826 pixels, and we scale all data to a uniform size
of 128 × 128 pixels before further processing. we refer to [4,8] for more
details about this dataset. this retrospective human subject dataset has been
made available to us by the authors of the prior studies [4,8], who are not
associated with this paper. appropriate ethical approval for the use of this
material in research has been obtained.deep clustering models applied to the
her2 dataset. we evaluate the performance and behavior of the dec, vade, and
cdvade models on the her2 dataset. we investigate whether the models will learn
to distinguish the her2 class labels, the scanner labels, or other potentially
meaningful data subgroups in a fully unsupervised fashion. to investigate the
clustering abilities of cdvade on the her2 dataset, we inject the her2 class
labels into the latent embedding space. we hypothesize that this will
disincentivize the encoder network from including information related to the
her2 class labels in the latent representations z. thus, with cdvade we aim to
guide the clustering towards identifying subgroup structures that are not
associated with the her2 classes, and potentially were not previously
recognized. the dimensionality of the latent embedding space was set to d = 500
for all three models. as illustrated by the bar graphs in fig. 3, there is an
association between her2 class 2 and predicted domain 2, as well as between her2
class 3 and predicted domain 3. similarly to the vade model, the dec model has
also shown the ability to separate between her2 class 2 and her2 class 3. to
investigate these observations further, we look at the distribution of the
ground truth her2/neu scores within each of the predicted domains. the boxplots
in fig. 4 show that both the vade and dec models tend to separate high her2/neu
scores from the lower ones. the pearson's correlation coefficient between the
clustering assignments c of vade and the her2/neu scores is 0.46. the
correlation coefficient between the dec clusters and the her2/neu scores is
0.71. however, neither vade nor dec clusters are associated to the scanner
labels. we investigate the proposed cdvade model with the goal of identifying
meaningful data subgroups which are not associated with the already known her2
class labels. as visualized in fig. 3, the predicted domains are again clearly
visually disparate. however, as intended, there is a weaker association with the
her2 class labels and a stronger association with the scanner labels, compared
to the results of vade and dec. in fig. 4, her2/neu median scores of the three
clusters move closer together, illustrating the decrease of association with
her2 class labels, as intended by the formulation of the cdvade model. the
correlation coefficient between the cdvade cluster assignments and the her2/neu
scores is 0.39. while the cdvade model does not achieve full independence
between the identified clusters and the her2 labels, it decreases this
association compared to vade and dec. moreover, the clusters identified by
cdvade are distinctly different from those of vade, with a 0.43 proportion of
agreement between the two algorithms (after matching the two sets of cluster
assignments using the hungarian algorithm).",8
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,1.0,Introduction,"brachial plexopathy is a form of peripheral neuropathy [1]. it occurs when there
is damage to the brachial plexus (bp) which is a complex nerve network under the
skin of the shoulder. there is a wide range of disease that may cause a brachial
plexopathy.radiation fibrosis, primary and metastatic lung cancer, and
metastatic breast cancer account for almost three-fourths of causes [2].
brachial plexus syndrome occurs not infrequently in patients with malignant
disease. it is due to compression or direct invasion of the nerves by tumor
which will bring many serious symptoms [3]. our research focuses on the brachial
plexopathy caused by metastatic breast cancers.magnetic resonance imaging (mri)
and ultrasound of the brachial plexus have become two reliable diagnostic tools
for brachial plexopathy [4]. automatic identification of the bp in mri and
ultrasound images has become a hot topic. currently, most of relevant research
in this field are focusing on ultrasound modality [5][6][7][8]. compared with
ultrasound, mri has become the primary imaging technique in the evaluation of
brachial plexus pathology [9]. however, to our knowledge, radiomics related bp
studies utilizing mri have not been reported previously.many radiomics studies
have experimentally demonstrated that image texture has great potential for
differentiation of different tissue types and pathologies [10]. in the past
several decades, many state-of-the-art methods have been proposed to extract
texture patterns [11,12]. however, how to most effectively combine texture
features with deep learning, called deep texture, is still an open area of
research. one prior approach, termed glcm-cnn, was proposed to carry out a polyp
differentiation task [13]. however, how to arrange these glcms to form the 3d
volume to optimize the performance is a major challenge.with the goal of
classifying normal from abnormal bp, we explored the approach of deep texture
learning. this paper constructed a bp dataset with the most commonly used bp
mris in our clinical practice. considering the shortcoming of traditional
patterns, triple point pattern (tpp) is proposed for the quantitative
representation of the heterogeneity of abnormal bp's. in contrast to glcm-cnn,
tppnet is designed to train models by feeding tpp matrices as the input with a
huge number of channels. finally, we analyze the model's performance in the
experimental section. the major contributions of this study include 1) directed
triangle construction idea for tpp, 2) huge number of tpp matrices as the
heterogeneity representations of bp, 3) tppnet with 15 layers and huge number of
channels, 4) the bp dataset containing mr images and their corresponding roi
masks.",8
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.1,Dataset Preparation and Preprocessing,"following irb approval for this study, we search for patients with metastatic
breast cancer who had a breast cancer mri performed between 2010 and 2020 and
had morphologically positive bp on the mri report from our electronic medical
records (emr) in * hospital. totally, we collect approximate 807 series which
include 274 t2, 254 t1 and 279 post-gadolinium. since some scans are seriously
degraded due to motion artifacts. therefore, each case underwent several
essential image adjustments such as multi-series splitting, two-series merging,
slice swapping, artifact checking and boundary corrections. to yield the roi,
firstly, we randomly sampled -40% of the sequences including both normal and
abnormal ones that were manually segmented with itk-snap by two skilled trainees
[14,15]. then, the manual segmentations were utilized to train a 3d nnunet model
which was utilized to train the model which was used to predict rois for the
rest series [16]. the predicted segmentations were manually divided into three
groups, i.e. good, fair and poor. good cases were added to the training set.
this process was repeated until no improvements in the predictions for the
remaining sequences was seen. the final dataset for radiomic analysis was
constructed by merging the datasets for each sequence type. only patients that
had all three sequences segmented (t2, t1 and post-gadolinium) were included in
the dataset. table 1 shows a breakdown of the final dataset.",8
A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,4.0,Conclusions,"in this paper, we develop an approach to carry out the pioneer study of
differentiating abnormal bp from normal ones relevant to breast cancer. in
particular, tpp is proposed to extract texture features as the representation of
bp's heterogeneity from mris. moreover, a tppnet with huge number of initial
channels is designed to train the model. to testify our proposed tppnet, a bp
dataset is constructed with 452 series including three most commonly used mr
sequences in clinical practice, i.e. t2, t1 and post-gadolinium. the best result
is yielded when the gray level is 12, intensity rescaling method adopts arc
tangent approach. experimental outcomes also demonstrate that the proposed
tppnet not only exhibit more stable performances but also outperform six famous
state-of-the-art approaches over three most commonly used bp mr sequences.",8
CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention,1.0,Introduction,"nuclei detection is a highly challenging task and plays an important role in
many biological applications such as cancer diagnosis and drug discovery.
rectangle object detection approaches that use cnn have made great progress in
the last decade [4,7,12,14,18]. these popular cnn models use boxes to represent
objects that are not optimized for circular medical objects, such as detection
of glomeruli in renal pathology. to address the problem, an anchor-free cnnbased
circular object detection method circlenet [16] is proposed for glomeruli
detection. different from centernet [18], circlenet estimates the radius rather
than the box size for circular objects. but it also suffers poor detection
accuracy for overlapping objects and requires additional post-processing steps
to obtain the final detection results.recently, detr [1], a transformer-based
object detection method reformulates object detection as a set-to-set prediction
problem, and it removes both the hand-crafted anchors and the non-maximum
suppression (nms) postprocessing. its variants ( [3,10,11,15,19]) demonstrate
promising results compared with cnn-based methods and detr by improving the
design of queries for faster training convergence. built upon conditional-detr,
dab-detr [10] introduces an analytic study of how query design affects rectangle
object detection. specifically, it models object query as 4d dynamic anchor
boxes (x, y, w, h) and iteratively refine them by a sequence of transformer
decoders. however, recent studies on transformer-based detection methods are
designed for rectangle object detection in computer vision, which are not
specifically designed for circular objects in medical images.in this paper, we
introduce circleformer, a transformer-based circular object detection for
medical image analysis. inspired by dab-detr, we propose to use an anchor circle
(x, y, r) as the query for circular object detection, where (x, y) is the center
of the circle and r is the radius. we propose a novel circle cross attention
module which enables us to apply circle center (x, y) to extract image features
around a circle and make use of circle radius to modulate the cross attention
map. in addition, a circle matching loss is adopted in the set-to-set prediction
part to process circular predictions. in this way, our design of circle-former
lends itself to circular object detection. we evaluate our circleformer on the
public monuseg dataset for nuclei detection in whole slide images. experimental
results show that our method outperforms both cnn-based methods for box
detection and circular object detection. it also achieves superior results
compared with recently transformer-based box detection approaches. meanwhile, we
carry out ablation studies to demonstrate the effectiveness of each proposed
component. to further study the generalization ability of our approach, we add a
simple segmentation branch to circleformer following the recent query based
instance segmentation models [2,17] and verify its performance on monuseg as
well.",8
Self-supervised Dense Representation Learning for Live-Cell Microscopy with Time Arrow Prediction,3.1,Datasets,"to demonstrate the utility of tap for a diverse set of specimen and microscopy
modalities we use the following four different datasets:hela. human cervical
cancer cells expressing histone 2b-gfp imaged by fluorescence microscopy every
30 min [29] . the dataset consists of four videos with overall 368 frames of
size 1100 × 700. we use δt = 1 for tap training.mdck. madin-darby canine kidney
epithelial cells expressing histone 2b-gfp (cf. fig. 3b), imaged by fluorescence
microscopy every 4 min [27,28]. the dataset consists of a single video with 1200
frames of size 1600×1200. we use δt ∈ {4, 8}.flywing. drosphila melanogaster
pupal wing expressing ecad::gfp (cf. fig. 3a), imaged by spinning disk confocal
microscopy every 5 min [4,20]. the dataset consists of three videos with overall
410 frames of size 3900 × 1900.we use δt = 1.yeast. s. cerevisiae cells (cf.
fig. 3c) imaged by phase-contrast microscopy every 3 min [16,17]. the dataset
consists of five videos with overall 600 frames of size 1024 × 1024. we use δt ∈
{1, 2, 3}.for each dataset we heuristically choose δt to roughly correspond to
the time scale of observable biological processes (i.e. larger δt for higher
frame rates).",8
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,2.2,Optimization,"our overall loss function is defined aswhere only the parameters of the g(•) and
the prompt p are optimized, while the feature extractor model f (•) is
frozen.training the entire pipeline in an end-to-end fashion on gigapixel images
is infeasible using the current hardware. to address this issue, we utilize the
patch batching and gradient retaining techniques from [25]. as shown in fig.
1(a), to reduce the gpu memory consumption, the n tissue patches {x 1 , x 2 , .
. . , x n } are grouped into m batches. the first step (step① in the figure) of
our optimization is to sequentially feed m batches of tissue patches forward to
the feature model to compute its respective features which are subsequently
concatenated into the h matrix. in this step, we just conduct a forward pass
like the inference stage, without storing the memory-intensive computational
graph for back-propagation.in the second step (step②), we feed h into the
classifier g(•) to calculate the loss l and update the parameters of g(•) by
back-propagate the loss. the back-propagated gradients g = ∂l/∂h on h are
retained for the next step.finally (step③), we feed the input batches into the
feature model f (•) again and use the output h and the retained gradients g from
the last step to update the trainable prompt tokens. in particular, the
gradients on the j th prompt token p j are calculated as:where g i is the
gradient calculated with respect to h i .to sum up, in each step, we only update
either f or g given the current batch, which avoid storing the gradients of the
whole framework for all the input patches. this patch batching and gradient
retaining techniques make the end-to-end training feasible.in this study, we use
dsmil [13] as the classifier and binary cross entropy as the classification loss
l cls when the task is a tumor sub-type classification or cross entropy
otherwise.",8
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,3.1,Datasets,"we assessed prompt-mil using three histopathological wsi datasets: tcga-brca
[14], tcga-crc [19], and bright [2]. these datasets were utilized for both the
self-supervised feature extractor pretraining and the end-to-end finetuning
(with or without prompts), including the mil component. note that the testing
data were not used in the ssl pretraining. tcga-brca contains 1034 diagnostic
digital slides of two breast cancer subtypes: invasive ductal carcinoma (idc)
and invasive lobular carcinoma (ilc). we used the same training, validation, and
test split as that in the first fold cross validation in [5]. the cropped
patches (790k training, 90k test) were extracted at 5× magnification. tcga-crc
contains 430 diagnostic digital slides of colorectal cancer for a binary
classification task: chromosomal instability (cin) or genome stable (gs).
following the common 4-fold data split [1,16], we used the first three folds for
training (236 gs, 89 cin), and the fourth for testing (77 gs, 28 cin). we
further split 20% (65 slides) training data as a validation set. the cropped
patches (1.07m training, 370k test) were extracted at 10× magnification. bright
contains 503 diagnostic slides of breast tissues. we used the official training
(423 wsis) and test (80 wsis) splits.",8
Prompt-MIL: Boosting Multi-instance Learning Schemes via Task-Specific Prompt Tuning,3.3,Results,"we chose overall accuracy and area under receiver operating characteristic curve
(auroc) as the evaluation metrics.evaluation of prompt tuning performance: we
compared the proposed prompt-mil with two baselines: 1) a conventional mil model
with a frozen feature extractor [13], 2) fine-tuning all parameters in the
feature model (full fine-tuning). table 1 highlights that our prompt-mil
consistently outperformed both. compared to the conventional mil method,
prompt-mil added negligible parameters (192, less than 0.3% of the total
parameters), achieving a relative improvement of 1.49% in accuracy and 0.25% in
auroc on tcga-brca, 3.36% in accuracy and 8.97% in auroc on tcga-crc, and 4.03%
in accuracy and 0.43% in auroc on bright. the observed improvement can be
attributed to a more optimal alignment between the feature representation
learned during the ssl pretraining and the downstream task, i.e., the prompt
explicitly calibrated the features toward the downstream task.the
computationally intensive full fine-tuning method under-performed conventional
mil and prompt-mil. compared to the full fine-tuning method, our method achieved
a relative improvement of 1.29% to 13.61% in accuracy and 3.22% to 27.18% in
auroc on the three datasets. due to the relatively small amount of slide-level
labels (few hundred to a few thousands) fully fine tuning 5m parameters in the
feature model might suffer from overfitting. in contrast, our method contained
less than 1.3% of parameters compared to full fine-tuning, leading to robust
training. evaluation of time and gpu memory efficiency: prompt-mil is an
efficient method requiring less gpu memory to train and running much faster than
full fine-tuning methods. we evaluated the training speed and memory consumption
of our method and compared to the full fine-tuning baseline on four different
sized wsis in the bright dataset. as shown in table 2, our method consumed
around 38% to 45% less gpu memory compared to full finetuning and was 21% to 27%
faster. as we scaled up the wsi size (i.e. wsis with more number of patches),
the memory cost difference between prompt-mil and full fine-tuning further
widened.evaluation on the pathological foundation models: we demonstrated our
prompt-mil also had a better performance when used with the pathological
foundation model. foundational models refer to those trained on large-scale
pathology datasets (e.g. the entire tcga pan-cancer dataset [28]). we utilized
the publicly available [26,27] vit-small network pretrained using moco v3 [6] on
all the slides from tcga [28] and paip [22]. in table 3, we showed that our
method robustly boosted the performance on both tcga (the same domain as the
foundation model trained on) and bright (a different domain). the improvement is
more prominent in bright, which further confirmed that prompt-mil aligns the
feature extractor to be more task-specific. ablation study: an ablation was
performed to study the effect of the number of trainable prompt tokens on
downstream tasks. table 4 shows the accuracy and auroc of our prompt-mil model
with 1, 2 and 3 trainable prompt tokens (k = 1, 2, 3) on the tcga-brca and the
bright datasets. on the tcga-brca dataset, our prompt-mil model with 1 to 3
prompt tokens reported similar performance. on the bright dataset, the
performance of our model dropped with the increased number of prompt tokens.
empirically, this ablation study shows that for classification tasks, one prompt
token is sufficient to boost the performance of conventional mil methods.",8
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,1.0,Introduction,"nucleus classification is to identify the cell types from digital pathology
image, assisting pathologists in cancer diagnosis and prognosis [3,30]. for
example, the involvement of tumor-infiltrating lymphocytes (tils) is a critical
prognostic variable for the evaluation of breast/lung cancer [4,29]. it is a
challenge to infer the nucleus types due to the diversity and unbalanced
distribution of nuclei. thus, we aim to automatically classify cell nuclei in
pathological images.a number of methods [7,10,14,[23][24][25]33,34] have been
proposed for automatic nuclei segmentation and classification. most of them use
a u-shape model [28] for training to produce dense predictions with expensive
pixel-level labels. in this paper, we aim to obtain the location and category of
cells, which only needs affordable labels of centroids or bounding boxes. the
task can be solved by generic object detector [17,26,27], but they are usually
built for everyday objects whose positions and combinations are quite random.
differently, in pathological images, experts often identify nuclear communities
via their relationships and spatial distribution. some recent methods resort to
the spatial contexts among nuclei. abousamra et al. [1] adopt a spatial
statistical function to model the local density of cells. hassan et al. [11]
build a location-based graph for nuclei classification. however, the semantics
similarity and dissimilarity between nucleus instances as well as the category
representations have not been fully exploited.based on these observations, we
develop a learnable grouping transformer based classifier (gtc) that leverages
the similarity between nuclei and their cluster representations to infer their
types. specifically, we define a number of nucleus clusters with learnable
initial embeddings, and assign nucleus instances to their most correlated
clusters by computing the correlations between clusters and nuclei. next, the
cluster embeddings are updated with their affiliated instances, and are further
grouped into the categorical representations. then, the cell types can be well
estimated using the correlations between the nuclei and the categorical
embeddings. we propose a novel fully transformer-based framework for nuclei
detection and classification, by integrating a backbone, a centroid detector,
and the grouping-based classifier. however, the transformer framework has a
relatively large number of parameters, which could cause high costs in
fine-tuning the whole model on large datasets. on the other hand, there exist
domain gaps in the pathological images of different organs, staining, and
institutions, which makes it necessary to fine-tune models to new applications.
thus, it is of great significance to tune our proposed transformer framework
efficiently.inspired by the prompt tuning methods [13,16,20] which train
continuous prompts with frozen pretrained models for natural language processing
tasks, we propose a grouping prompt based learning strategy for efficient
tuning. we prepend the embeddings of nucleus clusters to the input space and
freeze the entire pre-trained transformer backbone so that these group
embeddings act as prompt information to help the backbone extract grouping-aware
features. our contributions are: (1) a prompt-based grouping transformer
framework for end-to-end detection and classification of nuclei; (2) a novel
grouping prompt learning mechanism that exploits nucleus clusters to guide
feature learning with low tuning costs; (3) experimental results show that our
method achieves the state-of-the-art on three public benchmarks.",8
Prompt-Based Grouping Transformer for Nucleus Detection and Classification,3.1,Datasets and Implementation Details,"consep 1 [10] is a colorectal nuclear dataset with three types, consisting of 41
h&e stained image tiles from 16 colorectal adenocarcinoma whole-slide images
(wsis). the wsis are at 20× magnification and the size of the slides is 500 ×
500. we split them following the official partition [1,10].is a breast cancer
dataset with three types and consists of 120 image tiles from 113 patients. the
wsis are at 20× magnification and the size of the slides ranges from 465 × 465
to 504 × 504. we follow the work [1] to apply the slic [2] algorithm to generate
superpixels as instances and split them into 80/10/30 slides for
training/validation/testing.lizard 3 [9] has 291 histology images of colon
tissue from six datasets, containing nearly half a million labeled nuclei in h&e
stained colon tissue. the wsis are at 20× magnification with an average size of
1,016 × 917 pixels. our implementation and the setting of hyper-parameters are
based on mmdetection [5]. the number of grouping prompts g is 64. random crop,
flipping, and scaling are used for data augmentation. our method is trained with
pytorch on a 48 gb gpu (nvidia a100) for 12-24 h (depending on the dataset
size). more details are listed in the supplementary material.",8
Maximum-Entropy Estimation of Joint Relaxation-Diffusion Distribution Using Multi-TE Diffusion MRI,5.0,Summary,"in summary, this work introduced a maximum-entropy framework for estimating the
relaxation-diffusion distribution functions using rdmri. to our knowledge, this
is the first work showing that the estimation of multidimensional rdd functions
is equivalent to the classical multivariate hausdorff moment problem. although
this work focuses on the two dimensional rdd functions, the results generalize
to the special cases for one dimensional relaxation or diffusion distribution
functions. the contributions of this work also include the development of three
algorithms to estimate rdd functions and the comparisons with the standard
basis-function approach. the me-rdd functions can be estimated using convex
optimization algorithms. experimental results have shown that the proposed
methods provide more accurate parameters for each component and more accurate
volume fractions compared to the standard basis function methods. moreover,
results based on in vivo data have shown that the proposed me-rdd can resolve
multiple components that cannot be distinguished by the basis function approach.
the better performance me-rdd functions compared to basis-function methods may
relate to the superior performance of me spectral estimation methods compared to
fourier transform-based methods [21]. we expect the improved spectral resolution
will be useful in several clinical applications such as lesion and tumor
detection. but further theoretical analysis on the performance of the me methods
is needed in future work. moreover, further histological validations are needed
to examine the biological basis of the rdd functions. finally, we note a
limitation of the proposed method is that the optimization algorithm may have a
slow convergence speed because the hessian matrices may not be well conditioned.
moreover, the results may be sensitive to measurement noise. thus faster and
more reliable computation algorithms will be developed in future work.",8
Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model,3.0,Experiments and Results,"dataset: with irb approval and informed consent, we retrospectively used 126
clinical cases (113 training, 13 testing) from a internal private dataset3 using
gadoterate meglumine contrast agent (site a). for downstream task assessment we
used 159 patient studies from another site (site b) using gadobenate
dimeglumine. the detailed cohort description is given in table 1. the clinical
indications for both sites included suspected tumor, post-op tumor follow-up and
routine brain. for each patient, 3d t1w mprage scans were acquired for the
pre-contrast, low-dose, and post-contrast images. these paired images were then
mean normalized and affine co-registered (pre-contrast as the fixed image) using
simpleelastix [17]. the images were also skull-stripped, to account for
differences in fat suppression, using the hd-bet brain extraction tool [18] for
generating the ""soft labels"". evaluation settings: we quantitatively evaluated
the proposed model using psnr, ssim, rmse, and lpips perceptual metrics [19],
between the synthesized and true low-dose images. we replaced the gformer
backbone with other state-of-the-art methods to compare the efficacy of the
different methods. particularly, the following backbone networks were studied:
simple linear scaling (""scaling"") approach, rednet [20], mapnn [13], restormer
[21], and swinir [22]. unet [23] and swin-unet [24] models were not assessed due
to their tendency to synthesize blurry artifacts in the iterative modelling.
throughput metric (number of images generated per second) was also calculated to
assess the inference efficiency.evaluation results: figure 4(a) shows that the
proposed model is able to generate images that correspond to different dose
levels. as shown in the zoomed inset, the hyperintensity of the contrast uptake
in these images gradually reduces at each iteration. figure 4(b) shows that the
pathological structure in the synthesized low-dose image is similar to that of
the ground truth. figure 4(c) also shows that the model is robust to
hyperintensities that are not related to contrast uptake. figure 3 and table 2
show that proposed model can synthesize enhancement patterns that look close to
the true low-dose and that it performs better than the other competing methods
with a reasonable inference throughput. quantitative assessment of contrast
uptake: the above pixel-based metrics do not specifically focus on the contrast
uptake region. in order to assess the contrast uptake patterns of the
intermediate images, we used the following metrics as described in [25]:
contrast to noise ratio(cnr), contrast to background ratio(cbr), and contrast
enhancement percentage(cep). the roi for the contrast uptake was computed as the
binary mask of the corresponding ""soft labels"" in eq. 2. as shown in fig. 5, the
value of the contrast specific metrics increases in a non-linear fashion as the
iteration step increases. downstream tasks: in order to demonstrate the clinical
utility of the synthesized low-dose images, we performed two downstream tasks:
1) low-dose to full-dose synthesis using the dl-based algorithm to predict
full-dose image from pre-contrast and low-dose images described in [5], we
synthesized t1ce volumes using true low-dose (t1ce-real-ldose) and gformer (rot)
synthesized low-dose (t1ce-synth-ldose). we computed the psnr and ssim metrics
of t1ce vs t1ce-synth/t1ce vs t1ce-synth-sim which are 29.82 ± 3.90 db/28.10 ±
3.20 db and 0.908 ± 0.031/0.892 ± 0.026 respectively. this shows that the
synthesized low-dose images perform similar4 to that of the low-dose image in
the dose reduction task. for this analysis we used the data from site b. 2)
tumor segmentation using the t1ce volumes synthesized in the above step, we
perform tumor segmentation using the winning solution of brats 2018 challenge
[26]. let m true , m ldose and m ldose-sim be the whole tumor (wt) masks
generated using t1ce, t1ce-real-ldose and t1ce-synth-ldose (+ t1, t2 and flair
images) respectively. the mean dice scores dice(m true , m ldose ) and dice(m
true , m ldose-sim ) on the test set were 0.889±0.099 and 0.876±0.092
respectively. figure 6 shows visual examples of tumor segmentation performance.
this shows that the clinical utility provided by the synthesized low-dose is
similar5 to that of the actual low-dose image.",8
TractCloud: Registration-Free Tractography Parcellation with a Novel Local-Global Streamline Point Cloud Representation,0.0,Table 3 .,"works on unregistered tractography from neonate brains (much smaller than adult
brains). in the challenging btp (tumor patients) dataset, tractcloud reg-free
obtains significantly lower tda values than sota methods and comparable
performance to tractcloud regist . as shown in table2, our registration-free
framework is much faster than other compared methods.",8
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,1.0,Introduction,"recent advances in diffusion mri (dmri) and diffusion signal modeling equip
brain researchers with an in vivo probe into microscopic tissue compositions
[15,21]. signal differences between water molecules in restricted, hindered, and
free compartments can be characterized by higher-order diffusion models for
estimating the relative proportions of cell bodies, axonal fibers, and
interstitial fluids within an imaging voxel. this allows for the detection of
tissue compositional changes driven by development, degeneration, and disorders
[13,22]. however, accurate characterization of tissue composition is not only
affected by compartment-specific diffusivities but also transverse relaxation
rates [4,27]. several studies have shown that explicit consideration of the
relaxation-diffusion coupling may improve the characterization of tissue
microstructure [6,16,25].multi-compartment models are typically used to
characterize signals from, for example, intra-and extra-neurite compartments
[18,29]. however, due to the multitude of possible compartments and fiber
configurations, solving for these models can be challenging. the problem can be
simplified by considering per-axon diffusion models [8,10,28], which typically
factor out orientation information and hence involve less parameters. however,
existing models are typically constrained to data acquired with a single te
(ste) and do not account for compartment-specific t 2 relaxation. several
studies have shown that multi-te (mte) data can account better for intravoxel
architectures and fiber orientation distribution functions (fodfs)
[1,6,16,17,19].here, we propose a unified strategy to estimate using mte
diffusion data (i) compartment specific t 2 relaxation times; (ii) non-t 2
-weighted (non-t 2 w) parameters of multi-scale microstructure; and (iii) non-t
2 w multi-scale fodfs. our method, called relaxation-diffusion spectrum imaging
(rdsi), allows for the direct estimation of non-t 2 w volume fractions and t 2
relaxation times of tissue compartments. we evaluate rdsi using both ex vivo
monkey and in vivo human brain mte data, acquired with fixed diffusion times
across multiple b-values. using rdsi, we demonstrate the te dependence of t 2 w
fodfs. furthermore, we show the diagnostic potential of rdsi in differentiating
tumors and normal tissues.",8
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.2,In Vivo Data: Compartment-Specific Parameters,"figure 2 shows the rdsi t 2 relaxation maps of restricted, hindered, and free
diffusion across b-values. the values are consistent between healthy and glioma
subjects. the estimated relaxation times are in general in line with previous
reports [6,11]. rdsi shows substantial differences between tumor and normal
tissues in the relaxation maps (fig. 2(b)).figure 3 shows the voxel
distributions with respect to relaxation times and b-values. it is apparent that
at higher b-values, a greater fraction of voxels in the restricted compartment
have relaxation times within 100 to 200 ms, particularly for higher-grade
gliomas. this might be related to prolonged transverse relaxation time due to
increased water content within the tumor [5,7,24]. this property is useful in
the visualization of peritumoral edema, an area containing infiltrating tumor
cells and increased extracellular water due to plasma fluid leakage from
aberrant tumor capillaries that surrounds the tumor core in higher-grade
gliomas.",8
Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.3,In Vivo Data: Neurite Morphology,"figure 4(a) shows the relaxation times of the restricted compartment in white
matter lesions, indicating that relaxation times are longer in gliomas than
normal white matter tissue. the higher t 2 in grade 4 glioma is associated with
changes in metabolite compositions, resulting in remarkable changes in neurite
morphology in lesioned tissues (fig. 4(c-d)), consistent with previous
observations [12,23]. the rate of longitudinal relaxation time has been shown to
be positively correlated with myelin content. our results indicate that mte dmri
is more sensitive to neurite morphology than ste dmri (fig. 4(b)).figures 4(c-d)
show that the estimated mean nr in the gray matter is approximately in the range
of 10 µm, which is in good agreement with the sizes of somas in human brains,
i.e., 11 ± 7 µm [26]. rdsi improves the detection of small metastases,
delineation of tumor extent, and characterization of the intratumoral
microenvironment when compared to conventional microstructure models (fig.
4(c)). our studies suggest that rdsi provides useful information on
microvascularity and necrosis helpful for facilitating early stratification of
patients with gliomas (fig. 4(d)).",8
Exploring Unsupervised Cell Recognition with Prior Self-activation Maps,3.1,Implementation Details,"dataset. we validated the proposed method on the public dataset of multi-organ
nuclei segmentation (monuseg) [13] and breast tumor cell dataset (bcdata) [11].
monuseg consists of 44 images of size 1000 × 1000 with around 29,000 nuclei
boundary annotations. bcdata is a public large-scale breast tumor dataset
containing 1338 immunohistochemically ki-67 stained images of size 640 × 640.
evaluation metrics. in our experiments on monuseg, f1-score and iou are employed
to evaluate the segmentation performance. denote t p , f p , and f n as the
number of true positives, false positives, and false negatives. then f1score and
iou can be defined as: f 1 = 2t p/(2t p + f p + f n), iou = t p/(t p + f p + f
n). in addition, common object-level indicators such as dice coefficient and
aggregated jaccard index (aji) [13] are also considered to assess the
segmentation performance.in the experiment on bcdata, precision (p), recall (r),
and f1-score are used to evaluate the detection performance. predicted points
will be matched to ground-truth points one by one. and those unmatched points
are regarded as false positives. precision and recall are: p = t p/(t p + f p ),
and r = t p/(t p + f n). in addition, we introduce mp and mn to evaluate the
cell counting results. 'mp' and 'mn' denote the mean average error of positive
and negative cell numbers.hyperparameters. res2net101 [7] is adopted as the
activation network u ss with random initialization of parameters. the positive
sample is augmented by rotation. the weights β are set to 2.5 and 4 for training
in monuseg and bcdata, respectively. the weight λ is 0.5. the analysis for β and
λ is included in the supplementary. pixels of the fused semantic map will be
decoupled into three piles by k-means. the following segmentation and detection
are constructed with resnet-34. they are optimized using crossentropy loss by
the adam optimizer for 100 epochs with the initial learning rate of 1e -4 . the
function dif f (•) is instantiated as the measurement of manhattan distance.",8
Brain Anatomy-Guided MRI Analysis for Assessing Clinical Progression of Cognitive Impairment with Structural MRI,1.0,Introduction,"brain magnetic resonance imaging (mri) has been increasingly used to assess
future progression of cognitive impairment (ci) in various clinical and research
fields by providing structural brain anatomy [1][2][3][4][5][6]. many
learning-based methods have been developed for automated mri analysis and brain
disorder prognosis, which usually heavily rely on labeled training data
[7][8][9][10]. however, it is generally time-consuming and tedious to collect
category labels for brain mris in practice, resulting in a limited number of
labeled mris [11].fig. 1. illustration of brain anatomy-guided representation
(bar) learning framework for assessing the clinical progression of cognitive
impairment. the bar consists of a pretext model and a downstream model, with a
shared brain anatomy-guided encoder for mri feature extraction. the pretext
model also contains a decoder for brain tissue segmentation, while the
downstream model relies on a predictor for prediction. the pretext model is
trained on the large-scale adni [12] with 9,544 t1-weighted mris, yielding a
generalizable encoder. with this learned encoder frozen, the downstream model is
then fine-tuned on target mris for prediction tasks.even without task-specific
category label information, brain anatomical structures provided by auxiliary
mris can be employed as a prior to boost disease progression prediction
performance. considering that there are a large number of unlabeled mris in
existing large-scale datasets [12,13], several deep learning methods propose to
extract brain anatomical features from mri without requiring specific category
labels. for instance, song et al. [14] suggest that the anatomy prior can be
utilized to segment brain tumors, while yamanakkanavar et al. [15] discuss how
brain mri segmentation improves disease diagnosis. unfortunately, there are few
studies that try to utilize such brain anatomy prior for assessing the clinical
progression of cognitive impairment with structural mris.to this end, we propose
a brain anatomy-guided representation (bar) learning framework for cognitive
impairment prognosis with t1-weighted mris, incorporated with brain anatomy
prior provided by a brain tissue segmentation task. as shown in fig. 1, the bar
consists of a pretext model and a downstream model, with a shared anatomy-guided
encoder for mri feature extraction. these two models also use a decoder and a
predictor for brain tissue segmentation and disease progression prediction,
respectively. the pretext model is trained on 9,544 mri scans from the public
alzheimer's disease neuroimaging initiative (adni) [12] without any category
label information, yielding a generalizable encoder. the downstream model is
further fine-tuned on target mris for ci progression prediction. experiments are
performed on two ci-related studies with 391 subjects, with results suggesting
the efficacy of bar compared with state-ofthe-art (sota) methods. the pretext
model can also be used for brain tissue segmentation in other mri-based studies.
to the best of our knowledge, this is the first work that utilizes anatomy prior
derived from large-scale t1-weighted mris for automated cognitive decline
analysis. to promote reproducible research, the source code and trained models
have been made publicly available to the research community (see
https://github.com/goodaycoder/bar).",8
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,1.0,Introduction,"diffusion-weighted mri enables visualization of brain white matter structures.
it can be used to generate tractography data consisting of millions of synthetic
fibers or streamlines for a single subject stored in a tractogram that
approximate groups of biological axons [1]. many applications require
streamlines to be segmented into individual tracts corresponding to known
anatomy. tract segmentations are used for a variety of tasks, including surgery
planning or tract-specific analysis of psychiatric and neurodegenerative
diseases [2,11,12,17].automated methods built on supervised machine learning
algorithms have attained the current state-of-the-art in segmenting tracts
[3,18,21]. those are trained using various features, either directly from
diffusion data in voxel space or from tractography data. models may output
binary masks containing the target white matter tract, or perform a
classification on streamline level. however, such algorithms are commonly
trained on healthy subjects and have shown issues in processing cases with
anatomical abnormalities, e.g. brain tumors [20]. consequently, they are
unsuitable for tasks such as preoperative planning of neurosurgical patients, as
they may produce incomplete or false segmentations, which could have harmful
consequences during surgery [19]. additionally, supervised techniques are
restricted to fixed sets of predetermined tracts and are trained on substantial
volumes of hard-to-generate pre-annotated reference data.manual methods are
still frequently used for all cases not yet covered by automatic methods, such
as certain populations like children, animal species, new acquisition schemes or
special tracts of interests. experts determine regions of interest (roi) in
areas where a particular tract is supposed to traverse or through which it must
not pass, and segmentations can be accomplished either (1) by virtually
excluding and maintaining streamlines from tractography according to the defined
roi or (2) by using these regions for tract-specific roi-based tractography.
both approaches require a similar effort, although the latter is more commonly
used. the correct definition of rois can be time-consuming and challenging,
especially for inexperienced users. despite these limitations, roi-based
techniques are currently without vivid alternatives for segmenting tracts that
automated methods cannot handle.methods to simplify tract segmentation have been
proposed before. clustering approaches were developed to reduce complexity of
large amounts of streamlines in the input data [4,6]. tractome is a tool that
allows interactive segmentation of such clusters by representing them as single
streamlines that can interactively be included or excluded from the target tract
[14]. although the approach has shown promise, it has not yet supplanted
conventional roibased techniques.we propose a novel semi-automated tract
segmentation method for efficient and intuitive identification of arbitrary
white matter tracts. the method employs entropy-based active learning of a
random forest classifier trained on features of the dissimilarity representation
[13]. active learning has been utilized for several cases in the medical domain,
while it has never been applied in the context of tract segmentation [7,9,16].
it reduces manual efforts by iteratively identifying the most informative or
ambiguous samples, here, streamlines, during classifier training, to be
annotated by a human expert. the method is implemented as the tool attractive in
mitk diffusion1 , enabling researchers to quickly and intuitively segment tracts
in pathological datasets or other situations not covered by automatic
techniques, simply by annotating a few but informative streamlines.",8
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.1,Data,"the proposed technique was tested on a healthy-subject dataset and on a dataset
containing tumor cases. the first comprises 21 subjects of the human connectome
project (hcp) that were used for testing the automated methods tractseg and
classifyber [3,18]. visual examinations revealed false-negatives in the
reference tracts, meaning that some streamlines that belong to the target tract
were not included in the reference. these false-negatives did not affect the
generation of accurate segmentation masks, since most false-negatives are
occupied by true-positive streamlines, but negatively influenced our
experiments. to reduce false-negatives, the reference segmentation mask as well
as start-and end-region segmentations were used to reassign streamlines from the
tractogram using two criteria: streamlines must be inside the binary reference
segmentation (1) and start and end in the assigned regions (2). as the initial
size of ten million streamlines is computationally challenging and unsuitable
for most tools, all tractograms were randomly down-sampled to one million
streamlines. we focused on the left optic radiation (or), the left
cortico-spinal tract (cst), and the left arcuate-fasciculus (af), representing a
variety of established tracts.to test the proposed method on pathological data,
we used an in-house dataset containing ten presurgical scans of patients with
brain tumors. tractography was performed using probabilistic streamline
tractography in mitk diffusion. to reduce computational costs, we retained one
million streamlines that passed through a manually inserted roi located in an
area traversed by the or [15]. subjects have tumor appearance with varying sizes
((17.87±12.73 cm 3 )) in temporoloccipital, temporal, and occipital regions,
that cause deformations around the or and lead to deviations of the tract from
the normative model.",8
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.2,Experimental Setup,"to evaluate the proposed method, we conducted two types of experiments. manual
segmentation experiments using an interactive prototype of attractive were
initiated on the tumor data (holistic evaluation). additionally, reproducible
simulations on the freely available hcp and the internal tumor dataset were
created (algorithmic evaluation). in order to mimic expert annotation during
algorithmic evaluation, class labels were assigned to streamlines using
previously generated references. the quality of the predictions was measured by
calculating the dice score of the binary mask. the code used for these
experiments is publicly available2 .for the algorithmic evaluation, the initial
training dataset was created with 20 randomly selected streamlines from the
whole-brain tractogram, which have been shown to be a decent number to start
training. since some tracts contain only a fraction of streamlines from the
entire tractogram, it might be unlikely that the training dataset will contain
any streamline belonging to the target tract. therefore, two streamlines of the
specific tract were further added to the training dataset, and class weights
were used to compensate for the class unbalance. according to fig. 2, the
dissimilarity representation was determined, the random forest classifier was
trained and the converged model was used to predict on the unlabeled streamlines
and to calculate the entropy. in each iteration, the ten streamlines with the
highest entropy are added to the training dataset, which has been determined to
be a good trade-off between annotation effort and prediction improvement. the
process was terminated after 20 iterations, increasing the size of the training
data from 22 to 222 out of one million streamlines.the holistic evaluation was
conducted with equal settings, except that the workflow was terminated when the
prediction matched the expectation of the expert. to ensure that the initial
dataset s rand contained streamlines from the target tract, the expert initiated
the active learning workflow by defining a small roi that included fibers of the
tract. s rand was created by randomly sampling only those streamlines that pass
through this roi. to allow comparison between the proposed and traditional
roi-based techniques, the or of subjects from the tumor dataset were segmented
using both approaches by an expert familiar with the respective tool, and the
time required was reported to measure efficiency.note, in all experiments, the
classifier is trained from scratch every iteration, prototypes are generated for
each subject individually, and the classifier predicts on data from the same
subject it is trained with, as it performs subject-individual tract segmentation
and is not used as a fully automated method. to ensure a stable active learning
setup that generalizes across different datasets, the whole method was developed
on the hcp and applied with fixed settings to the tumor data [10].",8
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,3.3,Results,"in table 1, the dice score of the active learning simulation on the hcp and
tumor data after the fifth, tenth, and twentieth iterations are shown and
compared with outcomes of classifyber and tractseg. results for the hcp data
were already on par with the benchmark of automatic methods between the fifth
and tenth iterations. on the tumor data, the performance of the proposed method
remains above 0.7 while the performance of tractseg drops substantially.
furthermore, classifyber does not support the or and is therefore not listed in
table 1. figure 3 depicts the quantitative gain of active learning on the three
tracts of the hcp data and compares it to pure random sampling by displaying the
dice score depending on annotated streamlines. while active learning leads to an
increase in the metric until the predictions at around five to ten iterations
show no meaningful improvements, the random selection does not improve overall.
qualitative results of the algorithmic evaluation of the af of a randomly chosen
subject of the hcp dataset are shown in fig. 4(a). initially, the randomly
sampled streamlines in the training data are distributed throughout the brain,
while entropy-based selected streamlines from subsequent iterations cluster
around the af. the prediction improves iteratively, as indicated by a rising
dice score. when accessing qualitative results of the pathological dataset
visual inspection revealed particularly poorly performance of tractseg in cases
where or fibers were in close proximity to tumor tissue, leading to fragmented
segmentations, while complete segmentations were reached with active learning
even for these challenging tracts after a few iterations, as shown in fig.
4(b).the initial manual experiments with attractive were consistent with the
simulations. the prediction aligned with the expectations of the expert at
around five to seven iterations taking a mean of 4,5 min, while it took seven
minutes on average to delineate the tract with roi-based segmentation. during
the iterations, streamlines around the target tract were suggested for labeling,
and the prediction improved. visual comparison yielded more false-positive
streamlines with the roi-based approach while attractive created more compact
tracts.",8
atTRACTive: Semi-automatic White Matter Tract Segmentation Using Active Learning,4.0,Discussion,"active learning-based white matter tract segmentation enables the identification
of arbitrary pathways and can be applied to cases where fully automated methods
are unfeasible. in this work, algorithmic evaluation as well as the
implementation of the technique into the gui-based tool attractive including
further holistic manual experiments were conducted. the algorithmic evaluation
yielded consistent results from the fifth to the tenth iterations on both the
hcp and tumor datasets. as expected, outcomes obtained from the tumor dataset
were not quite as good as those of the hcp dataset. this trend is generally
observed in clinical datasets, which tend to exhibit lower performance levels
compared to high-quality datasets, which could be responsible for the decline in
the results. preliminary manual experiments with attractive indicated active
learning to have shorter segmentation times compared to traditional roi-based
techniques. these experiments are in line with the simulations as the generated
tracts matched the expectations of the expert after around five to seven
iterations, meaning that less than a hundred out of million annotated
streamlines are required to train the model. enhancements to the usability of
the prototype are expected to further improve efficiency. a current limitation
of attractive is the selection of the initial subset, based on randomly sampling
streamlines passing through a manually inserted roi. this approach does not
guarantee that streamlines of the target tract are included in the subset. in
that case, the roi has to be replaced or s rand needs to be regenerated.future
analyses, evaluating the inter-and intra-rater variability compared to other
interactive approaches, will be conducted on further tracts. for selected
scenarios, the ability of the classifier to generalize by learning from
previously annotated subjects will be investigated, which may even allow to
train a fully automatic classifier for new tracts once enough data is annotated.
to further optimize the method, the feature representation or sampling procedure
could be improved. uncertainty sampling may select redundant streamlines due to
similar high entropy values. instead, annotating samples with high entropy
values being highly diverse or correcting false classifications could convey
more information.by introducing active learning into tract segmentation, we
provide an efficient and intuitive alternative compared to traditional roi-based
approaches.attractive has the potential to interactively assist researchers in
identifying arbitrary white matter tracts not captured by existing automated
approaches.",8
B-Cos Aligned Transformers Learn Human-Interpretable Features,1.0,Introduction,"making artificial neural networks more interpretable, transparent, and
trustworthy remains one of the biggest challenges in deep learning. they are
often still considered black boxes, limiting their application in
safety-critical domains such as healthcare. histopathology is a prime example of
this. for years, the number of pathologists has been decreasing while their
workload has been increasing [23]. consequently, the need for explainable
computer-aided diagnostic tools has become more urgent.as a result, research in
explainable artificial intelligence is thriving [20]. much of it focuses on
convolutional neural networks (cnns) [13]. however, with the rise of
transformers [31] in computational pathology, and their increasing application
to cancer classification, segmentation, survival prediction, and mutation
detection tasks [26,32,33], the old tools need to be reconsidered. visualizing
filter maps does not work for transformers, and grad-cam [30] has known
limitations for both cnns and transformers.the usual way to interpret
transformer-based models is to plot their multihead self-attention scores [8].
but these often lead to fragmented and unsatisfactory explanations [10]. in
addition, there is an ongoing controversy about their trustworthiness [5]. to
address these issues, we propose a novel family of transformer architectures
based on the b-cos transform originally developed for cnns [7]. by aligning the
inputs and weights during training, the models are implicitly forced to learn
more biomedically relevant and meaningful features (fig. 1). overall, our
contributions are as follows:• we propose the b-cos vision transformer (bvt) as
a more explainable alternative to the vision transformer (vit) [12]. • we
extensively evaluate both models on three public datasets: nct-crc-he-100k [18],
tcga-coad-20x [19], munich-aml-morphology [25]. • we apply various post-hoc
visualization techniques and conduct a blind study with domain experts to assess
model interpretability. • we derive the b-cos swin transformer (bwin) based on
the swin transformer [21] (swin) in a generalization study.",8
B-Cos Aligned Transformers Learn Human-Interpretable Features,4.0,Implementation and Evaluation Details,"task-based evaluation: cancer classification and segmentation is an important
first step for many downstream tasks such as grading or staging. therefore, we
choose this problem as our target. we classify image patches from the public
colorectal cancer dataset nct-crc-he-100k [18]. we then apply our method to
tcga-coad-20x [19], which consists of 38 annotated slides from the tcga
colorectal cancer cohort, to evaluate the effectiveness of transfer learning.
this dataset is highly unbalanced and not color normalized compared fig. 5. in a
blinded study, domain experts ranked models (lower is better) based on whether
the models focus on biomedically relevant features that are known in the
literature to be important for diagnosis. we then performed the conover post-hoc
test after friedman with adjusted p-values according to the two-stage
benjamini-hochberg procedure. bvt ranks above vit with p < 0.1 (underlined) and
p < 0.05 (bold). to the first dataset. additionally, we demonstrate that the
b-cos vision transformer is adaptable to domains beyond histopathology by
training the model on the single white blood cell dataset munich-aml-morphology
[25], which is also highly unbalanced and also publicly available.domain-expert
evaluation: our primary objective is to develop an extension of the vision
transformer that is more transparent and trusted by medical professionals. to
assess this, we propose a blinded study with four steps: (i) randomly selecting
images from the test set of tcga-coad-20x (32 samples) and munich-aml-morphology
(56 samples), (ii) plotting the last-layer attention and transformer
attributions for each image, (iii) anonymizing and randomly shuffling the
outputs, (iv) submitting them to two domain experts in histology and cytology
for evaluation. most importantly, we show them all the available saliency maps
without pre-selecting them to get their unbiased opinion.implementation details:
in our experiments, we compare different variants of the b-cos vision
transformer and the vision transformer. specifically, we implement two versions
of vit: vit-t/8 and vit-s/8. they only differ in parameter size (5m for t models
and 22m for s models) and use the same patch size of 8. all bvt models (bvt-t/8
and bvt-s/8) are derivatives of the corresponding vit models. the b-cos
transform used in the bvt models has an exponent of b = 2. we use adamw with a
cosine learning rate scheduler for optimization and a separate validation set
for hyperparameter selection. following the findings of [7], we add [1 -r, 1 -g,
1 -b] to the rgb channels [r, g, b] of bvt. this allows us to encode each pixel
with the direction of the color channel vector, forcing the model to capture
more color information. furthermore, we train models with two different loss
functions: the standard categorical cross-entropy loss (cce) and the binary
cross-entropy loss (bce) with one-hot encoded entries. it was suggested in [7]
that bce is a more appropriate loss for b-cos cnns. we explore whether this is
also true for transformers in our experiments. additional details on training,
optimization, and datasets can be found in the appendix.",8
B-Cos Aligned Transformers Learn Human-Interpretable Features,0.0,Domain-Expert Evaluation:,"the results show that bvts are significantly more trustworthy than vits (p <
0.05). this indicates that bvt consistently attends to biomedically relevant
features such as cancer cells, nuclei, cytoplasm, or membrane [24] (fig. 5). in
many visualization techniques, we see that bvt, unlike vit, focuses exclusively
on these structures (fig. 3). in contrast, vit attributes high attention to
seemingly irrelevant features, such as the edges of the cells. a third expert
points out that vit might overfit certain patterns in this dataset, which could
aid the model in improving its performance.",8
Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,3.0,Experiments and Results,"datasets. to enable fair evaluation between our predictions and the reference hr
ground truths, the in-plane snr between the lr input scan and corresponding
ground truth has to match. to synthetically create 2d lr images, it is necessary
to downsample out-of-plane in the image domain anisotropically [32] while
preserving in-plane resolution. consequently, to mimic realistic 2d clinical
protocol, which often has higher in-plane details than that of 3d scans, we use
spline interpolation to model partial volume and downsampling. we demonstrate
our network's modeling capabilities for different contrasts (t1w, t2w, flair,
dir), views (axial, coronal, sagittal), and pathologies (ms, brain tumor). we
conduct experiments on two public datasets, brats [16], and msseg [4], and an
in-house clinical ms dataset (cms). in each dataset, we select 25 patients that
fulfill the isotropic acquisition criteria for both ground truth hr scans. note
that we only use the ground truth hr for evaluation, not anywhere in training.
we optimize separate inrs for each subject with supervision from only its two lr
scans. if required, we employ skull-stripping [12] and rigid registration to the
mni152 (msseg, cms) or sri24 (brats) templates. for details, we refer to table 2
in the supplementary.metrics. we evaluate our results by employing common sr
[5,14,29] quality metrics, namely psnr and ssim. to showcase perceptual image
quality, we additionally compute the learned perceptual image patch similarity
(lpips) [31] and measure the absolute error mi in mutual information of two
upsampled images to their ground truth counterparts as follows:baselines and
ablation. to the best of our knowledge, there are no prior data-driven methods
that can perform mcsr on a single-subject basis. hence, we provide
single-subject baselines that operate solely on single contrast and demonstrate
the benefit of information transfer from other contrasts with our proposed
models. quantitative analysis. table 1 demonstrates that our proposed framework
poses a trustworthy candidate for the task of mcsr. as observed in [32], lrtv
struggles for anisotropic up-sampling while smore's overall performance is
better than cubic-spline, but slightly worse to single-contrast inr. however,
the benefit of single-contrast inr may be limited if not complemented by
additional views as in [29]. for mcsr from single-subject scans, we achieve
encouraging results across all metrics for all datasets, contrasts, and views.
since t1w and t2w both encode anatomical structures, the consistent improvement
in brats for both sequences serves as a proof-of-concept for our approach. as
flair is the go-to-sequence for ms lesions, and t1w does not encode such
information, the results are in line with the expectation that there could be a
relatively higher transfer of anatomical information to pathologically more
relevant flair than vice-versa. lastly, given their similar physical acquisition
and lesion sensitivity, we note that dir/flair benefit to the same degree in the
cms dataset.qualitative analysis. figure 2 shows the typical behavior of our
models on cms dataset, where one can qualitatively observe that the split-head
inr pre-serves the lesions and anatomical structures shown in the yellow boxes,
which other models fail to capture. while our reconstruction is not identical to
the gt hr, the coronal view confirms anatomically faithful reconstructions
despite not receiving any in-plane supervision from any contrast during
training. we refer to fig. 4 in the supplementary for similar observations on
brats and msseg.",8
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,1.0,Introduction,"glioblastomas (gbms, known as grade iv gliomas) are the most common primary
malignant brain tumors with high spatial heterogeneity and varying degrees of
aggressiveness [22]. patients with gbm generally have a very poor survival rate;
the median overall survival time is about 14 months [17]; and the overall
survival time is affected by many factors, including patient characteristics
(e.g., age and physical status), tissue histopathology (e.g., cellular density
and nuclear atypia), and molecular pathology (e.g., mutations and gene
expression levels) [1,14,15]. although these factors, particularly molecular
information, have usually proved to be strong predictors of survival in gbm,
there remain substantial challenges and unmet clinical needs to exploit easily
accessible, noninvasive neuroimaging data acquired preoperatively to predict
overall survival time of gbm patients, which can benefit treatment planning.to
do so, magnetic resonance imaging (mri) and its derived radiomics have been
widely used to study gbm preoperative prognosis over the last few decades. for
example, anand et al. [2] first applied a forest of trees to assign an
importance value to each of the 1022 radiomic features extracted from t1 mri,
and then the 32 most important features were fed to the random forest regressor
for predicting overall survival time of a gbm patient. based on patches from
multi-modal mri images, nie et al. [19] trained a 3d convolutional neural
network (cnn) to learn the high-level semantic features, which were eventually
input to a support vector machine (svm) for classifying long-and short-term gbm
survivors. in addition, an integrated model by fusing radiomics features,
mri-based cnn features, and clinical features, was presented for gbm survival
group classification, resulting in better performance than using any single type
of features [12].although both mri and its derived radiomics features have been
demonstrated to have predictive power for survival analysis in the
aforementioned literature, they do not account for brain's functional
alternations caused by tumors, which are clinically significant as
biologically-interpretable biomarkers of recovery and therapy. these
alternations can be reflected by changes in resting-state functional mri
(fmri)-derived functional connectivities/connections (fcs) between the blood
oxygenation level-dependence (bold) time series of paired brain regions.
therefore, the use of fcs to predict overall survival time for gbm has recently
attracted increasing attention [7,16,24], and more importantly, survival-related
fc patterns or brain regions were found to guide therapeutic solutions aimed at
inhibiting tumor-brain communication.nevertheless, current fc-based survival
prediction still suffers from two main deficiencies when applied to gbm
prognosis. first, due to mass effect and physical infiltration of gbm in the
brain, fcs estimated directly from gbm patients' resting-state fmri might be
inaccurate, especially when the tumors are near or in the regions of interest.
second, resting-state fmri data are not routinely collected for gbm clinical
practices, which restricts the size of annotated datasets such that it is
infeasible to train a reliable prediction model based on deep learning for
survival prediction. in order to circumvent these issues, in this paper we
introduce a novel neuroimaging feature family, namely functional lesion network
(fln) maps that are generated by our augmented lesion network mapping (a-lnm),
for overall survival time prediction of gbm patients. our a-lnm is motivated by
lesion network mapping (lnm) [8] which can localize neurological deficits to
functional brain networks and identify regions relate to a clinical syndrome. by
embedding the lesion into a normative functional connectome and computing
functional connectivity between the lesion and the rest of the brain using fmri
of all healthy subjects in the normative cohort, lnm has been successfully
employed to the identification of the brain network underlying particular
symptoms or behavioral deficits in stoke [4,13].the details of our workflow are
described as follows.1) we first manually segment the whole tumor (regarded as
lesion in this paper) on structural mri for all gbm patients, and the resulting
lesion masks are mapped onto a reference brain template, e.g., the mni152 2mm 3
template.2) the proposed a-lnm is next used to generate fln maps for each gbm
patient by using resting-state fmri from a large cohort of healthy subjects.
specifically, for each patient, we correlate the mean bold time series of all
voxels within the lesion with the bold time series of every voxel in the whole
brain for all n subjects in the normative cohort, producing n functional
disconnection (fdc) maps of voxel-wise correlation values (transformed to
zscores). these resulting n fdc maps are partitioned into m disjoint subsets of
equal size, and m fln maps are separately obtained by averaging the fdc maps in
each of the m subsets. similar to data augmentation schemes, we can artificially
boost data volume (i.e., fln maps) up to m times through producing m fln maps
for each patient in the a-lnm, which helps to mitigate the risk of over-fitting
and improve the performance of overall survival time prediction when learning a
deep neural network from a small sized dataset.for this reason, we propose the
name ""augmented lnm (a-lnm)"", compared to the traditional lnm where only one fln
map is generated per patient by averaging all the n fdc maps. 3) finally, these
augmented fln maps are fed to a 3d resnet-based backbone network followed by the
average pooling operation and fully-connected layers for gbm survival
prediction.to our knowledge, this paper is the first to demonstrate a successful
extension of lnm for survival prediction in gbm. to evaluate the predictive
power of the fln maps generated by our a-lnm, we conduct extensive experiments
on 235 gbm patients in the training dataset of brats 2020 [18] to classify the
patients into three overall survival time groups viz. long, mid, and short.
experimental results show that our a-lnm based survival prediction framework
outperforms previous state-of-the-art methods. in addition, an explainable
analysis driven by the gradient-weighted class activation mapping (grad-cam)
[10] for survivalrelated brain regions is fulfilled.",8
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2.0,Materials and Methods,"2.1 materials gsp1000 processed connectome. it publicly released preprocessed
restingstate fmri data of 1000 healthy right-handed subjects with an average age
21.5 ± 2.9 years and approximately equal numbers of males and females from the
brain genomics superstruct project (gsp) [5], where the concrete image
acquisition parameters and preprocessing procedures can be found as well.
specifically, a slightly modified version of yeo's computational brain imaging
group (cbig) fmri preprocessing pipeline (https://github.com/bchcohenlab/cbig)
was employed to obtain either one or two preprocessed resting-state fmri runs of
each subject that had 120 time points per run and were spatially normalized into
the mni152 template with 2mm 3 voxel size. we downloaded and used the first-run
preprocessed resting-state fmri of each subject for the following analysis.brats
2020. it provided an open-access pre-operative imaging training dataset to
segment brain tumors of glioblastoma (gbm, belonging to high grade glioma) and
low grade glioma (lgg) patients, as well as to predict overall survival time of
gbm patients [18]. this training dataset contained 133 lgg and 236 gbm patients,
and each patient had four mri modalities, including t1, post-contrast
t1-weighted, t2-weighted, and t2 fluid attenuated inversion recovery. manual
expert segmentation delineated three tumor sub-regions, i.e., the gd-enhancing
tumor, the peritumoral edema, and the necrotic and non-enhancing tumor core.the
union of all the three tumor sub-regions was considered as the whole tumor,
which is regarded as the lesion in this paper.",8
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,2.2,Methods,"in this paper, we propose to investigate the feasibility of the novel
neuroimaging features, i.e., fln maps, for overall survival time prediction of
gbm patients in the training dataset of the brats 2020, in which one patient
alive was excluded, and the remaining 235 patients consisted of 89 short-term
survivors (less than 10 months), 59 mid-term survivors (between 10 and 15
months), and 87 long-term survivors (more than 15 months). to this end, our
framework for the three-class survival classification is shown in fig. 1, and
the details are described as follows.lesion mapping procedures. as stated above,
the whole tumor is referred to as a lesion for each gbm patient. from the manual
expert segmentation labels of lesions in the 235 gbm patients of the brats 2020,
we co-register the lesion masks to the mni152 2mm 3 template by employing a
symmetric normalization algorithm in antspy [3].",8
Overall Survival Time Prediction of Glioblastoma on Preoperative MRI Using Lesion Network Mapping,3.3,Brain Regions in Relation to GBM Survival,"to identify the most discriminative brain regions associated with overall
survival time in gbm, we estimated the relative contribution of each voxel to
the classification performance in our proposed method by using the grad-cam
[10]. to obtain steady results, as shown in fig. 2(a), the voxels with top 5%
weights in the class activation maps (cams) of all candidate models were
overlapped by class, and the position covered by more than half of the models is
displayed. the cams of three classes of survivors overlapped in fig. 2(b) where
both coincident and non-coincident areas exist.the association of an increased
degree of invasion within the frontal lobe with decreased survival time can be
observed, which is in concordance with a previous study [20]. patients whose
frontal lobe is affected by tumors showed more executive dysfunction, apathy,
and disinhibition [11]. on the dominant left hemisphere, the cams of long-term
survivors and mid-term survivors overlapped at the superior temporal gyrus and
wernicke's area which are involved in the sensation of sound and language
comprehension respectively, and have been associated with decreased survival in
patients with high-grade glioma [26]. in addition, the cam of mid-term survivors
covered more areas of the middle and inferior temporal gyri which were
considered as one of the higher level ventral streams of visual processing
linked to facial recognition [25].",8
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,1.0,Introduction,"a difficulty faced by surgeons performing endoscopic pituitary surgery is
identifying the areas of the bone which are safe to open. this is of particular
importance during the sellar phase as there are several critical anatomical
structures within close proximity of each other [9]. the sella, behind which the
pituitary tumour is located, is safe to open. however, the smaller structures
surrounding the sella, behind which the optic nerves and internal carotid
arteries are located, carry greater risk. failure to appreciate these critical
parasellar neurovascular structures can lead to their injury, and adverse
outcomes for the patient [9,11]. the human identification of these structures
relies on visual clues, inferred from the impressions made on the bone, rather
than direct visualisations of the structures [11]. this is especially
challenging as the pituitary tumour often compresses; distorts; or encases the
surrounding structures [11]. neurosurgeons utilise identification instruments,
such as a stealth pointer or micro-doppler, to aid in this task [9]. however,
once an identification instrument is removed, identification is lost upon
re-entry with a different instrument, and so the identification can only be used
in referenced to the more visible anatomical landmarks. automatic identification
from endoscopic vision may therefore aid surgeons in this effort while
minimising disruption to the surgical workflow [11]. this is a challenging
computer vision task due to the narrow camera angles enforced by minimally
invasive surgery, which lead to: (i) structure occlusions by instruments and
biological factors (e.g., blood); and (ii) image blurring caused by rapid camera
movements. additionally, in this specific task there are: (iii) numerous small
structures; (iv) visually similar structures; and (v) unclear structure
boundaries. hence, the task can be split into two sub-tasks to account for these
difficulties in identification: (1) the semantic segmentation of the two larger,
visually distinct, and frequently occurring structures (sella and clival
recess); and (2) the centroid detection of the eight smaller structures (fig.
1).to solve both tasks simultaneously, painet (pituitary anatomy identification
network) is proposed. this paper's contribution is therefore:1. the automated
identification of the ten critical anatomical structures in the sellar phase of
endoscopic pituitary surgery. to the best of the authors' knowledge, this is the
first work addressing the problem at this granularity. 2. the creation of
painet, a multi-task neural network capable of simultaneously semantic
segmentation and centroid detection of numerous anatomical structures within
minimally invasive surgery. painet uniquely utilises two loss functions for
improved performance over single-task neural networks due to the increased
information gain from the complementary task.",9
A Multi-task Network for Anatomy Identification in Endoscopic Pituitary Surgery,3.0,Methods,"painet: a multi-task encoder-decoder network is proposed to improve performance
by exchanging information between the semantic segmentation and centroid
detection tasks. efficientnetb3, pre-trained on imagenet, is used as the encoder
because of its accuracy, computational efficiency and proven generalisation
capabilities [13]. the decoder is based on u-net++, a state-of-the-art
segmentation network widely used in medical applications [16]. the
encoderdecoder architecture is modified to output both segmentation and centroid
predictions by sending the decoder output into two separate layers: (1) a
convolution for segmentation prediction; and (2) an average pooling layer for
centroid prediction. different loss functions were minimised for each sub-task
(fig. 2). ablation studies and granular details are provided below. the priority
was to find the optimal sella segmentation model, as it is required to be opened
to access the pituitary tumour behind it, indicating the surgical
""safe-zone"".semantic segmentation: first, single-class sella segmentation models
were trialed. 8-encoders (pre-trained convolution neural networks) and
15-decoders were used, with their selection based off architecture variety. two
loss functions were also used: (1) distribution-based logits cross-entropy; and
(2) region-based jaccard loss. boundary-based loss functions were not trialed
as: (1) the boundary of the segmentation masks are not well-defined; and (2) in
the cases of split structures (fig. 1c), boundary-based loss functions are not
appropriate [8]. the decoder output is passed through a convolution layer and
sigmoid activation.for multi-class sella and clival recess segmentation, the
optimal single-class model was extended by: (1) sending through each class to
the loss function separately (multi-class separate); and (2) sending both
classes through together (multi-class together). an extension of logits
cross-entropy, logits focal loss, was used instead as it accounts for data
imbalance between classes.centroid detection: 5-models were trialed: 3-models
consisted of encoders with a convolution layer and linear activation; and
2-models consisted of encoderdecoders with an average pooling layer and sigmoid
activation with 0.3 dropout. two distance-based loss functions were trialed: (1)
mean squared error (mse); and (2) mean absolute error (mae). loss was calculated
for all structures simultaneously as a 16 dimensional output (8 centroids × 2
coordinates) and set to 0 for a structure if ground-truth centroids of that
structure was not present.",9
Intraoperative CT Augmentation for Needle-Based Liver Interventions,1.0,Introduction,"needle-based liver tumor ablation techniques (e.g., radiofrequency, microwave,
laser, cryoablation) have a great potential for local curative tumor control
[1], with comparable results to surgery in the early stages for both primary and
secondary cancers. furthermore, as it is minimally invasive, it has a low rate
of major complications and procedure-specific mortality, and is tissue-sparing,
thus, its indications are growing exponentially and extending the limits to more
advanced tumors [3]. ct-guidance is a widely used imaging modality for placing
the needles, monitoring the treatment, and following up patients. however, it is
limited by the exposure to ionizing radiation and the need for intravenous
injection of contrast agents to visualize the intrahepatic vessels and the
target tumor(s).in standard clinical settings, the insertion of each needle
requires multiple check points during its progression, fine-tune maneuvers, and
eventual repositioning. this leads to multiple ct acquisitions to control the
progression of the needle with respect to the vessels, the target, and other
sensible structures [26]. however, intrahepatic vessels (and some tumors) are
only visible after contrast-enhancement, which has a short lifespan and
dose-related deleterious kidney effects. it makes it impossible to perform each
of the control ct acquisitions under contrast injection. a workaround to
shortcut these limitations is to perform an image fusion between previous
contrasted and intraoperative noncontrasted images. however, such a solution is
only available in a limited number of clinical settings, and the registration is
only rigid, usually deriving into bad results. in this work, we propose a method
for visualizing intrahepatic structures after organ motion and needle-induced
deformations, in non-injected images, by exploiting image features that are
generally not perceivable by the human eye in common clinical workflows.to
address this challenge, two main strategies could be considered: image fusion
and image processing techniques. image fusion typically relies on the estimation
of rigid or non-rigid transformations between 2 images, to bring into the
intraoperative image structures of interest only visible in the preoperative
data. this process is often described as an optimization problem [9,10] which
can be computationally expensive when dealing with non-linear deformations,
making their use in a clinical workflow limited. recent deep learning approaches
[11,12,14] have proved to be a successful alternative to solve image fusion
problems, even when a large non-linear mapping is required. when ground-truth
displacement fields are not known, state-of-the-art methods use unsupervised
techniques, usually an encoder-decoder architecture [7,13], to learn the unknown
displacement field between the 2 images. however, such unsupervised methods fail
at solving our problem due to lack of similar image features between the
contrasted (cct) and non-contrasted (ncct) image in the vascular tree region
(see sect. 3.3).on the other hand, deep learning techniques have proven to be
very efficient at solving image processing challenges [15]. for instance, image
segmentation [16], image style transfer [17], or contrast-enhancement to cite a
few. yet, segmenting vessels from non-contrasted images remains a challenge for
the medical imaging community [16]. style transfer aims to transfer the style of
one image to another while preserving its content [17][18][19]. however,
applying such methods to generate a contrasted intraoperative ct is not a
sufficiently accurate solution for the problem that we address.
contrast-enhancement methods could be an alternative. in the method proposed by
seo et al. [20], a deep neural network synthesizes contrast-enhanced ct from non
contrast-enhanced ct. nevertheless, results obtained by this method are not
sufficiently robust and accurate to provide an augmented intraoperative ct on
which needle-based procedures can be guided.in this paper we propose an
alternative approach, where a neural network learns local image features in a
ncct image by leveraging the known preoperative vessel tree geometry and
topology extracted from a matching (undeformed) cct. then, the augmented ct is
generated by fusing the deformed vascular tree with the non-contrasted
intraoperative ct. section 2 presents the method and its integration in the
medical workflow. section 3 presents and discusses the results, and finally we
conclude in sect. 4 and highlight some perspectives.",9
Intraoperative CT Augmentation for Needle-Based Liver Interventions,2.0,Method,"in this section, we present our method and its compatibility with current
clinical workflows. a few days or a week before the intervention, a preoperative
diagnostic multiphase contrast-enhanced image (mpcect) is acquired (fig. 1,
yellow box). the day of the intervention, a second mpcect image is acquired
before starting the needle insertion, followed by a series of standard,
non-injected acquisitions to guide the needle insertion (fig. 1, blue box).
using such a noncontrasted intraoperative image as input, our method performs a
combined non-rigid registration and augmentation of the intraoperative ct by
adding anatomical features (mainly intrahepatic vessels and tumors) from the
preoperative image to the current image. to achieve this result, our method only
requires to process and train on the baseline mpcect image (fig. 1,red box). an
overview of the method is shown in the fig 2 and the following sections describe
its main steps.",9
Intraoperative CT Augmentation for Needle-Based Liver Interventions,4.0,Conclusion,"in this paper, we proposed a method for augmenting intra-operative ncct images
as a means to improve needle ct-guided techniques while reducing the need for
contrast agent injection during tumor ablation procedures, or other needle-based
procedures. our method uses a u-net architecture to learn local vessel tree
image features in the ncct by leveraging the known vessel tree geometry and
topology extracted from a matching cct image. the augmented ct is generated by
fusing the predicted vessel tree with the ncct. our method is validated on
several porcine images, achieving an average dice score of 0.81 on the predicted
vessel tree location. in addition, it demonstrates robustness even in the
presence of large deformations between the preoperative and intraoperative
images. our future steps will essentially involve applying this method to
patient data and perform a small user study to evaluate the usefulness and
limitations of our approach.aknowledgments. this work was partially supported by
french state funds managed by the anr under reference anr-10-iahu-02 (ihu
strasbourg). the authors would like to thank paul baksic and robin enjalbert for
proofreading the manuscript.",9
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,1.0,Introduction,"healthy and cancerous soft tissue display different elastic properties, e.g. for
breast [19], colorectal [7] and prostate cancer [4]. different imaging
modalities can be used to detect the biomechanical response to an external load
for the characterization of cancerous tissue, e.g., ultrasound, magnetic
resonance and optical coherence elastography (oce). the latter is based on
optical coherence tomography (oct), which provides excellent visualization of
microstructures and superior spatial and temporal resolution in comparison to
ultrasound or magnetic resonance elastography [8]. one common approach for
quantitative oce is to determine the elastic properties from the deformation of
the sample and the magnitude of a quasi-static, compressive load [10]. however,
due to the attenuation and scattering of the near-infrared light, imaging depth
is generally limited to approximately 1 mm in soft tissue. therefore, oce is
well suited for sampling surface tissue and commonly involves bench-top imaging
systems [26], e.g. in ophthalmology [21,22] or as an alternative to
histopathological slice examination [1,16]. handheld oce systems for
intraoperative assessment [2,23] have also been proposed. while conventional oce
probes have been demonstrated at the surface, regions of interest often lie deep
within the soft tissue, e.g., cancerous tissue in percutaneous biopsy.taking
prostate cancer as an example, biomechanical characterization could guide needle
placement for improved cancer detection rates while reducing complications
associated with increased core counts, e.g. pain and erectile dysfunction
[14,18]. however, the measurement of both the applied load and the local sample
compression is challenging. friction forces superimpose with tip forces as the
needle passes through tissue, e.g., the perineum. furthermore, the prostate is
known to display large bulk displacement caused by patient movement and needle
insertions [20,24] in addition to actual sample compression (fig. 1, left). tip
force sensing for estimating elastic properties has been proposed [5] but bulk
tissue displacement of deep tissue was not considered. in principle, compression
and tip force could be estimated by oct. yet, conventional oce probes typically
feature flat tip geometry [13,17].to perform oce in deep tissue structures, we
propose a novel bevel tip oce needle design for the biomechanical
characterization during needle insertions. we consider a dual-fiber setup with
temporal multiplexing for the combined load and compression sensing at the
needle tip. we design an experimental setup that can simulate friction forces
and bulk displacement occurring during needle biopsy (fig. 1). we consider
tissue-mimicking phantoms for surface and deep tissue indentation experiments
and compare our results with force-position curves externally measured at the
needle shaft. finally, we consider how the obtained elasticity estimates can be
used for the classification of both materials.",9
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,2.3,Experimental Setup,"we build an experimental setup for surface and deep tissue indentations with
simulated force and bulk displacement (fig. 1). for deep tissue indentations,
different tissue phantoms are stacked on a sample holder with springs in
between.for surface measurements, we position the tissue phantoms separately
without additional springs or tissue around the needle shaft. we use a motorized
linear stage (zfs25b, thorlabs gmbh, ger) to drive the needle while
simultaneously logging motor positions. an external force sensor (kd24s 20n,
me-meßsysteme gmbh, ger) measures combined axial forces. we consider two gelatin
gels as tissue mimicking materials for healthy and cancerous tissue. the two
materials (mat. a and mat. b) display a young's modulus of 53.4 kpa and 112.3
kpa, respectively. reference elasticity is determined by unconfined compression
experiments of three cylindrical samples for each material according to eq. 1,
using force and position sensor data (see supplementary material). the young's
modulus is obtained by linear regression for the combined measurements of each
material. we calibrate tip force estimation (fiber 2) by indentation of silicone
samples with higher tear resistance to ensure that no partial rupture has taken
place. we then determine a linear fit according to eq. 5 and obtain a f = 174.4
mn mm -1 from external force sensor and motor position measurements (see
supplementary material).",9
Optical Coherence Elastography Needle for Biomechanical Characterization of Deep Tissue,4.0,Discussion and Conclusion,"we demonstrate our approach on two tissue mimicking materials that have similar
elastic properties as healthy and cancerous prostate tissue [5,11]. the con- 4b
and the auroc and auprc scores of 1. note that the high errors for external
measurements at the needle shaft are systematic, as friction and bulk
displacement are unknown. in contrast, our probe does not suffer from these
systematic errors. moreover, considering the standard deviation for oce
estimates, improved calibration of our dual-fiber needle probe is expected to
further improve performance. deep learning-based approaches for tip force
estimation could provide increased accuracy and sensitivity compared to the
assumed linear model [3]. weighted strain estimation based on oct signal
intensity [26] could address the underestimation of local strain during segments
of low signal-to-noise-ratio (see supplementary material). we are also currently
only considering the loading cycle and linear elastic models for our approach.
however, soft-tissue displays strong non-linearity in contrast to the mostly
linear behavior of gelatin gels. compression oce theoretically enables the
analysis of non-linear elastic behavior [26] and future experiments will
consider non-linear models and unloading cycles better befitting
needletissue-interaction [15,25]. interestingly, our needle works with a beveled
tip geometry that allows insertion into deep tissue structures. during
insertion, tip force estimation can be used to detect interfaces and select the
pre-rupture deformation phase for oce estimates (fig. 3). this was previously
not possible with flat tip needle probes [9,13,17]. while the cylindrical tip is
advantageous for calculating the young's modulus, it has been shown that the
calculation of an equivalent young's modulus is rarely comparable across
different techniques and samples [4,12]. instead, it is important to provide
high contrast and high reproducibility to reliably distinguish samples with
different elastic properties. we show that our dual-fiber oce needle probe
enables biomechanical characterization by deriving quantitative biomechanical
parameters as demonstrated on tissue mimicking phantoms. further experiments
need to include biological soft tissue to validate the approach for clinical
application, as our evaluation is currently limited to homogeneous gelatin. this
needle probe could also be very useful when considering robotic needle
insertions, e.g., to implement feedback control based on elasticity estimates.",9
Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,1.0,Introduction,"pelvic fracture is a severe type of high-energy injury, with a fatality rate
greater than 50%, ranking the first among all complex fractures [8,16]. surgical
planning and reduction tasks are challenged by the complex pelvic structure, as
well as the surrounding muscle groups, ligaments, neurovascular and other
tissues. robotic fracture reduction surgery has been studied and put into
clinical use in recent years, and has successfully increased reduction precision
and reduced radiation exposure [2]. accurate segmentation of pelvic fracture is
required in both manual and automatic reduction planning, which aim to find the
optimal target location to restore the healthy morphology of pelvic bones.
segmenting pelvic fragments from ct is challenging due to the uncertain shape
and irregular position of the bone fragments and the complex collision fracture
surface. therefore, surgeons typically annotate the anatomy of pelvic fractures
in a semi-automatic way. first, by tuning thresholds and selecting seed points,
adaptive thresholding and region-growing methods are used to extract bone
regions [1,11,15]. then, the fracture surfaces are manually delineated by
outlining the fragments in 3d view or even modifying the masks in a
slice-by-slice fashion. usually, this tedious process can take more than 30 min,
especially when the fractured fragments are collided or not completely
separated.several studies have been proposed to provide more efficient tools for
operators. a semi-automatic graph-cut method based on continuous max-flow has
been proposed for pelvic fracture segmentation, but it still requires the manual
selection of seed points and trail-and-error [4,22]. fully automatic max-flow
segmentation based on graph cut and boundary enhancement filter is useful when
fragments are separated, but it often fails on fragments that are collided or
compressed [9,19]. learning-based bone segmentation has been successfully
applied to various anatomy, including the pelvis, rib, skull, etc. [12,13]. some
deep learning methods have been proposed to detect fractures [17,18,21], but the
output from these methods cannot provide a fully automated solution for
subsequent operations. in fracnet, rib fracture detection was formulated as a
segmentation problem, but with a resultant dice of 71.5%, it merely outlined the
fracture site coarsely without delineating the fracture surface [7].
learning-based methods that directly deal with fracture segmentation have rarely
been studied.fracture segmentation is still a challenging task for the
learning-based method because (1) compared to the more common organ/tumor
segmentation tasks where the model can implicitly learn the shape prior of an
object, it is difficult to learn the shape information of a bone fragment due to
the large variations in fracture types and shapes [10]; (2) the fracture surface
itself can take various forms including large space (fragments isolated and
moved), small gap (fragments isolated but not moved), crease (fragments not
completely isolated), compression (fragments collided), and their combinations,
resulting in quite different image intensity profiles around the fracture site;
and (3) the variable number of bone fragments in pelvic fracture makes it
difficult to prescribe a consistent labeling strategy that applies to every type
and case.this paper proposes a deep learning-based method to segment pelvic
fracture fragments from preoperative ct images automatically. our major
contribution includes three aspects. (1) we proposed a complete automatic
pipeline for pelvic fractures segmentation, which is the first learning-based
pelvic fracture segmentation method to the best of our knowledge. (2) we
designed a novel multi-scale distance-weighted loss and integrated it into the
deeply supervised training of the fracture segmentation network to boost
accuracy near the fracture site. (3) we established a comprehensive pelvic
fracture ct dataset and provided ground-truth annotations. our dataset and
source code are publicly available at https://github.com/yzzliu/fracsegnet. we
expect them to facilitate further pelvis-related research, including but not
limited to fracture identification, segmentation, and subsequent reduction
planning.",9
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,1.0,Introduction,"gliomas are the most common central nervous system (cns) tumors in adults,
accounting for 80% of primary malignant brain tumors [1]. early surgical
treatment to remove the maximum amount of cancerous tissues while preserving the
eloquent brain regions can improve the patient's survival rate and functional
outcomes of the procedure [2]. although the latest multi-modal medical imaging
(e.g., pet, diffusion/functional mri) allows more precise pre-surigcal planning,
during surgery, brain tissues can deform under multiple factors, such as
gravity, intracranial pressure change, and drug administration. the phenomenon
is referred to as brain shift, and often invalidates the pre-surgical plan by
displacing surgical targets and other vital anatomies. with high flexibility,
portability, and cost-effectiveness, intra-operative ultrasound (us) is a
popular choice to track and monitor brain shift. in conjunction with effective
mri-us registration algorithms, the tool can help update the pre-surgical plan
during surgery to ensure the accuracy and safety of the intervention.as the true
underlying deformation from brain shift is impossible to obtain and the
differences of image features between mri and us are large, quantitative
validation of automatic mri-us registration algorithms often rely on homologous
anatomical landmarks that are manually labeled between corresponding mri and
intra-operative us scans [3]. however, manual landmark identification requires
strong expertise in anatomy and is costly in labor and time. moreover, inter-and
intra-rater variability still exists. these factors make quality assessment of
brain shift correction for us-guided brain tumor resection challenging. in
addition, due to the time constraints, similar evaluation of inter-modal
registration quality during surgery is nearly impossible, but still highly
desirable. to address these needs, deep learning (dl) holds the promise to
perform efficient and automatic inter-modal anatomical landmark
detection.previously, many groups have proposed algorithms to label landmarks in
anatomical scans [4][5][6][7][8][9]. however, almost all earlier techniques were
designed for mono-modal applications, and inter-modal landmark detection, such
as for usguided brain tumor resection, has rarely been attempted. in addition,
unlike other applications, where the full anatomy is visible in the scan and all
landmarks have consistent spatial arrangements across subjects, intra-operative
us of brain tumor resection only contains local regions of the pathology with
noncanonical orientations. this results in anatomical landmarks with different
spatial distributions across cases. to address these unique challenges, we
proposed a new contrastive learning (cl) framework to detect matching landmarks
in intra-operative us with those from mri as references. specifically, the
technique leverages two convolutional neural networks (cnns) to learn features
between mri and us that distinguish the inter-modal image patches which are
centered at the matching landmarks from those that are not. our approach has two
major novel contributions to the field. first, we proposed a multi-modal
landmark detection algorithm for us-guided brain tumor resection for the first
time. second, cl is employed for the first time in inter-modal anatomical
landmark detection. we developed and validated the proposed technique with the
public resect database [10] and compared its landmark detection accuracy against
the popular scale-invariant feature transformation (sift) algorithm in 3d [11].",9
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,3.1,Data and Landmark Annotation,"we employed the publicly available easy-resect (retrospective evaluation of
cerebral tumors) dataset [10] (https://archive.sigma2.no/pages/ public/dataset
detail.jsf?id=10.11582/2020.00025) to train and evaluate our proposed method.
this dataset is a deep-learning-ready version of the original resect database,
and was released as part of the 2020 learn2reg challenge [24]. specifically,
easy-resect contains mri and intra-operative us scans (before resection) of 22
subjects who have undergone low-grade glioma resection surgeries. all images
were resampled to a unified dimension of 256 × 256 × 288 voxels, with an
isotropic resolution of ∼0.5mm. between mri and the corresponding us images,
matching anatomical landmarks were manually labeled by experts and 15∼16
landmarks were available per case. a sample illustration of corresponding
inter-modal scans and landmarks is shown in fig. 1. for the target application,
we employed the t2flair mri to pair with intra-operative us since low-grade
gliomas are usually more discernible in t2flair than in t1-weighted mri [10].",9
Towards Multi-modal Anatomical Landmark Detection for Ultrasound-Guided Brain Tumor Resection with Contrastive Learning,6.0,Discussion,"inter-modal anatomical landmark localization is still a difficult task,
especially for the described application, where landmarks have no consistent
spatial arrangement across different cases and image features in us are rough.
we tackled the challenge with the cl framework for the first time. as the first
step towards more accurate inter-modal landmark localization, there are still
aspects to be improved. first, while the 2.5d approach is memory efficient and
quick, 3d approaches may better capture the full corresponding image features.
this is partially reflected by the observation that the quality of landmark
localization is associated with the level of tissue shift. however, due to
limited clinical data, 3d approaches caused overfitting in our network training.
second, in the current setup, we employed landmarks in pre-operative mris as
references since its contrast is easier to understand and it allows sufficient
time for clinicians to annotate the landmarks before surgery. future exploration
will also seek techniques to automatically tag mri reference landmarks. finally,
we only employed us scans before resection since tissue removal can further
complicate feature matching between mri and us, and requires more elaborate
strategies, such as those involving segmentation of resected regions [27]. we
will explore suitable solutions to extend the application scenarios of our
proposed framework as part of the future investigation. as a baseline
comparison, we employed the sift algorithm, which has demonstrated excellent
performance in a large variety of computer vision problems for keypoint
matching. however, in the described inter-modal landmark identification for
us-guided brain tumor resection, the sift algorithm didn't offer satisfactory
results. this could be due to the coarse image features and textures of
intra-operative us and the differences in the physical resolution between mri
and us. one major critique for using the sift algorithm is that it intends to
find geometrically interesting keypoints, which may not have good anatomical
significance. in the resect dataset, eligible anatomical landmarks were defined
as deep grooves and corners of sulci, convex points of gyri, and vanishing
points of sulci. the relevant local features may be hard to capture with the
sift algorithm. in this sense, dl-based approaches may be a better choice for
the task. with the cl framework, our method learns the common features between
two different modalities via the training process.besides better landmark
identification accuracy, the tighter standard deviations also imply that our dl
approach serves a better role in grasping the local image features within the
image patches.",9
Deep Reinforcement Learning Based System for Intraoperative Hyperspectral Video Autofocusing,1.1,Background,"traditional optical imaging samples the visual spectrum in three diffuse
spectral bands (rgb), while hyperspectral imaging (hsi) provides much more
detailed spectral information. this information is potentially valuable for
making intraoperative decisions, particularly in cases where tissue
differentiation is critical but challenging to perform using traditional
visualisation techniques. in the case of brain tumour excision,
fluorescence-guided resection is commonly used to minimize damage to healthy
tissue [2] but is limited to high-grade gliomas, and results in added cost and
workflow disruptions. thanks to a more detailed definition between tissue types
[5], hsi is seen as a promising alternative with wider applicability and
smoother integration into the workflow.while hsi has been integrated into
surgical microscope systems [11], it is suggested that handheld systems are
better suited to translational research [4]. such handheld systems consist of an
exoscope coupled to a draped optical stack, as shown in fig. 1. the optics in
the exoscope typically result in a short focal depth, making manual focusing
tricky, particularly as the tuning must be performed through the drape. as such,
these systems are commonly left at a fixed focal power and the surgeon must keep
the working distance fixed to keep the subject in focus. furthermore, the narrow
spectral bands of hsi sensors reduce the amount of light collected [12]. to
avoid increasing exposure time, a large aperture size is needed, at a cost of
further reducing focal depth. this exacerbates the focusing issues, making
current real-time handheld hsi imaging systems particularly challenging to
focus, posing significant usability issues. figure 1 highlights the limited
focal depth of our system, and shows a typical target that the surgeon must
manually bring into focus during surgery.the issue of reduced focal depth in
real-time hsi systems could be mitigated by the introduction of a video
autofocus system. autofocus methods are divided into active methods, which use
transmission to probe the scene, and passive methods, which rely only on
incoming light. passive methods are further split into phase-based, which
require specialised hardware, and contrast-based, which compare images captured
at different focal powers. our investigation focuses on contrast-based methods,
which require minimal hardware development.",9
Surgical Video Captioning with Mutual-Modal Concept Alignment,3.1,Dataset and Implementation Details,"neurosurgery video captioning dataset. to evaluate the effectiveness of surgical
video captioning, we collect a large-scale dataset with 41 surgical videos of
endonasal skull base neurosurgery. these surgical videos are recorded at the
prince of wales hospital, chinese university of hong kong, where surgeons remove
pituitary tumors through the endonasal corridor to the skull base. after
necessary data cleaning, we divide these surgical videos with resolution of 1,
920× 1, 080 into 11, 004 thirty-second video clips with clear surgical purposes.
these video clips are annotated under tool-tissue interaction (tti) principle
[18], and include a total of 16 instruments, 8 targets, and 10 surgical actions.
the annotation preprocessing follows [26] using nltk [16] toolkit. the
proportion of surgical concepts is illustrated in fig. 3. we split these video
clips at patientlevel, where the video clips of 31 patients are used for
training and the rest of 10 patients are utilized for test.endovis image
captioning dataset. we further compare our method with state-of-the-arts on the
public endovis-2018 image captioning dataset [1,23]. this dataset reveals
robotic nephrectomy procedures acquired by the da vinci x or xi system, and is
annotated with surgical actions between 9 possible tools and surgical targets
[23]. we follow the official split in [24] with 11 sequences for training and 3
sequences for test. in this way, these two datasets can comprehensively evaluate
the captioning tasks under both surgical videos and images.implementation
details. we implement our sca-net and state-of-the-art captioning methods
[5,9,21,24,26] in pytorch [20]. we optimize the sca-net and compared captioning
methods using adam with the batch size of 12 for both captioning datasets. all
models are trained for 20 and 50 epochs in neurosurgery and endovis datasets,
respectively. we adopt the step-wise learning rate decay strategy to facilitate
training convergence, where the learning rate is initialized as 1 × 10 -2 and
halved after every 5 epochs. the loss coefficients λ 1 of l scl and λ 2 of of l
mca are empirically set to 0.1 and 0.01, respectively. all experiments are
performed on a single nvidia a100 gpu.evaluation metrics. to evaluate the
captioning performance, we adopt standard metrics, including bleu@4 [19], meteor
[3], spice [2], rouge [12] and cider [22]. specifically, bleu@4 [19] evaluates
the 4-gram precision of the predicted caption, and cider [22] is based on the
n-gram similarity with tf-idf weights. meteor [3] considers both precision and
recall. rouge [12] and spice [2] measure the matching between predictions and
ground truth. the higher scores of these metrics indicate better performance in
surgical captioning.",9
Imitation Learning from Expert Video Data for Dissection Trajectory Prediction in Endoscopic Surgical Procedure,1.0,Introduction,"despite that deep learning models have shown success in surgical data science to
improve the quality of surgical intervention [20][21][22], such as intelligent
workflow analysis [7,13] and scene understanding [1,28], research on
higher-level cognitive assistance for surgery still remains underexplored. one
essential task is supporting decision-making on dissection trajectories
[9,24,29], which is challenging yet crucial for ensuring surgical safety.
endoscopic submucosal dissection (esd), a surgical procedure for treating early
gastrointestinal cancers [2,30], involves multiple dissection actions that
require considerable experience to determine the optimal dissection trajectory.
informative suggestions for dissection trajectories can provide helpful
cognitive assistance to endoscopists, for mitigation of intraoperative errors,
reducing risks of complications [15], and facilitating surgical skill training
[17]. however, predicting the desired trajectory for future time frames based on
the current endoscopic view is challenging. first, the decision of dissection
trajectories is complicated and depends on numerous factors such as safety
margins surrounding the tumor. second, dynamic scenes and poor visual conditions
may further hamper scene recognition [27]. to date, there is still no work on
data-driven solutions to predict such dissection trajectories, but we argue that
it is possible to reasonably learn this skill from expert demonstrations based
on video data.imitation learning has been widely studied in various domains
[11,16,18] with its good ability to learn complex skills, but it still needs
adaptation and improvement when being applied to learn dissection trajectory
from surgical data. one challenge arises from the inherent uncertainty of future
trajectories. supervised learning such as behavior cloning (bc) [3] tends to
average all possible prediction paths, which leads to inaccurate predictions.
while advanced probabilistic models are employed to capture the complexity and
variability of dissection trajectories [14,19,25], how to ensure reliable
predictions across various surgical scenes still remains a great challenge. to
overcome these issues, implicit models are emerging for policy learning,
inspiring us to rely on implicit behavior cloning (ibc) [5], which can learn
robust representations by capturing the shared features of both visual inputs
and trajectory predictions with a unified implicit function, yielding superior
expressivity and visual generalizability. however, these methods still bear
their limitations. for instance, approaches leveraging energy-based models
(ebms) [4][5][6]12] suffer from intensive computations due to reliance on the
langevin dynamics, which leads to a slow training process. in addition, the
model performance can be sensitive to data distribution and the noise in
training data would result in unstable trajectory predictions.in this paper, we
explore an interesting task of predicting dissection trajectories in esd surgery
via imitation learning on expert video data. we propose implicit diffusion
policy imitation learning (idiff-il), a novel imitation learning approach for
dissection trajectory prediction. to effectively model the surgeon's behaviors
and handle the large variation of surgical scenes, we leverage implicit modeling
to express expert dissection skills. to address the limitations of inefficient
training and unstable performance associated with ebm-based implicit policies,
we formulate the implicit policy using an unconditional diffusion model, which
demonstrates remarkable ability in representing complex high-dimensional data
distribution for videos. subsequently, to obtain predictions from the implicit
policy, we devise a conditional action inference strategy with the guidance of
forward-diffusion, which further improves the prediction accuracy. for
experimental evaluation, we collected a surgical video dataset of esd
procedures, and preprocessed 1032 short clips with dissection trajectories
labelled. results show that our method achieves superior performances in
different contexts of surgical scenarios compared with representative popular
imitation learning methods.",9
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,1.0,Introduction,"image-to-physical registration is a necessary process for computer-assisted
surgery to align preoperative imaging to the intraoperative physical space of
the patient to in-form surgical decision making. most intraoperatively utilized
image-to-physical regis-trations are rigid transformations calculated using
fiducial landmarks [1]. however, with better computational resources and more
advanced surgical field monitoring sensors, nonrigid registration techniques
have been proposed [2,3]. this has made image-guided surgery more tractable for
soft tissue organ systems like the liver, prostate, and breast [4][5][6]. this
work focuses specifically on nonrigid breast registration, although these
methods could be adapted for other soft tissue organs. current guidance
technologies for breast conserving surgery localize a single tumor-implanted
seed without providing spatial information about the tumor boundary. as a
result, resections can have several centimeters of tissue beyond the cancer
margin. despite seed information and large resections, reoperation rates are
still high (~17%) emphasizing the need for additional guidance technologies such
as computer-assisted surgery systems with nonrigid registration
[7].intraoperative data available for registration is often sparse and subject
to data collection noise. image-to-physical registration methods that accurately
model an elastic soft-tissue environment while also complying with
intraoperative data constraints is an active field of research. determining
correspondences between imaging space and geometric data is required for
image-to-physical registration, but it is often an inexact and ill-posed
problem. establishing point cloud correspondences using machine learning has
been demonstrated on liver and prostate datasets [8,9]. deep learning image
registration methods like voxelmorph have also been used for this purpose [10].
however, these methods require extensive training data and may struggle with
generalizability. other non-learning image-to-physical registration strategies
include [11] which utilized a corotational linear-elastic finite element method
(fem) combined with an iterative closest point algorithm. similarly, the
registration method introduced in [12] iteratively updated the image-to-physical
correspondence between surface point clouds while solving for an optimal
deformation state.in addition to a correspondence algorithm, a technique for
modeling a deformation field is required. both [11] and [12] leverage fem, which
uses a 3d mesh to solve for unique deformation solutions. however, large
deformations can cause mesh distortions with the need for remeshing. mesh-free
methods have been introduced to circumvent this limitation. the element-free
galerkin method is a mesh-free method that requires only nodal point data and
uses a moving least-squares approximation to solve for a solution [13]. other
mesh-free methods are reviewed in [14]. although these methods do not require a
3d mesh, solving for a solution can be costly and boundary condition designation
is often unintuitive. having identified these same shortcomings, [15] proposed
regularized kelvinlet functions for volumetric digital sculpting in computer
animation applications. this sculpting approach provided de-formations
consistent with linear elasticity without large computational overhead.in this
work, we propose an image-to-physical registration method that uses regularized
kelvinlet functions as a novel deformation basis for nonrigid registration.
regularized kelvinlet functions are analytical solutions to the equations for
linear elasticity that we superpose to compute a nonrigid deformation field
nearly instantaneously [15].we utilize ""grab"" and ""twist"" regularized kelvinlet
functions with a linearized iterative reconstruction approach (adapted from
[12]) that is well-suited for sparse data registration problems. sensitivity to
regularized kelvinlet function hyperparameters is explored on a supine mr breast
imaging dataset. finally, our approach is validated on an exemplar breast cancer
case with a segmented tumor by comparing performance to previously proposed
registration methods.",9
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,3.0,Experiments and Results,"in this section, two experiments are conducted. the first explores sensitivity
to regularized kelvinlet function hyperparameters k grab , k twist , ε grab ,
and ε twist and establishes optimal hyperparameters in a training dataset of 11
breast deformations. the second validates the registration method in a breast
cancer patient and compares registration accuracy and computation time to
previously proposed methods.",9
Regularized Kelvinlet Functions to Model Linear Elasticity for Image-to-Physical Registration of the Breast,3.2,Registration Methods Comparison,"this dataset consists of supine breast mr images simulating surgical
deformations from one breast cancer patient. a 71-year-old patient with invasive
mammary carcinoma in the left breast was enrolled in a study approved by the
institutional review board at vanderbilt university. skin fiducial placement,
image acquisition, arm placement, and preprocessing steps followed the same
protocol detailed in sect. 3.1. the tumor was segmented in both images by a
subject matter expert, and a 3d tumor model was created to evaluate tumor
overlap metrics after registration.regularized kelvinlet function registration
was compared to 3 other registration methods: rigid registration, an fem-based
image-to-physical registration method, and an image-to-image registration
method. a point-based rigid registration using the skin fiducials provided a
baseline comparator for accuracy without deformable correction. the fem-based
image-to-physical registration method, detailed in [12] and implemented in
breast in [16], utilizes the same optimization scheme as this method but with an
fem-generated basis. k = 40 control points were used for the fem-based
registration. the image-to-image registration method was a symmetric
diffeomorphic method with explicit b-spline regularization publicly available in
the advanced normalization toolkit (ants) repository [19,20]. image-to-image
registration would not be possible for intraoperative registration in most
surgical settings. however, it was included to demonstrate accuracy when
volumetric imaging data is available, as opposed to sparse geometric point data
as in the surgical application case. the rigid and image-to-physical
registrations were performed on a single thread of a 3.6 ghz amd ryzen 7 3700x
cpu. image-to-image registration was multithreaded on 2.3 ghz intel xeon
(e5-4610 v2) cpus.registration results for the 4 methods are shown in table 1.
the regularized kelvinlet method accuracy was comparable (if not slightly
improved) to the fem-based method for this example case. runtime for the
regularized kelvinlet method was improved compared to the fem-based method. as
expected, registration without deformable correction was poor, and
image-to-image registration had the best accuracy. registered tumor geometry
results are shown in fig. 4.",9
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,1.0,Introduction,"we address the important problem of intraoperative patient-to-image registration
in a new way by relying on preoperative data to synthesize plausible
transformations and appearances that are expected to be found intraoperatively.
in particular, we tackle intraoperative 3d/2d registration during neurosurgery,
where preoperative mri scans need to be registered with intraoperative surgical
views of the brain surface to guide neurosurgeons towards achieving a maximal
safe tumor resection [22]. indeed, the extent of tumor removal is highly
correlated with patients' chances of survival and complete resection must be
balanced against the risk of causing new neurological deficits [5] making
accurate intraoperative registration a critical component of
neuronavigation.most existing techniques perform patient-to-image registration
using intraoperative mri [11], cbct [19] or ultrasound [9,17,20]. for 3d-3d
registration, 3d shape recovery of brain surfaces can be achieved using
near-infrared cameras [15], phase-shift 3d shape measurement [10], pattern
projections [17] or stereovision [8]. the 3d shape can subsequently be
registered with the preoperative mri using conventional point-to-point methods
such as iterative closest point (icp) or coherent point drift (cpd). most of
these methods rely on cortical vessels that bring salient information for such
tasks. for instance, in [6], cortical vessels are first segmented using a deep
neural network (dnn) and then used to constrain a 3d/2d non-rigid registration.
the method uses physics-based modeling to resolve depth ambiguities. a manual
rigid alignment is however required to initialize the optimization.
alternatively, cortical vessels have been used in [13] where sparse 3d points,
manually traced along the vessels, are matched with vessels extracted from the
preoperative scans. a model-based inverse minimization problem is solved by
estimating the model's parameters from a set of pre-computed transformations.
the idea of pre-computing data for registration was introduced by [26], who used
an atlas of pre-computed 3d shapes of the brain surface for registration. in
[7], a dnn is trained on a set of pre-generated preoperative to intraoperative
transformations. the registration uses cortical vessels, segmented using another
neural network, to find the best transformation from the pre-generated set.the
main limitation of existing intraoperative registration methods is that they
rely heavily on processing intraoperative images to extract image features (eg.,
3d surfaces, vessels centerlines, contours, or other landmarks) to drive
registration, making them subject to noise and low-resolution images that can
occur in the operating room [2,25]. outside of neurosurgery, the concept of
pregenerating data for optimizing dnns for intraoperative registration has been
investigated for ct to x-ray registration in radiotherapy where x-ray images can
be efficiently simulated from cts as digital radiographic reconstructions
[12,27]. in more general applications, case-centered training of dnns is gaining
in popularity and demonstrates remarkable results [16].",9
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,3.0,Results,"dataset. we tested our method retrospectively on 6 clinical datasets from 6
patients (cases) (see fig. 5). these consisted of preoperative t1 contrast mri
scans and intraoperative images of the brain surface after dura opening.
cortical vessels around the tumors were segmented and triangulated to generate
3d meshes using 3d slicer. we generated 100 poses for each 3d mesh (i.e.: each
case) and used a total of 15 unique textures from human brain surfaces
(different from our 6 clinical datasets) for synthesis using s θ . in order to
account for potential intraoperative brain deformations [4] we augment the
textured projection with elastic deformation [21] resulting in approximately
1500 images per case. the surgical images of the brain (left image of the
stereoscopic camera) were acquired with a carl zeiss surgical microscope. the
ground-truth poses were obtained by manually aligning the 3d meshes on their
corresponding images. we evaluated the pose regressor network on both synthetic
and real data. the model training and validation were performed on the
synthesized images while the model testing was performed on the real images.
because a conventional train/validation/test split would lead to texture
contamination, we created our validation dataset so that at least one texture is
excluded from the training set. on the other hand, the test set consisted of the
real images of the brain surface acquired using the surgical camera and are
never used in the training. accuracy-threshold curves on the validation
set.metrics. we chose the average distance metric (add) as proposed in [23] for
evaluation. given a set of mesh's 3d vertices, the add computes the mean of the
pairwise distance between the 3d model points transformed using the ground truth
and estimated transformation. we also adjusted the default 5 cm-5 deg
translation and rotation error to our neurosurgical application and set the new
threshold to 3 mm-3 deg.accuracy-threshold curves. we calculated the number of
'correct' poses estimated by our model. we varied the distance threshold on the
validation sets (excluding 2 textures) in order to reveal how the model performs
w.r.t. that threshold. we plotted accuracy-threshold curves showing the
percentage of pose accuracy variation with a threshold in a range of 0 mm to 20
mm. we can see in fig. 3 that a 80.23% pose accuracy was reached within the 3
mm-3 deg threshold for all cases. this accuracy increases to 95.45% with a 5
mm-5 deg threshold.",9
Learning Expected Appearances for Intraoperative Registration During Neurosurgery,4.0,Discussion and Conclusion,"clinical feasibility. we have shown that our method is clinically viable. our
experiments using clinical data showed that our method provides accurate
registration without manual intervention, that it is computationally efficient,
and it is invariant to the visual appearance of the cortex. our method does not
require intraoperative 3d imaging such as intraoperative mri or ultrasound,
which require expensive equipment and are disruptive during surgery. training
patient-specific models from preoperative imaging transfers computational tasks
to the preoperative stage so that patient-to-image registration can be performed
in near real-time from live images acquired from a surgical
microscope.limitations. the method presented in this paper is limited to 6-dof
pose estimation and does not account for deformation of the brain due to changes
in head position, fluid loss, or tumor resection and assumes a known focal
length. in the future, we will expand our method to model non-rigid deformations
of the 3d mesh and to accommodate expected changes in zoom and focal depth
during surgery. we will also explore how texture variability can be controlled
and adapted to the observed image to improve model accuracy.",9
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,1.0,Introduction,"radiotherapy (rt) has proven effective and efficient in treating cancer
patients. however, its application depends on treatment planning involving
target lesion and radiosensitive organs-at-risk (oar) segmentation. this is
performed to guide radiation to the target and to spare oar from inappropriate
irradiation. hence, this manual segmentation step is very time-consuming and
must be performed accurately and, more importantly, must be patient-safe.
studies have shown that the manual segmentation task accounts for over 40% of
the treatment planning duration [7] and, in addition, it is also error-prone due
to expert-dependent variations [2,24]. hence, deep learning-based (dl)
segmentation is essential for reducing time-to-treatment, yielding more
consistent results, and ensuring resource-efficient clinical workflows.nowadays,
training of dl segmentation models is predominantly based on loss functions
defined by geometry-based (e.g., softdice loss [15]), distributionbased
objectives (e.g., cross-entropy), or a combination thereof [13]. the general
strategy has been to design loss functions that match their evaluation
counterpart. nonetheless, recent studies have reported general pitfalls of these
metrics [4,19] as well as a low correlation with end-clinical objectives
[11,18,22,23]. furthermore, from a robustness point of view, models trained with
these loss functions have been shown to be more prone to generalization issues.
specifically, the dice loss, allegedly the most popular segmentation loss
function, has been shown to have a tendency to yield overconfident trained
models and lack robustness in out-of-distribution scenarios [5,14]. these
studies have also reported results favoring distribution-matching losses, such
as the cross-entropy being a strictly proper scoring rule [6], providing
better-calibrated predictions and uncertainty estimates. in the field of rt
planning for brain tumor patients, the recent study of [17] shows that current
dl-based segmentation algorithms for target structures carry a significant
chance of producing false positive outliers, which can have a considerable
negative effect on applied radiation dose, and ultimately, they may impact
treatment effectiveness. in rt planning, the final objective is to produce the
best possible radiation plan that jointly targets the lesion and spares healthy
tissues and oars. therefore, we postulate that training dl-based segmentation
models for rt planning should consider this clinical objective.in this paper, we
propose an end-to-end training loss function for dl-based segmentation models
that considers dosimetric effects as a clinically-driven learning objective. our
contributions are: (i) a dosimetry-aware training loss function for dl
segmentation models, which (ii) yields improved model robustness, and (iii)
leads to improved and safer dosimetry maps. we present results on a clinical
dataset comprising fifty post-operative glioblastoma (gbm) patients. in
addition, we report results comparing the proposed loss function, called
dose-segmentation loss (doselo), with models trained with a combination of
binary cross-entropy (bce) and softdice loss functions.",9
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,2.0,Methodology,"figure 1 describes the general idea of the proposed doselo. a segmentation model
(u-net [20]) is trained to output target segmentation predictions for the gross
tumor volume (gtv) based on patient mri sequences. predicted segmentations and
their corresponding ground-truth (gt) are fed into a dose predictor model, which
outputs corresponding dose predictions (denoted as d p and d p in fig. 1). a
pixel-wise mean squared error between both dose predictions is then a
segmentation model (u-net [20]) is trained to output target segmentation
predictions ( st ) for the gross tumor volume (gtv) based on patient mri
sequences imr. predicted ( st ) and ground-truth segmentations (st ) are fed
into the dose predictor model along with the ct-image (ict ), and oar
segmentation (sor). the dose predictor outputs corresponding dose predictions dp
and dp . a pixel-wise mean squared error between both dose predictions is
calculated, and combined with the binary crossentropy (bce) loss to form the
final loss, l total = lbce + λldsl. calculated and combined with the bce loss to
form the final loss. in the next sections we describe the adopted dose
prediction model [9,12], and the proposed doselo.",9
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,2.1,Deep Learning-Based Dose Prediction,"recent dl methods based on cascaded u-nets have demonstrated the feasibility of
generating accurate dose distribution predictions from segmentation masks,
approximating analytical dose maps generated by rt treatment planning systems
[12]. originally proposed for head and neck cancer [12], this approach has been
recently extended for brain tumor patients [9] with levels of prediction error
below 2.5 gy, which is less than 5% of the prescribed dose. this good level of
performance, along with its ability to yield near-instant dose predictions,
enables us to create a training pipeline that guides learned features to be
dose-aware.following [12], the dose predictor model consists of a cascaded u-net
(i.e., the input to the second u-net is the output of the first concatenated
with the input to the first u-net) trained on segmentation masks, ct images, and
reference dose maps. the model's input is a normalized ct volume and
segmentation masks for target volume and oars. as output, it predicts a
continuous-valued dose map of the same dimension as the input. the model is
trained via deep supervision as a linear combination of l2-losses from the
outputs of each u-net in the cascade. we refer the reader to [9,12] for further
implementation details. we remark that the dose predictor model was also trained
with data augmentation, so imperfect segmentation masks and corresponding dose
plans are included. this allows us in this study to use the dose predictor to
model the interplay between segmentation variability and dosimetric
changes.formally, the dose prediction model m d receives as inputs:
segmentations masks for the gtv s t ∈ z w ×h and the oars s or ∈ z w ×h , the ct
image (used for tissue attenuation calculation purposes in rt) i ct ∈ r w ×h ,
and outputs m d (s t , s or , i ct ) → d p ∈ r w ×h , a predicted dose map where
each pixel value in d corresponds to the local predicted dose in gy. due to the
limited data availability, we present results using 2d-based models but remark
that their extension to 3d is straightforward. working in 2d is also feasible
from an rt point of view because the dose predictor is based on co-planar
volumetric modulated arc therapy (vmat) planning, commonly used in this clinical
scenario.",9
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,3.3,Results,"figure 2 shows results on the test set, sorted by their dosimetric impact. we
found an overall reduction of the relative mean absolute error (rmae) with
respect to the reference dose maps, from 0.449 ± 0.545, obtained via the
bce+softdice combo-loss, to 0.258 ± 0.201 for the proposed doselo (i.e., an
effective 42.5% reduction with λ = 1). this significant dose error reduction
shows the ability of the proposed approach to yield segmentation results in
better agreement with dose maps obtained using gt segmentations than those
obtained using the state-of-the-art bce+softdice combo-loss.table 1 shows
results for the first and most significant four cases from a rt point of view
(due to space limitations, all other cases are shown in supplementary material).
we observe the ability of the proposed approach to significantly reduce
outliers, generating a negative dosimetry impact on the dose fig. 2. relative
mean absolute dose errors/differences (rmae) between the reference dose map and
dose maps obtained using the predicted segmentations. lower is better. across
all tested cases and folds we observe a large rmae reduction for dose maps using
the proposed doselo (average rmae reduction of 42.5%).maps. we analyzed case
number 3, 4, and 5 from fig. 2 for which the standard bce+softdice was slightly
better than the proposed doselo. for case no. 3 the tumor presents a non-convex
shape alongside the skull's parietal lobe, which was not adequately modeled by
the training dataset used to train the segmentation models. indeed, we remark
that both models failed to yield acceptable segmentation quality in this area.
in case no. 4, both models failed to segment the diffuse tumor area alongside
the skull; however, as shown in fig. 2-case no. 4, the standard bce+softdice
model would yield a centrally located radiation dose, with strong negative
clinical impact to the patient. case no. 5 (shown in supplementary material) is
an interesting case called butterfly gbm, which is a rare type of gbm (around 2%
of all gbm cases [3]), characterized by bihemispheric involvement and invasion
of the corpus callosum. in this case, the training data also lacked
characterization for such cases. despite this limitation, we observed favorable
dose distributions with the proposed method.although we are aware that classical
segmentation metrics poorly correlate with dosimetric effects [18], we report
that the proposed method is more robust than the baseline bce+softdice loss
function, which yields outliers with hausdorff distances: 64.06 ± 29.84 mm vs
28.68 ± 22.25 mm (-55.2% reduction) for the proposed approach. as pointed out by
[17], segmentation outliers can have a detrimental effect on rt planning. we
also remark that the range of hd values is in range with values reported by
models trained using much more training data (see [1]), alluding to the
possibility that the problem of robustness might not be directly solvable with
more data. dice coefficients did not deviate significantly between the baseline
and the doselo models (dsc: 0.713 ± 0.203 (baseline) vs. 0.697 ± 0.216
(doselo)).table 1. comparison of dose maps and their absolute differences to the
reference dose maps (bce+softdice (bce+sd), and the proposed doselo). it can be
seen that doselo yields improved dose maps, which are in better agreement with
the reference dose maps (dose map color scale: 0 (blue) -70gy (red)).",9
Dose Guidance for Radiotherapy-Oriented Deep Learning Segmentation,4.0,Discussion and Conclusion,"the ultimate goal of dl-based segmentation for rt planning is to provide
reliable and patient-safe segmentations for dosimetric planning and optimally
targeting tumor lesions and sparing of healthy tissues. however, current loss
functions used to train models for rt purposes rely solely on geometric
considerations that have been shown to correlate poorly with dosimetric
objectives [11,18,22,23]. in this paper, we propose a novel dosimetry-aware
training loss function, called doselo, to effectively guide the training of
segmentation models toward dosimetric-compliant segmentation results for rt
purposes. the proposed doselo uses a fast-dose map prediction model, enabling
model guidance on how dosimetry is affected by segmentation variations. we merge
this information into a simple yet effective loss function that can be combined
with existing ones. these first results on a dataset of post-operative gbm
patients show the ability of the proposed doselo to deliver improved
dosimetric-compliant segmentation results. future work includes extending our
database of gbm cases and to other anatomies, as well as verifying potential
improvements when cotraining the segmentation and dose predictor models, and
jointly segmenting gtvs and oars. with this study, we hope to promote more
research toward clinically-relevant dl training loss functions.",9
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,1.0,Introduction,"residual tumor in the cavity after head and neck cancer (hnc) surgery is a
significant concern as it increases the risk of cancer recurrence and can
negatively impact the patient's prognosis [1]. hnc comprises the third highest
positive surgical margins (psm) rate across all oncology fields [2]. achieving
clear margins can be challenging in some cases, particularly in tumors with
involved deep margins [3,4].during transoral robotic surgery (tors), surgeons
may assess the surgical margin via visual inspection, palpation of the excised
specimen and intraoperative frozen sections analysis (ifsa) [5]. in the surgical
cavity, surgeons visually inspect for residual tumors and use specimen driven or
defect-driven frozen section analysis to identify any residual tumor [6,7]. the
latter involves slicing a small portion of the tissue at the edge of the cavity
and performing a frozen section analysis. these approaches are error-prone and
can result in psms and a higher risk of cancer recurrence [7]. in an effort to
improve these results, recent studies reported the use of exogenous fluorescent
markers [8] and wide-field optical coherence tomography [9] to inspect psms in
the excised specimen. while promising, each modality presents certain
limitations (e.g., time-consuming analysis, administration of a contrast agent,
controlled lighting environment), which has limited their clinical adoption
[10,11].label-free mesoscopic fluorescence lifetime imaging (flim) has been
demonstrated as an intraoperative imaging guidance technique with high
classification performance (auc = 0.94) in identifying in vivo tumor margins at
the epithelial surface prior to tumor excision [12]. flim can generate optical
contrast using autofluorescence derived from tissue fluorophores such as
collagen, nadh, and fad. due to the sensitivity of these fluorophores to their
microenvironment, the presence of tumor changes their emission properties (i.e.,
intensity and lifetime characteristics) relative to healthy tissue, thereby
enabling the optical detection of cancer [13].however, ability of label-free
flim to identify residual tumors in vivo in the surgical cavity (deep margins)
has not been reported. one significant challenge in developing a flim-based
classifier to detect tumor in the surgical cavity is the presence of highly
imbalanced labels.surgeons aim to perform an en bloc resection, removing the
entire tumor and a margin of healthy tissue around it to ensure complete
excision. therefore, in most cases, only healthy tissue in left in the cavity.
to address the technical challenge of highly imbalanced label distribution and
the need for intraoperative real-time cavity imaging, we developed an
intraoperative flim guidance model to identify residual tumors by classifying
residual cancer as anomalies. our proposed approach identified all patients with
psm. in contrast, the ifsa reporting a sensitivity of 0.5 [6,7].",9
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.0,Method,"as illustrated in fig. 1, the proposed method uses a clinically-compatible flim
system coupled to the da vinci sp transoral robotic surgical platform to scan
the surgical cavity in vivo and acquire flim data. we used the cumulative
distribution transform (cdt) of the fluorescence decay curves extracted from the
flim data as the input feature. the novelty detection model classified flim
points closer to the healthy distribution as healthy and further from the
healthy distribution as a residual tumor. we implemented the image guidance by
augmenting the classification map to the surgical view using the predictor
output and point locations of the scan.",9
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.1,FLIm Hardware and Data Acquisition,"this study used a multispectral fluorescence lifetime imaging (flim) device to
acquire data [14]. the flim device features a 355 nm uv laser for fluorescence
excitation, which is pulsed at a 480 hz repetition rate. a 365 µm multimode
optical fiber (0.22 na) delivers excitation light to tissue and relays the
corresponding fluorescence signal to a set of dichroic mirrors and bandpass
filters to spectrally resolve the autofluorescence. three variable gain uv
enhanced si apd modules with integrated trans-impedance amplifiers receive the
autofluorescence, which is spectrally resolved as follows: (1) 390/40 nm
attributed to collagen autofluorescence, (2) 470/28 nm to nadh, and (3) 542/50
nm to fad. the resulting autofluorescence waveform measurements for each channel
are averaged four times, thus with a 480 hz excitation rate, resulting in 120
averaged measurements per second [15].the flim device includes a 440 nm
continuous wave laser that serves as an aiming beam; this aiming beam enables
real-time visualization of the locations where fluorescence (point measurements)
is collected by generating visible blue illumination at the location where data
is acquired. segmentation of the 'aiming beam' allows for flim data points to be
localized as pixel coordinates within a surgical white light image (see fig. 1).
localization of these coordinates is essential to link the regions where data is
obtained to histopathology, which is used as the ground truth to link flim
optical data to pathology status [16]. flim data was acquired using the da vinci
sp robotic surgical platform. as part of the approved protocol for this study,
the surgeon performed in vivo flim scan on the tumor epithelial surface and the
surrounding uninvolved benign tissue. upon completing the scan, the surgeon
proceeded with en bloc excision of the tissue suspected of cancer. an ex vivo
flim scan was then performed on the surgically excised specimen. finally, the
patient's surgical cavity was scanned to check for residual tumor.",9
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.2,Patient Cohort and FLIm Data Labeling,"the research was performed under the approval of the uc davis institutional
review board (irb) and with the patient's informed consent. all patients were
anesthetized, intubated, and prepared for surgery as part of the standard of
care. n = 22 patients are represented in this study, comprising hnc in the
palatine tonsil (n = 15) and the base of the tongue (n = 7). for each patient,
the operating surgeon conducted an en bloc surgical tumor resection procedure
(achieved by tors-electrocautery instruments), and the resulting excised
specimen was sent to a surgical pathology room for grossing. the tissue specimen
was serially sectioned to generate tissue slices, which were then
formalin-fixed, paraffin-embedded, sectioned, and stained to create hematoxylin
& eosin (h&e) slides for pathologist interpretation (see fig. 1).after the
surgical excision of the tumor, an in vivo flim scan of approximately 90 s was
conducted within the patient's surgical cavity, where the tumor was excised. to
validate optical measurements to pathology labels (e.g., benign tissue vs.
residual tumor), pathology labels from the excision margins were digitally
annotated by a pathologist on each h&e section. the aggregate of h&e sections
was correspondingly labeled on the ex vivo specimen at the cut lines where the
tissue specimen was serially sectioned.thereafter, the labels were spatially
registered in vivo within the surgical cavity. this process enables the direct
validation of flim measurements to the pathology status of the electrocauterized
surgical margins (see table 1).",9
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.4,Novelty Detection Model,"the state-of-the-art novelty detection models were comprehensively reviewed in
the literature [18,19]. due to its robust performance, we chose the generalized
one-class discriminative subspaces (gods) classification model [20] to classify
healthy flim points from the residual tumor. the model trained only on the
healthy flim points and use a semi-supervised technique to classify residual
tumor from healthy. the gods is a pairwise complimentary classifier defined by
two separating hyperplanes to minimize the distance between the two classifiers,
limiting the healthy flim data within the smallest volume and maximizing the
margin between the hyperplanes and the data, thereby avoiding overfitting while
improving classification robustness. the first hyperplane (w 1 , b 1 ) projects
most of the healthy flim points to the positive half of the space, whereas the
second hyperplane (w 2 , b 2 ) projects most of the flim points in the negative
half.minwhere w 1 , w 2 are the orthonormal frames, minis the stiefel manifold,
η is the sensitivity margin, and was set η = 0.4 for our experiments. ν denote a
penalty factor on these soft constraints, and b is the biases. x i denotes the
training set containing cdt of the concatenated flim decay curve across channels
1-3 along the time axis. the cdt of the concatenated decay curves is computed as
follows: normalize the decay curves to 0-1. compute and normalize the cumulative
distribution function (cdf). transforming the normalized cdf into the cumulative
distribution transform by taking the inverse cumulative distribution function of
the normalized cdf [21].",9
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.5,Classifier Training and Evaluation,"the novelty detection model used for detecting residual cancer is evaluated at
the pointmeasurement level to assess the diagnostic capability of the method
over an entire tissue surface. the evaluation followed a leave-one-patient-out
cross-validation approach. the study further compared gods with two other
novelty detection models: robust covariance and, one-class support vector
machine (oc-svm) [22]. novelty detection model solely used healthy labels from
the in vivo cavity scan for training. the testing data contained both healthy
and residual cancer labels. we used grid search to optimize the hyper-parameters
and features used in each model and are tabulated in the supplementary section
table s1. the sensitivity, specificity, and accuracy were used as evaluation
metrics to assess the performance of classification models in the context of the
study.results of a binary classification model using svm are also shown in the
supplementary section table s2.",9
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,2.6,Classifier Augmented Display,"the classifier augmentation depends on three independent processing steps:
aiming beam localization, motion correction, and interpolation of the point
measurements. a detailed description of implementing the augmentation process is
discussed in [23]. the interpolation consists of fitting a disk to the segmented
aiming beam pixel location for each point measurement and applying a color map
(e.g., green: healthy and red: cancer) for each point prediction. individual
pixels from overlapping disks are averaged to produce the overall classification
map and augmented to the surgical field as a transparent overlay.",9
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,3.0,Results,"table 2 tabulates the classification performance comparison of novelty detection
models for classifying residual cancer vs. healthy on in vivo flim scans in the
cavity. three novelty detection models were evaluated, and all three models
could identify the presence of residual tumors in the cavity for the three
patients. however, the extent of the tumor classification over the entire tissue
surface varied among the models. the gods reported the best classification
performance with an average sensitivity of 0.75 ± 0.02 (see fig. 2). the lower
standard deviation indicates that the model generalizes well. the oc-svm and
robust covariance reported a high standard deviation, indicating that the
performance of the classification model is inconsistent across different
patients. the model's ability to correctly identify negative instances is
essential to its reliability. the gods model reported the highest mean
specificity of 0.78 ± 0.14 and the lowest standard deviation. the robust
covariance model reported the lowest specificity, classifying larger portions of
healthy tissue in the cavity as a residual tumor; indicating that the model did
not generalize well to the healthy labels. we also observed that changing the
hyper-parameter, such as the anomaly factor, biased the model toward a single
class indicating overfitting (see supplementary section fig. s1).the gods uses
two separating hyperplanes to minimize the distance between the two classifiers
by learning a low-dimensional subspace containing flim data properties of
healthy labels. residual tumor labels are detected by calculating the distance
between the projected data points and the learned subspace. points that are far
from the subspace are classified as residual tumors. we observed that the gods
with the flim decay curves in the cdt space achieve the best classification
performance compared to other novelty detection models with a mean accuracy of
0.76 ± 0.02. this is mainly due to the robustness of the model, the ability to
handle high-dimensional data, and the contrast in the flim decay curves. the
contrast in the flim decay curves was further improved in the cdt space by
transforming the flim decay curves to a normalized scale and improving linear
separability.",9
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,4.0,Discussion,"curent study demonstrates that label-free flim parameters-based classification
model, using a novelty detection aproach, enables identification of residual
tumors in the surgical cavity. the proposed model can resolve residual tumor at
the point-measurement level over a tissue surface. the model reported low
point-level false negatives and positives. moreover, the current approach
correctly identified all patients with psms (see fig. 2). this enhances surgical
precision for tors procedures otherwise limited to visual inspection of the
cavity, palpation of the excised specimen, and ifsa. the flimbased
classification model could help guide the surgical team in real-time, providing
information on the location and extent of cancerous tissue.in context to the
standard of care, the proposed residual tumor detection model exhibits high
patient-level sensitivity (sensitivity = 1) in detecting patients with psms. in
contrast, defect-driven ifsa reports a patient-level sensitivity of 0.5 [6,7].
our approach exhibits a low patient-level specificity compared to ifsa. surgeons
aim to achieve negative margins, meaning the absence of cancer cells at the
edges of the tissue removed during surgery. the finding of positive margins from
final histology would result in additional surgical resection, potentially
impacting the quality of life. combining the proposed approach and ifsa could
lead to an image-guided frozen section analysis to help surgeons achieve
negative margins in a more precise manner. therefore, completely resecting
cancerous tissue and improving patient outcomes.the false positive predictions
from the classification model presented two trends: false positives in an
isolated region and false positives spreading across a larger region. isolated
false positives are often caused by the noise of the flim system and are
accounted for by the interpolation approach used for the classifier augmentation
(refer to supplementary section fig. s2). on the other hand, false positives
spreading across a larger region are much more complex to interpret. one insight
is that the electrocautery effects on the tissues in the cavity may have
influenced them [24]. according to jackson's burn wound model, the thermal
effects caused by electrocautery vary with the different burnt zones. we
observed a correlation between a larger spread of false positive predictions
associated with a zone of coagulation to a zone of hyperemia.the novelty
detection model generalizes to the healthy labels and considers data falling off
the healthy distribution as residual cancer. the flim properties associated with
the healthy labels in the cavity are heterogeneous due to the electrocautery
effects. electrocautery effects are mainly thermal and can be observed by the
levels of charring in the tissue. refining the training labels based on the
levels of charring could lead to a more homogeneous representation of the
training set and result in an improved classification model with better
generalization.",9
FLIm-Based in Vivo Classification of Residual Cancer in the Surgical Cavity During Transoral Robotic Surgery,5.0,Conclusion,"this study demonstrates a novel flim-based classification method to identify
residual cancer in the surgical cavity of the oropharynx. the preliminary
results underscore the significance of the proposed method in detecting psms.
the model will be validated on a larger patient cohort in future work and
address the limitations of the point-level false positive and negative
predictions. this work may enhance surgical precision for tors procedures as an
adjunctive technique in combination with ifsa.",9
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,1.0,Introduction,"resection of early-stage brain tumors can greatly reduce the mortality rate of
patients. during the surgery, brain tissue deformation (called brain shift) can
occur due to various causes, such as gravity, drug administration, and pressure
change after craniotomy. while modern magnetic resonance imaging (mri)
techniques can provide rich anatomical and physiological information with
various contrasts (e.g., fmri) for more elaborate pre-surgical planning,
intra-operative mri that can track brain shift requires a complex setup and is
costly. in contrast, intra-operative ultrasound (ius) has gained popularity for
real-time imaging during surgery to monitor tissue deformation and surgical
tools because of its lower cost, portability, and flexibility [1]. accurate and
robust mri-ius registration techniques [2] can greatly enhance the value of ius
for updating pre-surgical plans and guiding the interpretation of ius, which has
an unintuitive contrast and non-standard orientations. this can greatly enhance
the safety and outcomes of the surgical procedure by allowing maximum brain
tumor removal while avoiding eloquent regions [3]. however, as the true
underlying tissue deformation is unknown due to the 3d nature of the surgical
data and the time constraint, real-time manual inspection of mri-ius
registration results is challenging and error-prone, especially for
precision-sensitive neurosurgery. therefore, algorithms that can detect and
quantify unreliable inter-modal medical image registration results are highly
beneficial.recently, automatic quality assessment for medical image registration
has attracted increasing attention [4] from the domains of big medical data
analysis and surgical interventions. with high efficiency, machine, and deep
learning techniques have been proposed to allow automatic grading and dense
estimation of medical image registration errors. early endeavors on this topic
primarily relied on hand-crafted features, including information theory-based
metrics [5][6][7][8][9][10]. more recently, deep learning (dl) techniques that
learn task-specific features have also been adopted in automatic evaluation of
medical image registration, with a primary focus on intra-contrast/modal
applications, including ct [9,10] and mri [11]. unfortunately, so far, error
grading and estimation in inter-contrast/modal registration have rarely been
explored, despite the particular demand in surgical applications. in this
direction, bierbrier et al. [12] made the first attempt using simulated ius from
mri to train 3d convolutional neural networks (cnns) to perform dense error
regression for mri-ius registration in brain tumor resection. although their
algorithm performed well in simulated cases, the results on real clinical scans
still required improvements. in this paper, we propose a novel 3d cnn to perform
patch-wise error estimation for mri-ius registration in neurosurgery, by using
focal modulation [13], a recent alternative dl technique to self-attention [14]
for encoding contextual information, and uncertainty estimation. we call our
method focalerrornet, which has three main novelties. first, we adapted the
focal modulation network [13] from 2d to 3d and employed the technique in
registration error assessment for the first time. second, we incorporated
uncertainty estimation using monte carlo (mc) dropouts [15] to offer assurance
for error regression. lastly, we developed and thoroughly evaluated our
technique against a recent baseline model [12] using real clinical data and
showed excellent results.",9
FocalErrorNet: Uncertainty-Aware Focal Modulation Network for Inter-modal Registration Error Estimation in Ultrasound-Guided Neurosurgery,2.1,Dataset and Preprocessing,"for methodological development and assessment, we used the resect
(retro-spective evaluation of cerebral tumors) dataset [16], which has
pre-operative mri, and ius scans at different surgical stages from 23 subjects
who underwent low-grade glioma resection surgeries. as it is still challenging
to model ius scans with tissue resection, we took 22 cases with t2flair mri that
better depicts tumor boundaries and ius acquired before resection. an example of
an mri-ius pair from a patient is shown in fig. 1. we hypothesized that directly
leveraging clinical ius could help learn more realistic image features with
potentially better outcomes in clinical applications than with simulated
contrasts [9,12]. however, since the true brain shift model is impossible to
obtain, we followed the strategy of creating silver ground truths for image
alignment [9,12], upon which simulated misalignment is augmented in the ius to
build and test our dl model. to create the silver registration ground truths, we
used the homologous landmarks between mri and ius in the resect dataset to
perform landmark-based 3d b-spline nonlinear registration to register ius to the
corresponding mri for all 22 cases. to tackle the limited field of view (fov) in
ius, we cropped the t2flair mri to the same fov of the ius, which was resampled
to a 0.5 × 0.5 × 0.5 mm 3 resolution. to perform spatial misalignment
augmentation, we continued to leverage 3d b-spline transformation, similar to
earlier reports on the same topic [10,12,17]. in short, b-spline transformation
can be modeled by a grid of regularly spaced control points and the associated
parameters to allow various levels of nonlinear deformation. while the spacing
of the control points determines the levels of details in local deformation
fields, the displacement parameters control the magnitude of the deformation. to
ensure that simulated registration errors are of different varieties and sizes,
we randomly selected the number of control points and the associated
displacements (in each 3d axis) with a maximum of 20 points and 30 mm,
respectively. note that the control point grid is isotropic, and the density is
arbitrarily determined per deformation in our case. each coregistered ius scan
was deformed ten times. after misalignment augmentation on the previously
co-registered ius, matching pairs of 3d image patches of size 33 × 33 × 33
voxels were taken from both the ius volume and the corresponding mri. as ius has
limited fov and may contain no anatomical features, to ensure that the patches
we extracted contain useful information (e.g. to avoid the dark background) in
ius, we focused on acquiring patches centered around the anatomical landmark
locations available through the resect database. since b-spline transformation
offers a displacement vector at each voxel of the ius volume, we directly
considered the norm of the vector as the simulated registration error at the
associated voxel. in our design, we determined the registration error of the
image patch pair as the mean of all voxel-wise errors within the ius patch.
finally, the image patch pairs, along with corresponding registration errors
were then fed to the proposed dl algorithm for training and validation.",9
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,1.0,Introduction,"cancer remains a significant public health challenge worldwide, with a new
diagnosis occurring every two minutes in the uk (cancer research uk 1 ). surgery
is one of the main curative treatment options for cancer. however, despite
substantial advances in pre-operative imaging such as ct, mri, or pet/spect to
aid diagnosis, surgeons still rely on the sense of touch and naked eye to detect
cancerous tissues and disease metastases intra-operatively due to the lack of
reliable intraoperative visualization tools. in practice, imprecise
intraoperative cancer tissue detection and visualization results in missed
cancer or the unnecessary removal of healthy tissues, which leads to increased
costs and potential harm to the patient. there is a pressing need for more
reliable and accurate intraoperative visualization tools for minimally invasive
surgery (mis) to improve surgical outcomes and enhance patient care. a recent
miniaturized cancer detection probe (i.e., 'sensei r ' developed by lightpoint
medical ltd.) leverages the cancer-targeting ability of nuclear agents typically
used in nuclear imaging to more accurately identify cancer intraoperatively from
the emitted gamma signal (see fig. 1b) [6]. however, the use of this probe
presents a visualization challenge as the probe is non-imaging and is air-gapped
from the tissue, making it challenging for the surgeon to locate the
probe-sensing area on the tissue surface.it is crucial to accurately determine
the sensing area, with positive signal potentially indicating cancer or affected
lymph nodes. geometrically, the sensing area is defined as the intersection
point between the gamma probe axis and the tissue surface in 3d space, but
projected onto the 2d laparoscopic image. however, it is not trivial to
determine this using traditional methods due to poor textural definition of
tissues and lack of per-pixel ground truth depth data. similarly, it is also
challenging to acquire the probe pose during the surgery.problem redefinition.
in this study, in order to provide sensing area visualization ground truth, we
modified a non-functional 'sensei' probe by adding a miniaturized laser module
to clearly optically indicate the sensing area on the laparoscopic images -i.e.
the 'probe axis-surface intersection'. our system consists of four main
components: a customized stereo laparoscope system for capturing stereo images,
a rotation stage for automatic phantom movement, a shutter for illumination
control, and a daq-controlled switchable laser module (see fig. 1a). with this
setup, we aim to transform the sensing area localization problem from a
geometrical issue to a high-level content inference problem in 2d. it is
noteworthy that this remains a challenging task, as ultimately we need to infer
the probe axis-surface intersection without the aid of the laser module to
realistically simulate the use of the 'sensei' probe.",9
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,3.0,Dataset,"to validate our proposed solution for the newly formulated problem, we acquired
and publicly released two new datasets. in this section, we introduce the
hardware and software design that was used to achieve our final goal, while fig.
2 shows a sample from our dataset. data collection. two miniaturized,
high-resolution cameras were coupled onto a stereo laparoscope using a
custom-designed connector. the accompanying api allowed for automatic image
acquisition, exposure time adjustment, and white balancing. an electrically
controllable shutter was incorporated into the standard laparoscopic
illumination path. to indicate the probe axis-surface intersection, we
incorporated a daq controlled cylindrical miniature laser module into a 'sensei'
probe shell so that the adapted tool was visually identical to the real probe.
the laser module emitted a red laser beam (wavelength 650 nm) that was visible
as a red spot on the tissue surface. we acquired the dataset on a silicone
tissue phantom which was 30 × 21 × 8 cm and was rendered with tissue color
manually by hand to be visually realistic. the phantom was placed on a rotation
stage that stepped 10 times per revolution to provide views separated by a
36-degree angle. at each position, stereo rgb images were captured i) under
normal laparoscopic illumination with the laser off; ii) with the laparoscopic
light blocked and the laser on; and iii) with the laparoscopic light blocked and
the laser off. subtraction of the images with laser on and off readily allowed
segmentation of the laser area and calculation of its central point, i.e. the
ground truth probe axis-surface intersection.all data acquisition and devices
were controlled by python and labview programs, and complete data sets of the
above images were collected on visually realistic phantoms for multiple probe
and laparoscope positions. this provided 10 tissue surface profiles for a
specific camera-probe pose, repeated for 120 different camera-probe poses,
mimicking how the probe may be used in practice. therefore, our first newly
acquired dataset, named jerry, contains 1200 sets of images. since it is
important to report errors in 3d and in millimeters, we recorded another dataset
similar to jerry but also including ground truth depth map for all frames by
using structured-lighting system [8]-namely the coffbee dataset.these datasets
have multiple uses such as:-intersection point detection: detecting intersection
points is an important problem that can bring accurate surgical cancer
visualization. we believe this is an under-investigated problem in surgical
vision. -depth estimation: corresponding ground truth will be released.-tool
segmentation: corresponding ground truth will be released.",9
Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,6.0,Conclusion,"in this work, a new framework for using a laparoscopic drop-in gamma detector in
manual or robotic-assisted minimally invasive cancer surgery was presented,
where a laser module mock probe was utilized to provide training guidance and
the problem of detecting the probe axis-tissue intersection point was
transformed to laser point position inference. both the hardware and software
design of the proposed solution were illustrated and two newly acquired datasets
were publicly released. extensive experiments were conducted on various
backbones and the best results were achieved using a simple network design,
enabling real time inference of the sensing area. we believe that our problem
reformulation and dataset release, together with the initial experimental
results, will establish a new benchmark for the surgical vision community.",9
A Novel Video-CTU Registration Method with Structural Point Similarity for FURS Navigation,1.0,Introduction,"flexible ureteroscopy (furs) is a routinely performed surgical procedure for
renal lithotripsy. this procedure inserts a flexible ureteroscope through the
blad-der and ureters to get inside the kidneys for diagnosis and treatment of
stones and tumors. unfortunately, such an examination and treatment depends on
skills and experiences of surgeons. on the other hand, surgeons may miss stones
and tumors and unsuccessfully orientate the ureteroscope inside the kidneys due
to limited field of views, just 2d images without depth information, and the
complex anatomical structure of the kidneys. to this end, ureteroscope tracking
and navigation is increasingly developed as a promising tool to solve these
issues.many researchers have developed various methods to boost endoscopic
navigation. these methods generally consist of vision-and sensor-based tracking.
han et al. [3] utilized the porous structures in renal video images to develop a
vision-based navigation method for ureteroscopic holmium laser lithotripsy. zhao
et al. [15] designed a master-slave robotic system to navigate the flexible
ureteroscope. luo et al. [7] reported a discriminative structural similarity
measure driven 2d-3d registration for vision-based bronchoscope tracking. more
recently, huang et al. [4] developed an image-matching navigation system using
shape context for robotic ureteroscopy. additionally, sensor-based methods are
widely sued in surgical navigation [1,6]. zhang et al. [14] employed
electromagnetic sensors to estimate the ureteroscope shape for
navigation.although these methods mentioned above work well, ureteroscopic
navigation is still a challenging problem. compared to other endoscopes such as
colonoscope and bronchoscope, the diameter of the ureteroscope is smaller,
resulting in more limited lighting source and field of view. particularly,
ureteroscopy involves much solids (impurities) and fluids (liquids), making
ureteroscopic video images low-quality, as well as these solids and fluids
inside the kidneys cannot be regularly observed in computed tomography (ct)
images. on the other hand, the complex internal structures such as calyx,
papilla, and pyramids of the kidneys are difficult to be observed in ct images.
these issues introduce a difficulty in directly aligning ureteroscopic video
sequences to ct images, leading to a challenge of image-based continuous
ureteroscopic navigation.this work aims to explore an accurate and robust
vision-based navigation method for furs procedures without using any external
positional sensors. based on ureteroscopic video images and preoperative
computed tomography urogram (ctu) images, we propose a novel video-ctu
registration method to precisely locate the flexible ureteroscope in the ctu
space. several highlights of this work are clarified as follows. to the best of
our knowledge, this work shows the first study to continuously track the
flexible ureteroscope in preoperative data using a vision-based method.
technically, we propose a novel 2d-3d (video-ctu) registration method that
introduces a structural point similarity measure without using image pixel
intensity information to characterize the difference between the structural
regions in real video images and ctu-driven virtual image depth maps.
additionally, our proposed method can successfully deal with solid and fluid
ureteroscopic video images and attains higher navigation accuracy than
intensity-based 2d-3d registration methods.",9
Cascade Transformer Encoded Boundary-Aware Multibranch Fusion Networks for Real-Time and Accurate Colonoscopic Lesion Segmentation,1.0,Introduction,"colorectal cancer (crc) is the third most commonly diagnosed cancer but ranks
second in terms of mortality worldwide [11]. intestinal lesions, particularly
polyps and adenomas, are usually developed to crc in many years. therefore,
diagnosis and treatment of colorectal polyps and adenomas at their early stages
are essential to reduce morbidity and mortality of crc. interventional
colonoscopy is routinely performed by surgeons to visually examine colorectal
lesions. however these lesions in colonoscopic images are easily omitted and
wrongly classified due to limited knowledge and experiences of surgeons.
automatic and accurate segmentation is a promising way to improve colorectal
examination.many researchers employ u-shaped network [7,13,18] for colonoscopic
polyp segmentation. resunet++ [7] combines residual blocks and atrous spatial
pyramid pooling and zhao et al. [18] designed a subtraction unit to generate the
difference features at multiple levels and constructed a training-free network
to supervise polyp-aware features. unlike a family of u-net driven segmentation
methods, numerous papers have been worked on boundary constraints to segment
colorectal polyps. fan et al. [2] introduced pranet with reverse attention to
establish the relationship between boundary cues from global feature maps
generated by a parallel partial decoder. both polyp boundary-aware segmentation
methods work well but still introduce much false positive. based on pranet [2]
and hardnet [1], huang et al. [6] removed the attention mechanism and replaced
res2net50 by hardnet to build hardnet-mseg that can achieve faster segmentation.
in addition, kim et al. [9] modified pranet to construct uacanet with parallel
axial attention and uncertainty augmented context attention to compute uncertain
boundary regions. although pranet and uacanet aim to extract ambiguous boundary
regions from both saliency and reverse saliency features, they simply set the
saliency score to 0.5 that cannot sufficiently detect complete boundaries to
separate foreground and background regions. more recently, shen et al. [10]
introduced task-relevant feature replenishment networks for crosscenter polyp
segmentation, while tian et al. [12] combined transformers and multiple instance
learning to detect polyps in a weakly supervised way.unfortunately, limited
field of view and illumination variations usually result in insufficient
boundary contrast between intestinal lesions and their surrounding tissues. on
the other hand, various polyps and adenomas with different pathological features
have similar visual characteristics to intestinal folds. to address these issues
mentioned above, we explore a new deep learning architecture called cascade
transformer encoded boundary-aware multibranch fusion (ctbmf) networks with
cascade transformers and multibranch fusion for polyp and adenoma segmentation
in colonoscopic white-light and narrow-band video images. several technical
highlights of this work are summarized as follows. first, we construct cascade
transformers that can extract global semantic and subtle boundary features at
different resolutions and establish weighted links between global semantic cues
and local spatial ones for intermediate reasoning, providing long-range
dependencies and a global receptive field for pixel-level segmentation. next, a
hybrid spatial-frequency loss function is defined to compensate for loss
features in the spatial domain but available in the frequency domain.
additionally, we built a new colonoscopic lesion image database and will make it
publicly available, while this work also conducts a thorough evaluation and
comparison on our new database and four publicly available ones (fig. 2).",9
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,1.0,Introduction,"medical imaging is essential during diagnosis, surgical planning, surgical
guidance, and follow-up for treating brain pathology. images from multiple
modalities are typically acquired to distinguish clinical targets from
surrounding tissues. for example, intra-operative ultrasound (ius) imaging and
magnetic resonance imaging (mri) capture complementary characteristics of brain
tissues that can be used to guide brain tumor resection. however, as noted in
[30], multi-modal data is expensive and sparse, typically leading to incomplete
sets of images. for example, the prohibitive cost of intra-operative mri (imri)
scanners often hampers the acquisition of imri during surgical procedures.
conversely, ius is an affordable tool but has been perceived as difficult to
read compared to imri [5]. consequently, there is growing interest in
synthesizing missing images from a subset of available images for enhanced
visualization and clinical training.medical image synthesis aims to predict
missing images given available images. deep-learning based methods have reached
the highest level of performance [29], including conditional generative
adversarial (gan) models [6,14,15,21] and conditional variational auto-encoders
[3]. however, a key limitation of these techniques is that they must be trained
for each subset of available images.to tackle this challenge, unified approaches
have been proposed. these approaches are designed to have the flexibility to
handle incomplete image sets as input, improving practicality as only one
network is used for generating missing images. to handle partial inputs, some
studies proposed to fill missing images with arbitrary values [4,17,18,24].
alternatively, other work aim at creating a common feature space that encodes
shared information from different modalities. feature representations are
extracted independently for each modality. then, arithmetic operations (e.g.,
mean [7,11,28], max [2] or a combination of sum, product and max [32]) are used
to fuse these feature representations. however, these operations do not force
the network to learn a shared latent representation of multi-modal data and lack
theoretical foundations. in contrast, multi-modal variational auto-encoders
(mvaes) provide a principled probabilistic fusion operation to create a common
representation space [8,30]. in mvaes, the common representation space is
low-dimensional (e.g., r 256 ), which usually leads to blurry synthetic images.
in contrast, hierarchical vaes (hvaes) [19,22,26,27] allow for learning complex
latent representations by using a hierarchical latent structure, where the
coarsest latent variable (z l ) represents global features, as in mvaes, while
the finer variables capture local characteristics. however, hvaeshave not yet
been extended to multi-modal settings to synthesize missing images.in this work,
we introduce multi-modal hierarchical latent representation vae (mhvae), the
first multi-modal vae approach with a hierarchical latent representation for
unified medical image synthesis. our contribution is four-fold.first, we
integrate a hierarchical latent representation into the multi-modal variational
setting to improve the expressiveness of the model. second, we propose a
principled fusion operation derived from a probabilistic formulation to support
missing modalities, thereby enabling image synthesis. third, adversarial
learning is employed to generate realistic image synthesis. finally, experiments
on the challenging problem of ius and mr synthesis demonstrate the effectiveness
of the proposed approach, enabling the synthesis of high-quality images while
establishing a mathematically grounded formulation for unified image synthesis
and outperforming non-unified gan-based approaches and the state-of-the-art
method for unified multi-modal medical image synthesis.",10
Unified Brain MR-Ultrasound Synthesis Using Multi-modal Hierarchical Representations,5.0,Discussion and Conclusion,"other potential applications. the current framework enables the generation of
ius data using t 2 mri data. since image delineation is much more efficient on
mri than on us, annotations performed on mri could be used to train a
segmentation network on pseudo-ius data, as performed by the top-performing
teams in the crossmoda challenge [9]. for example, synthetic ultrasound images
could be generated from the brats dataset [1], the largest collection of
annotated brain tumor mr scans. qualitative results shown in appendix
demonstrate the ability of our approach to generalize well to t 2 imaging from
brats. finally, the synthetic images could be used for improved ius and t 2
image registration.",10
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,1.0,Introduction,"2d-3d registration refers to the highly challenging process of aligning an input
2d image to its corresponding slice inside a given 3d volume [4]. it has
received growing attention in medical imaging due to the various contexts where
it applies, like image fusion between 2d real-time acquisitions and either
pre-operative 3d images for guided interventions or reference planning volumes
for patient positioning in radiation therapy (rt). another important task is the
volumetric reconstruction of a sequence of misaligned slices ex vivo, enabling
multimodal comparison toward improved diagnosis. in this respect, overlaying 3d
radiology and 2d histology could significantly enhance radiologists'
understanding of the links between tissue characteristics and radiologic signals
[9]. indeed, mri or ct scans are the baseline source of information for cancer
treatment but fail to provide an accurate assessment of disease proliferation,
leading to high variability in tumor detection [5,13,17]. on the other hand,
high-resolution digitized histopathology, called whole slide imaging (wsi),
provides cell-level information on the tumor environment from the surgically
resected specimens. however, the registration process is substantially difficult
due to the visual characteristics, resolution scale, and dimensional differences
between the two modalities. in addition, histological preparation involves
tissue fixation and slicing, leading to severe collapse and out-of-plane
deformations. (semi-)automated methods have been developed to avoid
time-consuming and biased manual mapping, including protocols with 3d mold or
landmarks [10,22], volume reconstruction to perform 3d registration
[2,18,19,23], or optimization algorithms for direct multimodal comparison
[3,15]. more recently, deep learning (dl) has been introduced but is limited to
2d/2d and requires prior plane selection [20]. on the other hand, successful dl
methods have been proposed to address the 2d/3d mapping problem for other
medical modalities [6,8,16,21]. however, given the extreme deformation that the
tissue undergoes during the histological process, additional guidance is needed.
one promising solution is to rely on rigid structures that are supposedly more
robust during the preparation. structural information to guide image
registration has been studied with the help of segmentations into the training
loop [11], or by learning new image representations for refined mapping [12].in
this paper, we propose to leverage the structural features of tissue and more
particularly the rigid areas to guide the registration process with two distinct
contributions: (1) a cascaded rigid alignment driven by stiff regions and
coupled with recursive plane selection, and (2) an improved 2d/3d deformable
motion model with distance field regularization to handle out-of-plane
deformation. to our knowledge, no previous study proposed 2d/3d registration
combined with structure awareness. we also use the cyclegan for image
translation and direct monomodal signal comparison [25]. like [14,24], we
combine registration with modality translation and integrate the two
aforementioned components. we demonstrate superior quantitative results for head
and neck (h&n) 3d ct and 2d wsis than traditional approaches failing due to the
histological constraints. in addition, we show that structuregnet performs
better than the state-of-the-art model from [14] on 3d ct/2d mr for the pelvis
in rt.",10
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,3.0,Experiments,"dataset and preprocessing. our clinical dataset consists of 108 patients for
whom were acquired both a pre-operative h&n ct scan and 4 to 11 wsis after
laryngectomy (with a total amount of 849 wsis). the theoretical spacing between
each slice is 5 mm, and the typical pixel size before downsampling is 100k ×
100k. two expert radiation oncologists on ct delineated both the thyroid and
cricoid cartilages for structure awareness and the gross tumor volume (gtv) for
clinical validation, while two expert pathologists did the same on wsis. they
then meet and agreed to place 6 landmarks for each slice at important locations
(not used for training). we ended up with images of size 256 × 256 (×64 for 3d
ct) of 1 mm isotropic grid space. we split the dataset patient-wise into three
groups for training (64), validation (20), and testing (24). to demonstrate the
performance of our model on another application, we also retrieved the datasets
from [14] for pelvis 3d ct/2d mr. it is made of 451 pairs between ct and
truefisp sequences, and 217 other pairs between ct and t2 sequences. we guided
the registration thanks to the rigid left/right femoral heads and computed
similarity metrics on the 7 additional organs at risk (anal canal, bladder,
rectum, penile bulb, seminal vesicle, and prostate). all masks were provided by
the authors and were originally segmented by internal experts.hyperparameters.
we drew our code from cyclegan and voxelmorph implementations with modifications
explained above, and we thank the authors of msv-regsynnet for making their code
and data available to us [1,14,25]. a detailed description of architectures and
hyperparameters can be found in the supplementary material. we implemented our
model with pytorch1.13 framework and trained for 600 (800 for mr/ct) epochs with
a batch size of 8 (4 for mr/ct) patients parallelized over 4 nvidia gtx 1080
tis.evaluation. we benchmarked our method against three baselines: first, to
assess the benefit of modality translation over the multimodal loss, we re-used
the original 3d voxelmorph model with mind as a multimodal metric for
optimization. we also modified this approach by masking the loss function to
account for the 2d-3d setting. next, we implemented the modality
translation-based msv-regsyn-net and modified it for our application to measure
the importance of joint structure-aware initialization and regularization.
finally, to differentiate the latter contributions, we tested two ablation
studies: without the cascaded rigid mapping or without the distance field
control. according to the mr/ct application in rt, we compared our model against
the state-of-the-art results of msv-regsynnet which were computed on the same
dataset.",10
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,4.1,Modality Translation,"three samples from the test set are displayed in fig. 1. from a qualitative
perspective, the densities of the different tissues are well reconstructed, with
rigid structures like cartilage being lighter than soft tissues or tumors. the
general shape of the larynx also complies with the original radiologic images.
we achieve a mean structural similarity (ssim) index of 0.76/1 between both
modalities, demonstrating the strong synthesis capabilities of our network
compared to msv-regsynnet and our ablative study without initialization process,
with an ssim of 0.72 (respectively 0.69). therefore, the cascaded rigid
initialization is crucial and helps the modality translation module in getting
more similar pairs of images for eased synthesis on the next pass.",10
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,4.2,Registration,"we present visual results in fig. 4. the initialization enables an accurate
plane selection as proved by the similar shape of cartilages in (b). even for
some severe difficulties inherent to the histological process like a cut larynx,
the model successfully maps both cartilage and soft tissue without completely
tearing the ct image thanks to regularization (c-d-e). for quantitative
assessment, we computed the dsc as well as the hausdorff distance between
cartilages, and the average distance between characteristic landmarks disposed
before registration(table 1).our method outperforms all baselines, proving the
necessity of a singular approach to handle the specific case of histology. the
popular voxelmorph framework fails, and the 2d-3d adaptation demonstrates the
value of the masked loss function. the superior performance of msv-regsynnet
advocates for a modality translation-based method compared to a direct
multimodal similarity criterion. in addition, the ablation studies prove the
benefit of the distance field regularization and more importantly the cascaded
initialization. concerning the gpu runtime, with a 3-step cascade for
initialization, the inference remains in a similar time scale to baseline
methods and performs mapping in less than 3s. we also compared against
msv-regsynnet on its own validation dataset for generalization assessment: we
yielded comparable results for the first cohort and significantly better ones
for the second, which proves that structuregnet behaves well on other modalities
and that the structure awareness is an essential asset for better registration,
as pelvis is a location where organs are moving. visuals of registration results
are displayed in the supplementary material. eventually, an important clinical
endpoint of our study is to compare the gtv delineated on ct with gold-standard
tumor extent after co-registration to highlight systematic errors and better
understand the biological environment from the radiologic signals. we show in
(f) that the gtv delineated on ct overestimates the true tumor extent of around
31%, but does not always encompass the tumor with a proportion of histological
tumor contained within the ct contour of 0.86. the typical error cases are the
inclusion of cartilage or edema, which highlights the limitations and
variability of radiology-based examinations, leading to increased toxicity or
untreated areas in rt.",10
StructuRegNet: Structure-Guided Multimodal 2D-3D Registration,5.0,Discussion and Conclusion,"we introduced a novel framework for 2d/3d multimodal registration.
struc-turegnet leverages the structure of tissues to guide the registration
through both initial plane selection and deformable regularization; it combines
adversarial training for modality translation with a 2d-3d mapping setting and
does not require any protocol for 3d reconstruction. it is worth noticing that
even if the annotation of cartilage was manual, automating this process is not a
bottleneck as the difference in contrast between soft tissue and stiff areas is
clear enough to leverage any image processing tool for this task. finally, it is
entirely versatile as we designed our experiments for ct-wsi but any 3d
radiological images are suitable. we achieve superior results than
state-of-the-art methods in dl-based registration in a similar time scale,
allowing precise mapping of both modalities and a better understanding of the
tumor microenvironment. the main limitation lies in the handling of organs
without any rigid areas like the prostate. future work also includes a study
with biomarkers from immunohistochemistry mapped onto radiology to go beyond
binary tumor masks and move toward virtual biopsy.",10
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,1.0,Introduction,"tomographic imaging estimates body density using hundreds of x-ray projections,
but it's slow and harmful to patients. acquisition time may be too high for
certain applications, and each projection adds dose to the patient. a quick,
low-cost 3d estimation of internal structures using only bi-planar x-rays can
revolutionize radiology, benefiting dental imaging, orthopedics, neurology, and
more. this can improve image-guided therapies and preoperative planning,
especially for radiotherapy, which requires precise patient positioning with
minimal radiation exposure.however, this task is an ill-posed inverse problem:
x-ray measurements are the result of attenuation integration across the body,
which makes them very fig. 1. current methods vs our method. feed-forward
methods do not manage to predict a detailed and matching tomographic volume from
a few projections. iterative methods based on neural radiance fields lack prior
for good reconstruction. by learning an embedding for the possible volumes, we
can recover an accurate volume from very few projections with an optimization
based on a bayesian formulation.ambiguous. traditional reconstruction methods
require hundreds of projections to get sufficient constraints on the internal
structures. with very few projections, it is very difficult to disentangle the
structures for even coarse 3d estimation. in other words, many 3d volumes may
have generated such projections a priori.classical analytical and iterative
methods [8] fail when very few projections are available. several works have
attempted to largely decrease the number of projections needed for an accurate
volumetric reconstruction. some deep learning methods [7,12,24,25,30] predict
directly a 3d volume in a forward way from very few projections. the volume is
however not guaranteed to be consistent with the projections and it is not clear
which solution is retrieved. other recent methods have adapted nerfs [20] to
tomographic reconstruction [23,31]. these non-learning methods show good results
when the number of input projections remains higher than a dozen but fail when
very few projections are provided, as our experiments in sect. 3.3 show.as
illustrated in fig. 1, to be able to reconstruct a volume accurately given as
low as two projections only, we first learn a prior on the volume. to do this,
we leverage the potential of generative models to learn a low-dimensional
manifold of the target body part. given projections, we find by a bayesian
formulation the intermediate latent vectors conditioning the generative model
that minimize the error between synthesized projections of our reconstruction
and these input projections. our work builds on hong et al. [10]'s 3d
style-based generative model, which we extend via a more complex network and
training framework.compared to other 3d gans, it is proven to provide the best
disentanglement of the feature space related to semantic features [2].by
contrast with feed-forward methods, our approach does not require paired
projections-reconstructions, which are very tedious to acquire, and it can be
used with different numbers of projections and different projection geometries
without retraining. compared to nerf-based methods, our method exploits prior
knowledge from many patients to require only two projections. we evaluate our
method on reconstructing cancer patients' head-and-neck cts, which involves
intricate and complicated structures. we perform several experiments to compare
our method with a feed-forward-based method [30] and a recent nerf-based method
[23], which are the previous state-of-the-art methods for the very few or few
projections cases, respectively.we show that our method allows to retrieve
results with the finest reconstructions and better matching structures, for a
variety of number of projections. to summarize, our contributions are two-fold:
(i) a new paradigm for 3d reconstruction with biplanar x-rays: instead of
learning to invert the measurements, we leverage a 3d style-based generative
model to learn deep image priors of anatomic structures and optimize over the
latent space to match the input projections; (ii) a novel unsupervised method,
fast and robust to sampling ratio, source energy, angles and geometry of
projections, all of which making it general for downstream applications and
imaging systems.",10
X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.1,Dataset and Preprocessing,"manifold learning. we trained our model with a large dataset of 3500 cts of
patients with head-and-neck cancer, more exactly 2297 patients from the publicly
available the cancer imaging archive (tcia) [1,6,16,17,28,32] and 1203 from
private internal data, after obtention of ethical approbations. we split this
data into 3000 cases for training, 250 for validation, and 250 for testing. we
focused ct scans on the head and neck region above shoulders, with a resolution
of 80 × 96 × 112, and centered on the mouth after automatic segmentation using a
pre-trained u-net [22]. the cts were preprocessed by min-max normalization after
clipping between -1024 and 2000 hounsfield units (hu).3d reconstruction. to
evaluate our approach, we used an external private cohort of 80 patients who had
undergone radiotherapy for head-and-neck cancer, with their consent. planning ct
scans were obtained for dose preparation, and cbct scans were obtained at each
treatment fraction for positioning with full gantry acquisition. as can be seen
in fig. 3 and the supplementary material, all these cases are challenging as
there are large changes between the original ct scan and the cbct scans. we
identified these cases automatically by comparing the cbcts with the planning
cts. to compare our reconstruction in the calibrated hu space, we registered the
planning cts on the cbcts by deformable registration with mrf minimization [4].
we hence obtained 3d volumes as virtual cts we considered as ground truths for
our reconstructions after normalization. from these volumes, we generated
projections using the projection module described in sect. 2.3.",10
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,1.0,Introduction,"magnetic resonance imaging (mri) and computed tomography (ct) are two commonly
used cross-sectional medical imaging techniques. mri and ct produce different
tissue contrast and are often used in tandem to provide complementary
information. while mri is useful for visualizing soft tissues (e.g. muscle, [20]
fails to preserve the smooth anatomy of the mri. (c) attentiongan [12] inflates
the head area in the synthetic ct, which is inconsistent with the original mri.
quantitative evaluations in mae (lower is better) are shown in yellow.fat), ct
is superior for visualizing bony structures. some medical procedures, such as
radiotherapy for brain tumors, craniosynostosis, and spinal surgery, typically
require both mri and ct for planning. unfortunately, ct imaging exposes patients
to ionizing radiation, which can damage dna and increase cancer risk [9],
especially in children and adolescents. given these issues, there are clear
advantages for synthesizing anatomically accurate ct data from mri.most
synthesis methods adopt supervised learning paradigms and train generative
models to synthesize ct [1][2][3]6,17]. despite the superior performance,
supervised methods require a large amount of paired data, which is prohibitively
expensive to acquire. several unsupervised mri-to-ct synthesis methods [4,6,14],
leverage cyclegan with cycle consistency supervision to eliminate the need for
paired data. unfortunately, the performance of unsupervised ct synthesis methods
[4,14,15] is inferior to supervised counterparts. due to the lack of direct
constraints on the synthetic outputs, cyclegan [20] struggles to preserve the
anatomical structure when synthesizing ct images, as shown in fig. 1(b). the
structural distortion in synthetic results exacerbates when data from the two
modalities are heavily misaligned, which usually occurs in pediatric scanning
due to the rapid growth in children.recent unsupervised methods impose
structural constraints on the synthesized ct through pixel-wise or shape-wise
consistency. pixel-wise consistency methods [8,14,15] capture and align
pixel-wise correlations between mri and synthesized ct. however, enforcing
pixel-wise consistency may introduce undesirable artifacts in the synthetic
results. this problem is particularly relevant in brain scanning, where both the
pixel-wise correlation and noise statistics in mr and ct images are different,
as a direct consequence of the signal acquisition technique. the alternative
shape-wise consistency methods [3,4,19] aim to preserve the shapes of major body
parts in the synthetic image. notably, shape-cyclegan [4] segments synthesized
ct and enforces consistency with the ground-truth mri segmentation. however,
these methods rely on segmentation annotations, which are time-consuming,
labor-intensive, and require expert radiological annotators. a recent natural
image synthesis approach, called attention-gan [12], learns attention masks to
identify discriminative structures. atten-tiongan implicitly learns prominent
structures in the image without using the ground-truth shape. unfortunately, the
lack of explicit mask supervision can lead to imprecise attention masks and, in
turn, produce inaccurate mappings of the anatomy, as shown in fig. 1(c). in this
paper, we propose maskgan, a novel unsupervised mri-to-ct synthesis method, that
preserves the anatomy under the explicit supervision of coarse masks without
using costly manual annotations. unlike segmentationbased methods [4,18],
maskgan bypasses the need for precise annotations, replacing them with standard
(unsupervised) image processing techniques, which can produce coarse anatomical
masks. such masks, although imperfect, provide sufficient cues for maskgan to
capture anatomical outlines and produce structurally consistent images. table 1
highlights our differences compared with previous shape-aware methods [4,12].
our major contributions are summarized as follows. 1) we introduce maskgan, a
novel unsupervised mri-to-ct synthesis method. maskgan is the first framework
that maintains shape consistency without relying on human-annotated
segmentation. 2) we present two new structural supervisions to enforce
consistent extraction of anatomical structures across mri and ct domains. 3)
extensive experiments show that our method outperforms state-of-the-art methods
by using automatically extracted coarse masks to effectively enhance structural
consistency.",10
Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,3.1,Experimental Settings,"data collection. we collected 270 volumetric t1-weighted mri and 267 thinslice
ct head scans with bony reconstruction performed in pediatric patients under
routine scanning protocols1 . we targeted the age group from 6-24 months since
pediatric patients are more susceptible to ionizing radiation and experience a
greater cancer risk (up to 24% increase) from radiation exposure [7].
furthermore, surgery for craniosynostosis, a birth defect in which the skull
bones fuse too early, typically occurs during this age [5,16]. the scans were
acquired by ingenia 3.0t mri scanners and philips brilliance 64 ct scanners. we
then resampled the volumetric scans to the same resolution of 1.0 × 1.0 × 1.0 mm
3 . the dataset comprises brain mr and ct volumes from 262 subjects. 13 mri-ct
volumes from the same patients that were captured less than three months apart
are registered using rigid registration algorithms. the dataset is divided into
249, 1 and 12 subjects for training, validating and testing set. following [13],
we conducted experiments on sagittal slices. each mr and ct volume consists of
180 to 200 slices, which are resized and padded to the size of 224 × 224. the
intensity range of ct is clipped into [-1000, 2000]. all models are trained
using the adam optimizer for 100 epochs, with a learning rate of 0.0002 which
linearly decays to zero over the last 50 epochs. we use a batch size of 16 and
train on two nvidia rtx 3090 gpus.evaluation metrics. to provide a quantitative
evaluation of methods, we compute the same standard performance metrics as in
previous works [6,14] including mean absolute error (mae), peak signal-to-noise
ratio (psnr), and structural similarity (ssim) between ground-truth and
synthesized ct. the scope of the paper centers on theoretical development;
clinical evaluations such as dose calculation and treatment planning will be
conducted in future work.",10
FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction,1.0,Introduction,"x-ray computed tomography (ct) is an established diagnostic tool in clinical
practice; however, there is growing concern regarding the increased risk of
cancer induction associated with x-ray radiation exposure [14]. lowering the
dose of ct scans has been widely adopted in clinical practice to address this
issue, following the ""as low as reasonably achievable"" (alara) principle in the
medical community [9]. sparse-view ct is one of the effective solutions, which
reduces the radiation by only sampling part of the projection data for image
reconstruction. nevertheless, images reconstructed by the conventional filtered
back-projection (fbp) present severe artifacts, thereby compromising their
clinical value.in recent years, the success of deep learning has attracted much
attention in the field of sparse-view ct reconstruction. existing learning-based
approaches mainly include image-domain methods [2,4,18] and dual-domain ones
[7,13,16], both involving image post-processing to restore a clean ct image from
the low-quality one with streak artifacts. for the image post-processing,
residual learning [3] is often employed to encourage learning the artifacts
hidden in the residues, which has become a proven paradigm for enhancing the
performance [2,4,6,16]. unfortunately, existing image post-processing methods
may fail to model the globally distributed artifacts within the image domain.
they can also produce over-smoothed images due to the lack of differentiated
supervision for each pixel. in this paper, we advance image post-processing to
benefit both classical image-domain methods and the dominant dual-domain
ones.motivation. we view the sparse-view ct image reconstruction as a two-step
task: artifact removal and detail recovery. for the former, few work has
investigated the fact that the artifacts exhibit similar pattern across
different sparseview scenarios, which is evident in fourier domain as shown in
fig. 1: they are aggregated mainly in the mid-frequency band and gradually
migrate from low to high frequencies as the number of views increases. inspired
by this, we propose a frequency-band-aware artifact modeling network (freenet)
that learns the artifact-concentrated frequency components to remove the
artifacts efficiently using learnable band-pass attention maps in the fourier
domain.while fourier domain band-pass maps help capture the pattern of the
artifacts, restoring the image detail contaminated by strong artifacts may still
be difficult due to the entanglement of artifacts and details in the residues.
consequently, we propose a self-guided artifact refinement network (seednet)
that provides supervision signals to aid freenet in refining the image details
contaminated by the artifacts. with these novel designs, we introduce a simple
yet effective model termed frequency-band-aware and self-guided network
(freeseed), which enhances the reconstruction by modeling the pattern of
artifacts from a frequency perspective and utilizing the artifact to restore the
details. freeseed achieves promising results with only image data and can be
further enhanced once the sinogram is available.our contributions can be
summarized as follows: 1) a novel frequency-bandaware network is introduced to
efficiently capture the pattern of global artifacts in the fourier domain among
different sparse-view scenarios; 2) to promote the restoration of heavily
corrupted image detail, we propose a self-guided artifact refinement network
that ensures targeted refinement of the reconstructed image and consistently
improves the model performance across different scenarios; and 3) quantitative
and qualitative results demonstrate the superiority of freeseed over the
state-of-the-art sparse-view ct reconstruction methods.",10
Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,0.0,3D Pseudo Brain MRI.,"to evaluate the performance of atlas-based registration, it is essential to have
the correct mapping of pathological regions to healthy brain regions. to create
such a mapping, we created a pseudo dataset by utilizing images from the oasis-1
and brats2020. from the resulting t1 sequences, a pseudo dataset of 300 images
was randomly selected for further analysis. appendix b provides a detailed
process for creating the pseudo dataset.real data with landmarks. brats-reg 2022
[2] provides extensive annotations of landmarks points within both the
pre-operative and the follow-up scans that have been generated by clinical
experts. a total of 140 images are provided, of which 112 are for training, and
28 for testing. comparison to pathology registration. we compared our method
(gir-net) with competitive algorithms: 1) three cutting-edge deep learning-based
unsupervised deformable registration approaches: voxelmorph [3], voxelmorph-df
[8] and symnet [14]. 2) two unsupervised deformable registration methods for
pathological images: dramms [18] and dirac [16]. dramms is an optimization-based
method that reduces the impact of non-corresponding regions. dirac jointly
estimates regions with absent correspondence and bidirectional deformation
fields and ranked first in the bratsreg2022 challenge.atlas-based registration.
after creating the pseudo dataset, we warped brain mr images without tumors to
the atlas and used the resulting deformation field as the gold standard for
evaluation. we then evaluated the mean deformation error (mde) [10], which is
calculated as the average euclidean distance between the coordinates of the
deformation field and the gold standard within specific regions of interest.
these regions include: 1) the tumor region. 2) the normal region near the tumor
(within 30 voxels). 3) the normal region far from the tumor (over 30 voxels but
within brain tissue). our results, presented in fig. 2, show that our method
with histogram matching (hm) outperforms other methods in all three regions,
particularly in the normal regions (near and far). by utilizing hm, our network
achieves an mde of less than 1 mm compared to the gold standard deformations.
these results demonstrate the effectiveness of our method in differentiating the
impact of pathology in atlas-based registration tasks. specifically, dirac is
unable to eliminate the influence of domain differences and resulting in the
largest registration error among the evaluated methods.longitudinal
registration. to perform the longitudinal registration task, we registered each
pre-operative scan to the corresponding follow-up scan of the same patient and
measured the mean target registration error (tre) of the paired landmarks using
the resulting deformation field. for this purpose, we leveraged segnet, trained
on brats2020, to segment the tumor of brat-sreg2022 and separated the landmarks
into two regions: near tumor and far from tumor. figure 3 shows the mean tre for
the various registration approaches. in our proposed framework, we replaced
regnet with cir-dm [15] (denoted as gir(cirdm)) without the need for supervised
training or pretraining, and achieved comparable performance with the
state-of-the-art method dirac. moreover, our gir approach outperforms other deep
learning-based methods and achieved accurate segmentation of pathological
images.to quantitatively evaluate the segmentation capability of our proposed
framework, we compared its performance with other unsupervised segmentation
techniques methods, including unsupervised clustering toolbox aucseg [25], joint
non-correspondence segmentation and registration method ncrnet [1], and dirac.
we used the mean dice similarity coefficient (dsc) to evaluate the similarity
between predicted masks and the ground truth. as shown in table 1, aucseg fails
to detect the lesion in t1 scans. our proposed framework achieved the highest
dsc result of 0.83, following post-processing.ablation study. we compared the
performance of the inpnet trained with histogram matching (hm) and the segnet
trained with ground truth masks (supervised). the results, shown in table 1 and
fig. 2, demonstrate that domain differences between s and t have a significant
effect on segmentation accuracy (without hm), leading to lower registration
quality overall. additionally, fig. 4 shows an example of a pseudo image. we
reconstructed the spatial correspondence by first using segnet to localize the
lesion and then using inpnet to inpaint it with the normal appearance.",10
Fast Reconstruction for Deep Learning PET Head Motion Correction,1.0,Introduction,"positron emission tomography (pet) has been widely used in human brain imaging,
thanks to the availability of a vast array of specific radiotracers. these
compounds allow for studying various neurotransmitters and receptor dynamics for
different brain targets [11]. brain pet images are commonly used to diagnose and
monitor neurodegenerative diseases, such as alzheimer's disease, parkinson's
disease, epilepsy, and certain types of brain tumors [3]. head motion in pet
imaging reduces brain image resolution, lowers tracer distribution estimation,
and introduces attenuation correction (ac) mismatch artifacts [12].
consequently, the capability to monitor and correct head motion is of utmost
importance in brain pet studies.the first step of pet head motion correction is
motion tracking. when head motion information is acquired, either frame-based
motion correction or eventby-event (ebe) motion correction methods can be
applied in the reconstruction workflow to derive motion-free pet images. ebe
motion correction provides better results for real-time motion tracking compared
to frame-based methods, as the latter does not allow for correction of motion
that occurs within each dynamic frame [1]. currently, there are two main
categories of head motion tracking methods, hardware-based motion tracking (hmt)
and data-driven methods. for hmt, head motion is obtained from external devices.
generally, hmt systems offer accurate tracking results with high time
resolution. marker-based hmt such as polaris vicra (ndi, canada) use
light-reflecting markers on the patient's head and track the markers for motion
correction [6]. however, vicra is not routinely used in the clinic, as setup and
calibration of the tracking device can be complicated and attaching markers to
each patient increases the logistical burden of the scan. in response, some
researchers began to use markerless motion tracking systems for brain pet
[4,13]. these methods typically rely on the use of cameras and computer vision
algorithms to detect and analyze the movement of a person's head in real-time,
but these methods still require additional hardware setup. in data-driven motion
tracking methods, head motion is estimated from pet reconstructions or raw data.
with the development of commercial pet systems and technological advancements
such as time of flight (tof), data-driven head pet motion tracking has shown
promising results in reducing motion artifacts and improving image quality. for
instance, [12] developed a novel data-driven head motion detection method based
on the centroid of distribution (cod) of pet 3d point cloud image (pci). image
registration methods that seek to align two or more images offer a data-driven
solution for correcting head motion. intensity-based registration methods have
been used to track head motion using good-quality pet reconstruction frames to
achieve stable performance [14]. however, because of the dynamic change in pet
images, current registration-based methods need to split the data into several
discrete time frames, e.g., 5 min. therefore, they will introduce a cumulative
error when dealing with inter-frame motion. finally, inspired by the development
of deep learning-based registration methods, a deep learning head motion
correction (dl-hmc) network using vicra as ground truth was proposed [15]. this
study achieved accurate motion tracking on single subject testing data, but
showed less accurate motion predictions for multi-subject motion studies.
meanwhile, the input images were low-resolution pcis without tof and had large
voxel spacing, which can negatively affect motion tracking accuracy.in this
study, we proposed a new method to perform deep learning-based brain pet motion
prediction across multiple subjects by utilizing high-resolution one-second fast
reconstruction images (fris) with tof. a novel encoder and data augmentation
strategy was also applied to improve model performance. ablation studies were
conducted to assess the individual contributions of key method components.
multi-subject studies were conducted on a dataset of 20 subject and its results
were quantitatively and qualitatively evaluated by molar reconstruction studies
and corresponding brain region of interest (roi) standard uptake values (suv)
evaluation.",10
Revealing Anatomical Structures in PET to Generate CT for Attenuation Correction,3.1,Materials,"the data used in our experiments are collected from the cancer image archive
(tcia) [4] (https://www.cancerimagingarchive.net/collections/), where a series
of public datasets with different types of lesions, patients, and scanners are
open-access. among them, 401, 108, 46, and 20 samples are extracted from the
head and neck scamorous cell carcinoma (hnscc), non-small cell lung cancer
(nsclc), the cancer genome atlas (tcga) -head-neck squamous cell carcinoma
(tcga-hnsc), and tcga -lung adenocarcinoma (tcga-luad), respectively. we use
these samples in hnscc for training and in other three datasets for
evaluation.each sample contains co-registered (acquired with pet-ct scans) ct,
pet, and nac-pet whole-body scans. in our experiments, we re-sampled all of them
to a voxel spacing of 2×2×2 and re-scaled the intensities of nac-pet/ac-pet
images to a range of [0, 1], of ct images by multiplying 0.001. the input and
output of our aseg framework are cropped patches with the size of 192 × 192 ×
128 voxels. to achieve full-fov output, the consecutive outputs of each sample
are composed into a single volume where the overlapped regions are averaged.",10
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.1,Dataset and Evaluation Metrics,"we use brain mri images of 1,251 subjects from brain tumor segmentation 2021
(brats2021) [3,4,17], which includes four aligned sequences, t1, t1gd, t2, and
flair, for each subject. we select 830 subjects for training, 93 for validation,
and 328 for testing. all the images are intensity normalized to [-1, 1] and
central cropped to 128 × 192 × 192. during training, for each subject, a random
number of sequences are selected as inputs and the rest as targets. for
validation and testing, we fixed the input combinations and the target for each
subject. the synthesis performance is quantified using the metrics of peak
signal noise rate (psnr), structural similarity index measure (ssim), and
learned perceptual image patch similarity (lpips) [21], which evaluate from
intensity, structure, and perceptual aspects.",10
An Explainable Deep Framework: Towards Task-Specific Fusion for Multi-to-One MRI Synthesis,3.5,Interpretability Visualization,"the proposed method not only achieves superior synthesis performance but also
has good interpretability. in this section, we will visualize the contribution
of different input combinations and tsem. sequence contribution. we use ω in eq.
2 to quantify the contribution of different input combinations for synthesizing
different target sequences. figure 3 shows the bar chart for the sequence
contribution weight ω with different taskspecific code c. as shown in fig. 3,
both t1 and t1gd contribute greatly to the sequence synthesis of each other,
which is expected because t1gd are t1weighted scanning after contrast agent
injection, and the enhancement between these two sequences is indispensable for
cancer detection and diagnosis. the less contribution of t2, when combined with
t1 and/or t1gd, is consistent with the clinical findings [22,23] that t2 can be
well-synthesized by t1 and/or t1gd.tsem vs. attention map. figure 4 shows the
proposed tsem and the attention maps extracted by resvit [9]. as shown in fig.
4, tsem has a higher resolution than the attention maps and can highlight the
tumor area which is hard to be synthesized by the networks. table 2 reports the
results of psnr for regions highlighted or not highlighted by tsem with a
threshold of the 99th percentile. to assist the synthesis models deploying in
clinical settings, tsem can be used as an attention and uncertainty map to
remind clinicians of the possible unreliable synthesized area.",10
Geometric Ultrasound Localization Microscopy,1.0,Introduction,"ultrasound localization microscopy (ulm) has revolutionized medical imaging by
enabling sub-wavelength resolution from images acquired by piezo-electric
transducers and computational beamforming. however, the necessity of beamforming
for ulm remains questionable. our work challenges the conventional assumption
that beamforming is the ideal processing step for ulm and presents an
alternative approach based on geometric reconstruction from time-of-arrival
(toa) information.the discovery of ulm has recently surpassed the
diffraction-limited spatial resolution and enabled highly detailed visualization
of the vascularity [8]. ulm borrows concepts from super-resolution fluorescence
microscopy techniques to precisely locate individual particles with sub-pixel
accuracy over multiple frames. by the accumulation of all localizations over
time, ulm can produce a superresolved image, providing researchers and
clinicians with highly detailed representation of the vascular structure.while
contrast-enhanced ultra-sound (ceus) is used in the identification of
musculoskeletal soft tissue tumours [5], the far higher resolution capability
offered by ulm has great potential for clinical translation to improve the
reliability of cancer diagnosis (i.e., enable differentiation of tumour types in
kidney cancer [7] or detect breast cancer tissue [1]). moreover, ulm has shown
promise in imaging neurovascular activity after visual stimulation (functional
ulm) [14]. the pioneering study by errico et al. [8] initially demonstrated the
potential of ulm by successfully localizing contrast agent particles
(microbubbles) using a 2d point-spread-function model. in general, the accuracy
in microbubble (mb) localization is the key to achieving sub-wavelength
resolution [4], for which classical imaging methods [11,17], as well as deep
neural networks [1,16], have recently been reported. however, the conventional
approach for ulm involves using computational beamformers, which may not be
ideal for mb localization. for example, a recent study has shown that ultrasound
image segmentation can be learned from radiofrequency data and thus without
beamforming [13]. beamforming techniques have been developed to render irregular
topologies, whereas mbs exhibit a uniform geometric structure, for which ulm
only requires information about its spatial position. although the impact of
adaptive beamforming has been studiedfor ulm to investigate its potential to
refine mb localization [3], optimization of the point-spread function (psf)
poses high demands on the transducer array, data storage, and algorithm
complexity.to this end, we propose an alternative approach for ulm, outlined in
fig. 1, that entirely relies on time-difference-of-arrival (tdoa) information,
omitting beamforming from the processing pipeline for the first time. we
demonstrate a novel geometry framework for mb localization through ellipse
intersections to overcome limitations inherent to beamforming. this approach
provides a finer distinction between overlapping and clustered spots, improving
localization precision, reliability, and computation efficiency. in conclusion,
we challenge the conventional wisdom that beamforming is necessary for ulm and
propose a novel approach that entirely relies on tdoa information for mb
localization. our proposed approach demonstrates promising results and indicates
a considerable trade-off between precision, computation, and memory.",10
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,1.0,Introduction,"near-infrared (nir) fluorescence imaging can allow the detection of fluorophores
up to 4 cm depth in tissue [11]. recently, with the availability of clinically
approved nir fluorophores such as indocyanine green or icg, fluorescence imaging
is increasingly being employed for intra-operative guidance during surgically
excision of malignant tumors and lymph nodes [6,15,16]. fluorescence imaging is
also a workhorse for small animal or preclinical research with multiple
commercial devices utilizing sensitive front or back-illuminated and cooled ccd
camera detectors available at prices ranging from 250-600k usd [9,14].a majority
of fluorescence imaging applications including fluorescence guided surgery (fgs)
rely upon visible 2d surface imaging [5,8,17,20] while reconstruction of the
invisible 3d target in tissue is not widely used for reflectance mode imaging
despite a large number of publications in 3d fluorescence diffuse optical
tomography (fdot) since early 1990s [1,4,10,18,19]. the primary cause of this
impasse is the ill-posedness of the mathematical inverse problem underlying the
3d reconstruction of the target in tissue from boundary measurements.the prime
motivation of our work is to enable an efficient 3d tumor shape reconstruction
for fgs in an operating room environment, where we do not have full control of
the ambient light and we cannot rely on sophisticated time or frequency domain
imaging instrumentation and setup. in these situations, one has to use clinical
cameras producing rapid continuous wave (cw) fluorescence boundary measurements
[19] in reflectance mode (i.e., the transmission of the light through the domain
is not measured), and with low signal-to-noise ratio which further exacerbates
the ill-posedness of fdot problem. the standard approach for solving fdot
problem with cw measurements is based on born approximation which works well in
the case of a small compared to the computational domain target and a very large
number of reflectance-transmission type measurements made by ""slow in
acquisition"" light sources and detector arrays of highly sensitive cooled ccd
cameras or photomultiplier tube arrays collecting both reflected and transmitted
light [18]. none of these is suitable for fgs settings where time is limited,
just a few reflectance mode cw-measurements are available, and the target can be
large compared to the imaged domain.we propose an incremental fluorescent target
reconstruction (iftr) scheme, based on the recent advances in quadratic and
conic convex optimization and sparse regularization, which can recover a
relatively large 3d target in tissuelike media. in our experiments, iftr scheme
demonstrates accurate reconstruction of 3d targets from reflectance mode
cw-measurements collected at the top surface of the domain. to our best
knowledge, this is the first report where the 3d shape of tumor-like target has
been recovered from reflectance mode steady-state cw measurements. previously
such results were reported in fdot literature only for time-consuming
frequency-domain or time-domain measurements [12] where photon path-length
information is available. moreover, the data is acquired almost instantly by an
inexpensive (<100 euros) camera with flexible fiber-optics making it suitable
for endoscopic fgs in contrast to the standard slow in acquisition
frequency-based measurements obtained by expensive (usd100k+ range) stationary
cameras. lastly, iftr scheme is implemented using fenics [3], a highlevel python
package for fem discretization of the physical model, and cvxpy [2,7], a convex
optimization package making this method easy to reuse/adjust for a different
setup. the code and data produced for this work are released as an open source
at https://github.com/ibm/dot.",10
Reflectance Mode Fluorescence Optical Tomography with Consumer-Grade Cameras,2.0,Methods,"figure 1 describes the setup representing a typical surgical field while
excising tumors. we simulate the provision of 3d surgical guidance via a
flexible endo- scope type fluorescence imager. for such provision we need to
solve the following fdot problem: estimate the spatial shape χ of the icg tagged
tumor target (the cube in green) within the tissue domain ω ∈ r 3 (the area in
grey) from measurements y. measurements are obtained by illuminating the tissue
domain with nir light at icg peak excitation wavelength via the expanded beam
from endoscope fiber bundle, and then measuring the light emitted by the tumor
like target diffusing to the top face of the phantom surface ∂ω obs , by the
fiber bundle with suitable emission filter which is coupled to a camera at the
backend.in this section we briefly describe the mathematical formulation of the
fdot problem and introduce the iftr scheme for solving it.forward and inverse
problems. photon propagation in tissue-like media is described by a coupled
system of elliptic partial differential equations (pdes) for determining photon
fluence φ (w/cm 2 ) at excitation and fluorescence emission wavelengths through
out the domain. wavelength and space dependent absorption and scattering
coefficients and fluorophore properties comprise the coefficients of this pde
system (see appendix a). the discretization of coupled diffusion pdes is
obtained by applying a standard fem methodology [13]: domain ω is covered by a
uniform grid comprised of n nodes {x i } n i=1 ; each function φ is approximated
by a vector φ ∈ r n with components φ i = φ(x i ); pdes are approximated using
weak formulations incorporating boundary conditions. this results in a system of
algebraic equations:where the first equation describes the excitation photon
fluence φ x ∈ r n , and the second describes photon emission fluence φ m ∈ r n ;
subscripts x and m indicate excitation and emission respectively. vectors f , χ
∈ r n are the source of excitation light and target's shape indicator, i.e., a
binary vector such that χ i = 1 if x i belongs to the target and 0 otherwise. s
x/m (•) ∈ r n ×n are the stiffness matrices obtained by discretizing the
diffusion terms of excitation/emission pdes respectively and additionally s x
depends on χ. m ∈ r n ×n is the mass matrix and denotes hadamard (elementwise)
product such that m χ = m diag(χ) and m χφ x = m φ x χ. finally, vector of
measurements y ∈ r k is related to the emission fluence φ m as followshere t ∈ r
k×n is a binary matrix that selects components of φ m corresponding to the
observed grid nodes and k is a number of observed nodes.in the following if
target indicator χ is given then the system (1) is referred to as the forward
fdot problem to compute unknown excitation and emission fluence φ x , φ m . if
vector χ is unknown but measurements of emission fluence are present then the
system (1)-( 2) is referred to as the fdot inverse problem.",10
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.1,Affine Registration of Brain US-MR,"in this experiment, we evaluate the performance of different methods for
estimating affine registration of the retrospective evaluation of cerebral
tumors (resect) miccai challenge dataset [23]. this dataset consists of 22 pairs
of pre-operative brain mrs and intra-operative ultrasound volumes. the initial
pose of the ultrasound volumes exhibits an orientation close to the ground truth
but can contain a significant translation shift. for both mind-ssc and disa-lc 2
, we resample the input volumes to 0.4 mm spacing and use the bfgs [18]
optimizer with 500 random initializations within a range of ±10 • and ±25 mm. we
report the obtained fiducial registration errors (fre) in table 1. disa-lc 2 is
significantly better than mind-ssc while the difference with lc 2 is not
significant. in conclusion, our experiments demonstrate that the proposed
disa-lc 2 , combined with a simple optimization strategy, is capable of
achieving equivalent performance to manually tuned lc 2 .",10
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration,4.3,Deformable Registration of Abdominal US-CT and US-MR,"as the most challenging experiment, we finally use our method to achieve
deformable registration of abdominal 3d freehand us to a ct or mr volume.we are
using a heterogeneous dataset of 27 cases, comprising liver cancer patients and
healthy volunteers, different ultrasound machines, as well as optical vs.
electro-magnetic external tracking, and sub-costal vs. inter-costal scanning of
the liver. all 3d ultrasound data sets are accurately calibrated, with overall
system errors in the range of commercial ultrasound fusion options. between 4
and 9 landmark pairs (vessel bifurcations, liver gland borders, gall bladder,
kidney) were manually annotated by an expert. in order to measure the capture
range, we start the registration from 50 random rigid poses around the ground
truth and calculate the fiducial registration error (fre) after optimization.
for local optimization, lc 2 is used in conjunction with bobyqa [15] as in the
original paper [22], while mind-scc and disa-lc 2 are instead used with bfgs.
due to an excessive computation time, we don't do global optimization with lc 2
while with other methods we use bfgs with 500 random initializations within a
range of ±40 • and ±150 mm. we use six parameters to define the rigid pose and
two parameters to describe the deformation caused by the ultrasound probe
pressure.from the results shown in table 3 and fig. 2, it can be noticed that
the proposed method obtains a significantly larger capture range than mind-scc
and lc 2 while being more than 300 times faster per evaluation than lc 2 (the
times reported in the table include not just the optimization but also
descriptor extraction). the differentiability of our objective function allows
our method to converge in fewer iterations than derivative-free methods like
bobyqa. furthermore, the evaluation speed of our objective function allows us to
exhaustively search the solution space, escaping local minima and converging to
the correct solution with pose and deformation parameters at once, in less than
two seconds.note that this registration problem is much more challenging than
the prior two due to difficult ultrasonic visibility in the abdomen, strong
deformations, and ambiguous matches of liver vasculature. therefore, to the best
of our knowledge, these results present a significant leap towards reliable and
fully automatic fusion, doing away with cumbersome manual landmark placements.",10
Solving Low-Dose CT Reconstruction via GAN with Local Coherence,4.0,Experiment,"datasets. first, our proposed approaches are evaluated on the ""mayo-clinic
low-dose ct grand challenge"" (mayo-clinic) dataset of lung ct images [19].the
dataset contains 2250 two dimensional slices from 9 patients for training, and
the remaining 128 slices from 1 patient are reserved for testing. the lowdose
measurements are simulated by parallel-beam x-ray with 200 (or 150) uniform
views, i.e., n v = 200 (or n v = 150), and 400 (or 300) detectors, i.e., n d =
400 (or n d = 300). in order to further verify the denoising ability of our
approaches, we add the gaussian noise with standard deviation σ = 2.0 to the
sinograms after x-ray projection in 50% of the experiments. to evaluate the
generalization of our model, we also consider another dataset rider with
nonsmall cell lung cancer under two ct scans [36] for testing. we randomly
select 4 patients with 1827 slices from the dataset. the simulation process is
identical to that of mayo-clinic. the proposed networks were implemented in the
pytorch framework and trained on nvidia 3090 gpu with 100 epochs.baselines and
evaluation metrics. we consider several existing popular algorithms for
comparison. ( 1) fbp [11]: the classical filter backward projection on low-dose
sinograms. ( 2) fbpconvnet [10]: a direct inversion network followed by the cnn
after initial fbp reconstruction. ( 3) lpd [1]: a deep learning method based on
proximal primal-dual optimization. ( 4) uar [21]: an end-toend reconstruction
method based on learning unrolled reconstruction operators and adversarial
regularizers. our proposed method is denoted by gan-lc.we set λ pix = 1.0, λ adv
= 0.01 and λ per = 1.0 for the optimization objective in eq. ( 7) during our
training process. following most of the previous articles on 3d ct
reconstruction, we evaluate the experimental performance by two metrics: the
peak signal-to-noise ratio (psnr) and the structural similarity index (ssim)
[32]. psnr measures the pixel differences of two images, which is negatively
correlated with mean square error. ssim measures the structure similarity
between two images, which is related to the variances of the input images. for
both two measures, the higher the better.results. a similar increasing trend
with our approach across different settings but has worse reconstruction
quality. to evaluate the stability and generalization of our model and the
baselines trained on mayo-clinic dataset, we also test them on the rider
dataset. the results are shown in table 2. due to the bias in the datasets
collected from different facilities, the performances of all the models are
declined to some extents. but our proposed approach still outperforms the other
models for most testing cases.to illustrate the reconstruction performances more
clearly, we also show the reconstruction results for testing images in fig. 3.
we can see that our network can reconstruct the ct image with higher quality.
due to the space limit, the experimental results of different views n v and more
visualized results are placed in our supplementary material.",10
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,1.0,Introduction,"radiotherapy (rt) is one of the cornerstones of cancer patients. it utilizes
ionizing radiation to eradicate all cells of a tumor. the total radiation dose
is typically divided over 3-30 daily fractions to optimize its effect. as the
surrounding normal tissue is also sensitive to radiation, highly accurate
delivery is vital. image guided rt (igrt) is a technique to capture the anatomy
of the day using in room imaging in order to align the treatment beam with the
tumor location [1]. cone beam ct (cbct) is the most widely used imaging modality
for igrt.a major challenge especially for cbct imaging of the thorax and
upperabdomen is the respiratory motion that introduces blurring of the anatomy,
reducing the localization accuracy and the sharpness of the image.a technique
used to alleviate motion artifacts is respiratory correlated cbct (4dcbct) [16].
from the projections, it is possible to extract a respiratory signal [12], which
indicates the position of the organs within the patient during breathing. with
this, subsets of the projections can be defined to create reconstructions that
resolve the motion. however, only 20 to 60 respiratory periods are imaged. this
limits the number of projections available and results in view-aliasing [16].
additionally, the projections are affected by stochastic measurement noise
caused by the finite imaging dose used, which further degrades the quality of
the reconstruction even when all projections are used.several traditional
methods based on iterative reconstruction algorithms and motion compensation
techniques are used to reduce view-aliasing in 4dcbcts [7,10,11,14,15]. although
effective, these methods suffer from motion modeling uncertainty and prolonged
reconstruction times.deep learning has been proposed as a way to address
view-aliasing with accelerated reconstruction [6]. however, the method cannot
reduce measurement noise because it is still present in the images used as
targets during training.a different method, called noise2inverse, uses an
unsupervised approach to reduce measurement noise in the traditional ct setting
[4]. there are two ways to apply it to 4dcbct and both fail to reduce stochastic
noise effectively. the first is to apply noise2inverse to each
respiratory-correlated reconstruction. in this case, the method will struggle
because of the very low number of projections that are available. the second is
to apply noise2inverse directly to all the projections. in this case, the motion
artifacts that blur the image will appear again, as noise2inverse requires
averaging the sub-reconstructions to obtain a clean reconstruction.we propose
noise2aliasing to address these limitations. the method can be used to provably
train models to reduce both view-aliasing artifacts and stochastic noise from
4dcbcts in an unsupervised way. training deep learning models for medical
applications often needs new data. this was not the case for noise2aliasing, and
historical clinical data sufficed for training.we validated our method on
publicly available data [15] against a supervised approach [6] and applied it to
an internal clinical dataset of 30 lung cancer patients. we explore different
dataset sizes to understand their effects on the reconstructed images.",10
Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,4.0,Experiments,"first, we used the spare varian dataset to study whether noise2aliasing can
match the performance of the supervised baseline and if it can outperform it
when adding noise to the projections. then, we use the internal dataset to
explore the requirements for the method to be applied to an existing clinical
dataset. these required around 64 gpu days on nvidia a100 gpus.training of the
model is done on 2d slices. the projections obtained during a scan are
sub-sampled according to the pseudo-average subset selection method described in
[6] and then used to obtain 3d reconstructions. in noise2aliasing these are used
for both input and target during training. given two volumes (x, y), the
training pairs (x i (k) , y i (k) ) are the same i-th slice along the k-th
dimension of each volume chosen to be the axial plane.the datasets used in this
study are two:1. the spare varian dataset was used to provide performance
results on publicly available patient data. to more closely resemble normal
respiratory motion per projection image, the 8 min scan has been used from each
patient (five such scans are available in the dataset). training is performed
over 4 patients while 1 patient is used as a test set. the hyperparameters are
optimized over the training dataset.2. an internal dataset (irb approved) of 30
lung cancer patients' 4dcbcts from 2020 to 2022, originally used for igrt, with
25 patients for training and 5 patients for testing. the scans are 4 min 205 •
scans with 120kev source and 512 × 512 sized detector, using elekta linacs. the
data were anonymized prior to analysis.projection noise was added using the
poisson distribution to the spare varian dataset to evaluate the ability of the
unsupervised method to reduce it. given a projected value of p and a photon
count π (chosen to be 2500), the rate of the poisson distribution is defined as
πe -p and given a sample q from this distribution, then the new projected value
is p =log q π .the architecture used in this work is the mixed scale dense cnn
(msd) [8], the most successful architecture from noise2inverse [4]. the msd
makes use of dilated convolutions to process features at all scales of the
image. we use the msd with depth 200 and width 1, adam optimizer, mse loss, a
batch size of 16, and a learning rate of 0.0001.the baselines we compare against
are two. the first is the traditional fdk obtained using rtk [13]. the second is
the supervised approach proposed by [6], where we replace the model with the
msd, for a fair comparison. in the supervised approach, the model is trained by
using as input reconstructions obtained from subsets defined with pseudo-average
subset selection while the targets use all of the projections available.the
metrics used in this work are the root mean squared error (rmse), peak
signal-to-noise ratio (psnr), and structural similarity index measure (ssim)
[17] all the metrics are defined between the output of the neural network and a
3d (cb)ct scan. for the spare varian dataset, we use the rois defined provided
[15] and used the 3d reconstruction using all the projections available as a
ground truth. for the internal dataset, we deformed the planning ct to each of
the phases reconstructed using the fdk algorithm and evaluate the metric over
only the 4dcbct volume boundaries.",10
DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,3.1,Experimental Evaluations,"forty 128 × 128 × 40 3d zubal brain phantoms [24] were used in the simulation
study as ground truth, and one clinical patient brain images with different dose
level were used for the robust analysis. two tumors with different size were
added in each zubal brain phantom. the ground truth images were firstly
forward-projected to generate the noise-free sinogram with count of 10 6 for
each transverse slice and then poisson noise were introduced. 20 percent of
uniform random events were simulated. in total, 1600 (40 × 40) 2d sinograms were
generated. among them, 1320 (33 samples) were used in training, 200 (5 samples)
for testing, and 80 (2 samples) for validation. a total of 5 realizations were
simulated and each was trained/tested independently for bias and variance
calculation [15]. we used peak signal to noise ratio (psnr), structural
similarity index (ssim) and root mean square error (rmse) for overall
quantitative analysis. the contrast recovery coefficient (crc) [25] was used for
the comparison of reconstruction results in the tumor region of interest (roi)
area.",10
Low-Dose CT Image Super-Resolution Network with Dual-Guidance Feature Distillation and Dual-Path Content Communication,1.0,Introduction,"following the ""as low as reasonably achievable"" (alara) principle [22], lowdose
computer tomography (ldct) has been widely used in various medical applications,
for example, clinical diagnosis [18] and cancer screening [28]. to balance the
high image quality and low radiation damage compared to normaldose ct (ndct),
numerous algorithms have been proposed for ldct superresolution [3,4].in the
past decades, image post-processing techniques attracted much attention from
researchers because they did not rely on the vendor-specific parameters [2] like
iterative reconstruction algorithms [1,23] and could be easily applied to
current ct workflows [29]. image post-processing super-resolution (sr) methods
could be divided into 3 categories: interpolated-based methods [16,25],
modelbased methods [13,14,24,26] and learning-based methods [7][8][9]17].
interpolatedbased methods could recover clear results in those flattened regions
but failed to reconstruct detailed textures because they equally recover
information with different frequencies. and model-based methods often involved
time-consuming optimization processes and degraded quickly when image statistics
were biased from the image prior [6].with the development of deep learning (dl),
various learning-based methods have been proposed, such as edsr [20], rcan [31],
and swinir [19]. those methods optimized their trainable parameters by
pre-degraded low-resolution (lr) and high-resolution (hr) pairs to build a
robust model with generalization and finally reconstruct sr images. however,
they were designed for known degradation (for example bicubic degradation) and
failed to deal with more complex and unknown degradation processes (such as ldct
degradation). facing more complex degradation processes, blind sr methods have
attracted attention. huang et al. [11] introduced a deep alternating network
(dan) which estimated the degradation kernels and corrected those kernels
iteratively and reconstructed results following the inverse process of the
estimated degradation. more recently, aiming at improving the quality of medical
images further, huang et al. [12] first composited degradation model proposed
for radiographs and proposed attention denoising super-resolution generative
adversarial network (aid-srgan) which could denoise and super-resolve
radiographs simultaneously. to accurately reconstruct hr ct images from lr ct
images, hou et al. [10] proposed a dual-channel joint learning framework which
could process the denoising reconstruction and sr reconstruction in parallel.the
aforementioned methods still have drawbacks: (1) they treated the regions of
interest (roi) and regions of uninterest equally, resulting in the extra cost in
computing source and inefficient use for hierarchical features. (2) most of them
extracted the features with a fixed resolution, failing to effectively leverage
multi-scale features which are essential to image restoration task [27,32].(3)
they connected the sr task and the ldct denoising task stiffly, leading to
smooth texture, residual artifacts and unclear edges.to deal with those issues,
as shown in fig. 1(a), we propose an ldct image sr network with dual-guidance
feature distillation and dual-path content com-fig. 1. architecture of our
proposed method. sam is sampling attention module. cam is channel attention
module. avg ct is the average image among adjacent ct slices of each patient.
munication. our contributions are as follows: (1) we design a dual-guidance
fusion module (dgfm) which could fuse the 3d ct information and roi guidance by
mutual attention to make full use of ct features and reconstruct clearer
textures and sharper edges. (2) we propose a sampling attention block (sab)
which consists of sampling attention module (sam), channel attention module
(cam) and elaborate multi-depth residual connection aiming at the essential
multi-scale features by up-sampling and down-sampling to leverage the features
in ct images. (3) we design a multi-supervised mechanism based on shared task
heads, which introducing the denoising head into sr task to concentrate on the
connection between the sr task and the denoising task. such design could
suppress more artifacts while decreasing the number of parameters.",10
Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,4.0,Results and Discussion,"table 1 presents the registration performance of our nice-trans and all
comparison methods. the registration accuracy of all methods degraded by 1-3% in
dsc when affine registration was not performed, which demonstrates the
importance of affine registration. however, using flirt or affinenet for affine
registration incurred extra computational loads and increased the registration
runtime. our nice-trans performed joint affine and deformable registration,
which enabled it to realize affine registration with negligible additional
runtime. moreover, we suggest that integrating affine and deformable
registration into a single network also brings convenience for network training.
training two separate affine and deformable registration networks will prolong
the whole training time, while joint training will consume more gpu memory. as
for registration accuracy, the transmorph and swin-vm achieved higher dscs than
the conventional vm and difvm, but still cannot outperform the existing
cnn-based coarse-to-fine registration methods (lapirn, ulae-net, and nice-net).
our nice-trans leverages swin transformer to perform coarse-to-fine
registration, which enabled it to achieve the highest dscs among all methods.
this means that our nice-trans also has advantages on registration accuracy. we
present a qualitative comparison in the supplementary materials, which shows
that the registration result produced by our nice-trans is more consistent with
the fixed image. in addition, there usually exists a trade-off between dsc and
njd as imposing constraints on the spatial transformations limits their
flexibility, which results in degraded registration accuracy [13,18]. for
example, compared with vm, the difvm with diffeomorphic constraints achieved
better njds and worse dscs. nevertheless, our nice-trans achieved both the best
dscs and njds. we suggest that, if we set λ as 0 to maximize the registration
accuracy with the cost of transformation invertibility, our nice-trans can
achieve higher dscs and outperform the comparison methods by a larger margin
(refer to the regularization analysis in the supplementary materials).table 2
shows the results of our ablation study. swin transformer improved the
registration performance when embedded into the decoder, but had limited
benefits in the encoder. this suggests that swin transformer can benefit
registration in modeling inter-image spatial relevance while having limited
benefits in learning intra-image representations. this finding is intuitive as
image registration aims to find spatial relevance between images, instead of
finding the internal relevance within an image. under this aim, embedding
transformers in the decoder helps to capture long-range relevance between images
and improves registration performance. we noticed that previous studies gained
improvements by embedding swin transformer in the encoder [21] or leveraging a
full transformer network [22]. this is attributed to the fact that they used a
vm-like architecture that entangles image representation learning and spatial
relevance modeling throughout the whole network. our nice-trans decouples these
two parts and provides further insight on using transformers for registration:
leveraging transformers to learn intra-image relevance might not be beneficial
but merely incurs extra computational loads. it should be acknowledged that
there are a few limitations in our study. first, the experiment (table 1)
demonstrated that our nice-trans can well address the inherent misalignments
among inter-patient brain mri images, but the sensitivity of affine registration
to different degrees of misalignments is still awaiting further exploration.
second, in this study, we evaluated the nice-trans on the benchmark task of
inter-patient brain mri registration, while we believe that our nice-trans also
could apply to other image registration applications (e.g., brain tumor
registration [37]).",10
Trackerless Volume Reconstruction from Intraoperative Ultrasound Images,1.0,Introduction,"liver cancer is the most prevalent indication for liver surgery, and although
there have been notable advancements in oncologic therapies, surgery remains as
the only curative approach overall [20].liver laparoscopic resection has
demonstrated fewer complications compared to open surgery [21], however, its
adoption has been hindered by several reasons, such as the risk of unintentional
vessel damage, as well as oncologic concerns such as tumor detection and margin
assessment. hence, the identification of intrahepatic landmarks, such as
vessels, and target lesions is crucial for successful and safe surgery, and
intraoperative ultrasound (ious) is the preferred technique to accomplish this
task. despite the increasing use of ious in surgery, its integration into
laparoscopic workflows (i.e., laparoscopic intraoperative ultrasound) remains
challenging due to combined problems.performing ious during laparoscopic liver
surgery poses significant challenges, as laparoscopy has poor ergonomics and
narrow fields of view, and on the other hand, ious demands skills to manipulate
the probe and analyze images. at the end, and despite its real-time
capabilities, ious images are intermittent and asynchronous to the surgery,
requiring multiple iterations and repetitive steps (probe-in -→ instruments-out
-→ probe-out -→ instruments-in). therefore, any method enabling a continuous and
synchronous us assessment throughout the surgery, with minimal iterations
required would significantly improve the surgical workflow, as well as its
efficiency and safety.to overcome these limitations, the use of intravascular
ultrasound (ivus) images has been proposed, enabling continuous and synchronous
inside-out imaging during liver surgery [19]. with an intravascular approach, an
overall view and full-thickness view of the liver can quickly and easily be
obtained through mostly rotational movements of the catheter, while this is
constrained to the lumen of the inferior vena cava, and with no interaction with
the tissue (contactless, a.k.a. standoff technique) as illustrated in fig. 1.
however, to benefit from such a technology in a computer-guided solution, the
different us images would need to be tracked and possibly integrated into a
volume for further processing. external us probes are often equipped with an
electromagnetic tracking system to track its position and orientation in
realtime. this information is then used to register the 3d ultrasound image with
the patient's anatomy. the use of such an electromagnetic tracking system in
laparoscopic surgery is more limited due to size reduction. the tracking system
may add additional complexity and cost to the surgical setup, and the tracking
accuracy may be affected by metallic devices in the surgical field [22].several
approaches have been proposed to address this limitation by proposing a
trackerless ultrasound volume reconstruction. physics-based methods have
exploited speckle correlation models between different adjacent frames [6][7][8]
to estimate their relative position. with the recent advances in deep learning,
recent works have proposed to learn a higher order nonlinear mapping between
adjacent frames and their relative spatial transformation. prevost et al. [9]
first demonstrated the effectiveness of a convolution neural network to learn
the relative motion between a pair of us images. xie et al. [10] proposed a
pyramid warping layer that exploits the optical flow features in addition to the
ultrasound features in order to reconstruct the volume. to enable a smooth 3d
reconstruction, a case-wise correlation loss based on 3d cnn and pearson
correlation coefficient was proposed in [10,12]. qi et al. [13] leverages past
and future frames to estimate the relative transformation between each pair of
the sequence; they used the consistency loss proposed in [14]. despite the
success of these approaches, they still suffer significant cumulative drift
errors and mainly focus on linear probe motions. recent work [15,16] proposed to
exploit the acceleration and orientation of an inertial measurement unit (imu)
to improve the reconstruction performance and reduce the drift error. motivated
by the weakness of the state-of-the-art methods when it comes to large
non-linear probe motions, and the difficulty of integrating imu sensors in the
case of minimally invasive procedures, we introduce a new method for pose
estimation and volume reconstruction in the context of minimally invasive
trackerless ultrasound imaging. we use a siamese architecture based on a
sequence to vector(seq2vec) neural network that leverages image and optical flow
features to learn relative transformation between a pair of images.our method
improves upon previous solutions in terms of robustness and accuracy,
particularly in the presence of rotational motion. such motion is predominant in
the context highlighted above and is the source of additional nonlinearity in
the pose estimation problem. to the best of our knowledge, this is the first
work that provides a clinically sound and efficient 3d us volume reconstruction
during minimally invasive procedures. the paper is organized as follows: sect. 2
details the method and its novelty, sect. 3 presents our current results on ex
vivo porcine data, and finally, we conclude in sect. 4 and discuss future work.",10
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,1.0,Introduction,"magnetic resonance imaging (mri) is critical to the diagnosis, treatment, and
follow-up of brain tumour patients [26]. multiple mri modalities offer
complementary information for characterizing brain tumours and enhancing patient
l. jiang and y. mao-contribute equally in this work.management [4,27]. however,
acquiring multi-modality mri is time-consuming, expensive and sometimes
infeasible in specific modalities, e.g., due to the hazard of contrast agent
[15]. trans-modal mri synthesis can establish the mapping from the known domain
of available mri modalities to the target domain of missing modalities,
promising to generate missing mri modalities effectively. the synthetic methods
leveraging multi-modal mri, i.e., many-to-one translation, have outperformed
single-modality models generating a missing modality from another available
modality, i.e., one-to-one translation [23,33]. traditional multi-modal methods
[21,22], e.g., sparse encoding-based, patch-based and atlasbased methods, rely
on the alignment accuracy of source and target domains and are poorly scalable.
recent generative adversarial networks (gans) and variants, e.g., mm-gan [23],
diamondgan [13] and provogan [30], have been successful based on multi-modal
mri, further improved by introducing multi-modal coding [31], enhanced
architecture [7], and novel learning strategies [29].despite the success,
gan-based models are challenged by the limited capability of adversarial
learning in modelling complex multi-modal data distributions [25] recent studies
have demonstrated that gans' performance can be limited to processing and
generating data with less variability [1]. in addition, gans' hyperparameters
and regularization terms typically require fine-tuning, which otherwise often
results in gradient vanish and mode collapse [2].diffusion model (dm) has
achieved state-of-the-art performance in synthesizing natural images, promising
to improve mri synthesis models. it shows superiority in model training [16],
producing complex and diverse images [9,17], while reducing risk of modality
collapse [12].for instance, lyu et al. [14] used diffusion and score-marching
models to quantify model uncertainty from monte-carlo sampling and average the
output using different sampling methods for ct-to-mri generation; özbey et al.
[19] leveraged adversarial training to increase the step size of the inverse
diffusion process and further designed a cycle-consistent architecture for
unpaired mri translation.however, current dm-based methods focus on one-to-one
mri translation, promising to be improved by many-to-one methods, which requires
dedicated design to balance the multiple conditions introduced by multi-modal
mri. moreover, as most dms operate in original image domain, all markov states
are kept in memory [9], resulting in excessive burden. although latent diffusion
model (ldm) [20] is proposed to reduce memory consumption, it is less feasible
for many-to-one mri translation with multi-condition introduced. further,
diffusion denoising processes tend to change the original distribution structure
of the target image due to noise randomness [14], rending dms often ignore the
consistency of anatomical structures embedded in medical images, leading to
clinically less relevant results. lastly, dms are known for their slow speed of
diffusion sampling [9,11,17], challenging its wide clinical application.we
propose a dm-based multi-modal mri synthesis model, cola-diff, which facilitates
many-to-one mri translation in latent space, and preserve anatomical structure
with accelerated sampling. our main contributions include: -present a denoising
diffusion probabilistic model based on multi-modal mri.as far as we know, this
is the first dm-based many-to-one mri synthesis model. -design a bespoke
architecture, e.g., similar cooperative filtering, to better facilitate
diffusion operations in the latent space, reducing the risks of excessive
information compression and high-dimensional noise. -introduce structural
guidance of brain regions in each step of the diffusion process, preserving
anatomical structure and enhancing synthesis quality. -propose an auto-weight
adaptation to balance multi-conditions and maximise the chance of leveraging
relevant multi-modal information.",10
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,2.2,Structural Guidance,"unlike natural images, medical images encompass rich anatomical information.
therefore, preserving anatomical structure is crucial for mri generation.
however, dms often corrupt anatomical structure, and this limitation could be
due to the learning and sampling processes of dms that highly rely on the
probability density function [9], while brain structures by nature are
overlapping in mri density distribution and even more complicated by
pathological changes. previous studies show that introducing geometric priors
can significantly improve the robustness of medical image generation. [3,28].
therefore, we hypothesize that incorporating structural prior could enhance the
generation quality with preserved anatomy. specifically, we exploit fsl-fast
[32] tool to segment four types of brain tissue: white matter, grey matter,
cerebrospinal fluid, and tumour. the generated tissue masks and inherent density
distributions (fig. 1 (e)) are then used as a condition y i to guide the reverse
diffusion.the combined loss function for our multi-conditioned latent diffusion
is defined aswhere kl is the kl divergence loss to measure similarity between
real q and predicted p θ distributions of encoded images.where d kl is the kl
divergence function.",10
CoLa-Diff: Conditional Latent Diffusion Model for Multi-modal MRI Synthesis,3.1,Comparisons with State-of-the-Art Methods,"datasets and baselines. we evaluated cola-diff on two multi-contrast brain mri
datasets: brats 2018 and ixi datasets. the brats 2018 contains mri scans from
285 glioma patients. each includes four modalities: t1, t2, t1ce, and flair. we
split them into (190:40:55) for training/validation/testing. for each subject,
we automatically selected axial cross-sections based on the perceptible
effective area of the slices, and then cropped the selected slices to a size of
224 × 224. the ixi1 dataset consists of 200 multi-contrast mris from healthy
brains, plit them into (140:25:35) for training/validation/testing. for
preprocessing, we registered t2-and pd-weighted images to t1-weighted images
using fsl-flirt [10], and other preprocessing are identical to the brats 2018.
we compared cola-diff with four state-of-the-art multi-modal mri synthesis
methods: mm-gan [23], hi-net [33], provogan [30] and ldm [20]. implementation
details. our code is publicly available at https://github. com/seemeincrown/cola
diff multimodal mri synthesis. the hyperparameters of cola-diff are defined as
follows: diffusion steps to 1000; noise schedule to linear; attention
resolutions to 32, 16, 8; batch size to 8, learning rate to 9.6e -5.the noise
variances were in the range of β 1 = 10 -4 and β t = 0.02. an exponential moving
average (ema) over model parameters with a rate of 0.9999 was employed. the
model is trained on 2 nvidia rtx a5000, 24 gb with adam optimizer on pytorch. an
acceleration method [11] based on knowledge distillation was applied for fast
sampling. quantitative results. we performed synthesis experiments for all
modalities, with each modality selected as the target modality while remaining
modalities and the generated region masks as conditions. seven cases were tested
in two datasets (table 1). the results show that cola-diff outperforms other
models by up to 6.01 db on psnr and 5.74% on ssim. even when compared to the
best of other models in each task, cola-diff is a maximum of 0.81 db higher in
psnr and 0.82% higher in ssim. qualitative results. the first three and last
three rows in fig. 2 illustrate the synthesis results of t1ce from brats and pd
from the ixi, respectively. from the generated images, we observe that cola-diff
is most comparable to the ground truth, with fewer errors shown in the heat
maps. the synthesis uncertainty for each region is derived by performing 100
generations of the same slice and calculating the pixel-wise variance. from the
uncertainty maps, cola-diff is more confident in synthesizing the gray and white
matter over other comparison models. particularly, cola-diff performs better in
generating complex brain sulcus and tumour boundaries. further, cola-diff could
better maintain the anatomical structure over comparison models.",10
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,1.0,Introduction,"end-to-end convolutional neural networks (cnns) have shown remarkable
performance compared to classical algorithms [14] on mri sr. deep cnns have been
widely applied in a variety of mri sr situations; for instance, slice imputation
on the brain, liver and prostate mri [29] and brain mri sr reconstruction on
scaling factors ×2, ×3, ×4 [32]. several techniques based on deep cnns have been
proposed to improve performance, such as densely connected networks [6],
adversarial networks [5], and attention network [32]. however, their supervised
training requires paired images, which necessitates re-training every time there
is a shift in the input distribution [4,16]. as a result, such methods are
unsuitable for mri sr, as it is challenging to obtain paired training data that
cover the variability in acquisition protocols and resolution of clinical brain
mri scans across institutions [14].building image priors through generative
models has recently become a popular approach in the field of image sr, for both
computer vision [1,2,7,17,19] as well as medical imaging [18,25], as they do not
require re-training in the presence of several types of input distribution
shifts. while these methods have shown promise in mri sr, they have so far been
limited to 2d slices [18,25], rendering them unsuitable for 3d brain mris slice
imputation.in this study, we propose solving the mri sr problem by building
powerful, 3d-native image priors through a recently proposed hr image generative
model, the latent diffusion model (ldm) [21,22]. we solve the inverse problem by
finding the optimal latent code z in the latent space of the pre-trained
generative model, which could restore a given lr mri i, using a known corruption
function f . in this study, we focus on slice imputation, yet our method could
be applied to other medical image sr problems by implementing different
corruption functions f . we proposed two novel strategies for mri sr:
inverse(ldm), which additionally inverts the input image through the
deterministic ddim model, and inversesr(decoder) which inverts the input image
through the corruption function f and through the decoder d of the ldm model. we
found that for large sparsity, inversesr(ldm) had a better performance, while
for low sparsity, inversesr(decoder) performed best. while the ldm model was
trained on uk biobank, we demonstrate our methods on an external dataset (ixi)
which was inaccessible to the pre-trained generative model. both quantitative
and qualitative results show that our method achieves significantly better
performance compared to two other baseline models. furthermore, our method can
also be applied to tumour/lesion filling by creating tumour/lesion shape masks.",10
InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model,4.0,Results,"figure 2 shows the qualitative results on the coronal slices of sr from 4 and 8
mm axial scans. the advantage of our approach is clear compared to baseline
methods because it is capable of restoring hr mris with smoothness even when the
slice thickness is large (i.e., 8 mm). this is the case because the pre-trained
ldm we use is able to build a powerful prior over the hr t1w mri domain.
therefore, the generated images of our method are hr mris with smoothness in 3
directions: axial, sagittal and coronal, no matter how sparse the input images i
are. qualitative results of applying our method on tumour and lesion filling are
available in the supplementary material.table 1 shows quantitative results on
100 hr t1 scans from the ixi dataset, which the brain ldm did not have access to
during training. we investigated mean peak signal-to-noise ratio (psnr), and
structural similarity index measure (ssim) [28] values and their corresponding
standard deviation. we compare our method to cubic interpolation, as well as a
similar unsupervised approach, unires [3]. we show our approach and the two
compared methods on two different settings of slice imputation: 4 mm and 8 mm
thick-sliced axial scans representing low sparsity and high sparsity lr mris,
respectively. all the metrics are computed on a 3d volume around the brain of
size 160 × 224 × 160. for sr at 4 mm, inversesr(decoder) achieves the highest
mean ssim and psnr scores among all compared methods, which are slightly higher
than the scores for inversesr(ldm). for sr at 8 mm, inverse(ldm) achieves the
highest mean ssim and psnr and lowest standard error than the two baseline
methods, which could be attributed to the stronger prior learned by the ddim
model.",10
Topology-Preserving Computed Tomography Super-Resolution Based on Dual-Stream Diffusion Model,1.0,Introduction,"computed tomography (ct) is a prevalent imaging modality with applications in
biology, disease diagnosis, interventional imaging, and other areas.
highresolution ct (hrct) is beneficial for clinical diagnosis and surgical
planning because it can provide detailed spatial information and specific
features, usually employed in advanced clinical routines [1]. hrct usually
requires high-precision ct machines to scan for a long time with high radiation
doses to capture the internal structures, which is expensive and can impose the
risk of radiation exposure [2]. these factors make hrct relatively less
available, especially in towns and villages, compared to low-resolution ct
(lrct). however, degradation in spatial resolution and imaging quality brought
by lrct can interfere with the original physiological and pathological
information, adversely affecting the diagnosis [3]. consequently, how to produce
high-resolution ct scans at a smaller radiation dose level with lower scanning
costs is a holy grail of the medical imaging field (fig. 1). with the
advancement of artificial intelligence, super-resolution (sr) techniques based
on neural networks indicate new approaches to this problem. by inferring
detailed high-frequency features from lrct, super-resolution can introduce
additional knowledge and restore lost information due to lowresolution scanning.
deep-learning (dl) based methods, compared to traditional methods, can
incorporate hierarchical features and representations from prior knowledge,
resulting in improved results in sr tasks [4]. according to different
neural-network frameworks, these sr methods can be broadly categorized into two
classes: 1) convolutional neural network (cnn) based model [5][6][7], and 2)
generative adversarial network (gan) based model [2,8,9]. very recently, the
diffusion model is emerging as the most promising deep generative model [11],
which usually consists of two stages: a forward stage to add noises and a
reverse stage to separate noises and recover the original images. the diffusion
model shows impressive generative capabilities for many tasks, including image
generation, inpainting, translation, and super-resolution [10,12,13].while
dl-based methods can generate promising results, there can still be geometric
distortions and artifacts along with structural edges in the superresolved
results [15,16]. these structural features always represent essential
physiological structures, including vasculature, fibrosis, tumor, and other
lesions. the distortion and infidelity of these features can lead to potential
misjudgment for diagnosis, which is unacceptable for clinical application.
moreover, the target image size and spatial resolution of hrct for most existing
sr methods is about 512 × 512 and 0.8 × 0.8 mm 2 . with the progress in hardware
settings, ultra-high-resolution ct (uhrct) with an image size of 1024 × 1024 and
spatial resolution of 0.3 × 0.3 mm 2 can be available very recently [17]. though
uhrct can provide much more detailed information, to our best knowledge, sr
tasks targeting uhrct have rarely been discussed and reported.in this paper, we
propose a novel dual-stream conditional diffusion model for ct scan
super-resolution to generate uhrct results with high image quality and structure
fidelity. the conditional diffusion model takes the form p(y|x), where x is the
lrct, and y is the targeted uhrct [14]. the novel diffusion model incorporates a
dual-stream structure-preserving network and a novel imaging enhancement
operator in the denoising process. the imaging enhancement operator can
simultaneously extract the vascular and blob structures in the ct scans and
provide structure prior to the dual-stream network. the dualstream network can
fully exploit the prior information with two branches. one branch optimizes the
sr results in the image domain, and the other branch optimizes the results in
the structure domain. in practice, we use a convolution-based lightweight module
to simulate the filtering operations, which enables faster and easier
back-propagation in the training process. furthermore, we constructed a new
ultra-high resolution ct scan dataset obtained with the most advanced ct
machines. the dataset contained 87 uhrct scans with a spatial resolution of
0.34×0.34 mm 2 and an image size of 1024×1024. extensive experiments, including
qualitative and quantitative comparisons in both image consistency, structure
fidelity, and high-level tasks, demonstrated the superiority of our method. our
contributions can be summarized as follows:1) we proposed a novel dual-stream
diffusion model framework for ct superresolution. the framework incorporates a
dual-stream structure-preserving network in the denoising process to realize
better physiological structure restoration. 2) we designed a new image
enhancement operator to model the vascular and blob structures in medical
images. to avoid non-derivative operations in image enhancement, we proposed a
novel enhancement module consisting of lightweight convolutional layers to
replace the filtering operation for faster and easier back-propagation in
structural domain optimization.3) we established an ultra-high-resolution ct
scan dataset with a spatial resolution of 0.34 × 0.34 mm 2 and an image size of
1024 × 1024 for training and testing the sr task. 4) we have conducted extensive
experiments and demonstrated the excellent performance of the proposed sr
methods in both the image and structure domains. in addition, we have evaluated
our proposed method on high-level tasks, including vascular-system segmentation
and lesion detection on the srct, indicating the reliability of our sr results.",10
LightNeuS: Neural Surface Reconstruction in Endoscopy Using Illumination Decline,1.0,Introduction,"colorectal cancer (crc) is the third most commonly diagnosed cancer and is the
second most common cause of cancer death [23]. early detection is crucial for a
good prognosis. despite the existence of other techniques, such as virtual
colonoscopy (vc), optical colonoscopy (oc) remains the gold standard for
colonoscopy screening and the removal of precursor lesions. unfortunately, we do
not yet have the ability to reconstruct densely the 3d shape of large sections
of the colon. this would usher exciting new developments, such as
post-intervention diagnosis, measuring polyps and stenosis, and automatically
evaluating exploration thoroughness in terms of the surface percentage that has
been observed. this is the problem we address here. it has been shown that the
colon 3d shape can be estimated from single images acquired during human
colonoscopies [3]. however, to model large sections of it while increasing the
reconstruction accuracy, multiple images must be used. as most endoscopes
contain a single camera, the natural way to do this is to use video sequences
acquired by these cameras in the manner of structure-from-motion algorithms. an
important first step in that direction is to register the images from the
sequences. this can now be done reliably using either batch [21] or slam
techniques [8]. unfortunately, this solves only half the problem because these
techniques provide very sparse reconstructions and going from there to dense
ones remains an open problem. and occlusions, specularities, varying albedos,
and specificities of endoscopic lighting make it a challenging one.to overcome
these difficulties, we rely on two properties of endoscopic images:-endoluminal
cavities such as the gastrointestinal tract, and in particular the human colon,
are watertight surfaces. to account for this, we represent its surface in terms
of a signed distance function (sdf), which by its very nature presents
continuous watertight surfaces. -in endoscopy the light source is co-located
with the camera. it illuminates a dark scene and is always close to the surface.
as a result, the irradiance decreases rapidly with distance t from camera to
surface; more specifically it is a function of 1/t 2 . in other words, there is
a strong correlation between light and depth, which remains unexploited to
date.to take advantage of these specificities, we build on the success of neural
implicit surfaces (neus) [25] that have been shown to be highly effective at
deriving surface 3d models from sets of registered images. as the neural
radiance fields (nerfs) [15] that inspired them, they were designed to operate
on regular images taken around a scene, sampling fairly regularly the set of
possible viewing directions. furthermore, the lighting is assumed to be static
and distant so that the brightness of a pixel and its distance to the camera are
unrelated. unfortunately, none of these conditions hold in endoscopies. the
camera is inside a cavity (in the colon, a roughly cylindrical tunnel) that
limits viewing directions. the light source is co-located with the camera and
close to the surface, which results in a strong correlation between pixel
brightness and distance to the camera. in this paper, we show that, far from
being a handicap, this correlation is a key information for neural network
self-supervision.neus training selects a pixel from an image and samples points
along its projecting ray. however, the network is agnostic to the sampling
distance. in lightneus, we explicitly feed to the renderer the distance of each
one of these sampled points to the light source, as shown in fig. 1. hence, the
renderer can exploit the inverse-square illumination decline. we also introduce
and calibrate a photometric model for the endoscope light and camera, so that
the inverse square law discussed above actually holds. together, these two
changes make the minimization problem better posed and the automatic depth
estimation more reliable.our results show that exploiting the illumination is
key to unlocking implicit neural surface reconstruction in endoscopy. it
delivers accuracies in the range of 3 mm, whereas an unmodified neus is either 5
times less accurate or even fails to reconstruct any surface at all. earlier
methods [3] have reported similar accuracies but only on very few synthetic
images and on short sections of the colon. by contrast, we can handle much
longer ones and provide a broad evaluation in a real dataset (c3vd) over
multiple sequences. this makes us the first to show accurate results of extended
3d watertight surfaces from monocular endoscopy images.",10
