paper_id,datasets_info
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"in this work, we pro-
pose a novel method to segment the 3d peripancreatic arteries solely
from one annotated 2d projection per training image with depth
supervision. we perform extensive experiments on the segmentation of
peripancreatic arteries on 3d contrast-enhanced ct images and demon-
strate how well we capture the rich depth information from 2d pro-
jections. we demonstrate that by annotating a single, randomly chosen
projection for each training sample, we obtain comparable performance
to annotating multiple 2d projections, thereby reducing the annotation
eﬀort."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"furthermore, by mapping the 2d labels to the 3d space using
depth information and incorporating this into training, we almost close
the performance gap between 3d supervision and 2d supervision. our
code is available at: https://github.com/alinafdima/3dseg-mip-depth.
keywords: vessel segmentation · 3d segmentation · weakly
supervised segmentation · curvilinear structures · 2d projections
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_14.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43907-0_14
142
a. f. dima et al.
1
introduction
automated segmentation of blood vessels in 3d medical images is a crucial step
for the diagnosis and treatment of many diseases, where the segmentation can aid
in visualization, help with surgery planning, be used to compute biomarkers, and
further downstream tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"[13], obtaining results comparable to full 3d supervision. compared to
centerline segmentation, where the vessel diameter is disregarded, training a 3d
vessel segmentation model from 2d annotations poses additional segmentation-
speciﬁc challenges, as 2d projections only capture the outline of the vessels,
providing no information about their interior. furthermore, the axes of projec-
tion are crucial for the model’s success, given the sparsity of information in 2d
annotations.
to achieve 3d vessel segmentation with only 2d supervision from projec-
tions, we ﬁrst investigate which viewpoints to annotate in order to maximize
segmentation performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"our contribution to 3d vessel segmentation is three-fold:
◦ our work shows that highly accurate automatic segmentation of 3d vessels
can be learned by annotating single mips. ◦ based on extensive experimental results, we determine that the best annota-
tion strategy is to label randomly selected viewpoints, while also substantially
reducing the annotation cost. 3d arterial segmentation via single 2d projections and depth supervision
143
◦ by incorporating additional depth information obtained from 2d annotations
at no extra cost to the annotator, we almost close the gap between 3d super-
vision and 2d supervision."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"2
related work
learning from weak annotations. weak annotations have been used in
deep learning segmentation to reduce the annotation eﬀort through cheaper,
less accurate, or sparser labeling [20]. bai et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"[1] learn to perform aortic image
segmentation by sparsely annotating only a subset of the input slices. multiple
instance learning approaches bin pixels together by only providing labels at the
bin level. jia et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"[12] use this approach to segment cancer on histopathology
images successfully. annotating 2d projections for 3d data is another approach
to using weak segmentation labels, which has garnered popularity recently in
the medical domain. bayat et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"chen et al. [4] train a vessel segmen-
tation model from unsupervised 2d labels transferred from a publicly available
dataset, however, there is still a gap to be closed between unsupervised and
supervised model performance. our work uses weak annotations in the form of
annotations of 2d mips for the task of peripancreatic vessel segmentation, where
we attempt to reduce the annotation cost to a minimum by only annotating a
single projection per training input without sacriﬁcing performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"depth is one of the properties of the 3d
world. loss of depth information occurs whenever 3d data is projected onto a
lower dimensional space. in natural images, depth loss is inherent through image
acquisition, therefore attempts to recover or model depth have been employed
for 3d natural data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"for instance, fu et al. [9] use neural implicit ﬁelds to
semantically segment images by transferring labels from 3d primitives to 2d
images. lawin et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"[14] propose to segment 3d point clouds by projecting
them onto 2d and training a 2d segmentation network. at inference time, the
predicted 2d segmentation labels are remapped back to the original 3d space
using the depth information. [7] to aid with visualization, but it has so
far not been employed when working with 2d projections of 3d volumes to
recover information loss."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"[14], by projecting 3d volumes onto 2d to facilitate and reduce
annotation. we use depth information to map the 2d annotations to the original
3d space at annotation time and generate partial 3d segmentation volumes,
which we incorporate in training as an additional loss term. 144
a. f. dima et al.
3
methodology
overview."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"given an input image i, depth-encoded mips pfw, pbw are generated by project-
ing the input image to 2d. 2d binary labels a are generated by annotating one 2d
projection per image. the 2d annotation is mapped to the 3d space using the depth
information, resulting in a partially labeled 3d volume d. during training, both 2d
annotations and 3d depth maps are used as supervision signals in a combined loss,
which uses both predicted 3d segmentation y and its 2d projection mip(y )."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"along each pro-
jection ray, we denote the ﬁrst and last z coordinates which have the same
intensity as the mip to be the forward depth zfw = arg maxz i(x, y, z) and
backward depth zbw = arg minz i(x, y, z). this information can be utilized for
the following: (1) enhancing the mip visualization, or (2) providing a way to
map pixels from the 2d mip back to the 3d space (depth map). the reason
why the maximum intensity is achieved multiple times along a ray is because
our images are clipped, which removes a lot of the intensity ﬂuctuations.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"foreground pixels from the 2d annotations are
mapped to the 3d space by combining a 2d annotation with the forward and
backward depth, resulting in a 3d partial vessel segmentation:
146
a. f. dima et al.
1. for each foreground pixel in the annotation a at location (x, y), we label
(x, y, zfw) and (x, y, zbw) as foreground pixels in d.
3. if the ﬂuctuation in intensity between zfw and zbw along the ray rxy is
below a certain threshold in the source image i, the intermediate pixels
are also labeled as foreground in d.
training loss."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"notably, the 2d loss constrains
the shape of the vessels, while the depth loss promotes the segmentation of the
vessel interior. 4
experimental design
dataset. the cohort consists of 141 patients with pancreatic ductal adeno-
carcinoma, of an equal ratio of male to female patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"in order to remove as much
of the cluttering surrounding tissue and increase the visibility of the vessels in
the projections, the input is windowed so that the vessels appear hyperintense. details of the exact preprocessing steps can be found in table 2 of the supple-
mentary material. the dataset contains binary 3d annotations of the peripan-
creatic arteries carried out by two radiologists, each having annotated half of the
dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"the 2d annotations we use in our experiments are projections of these
3d annotations. for more information about the dataset, see [6].
image augmentation and transformation. as the annotations lie on a 2d
plane, 3d spatial augmentation cannot be used due to the information sparsity
in the ground truth."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"the network initialization is diﬀer-
ent for each fold but kept consistent across diﬀerent experiments run on the same
fold. this way, both data variance and initialization variance are accounted for
through cross-validation. to measure the performance of our models, we use the
dice score, precision, recall, and mean surface distance (msd)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"models trained on full 3d
ground truth represent the upper bound baseline, which is very expensive to
annotate. we implement [13] as a baseline on our dataset, training on up to 3
ﬁxed orthogonal projections. we distinguish between models selected according
to the 2d performance on the validation set (2d) which is a fair baseline, and
models selected according to the 3d performance on the validation set (3d),
which is an unfair baseline as it requires 3d annotations on the validation set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"in table 1 we compare our method against the 3d baseline, as well as base-
lines trained on multiple viewpoints. we see that by using depth information
148
a. f. dima et al.
paired with training using a single random viewpoint per sample performs almost
at the level of models trained on 3d labels, at a very small fraction of the anno-
tation cost. the depth information also reduces model variance compared to the
same setup without depth information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"even without depth information, train-
ing the model on single randomly chosen viewpoints oﬀers a robust training
signal that the dice score is on par with training on 2 ﬁxed viewpoints under
ideal model selection at only half the annotation cost. randomly selecting view-
points for training acts as powerful data augmentation, which is why we are
able to obtain performance comparable to using more ﬁxed viewpoints. under
ideal 3d-based model selection, three views would come even closer to full 3d
performance; however, with realistic 2d-based model selection, ﬁxed viewpoints
are more prone to diverge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"this occurs because sometimes 2d-based model selec-
tion favors divergent models which only segment hollow objects, which cannot
be ﬁxed in postprocessing. single ﬁxed viewpoints contain so little information
on their own that models trained on such input fail to learn how to segment
the vessels and generally converge to over-segmenting in the blind spots in the
projections. we conclude that using random viewpoints is not only helpful in
reducing annotation cost but also decreases model variance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"in terms of other metrics, randomly chosen projection viewpoints with and
without depth improve both recall and skeleton recall even compared to fully 3d
annotations, while generally reducing precision. we theorize that this is because
the dataset itself contains noisy annotations and fully supervised models better
overﬁt to the type of data annotation, whereas our models converge to follow-
ing the contrast and segmenting more vessels, which are sometimes wrongfully
labeled as background in the ground truth. msd are not very telling in our
dataset due to the noisy annotations and the nature of vessels, as an under- or
over-segmented vessel branch can quickly translate into a large surface distance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"we vary the size of the training set from |dtr| =
80 to as little as |dtr| = 10 samples, while keeping the size of the validation and
test sets constant, and train models on single random viewpoints. in table 2, we compare single random projections trained with and with-
out depth information at varying dataset sizes to ilustrate the usefulness of the
depth information with diﬀerent amounts of training data. our depth loss oﬀers
consistent improvement across multiple dataset sizes and reduces the overall per-
formance variance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"the performance boost is noticeable across the board, the
only exception being precision. the smaller the dataset size is, the greater the
performance boost from the depth. we perform a wilcoxon rank-sum statisti-
cal test comparing the individual sample predictions of the models trained at
various dataset sizes with single random orthogonal viewpoints with or without
depth information, obtaining a statistically signiﬁcant (p-value of < 0.0001)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_14.pdf,"3d arterial segmentation via single 2d projections and depth supervision
149
table 2. dataset size ablation. we vary the training dataset size |dtr| and compare
models trained on single random viewpoints, with or without depth."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"most existing
approaches employ 3d convolutions to obtain representative features. however, these convolution-based approaches struggle to eﬀectively cap-
ture long-range dependencies in the volume mitochondria data, due to
their limited local receptive ﬁeld. to address this, we propose a hybrid
encoder-decoder framework based on a split spatio-temporal attention
module that eﬃciently computes spatial and temporal self-attentions in
parallel, which are later fused through a deformable convolution."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"further,
we introduce a semantic foreground-background adversarial loss during
training that aids in delineating the region of mitochondria instances
from the background clutter. our extensive experiments on three bench-
marks, lucchi, mitoem-r and mitoem-h, reveal the beneﬁts of the
proposed contributions achieving state-of-the-art results on all three
datasets. our code and models are available at https://github.com/
omkarthawakar/stt-unet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"keywords: electron microscopy · mitochondria instance
segmentation · spatio-temporal transformer · hybrid
cnn-transformers
1
introduction
mitochondria are membrane-bound organelles that generate the primary energy
required to power the cell activities, thereby crucial for metabolism. mitochon-
drial dysfunction, which occurs when mitochondria are not functioning properly
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3_59. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14227, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"[23,25]. elec-
tron microscopy (em) images are typically utilized to reveal the corresponding
3d geometry and size of mitochondria at a nanometer scale, thereby facilitating
basic biological research at ﬁner scales. therefore, automatic instance segmenta-
tion of mitochondria is desired, since manually segmenting from a large amount
of data is particularly laborious and demanding. however, automatic 3d mito-
chondria instance segmentation is a challenging task, since complete shape of
mitochondria can be sophisticated and multiple instances can also experience
entanglement with each other resulting in unclear boundaries."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"on the other hand, top-down methods typ-
ically rely on techniques such as mask r-cnn [7] for segmentation. however,
mask r-cnn based approaches struggle due to undeﬁned bounding-box scale
in em data volume. when designing a attention-based framework for 3d mitochondria instance
segmentation, a straightforward way is to compute joint spatio-temporal self-
attention where all pairwise interactions are modelled between all spatio-
temporal tokens."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"here, we present the corresponding
segmentation predictions of the baseline and our approach along with the ground truth. our stt-unet approach achieves superior segmentation performance by accurately
segmenting 16% more cell instances in these examples, compared to res-unet-r.
segmentation performance on all three datasets. on lucchi test set, our stt-
unet outperforms the recent [4] with an absolute gain of 3.0% in terms of
jaccard-index coeﬃcient."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"on mitoem-h val. [16] on examples from mitoem-r and mitoem-h datasets. 2
related work
most recent approaches for 3d mitochondria instance segmentation utilize con-
volution based designs within the “u-shaped” 3d encoder-decoder architecture.
in such an architecture, the encoder aims to generate a low-dimensional rep-
resentation of the 3d data by gradually performing the downsampling of the
extracted features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"the core component in vits is the self-attention mechanism that that
learns the relationships between sequence elements by performing relevance esti-
mation of one item to other items. [10,19] and based on the observation that attention-based vision
transformers architectures are an intuitive design choice for modelling long-range
global contextual relationships in volume data, we investigate designing a cnn-
transformers based framework for the task of 3d mitochondria instance segmen-
tation. [34] with skip-connections between encoder and
decoder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"we refer to [16] for more details. limitations: as discussed above, the recent res-unet approach utilizes 3d
convolutions to handle the volumetric input data. however, 3d convolutions are
designed to encode short-range spatio-temporal feature information and strug-
gle to model global contextual dependencies that extend beyond the designated
receptive ﬁeld."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"in this way, self-attention mechanism goes
much beyond the receptive ﬁeld of the conventional convolutional ﬁlters. while
self-attention has been shown to be beneﬁcial when combined with convolutional
layers for diﬀerent medical imaging tasks, to the best of our knowledge, no pre-
vious attempt to design spatio-temporal self-attention as an exclusive building
block for the problem of 3d mitochondria instance segmentation exists in liter-
ature. next, we present our approach that eﬀectively utilizes an eﬃcient spatio-
temporal attention mechanism for 3d mitochondria instance segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"consequently, these
semantic masks are post-processed by an instance segmentation module using
a connected component labelling scheme, thereby generating the ﬁnal instance-
level segmentation output prediction. to further enhance the semantic segmen-
tation quality with cluttered background we introduced semantic adversarial loss
which leads to improved semantic segmentation in noisy background. split spatio-temporal attention based encoder-decoder: our stt-
unet framework comprises four encoder and three decoder layers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"best results are in bold. [36] is a dense mitochondria instance seg-
mentation dataset from isbi 2021 challenge. the dataset consists of 2 em image
volumes (30 μm3) of resolution of 8 × 8 × 30 nm, from rat tissues (mitoem-r)
and human tissue (mitoem-h) samples, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"[27]
(rcom env) and models are trained using 2 amd mi250x gpus. during training
of mitoem, for the fair comparison, we adopt same data augmentation technique
from [36]. the 3d patch of size (32×320×320) is input to the model and trained
using batch size of 2."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"for lucchi, we follow training
details of [16,36] for semantic segmentation. for fair comparison with previous
works, we use the same evaluation metrics as in the literature for both datasets. we use 3d ap-75 metric [36] for mitoem-r and mitoem-h datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"table 2 presents the comparison on lucchi test set. our method
sets a new state-of-the-art on this dataset in terms of both jaccard and dsc.
fig. 3. qualitative 3d instance segmentation results of our stt-unet on the exam-
ple input regions from mitoem-h and mitoem-r val sets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_59.pdf,"table 4 shows ablation study with feature
fusion strategies in our sst module: addition, concat and deformable-conv. the
best results are obtained with deformable-conv on both datasets. for encoding
spatial and temporal information, we analyze two design choices with sst mod-
ule: cascaded and split, as shown in table 5."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf,"existing video-based breast lesion detection approaches
typically perform temporal feature aggregation of deep backbone fea-
tures based on the self-attention operation. we argue that such a
strategy struggles to eﬀectively perform deep feature aggregation and
ignores the useful local information. to tackle these issues, we propose a
spatial-temporal deformable attention based framework, named stnet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf,"in our encoder feature shuﬄe strategy, we
share the backbone and encoder features, and shuﬄe encoder features
for decoder to generate the predictions of multiple frames. the exper-
iments on the public breast lesion ultrasound video dataset show that
our stnet obtains a state-of-the-art detection performance, while oper-
ating twice as fast inference speed. the code and model are available at
https://github.com/alfredqin/stnet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf,"image-based breast lesion detection approaches perform detection in each frame
independently. compared to image-based breast lesion detection approaches,
methods based on videos are capable of utilizing temporal information for
improved detection performance. for instance, chen et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf,"although the recent cva-net aggregates clip and video
level features, we distinguish two key issues that hamper its performance. first,
the self-attention based cross-frame feature fusion is a global-level operation and
it operates once before the encoder-decoder, thereby ignoring the useful local
information and in turn missing an eﬀective deep feature fusion. second, cva-
net only performs one-frame prediction based on multiple frame inputs, which
is very time-consuming.
to address the aforementioned issues, we propose a spatial-temporal
deformable attention based network, named stnet, for detecting the breast
lesions in ultrasound videos."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf,"the proposed stnet takes six
frames as inputs and extracts multi-scale features of each frame. afterwards, the pro-
posed stnet utilizes a spatial-temporal deformable attention (stda) based encoder
(b) and decoder (c) for spatial-temporal multi-scale information fusion. finally, the
proposed stnet performs classiﬁcation and regression."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf,"the attention weight atlqk is predicted by feeding
query feature zq to a linear layer and a softmax layer. as a result, the sum of
attention weights is equal to one as
t

t=1
l

l=1
k

k=1
atlqk = 1.
(2)
compared to the standard deformable attention, the proposed spatial-temporal
deformable attention fully exploits spatial information within frame and tempo-
ral information across frames. 2.2
spatial-temporal deformable attention based encoder
and decoder
here, we integrate the proposed spatial-temporal deformable attention (stda)
into encoder and decoder (called st-encoder and st-decoder)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf,"we also stack six self-attention,
stda, and ffn layers in st-decoder for deep feature extraction.
2.3
multi-frame prediction with encoder feature shuﬄe
as discussed above, the proposed stnet adopts six frames to predict the results
of one frame. although stnet fully exploits temporal information for improved
breast lesion detection, it becomes time-consuming for multi-frame prediction. to accelerate the detection speed, we introduce multi-frame prediction with
encoder feature shuﬄe during inference."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf,"compared to the original stnet, the proposed encoder fea-
ture shuﬄe strategy only employs decoder forward three frames and accelerates
the inference speed. 3
experiments
3.1
dataset and implementation details
dataset. we conduct the experiments on the public bluvd-186 dataset [9],
comprising 186 videos including 112 malignant and 74 benign cases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf,"the grounding-truths in
a frame, including breast lesion bounding-boxes and corresponding categories,
are labeled by two pathologists, which have eight years of professional back-
ground in the ﬁeld of breast pathology. we adopt the same dataset splits as in
484
c. qin et al.
table 1. state-of-the-art quantitative comparison of our approach with existing meth-
ods in literature on the bluvd-186 dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_46.pdf,"[4] to initialize the remaining network parameters. to
enhance the diversity of training data, all videos are randomly subjected to hor-
izontal ﬂipping, cropping, and resizing. similar to that of cva-net, we employ a
two-phase training strategy to achieve better convergence."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"there is a wide range of disease that may cause a brachial plexopathy. supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3_46. © the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"compared
with ultrasound, mri has become the primary imaging technique in the evaluation of
brachial plexus pathology [9]. however, to our knowledge, radiomics related bp studies
utilizing mri have not been reported previously. however, how to most effectively combine texture features with deep
learning, called deep texture, is still an open area of research."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"with the goal of classifying normal from abnormal bp, we explored the approach
of deep texture learning. this paper constructed a bp dataset with the most commonly
used bp mris in our clinical practice. considering the shortcoming of traditional pat-
terns, triple point pattern (tpp) is proposed for the quantitative representation of the
heterogeneity of abnormal bp’s."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"finally, we analyze the model’s performance in the experimental section. the major
contributions of this study include 1) directed triangle construction idea for tpp, 2) huge
number of tpp matrices as the heterogeneity representations of bp, 3) tppnet with 15
layers and huge number of channels, 4) the bp dataset containing mr images and their
corresponding roi masks. 2
materials and method
2.1
dataset preparation and preprocessing
following irb approval for this study, we search for patients with metastatic breast
cancer who had a breast cancer mri performed between 2010 and 2020 and had mor-
phologically positive bp on the mri report from our electronic medical records (emr)
in * hospital."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"there-
fore, each case underwent several essential image adjustments such as multi-series
splitting,two-seriesmerging,sliceswapping,artifactcheckingandboundarycorrections.
table 1. the ﬁnal bp dataset including mris and their corresponding masks of t2, t1, post-
gadolinium (pg). image 
total
normal
abnormal
t2
t1
pg
t2
t1
pg
mri
462
123
123
123
31
31
31
mask
462
123
123
123
31
31
31
to yield the roi, ﬁrstly, we randomly sampled −40% of the sequences including
both normal and abnormal ones that were manually segmented with itk- snap by two
skilled trainees [14, 15]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"this process
was repeated until no improvements in the predictions for the remaining sequences was
seen. the ﬁnal dataset for radiomic analysis was constructed by merging the datasets
for each sequence type. only patients that had all three sequences segmented (t2, t1
and post-gadolinium) were included in the dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"one obvious shortcoming is the absence of global properties which need
other statistical methods as the aid to yield, such as histogram and invariants [20, 21]. in general, image textures extracted by these methods contain both
local texture properties and global texture information. their shortcomings might come
from pattern shapes which might lead to the overﬁtting risk while combining with deep
learning since the yielded texture matrix might have slim shapes or adaptive columns."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"in summary, as the requirement of the image texture and deep learning, an excellent
image texture pattern should have some essential features including 1) local proper-
ties to characterize the micro-unit of the image texture, 2) global properties to represent
a texture neural network to predict the abnormal brachial plexus
473
the macro-structure of the image texture, 3) uniform shapes under nonuniform-shape
images, 4) invariant or robustness under some common geometric transforms such as
rotation, scaling and so on. according above requirements, we developed a method to produce a serial of novel
texture patterns by introducing a directed triangle idea with an adjacent triple pixel as a
ternary group, called triple point pattern (tpp), to extract the local texture information. then, a statistical method like histogram is employed to count the number of the same
type of pixel-triplets within the roi or throughout the whole image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"moreover, tpps generated by 135°
could also be treated as afﬁne transformations. therefore, data augmentation could be
omitted when we combine tpp with deep learning for this study. as its deﬁnition, the
tpp matrix should be a cubic array with the shape of lxlxl where l is the gray level
of the image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"2.3
tppnet
the pipeline of the proposed method tppnet is illustrated in fig. 2. the tpp matrix
calculation is the preprocessing module which feeds mri and its roi and yield tpp
a texture neural network to predict the abnormal brachial plexus
475
matrix set. the following step is the tppnet architecture to yield training models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"it has four particular features as follows:
1) avoidance of image augmentation. due to the stability of tpp matrix under rota-
tion, scale and afﬁne transformations, image augmentation could be omitted in the
preprocessing step which can lead to image deformation. 2) huge number of channels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"since each channel
is corresponding with one tpp, it can solve the pattern arrangement issue occurred
in glcm-cnn.
table 2. test performances for different gray levels over t2, t1 and post-gadolinium where acc
denotes accuracy and pg denotes post-gadolinium. the whole dataset is divided into three subsets according to the mr
sequence, i.e. t2, t1 and post-gad. for each subset, both normal cases and abnormal
cases were randomly and evenly split into ﬁve subgroups."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"each
cohort consists of training set, validation set and testing set by the ratio of 6:2:2. 3.2
ablation studies
since all images in our dataset are 3d images, therefore, the initial channel is set 325
which is equal to the tpp number. the loss functions in the following experiments
shared categorical_crossentropy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_46.pdf,"as a comparison, we test single channel mode
as well. by sharing every tpp matrix’s label with the original case label, our tppnet
works well by assigning one channel for the initial input. once the trained model with
solo channel is generated, all tpp matrices of the testing set could be tested."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"the pro-
posed network not only combines both ﬁne and coarse histological pat-
terns but also utilizes their interactions for improved risk stratiﬁcation. we compared the performance of our proposed model against the state-
of-the-art (sota) techniques in histopathology risk stratiﬁcation in two
cancer datasets. our results suggest that the proposed model is capable
of stratifying patients into statistically signiﬁcant risk groups (p < 0.01
across the two datasets) with clinical utility while competing models fail
to achieve a statistical signiﬁcance endpoint (p = 0.148 − 0.494)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"the great rise of deep learning in the past decade and our ability to digitize
histopathology slides using high-throughput slide scanners have fueled inter-
ests in the applications of deep learning in histopathology image analysis. the
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 74.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. et al.
majority of the eﬀorts, so far, focus on the deployment of these models for diag-
nosis and classiﬁcation [27]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"however, most gnn-based models suﬀer from over smoothing [22] which
limits nodes’ receptive ﬁelds [3]. while local contexts mainly capture cell-cell
interactions, global patterns such as immune cell inﬁltration patterns and tumor
invasion in normal tissue structures (e.g., depth of invasion through myometrium
in endometrial cancer [1]) could capture critical information about outcome [10].
hence, locally focused methods are unable to beneﬁt from the coarse properties
of slides due to their high dimensions which may lead to poor performance. this paper aims to investigate the potential of extracting ﬁne and coarse
features from histopathology slides and integrating them for risk stratiﬁcation
in cancer patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"finally, a multilayer perceptron (mlp) is deployed to esti-
mate the risk using ﬁnal resultant vectors. representations without any labels. then, by using distillation loss, it makes the
representations’ distribution similar to each other."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"assume that x′
n ∈ rm×d and sn ∈ rk×d are
the feature matrices taken from local gnn (sect. 3.3) and super-nodes for pn,
respectively. sigm(wk,gst
n )

,
(7)
where x′
n,i and sn,i are rows of x′
n and sn, respectively, and the ﬁnal repre-
sentation (ˆh) is generated as ˆh = cat(ˆhl, ˆhg).
– mixed guided attention (mga): in the ﬁrst strategy, the information
ﬂows from local and global features to the ﬁnal representations in paral-
lel without mixing any knowledge. the purpose of this policy is the heavy
fusion of ﬁne and coarse knowledge by exploiting shared weights (wφ,shared,
wk,shared, wq,shared, wv,shared) in both routes and beneﬁting from the guid-
ance of local representation on learning the global one by modifying eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"here, we take a balanced policy between the independence and knowl-
edge mixture of the two routes by only sharing the weights without using any
guidance. 4
experiments and results
4.1
dataset
we utilize two prostate cancer (pca) datasets to evaluate the performance of
our proposed model. the ﬁrst set (pca-as) includes 179 pca patients who
were managed with active surveillance (as)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"although
majority of patients in our cohort are classiﬁed as low-risk based on nccn guide-
lines [21], a signiﬁcant subset of them experienced disease upgrade that triggered
deﬁnitive therapy (range: 6.2 to 224 months after diagnosis). the second dataset (pca-bt) includes 105 pca patients with low to high
risk disease who went through brachytherapy. this treatment involves placing a
radioactive material inside the body to safely deliver larger dose of radiation at
all-in
771
table 1. comparison of our method against baselines and ablation study on policies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"the recorded endpoint for this set is biochemical recurrence with
time to recurrence ranging from 11.7 to 56.1 months. we also utilized the prostate cancer grade assessment (panda) challenge
dataset [7] that includes more than 10,000 pca needle biopsy slides (no outcome
data) as an external dataset for training the encoder of our model.
4.2
experiments
we evaluate the models’ performance in two scenarios utilizing several objective
metrics. implementation details are available in supplementary material."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"statistical tests
(paired t-test) on c-indices also show that our model is statistically better than
all baselines in pca-as and also superior to all models, except dgc, in pca-bt. superior performance of our mca policy implies that balanced exploitation of
ﬁne and coarse features with shared weights may provide more robust contex-
tual information compared to using mixed guided information or utilizing them
independently. the capacity of stratifying patients into risk groups
(e.g., low and high risk) is another criterion that we employ to assess the util-
ity of models in clinical practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"moreover, pca-bt cases assigned to high- and
low-risk groups have median recurrence time of 21.86 and 35.7 months. while
none of the baselines are capable of assigning patients into risk groups with
statistical signiﬁcance, our distillation policies achieve signiﬁcant separation in
both pca-as and pca-bt datasets; suggesting that global histo-morphological
properties improve patient stratiﬁcation performance. furthermore, our ﬁndings
have signiﬁcant clinical implications as they identify, for the ﬁrst time, high-
risk prostate cancer patients who are otherwise known to be low-risk based on
clinico-pathological parameters."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"this group should be managed diﬀerently from
the rest of the low-risk prostate cancer patients in the clinic. therefore, pro-
viding evidence of the predictive (as opposed to prognostic) clinical information
that our model provides. while a prognostic biomarker provides information
about a patient’s outcome (without speciﬁc recommendation on the next course
of action), a predictive biomarker gives insights about the eﬀect of a therapeutic
intervention and potential actions that can be taken.
ablation study."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_74.pdf,"we also assess the impact of our vit on
the baselines (full-results in appendix), showing that it can, on average, improve
their performance by an increase of ∼ 0.03 in c-index for pca-as. however, the
best baseline with vit still has poorer performance compared to our model in
both datasets, while the number of parameters (reported for vit embeddings’
size in table 1) in our full-model is about half of this baseline. achieving higher
c-indices in our all model versions indicates the important role of coarse features
and global context in patient risk estimation in addition to local patterns.
all-in
773
table 2. ablation study on diﬀerent modules."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf,"speciﬁcally, we dynamically determine each region
by ﬁrst identifying an informative area and then detecting its optimal
bounding box, as opposed to selecting regions of a uniform predeﬁned
shape and size as in the standard method. we evaluate our method
using the task of breast cancer metastases segmentation on the public
camelyon16 dataset and show that it consistently achieves higher
sampling eﬃciency than the standard method across various al step
sizes. with only 2.6% of tissue area annotated, we achieve full annota-
tion performance and thereby substantially reduce the costs of annotat-
ing a wsi dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf,"the source code is available at https://github.com/
deepmicroscopy/adaptiveregionselection. keywords: active learning · region selection · whole slide images
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 9. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14221, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf,"however, pixel-level anno-
tations of gigapixel-sized wsis (e.g. 100, 000 × 100, 000 pixels) for training a
segmentation model are diﬃcult to acquire. [18]. identifying potentially informative image regions (i.e., providing
useful information for model training) allows requesting the minimum amount
of annotations for model optimization, and a decrease in annotated area reduces
both localization and delineation workloads. the challenge is to eﬀectively select
annotation regions in order to achieve full annotation performance with the least
annotated area, resulting in high sampling eﬃciency."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf,"[5] and highest disagreement between a set of models [19]). the enhancement of priority maps, such as highlighting easy-to-label pixels [13],
edge pixels [6] or pixels with a low estimated segmentation quality [2], is also
a popular area of research. second, on the priority map, regions are selected
according to a region selection method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf,"(image resolution: 0.25 µm
px )
region by ﬁrst identifying an informative area with connected component detec-
tion and then detecting its bounding box. we test our method using a breast
cancer metastases segmentation task on the public camelyon16 dataset and
demonstrate that determining the selected regions individually provides greater
ﬂexibility and eﬃciency than selecting regions with a uniform predeﬁned shape
and size, given the variability in histological tissue structures. results show that
our method consistently outperforms the standard method by providing a higher
sampling eﬃciency, while also being more robust to al step size choices."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf,"a patch with less than 1% tissue content is discarded. data augmentation includes random ﬂip, random rotation, and stain augmenta-
tion [12]. inference. [0, 1]w ′
i ×h′
i, where each pixel represents a patch prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf,"[10], licensed
under the creative commons cc0 license. the collection of the data was
approved by the responsible ethics committee (commissie mensgebonden onder-
zoek regio arnhem-nijmegen). the camelyon16 dataset consists of 399
hematoxylin & eosin (h&e)-stained wsis of sentinel axillary lymph node sec-
tions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf,"[14]
weights as the backbone of the patch classiﬁcation model. it is extended with two
1 test 114 is excluded due to non-exhaustive annotation, as stated by data provider. adaptive region selection for al in wsi semantic segmentation
95
fully-connected layers with sizes of 512 and 2, followed by a softmax activation
layer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf,"the
running time of one al cycle (select-train-test) on a single nvidia geforce
rtx3080 gpu (10gb) is around 7 h.
active learning setups. since the camelyon16 dataset is fully annotated,
we perform al by assuming all wsis are unannotated and revealing the anno-
tation of a region only after it is selected during the al procedure. we divide
the wsis in u randomly into ﬁve stratiﬁed subsets of equal size and use them
sequentially."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_9.pdf,"3.3
results
full annotation performance. to validate our segmentation framework, we
ﬁrst train on the fully-annotated data (average performance of ﬁve repetitions
reported). with a patch extraction stride s = 256 pixels, our framework yields
an froc score of 0.760 that is equivalent to the challenge top 2, and an miou
(tumor) of 0.749, which is higher than the most comparable method in [3] that
achieved 0.741 with s = 128 pixels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"immunohistochemical (ihc) staining highlights the molecu-
lar information critical to diagnostics in tissue samples. however, com-
pared to h&e staining, ihc staining can be much more expensive in
terms of both labor and the laboratory equipment required."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"the asp loss is built upon a patch-
based contrastive learning criterion, named supervised patchnce (sp),
and augments it further with weight scheduling to mitigate the negative
impact of noisy supervision. lastly, we introduce the multi-ihc stain
translation (mist) dataset, which contains aligned h&e-ihc patches
for 4 diﬀerent ihc stains critical to breast cancer diagnosis. in our exper-
iment, we demonstrate that our proposed method outperforms existing
image-to-image translation methods for stain translation to multiple ihc
stains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"all of our code and datasets are available at https://github.com/
lifangda01/adaptivesupervisedpatchnce. keywords: generative adversarial network · contrastive learning ·
h&e-to-ihc stain translation
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 61. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"for instance, the her2 (human epidermal growth factor
receptor 2) biomarker is associated with aggressive breast tumor development
and is essential in forming a precise treatment plan. despite its capability to
provide highly valuable diagnostic information, the process of ihc staining is
very labor-intensive, time-consuming and requires specialized histotechnologists
and laboratory equipments [2]. such factors hinder the general availability of
ihc staining in histopathological applications."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"in this paper, we argue that the ihc slides, despite the disparities vis-a-vis
their h&e counterparts, can still serve as useful targets for stain translation. the
634
f. li et al.
work we present in this paper is based on the important realization that even
when pairs of consecutive tissue slices do not yield images that are pixel-perfect
aligned, it is highly likely that the corresponding patches in the two stains share
the same diagnostic label. for example, if the levels of expression in a region of
the her2 slide are high, the corresponding region in the h&e slide is highly
likely to contain a high density of cancerous cells."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"toward this goal, we propose a supervised patchwise contrastive loss named
the adaptive supervised patchnce (asp) loss. our formulation of this loss
was inspired by the recent research ﬁndings that contrastive loss beneﬁts model
robustness under label noise [3,12]. furthermore, based on the observation that
any dissimilarity between the patch embeddings at corresponding locations in the
generated and groundtruth ihc images is indicative to the level of inconsistency
of the gt at that location, we employ an adaptive weighting scheme in asp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"by
down-weighting the contrastive loss at locations with low similarities, i.e. high
inconsistencies, our proposed asp loss helps the network learn more robustly.
lastly, to support further research in virtual ihc-restaining, we present the
multi-ihc stain translation (mist) as a new public dataset. the mist dataset
contains 4k+ training and 1k testing aligned h&e-ihc patches for each of the
following ihc stains that are critical for breast cancer diagnostics: her2, ki67,
er (estrogen receptor) and pr (progesterone receptor). we evaluated existing
i2it methods and ours for multiple ihc stains and demonstrate the superior
performance achieved by our method both qualitatively and quantitatively.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"the patch embeddings z are extracted by a shared network f.
adaptive supervised patchnce loss
635
2
method description
2.1
the supervised patchnce (sp) loss
before getting to our asp loss, we need to ﬁrst introduce the sp loss as a robust
means to learning from inconsistent gt image pairs. the sp loss was inspired by
the ﬁndings in recent literature that demonstrate the positive eﬀect of contrastive
learning on boosting model robustness against label noise [3,12,14]. it takes the
same form as the patchnce loss as introduced in [11], except that it is applied
on the generated-gt image pair (instead of the input-generated pair)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"ladv + λpatchncelpatchnce + λasplasp + λgplgp,
(4)
where lgp is the gaussian pyramid based reconstruction loss from [7].
table 1. quantitative evaluations on all three datasets using both paired and unpaired
metrics. the best values are highlighted."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"cut is from [11]
3
experiments
datasets. [7] and our own mist dataset
that is now in the public domain. the publicly available portion of bci contains
3396 h h&e-her2 patches for training and 500 of the same for testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"note
that we have additionally normalized the brightness levels of all bci images to
the same level. due to the page limit, from the mist dataset, here we only
present detailed results on her2 and er. for misther2, we extracted 4642
paired patches for training and 1000 for testing from 64 wsis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_61.pdf,"the full results comparing existing i2it methods
to ours are tabulated in tab. 1. overall, it can be observed that the proposed
framework with the asp loss consistently outperforms existing methods across
all three datasets. for those methods, fig. 5 visually illustrates the extent of
hallucinations which we believe is the reason for their poor quantitative perfor-
mance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"we introduce a new ai-ready computational pathology
dataset containing restained and co-registered digitized images from
eight head-and-neck squamous cell carcinoma patients. speciﬁcally,
the same tumor sections were stained with the expensive multiplex
immunoﬂuorescence (mif) assay ﬁrst and then restained with cheaper
multiplex immunohistochemistry (mihc)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"this is a ﬁrst public dataset
that demonstrates the equivalence of these two staining methods which in
turn allows several use cases; due to the equivalence, our cheaper mihc
staining protocol can oﬀset the need for expensive mif staining/scanning
which requires highly-skilled lab technicians. as opposed to subjec-
tive and error-prone immune cell annotations from individual pathol-
ogists (disagreement > 50%) to drive sota deep learning approaches,
this dataset provides objective immune and tumor cell annotations via
mif/mihc restaining for more reproducible and accurate characteri-
zation of tumor immune microenvironment (e.g. for immunotherapy). we demonstrate the eﬀectiveness of this dataset in three use cases: (1)
ihc quantiﬁcation of cd3/cd8 tumor-inﬁltrating lymphocytes via style
transfer, (2) virtual translation of cheap mihc stains to more expensive
mif stains, and (3) virtual tumor/immune cellular phenotyping on stan-
dard hematoxylin images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"keywords: multiplex immuoﬂuorescence · multiplex
immunohistochemistry · tumor microenvironment · virtual
stain-to-stain translation
p. ghahremani, j. marino, c. h. chung, and s. nadeem—equal contribution. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 68.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43987-2_68
an ai-ready multiplex staining dataset
705
1
introduction
accurate spatial characterization of tumor immune microenvironment is critical
for precise therapeutic stratiﬁcation of cancer patients (e.g. via immunother-
apy)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"however, this results in high interobserver variability
among pathologists, primarily due to the large (> 50%) disagreement among
pathologists for immune cell phenotyping [10]. this is also a big cause of con-
cern for publicly available h&e/ihc cell segmentation datasets with immune
cell annotations from single pathologists. multiplex staining resolves this issue
by allowing diﬀerent tumor and immune cell markers to be stained on the same
tissue section, avoiding any phenotyping guesswork from pathologists."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"[2], both without any immune or tumor markers. in contrast, we
release the ﬁrst denovo mif/mihc stained dataset with tumor and immune
markers for more accurate characterization of tumor immune microenvironment. we also demonstrate several interesting use cases: (1) ihc quantiﬁcation of
cd3/cd8 tumor-inﬁltrating lymphocytes (tils) via style transfer, (2) virtual
translation of cheap mihc stains to more expensive mif stains, and (3) virtual
tumor/immune cellular phenotyping on standard hematoxylin images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"demographics and other relevant details of the eight anonymized head-and-
neck squamous cell carcinoma patients, including ecog performance score, pack-year,
and surgical pathology stage (ajcc8). 2
oral cavity lateral tongue
case7 73
male
white 1
former
100
3
larynx
glottis
case8 56
male
white 0
never
0
2
oral cavity tongue
2
dataset
the complete staining protocols for this dataset are given in the accompany-
ing supplementary material. images were acquired at 20× magniﬁcation at
moﬃtt cancer center."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"the size of the rois was
standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total
surface area of 0.343 mm2. hematoxylin-stained rois were ﬁrst used to align all
an ai-ready multiplex staining dataset
707
the mihc marker images in the open source fiji software using aﬃne registra-
tion. after that, hematoxylin- and dapi-stained rois were used as references
to align mihc and mif rois again using fiji and subdivided into 512×512
patches, resulting in total of 268 co-registered mihc and mif patches (∼33
co-registered mif/mihc images per patient)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"the fourth column shows the
results of the concordance study. each square represents an image in the dataset and
the top half of each square shows the mean color value of the positive cells, extracted
from mihc-aec using its corresponding mif image and the bottom half of it shows
the mean color value of its background. the high intensity of the top half of the squares
represents positive cells and the low intensity of the bottom half represents non-positive
cells (background), which is seen in almost all squares, demonstrating high concor-
dance among mihc-aec and mif data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"et al.
using identical slides. it provides a standardized dataset to demonstrate the
equivalence of the two methods and a source that can be used to calibrate other
methods. 3
use cases
in this section, we demonstrate some of the use cases enabled by this high-quality
ai-ready dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"3. examples of synthesized ihc images and corresponding input images. style ihc
images were taken from the public lyon19 challenge dataset [14]. we used grayscale
hematoxylin images because they performed better with style transfer.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"the segmented
masks were classiﬁed using the cd3 channel intensities. an ai-ready multiplex staining dataset
709
table 2. quantitative metrics for nuclick and lysto testing sets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"the complete architecture diagram is given in the
supplementary material. speciﬁcally, the model consists of two sub-networks:
(a) marker generation: this sub-network is used for generating mif marker
data from the generated stylized image. [4] for generating the marker images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"[9] tool https://github.com/nadeemlab/impartial and then classiﬁed
the segmented masks using the corresponding cd3/cd8 channel intensities, as
shown in fig. 4. we extracted 268 tiles of size 512×512 from this ﬁnal segmented
and co-registered dataset. for the purpose of training and testing all the models,
we extract four images of size 256 × 256 from each tile due to the size of the
external ihc images, resulting in a total of 1072 images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"[14] to use as style ihc images. using
these images, we created a dataset of synthetically generated ihc images from
the hematoxylin and its marker image as shown in fig. we randomly selected 840 and 230 patches of size 256×256
from the created dataset for training and validation, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"[15] with the backbone of resnet50 for
fig. 5. examples of ground-truth and generated mif data from mihc-aec images. an ai-ready multiplex staining dataset
711
200 epochs and early stopping on validation score with patience of 30 epochs,
using binary cross entropy loss and adam optimizer with learning rate of 0.0001.
as shown in table 2, models trained with our synthetic training set outperform
those trained solely with nuclick data in all metrics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"[1], con-
taining image patches of size 299 × 299 obtained at a magniﬁcation of 40× from
breast, prostate, and colon cancer whole slide images stained with cd3 and cd8
markers. only the total number of lymphocytes in each image patch are reported
in this dataset. to evaluate the performance of trained models on this dataset,
we counted the total number of marked lymphocytes in a predicted mask and
calculated the diﬀerence between the reported number of lymphocytes in each
image with the total number of lymphocytes in the predicted mask by the model.
in table 2, the average diﬀerence value (diﬀcount) of lymphocyte number for
the whole dataset is reported for each model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"3, where
brown marker channel has a blue hematoxylin nuclear counterstain to stain
for all the cells, our mihc aec-stained marker images (fig. 5) do not stain
for all the cells including nuclei. in this use case, we show that mihc marker
images can be translated to higher quality mif dapi and marker images which
fig. 6. examples of ground-truth and generated mif immune (cd3) and tumor
(panck) markers from standard hematoxylin images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"some examples of testing
the trained model on cd3 images are shown in fig. 5. we calculated the mean
squared error (mse) and structural similarity index (ssim) to evaluate the
quality of the inferred modalities by the trained model. the mse and ssim
for mif dapi was 0.0070 and 0.9991 and for mif cd3 was 0.0021 and 0.9997,
indicating high accuracy of mif inference."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_68.pdf,"we train the translation task of deepliif model
using the hematoxylin, immune (cd3) and tumor (panck) markers. sample
images/results taken from the testing dataset are shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"the bethesda system for reporting thyroid cytopathol-
ogy (tbsrtc) has been widely accepted as a reliable criterion for
thyroid cytology diagnosis, where extensive diagnostic information can
be deduced from the allocation and boundary of cell nuclei. however,
two major challenges hinder accurate nuclei segmentation from thyroid
cytology."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"secondly,
the insuﬃciency of densely annotated images results in a less gener-
alized model. in contrast, image-wise tbsrtc labels, while contain-
ing lightweight information, can be deeply explored for segmentation
guidance. to this end, we propose a tbsrtc-category aware nuclei
segmentation framework (tcsegnet)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"to top up the small amount of
pixel-wise annotations and eliminate the category preference, a larger
amount of image-wise labels are taken in as the complementary supervi-
sion signal in tcsegnet. this integration of data can eﬀectively guide
the pixel-wise nuclei segmentation task with a latent global context. we also propose a semi-supervised extension of tcsegnet that lever-
ages images with only tbsrtc-category labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"in conclu-
sion, our study explores the weak annotations by constructing an image-
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43987-2_56
tcseqnet
581
wise-label-guided nuclei segmentation framework, which has the poten-
tial medical importance to assist thyroid abnormality examination. code
is available at https://github.com/junchao-zhu/tcsegnet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"clinically, pathologists rely on the
six-category of “the bethesda system for reporting thyroid cytopathology”
(tbsrtc) [2,3] to distinguish the cell morphology in the stained cytopathologic
sections. the emergence of computational pathology allows automatic diagno-
sis of thyroid cancer, and nuclei segmentation becomes one of the most critical
diagnostic tasks [4,5], as the shapes of nuclei, whether round, oval, or elongated,
can provide valuable information for further analysis [6]. for example, small and
scattered thyroid cells with a light hue and relatively low cell density are usually
low-grade and indicative of early-stage cancer; whereas large and dark cells with
extreme-dense agglomeration are usually middle- or late-grade [3]. correspond-
ingly, accurate location of cell boundaries is essential for both pathologists and
computer-aided diagnosis (cad) systems to assist decision [7]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"by contrast,
high-grade cells (v & vi) are densely packed and severely clustered, thus much
more are presented in a training set. such distinct morphological diﬀerences can
be characterized by the tbsrtc category, which thus inspires us to utilize
the handy image-wise grading labels to guide the nuclei segmentation model
learning from unbalanced datasets. we also noticed that another challenge for
accurate nuclei identiﬁcation is the heavy reliance on large-scale high-quality
annotations [11]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"moreover, amongst multiple annotation paradigms [12], pixel-
level labeling is the most time-consuming and laborious, whereas the image-wise
diagnostic labels, i.e. tbsrtc categories, are comparatively simpler. [15], are limited to pixel-wise annotations, where
the potential beneﬁts of integrating accessible image-wise labels are unaware.
to narrow the gap discussed, we propose a novel tbsrtc-category-aware
nuclei segmentation framework. our contributions are three-fold."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"582
j. zhu et al.
innovatively, our approach can help reduce bias in the learning process of the
segmentation model with the routine unbalanced training set. (2) we expand
tcsegnet to semi-tcsegnet to leverage image-wise labels in a semi-supervised
learning manner, which signiﬁcantly reduces the reliance on annotation-intensive
pixel-wise labels. additionally, an hsv-intensity noise is designed speciﬁcally
for cytopathology images to boost the generalization ability."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"(3) we establish a
dataset of thyroid cytopathology image patches of 224 × 224, where 4,965 image
labels are provided following tbsrtc, and 1,473 of them are densely anno-
tated [3] (to be on github upon acceptance). to the best of our knowledge,
it is the ﬁrst publicized thyroid cytopathology dataset of both image-wise and
pixel-wise labels. the annotated dataset well alleviates the insuﬃciency of an
open cytopathology dataset for computer-assisted analysis (fig. 1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"1. an overview of the proposed tcsegnet and semi-tcsegnet. tcsegnet uti-
lizes annotation-lightweight image-wise tbsrtc-category labels to aid in the learning
of unbalanced nuclei morphology in segmentation. semi-tcsegnet reduces the heavy
reliance on annotation-intensive pixel-wise labels through the use of a semi-supervised
framework."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"2
methodology
overview. we propose a novel tbsrtc-category aware segmentation net-
work (tcsegnet) to segment nuclei boundaries in cytopathology images, which
tcseqnet
583
is guided by tbsrtc-category label to learn from unbalanced data. considering
the spatial distributions of thyroid cells in cytopathology images, our design
provides extended global information for more accurate segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"we set the balancing coeﬃcient γni to 1 and γnb to 10.
additionally, to ensure the consistency between the two branches, we impose a
dice consistency loss (lcons) between the nuclei instance predictions from the
cnn branch and the transformer branch, namely lcons = dice(ˆycnn
ni , ˆytrans
ni
). tbsrtc-category label guidance block. in tcsegnet, we introduce
a tbsrtc-category label guidance block to address the learning issue from
unbalanced routine datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"this block consists of two learnable fully connected
layers that process the feature extracted by the cnn and transformer branches
separately, which obtains image-wise tbsrtc-category prediction denoted as
ˆycnn
cls and ˆytrans
cls
. = ce(ˆycnn
cls , ycls) + γcls · ce(ˆytrans
cls
, ycls),
(2)
where ycls is the image-wise tbsrtc-category label, and the balancing coeﬃ-
cient γcls is set to 3, as the global feature captured by the transformer branch
is tightly correlated with the image-level classiﬁcation tag. finally, the overall
loss for tcsegnet becomes
ls = lseg + lcls + lcons."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"(3)
extension to semi-supervised learning. to leverage images that only have
image-wise labels, we extend to a semi-supervised mean teacher [18] framework
called semi-tcsegnet. in this framework, both the student and teacher share the
same full-supervised nuclei segmentation architecture of tcsegnet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"the weights
of the teacher θt are updated with the exponential moving average (ema) of the
weights of student θs, and smoothing coeﬃcient α = 0.99, following the previous
work [19]. formally, the weights of the teacher at e-th epoch are updated by
θe
t = αθe−1
t
+ (1 − α)θe
s.
(4)
584
j. zhu et al.
during the training stage, the teacher model assigns pixel-wise soft labels to
the images with exclusive image-wise labels, thus expanding the scale of labeled
data to the student model. the overall loss function for training the student is
a combination of the supervised loss ls in eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"finally, the value of the
obtained noise is clamped to [−0.2, 0.2] before being added to the images. 3
experiments
image dataset. some representative images are presented in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"2,
together with the proﬁle of the dataset. the dataset comprises 4,965 h&e stained
image patches and labels of tbsrtc, where a subset of 1,473 images was
densely annotated for nuclei boundaries by three experienced cytopathologists
and reached a total number of 31,064 elaborately annotated nuclei. patient-level
images were partitioned ﬁrst for training and test images, and patch-level cura-
tion was performed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"we divided the dataset with image-wise labels into 80%
training samples and the remaining 20% testing samples. our collection of thy-
roid cytopathology images was granted with an ethics approval document. tcseqnet
585
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"the annotated thyroid cytopathology benchmark. a) examples from i to vi
of tbsrtc six diagnostic categories with pixel-wise nuclei mask and boundary anno-
tations; b) the proﬁle of the dataset. quantitative comparisons in both fully-supervised and semi-supervised man-
ners."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"also, it yields that the incor-
poration of tbsrtc-category can contribute to a partial alleviation of a biased
model, resulting in more satisfying segmentation performance experimentally. furthermore, the fact that the tbsrtc-category label is easy to obtain endows
tcseqnet
587
the applicability of our model to various circumstances that nuclei in various
sizes, shapes, and dyeing styles can be accurately recognized and segmented. consequently, it can serve as a guarantee for the validity and accuracy of the
subsequent analysis in real clinical practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"the performance improvement of 1.2% dice, 1.7% iou,
together with the general improvement is shown in the boxplot in fig. 4 (c, d), as
a demonstration of the advantage using full data resources with semi-tcsegnet.
fig. 4. quantitative comparison presented by boxplot and line chart."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_56.pdf,"to evaluate the eﬀectiveness of each functional block and
demonstrate the functionality of semi-supervised learning, we illustrate the abla-
tion study in table 2. the results indicate that performance improvement is
accumulated with increasing data size. besides, training with a classiﬁcation-
learning block alone can increase the nuclei segmentation performance by 1.7%
and 2.6% in the dice score and iou, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_5.pdf,"recent deep learning-based methods have achieved
good performance in combining multiple available sequences for missing
sequence synthesis. despite their success, these methods lack the ability
to quantify the contributions of diﬀerent input sequences and estimate
region-speciﬁc quality in generated images, making it hard to be practical. hence, we propose an explainable task-speciﬁc synthesis network, which
adapts weights automatically for speciﬁc sequence generation tasks and
provides interpretability and reliability from two sides: (1) visualize and
quantify the contribution of each input sequence in the fusion stage by
a trainable task-speciﬁc weighted average module; (2) highlight the area
the network tried to reﬁne during synthesizing by a task-speciﬁc atten-
tion module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_5.pdf,"keywords: missing-sequence mri synthesis · explainable synthesis ·
multi-sequence fusion · task-speciﬁc attention
t. zhang and l. han—contributed equally to this work. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 5.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43999-5_5
46
l. han et al.
1
introduction
magnetic resonance imaging (mri) consists of a series of pulse sequences, e.g.
t1-weighted (t1), contrast-enhanced (t1gd), t2-weighted (t2), and t2-ﬂuid-
attenuated inversion recovery (flair), each showing various contrast of water and
fat tissues."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_5.pdf,"most of these works introduce an autoencoder-like architecture
for image-to-image translation and employ adversarial loss to generate more
realistic images. unlike these one-to-one approaches, mri synthesis faces the
challenge of fusing complementary information from multiple input sequences. recent studies about multi-sequence fusion can speciﬁcally be divided into two
groups: (1) image fusion and (2) feature fusion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_5.pdf,"the
model architectures of these methods are not ﬂexible and diﬃcult to adapt to
various sequence combinations. more importantly, recent studies only focus on
proposing end-to-end models, lacking quantifying the contributions for diﬀerent
sequences and estimating the qualities of generated images. in this work, we propose an explainable task-speciﬁc fusion sequence-to-
sequence (tsf-seq2seq) network, which has adaptive weights for speciﬁc synthe-
sis tasks with diﬀerent input combinations and targets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_5.pdf,"finally, the fused features are decoded to the
target sequence by g. furthermore, to explain the mechanism of multi-sequence
fusion, our network can quantify the contributions of diﬀerent input sequences
with the task-speciﬁc weighted average module and visualize the tsem with
the task-speciﬁc attention module. to leverage shared information between sequences, we use e and g from
seq2seq [10], which is a one-to-one synthetic model that integrates arbitrary
sequence synthesis into single e and g. they can reduce the distance between
diﬀerent sequences at the feature level to help more stable fusion. details of the
multi-sequence fusion module and tsem are described in the following sections."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"speciﬁcally, we ﬁrst seg-
ment the liver and vessels from the ct image, and generate 3d
liver point clouds and voxel grids embedded with vessel structure
prior. then, we design a multi-scale point-voxel fusion network to cap-
ture the anatomical structure and semantic information of the liver
and vessels, respectively, while also increasing important data access
through vessel structure prior. finally, the network outputs the clas-
siﬁcation of couinaud segments in the continuous liver space, producing
a more accurate and smooth 3d couinaud segmentation mask."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"our pro-
posed method outperforms several state-of-the-art methods, both point-
based and voxel-based, as demonstrated by our experimental results on
two public liver datasets. code, datasets, and models are released at
https://github.com/xukun-zhang/couinaud-segmentation. keywords: couinaud segmentation · point-voxel network · liver ct
x. zhang and y. liu—contributed equally."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"it can supplement the cnn-based method and improve the segmentation per-
formance in regions without intensity contrast. in this paper, to tackle the aforementioned challenges, we propose a point-
voxel fusion framework that represents the liver ct in continuous points to
better learn the spatial structure, while performing the convolutions in voxels
to obtain the complementary semantic information of the couinaud segments.
fig. 1. couinaud segments (denoted as roman numbers) in relation to the liver vessel
structure."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"2
method
the overview of our framework to segment couinaud segments from ct images
is shown in fig. 2, including the liver segmentation, vessel attention map gener-
ation, point data sampling and multi-scale point-voxel fusion network. 2.1
liver mask and vessel attention map generation
liver segmentation is a fundamental step in couinaud segmentation task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"we employ the bce loss to supervise the learning process. 2.2
couinaud segmentation
based on the above work, we ﬁrst use the m ′ and the l to sample get point data,
which can convert into a voxel grid through re-voxelization. the converted voxel
grid embeds the vessel prior and also dilutes the liver parenchyma information.
inspired by [12], a novel multi-scale point-voxel fusion network then is proposed
to simultaneously process point and voxel data through point-based branch and
voxel-based branch, respectively, aiming to accurately perform couinaud seg-
mentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"continuous spatial point sampling based on the vessel attention
map. in order to obtain the topological relationship between couinaud seg-
ments, a direct strategy is to sample the coordinate point data with 3d spatial
information from liver ct and perform point-wise classiﬁcation. p = i ∗ spacing ∗ direction + origin,
(1)
where spacing represents the voxel spacing in the ct images, direction repre-
sents the direction of the scan, and origin represents the world coordinates
of the image origin."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"based on equation(1), we obtain the world coordinate
pt = (xt, yt, zt) corresponding to each point it in the liver space. however,
directly feeding the transformed point data as input into the point-based branch
undoubtedly ignores the vessel structure, which is crucial for couinaud segmen-
tation. to solve this issue, we propose a strategy of continuous spatial sampling
point data based on the m ′. speciﬁcally, the model randomly samples t points
in each training epoch, of which t/2 points fall in the smaller space covered by
the m ′, which enables the model to increase access to important data in the
region during training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"based on this, we achieve arbi-
trary resolution sampling in the continuous space covered by the m ′.
anatomical-aware point-voxel network
469
re-voxelization. it is not enough to extract the topological information and
ﬁne-grained information of independent points only by point-based branch for
accurate couinaud segmentation. to this end, we transform the point data
{(pt, ft)} into voxel grid {vu,v,w} by re-voxelization, where ft ∈ rc is the feature
corresponding to point pt, aiming to voxel-based convolution to extract comple-
mentary semantic information in the grid."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"note that the re-voxelization in the model is used three times
(as shown in fig. 2), and the ft,c in the ﬁrst operation is the coordinate and
intensity, with c = 4. moreover, due to the previously mentioned point sampling
strategy, the converted voxel grid also inherits the vessel structure from the point
data and dilutes the unimportant information in the ct images. multi-scale point-voxel fusion network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"intuitively, due to the image
intensity between diﬀerent couinaud segments being similar, the voxel-based
cnn model is diﬃcult to achieve good segmentation performance. we propose
a multi-scale point-voxel fusion network for accurate couinaud segmentation,
take advantage of the topological relationship of coordinate points in 3d space,
and leverage the semantic information of voxel grids. as shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"the features extracted
by these two branches on multiple scales are fused to provide more accurate
and robust couinaud segmentation performance. speciﬁcally, in the point-based
branch, the input point data {(pt, ft)} passes through an mlp, denoted as ep,
which aims to extract ﬁne-grained features with topological relationships. at the
same time, the voxel grid {vu,v,w} passes the voxel branch based on convolution,
denoted as ev, which can aggregate the features of surrounding points and learn
the semantic information in the liver 3d space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"+ tri(ev(v ))t,
(4)
where the superscript 1 of (pt, f 1
t ) indicates that the fused point data and corre-
sponding features f 1
t are obtained after the ﬁrst round of point-voxel operation. then, the point data (pt, f 1
t ) is voxelized again and extracted point features and
voxel features through two branches. note that the resolution of the voxel grid in
470
x. zhang et al.
this round is reduced to half of the previous round."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"we employ the bce loss and the dice loss to supervise the
learning process. the 3dircadb dataset [20] contains 20 ct images
with spacing ranging from 0.56 mm to 0.87 mm, and slice thickness ranging from
1 mm to 4 mm with liver and liver vessel segmentation labels. the lits dataset
[2] consists of 200 ct images, with a spacing of 0.56 mm to 1.0 mm and slice
thickness of 0.45 mm to 6.0 mm, and has liver and liver tumour labels, but with-
out vessels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"we have used three widely used metrics, i.e., accuracy (acc, in %), dice
similarity metric (dice, in %), and average surface distance (asd, in mm) to
evaluate the performance of the couinaud segmentation.
3.2
implementation details
the proposed framework was implemented on an rtx8000 gpu using pytorch. [6] on the
3diradb dataset [20] to generate the vessel attention map of two datasets. then,
we sample t = 20, 000 points in each epoch to train our proposed multi-scale
point-voxel fusion network each epoch."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"diﬀerent
colours represent diﬀerent couinaud segments. [2], and the last two rows show another subject from the 3diradb
dataset [20]. in the ﬁrst column, we show the vessel attention map in 2d and 3d views
as an additional reference for the segmentation results.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"3.4
ablation study
to further study the eﬀectiveness of our proposed framework, we compared two
ablation experiments: 1) random sampling of t points in the liver space, with-
anatomical-aware point-voxel network
473
out considering the guidance of vascular structure, and 2) considering only the
voxel-based branch, where the couinaud segments mask is output by a cnn
decoder. figure 4 shows the ablation experimental results obtained on all the
couinaud segments of two datasets, under the dice and the asd metrics. it
can be seen that our full method is signiﬁcantly better than the cnn branch
joint decoder method on both metrics of two datasets, which demonstrates the
performance gain by the combined point-based branch."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_45.pdf,"in addition, compared
with the strategy of random sampling, our full-method reduces the average asd
by more than 2mm on eight couinaud segments. this is because to the ves-
sel structure-guided sampling strategy can increase the important data access
between the boundaries of the couinaud segments. besides, perturbations are
applied to the points in the coverage area of the vessel attention map, so that our
full method performs arbitrary point sampling in the continuous space near the
vessel, and is encouraged to implicitly learn the couinaud boundary in countless
points."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"pathology detection and delineation enables the automatic
interpretation of medical scans such as chest x-rays while providing a
high level of explainability to support radiologists in making informed
decisions. however, annotating pathology bounding boxes is a time-
consuming task such that large public datasets for this purpose are
scarce. current approaches thus use weakly supervised object detec-
tion to learn the (rough) localization of pathologies from image-level
annotations, which is however limited in performance due to the lack of
bounding box supervision."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"we therefore propose anatomy-driven pathol-
ogy detection (adpd), which uses easy-to-annotate bounding boxes of
anatomical regions as proxies for pathologies. we study two training
approaches: supervised training using anatomy-level pathology labels
and multiple instance learning (mil) with image-level pathology labels. our results show that our anatomy-level training approach outperforms
weakly supervised methods and fully supervised detection with limited
training samples, and our mil approach is competitive with both base-
line approaches, therefore demonstrating the potential of our approach."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"unlike classiﬁcation, which
only predicts the presence of pathologies, it provides a high level of explainability
supporting radiologists in making informed decisions. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_6. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14220, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"additionally, manually annotating pathology bound-
ing boxes is a time-consuming task, further exacerbating the issue. the result-
ing scarcity of large, publicly available datasets with pathology bounding boxes
limits the use of supervised methods for pathology detection, such that cur-
rent approaches typically follow weakly supervised object detection approaches,
where only classiﬁcation labels are required for training. however, as these meth-
ods are not guided by any form of bounding boxes, their performance is limited."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"we, therefore, propose a novel approach towards pathology detection that
uses anatomical region bounding boxes, solely deﬁned on anatomical structures,
as proxies for pathology bounding boxes. these region boxes are easier to anno-
tate – the physiological shape of a healthy subject’s thorax can be learned rela-
tively easily by medical students – and generalize better than those of patholo-
gies, such that huge labeled datasets are available [21]. in summary:
– we propose anatomy-driven pathology detection (adpd), a pathology detec-
tion approach for chest x-rays, trained with pathology classiﬁcation labels
together with anatomical region bounding boxes as proxies for pathologies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"2
related work
weakly supervised pathology detection. due to the scarcity of bounding box
annotations, pathology detection on chest x-rays is often tackled using weakly
supervised object detection with class activation mapping (cam) [25], which
only requires image-level classiﬁcation labels. after training a classiﬁcation
model with global average pooling (gap), an activation heatmap is com-
puted by classifying each individual patch (extracted before pooling) with the
trained classiﬁer, before thresholding this heatmap for predicting bounding
boxes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"while utilizing the same form
of supervision as our method, these methods do not tackle pathology detection. [24], anatomy-level pathology classiﬁcation labels are used to
train a weakly-supervised pathology detection model. unlike our and the other
described methods, it does however not use anatomical region bounding boxes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"for predicting whether the associated region is present, we use a binary clas-
siﬁer with a single linear layer, for bounding box prediction we use a three-layer
mlp followed by sigmoid. we consider the prediction of observed pathologies as
a multi-label binary classiﬁcation task and use a single linear layer (followed by
sigmoid) to predict the probabilities of all pathologies. each of these predictors
is applied independently to each region with their weights shared across regions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"mil-adpd: region predictions are ﬁrst aggregated
using lse pooling and then trained using image-level supervision. for our loc-adpd model, we utilize anatomy-level pathology classiﬁca-
tion labels. here, the target set of observed pathologies is available for each
anatomical region individually such that the pathology observation prediction
can directly be trained for each anatomical region."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"the decoder feature dimension is set to 512. for our mil-adpd model, we experiment with a weaker form of supervision,
where pathology classiﬁcation labels are only available on the per-image level. we utilize multiple instance learning (mil), where an image is considered a bag
of individual instances (i.e. the anatomical regions), and only a single label (per
pathology) is provided for the whole bag, which is positive if any of its instances
is positive."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"in this
model, the decoder feature dimension is set to 256.
in both models, the asl loss is weighted by a factor of 0.01 before adding
it to the detr loss. we train using adamw [12] with a learning rate of 3e−5
(loc-adpd) or 1e−4 (mil-adpd) and weight decay 1e−5 (loc-adpd) or
1e−4 (mil-adpd) in batches of 128 samples with early stopping (with 20 000
steps patience) for roughly 7 h on a single nvidia rtx a6000.
3.4
dataset
training dataset. we train on the chest imagenome dataset [4,21,22]1, con-
sisting of roughly 240 000 frontal chest x-ray images with corresponding scene
graphs automatically constructed from free-text radiology reports."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"it is derived
from the mimic-cxr dataset [9,10], which is based on imaging studies from
65 079 patients performed at beth israel deaconess medical center in boston,
us. amongst other information, each scene graph contains bounding boxes for 29
1 https://physionet.org/content/chest-imagenome/1.0.0
(physionet
credentialed
health data license 1.5.0). 62
p. müller et al.
unique anatomical regions with annotated attributes, where we consider positive
anatomical finding and disease attributes as positive labels for pathologies,
leading to binary anatomy-level annotations for 55 unique pathologies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"during training, we use random resized cropping with size 224 × 224, apply
contrast and brightness jittering, random aﬃne augmentations, and gaussian
blurring. evaluation dataset and class mapping. [20]3 from the national insti-
tutes of health clinical center in the us."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"all images are center-cropped and resized to 224 × 224. the dataset contains bounding boxes for 8 unique pathologies. while partly
overlapping with the training classes, a one-to-one correspondence is not possible
for all classes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"we refer to the supp. [13]), trained on the cxr8 training set using only
image-level pathology labels. note that some of these methods focus on (image-
level) classiﬁcation and do not report quantitative localization results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"neverthe-
less, we compare their localization approaches quantitatively with our method. we also use agxnet [24] for comparison, a weakly supervised method trained
using anatomy-level pathology labels but without any bounding box supervi-
sion. it was trained on mimic-cxr (sharing the images with our method) with
labels from radgraph [8] and ﬁnetuned on the cxr8 training set with image-
level labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"additionally, we also compare with a faster-rcnn [16] trained on a
small subset of roughly 500 samples from the cxr8 training set that have been
2 https://physionet.org/content/mimic-cxr-jpg/2.0.0/
(physionet
credentialed
health data license 1.5.0). 3 https://www.kaggle.com/datasets/nih-chest-xrays/data (cc0: public domain). anatomy-driven pathology detection on chest x-rays
63
annotated with pathology bounding boxes by two medical experts, including one
board-certiﬁed radiologist."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"results on the nih chestx-ray 8 dataset [20]. our models loc-adpd and
mil-adpd, trained using anatomy (an) bounding boxes, both outperform all weakly
supervised methods trained with image-level pathology (pa) and anatomy-level pathol-
ogy (an-pa) labels by a large margin. mil-adpd is competitive with the supervised
baseline trained with pathology (pa) bounding boxes, while loc-adpd outperforms
it by a large margin."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"we report the standard object detection metrics average preci-
sion (ap) at diﬀerent iou-thresholds and the mean ap (map) over thresholds
(0.1, 0.2, . . . [20], a common localization metric
on this dataset, where we use a box score threshold of 0.7 for our method. 4.2
pathology detection results
comparison with baselines."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"while
using image-level annotations (mil-adpd) already gives promising results, the
full potential is only achieved using anatomy-level supervision (loc-adpd). unlike loc-adpd and mil-adpd, all baselines were either trained or ﬁne-
tuned on the cxr8 dataset, showing that our method generalizes well to unseen
datasets and that our class mapping is eﬀective. for detailed results per pathology we refer to the supp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_6.pdf,"we however note that in clinical practice, chest x-rays are not used for the ﬁnal
diagnosis of such pathologies and even rough localization can be beneﬁcial. addi-
tionally, while not requiring pathology bounding boxes, our models still require
supervision in the form of anatomical region bounding boxes, and loc-adpd
requires anatomy-level labels. however, anatomical bounding boxes are easier to
annotate and predict than pathology bounding boxes, and the used anatomy-
level labels were extracted automatically from radiology reports [21]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"data augmentation (da) is a key factor in medical image
analysis, such as in prostate cancer (pca) detection on magnetic reso-
nance images. state-of-the-art computer-aided diagnosis systems still rely
on simplistic spatial transformations to preserve the pathological label
post transformation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"however, such augmentations do not substantially
increase the organ as well as tumor shape variability in the training set,
limiting the model’s ability to generalize to unseen cases with more diverse
localized soft-tissue deformations. we propose a new anatomy-informed
transformation that leverages information from adjacent organs to sim-
ulate typical physiological deformations of the prostate and generates
unique lesion shapes without altering their label. due to its lightweight
d. bonekamp and k. h. maier-hein—equal contribution."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"https://doi.org/10.1007/978-3-031-43990-2_50
532
b. kovacs et al.
computational requirements, it can be easily integrated into common da
frameworks. we demonstrate the eﬀectiveness of our augmentation on a
dataset of 774 biopsy-conﬁrmed examinations, by evaluating a state-of-
the-art method for pca detection with diﬀerent augmentation settings. keywords: data augmentation · soft-tissue deformation · prostate
cancer detection
1
introduction
data augmentation (da) is a key factor in the success of deep neural networks
(dnn) as it artiﬁcially enlarges the training set to increase their generaliza-
tion ability as well as robustness [22]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"dnns
have already successfully supported radiologists in the interpretation of mag-
netic resonance images (mri) for prostate cancer (pca) diagnosis [3]. however,
the da scheme received less attention, despite its potential to leverage the data
characteristic and address overﬁtting as the root of generalization problems. state-of-the-art approaches still rely on simplistic spatial transformations,
like translation, rotation, cropping, and scaling by globally augmenting the mri
sequences [12,20]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"recent motion models rely on dnns using either a fem
model [15] or complex training with population-based models [18]. motion mod-
els have not been integrated into any deep learning framework as an online data
augmentation yet, thereby leaving the high potential of inducing application-
speciﬁc knowledge into the training procedure unexploited. anatomy-informed data augmentation
533
in this work we propose an anatomy-informed spatial augmentation, which
leverages information from adjacent organs to mimic typical deformations of the
prostate."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"due to its lightweight computational requirements, it can be easily
integrated into common da frameworks. this technique allows us to simulate
diﬀerent physiological states during the training and enrich our dataset with a
wider range of organ and lesion shapes. inducing this kind of soft tissue deforma-
tion ultimately led to improved model performance in patient- and lesion-level
pca detection on an independent test set.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"2. random deformable transformations as implemented in the nnu-net [8]
da pipeline extending the basic da scheme (1) to test its presence in the
medical domain. our hypothesis is that it will produce counterproductive
examples, resulting in inferior performance compared to our proposed da.
anatomy-informed data augmentation
535
3. proposed anatomy-informed transformation in addition to the simple da
scheme (1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"the ethics
committee of the medical faculty heidelberg approved the study (s-164/2019)
and waived informed consent to enable analysis of a consecutive cohort. all
experiments were performed in accordance with the declaration of helsinki
[2] and relevant data privacy regulations. [24]
interpretation was performed by a board-certiﬁed radiologist."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"in
addition to the lesions, the rectum and the bladder segmentations were auto-
matically predicted by a model built upon nnu-net [8] trained iteratively on
an in-house cohort initially containing a small portion of our cohort. multiple
radiologists conﬁrmed the quality of the predicted segmentations. 2.4
training protocol
774 exams were split into 80% training set (619 exams) and 20% test set (155
exams) by stratifying them based on the prevalence of cspca (36.3%)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"we got additional, but slight improvements
by extending the da scheme with bladder distensions. a possible explanation for
this result is that less than 30% of the lesions are located close to the bladder, and
our dataset did not contain enough training examples for more improvements. realistic modeling of organ deformation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"the random deformable da
scheme generated high lesion shape variability, but it resulted in lower perfor-
mance values. this could be due to the fact that it can also cause implausible or
anatomy-informed data augmentation
539
even harmful image warping, distorting important features, and producing coun-
terproductive training examples. in comparison, our proposed anatomy-informed
da outperformed the basic and random deformable da, demonstrating the sig-
niﬁcance of realistic transformations for achieving superior model performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_50.pdf,"additionally, our
transformation computation allows certain errors in the organ segmentations
compared to applications where fully accurate segmentations are needed. the
success of anatomy-informed da opens the research question of whether it
enhances performance across diverse datasets and model backbones."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"modern deep learning methods for semantic segmentation
require labor-intensive labeling for large-scale datasets with dense pixel-
level annotations. recent data augmentation methods such as dropping,
mixing image patches, and adding random noises suggest eﬀective ways to
address the labeling issues for natural images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"however, they can only be
restrictively applied to medical image segmentation as they carry risks of
distorting or ignoring the underlying clinical information of local regions
of interest in an image. in this paper, we propose a novel data augmen-
tation method for medical image segmentation without losing the seman-
tics of the key objects (e.g., polyps). this is achieved by perturbing the
objects with quasi-imperceptible adversarial noises and training a network
to expand discriminative regions with a guide of anti-adversarial noises."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"such guidance can be realized by a consistency regularization between the
two contrasting data, and the strength of regularization is automatically
and adaptively controlled considering their prediction uncertainty. our
proposed method signiﬁcantly outperforms various existing methods with
high sensitivity and dice scores and extensive experiment results with mul-
tiple backbones on two datasets validate its eﬀectiveness. keywords: adversarial attack and defense · data augmentation ·
semantic segmentation
1
introduction
semantic segmentation aims to segment objects in an image by classifying each
pixel into an object class."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"training a deep neural network (dnn) for such a task
is known to be data-hungry, as labeling dense pixel-level annotations requires
laborious and expensive human eﬀorts in practice [23,32]. furthermore, semantic
segmentation in medical imaging suﬀers from privacy and data sharing issues [13,
35] and a lack of experts to secure accurate and clinically meaningful regions of
interest (rois). this data shortage problem causes overﬁtting for training dnns,
resulting in the networks being biased by outliers and ignorant of unseen data.
to alleviate the sample size and overﬁtting issues, diverse data augmentations
have been recently developed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"geometric transforma-
tions such as elastic transformation [26] warp images and deform the original
shape of objects. alternatively, feature perturbation methods augment data by
perturbing data in feature space [7,22] and logit space [9].
although these augmentation approaches have been successful for natural
images, their usage for medical image semantic segmentation is quite restricted
as objects in medical images contain non-rigid morphological characteristics that
should be sensitively preserved. in these cases, the underlying
clinical features of target rois (e.g., polyp, tumor and cancer) can be distorted if
regional colors and textures are modiﬁed with blur-based augmentations or geo-
metric transformations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"also, cut-and-paste and crop-based methods carry risks
of dropping or distorting key objects such that expensive pixel-level annotations
could not be properly used. considering the rois are usually small and under-
represented compared to the backgrounds, the loss of information may cause a
fatal class imbalance problem in semantic segmentation tasks. in these regards, we tackle these issues with a novel augmentation method
without distorting the semantics of objects in image space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"conversely, anti-adversaries are obtained with anti-
adversarial perturbations that minimize a loss which eventually become easier
samples to predict. we impose consistency regularization between these contrast-
ing samples by evaluating their prediction ambiguities via supervised losses with
true labels. with this regularization, the easier samples provide adaptive guid-
ance to the misclassiﬁed data such that the diﬃcult (but object-relevant) pixels
can be gradually integrated into the correct prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"from active learning
perspective [12,19], as vague samples near the decision boundary are augmented
and trained, improvement on a downstream prediction task is highly expected. we summarize our main contributions as follows: 1) we propose a novel
online data augmentation method for semantic segmentation by imposing object-
speciﬁc consistency regularization between anti-adversarial and adversarial data.
2) our method provides a ﬂexible regularization between diﬀerently perturbed
data such that a vulnerable network is eﬀectively trained on challenging sam-
ples considering their ambiguities. 3) our method preserves underlying mor-
phological characteristics of medical images by augmenting data with quasi-
imperceptible perturbation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"1. (a) conceptual illustration of the adversarial attack (red) and anti-adversarial
perturbation (blue) in the latent feature space. given a predicted sample embedding
xi, let its true label be a class 1 (c1). the adversarial attack sends the data point
toward class 2 (c2) whereas the anti-adversarial perturbation increases its classiﬁca-
tion score."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"figure 1a shows multi-
step adversarial and anti-adversarial perturbations in the latent space. to increase
a classiﬁcation score, the anti-adversarial noises move data away from the decision
boundary, which is the opposite direction of the adversarial perturbations. 3
method
let {xi}n
i=1 be an image set with n samples each paired with correspond-
ing ground truth pixel-level annotations yi."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"= yi for robust semantic segmentation with anti-adversarial con-
sistency regularization (aac). figure 2 shows the overall training scheme with
three phases: 1) online data augmentation, 2) computing adaptive aac between
558
h. cho et al.
fig. 2. an overview of our training scheme."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"adversarial and anti-adversarial perturba-
tions are iteratively performed for the objects of a given image xi. adversarial noise
µ−
i,k moves xi across the decision boundary, whereas anti-adversarial noise µ+
i,k pushes
xi away from the boundary. downstream consistency regularization loss rcon mini-
mizes the gap between adversaries {x−
i,k}k
k=1 and anti-adversary x+
i,k.
diﬀerently perturbed samples, and 3) updating the segmentation model using
the loss from the augmented and original data. first, we generate plausible
images with iterative adversarial and anti-adversarial perturbations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"we sepa-
rate the roles of perturbed data: adversaries are used as training samples and
anti-adversaries are used to provide guidance (i.e., pseudo-labels) to learn the
adversaries. speciﬁcally, consistency regularization is imposed between these
contrasting data by adaptively controlling the regularization magnitude in the
next phase. lastly, considering each sample’s ambiguity, the network parame-
ters θ are updated for learning the adversaries along with the given data so that
discriminative regions are robustly expanded for challenging samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"training a segmentation network. let ˆy +
i,k be a segmentation outcome, i.e.,
one-hot encoded pseudo-label from the network output p +
i,k of anti-adversary
x+
i,k. given xi and {x−
i,k}k
k=1 as training data, the supervised segmentation
loss lsup and the consistency regularization rcon are deﬁned as
lsup = 1
n
n

i=1
l(pi, yi) +
1
nk
n

i=1
k

k=1
l(p −
i,k, yi)
and
(4)
rcon = 1
n
n

i=1
w(xi, x+
i,k)l(pi, ˆy +
i,k) +
1
nk
n

i=1
k

k=1
w(x−
i,k, x+
i,k)l(p −
i,k, ˆy +
i,k). (5)
560
h. cho et al.
using the pseudo-label from anti-adversary as a perturbation of the ground truth,
the network is supervised by diverse and realistic labels that contain auxiliary
information that the originally given labels do not provide."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"with a hyperparame-
ter α, the whole training loss l = lsup+αrcon is minimized via backpropagation
to optimize the network parameters for semantic segmentation. 4
experiments
4.1
experimental setup
dataset. we conducted experiments on two representative public polyp segmen-
tation datasets: kvasir-seg [11] and etis-larib polyp db [25] (etis)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"both
are comprised of two classes: polyp and background. they provide 1000/196
(kvasir-seg/etis) input-label pairs in total and we split train/validation/test
sets into 80%/10%/10% as in [5,10,24,27,29]. the images of kvasir-seg were
resized to 512 × 608 (h × w) and that of etis was set with 966 × 1255 resolu-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"however, our method
anti-adversarial consistency regularization
561
table 1. performance comparison with existing data augmentation methods. 3. comparisons of precision and recall on the test set of kvasir-seg with u-net.
performed better even compared with the tumorcp which uses seven diﬀer-
ent augmentations methods together for tumor segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"also, as we augment uncertain samples that deliber-
ately deceive a network as in active learning [12,16], our method is able to
sensitively include the challenging (but roi-relevant) features into prediction,
unlike existing noise-based methods that extract noises from known distributions
[9,22,30].
562
h. cho et al.
fig. 4. (a) input data (b) ground truth label (c) adversarially perturbed data (d)
adversarial noise (e) anti-adversarially perturbed data (f) anti-adversarial noise.
fig. 5. (a) density of pixel embeddings (orange and green), a sample (black) and its
perturbations (red and blue) in 2d feature space via t-sne."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"supervised losses
are compared w.r.t. epochs and perturbation steps, and the anti-adversaries (blue)
always demonstrate the lowest loss (i.e., closer to the ground truth). (color ﬁgure
online)
4.3
analysis on anti-adversaries and adversaries
in fig. 4, we visualize data augmentation results with (anti-) adversarial pertur-
bations on kvasir-seg dataset. the perturbed data (c and e) are the addition
of noise (d and f) to the given data (a), respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"also, loss comparisons
in fig. 5b and 5c demonstrate that the anti-adversaries (blue) are consistently
easier to predict than the given data (grey) and adversaries (red) during the
training and their diﬀerences get larger as the perturbations are iterated. anti-adversarial consistency regularization
563
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"6. comparison of ground truths and pseudo labels from anti-adversaries. (a) input
data (b) ground truth label (c) pseudo-label ˆy +
k . (color ﬁgure online)
these results conﬁrm that the anti-adversaries send their pseudo label ˆy +
k
closer to the ground truth with a slight change."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_53.pdf,"therefore, they can be regarded
as a perturbation of the ground truth that contain a potential to provide addi-
tional information to train a network on the adversaries. we empirically show
that ˆy +
k is able to provide such auxiliary information that the true labels do not
provide, as our method performs better with rcon (i.e., l = lsup+αrcon, 92.43%
miou) than the case without rcon (i.e., l = lsup, 92.15% miou) using u-net
on kvasir-seg. training samples in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"colonoscopy analysis, particularly automatic polyp segmen-
tation and detection, is essential for assisting clinical diagnosis and treat-
ment. however, as medical image annotation is labour- and resource-
intensive, the scarcity of annotated data limits the eﬀectiveness and gen-
eralization of existing methods. although recent research has focused on
data generation and augmentation to address this issue, the quality of the
generated data remains a challenge, which limits the contribution to the
performance of subsequent tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"further-
more, arsdm incorporates a pre-trained segmentation model to reﬁne
the training process by reducing the diﬀerence between the ground-truth
mask and the prediction mask. extensive experiments on segmentation
and detection tasks demonstrate the generated data by arsdm could
signiﬁcantly boost the performance of baseline methods. keywords: diﬀusion models · colonoscopy · polyp segmentation ·
polyp detection
y. du and y. jiang—equal contributions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"https://doi.org/10.1007/978-3-031-43895-0_32
340
y. du et al.
1
introduction
colonoscopy is a critical tool for identifying adenomatous polyps and reducing
rectal cancer mortality. however, the scarcity of annotated data due to high
manual annotation costs results in poorly trained and low generalizable models. [9,25]
or data augmentation methods [3,13,28] to enhance learning features, but
these methods yielded limited improvements in downstream tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"2.
despite recent progress in these methods for medical image analysis, existing
models face two major challenges when applied to colonoscopy image analysis. firstly, the foreground (polyp) of colonoscopy images contains rich pathological
information yet is often tiny compared with the background (intestine wall) and
can be easily overwhelmed during training. thus, naive generative models may
generate realistic colonoscopy images but those images seldom contain polyp
regions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"in addition, in order to generate high-quality annotated samples, it is
crucial to maintain the consistency between the polyp morphologies in synthe-
sized images and the original masks, which current generative models struggle
to achieve. to tackle these issues and inspired by the remarkable success achieved by dif-
fusion models in generating high-quality ct or mri data [8,11,23], we creatively
propose an eﬀective adaptive reﬁnement semantic diﬀusion model (arsdm) to
generate polyp-contained colonoscopy images while preserving the original anno-
tations. the pipeline of the data generation and downstream task training is
shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"in summary, our contributions are three-fold: (1) adaptive reﬁnement
sdm: based on the standard semantic diﬀusion model [21], we propose a novel
arsdm with the adaptive loss re-weighting and the prediction-guided sample
reﬁnement mechanisms, which is capable of generating realistic polyp-contained
colonoscopy images while preserving the original annotations. to the best of our
knowledge, this is the ﬁrst work for adapting diﬀusion models to colonoscopy
image synthesis. (2) large-scale colonoscopy generation: the proposed
approach can be used to generate large-scale datasets with no/arbitrary anno-
tations, which signiﬁcantly beneﬁts the medical image society, laying the foun-
dation for large-scale pre-training models in automatic colonoscopy analysis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"[6] are classes
of deep generative models, which have forward and reverse processes. the for-
ward process is a markov chain that gradually adds gaussian noise to the orig-
inal data. = n

xt;

1 − βtxt−1, βti

,
(1)
where q (x0) is the original data distribution with x0 ∼ q (x0), x1:t are latents
with the same dimension of x0 and βt is a variance schedule."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"(5)
thus, the u-net model ϵθ in eq. 3 becomes ϵθ (xt, t, c0), and the loss function
in eq. 3 is changed to:
lcondition = et,xt,c0,ϵ∼n(0,i)

∥ϵ − ϵθ (xt, t, c0)∥2	
. 
+ σiz
4 end for
5 ˜c0 = p(˜x0)
6 take gradient descent step on ∇θltotal
2.2
adaptive loss re-weighting
the polyp regions in the colonoscopy images diﬀer from the background regions,
which contain more pathological information and should be adequately treated
to learn a better model. however, training the diﬀusion models using the original
loss function ignores the diﬀerence between diﬀerent regions, where each pixel
shares the same weights when calculating the loss."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"thus, the loss function becomes:
ladaptive = et,xt,c0,ϵ∼n(0,i)

w λ · ∥ϵ − ϵθ (xt, t, c0)∥2	
. (8)
2.3
prediction-guided sample reﬁnement
the downstream tasks of polyp segmentation and detection require rich semantic
information on polyp regions to train a good model. through extensive exper-
iments, we found inaccurate sample images with coarse polyp boundary that
is not aligned properly with the original masks may introduce large biases and
noises to the datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"we propose the following reﬁnement loss based on iou loss and binary cross
entropy (bce) loss between ˜c0 and c0. the reﬁnement loss is:
lreﬁne = l(c, ˜cg) +
i=5

i=3
l ( ˜ci) ,
˜c0 = { ˜c3, ˜c4, ˜c5, ˜cg} = p (s (˜ϵ)) ,
(9)
where l = liou + lbce is the sum of the iou loss and bce loss, ˜c0 is the
collection of the three side-outputs ( ˜c3, ˜c4, ˜c5) and the global map ˜cg as described
in [5]. p(·) represents the pranet model and s(·) is the ddim [16] sampler. the
detailed procedure of one training iteration is shown in algorithm 1 and the
overall loss function is deﬁned as:
ltotal = ladaptive + lreﬁne ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"following the standard of pranet, 1,450 image-mask pairs from kvasir
and cvc-clinicdb are taken as the training set. the evaluations are conducted
on the ﬁve datasets separately to verify the learning and generalization capa-
bility. the training image-mask pairs are padded to have the same height and
width and then resized to the size of 384 × 384."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"3.3
quantitative comparisons
the experimental results presented in table 1 and 2 demonstrate the eﬀective-
ness of our proposed method in training better downstream models to achieve
superior performance. speciﬁcally, data generated by our approach assists the
346
y. du et al.
table 3. ablation study of diﬀerent com-
ponents on polyp segmentation tasks. methods
pranet
sanet
ada."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"signiﬁcant improvements for each model in mdice and miou, with increases of
6.0% and 5.7% over pranet, 2.1% and 2.7% over sanet, and 0.7% and 0.7% over
polyp-pvt. we also observe superior ap and f1-scores compared to centernet,
sparse-rcnn, and deformable-detr trained with original data, with gains of
9.1% and 5.3%, 2.7% and 5.8%, and 3.4% and 6.1%, respectively. moreover, we
conducted a comprehensive comparison with sota models, noting that these
models were not speciﬁcally designed for colonoscopy images and may generate
data that hinder the training process or lack the ability for eﬀective improvement."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_33.pdf,"the gener-
ated samples demonstrate diﬀerences from the original images in both the polyp
arsdm
347
regions and the backgrounds while maintaining alignment with the masks. addi-
tionally, we sought evaluations from medical professionals to assess the authen-
ticity of the generated samples, and non-medical professionals to locate polyps
in the images, which yielded positive feedback on the quality of the generated
samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf,"however, it may require multiple attempts before obtain-
ing a satisfactory wsi, which can lead to a waste of time, medical resources,
and deplete tissue samples. discarding the local region with artifacts for deep
learning models is another solution, but it may result in the loss of critical con-
textual information. therefore, learning-based artifact restoration approaches
have gained increasing attention."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf,"to address these issues, we make the ﬁrst attempt at a diﬀusion
probabilistic model for artifact restoration approach [5], as shown in fig. innovatively, our framework formulates the artifact restoration as a regional
denoising process, which thus can to the most extent preserve the stain style
and avoid the loss of contextual information in the non-artifact region. further-
more, our approach is trained solely with artifact-free images, which reduces the
diﬃculty in data collection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf,"(2) to capture the
local-global correlations in the gradual regional artifact restoration process, we
innovatively propose a swin-transformer denoising architecture to replace the
commonly-used u-net and a time token scheme for optimal swin-transformer
denoising. extensive evaluations on real-world histology datasets and down-
stream tasks demonstrate the superiority of our framework in artifact removal
performance, which can generate reliable restored images while preserving the
stain style.
fig. the semantic illustration of inference stage in artifusion for local regional
artifact restoration."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf,"the proposed histology artifact restoration diﬀusion model
artifusion, comprises two stages, namely the training, and inference. during
the training stage, artifusion learns to generate regional histology tissue struc-
tures based on the contextual information from artifact-free images. in the infer-
ence stage, artifusion formulates the artifact restoration as a gradual denoising
process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf,"diﬀusion training stage. the proposed artifusion learns the capability
of generating local tissue representation from contextual information during
the training stage. [5],
which involve a forward process that gradually injects gaussian noise into an
artifact restoration in histology images with diﬀusion probabilistic models
521
artifact-free image and a reverse process that aims to reconstruct images from
noise."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf,"3, our network follows a u-shape architec-
ture, where the encoder, bottleneck, and decoder modules all employ swin-
transformer as the basic building block. additionally, we introduce an inno-
vative auxiliary time token to inject the time information. in an arbitrary time
step t during the training process, to obtain a time token, we ﬁrst embed the
scalar t by learnable linear layers, with weights that are speciﬁc to each swin-
transformer block."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf,"the resulting tokens are then processed by the attention layers,
and the auxiliary time token is discarded to retain the original feature dimension
to ﬁt the swin-transformer block design after the attention layers. 3
experiments
dataset. to evaluate the performance of artifact restoration, a training set is
curated from a subset of camelyon17 [8]1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf,"consequently, we leverage the prevalent cycle-
gan [19] as the baseline for comparison, because of its excellent performance
2 available at https://github.com/lu-yizhou/clusterseg. 524
z. he et al.
in the image transfer, and also its nature that requires no paired data can ﬁt
our circumstance. unlike cyclegan which requires both artifact-free images
and artifact images, artifusion only relies on artifact-free images, leading to
a size of the training set that is half that of cyclegan."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf,"for instance, artifusion can reduce the l2 and mse by
artifact restoration in histology images with diﬀusion probabilistic models
525
more than 50%, namely from 1 × 104 to 0.5 × 104 and from 0.55 to 0.25 respec-
tively. it implying that our method can to the large extent restore the artifact
regions using the global information. in addition, artifusion can improve other
metrics, including ssim, psnr, fsim and sre by 0.0204, 5.72, 0.1028 and 4.02
respectively, indicating that it can preserve the stain style during the restoration
process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_50.pdf,"we further evaluate the
proposed artifact restoration framework on a downstream tissue classiﬁcation
task. to this end, we use the public dataset nct-crc-he-100k for train-
ing and crc-val-he-7k for testing, which together contains 100, 000 training
samples and 7, 180 test samples. we consider the performance on the original
unprocessed data, denoted as ‘clean’, as the upper bound."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"in this paper, we present a novel method which eliminates the
need for scout scans in ct lung cancer screening by estimating patient
scan range, isocenter, and water equivalent diameter (wed) from 3d
camera images. we achieve this task by training an implicit generative
model on over 60,000 ct scans and introduce a novel approach for updat-
ing the prediction using real-time scan data. we demonstrate the eﬀec-
tiveness of our method on a testing set of 110 pairs of depth data and ct
scan, resulting in an average error of 5 mm in estimating the isocenter,
13 mm in determining the scan range, 10 mm and 16 mm in estimating
the ap and lateral wed respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"however, the current workﬂow for performing ct lung
scans still requires an experienced technician to manually perform pre-scanning
steps, which greatly decreases the throughput of this high volume procedure. while recent advances in human body modeling [4,5,12,13,15] have allowed for
automation of patient positioning, scout scans are still required as they are used
by automatic exposure control system in the ct scanners to compute the dose
to be delivered in order to maintain constant image quality [3]. since ldct scans are obtained in a single breath-hold and do not require
any contrast medium to be injected, the scout scan consumes a signiﬁcant por-
tion of the scanning workﬂow time."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"addi-
tionally, we introduce a novel approach for updating the estimated wed in
real-time, which allows for reﬁnement of the scan parameters during acquisition,
thus increasing accuracy. we present a method for automatically aborting the
scan if the predicted wed deviates from real-time acquired data beyond the
clinical limit. we trained our models on a large collection of ct scans acquired
from over 60, 000 patients from over 15 sites across north america, europe and
asia."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"we propose a semi-supervised approach to estimate wed from
depth images. first, we train a wed generative model on a large collection of
automated ct lung cancer screening
425
fig. 1. overview of the proposed workﬂow."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"we then train an encoder network to map the patient depth image to
the wed manifold. finally, we propose a novel method to reﬁne the prediction
using real-time scan data. 2.1
wed latent space training
we use an autodecoder [10] to learn the wed latent space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"since our network only takes
the craniocaudal coordinate and the latent vector as input, it can be trained on
partial scans of diﬀerent sizes. the training consists of a joint optimization of
the autodecoder and the latent vector: the autodecoder is learning a realistic
representation of the wed function while the latent vector is updated to ﬁt the
data. during training, we initialize our latent space to a unit gaussian distribution
as we want it to be compact and continuous."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"we then randomly sample points
along the craniocaudal axis and minimize the l1 loss between the prediction and
the ground truth wed. we also apply l2-regularization on the latent vector as
part of the optimization process. 426
b. teixeira et al.
2.2
depth encoder training
after training our generative model on a large collection of unpaired ct scans,
we train our encoder network on a smaller collection of paired depth images
and ct scans. [1] taking as input
the depth image and outputting a latent vector in the previously learned latent
space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"we here again apply l2-
regularization on the latent vector during training. 2.3
real-time wed reﬁnement
while the depth image provides critical information on the patient anatomy,
it may not always be suﬃcient to accurately predict the wed proﬁles. for
example, some patients may have implants or other medical devices that cannot
be guessed solely from the depth image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"additionally, since the encoder is trained
on a smaller data collection, it may not be able to perfectly project the depth
image to the wed manifold. to meet the strict safety criteria deﬁned by the
iec, we propose to dynamically update the predicted wed proﬁles at inference
time using real-time scan data. first, we use our encoder network to initialize
the latent vector to a point in the manifold that is close to the current patient."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"then, we use our autodecoder to generate initial wed proﬁles. as the table
moves and the patient gets scanned, ct data is being acquired and ground truth
wed can be computed for portion of the body that has been scanned, along
with the corresponding craniocaudal coordinate. we can then use this data to
optimize the latent vector by freezing the autodecoder and minimizing the l1
loss between the predicted and ground truth wed proﬁles through gradient
descent."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"we can then feed the updated latent vector to our autodecoder to
estimate the wed for the remaining portions of the body that have not yet
been scanned and repeat the process. in addition to improving the accuracy of the wed proﬁles prediction, this
approach can also help detect deviation from real data. after the latent vector
has been optimized to ﬁt the previously scanned data, a large deviation between
the optimized prediction and the ground truth proﬁles may indicate that our
approach is not able to ﬁnd a point in the manifold that is close to the data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"overall
ﬂowchart of the proposed approach is shown in fig. 1.
3
results
3.1
data
our ct scan dataset consists of 62, 420 patients from 16 diﬀerent sites across
north america, asia and europe. our 3d camera dataset consists of 2, 742 pairs
automated ct lung cancer screening
427
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_40.pdf,"results can be seen in fig. 2.
3.3
water equivalent diameter
we trained our autodecoder model on our unpaired ct scan dataset of 62, 420
patients with a latent vector of size 32. the encoder was trained on our paired
ct scan and depth image dataset of 2, 742 patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"due to the
complexity of gv structures with large intra-class variation and small
inter-class variations, we found that existing models perform poorly on
this task and tend to lose focus on the important varices regions. to solve
this issue, we constructively introduce the segmentation of gv into the
classiﬁcation framework and propose the region-constraint module and
cross-region attention module for better feature localization and to learn
the correlation of context information. we also collect a gv bleeding
risks rating dataset (gvbleed) with 1678 gastroscopy images from 411
patients that are jointly annotated in three levels of risks by senior clin-
ical endoscopists."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"the experiments on our collected dataset show that
our method can improve the rating accuracy by nearly 5% compared to
the baseline. codes and dataset will be available at https://github.com/
luyueshi/gastric-varices. y. jiang, l. shi and w. qi—contributed equally to this work."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"3. to encourage the model to learn more
robust representations, we constructively introduce segmentation into the clas-
siﬁcation framework. with the segmentation information, we further propose a
region-constraint module (rcm) and a cross-region attention module (cram)
for better feature localization and utilization. speciﬁcally, in rcm, we utilize
automatic bleeding risk rating system of gastric varices
5
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"in cram, the varices features are extracted using the
segmentation results and combined with an attention mechanism to learn the
intra-class correlation and cross-region correlation between the target area and
the context. to learn from experienced endoscopists, gv datasets with bleeding risks
annotation is needed. while most works and public datasets focus on colonoscopy
[13,15] and esophagus [5,9], with a lack of study on gastroscopy images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"in the
public dataset of endocv challenge [2], the majority are colonoscopies while only
few are gastroscopy images. in this work, we collect a gv bleeding risks rating
dataset (gvbleed) that contains 1678 gastroscopy images from 411 patients
with diﬀerent levels of gv bleeding risks. three senior clinical endoscopists are
invited to grade the bleeding risk of the retrospective data in three levels and
annotated the corresponding segmentation masks of gv areas."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"in sum, the contributions of this paper are: 1) a novel gv bleeding risk rating
framework that constructively introduces segmentation to enhance the robust-
ness of representation learning; 2) a region-constraint module for better feature
localization and a cross-region attention module to learn the correlation of tar-
get gv with its context; 3) a gv bleeding risk rating dataset (gvbleed) with
high-quality annotation from multiple experienced endoscopists. baseline meth-
ods have been evaluated on the newly collected gvbleed dataset. experimental
results demonstrate the eﬀectiveness of our proposed framework and modules,
where we improve the accuracy by nearly 5% compared to the baseline model.
6
y. jiang et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"although such strategy can improve the classiﬁcation
performance, it may still lose focus in some hard cases where the gv area can
hardly be distinguished. to further regularize the attention and fully utilize the
context information around the gv area, on top of the segmentation framework
we proposed the cross-region attention module and the region-constraint module. 2.2
cross-region attention module
inspired by the self-attention mechanism [17,18], we propose a cross-region atten-
tion module (cram) to learn the correlation of context information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"after getting the cam, we regularize cam by calculating the dice loss between
the cam and ground truth mask of varices region lco.
2.4
network training
in our framework, we use the cross entropy loss as the classiﬁcation loss:
lcl = −
c

c=1
log
exp(pc)
σc
i=1exp(pi)yc
(5)
8
y. jiang et al.
table 1. distribution of the three-level gv bleeding risks in the gvbleed dataset. mild moderate severe total
train 398
484
455
1337
test
94
145
102
341
total 462
629
557
1678
where p is the prediction of the classiﬁer and y is the ground-truth label."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"and
the total loss of our framework can be summarized as:
ltotal = 1
n

(ωslse + ωcolco + ωcllcl),
(6)
where n is the total number of samples, ωs, ωco and ωcl are weights of the three
losses, respectively. the training process of the proposed network consists of three steps: 1) the
segmentation network is trained ﬁrst; 2) the ground-truth segmentation masks
and images are used as the inputs of the cram, the classiﬁcation network,
including cram and rcm, are jointly trained; 3) the whole framework is
jointly ﬁne-tuned.
3
gvbleed dataset
data collection and annotation. the gvbleed dataset contains 1678 endo-
scopic images with gastric varices from 527 cases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"the images are selected from the raw endoscopic videos and frames.
to maximize the variations, non-consecutive frames with larger angle diﬀerences
are selected. to ensure the quality of our dataset, senior endoscopists are invited
to remove duplicates, blurs, active bleeding, chromoendoscopy, and nbi pictures. criterion of gv bleeding risk level rating."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"the varices are thicker (usually with a diameter
greater than 10 mm) or less than 10mm but with positive red signs. note that
the diameter is only one reference for the ﬁnal risk rating since the gv is with
1 please refer to the supplementary material for more detailed information about our
dataset. automatic bleeding risk rating system of gastric varices
9
table 2."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"the other facts are more subjectively evaluated
based on the experience of endoscopists. to ensure the accuracy of our anno-
tation, three senior endoscopists with more than 10 years of clinical experience
are invited to jointly label each sample in our dataset. if three endoscopists have
inconsistent ratings for a sample, the ﬁnal decision is judged by voting."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"a sample
is selected and labeled with a speciﬁc bleeding risk level only when two or more
endoscopists reach a consensus on it. the gvbleed dataset is partitioned into training and testing sets for evalu-
ation, where the training set contains 1337 images and the testing set has 341
images. the detailed statistics of the three levels of gv bleeding risk in each set
are shown in table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"adam is used as the
optimizer, the learning rate is set to 1e−3; 3) jointly ﬁne-tuning: the whole
framework is jointly ﬁne-tuned for 100 epochs with adam as optimizer and the
learning rate set to 1e−3. in addition, common data augmentation techniques
such as rotation and ﬂipping were adopted here. 4.2
results analysis
table 2 reports the quantitative results of diﬀerent models and fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"we tested several baseline models, including both sim-
ple cnn models and state-of-the-art transformer-based models. however, the
transformer-based models achieves much worse performances since they always
require more training data, which is not available in our task. thus, we selected
10
y. jiang et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"with the help of cram,
the performance of the model can be further improved. although the model can
extract more important context information at the varices regions, the perfor-
mance improvement is not very large since the focus ability is not the best and
the model may still make predictions based on the incorrect regions for some
hard images. by adding the rcm to the cram, the focus ability of the model
can be further improved, and thus the model has a signiﬁcant improvement in
performance by 5% compared to the baseline model, this proves the eﬀective-
ness of our proposed modules."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_1.pdf,"besides, we further design a region-constraint module for better
feature localization and a cross-region attention module to learn the correlation
of target gv with its context. in addition, we collected the gvbleed dataset with
high-quality annotation of three-level of gv bleeding risks. the experiments on
our dataset demonstrated the eﬀectiveness and superiority of our framework."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_50.pdf,"this is also true for the b-cos swin trans-
former (bwin). compared to the swin transformer, it even improves the
f1-score by up to 4.7% on two public datasets. keywords: transformer · self-attention · explainability ·
interpretability
1
introduction
making artiﬁcial neural networks more interpretable, transparent, and trustwor-
thy remains one of the biggest challenges in deep learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_50.pdf,"they are often still
considered black boxes, limiting their application in safety-critical domains such
e. klaiman—equal contribution. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3_50.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. 1. attention maps of vit and bvt (ours) on the test set of (a) nct-crc-
he-100k, (b) tcga-coad-20x, and (c) munich-aml-morphology."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_50.pdf,"both query and key are then correlated with a scaled dot-product and
normalized with a softmax. these self-attention scores are then used to weight
the value by importance:
attention(q, k, v ) = softmax
qkt
√
d

v.
(1)
to extract more information, this process is repeated h times in parallel
(multi-headed self-attention). each self-attention layer is followed by a fully-
connected layer consisting of two linear transformations and a relu activation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_50.pdf,"[19], which consists of 38 annotated slides
from the tcga colorectal cancer cohort, to evaluate the eﬀectiveness of transfer
learning. this dataset is highly unbalanced and not color normalized compared
fig. 5. in a blinded study, domain experts ranked models (lower is better) based on
whether the models focus on biomedically relevant features that are known in the
literature to be important for diagnosis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_50.pdf,"we use adamw with a cosine learning rate scheduler for optimization
and a separate validation set for hyperparameter selection. this allows
us to encode each pixel with the direction of the color channel vector, forcing
the model to capture more color information. furthermore, we train models with
two diﬀerent loss functions: the standard categorical cross-entropy loss (cce)
and the binary cross-entropy loss (bce) with one-hot encoded entries."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_50.pdf,"we explore
whether this is also true for transformers in our experiments. additional details
on training, optimization, and datasets can be found in the appendix.
5
results and discussion
task-based evaluation: when trained from scratch, all bvt models
underperform their vit counterparts by about 2% on nct-crc-he-100k and
b-cos transformer
521
fig. 6. attention maps of the last layer of the modiﬁed swin and bwin (ours)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_50.pdf,"in contrast, vit
attributes high attention to seemingly irrelevant features, such as the edges of
the cells. a third expert points out that vit might overﬁt certain patterns in
this dataset, which could aid the model in improving its performance. 6
generalization to other architectures
we aim to explore whether the b-cos transform can enhance the interpretability
of other transformer-based architectures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"although deep learning techniques have been widely investi-
gated, the enormous types and diverse appearances of histopathological
cells still pose signiﬁcant challenges for clinical applications. moreover,
data protection policies in diﬀerent clinical centers and hospitals limit
the training of data-dependent deep models. in this paper, we present
a novel framework for cross-tissue domain adaptative cell segmentation
without access both source domain data and model parameters, namely
multi-source black-box domain adaptation (mbda)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"given the target
domain data, our framework can achieve the cell segmentation based on
knowledge distillation, by only using the outputs of models trained on
multiple source domain data. considering the domain shift cross diﬀer-
ent pathological tissues, predictions from the source models may not be
reliable, where the noise labels can limit the training of the target model. to address this issue, we propose two practical approaches for weight-
ing knowledge from the multi-source model predictions and ﬁltering out
noisy predictions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"first, we assign pixel-level weights to the outputs of
source models to reduce uncertainty during knowledge distillation. sec-
ond, we design a pseudo-label cutout and selection strategy for these pre-
dictions to facilitate the knowledge distillation from local cells to global
pathological images. experimental results on four types of pathologi-
cal tissues demonstrate that our proposed black-box domain adaptation
approach can achieve comparable and even better performance in com-
parison with state-of-the-art white-box approaches."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"it can
help people conduct cell counting, cell morphology analysis, and tissue analysis,
which reduces human labor [19]. however, data acquisition for medical images
poses unique challenges due to privacy concerns and the high cost of manual
annotation. moreover, pathological images from diﬀerent tissues or cancer types
often show signiﬁcant domain shifts, which hamper the generalization of mod-
els trained on one dataset to others."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"source-free domain adaptation methods have
been also widely investigated due to the privacy protection. [3,5,14] explore how
to implicitly align target domain data with the model trained on the source
domain without accessing the source domain data. there are also many studies
on multi-source white-box domain adaptation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"while black-box models are proﬁcient
in speciﬁc domains, their performances greatly degrade when the target domain
is updated with new pathology slices. therefore, how to leverage the existing
knowledge of black-box models to eﬀectively train new models for the target
domain without accessing the source domain data remains a critical challenge. in this paper, we present a novel source-free domain adaptation framework
for cross-tissue cell segmentation without accessing both source domain data
and model parameters, which can seamlessly integrate heterogeneous models
from diﬀerent source domains into any cell segmentation network with high gen-
erality."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"in
this setting, conventional multi-source ensemble methods are not applicable due
to the unavailability of model parameters, and simply aggregating the black-box
outputs would introduce a considerable amount of noise, which can be detri-
mental to the training of the target domain model. therefore, we develop two
black-box domain adaptative cell segmentation
751
multi-source 
models
(black-box)
logits map
prediction uncertainty boundary ambiguity
score
weighted logits map
confidence threshold
adaptive
unlabeled
labeled
pseudo-cutout 
label
student model
pixel-level weight
target domain
0.97 0.90
0.95
0.95
0.95
0.92
0.93 0.99
0.92 0.92
0.95 0.92
vote
teacher model
ema
fig. 1. overview of our purposed framework, where logits maps denote the raw pre-
dictions from source models and ω denotes pixel-level weight for each prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"the
semi-supervised loss, denoted as lssl, encompasses the supervised loss, consistency loss,
and maximize mutual information loss.
strategies within this new framework to address this issue. firstly, we propose a
pixel-level multi-source domain weighting method, which reduces source domain
noise by knowledge weighting. this method eﬀectively addresses two signiﬁcant
challenges encountered in the analysis of cellular images, namely, the uncertainty
in source domain output and the ambiguity in cell boundary semantics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"this strategy enables us to ignore low-conﬁdence regions, similar
to cutout [6], but with selective masking of pixels, which eﬀectively balances
the trade-oﬀ between exploiting similarities and preserving diﬀerences of diﬀer-
ent domains. as a result, we refer to the labels generated through the voting
strategy as pseudo-cutout labels. 2
method
overview: figure 1 shows a binary cell segmentation task with three source
models trained on diﬀerent tissues and a target model, i.e., the student model
in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"sub-
sequently, we feed the perturbed images into the source domain predictor to
generate the corresponding raw segmentation outputs. these outputs are then
processed by two main components of our framework: a pixel-level weighting
method that takes into account the prediction uncertainty and cell boundary
ambiguity, and an adaptive knowledge voter that utilizes conﬁdence gates and a
dynamic ensemble strategy. these components we designed are to extract reli-
able knowledge from the predictions of source domain models and reduce noise
752
x. wang et al.
during distillation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"finally, we obtain a weighted logit for knowledge distillation
from pixel level and a high-conﬁdence pseudo-cutout label for further structured
distillation from cell to global pathological image. knowledge distillation by weighted logits map: we denote dn
s
=
{xs, ys}n as a collection of n source domains and dt = {xi
t, y j
t } as single
target domain, where the number of labeled instances y j
t ≪ xi
t. we are only
provided with black-box models {f n
s }n
n=1 trained on multiple source domains
{xi
s, yi
s}n
n=1 for knowledge transfer. the parameters {θn
s }n
n=1 of these source
domain predictors are not allowed to participate in gradient backpropagation
as a result of the privacy policy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"thus, our ultimate objective is to derive a
novel student model ft : xt → yt that is relevant to the source domain task. accordingly, direct knowledge transfer using the output of the source domain
predictor may lead to feature bias in the student model due to the unavoid-
able covariance [20] between the target and source domains. inspired by [21],
we incorporate prediction uncertainty and cell boundary impurity to establish
pixel-level weights for multi-source outputs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"∈ xi
t, we initially feed it into the source predic-
tors {f n
s }n
n=1 to obtain their respective prediction {pn
s }n
n=1. to leverage the rich
semantic information from the source domain predictor predictions, we utilize
predictive entropy of the softmax outputs to measure the prediction uncertainty
scores. in the semantic segmentation scenario of c-classes classiﬁcation, we deﬁne
the pixel-level uncertainty score u(i,j)
n
as follow:
u(i,j)
n
= −
c

c=1
on(i,j,c)
s
log on(i,j,c)
s
(2)
where on
s denotes softmax output,i.e.,on
s = softmax(pn
s ) from nth source predic-
tor."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"due to the unique characteristics of cell morphology, merely relying on uncer-
tainty information is insuﬃcient to produce high-quality ensemble logits map
that accurately capture the relevance between the source and target domains. the target pseudo-label for the nth predictor f n
s can be obtained by applying
the softmax function to the output and selecting the category with the high-
est probability score, i.e., yt = arg maxc∈{1,...,c}(softmax(pn
s )). then accord-
ing to c-classes classiﬁcation tasks, we divide the cell region into c subsets,
n c
k(i, j) = {(u, v) ∈ nk(i, j) | yt = c}."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"(5)
where dkl denotes the kullback-leibler (kl) divergence loss. adaptive pseudo-cutout label: as previously mentioned, the outputs from
the source domain black-box predictors have been adjusted by the pixel-level
weight. however, they are still noisy and only pixel-level information is consid-
ered while ignoring structured information in the knowledge distillation process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"thus, we utilize the output of the black-box predictor on the target domain
to produce an adaptive pseudo-cutout label, which will be employed to further
regularize the knowledge distillation process. we have revised the method in [7]
to generate high-quality pseudo labels that resemble the cutout augmentation
technique. for softmax outputs {on
s }n
n=1 from n source predictors, we ﬁrst set
a threshold α to ﬁlter low-conﬁdence pixels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"subsequently, we apply an adaptive voting strategy to the
n source domain outputs. initially, during the training of the target model, if at
least one source domain output exceeds the threshold, we consider the pixel as
a positive or negative sample, which facilitates rapid knowledge acquisition by
the model. as the training progresses, we gradually tighten the voting strategy
and only retain regional pixels that have received adequate votes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"then we will aggregate the voting scores, i.e., v (i,j) = n
n=1 v(i,j)
n
and deter-
mine whether to retain each pixel using an adaptive vote gate g ∈ {1, 2, 3, etc.}. by ﬁltering with a threshold and integrating the voting strategy, we generate
high-conﬁdence pseudo-labels that remain eﬀective even when the source and
target domains exhibit covariance. (7)
where lce denotes cross-entropy loss function."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"loss functions: finally, we incorporate global structural information about
the predicted outcome of the target domain into both distillation and semi-
supervised learning. to mitigate the noise eﬀect of the source domain predictors,
we introduce maximize mutual information targets to facilitate discrete repre-
sentation learning by the network. i pi log pi as conditional
entropy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"[15].
we adopt the classical and eﬀective mean-teacher framework as a baseline
for semi-supervised learning and update the teacher model parameters by expo-
nential moving average. also, we apply two diﬀerent perturbations (η, η′) to the
target domain data and feed them into the student model and the mean-teacher
model respectively. lall = lkd + lpcl + lcons − lmmi + lsup
(10)
where lsup denotes the ordinary cross-entropy loss for supervised learning and
we set the weight of each loss function to 1 in the training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"hou et al. [10] publish a dataset of nucleus segmentation containing 5,060 seg-
mented slides from 10 tcga cancer types. in this work, we use 98 images from
black-box domain adaptative cell segmentation
755
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"awan et al. [2] publicly release a dataset
containing tissue slide images and associated clinical data on colorectal cancer
(crc), from which we randomly select 200 patches for our study. in our exper-
iments, we transfer knowledge from three black-box models trained on diﬀerent
source domains to a new target domain model (e.g.,from crc, tnbc, kirc
to brca)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"the backbone network for the student model and source domain
black-box predictors employ the widely adopted residual u-net [12], which is
commonly used for medical image segmentation. for each source domain net-
work, we conduct full-supervision training on the corresponding source domain
data and directly evaluate its performance on target domain data. the upper
performance metrics (source-only upper) are shown in the table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"for the target domain network, we use unsupervised and semi-supervised as our
task settings respectively. in semi-supervised domain adaptation, we only use
10% of the target domain data as labeled data. [8], an adversarial learning based semi-
supervised domain adaptation approach."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"[17], an unsupervised black-box
model domain adaptation framework. a point worth noting is that most of the
methods we compared with are white-box methods, which means they can obtain
more information from the source domain than us. for single-source domain
adaptation approach, cellsegssda and sfda-dpl, we employ two strategies
to ensure the fairness of the experiments: (1) single-source, i.e. performing adap-
tation on each single source, where we select the best results to display in the
table 1; (2) source-combined, i.e. all source domains are combined into a tra-
ditional single source."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"quantitative comparison with unsupervised and semi-supervised domain
adaptation methods under 3 segmentation metrics. crc&kirc&brca to tnbc
wl pcl mmi
dice
hd95
assd
×
×
×
0.6708
56.9111
16.3837
✓
×
×
0.6822
54.3386 14.9817
✓
✓
×
0.6890
57.0889
12.9512
✓
✓
✓
0.7075 58.8798
10.7247
source data into a traditional single source will result in performance degrada-
tion in some cases, which also proves the importance of studying multi-source
domain adaptation methods. ablation study: to evaluate the impact of our proposed methods of weighted
logits(wl), pseudo-cutout label(pcl) and maximize mutual information(mmi)
on the model performance, we conduct an ablation study."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"we chose
crc, kirc and brca as our source domains, and tnbc as our target domain. the results of these experiments, presented in the table 2, show that our pro-
posed modules are indeed useful.
4
conclusion
our proposed multi-source black-box domain adaptation method achieves com-
petitive performance by solely relying on the source domain outputs, without
the need for access to the source domain data or models, thus avoiding informa-
tion leakage from the source domain. additionally, the method does not assume
black-box domain adaptative cell segmentation
757
the same architecture across domains, allowing us to learn lightweight target
models from large source models, improving learning eﬃciency."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_71.pdf,"moving forward, we
plan to integrate our approach with active learning methods to enhance annota-
tion eﬃciency in the semi-supervised setting. by leveraging multi-source domain
knowledge, we aim to improve the reliability of the target model and enable more
eﬃcient annotation for better model performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"breast ultrasound videos contain richer information than
ultrasound images, therefore it is more meaningful to develop video
models for this diagnosis task. however, the collection of ultrasound
video datasets is much harder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"the coherence loss uses the feature centers generated
by the static images to guide the frame attention in the video model. our kga-net boosts the performance on the public busv dataset by
a large margin. the visualization results of frame attention prove the
explainability of our method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"this task is also an
essential component of computer-aided diagnosis. since each frame in an ultra-
sound video can only capture a speciﬁc view of a lesion, it is essential to aggregate
information from the entire video to perform accurate automatic lesion diagno-
sis. therefore, in this study, we focus on the classiﬁcation of breast ultrasound
videos for detecting malignant and benign breast lesions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"we use a 2d resnet trained on ultrasound images to get the features. while ultrasound videos oﬀer more information, prior studies have primarily
focused on static image classiﬁcation [2,11,27]. obtaining ultrasound video data
with pathology gold standard results poses a major challenge. sonographers typ-
ically record keyframe images during general ultrasound examinations, not entire
videos."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"prospective collection requires additional eﬀorts to track corresponding
pathological results. consequently, while there are many breast ultrasound image
datasets [1,28], breast ultrasound video datasets remain scarce, with only one
relatively small dataset [15] containing 188 videos available currently. given the diﬃculties in collecting ultrasound video data, we investigate
the feasibility of enhancing the performance of ultrasound video classiﬁcation
using a static image dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"to achieve this, we ﬁrst analyze the relationship
between ultrasound videos and images. the images in the ultrasound dataset
are keyframes of a lesion that exhibit the clearest appearance and most typical
symptoms, making them more discriminative for diagnosis. although ultrasound
videos provide more information, the abundance of frames may introduce redun-
dancy or vagueness that could disrupt classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"therefore, it is a
promising approach to guide the video model to pay more attention to important
frames close to the class center with the assistance of static keyframe images. meanwhile, our approach aligns with the diagnosis of ultrasound physicians,
automatically evaluates the importance of frames, and diagnoses based on the
information of key frames. additionally, our method provides interpretability
through key frames."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"in this paper, we propose a novel keyframe guided attention network
(kga-net) to boost ultrasound video classiﬁcation. our approach leverages both
image (keyframes) and video datasets to train the network. to classify videos, we
use frame attention to predict feature weights for all frames and aggregate them
to make the ﬁnal classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"speciﬁcally, we propose coherence loss, which promotes the frames
close to the centers to have high attention weights and decreases the weights for
frames far from the centers. due to the feature centers being generated by the
larger scale image dataset, it provides more accurate and discriminative feature
centers which can guide the video frame attention to focus on important frames,
and ﬁnally leads to better video classiﬁcation. our experimental results on the public busv dataset [15] show that our
kga-net signiﬁcantly outperforms other video classiﬁcation models by using an
external ultrasound image dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"this phenomenon makes our method more
explainable and provides a new perspective for selecting keyframes from video. we analyze the relationship between ultrasound video data and image data,
and propose the coherence loss to use image feature centers to guide the
training of frame attention. 2. we propose kga-net, which adopts a static image dataset to boost the
performance of ultrasound video classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"how-
ever, these methods rely on keyframe supervision, which limits their applicabil-
ity. fortunately, the recent publicly available dataset busv [15] has made the
research on the task of bus video-based classiﬁcation possible. in this paper, we
build our model based on this dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"2. overview of our proposed keyframe-guided attention network.
performance. [8] designed two branches to focus on temporal information and
spatial features, respectively. however, 3d cnns have a limited receptive ﬁeld,
and thus struggle to capture long-range dependency."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"empirically, we set λ = 1 in our experiments. during inference, to perform classiﬁcation on video data, the video classiﬁ-
cation network can be utilized individually for prediction. 4
experiments
4.1
implementation details
datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"busi contains 445 images of benign lesions and 210 images
of malignant lesions. for the busv dataset, we use the oﬃcial data split in [15]. all images of the busi dataset are adopted to train our kga-net."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"therefore, we compare our method with strong
video baselines on natural images. for fairness comparison, we train these models using both video
and image data, treating images as static videos. evaluation metrics are reported
on the busv test set for performance assessment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"classiﬁcation thresholds are
determined by youden index. [7]
90.53
82.05
80.77
84.62
kga-net (our) 94.67
89.74
88.46
92.31
formed by the image dataset with larger data size and clear appearance eﬀec-
tively improve the accuracy of frame attention hence boosting the video classi-
ﬁcation performance. 4.3
ablation study
in this section, we ablate the contribution of each key design in our kga-net."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"image guidance is the main purpose of our method. to portray the eﬀect of
using the image dataset, we train the kga-net using busv dataset alone in the
ﬁrst row of table 2. without the image dataset, we generate the feature centers
from the video frames."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"as a result, the performance signiﬁcantly drops due to the
decrease in dataset scale. it also shows that the feature centers generated by the
image dataset are more discriminative than that of the video dataset. it is not
only because the lesion number of busi is larger than busv, but also because
the images in busi are all the keyframes that contain typical characteristics of
lesions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_43.pdf,"kga-net for breast ultrasound video classiﬁcation
449
5
conclusion
we propose kga-net, a novel video classiﬁcation model for breast ultrasound
diagnosis. our kga-net takes as input both the video data and image data to
train the network. we propose the coherence loss to guide the training of the
video model by the guidance of feature centers of the images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"capturing global contextual information plays a critical
role in breast ultrasound (bus) image classiﬁcation. although con-
volutional neural networks (cnns) have demonstrated reliable perfor-
mance in tumor classiﬁcation, they have inherent limitations for mod-
eling global and long-range dependencies due to the localized nature
of convolution operations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"in this study,
we proposed a hybrid multitask deep neural network called hybrid-mt-
estan, designed to perform bus tumor classiﬁcation and segmentation
using a hybrid architecture composed of cnns and swin transformer
components. the proposed approach was compared to nine bus clas-
siﬁcation methods and evaluated using seven quantitative metrics on
a dataset of 3,320 bus images. the results indicate that hybrid-mt-
estan achieved the highest accuracy, sensitivity, and f1 score of 82.7%,
86.4%, and 86.0%, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"https://doi.org/10.1007/978-3-031-43901-8_33
bus tumor classiﬁcation using multitask cnn-transformer network
345
with residual connections. nevertheless, one disadvantage of such architectural
choice is that the feature representations in the deeper layers become increasingly
abstract, leading to a loss of spatial and contextual information. the intrinsic
locality of convolutional operations hinders the ability of cnns to model long-
range dependencies while preserving spatial information in images eﬀectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"self-attention enables vits to capture long-range
dependencies and model complex relationships between diﬀerent regions of the
image. however, the eﬀectiveness of vit-based approaches heavily relies on
access to large datasets for learning meaningful representations of input images. this is primarily because the architectural design of vits does not rely on the
same inductive biases in feature extraction which allow cnns to learn spatially
invariant features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"despite the promising results
of such hybrid approaches, eﬀectively capturing the local patterns and global
long-range dependencies in bus images remains challenging [4,5,24]. multitask learning leverages shared information across related tasks by
jointly training the model. it constrains models to learn representations that
are relevant to all tasks rather than learning task-speciﬁc details."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"in this study, we introduce a hybrid multitask approach, hybrid-mt-estan,
which encompasses tumor classiﬁcation as a primary task and tumor segmenta-
tion as a secondary task. hybrid-mt-estan combines the advantages of cnns
and transformers in a framework incorporating anatomical tissue information in
bus images. speciﬁcally, we designed a novel attention block named anatomy-
aware attention (aaa), which modiﬁes the attention block of swin trans-
former by considering the breast anatomy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"the main contributions of this work are summarized as:
• the proposed architecture eﬀectively integrates the advantages of cnns for
extracting hierarchical and local patterns in bus images and swin trans-
formers for leveraging long-range dependencies. • the designed anatomy-aware attention (aaa) block improves the learning
of contextual information based on the anatomy of the breast. • the multitask learning approach leverages the shared representations across
the classiﬁcation and segmentation tasks to improve the model performance.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"mt-
estan [3] is a cnn-based multitask learning network that simultaneously per-
forms bus classiﬁcation and segmentation. [17], which employs row-column-wise kernels to learn and
fuse context information in bus images at diﬀerent context scales (see fig. speciﬁcally, each mt-estan block is composed of two parallel branches con-
sisting of four square convolutional kernels and two consecutive row-column-wise
kernels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"bus tumor classiﬁcation using multitask cnn-transformer network
347
fig. [18] is a hierarchical transformer-based approach that uses
shifted windows to model global context information. swin transformer parti-
tions an input image into non-overlapping patches of size 4×4, where each patch
is treated as a “token”."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"speciﬁcally, focal loss adds a factor (1 − pi)γ
to the cross-entropy loss where γ is a focusing parameter, resulting in lf ocal =
−1/n n
i=1[(α·ti·(1−pi)γ ·log(pi)+(1−α)·pi·log(1−pi)]. in the formulation, α
bus tumor classiﬁcation using multitask cnn-transformer network
349
is a weighting coeﬃcient, n denotes the number of image samples, ti is the target
label of the ith training sample, and pi denotes the prediction. the segmentation
loss is calculated using the commonly-employed dice loss (ldice) function."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"we combined all four
datasets to build a large and diverse dataset with a total of 3,320 b-mode bus
images, of which 1,664 contain benign tumors and 1,656 have malignant tumors. table 1 shows the detailed information for each dataset. hmss dataset does not
provide the segmentation ground-truth masks, and for this study we arranged
with a group of experienced radiologists to prepare the masks for hmss."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"refer
to the original publications of the datasets for more details. breast ultrasound (bus) datasets. ‘b’ denotes benign tumor and ‘m’ is malig-
nant tumor."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"all experiments were performed on a machine with nvidia quadro rtx 8000
gpus and two intel xeon silver 4210r cpus (2.40ghz) with 512 gb of ram. all bus images in the dataset were zero-padded and reshaped to form square
images. to avoid data leakage and bias, we selected the train, test, and vali-
dation sets based on the cases, i.e., the images from one case (patient) were
350
b. shareef et al.
table 2. performance metrics of the compared methods for bus image classiﬁcation
and segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_33.pdf,"furthermore,
we employed horizontal ﬂip, height shift (20%), width shift (20%), and rota-
tion (20◦c) for data augmentation. the proposed approach utilizes the building
blocks of resnet50 and swin-transformer-v2, pretrained on imagenet dataset. namely, mt-estan uses pretrained resnet50 as a base model for the ﬁve
encoder blocks (the implementation details of mt-estan can be found in
[3])."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"purpose: the use of intra-operative mass spectrometry
along with graph transformer models showed promising results for mar-
gin detection on ex-vivo data. although highly interpretable, these meth-
ods lack the ability to handle the uncertainty associated with intra-
operative decision making."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"the association of the model with clinical
features were explored. the model was further validated for a prospective
ex-vivo data, as well as a breast conserving surgery intra-operative data. results: the purposed model outperformed all baselines, statistically
signiﬁcantly, with average balanced accuracy of 91.6%."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"the estimated attention distribution for status of dif-
ferent hormone receptors agreed with reported metabolic ﬁndings in the
literature. conclusion: deployment of ex-vivo models is challenging
due to the tissue heterogeneity of intra-operative data. the proposed
evidential graph transformer is a powerful tool that while providing
the attention distribution of biochemical subbands, improve the surgical
deployment power by providing decision conﬁdence."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"each spectrum contains the distribution of sampled ions with
respect to their mass to charge ratio (m/z). previously, learning models have
been used in combination with iknife data for ex-vivo tissue characterization
and real-time margin detection [16,17]. the success of clinical deployment of learning models heavily relies on
approaches that are not only accurate but also interpretable."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"therefore, it should
be clear how models reach their decisions and the conﬁdence they have in such
decision. studies suggest that one way to improve these factors is through data
centric approaches i.e. to focus on appropriate representation of data. it has also been shown that graph neural networks
can accurately capture the biochemical signatures of iknife and determine the
tissue type."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"particularly, graph transformer networks (gtn) has have shown
to further enhance the transparency of underlying relation between the graph
nodes and decision making via attention mechanism [11].
biological data, specially those acquired intra-opertively, are heterogeneous
by nature. while the use of ex-vivo data collected under speciﬁc protocols are
beneﬁcial to develop baseline models, intra-operative deployment of these models
is challenging. sus the intra-operative data is recorded continuously while the surgeon cutting
through tissues with diﬀerent heterogeneity and pathology."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"564
a. jamzad et al.
fig. 1. an overview of the proposed approach including data collection and prepro-
cessing, graph conversion, and interpretation of uncertainty and attentions. in this paper, we propose evidential graph transformer (egt), a combina-
tion of graph-based feature-level attention mechanism with sample-level uncer-
tainty estimation, to increase the performance and interpretability of surgical
margin assessment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"this is done by implementing the evidential loss and pre-
diction functions within a graph transformer model to output the uncertainty,
intermediate attention, and model prediction. to demonstrate the state-of-the-
art performance of the proposed approach on mass spectrometry data, the model
is compared with diﬀerent baselines in both cross-validation and prospective
schemes on ex-vivo data. furthermore, the performance of model is also inves-
tigated intraoperatively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"2
materials and methods
figure 1 presents the overview of the proposed approach. following data collec-
tion and curation, each burn (spectrum) is converted to a single graph structure. the proposed graph model learns from the biochemical signatures of the tissue
to classify cancer versus normal tissue."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"the uncertainty and intermediate atten-
tions generated by the model are visualized and explored for their association
with the biochemical mechanisms of cancer. 2.1
data curation
ex-vivo: data is collected from fresh breast tissue samples from the patients
referred to bcs at kingston health sciences center over two years. the study
is approved by the institutional research ethics board and patients consent to be
included."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"peri-operatively, a pathologist guides and annotates the ex-vivo point-
burns, referred to as spectra, from normal or cancerous breast tissue immediately
after excision. in addition to spectral data, clinicopathological details such as the
evidential graph transformer
565
fig. 2. graph conversion: each spectrum is converted to a multi-level hierarchical
graph."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"in total 51 cancer
and 149 normal spectra are collected and stratiﬁed into ﬁve folds (4 for cross
validation and 1 prospectively) with each patient restricted to one fold only. intra-operative: a stream of iknife data is collected during a bcs case
(27 min) at kingston health sciences center. at the sampling rate of 1 hz, a
total of 1616 spectra are recorded."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"each spectrum is then labeled based both on
surgeons comments during the operation and post-operative pathology report. preprocessing:
each spectrum is converted to a hierarchical graph as illus-
trated in fig. 2. the nodes are generated from a speciﬁc subband in each spec-
trum."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"therefore, there are two mechanisms embedded in egt: i) node-level
attention calculation - via aggregation of neighboring nodes according to their
relevance to the predictions, and ii) graph-level uncertainty estimation - via ﬁt-
ting the dirichlet distribution to the predictions. in the context of surgical margin assessment, the attentions reveal the rele-
vant metabolic ranges to cancerous tissue, while uncertainty helps identify and
ﬁlter data with unseen pathology. speciﬁcally, the attentions aﬀect the predic-
tions by selectively emphasizing the contributions of relevant nodes, enabling
the model to make more accurate predictions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"the number of gtls and dense layers are both ﬁxed at 3. additionally, we
run ablation studies on the graph structure themselves to show the importance
of presenting the data as graphs. we try randomizing the edge connections and
dropping the nodes with overlapping m/z subbands."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"average(standard deviation) of accuracy (acc), balanced accuracy (bac)
sensitivity (sen), speciﬁcity (spc), and the area under the curve (auc) for the
proposed evidential graph transformer in comparison with graph transformer (gtn),
graph convolution (gcn), and non-graph convolution (cnn) baselines. 3. left estimated probabilities and uncertainty scores for data samples in test set. right eﬀect of uncertain data exclusion on accuracy and auc during model deploy-
ment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"3. as seen, the higher the uncertainty score
(bottom bar plot), the closer the estimated cancer probability is to 0.5 (top bar
plot). this information can be provided during deployment to further augment
surgical decision making for uncertain data instances. this is demonstrated in
the right plot of fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"3, where the samples with high uncertainties are gradually
disregarded. it can be seen that by not using the network prediction for up to
10% of most uncertain test data, the auc increases to 1. providing surgeons with
not only the model decision but also a measure of model conﬁdence will improve
their intervention decisions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"although
the model still trained due to node aggregation, random graph structure acts
as noise and aﬀects the performance. multi-level graphs were shown to outper-
form other structures for masspect data [akbarifar 2021] as they preserve the
receptive ﬁeld in the neighborhood of subbands (metabolites). evidential graph transformer
569
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"4. visualization of attention distribution for her2 (left) and pr (right) hormone
receptors in cancerous spectra.
fig. 5. intra-operative data and label from a bcs case (top) and the temporal predic-
tion of diﬀerent ex-vivo models (bottom). clinical relevance:
an appropriate visualization of the attention map for
samples can be used to help with this exploration."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"the polar bars in this ﬁgure
show the attention level paid to the nodes in the associated m/z subband. we have also found that there’s
more attention in this range for pr negative breast cancer in comparison pr
positive, which is in concordance with previous literature demonstrating that
these subtypes have higher glutamine metabolic activity [4,5].
intra-operative deployment:
the raw intra-operative iknife data (y-axis
is m/z spectral range and x-axis is the surgery timeline) along with the tem-
poral reference labels extracted from surgeon’s call-outs and pathology report
are shown in fig. as seen, the iknife stream contains spectra from skin
cuts, which is considered as an unseen label for the ex-vivo models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_53.pdf,"previous studies showed the similarity between skin and breast cancer mass spec-
trum that can confuse the binary models. since our proposed egt is equipped
with uncertainty estimation, this information can be used to eliminate skin spec-
tra from being wrongly detected as cancer. by integrating uncertainty, predic-
tions for such burns are ﬂagged as uncertain so clinicians can compensate for
surgical decision making with other sources of information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf,"[13] both employ a convolutional neural network
(cnn) model with axial view ct images to classify the tumor histology into scc and
adc. multi-view deep learning, a 2.5d method, represents a
promising solution to this issue, as it focuses on obtaining a uniﬁed joint representation
from different views of lung nodules to capture abundant spatial information [16, 20]. for example, wu et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf,"in fact, due to the
limitation of scan time and hardware capacity in clinical practice, different views of ct
volumes are anisotropic in terms of in-plane and inter-plane resolution [21]. additionally,
images from certain views may inevitably contain some unique background information,
e.g., the spine in the sagittal view [17]. such anisotropy and background dissimilarity
both reveal the existence of signiﬁcant variations between different views, which lead
to markedly various representations in feature space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf,"speciﬁcally, carl
incorporates a cross-view representation alignment learning network which targets the
reduction of multi-view discrepancies by obtaining discriminative view-invariant repre-
sentations.asharedencoderwithanoveldiscriminability-enforcingsimilarityconstraint
is utilized to map all representations learned from multi-view ct images to a common
subspace, enabling cross-view representation alignment. such aligned projections help
to capture view-invariant features of cross-view ct images and meanwhile make full
use of the discriminative information obtained from each view. additionally, carl
learns view-speciﬁc representations as well which complement the view-invariant ones,
providing a comprehensive picture of the ct volume data for histological subtype pre-
diction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf,"– we employ a view-speciﬁc representation learning network to learn view-speciﬁc
representations as a complement to the view-invariant representations. – we conduct experiments on a publicly available dataset and achieve superior
performance compared to the most advanced methods currently available.
fig. 1. illustration of the proposed carl."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf,"the cross-view representation align-
mentlearningnetworkincludesasharedencoderwhichprojectspatchesofaxial,coronal,
and sagittal views into a common subspace with a discriminability-enforcing similarity
constraint to obtain discriminative view-invariant representations for multi-view dis-
crepancy reduction. in addition, carl introduces a view-speciﬁc representation learn-
ing network consisting of three unique encoders which focus on learning view-speciﬁc
carl: cross-aligned representation learning
361
representations in respective private subspaces to yield complementary information to
view-invariant representations. finally, we introduce a histological subtype classiﬁca-
tion module to fuse the view-invariant and -speciﬁc representations and make accurate
nsclc histological subtype classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf,"inspired by recent work on
362
y. luo et al.
multimodal feature extraction [12, 14], we impose a direct supervision by inputting hmain
into a classiﬁer f (·) to obtain the prediction of histological subtype, and use a cross-
entropy loss to enforce the discriminability of the main-view representations. finally,
the discriminability-enforcing similarity loss ldsim is as follows:
ldsim = lsim + λ · lce

f

hmain
, y

(3)
where y denotes the ground-truth subtype labels, λ controls the weight of lce. = 110 to balance the magnitude of two terms."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf,"secondly, since the alignment between distinct views compels
the representation distribution of the sub-views to match that of the discriminative main
view, it can also enhance the discriminative power of the sub-view representations. in other words, the cross-alignment procedure spontaneously promotes the transfer of
discriminative information learned by the representations of the main view to those of
the sub-views. as a result, the introduced cross-view representation alignment learning
network is able to generate consistent and discriminative view-invariant representations
cross all views to effectively narrow the multi-view discrepancies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf,"2.3
view-speciﬁc representation learning
on the basis of learning view-invariant representations, carl additionally learns
view-speciﬁc representations in respective private subspaces, which provides supple-
mentary information for the view-invariant representations and contribute to subtype
classiﬁcation as well. to be speciﬁc, a view-speciﬁc representation learning network
containing three unique encoders is proposed to learn view-speciﬁc representations
hp
v, v ∈ {av, cv, sv}, enabling effective exploitation of the speciﬁc information from
each view. we formulate the unique encoders as follows:
hp
υ = ep
υ(iυ), υ ∈ {aυ, cυ, sυ}
(4)
where ep
v(·) is the encoder function dedicated to capture single-view characteristics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf,"finally, a total of 325 avail-
able cases (146 adc cases and 179 scc cases) are used for our study. we evaluate the
performance of nsclc classiﬁcation in ﬁve-fold cross validation on the nsclc-tcia
dataset, and measure accuracy (acc), sensitivity (sen), speciﬁcity (spe), and the area
under the receiver operating characteristic (roc) curve (auc) as evaluation metrics. we
also conduct analysis including standard deviations and 95% ci, and delong statistical
test for further auc comparison.
for preprocessing, given that the ct data from nsclc-tcia has an in-plane reso-
lution of 1 mm × 1 mm and a slice thickness of 0.7–3.0 mm, we resample the ct images
using trilinear interpolation to a common resolution of 1mm × 1mm × 1mm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf,"these results demonstrate that carl can effectively narrow
the discrepancies of different views by obtaining view-invariant representations in a
discriminative way, thus leading to excellent classiﬁcation accuracy compared to other
methods. results on nsclc-tcia dataset when carl was compared with other sota methods
using ﬁve-fold cross validation. * indicates the p-value is less than 0.05 in delong test between
the auc of compared method and carl."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_35.pdf,2. roc plots of (a) compared methods and (b) ablation analysis on nsclc-tcia dataset. table 2. results of ablation analysis on the nsclc-tcia dataset. we evaluate the efﬁcacy of different losses in our method.
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"lung nodule malignancy prediction has been enhanced by
advanced deep-learning techniques and eﬀective tricks. nevertheless, cur-
rent methods are mainly trained with cross-entropy loss using one-hot
categorical labels, which results in diﬃculty in distinguishing those nod-
ules with closer progression labels. interestingly, we observe that clinical
text information annotated by radiologists provides us with discrimina-
tive knowledge to identify challenging samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"third, we align image features with both class and attribute
features via contrastive learning, rectifying false positives and false nega-
tives in latent space. experimental results on the benchmark lidc-idri
dataset demonstrate the superiority of clip-lung, in both classiﬁcation
performance and interpretability of attention maps. source code is avail-
able at https://github.com/ymleifdu/clip-lung."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"(a) unimodal contrastive learning. (b) proposed
textual knowledge-guided contrastive learning. yellow values are the annotated malig-
nancy scores."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"usually, the malignancy predic-
tion is often formulated as benign-malignant binary classiﬁcation [9,10,19], and
the higher classiﬁcation performance and explainable attention maps are impres-
sive. most previous works employ a learning paradigm that utilizes cross-entropy
loss between predicted probability distributions and ground-truth one-hot labels. [2,11,13,18,21], where the training set addi-
tionally includes nodules with uncertain labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"indeed, the ordinal regression-
based methods are able to learn ordered manifolds and to further enhance the
prediction accuracy. however, the aforementioned methods still face challenges in distinguishing
visually similar samples with adjacent rank labels. for example, in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"therefore, we propose
leveraging text annotations to guide the learning of visual features. in practice,
this also aligns with the fact that the annotated text information represents the
direct justiﬁcation for identifying lesion regions in the clinic. as shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"[16], provide us with a powerful text encoder
pre-trained with text-based supervisions and have shown impressive results in
downstream vision tasks. nevertheless, it is ineﬀective to directly transfer clip
to medical tasks due to the data covariate shift. therefore, in this paper, we pro-
clip-lung for lung nodule malignancy prediction
405
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"diﬀerent from cocoop,
ccp constructs speciﬁc learnable prompts conditioned on grouped feature maps
and triggers more explainable attention maps such as grad-cam [17], whereas
cocoop provides only the common condition for all the prompt tokens. then,
we design a textual knowledge-guided contrastive learning based on obtained
image features and textual features involving classes and attributes. tal results on lidc-idri [1] dataset demonstrate the eﬀectiveness of learning
with textual knowledge for improving lung nodule malignancy prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"the contributions of this paper are summarized as follows. 1) we propose clip-lung for lung nodule malignancy prediction, which lever-
ages clinical textual knowledge to enhance the image encoder and classiﬁer.
2) we design a channel-wise conditional prompt module to establish consistent
relationships among the correlated text tokens and feature maps.
3) we simultaneously align the image features with class and attribute fea-
tures through contrastive learning while generating more explainable atten-
tion maps. 2
methodology
2.1
overview
problem formulation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"in this paper, we arrange the lung nodule classiﬁcation
dataset as {i, y, c, a}, where i = {ii}n
i=1 is an image set containing n lung nod-
ule images. y = {yi}n
i=1 is the corresponding class label set and yi ∈ {1, 2, . . . , k},
406
y. lei et al.
and k is the number of classes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"= 512 in this
paper. ∈ rk×d,
and attribute feature a ∈ rm×d to conduct the textual knowledge-guided con-
trastive learning. 2.2
instance-speciﬁc attribute weighting
for the attribute annotations, all the lung nodules in the lidc-idri dataset
are annotated with the same eight attributes: “subtlety”, “internal structure”,
“calciﬁcation”, “sphericity”, “margin”, “lobulation”, “spiculation”, and “tex-
ture” [4,8], and the annotated value for each attribute ranges from 1 to 5 except
for “calciﬁcation” that is ranged from 1 to 6."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"hence,
the conditional prompt for the t-th token is lt+l′
t. in addition, ccp also outputs
the f t,: for image-class and image-attribute contrastive learning. 2.4
textual knowledge-guided contrastive learning
recall that our aim is to enable the visual features to be similar to the textual
features of the annotated classes or attributes and be dissimilar to those of irrele-
vant text annotations. consequently, we accomplish this goal through contrastive
learning [3,5,7]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"2, we align f ∈ rt ×d
with c ∈ rk×d and a ∈ rm×d, i.e., using class and attribute knowledge to
regularize the feature maps. first, the same to clip, we align the image and class
information by minimizing the cross-entropy (ce) loss for the sample {ii, yi}:
lic = −
t

t=1
k

k=1
yilog
exp(σ(f t,:, ck,:)/τ)
k
k′=1 exp(σ(f t,:, ck′,:)/τ)
,
(2)
where ck,: = gφ(ck
(l1+l′
1, l2+l′
2, . . ∈ rd×1 and “” denotes con-
catenation, i.e., ck,: is conditioned on learnable prompts lt +l′
t. σ(·, ·) calculates
the cosine similarity and τ is the temperature term."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"lic + α · lia + β · lca

,
(5)
where α and β are hyperparameters for adjusting the losses and are set as 1 and
0.5, respectively. lce denotes the cross-entropy loss between predicted probabil-
ities obtained by the classiﬁer and the ground-truth labels. note that during the
inference phase, test images are only fed into the trained image encoder and clas-
siﬁer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"as a result, clip-lung does not introduce any additional computational
overhead in inference. 3
experiments
3.1
dataset and implementation details
dataset. [1] is a dataset for pulmonary nodule classiﬁcation or detec-
tion based on low-dose ct, which involves 1,010 patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"according to existing works [11,18], we
regard a nodule with an average score between 2.5 and 3.5 as unsure nodules, benign
and malignant categories are those with scores lower than 2.5 and larger than 3.5,
respectively. in this paper, we construct three sub-datasets: lidc-a contains three
classes of nodules both in training and test sets; according to [11], we construct the
lidc-b,whichcontainsthreeclassesofnodulesonly inthetrainingset,andthetest
set contains benign and malignant nodules; lidc-c includes benign and malignant
nodules both in training and test sets. [6] due to the relatively smaller scale of training data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"3.2
experimental results and analysis
performance comparisons. in table 1, we compare the classiﬁcation perfor-
mances on the lidc-a dataset, where we regard the benign-unsure-malignant
clip-lung for lung nodule malignancy prediction
409
table 1. compared with ordinal classiﬁcation methods such
as poisson, nsb, udm, and corf, clip-lung achieves the highest accuracy
and f1-scores for the three classes, demonstrating the eﬀectiveness of textual
knowledge-guided learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"we argue that this is due to the indistinguishable textual annotations, such as
similar attributes of diﬀerent nodules. [12] on lidc-a dataset. the obtained
accuracy values with and without the textual branch are 58.9% and 57.3%,
respectively, demonstrating the eﬀectiveness of integrating textual knowledge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"we report classiﬁcation accuracies. to illustrate the inﬂuence of incor-
porating class and attribute knowledge, we provide the t-sne [14] and grad-
cam [17] results obtained by clip, cocoop, and clip-lung. 3, we
can see that clip yields a non-compact latent space for two kinds of nodules.
cocoop and clip-lung alleviate this phenomenon, which demonstrates that
the learnable prompts guided by nodule classes are more eﬀective than ﬁxed
prompt engineering."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"simultaneously, our clip-lung performs better than cocoop, which
demonstrates the guidance from textual descriptions such as “spiculation”. in table 3, we verify the eﬀectiveness of diﬀerent loss compo-
nents on the three constructed datasets. based on lic, lia and lca improve the
performances on lidc-a, indicating the eﬀectiveness of capturing ﬁne-grained
features of ordinal ranks using class and attribute texts."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"that is to
say, lia is more important in latent space rectiﬁcation, i.e., image-attribute con-
sistency. in addition, we observe that lic+lia performs better than lia+lca,
which is attributed to that lca regularizes the image features indirectly.
clip-lung for lung nodule malignancy prediction
411
4
conclusion
in this paper, we proposed a textual knowledge-guided framework for pulmonary
nodule classiﬁcation, named clip-lung. we explored the utilization of clini-
cal textual annotations based on large-scale pre-trained text encoders."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_38.pdf,"finally, clip-lung outperforms com-
pared methods, including clip on lidc-idri benchmark. future work will
focus on extending clip-lung with more diverse textual knowledge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"addition-
ally, a hybrid spatial-frequency loss function is explored to adaptively
concentrate on the loss of important frequency components due to the
inherent bias of neural networks. we evaluated our method not only on
an in-house database with four types of colorectal lesions with diﬀerent
pathological features, but also on four public databases, with the exper-
imental results showing that our method outperforms state-of-the-art
network models. in particular, it can improve the average dice similarity
coeﬃcient and intersection over union from (84.3%, 78.4%) to (87.0%,
80.5%)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"interventional colonoscopy is
routinely performed by surgeons to visually examine colorectal lesions. however
these lesions in colonoscopic images are easily omitted and wrongly classiﬁed
due to limited knowledge and experiences of surgeons. automatic and accurate
segmentation is a promising way to improve colorectal examination."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"next, a hybrid spatial-frequency loss function is deﬁned to compensate for loss
720
a. wang et al.
features in the spatial domain but available in the frequency domain. , we built a new colonoscopic lesion image database and will make it publicly
available, while this work also conducts a thorough evaluation and comparison
on our new database and four publicly available ones (fig. 2).
fig. 1. ctbmf consists of cascade transformers, boundary-aware multibranch fusion,
and hybrid spatial-frequency loss.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"these white-light and narrow-band colonoscopic images contain four
types of colorectal lesions with diﬀerent pathological features classiﬁed by sur-
geons: (1) 268 cases of hyperplastic polyp, (2) 815 cases of inﬂammatory polyp,
(3) 1363 cases of tubular adenoma, and (4) 143 cases of tubulovillous adenoma. additionally, four public datasets including kvasir, etis-laribpolypdb, cvc-
colondb, and cvc-clinicdb were also used to evaluate our network model. we implemented ctbmf on pytorch and trained it with a single nvidia
rtx3090 to accelerate the calculations for 100 epochs at mini-batch size 16.
factors λ (eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"we employ the stochastic gradient descent
fig. 3. visual comparison of the segmentation results of using the four diﬀerent meth-
ods tested on those in-house and public datasets.green and blue show ground truth
and prediction. a. wang et al.
algorithm to optimize the overall parameters with an original learning rate of
0.0001 for cascade transformer encoding and 0.05 for other parts and use warm-
up and linear decay strategies to adjust it."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"we employ three metrics to evaluate the segmentation: dice similarity coeﬃcient
(dsc), intersection over union (iou), and weighted f-measure (fβ).
fig. 4. dsc boxplots of using the four methods evaluated on our in-house and publicly
available databases
table 2. public data segmented results of our ablation study
modules dsc
iou
fβ
d1
0.681 0.593 0.634
d2
0.822 0.753 0.798
d3
0.820 0.748 0.793
residual 0.828 0.757 0.802
fl
0.834 0.764 0.804
4
results and discussion
figure 3 visually compares the segmentation results of the four methods tested on
our in-house and public databases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"the segmented boundaries of our method are sharper and
clear than others especially in textureless lesions that resemble intestinal lining. cascade transformer encoded boundary-aware multibranch fusion
725
figure 4 shows the dsc-boxplots to evaluate the quality of segmented polyps and
adenomas, which still demonstrate that our method works much better than the
others. figure 5 displays the enhanced feature maps using the boundary-aware
attention module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"table 1 summarizes the quantitative results in accordance with the three
metrics and computational time of four methods. evidently, ctbmf generally
works better than the compared methods on the in-house database with four
types of colorectal lesions. furthermore, we also summarizes the average three
metrics computed from all the ﬁve databases (the in-house dataset and four
public datasets)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"modules d1, d2, d3, residual connections, and frequency loss fl are
gradually added into the baseline, evaluating the eﬀectiveness of each module
and comparing the variants with each other. we tested these modules on the
four public databases. table 2 shows all the ablation study results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"next, the boundary-aware
attention mechanism drives the multibranch fusion, enhancing the representa-
tion of intestinal lesions in weak boundary and nonuniform lighting. such a
mechanism ﬁrst extracts the channel-spatial attention feature map, from which
subtracts the current pyramid transformer’s feature map to enhance the bound-
ary information. also, the multibranch fusion generates multilevel boundary
maps by subtracting the next pyramid transformer’s upsampling output from
the current pyramid transformer’s output, further improving the boundary con-
trast."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"additionally, the hybrid spatial-frequency loss was also contributed to the
improvement of colorectal lesion segmentation. the frequency-domain informa-
tion can compensate loss feature information in the spatial domain, leading to
a better supervision in training. 5
conclusion
this work proposes a new deep learning model of cascade pyramid transformer
encoded boundary-aware multibranch fusion networks to automatically segment
diﬀerent colorectal lesions of polyps and adenomas in colonoscopic imaging."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_69.pdf,"the thorough experimental results show
that our method outperforms the current segmentation models without any pre-
processing. in particular, our method attains much higher accuracy on colono-
scopic images with small, illumination changes, weak-boundary, textureless, and
motion blurring lesions, improving the average dice similarity coeﬃcient and
intersection over union from (89.5%, 84.1%) to (90.3%, 84.4%) on our in-house
database, from (78.9%, 72.6%) to (83.4%, 76.5%) on the four public databases,
and from (84.3%, 78.4%) to (87.0%, 80.5%) on the ﬁve databases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"segmenting prostate from mri is crucial for diagnosis and
treatment planning of prostate cancer. given the scarcity of labeled data
in medical imaging, semi-supervised learning (ssl) presents an attractive
option as it can utilize both limited labeled data and abundant unlabeled
data. however, if the local center has limited image collection capability,
there may also not be enough unlabeled data for semi-supervised learn-
ing to be eﬀective."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"to overcome this issue, other partner centers can be
consulted to help enrich the pool of unlabeled images, but this can result
in data heterogeneity, which could hinder ssl that functions under the
assumption of consistent data distribution. tailoring for this important
yet under-explored scenario, this work presents a novel category-level
regularized unlabeled-to-labeled (cu2l) learning framework for semi-
supervised prostate segmentation with multi-site unlabeled mri data. speciﬁcally, cu2l is built upon the teacher-student architecture with
the following tailored learning processes: (i) local pseudo-label learning
for reinforcing conﬁrmation of the data distribution of the local center;
(ii) category-level regularized non-parametric unlabeled-to-labeled learn-
ing for robustly mining shared information by using the limited expert
labels to regularize the intra-class features across centers to be discrim-
inative and generalized; (iii) stability learning under perturbations to
further enhance robustness to heterogeneity."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"keywords: prostate segmentation · semi-supervised · heterogeneity
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43901-8_1
4
z. xu et al.
40
45
50
55
60
65
70
75
80
suponly
mt
ua-mt
ict
cpcl
cct
cps
ssnet
local labeled 
local unlabeled
external multi-site unlabeled support data 
…
,
,
,
typical semi-supervised learning (ssl)
effectively work under the assumption that the 
centralized local i.i.d. unlabeled data is abundant

limited performance gain or even fail when the local
unlabeled data is also limited (e.g., due to restricted 
image collection capabilities or a scarcity of patients)

multi-site semi-supervised learning (ms-ssl)
the unlabeled image pool can be quickly enriched
via the support from partner clinical centers with low 
barriers of entry (only unlabeled images are required)

data heterogeneity due to different scanners, 
scanning protocols and subject groups, which violate 
the typical ssl assumption of i.i.d. data
local: c1. metric: dsc (%)
exploratory study on recent ssl methods 
with limited local unlabeled data
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"recently, deep learning-
based approaches have greatly improved the accuracy and eﬃciency of automatic
prostate mri segmentation [7,8]. yet, their success usually requires a large
amount of labeled medical data, which is expensive and expertise-demanding
in practice. in this regard, semi-supervised learning (ssl) has emerged as an
attractive option as it can leverage both limited labeled data and abundant
unlabeled data [3,9–11,15,16,21–26,28]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"nevertheless, the eﬀectiveness of ssl
is heavily dependent on the quantity and quality of the unlabeled data. regarding quantity, the abundance of unlabeled data serves as a way to
regularize the model and alleviate overﬁtting to the limited labeled data. unfor-
tunately, such “abundance” may be unobtainable in practice, i.e., the local unla-
beled pool is also limited due to restricted image collection capabilities or scarce
patient samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"as a speciﬁc case shown in table 1, there are only limited
prostate scans available per center. taking c1 as a case study, if the amount of
local unlabeled data is limited, existing ssl methods may still suﬀer from inferior
performance when generalizing to unseen test data (fig. 1). to eﬃciently enrich
the unlabeled pool, seeking support from other centers is a viable solution, as
illustrated in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"being an under-explored scenario, few eﬀorts have been made. however, it only
deals with additional unlabeled data from a speciﬁc source rather than multiple
arbitrary sources. thus, it intuitively utilizes image-level mapping to minimize
dual-distribution discrepancy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"[3] in
ms-ssl as elaborated in sec. , the local unlabeled data is involved into pseudo-
label supervised-like learning to reinforce ﬁtting of the local data distribution;
(ii) considering that intra-class variance hinders eﬀective ms-ssl, we introduce
a non-parametric unlabeled-to-labeled learning scheme, which takes advantage of
the scarce expert labels to explicitly constrain the prototype-propagated predic-
tions, to help the model exploit discriminative and domain-insensitive features
from heterogeneous multi-site data to support the local center. yet, observing
that such scheme is challenging when signiﬁcant shifts and various distributions
are present, we further propose category-level regularization, which advocates
prototype alignment, to regularize the distribution of intra-class features from
arbitrary external data to be closer to the local distribution; (iii) based on the
fact that perturbations (e.g., gaussian noises [15]) can be regarded as a simu-
lation of heterogeneity, perturbed stability learning is incorporated to enhance
the robustness of the model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"our method is evaluated on prostate mri data
from six diﬀerent clinical centers and shows promising performance on tackling
ms-ssl compared to other semi-supervised methods. 2
methods
2.1
problem formulation and basic architecture
in our scenario of ms-ssl, we have access to a local target dataset dlocal
(consisted of a labeled sub-set dl
local and an unlabeled sub-set du
local) and
the external unlabeled support datasets du
e
= m
j=1 du,j
e , where m is the
number of support centers. i=nl+1 with nu unlabeled scans."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"ema: exponential moving average. xl
local(i), xu
local(i) ∈ rh×w ×d denote the scans with height h, width w and
depth d, and y l
local(i) ∈ {0, 1}h×w ×d denotes the label of xl
local(i) (we focus
on binary segmentation). similarly, the j-th external unlabeled support dataset
is denoted as du,j
e
= {xu,j
e(i)}nj
i=1 with nj unlabeled samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"compared to the stu-
dent, the teacher performs self-ensembling by nature which helps smooth out the
noise and avoid sudden changes of predictions [15]. thus, the teacher model is
suitable for handling the heterogeneous external images and producing relatively
stable pseudo labels (will be used later). as such, our task of ms-ssl can be
formulated as optimizing the following loss:
l = ll
sup

θ, dl
local

+ λlu(θ, ˜θ, du
local, du
e ),
(1)
where ll
sup is the supervised guidance from local labeled data and lu denotes the
additional guidance from the unlabeled data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"λ is a trade-oﬀ weight scheduled by
the time-dependent ramp-up gaussian function [15] λ(t) = wmax·e−5(1−t/tmax)2,
where wmax and tmax are the maximal weight and iteration, respectively. the key
challenge of ms-ssl is the proper design of lu for robustly exploiting multi-site
unlabeled data {du
local, du
e } to support the local center. category-level regularized unlabeled-to-labeled learning
7
2.2
pseudo labeling for local distribution fitting
as mentioned above, supervised-like learning is advocated for local unlabeled
data to help the model ﬁt local distribution better."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"owning the self-ensembling
property, the teacher model provides relatively stable pseudo labels for the stu-
dent model. given the predicted probability map p u,t
local of xu
local from the teacher
model, the pseudo label ˆy u,t
local corresponds to the class with the maximal pos-
terior probability. yet, with limited local labeled data for training, it is diﬃcult
to generate high-quality pseudo labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"thus, for each pixel, if maxc(pu,t
local) ≥ δ,
where c denotes the c-th class and δ is a ramp-up threshold ranging from 0.75
to 0.9 as training goes, this pixel will be included in loss calculation. [27] to perform pseudo label learning,
formulated as: lu
p l =
1
k
k
k=1 ldice

p u,s,k
local , ˆy u,t,k
local

, where p u,s
local denotes the
prediction of xu
local from the student model. the dice loss is calculated for each
of the k equally-sized regions of the image, and the ﬁnal loss is obtained by
taking their mean."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"2.3
category-level regularized unlabeled-to-labeled learning
unlabeled-to-labeled learning. inherently, the challenge of ms-ssl stems
from intra-class variation, which results from diﬀerent imaging protocols, disease
progress and patient demographics. inspired by prototypical networks [13,19,25]
that compare class prototypes with pixel features to perform segmentation,
here, we introduce a non-parametric unlabeled-to-labeled (u2l) learning scheme
that utilizes expert labels to explicitly constrain the prototype-propagated pre-
dictions. such design is based on two considerations: (i) a good prototype-
propagated prediction requires both compact feature and discriminative pro-
totypes, thus enhancing this prediction can encourage the model to learn in a
variation-insensitive manner and focus on the most informative clues; (ii) using
expert labels as ﬁnal guidance can prevent error propagation from pseudo labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"note that f u,t
e
has
been upsampled to the same size of xu
e via bilinear interpolation but with l
channels. with the argmax pseudo label ˆy u,t
e
and the predicted probability map
p u,t
e
, the object prototype from the external unlabeled data can be computed via
conﬁdence-weighted masked average pooling: cu(obj)
e
=

v

ˆy u,t,obj
e(v)
·p u,t,obj
e(v)
·f u,t
e(v)


v

ˆy u,t,obj
e(v)
·p u,t,obj
e(v)

. likewise, the background prototype cu(bg)
e
can also be obtained."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"e
} with the
features f l
local pixel-by-pixel and obtain the prototype-propagated prediction
8
z. [19]. note that a similar procedure can also be applied to the local unlabeled
data xu
local, and thus we can obtain another prototype-propagated unlabeled-
to-labeled prediction p u2l
local for xl
local. as such, given the accurate expert label
y l
local, the unlabeled-to-labeled supervision can be computed as:
lu2l = 1
k
 k
	
k=1
ldice (p u2l,k
e
, y l,k
local) +
k
	
k=1
ldice (p u2l,k
local , y l,k
local)

."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"thus, proper mecha-
nisms are needed to alleviate the negative impact of signiﬁcant shift and multi-
ple distributions. speciﬁcally, we introduce category-level regularization, which
advocates class prototype alignment between local and external data, to regu-
larize the distribution of intra-class features from arbitrary external data to be
closer to the local one, thus reducing the diﬃculty of u2l learning. in u2l, we
have obtained prototypes from local unlabeled data {cu(obj)
local , cu(bg)
local } and exter-
nal unlabeled data {cu(obj)
e
, cu(bg)
e
}."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"the network is trained using the sgd optimizer and the learning
rate is initialized as 0.01 and decayed by multiplication with (1.0 − t/tmax)0.9. data augmentation is applied, including random ﬂip and rotation. we adopt the
dice similarity coeﬃcient (dsc) and jaccard as the evaluation metrics and the
results are the average over three runs with diﬀerent seeds."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"all methods are imple-
mented with the same backbone and training protocols to ensure fairness. as
observed, compared to the supervised-only baselines, our cu2l with {6, 8} local
labeled scans achieves {19.15%, 17.42%} and {9.1%, 6.44%} dsc improvements
in {c1, c2}, showing its eﬀectiveness in leveraging multi-site unlabeled data. despite the violation of the assumption of i.i.d. data, existing ssl methods can
still beneﬁt from the external unlabeled data to some extent compared to the
results using local data only as shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"1, revealing that the quantity of
unlabeled data has a signiﬁcant impact. particularly,
cps relies on cross-modal pseudo labeling which exploits all the unlabeled data
in a supervised-like fashion. we attribute its degradation to the fact that super-
vised learning is crucial for distribution ﬁtting, which supports our motivation
of performing pseudo-label learning on local unlabeled data only."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"[2] is mediocre in ms-ssl, mainly due to the instabil-
ity of adversarial training and the diﬃculty of aligning multiple distributions
to the local distribution via a single image-mapping network. in contrast, with
specialized mechanisms for simultaneously learning informative representations
category-level regularized unlabeled-to-labeled learning
11
from multi-site data and handling heterogeneity, our cu2l obtains the best
performance over the recent ssl methods. figure 3(a) further shows that the
predictions of our method ﬁt more accurately with the ground truth."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_1.pdf,"firstly, when we remove lu
p l (cu2l-1), the performance drops by
{5.69% (c1), 3.05%(c2)} in dsc, showing that reinforcing conﬁrmation on local
distribution is critical. cu2l-2 represents the removal of both lu2l and lcr, and
it can be observed that such an unlabeled-to-labeled learning approach com-
bined with class-level regularization is crucial for exploring multi-site data. if we
remove lcr which accompanies with lu2l (cu2l-3), the performance degrades,
which justiﬁes the necessity of this regularization to reduce the diﬃculty of
unlabeled-to-labeled learning process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"in this paper, we propose cellgan to synthesize cytopatholog-
ical images of various cervical cell types for augmenting patch-level cell
classiﬁcation. built upon a lightweight backbone, cellgan is equipped
with a non-linear class mapping network to eﬀectively incorporate cell
type information into image generation. we also propose the skip-layer
global context module to model the complex spatial relationship of the
cells, and attain high ﬁdelity of the synthesized images through adver-
sarial learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"our code and model check-
point are available at https://github.com/zhenrongshen/cellgan. [23].
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2_47.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. typically there are
ﬁve types of cervical squamous cells under tct examinations [5], including nor-
mal class or negative for intraepithelial malignancy (nilm), atypical squamous
cells of undetermined signiﬁcance (asc-us), low-grade squamous intraepithelial
lesion (lsil), atypical squamous cells that cannot exclude hsil (asc-h), and
high-grade squamous intraepithelial lesion (hsil)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"the promising performance of cell classiﬁca-
tion at the patch level is critical, which contributes to sample-level diagnosis
after integrating outcomes from many patches in a wsi. however, such a patch-
level classiﬁcation task requires a large number of annotated training data. and
the eﬀorts in collecting reliably annotated data can hardly be negligible, which
requires high expertise due to the intrinsic diﬃculty of visually reading wsis.
to alleviate the shortage of suﬃcient data to supervise classiﬁcation, one
may adopt traditional data augmentation techniques, which yet may bring little
improvement due to scarcely expanded data diversity [26]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"we lever-
age fastgan [16] as the backbone for the sake of training stability and compu-
tational eﬃciency. to inject cell type for ﬁne-grained conditioning, a non-linear
mapping network embeds the class labels to perform layer-wise feature modu-
lation in the generator. meanwhile, we introduce the skip-layer global context
(sgc) module to capture the long-range dependency of cells for precisely mod-
eling their spatial relationship."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"the numbers in the center and
the bottom right corner of each square indicate the feature map size and the channel
number, respectively. tional data distribution. to the best of our knowledge, our proposed cellgan is
the ﬁrst generative model with the capability to synthesize realistic cytopatho-
logical images for various cervical cell types."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"the experimental results validate
the visual plausibility of cellgan synthesized images, as well as demonstrate
their data augmentation eﬀectiveness on patch-level cell classiﬁcation. 2
method
the dilemma of medical image synthesis lies in the conﬂict between the lim-
ited availability of medical image data and the high demand for data amount
to train reliable generative models. to ensure the synthesized image quality
given relatively limited training samples, the proposed cellgan is built upon
fastgan [16] towards stabilized and fast training for few-shot image synthesis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"490
z. shen et al.
2.1
architecture of the generator
the generator of cellgan has two input vectors. the ﬁrst input of the class
label y, which adopts one-hot encoding, provides class-conditional information to
indicate the expected cervical cell type in the synthesized image isyn. the second
input of the 128-dimensional latent vector z represents the remaining image
information, from which isyn is gradually expanded."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"we stack six upblocks to
form the main branch of the generator. speciﬁcally, the class label y is ﬁrst projected to a class embed-
ding c via a non-linear mapping network, which is implemented using four groups
of fully connected layers and leakyrelu activations. we set the dimensions of
class embedding c to the same as the latent vector z."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"[13] in each upblock. the motivation for the design above comes from
our hypothesis that the class-conditional information mainly encodes cellular
attributes related to cell types, rather than common image appearance. there-
fore, by modulating the feature maps at multiple scales, the input class label
can better control the generation of cellular attributes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"it ﬁrst
performs global context modeling on the low-resolution feature maps, then trans-
forms global context to capture channel-wise dependency, and ﬁnally merges the
transformed features into high-resolution feature maps. in this way, the proposed
sgc module learns a global understanding of the cell-to-cell spatial relationship
and injects it into image generation via computationally eﬃcient modeling of
long-range dependency.
2.2
discriminator and adversarial training
in an adversarial training setting, the discriminator forces the generator to
faithfully match the conditional data distribution of real cervical cytopatho-
logical images, thus prompting the generator to produce visually and seman-
tically realistic images. for training stability, the discriminator is trained as a
feature encoder with two extra decoders."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"by penalizing image content at the scale of patches, the
color ﬁdelity of synthesized images is guaranteed as illustrated in our ablation
study (see fig. 3). to align the class-conditional fake and real data distributions
in the adversarial setting, the discriminator directly incorporates class labels
as additional inputs in the manner of projection discriminator [20]. the class
label is projected to a learned 512-dimensional class embedding and takes inner-
product at every spatial position of the 8 × 8 × 512 feature map."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"combining all the loss functions above,
the total objective ltotal to train the proposed cellgan in an adversarial manner
can be expressed as:
ltotal = ladv + lrecon + λreglreg,
(2)
where λreg is empirically set to 0.01 in our experiments. 3
experimental results
3.1
dataset and experimental setup
dataset. in this study, we collect 14,477 images with 256 × 256 pixels from
three collaborative clinical centers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"all the images are manually inspected to
contain diﬀerent cervical squamous cell types. all the 256×256
images with their class labels are selected as the training data. we use the learning rate of 2.5×10−4, batch size of
64, and adam optimizer [14] to train both the generator and the discriminator
for 100k iterations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"quantitative comparison between state-of-the-art generative models and the
proposed cellgan (↓: lower is better). and it also
fails to capture the morphological features of hsil cells that are relatively lim-
ited in training data quantity. ldm only yields half-baked cell structures since
the generated cells are mixed, and there exists negligible class separability among
abnormal cell types."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"on the contrary, our proposed cellgan is able to synthe-
size visually plausible cervical cells and accurately model distinguishable cellular
features for each cell type. the quantitative comparison by fid in table 1 also
demonstrates the superiority of cellgan in synthesized image quality.
to verify the eﬀects of key components in the proposed cellgan, we conduct
an ablation study on four model settings in table 2 and fig. 3. we denote the
models in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"the quantitative results further state the eﬀects of the components above. 3.3
evaluation of augmentation eﬀectiveness
to validate the data augmentation capacity of the proposed cellgan, we con-
duct 5-fold cross-validations on the cell classiﬁcation performances of two classi-
494
z. shen et al.
table 3. data augmentation comparison between the proposed cellgan and other
synthesis-based methods (↑: higher is better)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"for each cell type, we randomly select 400 real images and
divide them into 5 groups. in each fold, one group is selected as the testing data
while the other four are used for training. for diﬀerent data settings, we synthe-
size 2,000 images for each cell type using the corresponding generative method,
and add them to the training data of each fold."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"we use the learning rate of
1.0 × 10−4, batch size of 64, and sgd optimizer [24] to train all the classiﬁers
for 30 epochs. random ﬂip is applied to all data settings since it is reasonable
to use traditional data augmentation techniques simultaneously in practice. the experimental accuracy, precision, recall, and f1 score are listed in
table 3."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_47.pdf,"compared with the
baselines, the accuracy values of resnet-34 and densenet-121 are improved by
5.25% and 4.05%, respectively. meanwhile, the scores of other metrics are all
improved by more than 4%, indicating that our synthesized data can signif-
icantly enhance the overall classiﬁcation performance. thanks to the visually
plausible and semantically realistic synthesized data, cellgan is conducive to
the improvement of cell classiﬁcation, thus serving as an eﬃcient tool for aug-
menting automatic abnormal cervical cell screening."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"cancer grading is an essential task in pathology. the recent develop-
ments of artiﬁcial neural networks in computational pathology have shown that
these methods hold great potential for improving the accuracy and quality of
cancer diagnosis. however, the issues with the robustness and reliability of such
methods have not been fully resolved yet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"the proposed network maps an input pathology image into an
embedding space and adjusts it by using centroids embedding vectors of different
cancer grades via attention mechanism. equipped with the recalibrated embed-
ding vector, the proposed network classiﬁers the input pathology image into a
pertinent class label, i.e., cancer grade. we evaluate the proposed network using
colorectal cancer datasets that were collected under different environments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"keywords: cancer grading · attention · feature calibration · pathology
1
introduction
globally, cancer is a leading cause of death and the burden of cancer incidence and mor-
tality is rapidly growing [1]. in cancer diagnosis, treatment, and management, pathology-
driven information plays a pivotal role. cancer grade is, in particular, one of the major
factors that determine the treatment options and life expectancy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"such attention mechanisms have been usually utilized in a multiple
instance learning framework or as self-attention for feature representations. to the best
of our knowledge, attention mechanisms have not been used for feature representations
of class centroids. in this study, we propose a centroid-aware feature recalibration network (cafenet)
for accurate and robust cancer grading in pathology images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"the feature extractor is
utilized to obtain the feature representation of pathology images. cup module obtains
and updates the centroids of class labels, i.e., cancer grades. cafe module adjusts the
input embedding vectors with respect to the class centroids (i.e., training data distri-
bution)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"this indicates that the centroid embedding vectors can be used to recal-
ibrate the input embedding vectors of pathology images. during inference, we ﬁx the
centroid embedding vectors so that the recalibrated embedding vectors do not vary much
compared to the input embedding vectors even though the data distribution substantially
changes, leading to improved stability and robustness of the feature representation. in
this manner, the feature representations of the input pathology images are re-calibrated
and stabilized for a reliable cancer classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"1. overview of cafenet. cafenet consists of a feature extractor, a cafe module, a cup
module, and a classiﬁcation layer.
2.1
centroid-aware feature recalibration
let {xi, yi}n
i=1 be a set of pairs of pathology images and ground truth labels where n
is the number of pathology image-ground truth label pairs, xi ∈ rh×w×c is the i th
pathology image, yi ∈ {c1, . . . , cm } represents the corresponding ground truth label."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"h, w, and c denote the height, width, and the number of channels, respectively. m is
the cardinality of the class labels. given xi, a deep neural network f maps xi into an
embedding space, producing an embedding vector ei ∈ rd."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"the embedding vector ei is
fed into 1) a centroid update (cup) module and 2) a centroid-aware feature recalibration
(cafe) module. cup module obtains and updates the centroid of the class label in the
embedding space ec ∈ rm ×d. cafe module adjusts the embedding vector ei in regard
to the embedding vectors of the class centroids and produces a recalibrated embedding
vector er
i ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"given a batch of input embedding vectors e = {ei|i = 0, . . − 1}, cup module
computes and updates the centroid embedding vector of each class label per epoch. speciﬁcally, cup module adds up the embedding vectors of different class labels over
the iterations per epoch, computes the average embedding vectors, and updates the
centroid embedding vectors ec =

ec
j |j = 0, . . ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,", m − 1

.
cafe module receives a batch of embedding vectors e = {ei|i = 0, . . − 1}
and the ground truth labels y = {yi|i = 0, . . . n − 1} and a set of centroid embedding
vectors ec =

ec
j |j = 0, . ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"each mobile inverted bottleneck block
comprises one pointwise convolution (1 × 1 convolution for the channel expansion), one
depth-wise separable convolution with a kernel size of 3 or 5, and one project pointwise
convolution (1 × 1 convolution for the channel reduction). 264
192
8394
md
2997
370
738
61985
pd
1391
234
205
11895
3
experiments and results
3.1
datasets
two publicly available colorectal cancer datasets [9] were employed to evaluate the
effectiveness of the proposed cafenet. table 1 shows the details of the datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"both
datasets provide colorectal pathology images with ground truth labels for cancer grad-
ing. the ground labels are benign (bn), well-differentiated (wd) cancer, moderately-
differentiated (md) cancer, and poorly-differentiated (pd) cancer. the ﬁrst dataset
includes 1600 bn, 2322 wd, 4105 md, and 1830 pd image patches that were col-
lected between 2006 and 2008 using an aperio digital slide scanner (leica biosystems)
at 40x magniﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"each image patch has a spatial size of 1024 × 1024 pixels. this
dataset is divided into a training dataset (ctrain), validation dataset (cvalidation), and a
test dataset (ctesti). the second dataset, designated as ctestii, contains 27986 bn, 8394
wd, 61985 md, and 11985 pd image patches of size 1144 × 1144 pixels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"these were
acquired between 2016 and 2017 using a nanozoomer digital slide scanner (hamamatsu
photonics k.k). [9], which
demonstrates the state-of-the-art performance on the two colorectal cancer datasets under
consideration. for triplet and sc, efﬁcientnet was used as a backbone network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"the results of mmae−ceo were obtained from the original literature. 3.3
implementation details
we initialized all models using the pre-trained weights on the imagenet dataset, and then
trained them using the adam optimizer with default parameter values (β1= 0.9, β2 =
0.999, ε = 1.0e-8) for 50 epochs. we employed cosine anneal warm restart schedule with
initial learning rates of 1.0 e−3, ηmin= 1.0 e−3, and t0 = 20."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"we implemented all models using the pytorch platform and trained on a workstation
equipped with two rtx 3090 gpus. to increase the variability of the dataset during
the training phase, we applied several data augmentation techniques, including afﬁne
transformation, random horizontal and vertical ﬂip, image blurring, random gaussian
noise, dropout, random color saturation and contrast conversion, and random contrast
transformations. all these techniques were implemented using the aleju library (https://
github.com/aleju/imgaug)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"in a head-
to-head comparison of the classiﬁcation results between ctesti and ctestii, there was
a consistent performance drop in the proposed cafenet and other competing models. this is ascribable to the difference between the test datasets (ctesti and ctestii) and the
training and validation datasets (ctrain and cvalidation). in regard to such differences, it is
striking that the proposed cafenet achieved the best performance on ctestii."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"efﬁcientnet,
however, obtained poorer performance on both ctesti and ctestii. these results suggest
that cafenet has the better generalizability so as to well adapt to unseen histopathology
image data. we conducted ablation experiments to investigate the effect of the cafe module on
cancer classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_20.pdf,"the proposed approach proposes to
improve the feature representation of deep neural networks by re-calibrating input
embedding vectors via an attention mechanism in regard to the centroids of cancer
grades. in the experiments on colorectal cancer datasets against several competing mod-
els, the proposed network demonstrated that it has a better learning capability as well as
a generalizability in classifying pathology images into different cancer grades. however,
the experiments were only conducted on two public colorectal cancer datasets from a
single institute."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf,"moreover, our approach is easy to generalize to
the segmentation task by adding a simple segmentation branch to cir-
cleformer. we evaluate our method in circular nuclei detection and seg-
mentation on the public monuseg dataset, and the experimental results
show that our method achieves promising performance compared with
the state-of-the-art approaches. the eﬀectiveness of each component is
validated via ablation studies as well."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf,"in this way, our design of circle-
former lends itself to circular object detection. we evaluate our circleformer on
the public monuseg dataset for nuclei detection in whole slide images. experi-
mental results show that our method outperforms both cnn-based methods for
box detection and circular object detection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf,"we denote ci = (xi, yi, ri) as the i-th
anchor, xi, yi, ri ∈ r. its corresponding content part and positional part are
zi ∈ rd and pi ∈ rd, respectively. kx,y = concat(fx,y, pe(x, y)), vx,y = fx.y,
(3)
496
h. zhang et al.
where fx,y ∈ rd denote the image feature at position (x, y) and an mlp(csq) :
rd → rd is used to obtain a scaled vector conditioned on content information
for a query.
by representing a circle query as (x, y, r), we can reﬁne the circle query
layer-by-layer in the transformer decoder. speciﬁcally, each transformer decoder
estimates relative circle information (δx, δy, δr)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf,"[0, 1].
deformable circle cross attention. we modify standard deformable atten-
tion to deformable circle cross attention by applying radius information as con-
straint. cos δθmik, piy + δrmik × ri,ref × sin δθmik)) ,
(5)
where m indexes the attention head, k indexes the sampled keys."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf,"circle training loss. following detr, i-th each element of the groundtruth
set is yi = (li, ci), where li is the target class label (which may be ∅) and
ci = (x, y, r). + i{li̸=∅}lcircle(ci, ˆcσ(i)),
(6)
where σ ∈ sn is a permutation of all prediction elements, ˆyσ(i) = (ˆlσ(i), ˆcσ(i)) is
the prediction, λfocal ∈ r are hyperparameters, and lfocal is focal loss [8]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf,"+ lseg,
(7)
where ˆσ(i) is the index of prediction ˆy corresponding to the i-th ground truth
y after completing the match. mi is the ground truth obtained by roi align [5]
corresponding to ˆmi.
498
h. zhang et al.
3
experiment
3.1
dataset and evaluation
monuseg dataset. monuseg dataset is a public dataset from the 2018 multi-
organ nuclei segmentation challenge [6]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf,"the mlps of the prediction heads share the same
parameters. since the maximum number of objects per image in the dataset is
close to 1000, we set the number of queries to 1000. the parameter of focal loss
for classiﬁcation is set to α = 0.25, γ = 0.1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf,"circleformer-d-joint which
circleformer: circular nuclei detection
499
table 1. results of nuclei detection on monuseg dataset. best and second-best
results are colored red and blue, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_48.pdf,"we have provided additional visual analysis
in the open source code repository.
3.4
ablation studies
we conduct ablation studies with circleformer on the nuclei detection task
(table 5). 500
h. zhang et al.
table 3. results of the ablation study analyzing the eﬀects of proposed com-
ponents in circleformer on monuseg dataset. wh-ma: wh-modulated atten-
tion; c-ma: circle-modulated attention; sda: standard deformable attention;
cda-r: circle deformable attention with random initialization; cda-c: circle
deformable attention with cirle initialization."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"288 patients were used for model training and 15 patients were used to synthesize
vce-mri for clinical evaluation. two board-certiﬁed oncologists were invited
for evaluating the vce-mri in two aspects: image quality and effectiveness in
primary tumor delineation. image quality of vce-mri evaluation includes dis-
tinguishability between real contrast-enhanced mri (ce-mri) and vce-mri,
clarity of tumor-to-normal tissue interface, veracity of contrast enhancement in
tumorinvasionriskareas,andefﬁcacyinprimarytumorstaging.forprimarytumor
delineation, the gtv was manually delineated by oncologists."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"results showed the
mean accuracy to distinguish vce-mri from ce-mri was 53.33%; no signiﬁ-
cant difference was observed in clarity of tumor-to-normal tissue interface between
vce-mri and ce-mri; for the veracity of contrast enhancement in tumor inva-
sion risk areas and efﬁcacy in primary tumor staging, a jaccard index of 76.04%
and accuracy of 86.67% were obtained, respectively. the image quality evalu-
ation suggests that the quality of vce-mri is approximated to real ce-mri.
in tumor delineation evaluation, the dice similarity coefﬁcient and hausdorff
distance of the gtvs that delineated from vce-mri and ce-mri were 0.762
(0.673–0.859) and 1.932 mm (0.763 mm–2.974 mm) respectively, which were
clinically acceptable according to the experience of the radiation oncologists. this study demonstrated the vce-mri is highly promising in replacing the use
of gadolinium-based ce-mri for npc delineation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"in 2018, gong et al. [11] utilized pre-contrast and 10% low-dose t1w mri
to synthesize the vce-mri for brain disease diagnosis using a u-shape model, they
found that gadolinium dose is able to be reduced by 10-fold by deep learning while the
contrast information could be preserved. followed by their work, kleesiek et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"rigorous clinical evaluations can establish the safety and efﬁcacy of ai-based tech-
niques, identify potential biases and limitations, and facilitate the integration of clinical
expertise to ensure accurate and meaningful results [13]. furthermore, the clinical evalu-
ation of ai-based techniques can help identify areas for improvement and optimization,
leading to development of more effective algorithms.
clinical evaluation of ai-assisted virtual contrast enhanced mri
543
to bridge this bench-to-bedside research gap, in this study, we conducted a series of
clinicalevaluationstoassesstheeffectivenessofsyntheticvce-mriinnpcdelineation,
with a particular focus on assessment in vce-mri image quality and primary gtv
delineation. this study has two main novelties: (i) to the best of our knowledge, this
is the ﬁrst clinical evaluation study of the vce-mri technique in rt; and (ii) multi-
institutional mri data were included in this study to obtain more reliable results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"the
success of this study would ﬁll the current knowledge gap and provide the medical
community with a clinical reference prior to clinical application of the novel vce-mri
technique in npc rt.
2
materials and methods
2.1
data description
patient data was retrospectively collected from three oncology centers in hong kong. this dataset included 303 biopsy-proven (stage i-ivb) npc patients who received radi-
ation treatment during 2012–2016. the three hospitals were labelled as institution-1
(110 patients), institution-2 (58 patients), and institution-3 (135 patients), respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"mri images were automatically registered as mri images for each
patient were scanned in the same position. the use of this dataset was approved by the
institutional review board of the university of hong kong/hospital authority hong
kong west cluster (hku/ha hkw irb) with reference number uw21-412, and the
research ethics committee (kowloon central/kowloon east) with reference number
kc/ke-18-0085/er-1. due to the retrospective nature of this study, patient consent was
waived."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"for model development, 288 patients were used for model development and
15 patients were used to synthesize vce-mri for clinical evaluation. the details of
patient characteristics and the number split for training and testing of each dataset were
illustrated in table 1. prior to model training, mri images were resampled to 256*224
by bilinear interpolation [14] due to the inconsistent matrix sizes of the three datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"in this work, we obtained
12806 image pairs for model training. different from the original study, which used
single institutional data for model development and utilized min-max value of the whole
dataset for data normalization, in this work, we used mean and standard deviation of
each individual patient to normalize mri intensities due to the heterogeneity of the mri
intensities across institutions [15].
544
w. li et al.
table 1. details of the multi-institutional patient characteristics. fs: ﬁeld strength; tr: repetition
time; te: echo time; no.: number; avg: average."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"institution
(vendor-fs)
patient no. two board-certiﬁed radiation oncologists (with
8 years’ and 6 years’ clinical experience, respectively) were invited to perform the
vce-mri quality assessment and gtv delineation according to their clinical expe-
rience. considering the clinical burden of oncologists, 15 patients were included for
clinical evaluations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"the results were obtained under the
consensus of the two oncologists. image quality assessment of vce-mri. to evaluate the image quality of synthetic
vce-mri against the real ce-mri, we conducted four rt-related evaluations: (i) dis-
tinguishability between ce-mri and vce-mri; (ii) clarity of tumor-to-normal tissue
interface; (iii) veracity of contrast enhancement in tumor invasion risk areas; and (iv)
efﬁcacy in primary tumor staging."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"(i) distinguishability between ce-mri and vce-mri. to evaluate the reality of vce-
mri, oncologists were invited to differentiate the synthetic patients (i.e., image
volumes that generated from synthetic vce-mri) from real patients (i.e., image
volumes that generated from real ce-mri). different from the previous studies
that utilized limited number (20-50 slices, axial view) of 2d image slices for reality
evaluation [9, 10], we used 3d volumes in this study to help oncologists visualize
the inter-slice adjacent information. the judgement results were recorded, and the
accuracy of each institution and the overall accuracy were calculated."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"the ji could be calculated by:
ji = |rce ∩ rvce|/|rce ∪ rvce|
(1)
where rce and rvce represents the set of risk areas that recorded from ce-mri and
corresponding vce-mri, respectively. ji measures similarity of two datasets, which
ranges from 0% to 100%. higher ji indicates more similar of the two sets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"to mimic the real clinical setting, contrast-free t1w, t2w mri and corresponding ct
546
w. li et al.
of each patient were imported into the eclipse system since sometimes t1w and t2w
mri will also be referenced during tumor delineation. due to both real patients and syn-
thetic patients were involved in delineation, to erase the delineation memory of the same
patient, we separated the patients to two datasets, each with the same number of patients,
both two datasets with mixed real patients and synthetic patients without overlaps (i.e.,
the ce-mri and vce-mri from the same patient are not in the same dataset).when ﬁn-
ished the ﬁrst dataset delineation, there was a one-month interval before the delineation
of the second dataset. [22] of the gtvs delineated from real
patients and corresponding synthetic patients were calculated to evaluate the accuracy
of delineated contours."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"hd is a metric to measure the maximum distance
between two contours. (3)
where d(x, cvce) and d(y, cce) represent the distance from point x in contour cce to
contour cvce and the distance from point y in contour cvce to contour cce.
3
results and discussion
3.1
image quality of vce-mri
table 2 summarizes the results of the four vce-mri quality evaluation metrics, includ-
ing: (i) distinguishability between ce-mri and vce-mri; (ii) clarity of tumor-to-
normal tissue interface; (iii) veracity of contrast enhancement in tumor invasion risk
areas; and (iv) efﬁcacy in primary tumor staging. (i) distinguishability between ce-mri and vce-mri."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"the overall
ji score between the recorded tumor invasion risk areas from ce-mri and vce-
mri was 74.06%. the average ji obtained from institution-1, institution-2, and
institution-3 dataset were similar with a result of 71.54%, 74.78% and 75.85%,
respectively. in total, 126 risk areas were recorded from the ce-mri for all of the
evaluation patients, while 10 (7.94%) false positive high risk invasion areas and 9
(7.14%) false negative high risk invasion areas were recorded from vce-mri.
(iv) efﬁcacy in primary tumor staging."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"a t-staging accuracy of 86.67% was obtained
using vce-mri. 13 patient pairs obtained the same staging results. for the
institution-2 data, all synthetic patients observed the same stages as real patients. for the two t-stage disagreement patients, one synthetic patient was staged as phase
iv while the corresponding real patient was staged as phase iii, the other synthetic
patient was staged as i while corresponding real patient was staged as phase iii.
table 2. image quality evaluation results of vce-mri: (a) distinguishability between ce-mri
and vce-mri; (b) clarity of tumor-to-normal tissue interface; (c) veracity of contrast enhance-
ment in risk areas; and (d) t-staging."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_51.pdf,"compared to ce-mri,
t1w mri has the similar tumor shape but with indistinguishable tumor-to-normal tissue
interface, while t2w mri shows superior lesion contrast but with smaller tumor volume
(yellow boxes). the deep learning model integrated the complementary information of
t1w mri and t2w mri, and successfully synthesized vce-mri with similar contrast
and tumor volume as ce-mri, with no obvious contrast differences in tumor regions,
as shown in difference map between ce-mri and vce-mri.
fig. 1. illustration of the synthetic vce-mri.
3.2
primary gtv delineation
the average dsc and hd between the cce and cvce was 0.762 (0.673–0.859) with a
median of 0.774, and 1.932 mm (0.763 mm–2.974 mm) with a median of 1.913 mm,
respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf,"given a non-contrast ct scan, cancer screening
is a binary classiﬁcation with two classes as l = {0, 1}, where 0 stands
for“normal” and 1 for“gc” (gastric cancer). the entire dataset is denoted by
s = {(xi, yi, pi)|i = 1, 2, · · · , n}, where xi is the i-th non-contrast ct vol-
ume, with yi being the voxel-wise label map of the same size as xi and k
channels. here, k = 3 represents the background, stomach, and gc tumor."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf,"pi ∈ l is the class label of the image, conﬁrmed by pathology, radiology, or
clinical records. in the testing phase, only xi is given, and our goal is to predict
a class label for xi.
knowledge transfer from contrast-enhanced to non-contrast ct. to
address diﬃculties with tumor annotation on non-contrast cts, the radiologists
start by annotating a voxel-wise tumor mask on the contrast-enhanced ct,
referring to clinical and endoscopy reports as needed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf,"we ﬁrst train a
unet [8,18] to segment the stomach and tumor regions using the masks from the
previous step. this unet considers local information and can only extract stom-
ach rois well during testing. however, local textures are inadequate for accurate
gastric tumor detection on non-contrast cts, so we need a network of both local
sensitivity to textures and global awareness of the organ-tumor morphology."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf,"the overall training objective is formulated as,
l = lseg(z, y) + lcls(ˆp, p),
(3)
where the segmentation loss lseg(·, ·) is a combination of dice and cross entropy
losses, and the classiﬁcation loss lcls(·, ·) is cross entropy loss. 4
experiments
4.1
experimental setup
dataset and ground truth. our study analyzed a dataset of ct scans col-
lected from guangdong province people’s hospital between years 2018 and 2020,
with 2,139 patients consisting of 787 gastric cancer and 1,352 normal cases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf,"we
used the latest patients in the second half of 2020 as a hold-out test set, result-
ing in a training set of 687 gastric cancer and 1,204 normal cases, and a test
set of 100 gastric cancer and 148 normal cases. we randomly selected 20% of
the training data as an internal validation set. to further evaluate speciﬁcity
in a larger population, we collected an external test set of 903 normal cases
from shengjing hospital."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf,"the patch size used during train-
ing and inference is (192, 224, 40) voxel. we followed [8] to augment data. we
trained the model with radam using a learning rate of 10−4 and a (backbone)
learning rate multiplier of 0.1 for 1000 epochs, with a frozen backbone of the pre-
trained nnunet [8] for the ﬁrst 50 epochs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf,"tumor-level localization evaluates how segmented
masks overlap with the ground-truth cancer (dice > 0.01 for correct detection). miss-t:
missing of t stage information. a reader
study was conducted with two experienced radiologists, one from guangdong
province people’s hospital with 20 years of experience and the other from the
first aﬃliated hospital of zhejiang university with 9 years of experience in
gastric imaging."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf,"the readers were given 248 non-contrast ct scans from the
test set and asked to provide a binary decision for each scan, indicating whether
the scan showed gastric cancer. no patient information or records were provided
to the readers. readers were informed that the dataset might contain more tumor
cases than the standard prevalence observed in screening, but the proportion of
case types was not disclosed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf,"the third
baseline (denoted as “nnunet-joint”) integrates a cnn classiﬁcation head into
unet [8] and trained end-to-end. we obtain the 95% conﬁdence interval of auc,
sensitivity, and speciﬁcity values from 1000 bootstrap replicas of the test dataset
for statistical analysis. for statistical signiﬁcance, we conduct a delong test
between two aucs (ours vs. compared method) and a permutation test between
two sensitivities or speciﬁcities (ours vs. compared method and radiologists)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_15.pdf,"our method outperforms three base-
lines (table 1) in all metrics, particularly in auc and sensitivity. the advantage
of our approach is that it captures the local and global information simultane-
ously in virtue of the unique architecture of mask transformer. it also extracts
high-level semantics from cluster representations, making it suitable for classiﬁ-
cation and facilitating a holistic decision-making process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"furthermore, these clusters were highly
interpretable to ophthalmologists who conﬁrmed that many of the clus-
ters represent dynamics that have previously been linked to the progres-
sion of amd, even though they are currently not included in any clinical
grading system. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_68.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43990-2_68
clustering disease trajectories for temporal biomarker proposal in amd
725
keywords: contrastive learning · biomarker discovery · clustering ·
disease trajectories · age-related macular degeneration
1
introduction
age-related macular degeneration (amd) is the leading cause of blindness in the
elderly, aﬀecting nearly 200 million people worldwide [24]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"[12]. clinicians
currently diagnose amd, and stratify patients, using biomarkers derived from
optical coherence tomography (oct), which provides high-resolution images of
fig. 1. our method ﬁnds common patterns of disease progression in datasets of lon-
gitudinal images. we partition time series into sub-trajectories before introducing a
clinically motivated distance function to cluster the sub-trajectories in feature space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"however, the widely adopted amd grading system [7,13], which
coarsely groups patients into broad categories for early and intermediate amd,
only has limited prognostic value for late amd. clinicians suspect that this
is due to the grading system’s reliance on static biomarkers that are unable
to capture temporal dynamics which contain critical information for assessing
progression risk. in their search for new biomarkers, clinicians have annotated known biomark-
ers in longitudinal datasets that monitor patients over time and mapped them
against disease progression [2,16,19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"however, these approaches neglect
temporal relationships between images and the obtained biomarkers are by def-
inition static and cannot capture the dynamic nature of the disease. our contribution: in this work, we present a method to automatically propose
biomarkers that capture temporal dynamics of disease progression in longitudi-
nal datasets (see fig. 1). at the core of our method is the novel strategy to
represent patient time series as trajectories in a latent feature space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"[21] ﬁnd four distinct spatiotemporal trajectories
for tau pathology in the brain. however, these neural networks are prone to overﬁt to their speciﬁc
task and lose semantic information regarding the disease. contrastive methods
[3,8,26] encode invariance to a set of image transformations, which are uncorre-
lated with disease features, resulting in a more expressive feature space.
however, all aforementioned methods group single images acquired at one point
in time, and in doing so neglect temporal dynamics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"[5].
3
materials and methods
3.1
oct image datasets
we use two retinal oct datasets curated in the scope of the pinnacle study
[20]. we ﬁrst design and test our method on a development dataset, which was
collected from the southampton eye unit. afterwards, we test our method on
a second independent unseen dataset, which was obtained from moorﬁelds eye
hospital."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"all images were acquired using topcon 3d oct devices (topcon cor-
poration, tokyo, japan). after strict quality control, the development dataset
consists of 46,496 scans of 6,236 eyes from 3,456 patients. eyes were scanned 7.7
times over 1.9 years on average at irregular time intervals."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"[13] and healthy cases with no
visible biomarkers. visual acuity scores, which measured the patient’s functional
quality of vision using a logmar chart, are available at 83,964 time points. = 2 − 2
⟨f(x), f ′(x)⟩
||f(x)||2 · ||f ′(x)||2
(1)
where the output of the momentum updated ‘teacher’ network f ′ is passed
through a stop-gradient, so that only the student network f is updated."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"as
several of the contrastive transformations designed for natural images are inap-
plicable to medical images, such as solarisation, colour shift and greyscale, we
use the set tailored for retinal oct images by holland et al. [9]. models were
trained on the entire dataset for 120,000 steps using the adam optimiser with a
momentum of 0.9, weight decay of 1.5·10−6 and a learning rate of 5·10−4. after
training f, we ﬁrst remove the ﬁnal linear layer before projecting all labelled
images to the feature space of 2048 dimensions.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"2. illustration of sub-trajectory distance functions which each encode temporal
criteria for similarity (see eq. 4). we illustrate clusters assignments, denoted by colour,
resulting from three combinations of φ and λ.
3.3
extracting sub-trajectories via partitioning
naively clustering whole time series of patients ignores two characteristics of
longitudinal data. firstly, individual time series are not directly comparable as
patients enter and leave the study at diﬀerent stages of their overall progression."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"730
r. holland et al.
fig. 3. we show four clusters from the development dataset (left half) and the equiv-
alent clusters in the unseen dataset (right half). ophthalmologists identiﬁed clusters
capturing the same progression dynamics in both datasets, providing clinical interpre-
tations (underlined)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"two teams of two ophthalmologists then review
20 sub-trajectories from distinct patients in each cluster, interpreting and sum-
marising any consistently observed temporal dynamics. next, using the same
hyperparameters we apply the method directly to the unseen dataset. the oph-
thalmologists then review these clusters and conﬁrm whether they capture the
clustering disease trajectories for temporal biomarker proposal in amd
731
same temporal biomarkers observed in the development dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"sub-trajectory clusters were comparable to the established clinical grading
systems for amd in predicting future disease, shown by reduced mae in years for
late amd, cnv and crora and mae in logmar for visual acuity. achiev-
ing the same cluster quality with smaller values of φ required many more clusters
in order to encode all combinations of possible start and end disease states. the
expert ophthalmologists remarked that many of the identiﬁed clusters capture
732
r. holland et al.
dynamics that have already been linked to the progression of amd, even though
they are not currently included in any clinical grading system."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"using the same
hyperparameters our method generalised to the unseen dataset which yielded
clusters with equivalent dynamics and quality (see fig. ophthalmologists identiﬁed clusters capturing the same variants of temporal
progression in both datasets. they named these as ‘rapid growth of drusen pig-
ment epithelial detachments (ped)’, ‘regression of drusen ped’, ‘development
of subretinal ﬂuid’, ‘development of intraretinal ﬂuid’, ‘development of hyper-
transmission’ and ‘stable state’ (no signs of progression at each disease state)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"5
discussion and conclusion
motivated to improve inadequate grading systems for amd that do not incor-
porate temporal dynamics we developed a method to automatically propose
biomarkers that are time-dependent, interpretable, and predictive of conversion
to late-stage amd. we applied our method to two large longitudinal datasets,
cataloguing 3,218 total years of disease progression. the found time-dependent
clusters were subsequently interpreted by four ophthalmologists."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_68.pdf,"we will also use the full volumetric image to model progression
dynamics outside the macular. as late stage patients were overrepresented in our
datasets, we also intend to apply our method to datasets with greater numbers
of patients progressing from earlier disease stages. ultimately, we envision that
proposals from our method may inform the next generation of grading systems
for amd that incorporate the temporal dimension intrinsic to this dynamic dis-
ease."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"the presence of corrupted labels is a common problem in the
medical image datasets due to the diﬃculty of annotation. meanwhile,
corrupted labels might signiﬁcantly deteriorate the performance of deep
neural networks (dnns), which have been widely applied to medical
image analysis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"speciﬁcally, the
proposed framework consists of two modules, i.e., noise detector and
noise cleaner. the noise detector designs a cnn-based model to distin-
guish corrupted labels from all samples, while the noise cleaner investi-
gates class-based gcns to correct the detected corrupted labels. more-
over, we design a new bi-level optimization algorithm to optimize our pro-
posed objective function."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"source codes of the proposed
method are available on https://github.com/shannak-chen/cnlc. keywords: corrupted labels · label correction · cnn · gcn
1
introduction
the success of deep neural networks (dnns) mainly depends on the large num-
ber of samples and the high-quality labels. however, either of them is very
diﬃcult to be obtained for conducting medical image analysis with dnns."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"moreover, sample annota-
tion needs expensive cost. hence, correcting corrupted labels might be one of
eﬀective solutions to solve the issues of high-quality labels. numerous works have been proposed to tackle the issue of corrupted labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"the architecture of the proposed cnlc framework consists of two modules,
i.e., noise detector and noise cleaner. the noise detector outputs the embedding of all
training samples and classiﬁes the training samples of each class into three subgroups,
including clean samples, uncertain samples and corrupted samples. the noise cleaner
constructs a gcn for each class to correct the labels of both corrupted samples and a
subset of uncertain samples for all classes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"divided into two categories, i.e., robustness-based methods [12,17] and label cor-
rection methods [14,22]. robustness-based methods are designed to utilize vari-
ous techniques, such as dropout, augmentation and loss regularization, to avoid
the adverse impact of corrupted labels, thereby outputting a robust model. label
correction methods are proposed to ﬁrst detect corrupted labels and then correct
them."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"[5] ﬁrst regards the outputs of
dnn as the class probability of the training samples and then changes the labels
of samples with low class probability. label correction methods are signiﬁcant
for disease diagnosis, because physicians can double check the probably misla-
beled samples to improve diagnosis accuracy. however, current label correction
methods still have limitations to be addressed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"first, they cannot detect and cor-
rect all corrupted labels, and meanwhile they usually fail to consider boosting
the robustness of the model itself, so that the eﬀectiveness of dnns is possibly
degraded. second, existing label correction methods often ignore to take into
account the relationship among the samples so that inﬂuencing the eﬀectiveness
of label correction. to address the aforementioned issues, in this paper, we propose a new
co-assistant framework, namely co-assistant networks for label correction
(cnlc) (shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"1), which consists of two modules, i.e., noise detector
and noise cleaner. speciﬁcally, the noise detector ﬁrst adopts a convolutional
neural network (cnn [6,20]) to predict the class probability of samples, and
then the loss is used to partition all the training samples for each class into
three subgroups, i.e., clean samples, uncertain samples and corrupted samples. moreover, we design a robust loss (i.e., a resistance loss) into the cnn frame-
work to avoid model overﬁtting on corrupted labels and thus exploring the ﬁrst
issue in previous label correction methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"the noise cleaner constructs a graph
convolutional network (gcn [18,19]) model for each class to correct the cor-
rupted labels. during the process of noise cleaner, we consider the relationship
co-assistant networks for label correction
161
among samples (i.e., the local topology structure preservation by gcn) to touch
the second issue in previous methods. in particular, our proposed cnlc itera-
tively updates the noise detector and the noise cleaner, which results in a bi-level
optimization problem [4,10]
compared to previous methods, the contributions of our method is two-fold."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"first, we propose a new label correction method (i.e., a co-assistant framework)
to boost the model robustness for medical image analysis by two sequential
modules. either of them adaptively adjusts the other, and thus guaranteeing to
output a robust label correction model. second, two sequential modules in our
framework results in a bi-level optimization problem."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"we thus design a bi-level
optimization algorithm to solve our proposed objective function. 2
methodology
in this section, our proposed method ﬁrst designs a noise detector to discriminate
corrupted samples from all samples, and then investigates a noise cleaner to
correct the detected corrupted labels.
2.1
noise detector
noise detector is used to distinguish corrupted samples from clean samples. the
prevailing detection method is designed to ﬁrst calculate the loss of dnns on all
training samples and then distinguish corrupted samples from clean ones based
on their losses."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"speciﬁcally, the samples with small losses are regarded as clean
samples while the samples with large losses are regarded as corrupted samples. diﬀerent from previous literature [6,7], our noise detector involves two steps,
i.e., cnn and label partition, to partition all training samples for each class into
three subgroups, i.e., clean samples, uncertain samples and corrupted samples. speciﬁcally, we ﬁrst employ cnn with the cross-entropy loss as the backbone
to obtain the loss of all training samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"(1), the ﬁrst term is the cross-entropy
loss. the second term is the resistance loss which is proposed to smooth the
update of model parameters so that preventing model overﬁtting on corrupted
labels to some extent [12].
in label partition, based on the resistant loss in eq. (1), the training samples
for each class are divided into three subgroups, i.e., clean samples, uncertain
162
x. chen et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"samples, and corrupted samples. speciﬁcally, the samples with n1 smallest losses
are regarded as clean samples and the samples with n1 largest loss values are
regarded as corrupted samples, where n1 is experimentally set as 5.0% of all
training sample for each class. the rest of the training samples for each class are
regarded as uncertain samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"in noise detector, our goal is to identify the real clean samples and real cor-
rupted samples, which are corresponded to set as positive samples and negative
samples in noise cleaner. if we select a large number of either clean samples or
corrupted samples (e.g., larger than 5.0% of all training samples), they may con-
tain false positive samples or false negative samples, so that the eﬀectiveness of
the noise cleaner will be inﬂuenced. as a result, our noise detector partitions all
training samples for each class into three subgroups, including a small proportion
of clean samples and corrupted samples, as well as uncertain samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"2.2
noise cleaner
noise cleaner is designed to correct labels of samples with corrupted labels. [15]) to correct the
corrupted labels. first, these methods ignore to take into account the relation-
ship among the samples, such as local topology structure preservation, i.e., one
of popular techniques in computer vision and machine learning, which ensures
that nearby samples have similar labels and dissimilar samples have diﬀerent
labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"this indicates that it is necessary to preserve the local topology structure
of samples within the same class. second, in noise detector, we only select a
small proportion of clean samples and corrupted samples for the construction of
noise cleaner. limited number of samples cannot guarantee to build robust noise
cleaner."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"in this paper, we address the above issues by employing semi-supervised
learning, i.e., a gcn for each class, which keeps the local topology structure of
samples on both labeled samples and unlabeled samples. speciﬁcally, our noise
cleaner includes three components, i.e., noise rate estimation, class-based gcns,
and corrupted label correction. co-assistant networks for label correction
163
the inputs of each class-based gcn include labeled samples and unlabeled
samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"as shown in fig. 2, we observe that the mean
value of gaussian model for corrupted samples is greater than that of gaussian
model for clean samples. thus, the gaussian model with a large mean value is
probably the curve of corrupted labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"mi,2
0,
otherwise
. (2)
hence, the noise rate r of training samples is calculated by:
r =
n
i=1 vi
n
,
(3)
where n represents the total number of samples in training dataset. supposing
the number of samples in the c-th class is sc, the number of uncertain samples
of each class is sc × r − n1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"given 2 × n1 labeled samples and n × r − n1 unlabeled samples, the class-
based gcn for each class conducts semi-supervised learning to predict n × r
samples, including n × r − n1 unlabeled samples and n1 corrupted samples for
this class. [8].
164
x. chen et al.
in corrupted label correction, given c well-trained gcns and the similarity
scores on each class for a subset of uncertain samples and all corrupted samples,
their labels can be determined by:
˜yi = argmax
0≤c≤c−1
(qic) . (6)
2.3
objective function
the optimization of the noise detector is associated with the corrupted label set
˜y, which is determined by noise cleaner."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"similarly, the embedding of all samples
e is an essential input of the noise cleaner, which is generated by the noise
detector. as the optimizations of two modules are nested, the objective function
of our proposed method is the following bi-level optimization problem:
⎧
⎨
⎩
minθ lr

f t (x; θ) , f t−1 (x; θ) , ˜y

,
minωc lbce (gt
c (ac, ec; ωc) , zc) + lmse

gt
c (ac, ec; ωc) , gt−1
c
(ac, ec; ωc)

,
(7)
where f t(x; θ) denotes the output of the upper-level (i.e., the noise detector)
in the t-th epoch, ac and ec represent the adjacency matrix and the feature
matrix of class c, zc are labels of labeled samples in class c, gt
c(ac, ec; ωc) and
gt−1
c
(ac, ec; ωc) denote the output of gcn model for class c at the t-th and
t-1-th epochs, respectively. in this paper, we construct a bi-level optimization algorithm to search optimal
network parameters of the above objective function."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"we randomly select
8,574 images for training and the rest of images for testing. in particular, the
random selection in our experiments guarantees that three datasets (i.e., the
training set, the testing set, and the whole set) have the same ratio for each
co-assistant networks for label correction
165
table 1. the classiﬁcation results (average ± std) on three datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"we evaluate
the eﬀectiveness of all methods in terms of four evaluation metrics, i.e., classiﬁ-
cation accuracy (acc), speciﬁcity (spe), sensitivity (sen) and area under the
roc curve (auc). 3.2
results and analysis
table 1 presents the classiﬁcation results of all methods on three datasets. due
to the space limitation, we present the results at ϵ = 0.0 of all methods in the
supplemental materials."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"for example, our method on average improves by 2.4% and 15.3%,
respectively, compared to the best comparison method (i.e., ct) and the worst
comparison method (i.e., ce), on all cases. this might be because our proposed
method not only utilizes a robust method to train a cnn for distinguishing
corrupted labels from clean labels, but also corrects them by considering their
relationship among the samples within the same class. second, all methods out-
perform the fundamental baseline (i.e., ce) on all cases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"= 0.2 and
ϵ = 0.4, respectively, on isic. the reason is that the cross-entropy loss easily
results in the overﬁtting issue on corrupted labels. 3.3
ablation study
to verify the eﬀectiveness of the noise cleaner, we compare our method with
the following comparison methods: 1) w/o nc: without noise cleaner, and 2)
mlp: replace gcn with multi-layer perceptron, i.e., without considering the
relationship among samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"for example,
cnlc improves by 1.0% and 2.3%, respectively, compared to cnlc-rl, in terms
of four evaluation metrics at ϵ = 0.2 and ϵ = 0.4. the reason is that the robustness
loss can prevent the model from overﬁtting on corrupted labels, and thus boosting
the model robustness. this veriﬁes the eﬀectiveness of the resistance loss deﬁned
in eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_16.pdf,"(1) for medical image analysis, which has been theoretically and experimen-
tally veriﬁed in the application of natural images [12].
co-assistant networks for label correction
167
4
conclusion
in this paper, we proposed a novel co-assistant framework, to solve the prob-
lem of dnns with corrupted labels for medical image analysis. experiments
on three medical image datasets demonstrate the eﬀectiveness of the proposed
framework. although our method has achieved promising performance, its accu-
racy might be further boosted by using more powerful feature extractors, like
pre-train models on large-scale public datasets or some self-supervised methods,
e.g., contrastive learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"machine learning based computer-aided diagnosis (cad)
aims to assist clinicians in the pathological diagnosis process. while
dealing with video pathological diagnosis such as colonoscopy polyp
detection, the recent sota method employs weakly-supervised video
anomaly detection (wvad) in the multiple instance learning (mil)
scenarios to concern the temporal correlation within data and to formu-
late the concept of the interest disease simultaneously. such a mil-based
wvad method leverages video-level annotations to detect frame-level
diseases and shows promising results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"the former is used to
learn atoms for representing normal features, and the latter is used to
encourage our model to gain robust disease detection. we demonstrate
that our cfd network is achieving new sota performance on the exist-
ing polyp dataset and the introduced panda-mil dataset. our dataset
are available at https://github.com/jhih-ciang/panda-mil."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"with the assistance of cad
techniques, the clinicians merely need to check the possible pathological regions
narrowed down by computer-aided diagnosis method, signiﬁcantly reducing the
entire diagnosis time. with the recent success of deep learning, researchers are
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 25. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp. 252–261, 2023."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"https://doi.org/10.1007/978-3-031-43904-9_25
contrastive feature decoupling for weakly-supervised disease detection
253
able to raise the reliability of cad methods and assist clinicians in diagnosing
more complex clinical tasks. however, a reliable machine learning-based cad
method usually relies on the supervision of abundant annotated training data. yet diseased pathological data are rare and diverse, and acquiring reliable patho-
logical annotations are labor-intensive and expertise-required."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"as a result, the
diﬃculty of data collection restricts the development of the supervised cad. due to the diﬃculty of acquiring the abundant annotated training data, the
current sota method, i.e., csm [14], proposes a mil-based wvad manner to
speciﬁcally tackle one speciﬁc disease detection task, i.e., colorectal cancer diag-
nosis via colonoscopy. considering the case of colonoscopy, the csm’s anomaly
detection setting is used to handle the rare and diverse diseased pathological
data by commonly assuming that only video-level annotations are available for
training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"furthermore, its video setting concerns the temporal correlation within
data. the setting of such mil-based weakly supervision prevents the need for
abundant annotated training data by assuming that merely the video-level anno-
tations, including normal and diseased ones, are available for training. similar to the previous mil-based wvad methods [4,13,14], our model
assumes all training snippets (consecutive video frames) within a non-diseased
video are all normal snippets, yet each diseased video has at least one abnor-
mal snippet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"consequently, we are able to decouple each snippet as normal
ingredients (reconstructed parts) and abnormal ingredients (residual parts) by
leveraging the memory bank. with the decoupled snippet-level feature ingredi-
ents, our cfd employs both the normal and abnormal feature ingredients via a
contrastive learning paradigm to concurrently optimize video-level and snippet-
level disease scores for pursuing more accurate detection.
to assess the proposed contrastive feature decoupling network, we conduct
experiments on two datasets, i.e., polyp and panda-mil. the main contribu-
tions are summarized as follows.
– our contrastive feature decoupling network learns a memory bank to learn
normal atoms for decoupling each snippet as normal and diseased feature
ingredients as opposite contrastive learning samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"the ablation study shows the decoupled
diseased ingredients enable accurate disease detection, and the accompanied
contrastive learning paradigm provides further improvement. – we introduce the new biomedical imaging dataset for prostate cancer detec-
tion, i.e., panda-mil. the dataset is organized to ﬁt the mil-based wvad
task, including video-level annotations and video-wise format data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"for example, li et al. [8] established a large-scale attention-
based database and designed a specialized model using retinal fundus images for
detecting glaucoma. windsor et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"unlike pre-
vious methods of handling one speciﬁc pathological modality, we simultaneously
address disease detection across pathological modalities of colonoscopy videos
and prostate tissue biopsies using our contrastive feature decoupling network. 2.2
contrastive learning
the characteristics of self-supervised learning are deﬁning the proxy objective
or addressing pretext tasks using pseudo labels for the unlabeled instances. one popular branch is contrastive learning which shows a remarkable ability to
obtain the desired semantic representation from various perspectives."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,", t
is the number of instances, and c represents the instance-level feature dimension. we then collect all normal instance-level features f ∈ r1×c from d0 to learn
the memory bank m by using the dictionary learning technique [7] via
argmin
m,{wt}

b∈d0
t
t=1(∥f t − mwt∥2 + λ∥wt∥0) ,
(1)
where d0 is the normal sub-dataset collected from the training split, wt is the
learned weights within the memory bank learning process, and λ is a hyperpa-
rameter to constrain the memory bank sparsity. 3.2
contrastive feature decoupling
with the learned normal instance features stored in the memory bank m, we
are able to reconstruct a normal version for any given bag-level feature f."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"notice that (8) is simpliﬁed for the sake of clarity. a complete objec-
tive should consider the symmetric form by switching d1 and d0 in (8).
4
experiments
4.1
dataset and metric
we evaluate our model against sotas on the existing polyp [14] dataset and
the panda-mil dataset introduced in this work. we employ the same eval-
uation criteria as the previous work for a fair comparison."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"please refer to the
supplementary material for the statistics of the two datasets. this dataset collects colonoscopy videos from hyper-kvasir [1] and
ldpolypvideo [10]. its training split contains 163 videos of video-level anno-
tations, and the testing split includes 90 videos of frame-level annotations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"the larger values of both metrics mean better disease detection performance. wu et al.
table 1. comparison with auc and ap metrics on polyp and panda-mil datasets. [5], for a fair comparison."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"our method
is trained using adam optimizer with the learning rate of 0.001, batch size 32,
and 200 epochs. each bag/video is encoded into t = 32 snippets among both
datasets via linear interpolation. 4.3
comparison results
table 1 shows the compared results of our cfd model against recent wvad
methods [4,13,14,18] for tackling the disease detection task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"the results in
table 1 demonstrate that our cfd consistently outperforms all the other meth-
ods on two datasets. precisely, our model achieves the new sota by 1.1%
auc and 1.5% ap improvements on the polyp dataset and 1.09% auc and
2.45% ap improvements on the panda-mil dataset. please refer to the sup-
plementary material for the completed results, including more wvad methods
[12,16,20,24].
figure 2 visualizes one disease detection result of our cfd model on the
panda-mil dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"4.4
ablation study
we analysis on why the cfd network performs better than other methods listed
in table 1 by ablating the contributed components in cfd. the ablation study in
table 2 is conducted on the panda-mil dataset to evaluate the eﬀectiveness of
the memory bank and loss functions in our model. row one in table 2 indicates
our cfd model without regularization, yet it has shown better auc values
than other methods besides the s3r."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_25.pdf,"contrastive feature decoupling for weakly-supervised disease detection
259
fig. 2. qualitative results of our cfd model on one testing prostate tissue biopsy of
the panda-mil dataset. our disease detection results are close to the ground-truths."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"in this work, we propose a few-shot colorectal tissue image
generation method for addressing the scarcity of histopathological train-
ing data for rare cancer tissues. our few-shot generation method, named
xm-gan, takes one base and a pair of reference tissue images as input
and generates high-quality yet diverse images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"within our xm-gan, a
novel controllable fusion block densely aggregates local regions of refer-
ence images based on their similarity to those in the base image, resulting
in locally consistent features. to the best of our knowledge, we are the ﬁrst
to investigate few-shot generation in colorectal tissue images. we evaluate
our few-shot colorectral tissue image generation by performing extensive
qualitative, quantitative and subject specialist (pathologist) based evalu-
ations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"speciﬁcally, in specialist-based evaluation, pathologists could dif-
ferentiate between our xm-gan generated tissue images and real images
only 55% time. moreover, we utilize these generated images as data aug-
mentation to address the few-shot tissue image classiﬁcation task, achiev-
ing a gain of 4.4% in terms of mean accuracy over the vanilla few-shot clas-
siﬁer. keywords: few-shot image generation · cross modulation
1
introduction
histopathological image analysis is an important step towards cancer diagno-
sis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"there-
fore, developing automatic and accurate histopathological image analysis meth-
ods that leverage recent progress in deep learning has received signiﬁcant atten-
tion in recent years. in this work, we investigate the problem of diagnosing
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_13.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43898-1_13
few-shot image generation for colorectal tissue classiﬁcation
129
colorectal cancer, which is one of the most common reason for cancer deaths
around the world and particularly in europe and america [23].
existing deep learning-based colorectal tissue classiﬁcation methods [18,21,22]
typically require large amounts of annotated histopathological training data for all
tissue types to be categorized."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"however, obtaining large amount of training data is
challenging, especially for rare cancer tissues. to this end, it is desirable to develop
a few-shot colorectal tissue classiﬁcation method, which can learn from seen tissue
classes having suﬃcient training data, and be able to transfer this knowledge to
unseen (novel) tissue classes having only a few exemplar training images. [6] have been utilized to syn-
thesize images, they typically need to be trained using large amount of real
images of the respective classes, which is not feasible in aforementioned few-shot
setting."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"moreover, we demonstrate the applicability of these generated
images for the challenging problem of fs colorectal tissue classiﬁcation. contributions: we propose a few-shot colorectal tissue image generation
framework, named xm-gan, which simultaneously focuses on generating high-
quality yet diverse images. within our tissue image generation framework, we
introduce a novel controllable fusion block (cfb) that enables a dense aggrega-
tion of local regions of the reference tissue images based on their congruence to
those in the base tissue image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"consequently, colorectal tissue images are generated with reduced artifacts. to further enhance the diversity and quality of the generated tissue images,
we introduce a mapping network along with a controllable cross-modulated layer
normalization (cln) within our cfb. our mapping network generates ‘meta-
weights’ that are a function of the global-level features of the reference tissue
image and the control parameters."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"in our subject specialist (pathologist) based evalua-
tion, pathologists could diﬀerentiate between our xm-gan generated colorec-
tral tissue images and real images only 55% time. furthermore, we evaluate the
eﬀectiveness of our generated tissue images by using them as data augmentation
during training of fs colorectal tissue image classiﬁer, leading to an absolute gain
of 4.4% in terms of mean classiﬁcation accuracy over the vanilla fs classiﬁer. 130
a. kumar et al.
2
related work
the ability of generative models [6,15] to ﬁt to a variety of data distributions
has enabled great strides of advancement in tasks, such as image generation [3,
12,13,19], and so on."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"in contrast, few-
shot (fs) image generation approaches [2,4,7,9,16] strive to generate natural
images from disjoint novel categories from the same domain as in the training. the transformation-based approach learns to perform generalized data
augmentations to generate intra-class images from a single conditional image. on
the other hand, optimization-based approaches typically utilize meta-learning
techniques to adapt to a diﬀerent image generation task by optimizing on a few
reference images from the novel domain."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"diﬀerent from these two paradigms
that are better suited for simple image generation task, fusion-based approaches
ﬁrst aggregate latent features of reference images and then employ a decoder to
generate same class images from these aggregated features. our approach: while the aforementioned works explore fs generation in nat-
ural images, to the best of our knowledge, we are the ﬁrst to investigate fs gener-
ation in colorectal tissue images. in this work, we look into multi-class colorectal
tissue analysis problem, with low and high-grade tumors included in the set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"[5,20] from all relevant local regions of the reference
tissue images at a global-receptive ﬁeld along with a controllable mechanism for
modulating the tissue image features by utilizing meta-weights computed from
the input reference tissue image features. as a result, this leads to high-quality
yet diverse colorectal tissue image generation in fs setting. 3
method
problem formulation: in our few-shot colorectal tissue image generation
framework, the goal is to generate diverse set of images from k input exam-
ples x of a unseen (novel) tissue classes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"as a result of this control-
lable feature modulation, the out-
put features fi enable the gen-
eration of tissue images that are
diverse yet aligned with the seman-
tics of the input tissue images. next, we introduce a controllable fea-
ture modulation mechanism in our cross-
transformer to further enhance the diversity
and quality of generated images. controllable feature modulation: the
standard cross-attention mechanism, described
above, computes locally consistent features
that generate images with reduced artifacts.
however, given the deterministic nature of the
cross-attention and the limited set of reference
images, simultaneously generating diverse and
high-quality images in the few-shot setting is
still a challenge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"controllable cross-modulated layer normalization (cln): our cln learns
sample-dependent modulation weights for normalizing features since it is desired
few-shot image generation for colorectal tissue classiﬁcation
133
to generate images that are similar to the few-shot samples. such a dynamic
modulation of features enables our framework to generate images of high-quality
and diversity. to this end, we utilize the meta-weights wi for computing the
modulation parameters λ and β in our layer normalization modules."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"a similar computation
is performed for β(wi). consequently, our proposed normalization mechanism
achieves a controllable modulation of the input features based on the reference
image inputs and enables enhanced diversity and quality in the generated images. the resulting features oi are then passed through a feed-forward network (ffn)
followed by another cln for preforming point-wise feature reﬁnement, as shown
in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"our xm-gan is then trained using the formulation: l = ladv + ηplp + ηcllcl,
where ηp and ηcl are hyperparameters for weighting the loss terms. inference: during inference, multiple high-quality and diverse images ˆx are
generated by varying the control parameter αi for a set of ﬁxed k-shot samples. while a base image xb and αi can be randomly selected, our framework enables
a user to have control over the generation based on the choice of αi values.
134
a. kumar et al.
4
experiments
we conduct experiments on human colorectal cancer dataset [14]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"3, we
present a qualitative comparison of our xm-gan with lofgan [7].
table 1. our xm-gan achieves consis-
tent gains in performance on both fid and
lpips scores, outperforming lofgan on
[14] dataset. [7]
85.9
0.44
ours: xm-gan
55.8
0.48
low-data classiﬁcation: here, we
evaluate the applicability of the tissue
images generated by our xm-gan as
a source of data augmentation for the
downstream task of low-data colorec-
tal tissue classiﬁcation for unseen cate-
gories."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"compared to the lofgan [7], our xm-gan achieves
absolute gains of 2.8%. few-shot image generation for colorectal tissue classiﬁcation
135
table 2. low-data image classi-
ﬁcation. the proposed xm-gan
achieves superior classiﬁcation per-
formance
compared
to
recently
introduced lofgan."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"in the middle: images
generated by lofgan. on the right: images generated by our xm-gan. compared
to lofgan, our xm-gan generates images that are high-quality yet diverse. best
viewed zoomed in."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_13.pdf,"4.2
ablation study
here, we present our ablation study to validate the merits of the proposed
contributions. table 3 shows the baseline comparison on the [14] dataset. our
baseline comprises an encoder, a standard cross-transformer with standard
layer normalization (ln) layers and a decoder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf,"second, we pro-
pose a new instance normalization method that is robust to the variation
in foreground-background ratios. we evaluate the proposed methods on
two h&e stained image datasets, named consep and cpm17, and two
ihc stained image datasets, called deepliif and bc-deepliif. exten-
sive experimental results justify the eﬀectiveness of our proposed darc
model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf,"keywords: domain generalization · nucleus segmentation · instance
normalization
1
introduction
automatic nucleus segmentation has captured wide research interests in recent
years due to its importance in pathological image analysis [1–4]. however, as
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 57. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf,"1, the variations in image modalities, staining protocols, scanner
types, and tissues signiﬁcantly aﬀect the appearance of nucleus images, result-
ing in notable gap between source and target domains [5–7]. if a number of
target domain samples are available before testing, one can adopt domain adap-
tation algorithms to transfer the knowledge learned from the source domain to
the target domain [8–10]. unfortunately, in real-world applications, it is usually
expensive and time-consuming to collect new training sets for the ever changing
target domains; moreover, extra computational cost is required, which is usually
unrealistic for the end users."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf,"in recent years, the research on domain generalization (dg) has attracted
wide attention. [12,13] and they can be roughly grouped into data augmentation-, representation
learning-, and optimization-based methods. the ﬁrst category of methods [14–17]
focus on the way to diversify training data styles and expect the enriched styles
cover those appeared in target domains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf,"then, the re-colored image is fed into darc again with ρ for ﬁnal prediction. for
simplicity, we only illustrate the data-ﬂow of the ﬁrst da-resblock in details.
instances. the area of each nucleus instance is obtained via subtraction between
the segmentation and contour prediction maps [1]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf,"however, de-colorization results in the loss of ﬁne-grained textures and may
harm the segmentation accuracy. [40] show
that semantic information can be reﬂected via the order of pixels according
to their gray value. [41] to
combine the semantic information in i with the color values in ir."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf,"details of the module t are included in the supplementary material. via rc, the original ﬁne-grained structure information from ig is recovered
in ir. in this way, the re-colored image is advantageous in two aspects."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf,"first,
the appearance diﬀerence between pathological images caused by the change in
scanners and staining protocols is eliminated. second, the re-colored image pre-
serves ﬁne-grained structure information, enabling precise instance segmentation
to be possible. 2.3
distribution-aware instance normalization
due to dramatic domain gaps, feature statistics may diﬀer signiﬁcantly between
domains [5–7], which means that feature statistics obtained from the source
domain may not apply to the target domain."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf,"in all experiments, the segmentation and contour detection
predictions are penalized using the binary cross entropy loss. darc: distribution-aware re-coloring model
597
table 2. comparisons in generalization performance on nucleus segmentation datasets.
results in each column are related to models trained on one domain and evaluated on
the other three unseen domains. methods marked by * are proposed in this paper."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_57.pdf,"[33] and dice scores. in
the experiments, models trained on one of the datasets will be evaluated on the
three unseen ones. to avoid the inﬂuence of the diﬀerent sample numbers of the
datasets, we calculate the average scores within each unseen domain respectively
and then average them across domains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"nevertheless, the
majority of such approaches overlook the multi-scale nature of the wsis;
the few existing hierarchical mil proposals simply ﬂatten the multi-
scale representations by concatenation or summation of features vectors,
neglecting the spatial structure of the wsi. our work aims to unleash the
full potential of pyramidal structured wsi; to do so, we propose a graph-
based multi-scale mil approach, termed das-mil, that exploits mes-
sage passing to let information ﬂows across multiple scales. by means of
a knowledge distillation schema, the alignment between the latent space
representation at diﬀerent resolutions is encouraged while preserving the
diversity in the informative content."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"the source code is
available at https://github.com/aimagelab/mil4wsi. [18], facilitating their preservation and
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 24.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14220, pp. https://doi.org/10.1007/978-3-031-43907-0_24
das-mil: distilling across scales for mil classiﬁcation of wsis
249
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"the nodes
of both graphs are later fused into a third one, respecting the rule “part of”. the
contextualized features are then passed to distinct attention-based mil modules that
extract bag labels. furthermore, a knowledge distillation mechanism encourages the
agreement between the predictions delivered by diﬀerent scales.
retrieval, but also introducing multiple challenges."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"on the one hand, annotat-
ing wsis requires strong medical expertise, is expensive, time-consuming, and
labels are usually provided at the slide or patient level. on the other hand,
feeding modern neural networks with the entire gigapixel image is not a feasible
approach, forcing to crop data into small patches and use them for training. this
process is usually performed considering a single resolution/scale among those
provided by the wsi image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"mil approaches consider the image slide as a bag composed of many
patches, called instances; afterwards, to provide a classiﬁcation score for the
entire bag, they weigh the instances through attention mechanisms and aggregate
them into a single representation. [15],
which have been proven to be more eﬀective than single-resolution [4,13,15,19].
however, to the best of our knowledge, none of the existing proposals leverage the
full potential of the wsi pyramidal structure. indeed, the ﬂat concatenation of
features [19] extracted at diﬀerent resolutions does not consider the substantial
diﬀerence in the informative content they provide."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"a proﬁcient learning app-
roach should instead consider the heterogeneity between global structures and
local cellular regions, thus allowing the information to ﬂow eﬀectively across the
image scales. to proﬁt from the multi-resolution structure of wsi, we propose a pyrami-
dal graph neural network (gnn) framework combined with (self) knowledge
distillation (kd), called das-mil (distilling across scales). a visual represen-
tation of the proposed approach is depicted in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"1. distinct gnns provide
contextualized features, which are fed to distinct attention-based mil modules
that compute bag-level predictions. through knowledge distillation, we encour-
250
g. bontempo et al.
age agreement across the predictions delivered at diﬀerent resolutions, while indi-
vidual scale features are learned in isolation to preserve the diversity in terms
of information content. by transferring knowledge across scales, we observe that
the classiﬁer self-improves as information ﬂows during training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"recently, self-supervised representa-
tion learning approaches have also employed such a schema: as an example, [5,9]
exploit kd to obtain an agreement between networks fed with diﬀerent views
of the same image. in [28], kd is used to transfer the knowledge between mil
tiers applied on diﬀerent subsamples bags. taking inspiration from [23] and [30],
our model applies (self) knowledge distillation between wsi scale resolutions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"das-mil: distilling across scales for mil classiﬁcation of wsis
251
3
method
our approach aims to promote the information ﬂow through the diﬀerent
employed resolutions. while existing works [19,20,25] take into account inter-
scales interactions by mostly leveraging trivial operations (such as concatenation
of related feature representations), we instead provide a novel technique that
builds upon: i) a gnn module based on message passing, which propagates
patches’ representation according to the natural structure of multi-resolutions
wsi; ii) a regulation term based on (self) knowledge distillation, which pins the
most eﬀective resolution to further guide the training of the other one(s). in the
following, we are delving into the details of our architecture."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"although we focus only on two resolutions at
time (i.e., m = 2) the approach can be extended to more scales. the representations yield by dino provide a detailed descrip-
tion of the local patterns in each patch; however, they retain poor knowledge of
the surrounding context. to grasp a global guess about the entire slide, we allow
patches to exchange local information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"252
g. bontempo et al.
scores ybag
1
, ybag
2
∈ r1×c where c equals the number of classes. notably, such
a module provides additional importance scores z1 ∈ rn1 and z2 ∈ rn2, which
quantiﬁes the importance of each original patch to the overall prediction.
aligning scales with (self) knowledge distillation. we have hence obtai-
ned two distinct sets of predictions for the two resolutions: namely, a bag-level
score (e.g., a tumor is either present or not) and a patch-level one (e.g., which
instances contribute the most to the target class)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"further than improving
the results of the lowest scale only, we expect its beneﬁts to propagate also to
the shared message-passing module, and so to the higher resolution. softmax(ybag
2
τ
)),
(1)
where kl stands for the kullback-leibler divergence and τ is a temperature
that lets secondary information emerge from the teaching signal. the second aligning term regards the instance scores."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"we encourage such a constraint
by minimizing the euclidean distance between the low-resolution criticality grid
map z1 and its subsampled counterpart computed by the high-resolution branch:
lcrit = ∥z1 − graphpooling(z2)∥2
2.
(2)
in the equation above, graphpooling identiﬁes a pooling layer applied over the
higher scale: to do so, it considers the relation “part of” between scales and then
averages the child nodes, hence allowing the comparison at the instance level. to sum up, the overall optimization problem is formulated
as a mixture of two objectives: the one requiring higher conditional likelihood
w.r.t. ground truth labels y and carried out through the cross-entropy loss
lce(·; y); the other one based on knowledge distillation:
min
θ
(1 − λ)lce(ybag
2
) + lce(ybag
1
) + λlkd + βlcrit,
(3)
where λ is a hyperparameter weighting the tradeoﬀ between the teaching signals
provided by labels and the higher resolution, while β balances the contributions
of the consistency regularization introduced in eq. das-mil: distilling across scales for mil classiﬁcation of wsis
253
4
experiments
wsis pre-processing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"[1] we adhere to the oﬃcial training/test sets. to produce the
fairest comparison with the single-scale state-of-the-art solution, the 270 remain-
ing wsis are split into training and validation in the proportion 9:1.
254
g. bontempo et al.
tcga lung dataset. it is available on the gdc data transfer portal and
comprises two subsets of cancer: lung adenocarcinoma (luad) and lung squa-
mous cell carcinoma (lusc), counting 541 and 513 wsis, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"such a statement holds not only for the lower resolution (as one could expect),
but also for the higher one, thus corroborating the claims we made in sect. 3 on
the bidirectional beneﬁts of knowledge distillation in our multi-scale architecture. das-mil: distilling across scales for mil classiﬁcation of wsis
255
table 4. comparison between scales."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"overall, the best results are obtained with 10× and 20×
input resolutions; the table also highlights that 5× magnitude is less eﬀective
and presents a worst discriminative capability. we ascribe it to the specimen-
level pixel size relevant for cancer diagnosis task; diﬀerent datasets/tasks may
beneﬁt from diﬀerent scale combinations. table 5. comparison between das-mil with and w/o (✗) the graph contextualization
mechanism, and the most recent graph-based multi-scale approach h2-mil, when using
diﬀerent resolutions as input (5× and 20×)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_24.pdf,"256
g. bontempo et al.
h2-mil exploits a global pooling layer (ihpool) that fulﬁls only the spatial
structure of patches: as a consequence, if non-tumor patches surround a tumor
patch, its contribution to the ﬁnal prediction is likely to be outweighed by the
ihpool module of h2-mil. diﬀerently, our approach is not restricted in such a
way, as it can dynamically route the information across the hierarchical structure
(also based on the connections with the critical instance)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"we propose a generic framework for
creating expressive cross-modal descriptors that enable fast deformable
global registration. we achieve this by approximating existing metrics
with a dot-product in the feature space of a small convolutional neural
network (cnn) which is inherently diﬀerentiable can be trained without
registered data. our method is several orders of magnitude faster than
local patch-based metrics and can be directly applied in clinical settings
by replacing the similarity measure with the proposed one."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"experiments
on three diﬀerent datasets demonstrate that our approach generalizes
well beyond the training data, yielding a broad capture range even on
unseen anatomies and modality pairs, without the need for specialized
retraining. we make our training code and data publicly available. keywords: image registration · multimodal · metric learning ·
diﬀerentiable · deformable registration
1
introduction
multimodal imaging has become increasingly popular in healthcare due to its
ability to provide complementary anatomical and functional information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"how-
ever, to fully exploit its beneﬁts, it is crucial to perform accurate and robust
registration of images acquired from diﬀerent modalities. multimodal image reg-
istration is a challenging task due to diﬀerences in image appearance, acquisition
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 72.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43999-5_72
762
m. ronchetti et al.
protocols, and physical properties of the modalities."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"[16] work well in monomodal settings, a more sophisticated
approach is needed when intensities cannot be directly correlated. historically,
a breakthrough in ct-mri registration was achieved by viola and wells, who
proposed mutual information [19]. essentially, it abstracts the problem to the
statistical concept of information theory and optimizes image-wide alignment
statistics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"more recently, multimodal registration has been approached using various
machine learning (ml) techniques. although these methods have demonstrated promising results,
they are anatomy-speciﬁc and require the identiﬁcation and labeling of structures
that are visible in both modalities. other approaches are trained using ground
truth registrations to directly predict the pose [9,12] or to establish keypoint
correspondences [1,11]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"this
deﬁnition encompasses ssd but also other more elaborate metrics like lc2 or
mind. the function w is typically used to reduce the impact of patches with
ambiguous content (e.g. with uniform intensities), or can be chosen to encode
prior information on the target application. the core idea of our method is to approximate the similarity metric s(p1, p2)
of two image patches with a dot product ⟨φ(p1), φ(p2)⟩ where φ(·) is a function
that extracts a feature vector, for instance in r16, from its input patch."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"in order to be consistent with
the original implementation of lc2 we use the same weighting function w based
on local patch variance. note that the network will be trained only once, on a
ﬁxed dataset that is fully independent of the datasets that will be used in the
evaluation (see sect. 4).
dataset. our neural network is trained using patches from the “gold atlas
- male pelvis - gentle radiotherapy” [14] dataset, which is comprised of 18
patients each with a ct, mr t1, and mr t2 volumes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"since our approach is unsupervised, we don’t make
use of the provided registration but leave the volumes in their standard dicom
orientation. as lc2 requires the usage of gradient magnitude in one of the
modalities, we randomly pick it from either ct or mr.
we would like to report that, initially, we also made use of a proprietary
dataset including us volumes. however, as our investigation progressed, we
observed that the incorporation of us data did not signiﬁcantly contribute to the
generalization capabilities of our model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"consequently, for the purpose of ensur-
ing reproducibility, all evaluations presented in this paper exclusively pertain to
the model trained solely on the public mr-ct dataset. patch sampling from unregistered datasets. [0, 1]; (4) pick the patch of f
with similarity score closest to t. running this procedure on our training data
results in a total of 510000 pairs of patches.
architecture and training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"the architecture
consists of ten layers and a total of 90,752 parameters, making it notably smaller
than many commonly utilized neural networks. augmentation on the training data is used to make the model as robust as
possible while leaving the target similarity unchanged. in particular, we apply
the same random rotation to both patches, randomly change the sign and apply
random linear transformation on the intensity values."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"the total training time on an nvidia rtx4090 gpu is 5 h,
and inference on a 2563 volume takes 70 ms. we make the training code and
preprocessed data openly available online1. 4
experiments and results
we present an evaluation of our approach across tasks involving diverse modali-
ties and anatomies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"to assess the eﬀectiveness of our method,
1 https://github.com/imfusiongmbh/disa-universal-multimodal-registration. 766
m. ronchetti et al.
table 1. results on registration of brain us-mr data from the resect challenge. fre is the average of ﬁducial errors in millimeters across all cases, while fre25,
fre50, and fre75 refer to the 25th, 50th, and 75th percentiles.
method
mode
avg."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"in particular, on abdominal us registration (sect. this dataset consists of
22 pairs of pre-operative brain mrs and intra-operative ultrasound volumes. the initial pose of the ultrasound volumes exhibits an orientation close to the
ground truth but can contain a signiﬁcant translation shift."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"in conclusion, our experiments demonstrate that the proposed
disa-lc2, combined with a simple optimization strategy, is capable of achieving
equivalent performance to manually tuned lc2. the dataset comprises 8 sets of mr and ct volumes, both depicting
the abdominal region of a single patient and exhibiting notable deformations. we
estimate dense deformation ﬁelds using the methodology outlined in [6] (without
inverse consistency) which ﬁrst estimates a discrete displacement using explicit
search and then iteratively enforces global smoothness."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"segmentation maps of
anatomical structures are used to measure the quality of the registration. in
particular, we compute the 25th, 50th, and 75th quantile of the dice similar-
ity coeﬃcient (dsc) and the 95th quantile of the hausdorﬀ distance (hd95)
between the registered label maps. we compare mind-scc and disa-lc2 used
with diﬀerent strides and followed by a downsampling operation that brings the
spacing of the descriptors volumes to 8 mm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_72.pdf,"similarity
search
converged cases w.r.t. initialization error
time (s)
num. eval. all 3d ultrasound data sets are accurately calibrated, with overall
system errors in the range of commercial ultrasound fusion options. between
4 and 9 landmark pairs (vessel bifurcations, liver gland borders, gall bladder,
kidney) were manually annotated by an expert."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"weakly supervised classiﬁcation of whole slide images (wsis) in dig-
ital pathology typically involves making slide-level predictions by aggregating
predictions from embeddings extracted from multiple individual tiles. however,
these embeddings can fail to capture valuable information contained within the
individual cells in each tile. here we describe an embedding extraction method
that combines tile-level embeddings with a cell-level embedding summary."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"for all tasks,
the new method outperformed embedding extraction methods that did not include
cell-level representations. using the publicly available herohe challenge data
set, the method achieved a state-of-the-art performance of 90% area under the
receiver operating characteristic curve. additionally, we present a novel model
explainability method that could identify cells associated with different classiﬁca-
tion groups, thus providing supplementary validation of the classiﬁcation model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"this deep learning approach has the potential to provide morphological insights
that may improve understanding of complex underlying tumor pathologies. for many cancers, clinically reliable genomic, molecular, or imaging biomarkers
supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2_75. © the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"current deep learning approaches
to wsi analysis typically operate at three different histopathological scales: whole slide-
level, region-level, and cell-level [4]. although cell-level analysis has the potential to
produce more detailed and explainable data, it can be limited by the unavailability
of sufﬁciently annotated training data. to overcome this problem, weakly supervised
and multiple instance learning (mil) based approaches have been applied to numer-
ous wsi classiﬁcation tasks [6–10]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"our
new method achieved better performance on wsi classiﬁcation tasks and had a greater
level of explainability than models that used only tile-level embeddings. 2
embedding extraction scheme
transfer learning using backbones pretrained on natural images is a common method
that addresses the challenge of using data sets that largely lack annotation. however,
using backbones pretrained on natural images is not optimal for classiﬁcation of clinical
images [11]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"extracted
tiles that contained artifacts were discarded (e.g., tiles that had an overlap of >10%
with background artifacts such blurred areas or pen markers). [15] that was trained on a
subset of data from one of the medical centers in the camelyon17 data set to ensure
homogeneity of staining [16].
to create the tile-level embeddings, we used the method proposed by [17] to sum-
marize the convolutional neural network (cnn) features with nonnegative matrix fac-
torization (nmf) for k = 2 factors. we observed that the feature activations within
the last layer of the network were not aligned with the cellular content."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"resolution: 0.25 µm/pixel.
2.2
cell-level embeddings
tiles extracted from wsis may contain different types of cells, as well as noncellular
tissue such as stroma and blood vessels and nonbiological features (e.g., glass). cell-
level embeddings may be able to extract useful information, based on the morphological
appearance of individual cells, that is valuable for downstream classiﬁcation tasks but
would otherwise be masked by more dominant features within tile-level embeddings. we extracted deep cell-level embeddings by ﬁrst detecting individual cellular bound-
aries using stardist [18] and extracting 32 × 32-pixel image crops centered around each
segmented nucleus to create cell-patch images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"higher
proportions of noncellular features in an image may cause the resultant embeddings to
be dominated by noncellular tissue features or other background features. therefore, to
limit the information used to create the cell-level embeddings to only cellular features, we
removed portions of the cell-patch images that were outside of the segmented nuclei by
setting their pixel values to black (rgb 0, 0, 0). finally, to prevent the size of individual
nuclei or amount of background in each cell-patch image from dominating over the cell-
level features, we modiﬁed the resnet50 global average pooling layer to only average
deep cellular embeddings: an explainable plug and play improvement
779
the features inside the boundary of the segmented nuclei, rather than averaging across
the whole output tensor from the cnn.
2.3
combined embeddings
to create a combined representation of the tile-level and cell-level embeddings, we
ﬁrst applied a nuclei segmentation network to each tile."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"in addition to the wsi classiﬁcation results presented in the next sections, we also
performed experiments to compare the ability of combined embeddings and tile-level
embeddings to predict nuclei-related features that were manually extracted from the
images and to identify tiles where nuclei had been ablated. the details and results of these
experiments are available in supplementary materials and provide further evidence of
the improved ability to capture cell-level information when using combined embeddings
compared with tile-level embeddings alone. 3
wsi classiﬁcation tasks
for each classiﬁcation task we compared different combinations of tile-level and cell-
level embeddings using a mil framework."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"2. the models were selected using
780
j. gildenblat et al. a validation set, that was a random sample of 20% of the training data. all training was
done using pytorch version 1.12.1 (pytorch.org) on 8 nvidia tesla v100 gpus with
cuda version 10.2.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"tile-level and cell-level embeddings are extracted in parallel and then concatenated embedding
vectors are passed through the mil model for the downstream task. ami equals the number of
cells in tile i.
3.1
data
we tested our feature representation method in several classiﬁcation tasks involving
wsis of h&e-stained histopathology slides. the number of slides per class for each
classiﬁcation task are shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"numbers in the bars
represent the number of wsis by classiﬁcation for each task. for breast cancer human epidermal growth factor receptor 2 (her2) prediction,
we used data from the herohe challenge data set [26]. to enable comparison with
previous results we used the same test data set that was used in the challenge [27]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"for
prediction of estrogen receptor (er) status, we used images from the tcga-breast
invasive carcinoma (tcga-brca) data set [28] for which the er status was known. deep cellular embeddings: an explainable plug and play improvement
781
for these two tasks we used artifact-free tiles from tumor regions detected with an
in-house tumor detection model.
for breast cancer metastasis detection in lymph node tissue, we used wsis of h&e-
stained healthy lymph node tissue and lymph node tissue with breast cancer metastases
from the publicly available camelyon16 challenge data set [16, 29]. all artifact-free
tissue tiles were used."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"all slides were h&e-stained
and scanned using ventana dp200 scanners at 40× magniﬁcation. ct1 was used for
training and testing the classiﬁer and ct2 was used only as an independent holdout
data set. for these data sets we used artifact-free tiles from regions annotated by expert
pathologists to contain tumor tissue."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"error bars represent 95% conﬁdence intervals computed by a 5000-sample
bias-corrected and accelerated bootstrap. in fact, for the her2 classiﬁcation task, combined embeddings obtained using the
xformer architecture achieved, to our knowledge, the best performance yet reported on
the herohe challenge data set (area under the receiver operating characteristic curve
[auc], 90%; f1 score, 82%). for coo classiﬁcation in dlbcl, not only did the combined embeddings achieve
better performance than the tile-level only embeddings with both the xformer and a-
mil architectures (fig. 5) on the ct1 test set and ct2 holdout data set, but they also
782
j. gildenblat et al.
had a signiﬁcant advantage versus tile-only level embeddings in respect of the additional
insights they provided through cell-level model explainability (sect. 4.1).
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_75.pdf,"visual example results. examples of our cellular explainability method applied to
weakly supervised tumor detection on wsis from the camelyon16 data set using a-
mil are shown in fig. 6. cells with positive attention gradients shifted the output towards
deep cellular embeddings: an explainable plug and play improvement
783
a classiﬁcation of tumor and are labeled green."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"this
paper presents a deep-learning approach to identify and characterize
tumor-associated stroma in multi-modal prostate histopathology slides. the model achieved an average testing auroc of 86.53% on a large
curated dataset with over 1.1 million stroma patches. our experimental
results indicate that stromal alterations are detectable in the presence of
prostate cancer and highlight the potential for tumor-associated stroma
to serve as a diagnostic biomarker in prostate cancer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"analyzing tumor-associated stroma in prostate cancer requires combining
whole-mount and biopsy histopathology slides. biopsy slides provide information
on the presence of pca, while whole-mount slides provide information on the
extent and distribution of pca, including more information on tumor-associated
stroma. combining the information from both modalities can provide a more
644
z. wang et al.
accurate understanding of the tumor microenvironment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"in this work, we explore
the ﬁeld eﬀect in prostate cancer by analyzing tumor-associated stroma in multi-
modal histopathological images. our main contributions can be summarized as
follows:
– to the best of our knowledge, we present the ﬁrst deep-learning approach
to characterize prostate tumor-associated stroma by integrating histological
image analysis from both whole-mount and biopsy slides. our research oﬀers
a promising computational framework for in-depth exploration of the ﬁeld
eﬀect and cancer progression in prostate cancer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"given the spatial nature of cancer
ﬁeld eﬀect and tumor microenvironment, our graph-based method oﬀers valu-
able insights into stroma region analysis. – we developed a comprehensive pipeline for constructing tumor-associated
stroma datasets across multiple data sources, and employed adversarial train-
ing and neighborhood consistency regularization techniques to learn robust
multimodal-invariant image representations. 2
method
2.1
stroma tissue segmentation
accurately analyzing tumor-associated stroma requires a critical pre-processing
step of segmenting stromal tissue from the background, including epithelial tis-
sue."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"2.2
stroma classiﬁcation with spatial patch graphs
to capture the spatial nature of ﬁeld eﬀect and analyze tumor-associated stroma,
modeling spatial relationships between stroma patches is essential. the spatial
relationship can reveal valuable information about the tumor microenvironment,
and neighboring stroma cells can undergo similar phenotypic changes in response
to cancer. therefore, we propose using a spatial patch graph to capture the high-
order relationship among stroma tissue regions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"2. overview of the proposed model for identifying tumor-associated stroma in
multi-modal prostate histopathology slides: the input patches are represented as spa-
tial graphs and passed through a feature extractor. the patch embeddings are fed
into a graph attention network (gat) module to capture inter-patch relationships
and reﬁne the features with neighborhood consistency regularization (ncr) for han-
dling noisy labels. the source discriminator serves as adversarial multi-modal learning
(aml) module to predict data source (biopsy/whole-mount)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"the use of neighbor
sampling enables eﬃcient processing of large images and allows for stochastic
training of the model. to predict tumor-associated binary labels of stroma patches, we employ a
message-passing approach that propagates patch features in the spatial graph. the gat uses an attention mechanism
on node features to construct a weighting kernel that determines the impor-
tance of nodes in the message-passing process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"and the module was optimized using the cross-
entropy loss lgat in an end-to-end fashion. 2.3
neighbor consistency regularization for noisy labels
the labeling of tumor-associated stroma can be aﬀected by various factors, which
can result in noisy labels. one of the reasons for noisy labels is the irregular dis-
tribution of the ﬁeld eﬀect, which makes it challenging to deﬁne a clear boundary
between the tumor-associated and normal stroma regions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"additionally, the pres-
ence of tumor heterogeneity and the varied distribution of tumor foci can further
complicate the labeling process. [20] to prevent the model from overﬁtting to incorrect labels. [21], which suggests that feature representations
are capable of distinguishing between noisy and clean examples during model
training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"2.4
adversarial multi-modal learning
biopsy and whole-mount slides provide complementary multi-modal informa-
tion on the tumor microenvironment, and combining them can provide a more
comprehensive understanding of tumor-associated stroma. however, using data
from multiple modalities can introduce systematic shifts, which can impact the
performance of a deep learning model. speciﬁcally, whole-mount slides typically
contain larger tissue sections and are processed using diﬀerent protocols than
biopsy slides, which can result in diﬀerences in image quality, brightness, and
contrast."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"the stroma classi-
ﬁer and source discriminator are trained simultaneously, aiming to eﬀectively
classify tumor-associated stroma while impeding accurate source prediction by
the discriminator. the optimization process aims to achieve a balance between
these two goals, resulting in an embedding space that encodes as much informa-
tion as possible about tumor-associated stroma identiﬁcation while not encod-
ing any information on the data source. by adopting the adversarial learning
648
z. wang et al.
strategy, our model can maintain the correlated information and shared charac-
teristics between two modalities, which will enhance the model’s generalization
and robustness."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"3
experiment
3.1
dataset
in our study, we utilized three datasets for tumor-associated stroma analysis. (1) dataset a comprises 513 tiles extracted from the whole mount slides of 40
patients, sourced from the archives of the pathology department at cedars-
sinai medical center (irb# pro00029960). [22], along with 289 images from 20 patients with dense high-grade
cancer (gleason grades 4 and 5) and cribriform/non-cribriform glands [23]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"the tiles were annotated
at the pixel-level by expert pathologists to generate stroma tissue segmentation
masks and were cross-evaluated and normalized to account for stain variabil-
ity. (2) dataset b included 97 whole mount slides with an average size of over
174,000×142,000 pixels at 40x magniﬁcation. the prostate tissue within these
slides had an average tumor area proportion of 9%, with an average tumor area of
77 square mm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"an expert pathologist annotated the tumor region boundaries at
the region-level, providing exhaustive annotations for all tumor foci. (3) dataset
c comprised 6134 negative biopsy slides obtained from 262 patients’ biopsy pro-
cedures, where all samples were diagnosed as negative. these slides are presumed
to contain predominantly normal stroma tissues without phenotypic alterations
in response to cancer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"dataset a was utilized for training the stroma segmentation model. extensive
data augmentation techniques, such as image scaling and staining perturbation,
were employed during the training process. the model achieved an average test
dice score of 95.57 ± 0.29 through 5-fold cross-validation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"setting a 5mm threshold accounts
for the typically minimal inﬂammatory responses induced by prostate cancers,
deep learning for prostate tumor-associated stroma identiﬁcation
649
particularly in lower-grade cases. to incorporate multi-modal information, we
randomly sampled negative stroma patches from all biopsy slides in dataset c.
overall, we selected over 1.1 million stroma patches of size 256×256 pixels at 40x
magniﬁcation for experiments. during model training and testing, we performed
stain normalization and standard image augmentation methods.
3.2
model training and evaluation
for constructing knn-based patch graphs, we limited the graph size by setting
k = 4 and layer number l = 3."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_62.pdf,"the results show that the full model outperforms the base model by a
large margin with 10.04% in auroc and 10.97% in f1 score, and each module
contributes to the overall performance. compared to the base model, the addi-
tion of the gat module resulted in a signiﬁcant improvement in all metrics,
suggesting spatial information captured by the patch graph was valuable for
stroma classiﬁcation. the most notable performance improvement was achieved
by the aml module, with a 5.72% increase in auroc and 5.55% increase in
650
z. wang et al.
recall."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"furthermore, recent tumour con-
trol techniques such as intensity modulated radiation therapy (imrt)
dose painting requires the accurate calculation of multiple nested con-
tours of intensity values to optimise dose distribution across the tumour.
recently, convolutional neural networks (cnns) have achieved tremen-
dous success in image segmentation tasks, most of which present the
output map at a pixel-wise level. however, its ability to accurately rec-
ognize precise object boundaries is limited by the loss of information in
the successive downsampling layers. in addition, for the dose painting
strategy, there is a need to develop image segmentation approaches that
reproducibly and accurately identify the high recurrent-risk contours."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"instead of user-supplied tuning parameters, our
ﬁnal model, named kspc-net, applies a cnn backbone to automatically
learn the parameters and leverages the advantage of kspc to simultane-
ously identify object boundaries and provide probability contour accord-
ingly. the proposed model demonstrated promising performance in com-
parison to state-of-the-art models on the miccai 2021 challenge dataset
(hecktor). [10], playing an important role in the stag-
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 51.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"manual delineation is a time-consuming and laborious task that is prone to
poor reproducibility in medical imaging, and this is particularly true for pet,
due to its low signal-to-noise ratio and limited spatial resolution [10]. in addition,
manual delineation depends heavily on the expert’s prior knowledge, which often
leads to large inter-observer and intra-observer variations [8]. therefore, there
is an urgent need for developing accurate automatic segmentation algorithms in
pet images which will reduce expert workload, speed up rt planning while
reducing intra-observer variability."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"in the last decade, cnns have demonstrated remarkable achievements in
medical image segmentation tasks. this is primarily due to their ability to learn
informative hierarchical features directly from data. however, as illustrated in
[9,23], it is rather diﬃcult for cnns to recognize the object boundary precisely
due to the information loss in the successive downsampling layers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"the kspc provides a surface
over images that naturally produces contour-based results rather than pixel-wise
results, thus mimicking experts’ hand segmentation. however, the performance
of kspc depends heavily on the tuning parameters of bandwidth and threshold
in the model, and it lacks information from other patients. beyond tumour delineation, another important use of functional images, such
as pet images is their use for designing imrt dose painting (dp)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"to address both tumour delineation and corresponding dose painting chal-
lenges, we propose to combine the expressiveness of deep cnns with the versa-
536
w. zhang and s. ray
tility of kspc in a uniﬁed framework, which we call kspc-net. in the proposed
kspc-net, a cnn is employed to learn directly from the data to produce the
pixel-wise bandwidth feature map and initial segmentation map, which are used
to deﬁne the tuning parameters in the kspc module. our framework is com-
pletely automatic and diﬀerentiable."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"on the other hand, since xt is counted yi times at the same position, eq. 1 can
be further simpliﬁed as
ˆf(x; h) =
 n

i=1
yi
−1
n

i=1
kh(x − xi)yi. (2)
a scaled kernel is positioned so that its mode coincides with each data point
xi which is expressed mathematically as kh(x−xi). − xi)t h−1(x − xi)),
deep probability contour framework
537
which is a normal distribution with mean xi and variance-covariance matrix h.
therefore, we can interpret ˆf in eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"the primary advantage of utilizing probability contours is their ability to
assign a clear probabilistic interpretation on the deﬁned contours, which are
scale-invariant [5]. this provides a robust deﬁnition of probability under the
perturbation of the input data. in addition, these contours can be mapped to
the imrt dose painting contours, thus providing an alternative prescription
strategy for imrt."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"2 the proposed kspc-net integrates the kspc approach with a
cnn backbone (unet) in an end-to-end diﬀerentiable manner. first, the ini-
tial segmentation map and pixel-level bandwidth parameter map h(xi1, xi2) of
kspc are learned from data by the cnn backbone. then the kspc module
obtains the quantile threshold value for each image by identifying the quantile
corresponding to the minimum suv of the tumour class in the initial segmen-
tation map."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"the next step involves transmitting the bandwidth map, quantile
threshold, and raw image to kspc module to generate the segmentation map
and its corresponding probability contours. the resulting output from kspc is
then compared to experts’ labels using a dice similarity loss function, referred
to kspc loss. additionally, the initial unet segmentation can produce another
loss function, called cnn loss, which serves as an auxiliary supervision for the
cnn backbone."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"deep probability contour framework
539
2.3
loss function
the dice similarity coeﬃcient is widely employed to evaluate segmentation mod-
els. we utilize the dice loss function to optimize the model performance during
training, which is deﬁned as:
ldice(y, ˆy) = 1 −
2 n
i yiˆyi
n
i yi + n
i ˆyi + ϵ
,
where yi is the label from experts and ˆyi is the predicted label of i-th pixel. n
is the total number of pixels and ϵ is a small constant in case of zero division."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"in addition, α is a balancing parameter
and is set to be 0.01 in this work. 3
experiments
3.1
dataset
the dataset is from the hecktor challenge in miccai 2021 (head and neck
tumor segmentation challenge). for each patient, fdg-
pet input images and corresponding labels in binary description (0 s and 1 s)
for the primary gross tumour volume are provided and co-registered to a size
of 144 × 144 × 144 using bounding box information encompassing the tumour."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"each convolutional layer is
followed by relu activation and batch normalization. 4
results
4.1
results on hecktor 2021 dataset
to evaluate the performance of our kspc-net, we compared it with the results
of 5-fold cross-validation against three widely-used models namely, the standard
2d unet, the 2d residual unet and the 3d unet. [7] and ccut-net
[21] which were reported in the hecktor 2021 challenges [1]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_51.pdf,"to quantify the
performance, we report several metrics including dice similarity scores, preci-
sion, recall, and hausdorﬀ distance. table 1 shows the quantitative comparison
of diﬀerent approaches on hecktor dataset. it is worth mentioning that since
our kspc-net is in a 2d unet structure, the hausdorﬀ distance here was calcu-
lated on slice averages to use a uniform metric across all 2d and 3d segmentation
models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"consideration of subgroups or domains within medical image
datasets is crucial for the development and evaluation of robust and gen-
eralizable machine learning systems. to tackle the domain identiﬁcation
problem, we examine deep unsupervised generative clustering approaches
for representation learning and clustering."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"the variational deep embed-
ding (vade) model is trained to learn lower-dimensional representations
of images based on a mixture-of-gaussians latent space prior distribu-
tion while optimizing cluster assignments. we propose the conditionally
decoded variational deep embedding (cdvade) model which incorpo-
rates additional variables of choice, such as the class labels, as condition-
ing factors to guide the clustering towards subgroup structures in the
data which have not been known or recognized previously. we analyze
the behavior of cdvade on multiple datasets and compare it to other
deep clustering algorithms."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"keywords: domain identiﬁcation · deep clustering · subgroup
identiﬁcation · variational autoencoder · generative model
1
introduction
machine learning (ml), speciﬁcally deep learning (dl), algorithms have shown
exceptional performance on numerous medical image analysis tasks [2]. never-
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3 64.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. for a generalizabil-
ity assessment, reporting only aggregate performance measures is not suﬃcient."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"thus, achieving (through training) and demonstrating (as part of testing)
satisfactory ml model performance across relevant subgroups is crucial before
the real-world clinical deployment of a medical ml system [13].
however, a challenging situation arises when relevant subgroups are unrec-
ognized. one solution to this issue is to apply a clustering algorithm to the data,
with the goal of identifying the unannotated subgroups. the main objective of
unsupervised clustering is to group data points into distinct classes of similar
traits."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"unsupervised generative clustering aims
to simultaneously address both domain identiﬁcation and dimensionality reduc-
tion. deep unsupervised clustering algorithms could map the medical imaging
data back to their causal factors or underlying domains, such as image acqui-
sition equipment, patient subpopulations, or other meaningful data subgroups. however, there is a practical need to be able to guide the deep clustering model
towards the identiﬁcation of grouping structures in a given dataset that have not
been already annotated."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"in our study, vade is deployed
as a deep clustering model using convolutional neural network (cnn) archi-
tectures for the encoder g(x; φ) and the decoder f(z; θ). the encoder learns to
668
m. sidulova et al.
compress the high-dimensional input images x into lower-dimensional latent rep-
resentations z. using a mixture-of-gaussians (mog) prior distribution for the
latent representations z, we examine subgroups or domains within the dataset,
revealed by the individual gaussians within the learned latent space, and how z
aﬀects the generation of x. the model can be used to perform inference, where
observed images x are mapped to corresponding latent variables z and their
cluster/domain assignments c. we denote the latent space dimensionality by d
(i.e., z ∈ rd), and the number of clusters by d (i.e., c ∈ {1, 2, . . . the
trained decoder cnn can also be used to generate synthetic images from the
algorithmically identiﬁed subgroups."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"[10] to maxi-
mize a statistical measure called the evidence lower bound (elbo). we denote
the true data distribution by p(z, x, c) and the variational posterior distribution
by q(z, c|x). − log q(c|x)],
(1)
where p(x|z) is modeled by the decoder cnn, and q(z|x) is modeled by the
encoder cnn g(x; φ) as
q(z|x) = n

z; ˜μ, diag
˜σ2
,
˜μ, log ˜σ2
= g(x; φ)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"we propose the conditionally decoded variational deep embedding (cdvade)
model as an extension to vade as shown in fig. the generative process of
cdvade diﬀers from vade in that it concatenates additional variables y to the
latent representation z. for example, y may contain the available class labels
or already known subgroup structures, which do not need to be discovered. it
is assumed that these additional variables y are available at training and test
time."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"speciﬁcally, the generative process of cdvade takes the form
p(c) = cat(π)
(4)
p(z|c) = n

z; μc, diag

σ2
c

,
(5)

μxy, log σ2
xy

= f(z, y; φ),
(6)
p(x|z, y) = n

x; μxy, diag

σ2
xy

(7)
since our goal is to ﬁnd clusters c that are unassociated with the available
variables y of choice and to learn latent representations z that do not contain
information about y, the generative process of cdvade assumes that z, c are
jointly independent of y.
the changes compared to the generative process of vade can also be
regarded as imposing a structure on the model, where the encoder learns hidden
representations of the image x conditioned to the additional variables y (i.e.,
q(z|x, y)), but acts as an identity function with respect to y (i.e., y can be
regarded as being simply concatenated to the latent space representations z). the decoder then translates this data representation in the form of (z, y) to the
input space (i.e., p(x|z, y)). given that the underlying vae architecture seeks
to eﬃciently compress the input data x into a learned representation, this incen-
tivizes the model to exclude information about y from the learned variables z
and c."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"in our dec experiments,
we use the same autoencoder architecture and the same initialization as for the
vade. 2.4
related works in medical imaging
a number of studies have been conducted with several approaches of deep clus-
tering for medical imaging data. typically, clustering is performed on top of
features extracted with the use of an encoder neural network, and the cluster
assignments are determined by using conventional clustering algorithms, such
as k-means, on top of the learned latent representations [1,5,7,12]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"in contrast,
this work investigates models which enforce a clustering structure in the latent
space through the use of a mog prior distribution, as well as guidance of the
clustering model via the proposed conditioning mechanism. 3
experiments
3.1
colored mnist
the colored mnist is an extension to the classic mnist dataset [3], which
contains binary images of handwritten digits. the colored mnist includes col-
ored images of the same digits, where each number and background have a color
assignment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"we use latent space
dimensionality d = 20 for all models.
fig. 2. both vade and dec cluster the colored mnist digits by the color, while
cdvade clusters are associated with the digit label. bar graphs labeled “color” –
each color represents a speciﬁc color of colored mnist digits."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"in the “digit” plots,
colors correspond to digits labels. 2 a summary of the results for the experiments on the colored mnist
dataset is presented. the results demonstrate that by allowing for the incorpora-
tion of additional information, particularly color labels, the proposed cdvade
model is more sensitized to learning other underlying features, which allows for
distinguishing between the diﬀerent digits in this particular example."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"notably,
both vade and dec end up clustering the data by color, as it is the most
striking distinguishing characteristic of these images. on the other hand, the
predicted domains of cdvade have no association with color, and the data are
separated by the shapes in the images, distinguishing some of the digit labels
(albeit imperfectly). this example serves as a proof of concept for the proposed
conditioning mechanism of cdvade.
3.2
application to a digital pathology dataset
her2 dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"each tissue slide has been digitized at three
diﬀerent sites using three diﬀerent whole slide imaging systems, evaluated by 7
pathologists on a 0–100 scale, and following clinical practice labeled as her2
class 1, 2, or 3 (based on mean pathologists’ scores with cut-points at 33 and
66). we use a subset of this dataset consisting of 672 images (the remainder is
held out for future research). because the intended purpose is ﬁnding subgroups
in the given dataset only, a separate test set is not used."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"the dimensions of the
images vary from 600 to 826 pixels, and we scale all data to a uniform size of
128 × 128 pixels before further processing. we refer to [4,8] for more details
about this dataset. 672
m. sidulova et al.
this retrospective human subject dataset has been made available to us by
the authors of the prior studies [4,8], who are not associated with this paper."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"appropriate ethical approval for the use of this material in research has been
obtained. deep clustering models applied to the her2 dataset. we evaluate
the performance and behavior of the dec, vade, and cdvade models on
the her2 dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"we investigate whether the models will learn to distinguish
the her2 class labels, the scanner labels, or other potentially meaningful data
subgroups in a fully unsupervised fashion. to investigate the clustering abilities
of cdvade on the her2 dataset, we inject the her2 class labels into the
latent embedding space. we hypothesize that this will disincentivize the encoder
network from including information related to the her2 class labels in the latent
representations z."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"3. results summary for vade, cdvade, and dec, all with d = 3. example
images from the identiﬁed clusters are visualized for each method. distributions of
her2 class and scanner labels are shown per cluster (i.e., predicted domain). figure 3 demonstrates that even without scrutinizing, one can observe a
strong visual separation between the algorithmically identiﬁed image domains
for both vade and dec experiments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,"the corre-
lation coeﬃcient between the dec clusters and the her2/neu scores is 0.71. however, neither vade nor dec clusters are associated to the scanner labels. 4. boxplots of her2/neu scores per predicted domain for all experiments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_64.pdf,", the predicted domains are again
clearly visually disparate. however, as intended, there is a weaker association
with the her2 class labels and a stronger association with the scanner labels,
compared to the results of vade and dec. in fig. 4, her2/neu median scores
of the three clusters move closer together, illustrating the decrease of association
with her2 class labels, as intended by the formulation of the cdvade model. the correlation coeﬃcient between the cdvade cluster assignments and the
her2/neu scores is 0.39."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"multiple-instance learning (mil) is an attractive approach
for digital pathology applications as it reduces the costs related to data
collection and labelling. however, it is not clear how sensitive mil is to
clinically realistic domain shifts, i.e., diﬀerences in data distribution that
could negatively aﬀect performance, and if already existing metrics for
detecting domain shifts work well with these algorithms."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"we trained an
attention-based mil algorithm to classify whether a whole-slide image
of a lymph node contains breast tumour metastases. the algorithm was
evaluated on data from a hospital in a diﬀerent country and various sub-
sets of this data that correspond to diﬀerent levels of domain shift. our
contributions include showing that mil for digital pathology is aﬀected
by clinically realistic diﬀerences in data, evaluating which features from
a mil model are most suitable for detecting changes in performance,
and proposing an unsupervised metric named fr´echet domain distance
(fdd) for quantiﬁcation of domain shifts."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"supported by swedish e-science research center, vinnova, the ceniit career
development program at link¨oping university, and wallenberg ai, wasp funded by
the knut and alice wallenberg foundation. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 16. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"multiple instance learning (mil) alleviates the need for detailed anno-
tations and has seen increased adoption in recent years. mil approaches have
proven to work well in academic research on histopathology data [1,17,29] as
well as in commercial applications [26]. this remains a signiﬁcant obstacle to the deployment of dl
applications in clinical practice [7]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"however, it may not
be feasible to perform an explicit domain adaptation, and an already adapted
model could still experience problems with domain shifts. hence, it is important
to provide indications of the expected performance on a target dataset without
requiring annotations [5,25]. [33] which aims to detect individual samples that are ood, in contrast
to our objective of estimating a diﬀerence of expected performances between
some datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"for supervised algorithms, techniques of uncertainty estimation
have been used to measure the eﬀect of domain shift [4,15,18] and to improve
the robustness of predictions [19,21,30]. alternatively,
a drop in performance can be estimated by comparing the model’s softmax out-
puts [8] or some hidden features [24,28] acquired on in-domain and domain shift
datasets. although such methods have been demonstrated for supervised algo-
rithms, as far as we know no previous work has explored domain shift in the
speciﬁc context of mil algorithms."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"hence, it is not clear how well they will work
in such a scenario. in this work, we evaluate an attention-based mil model on unseen data
from a new hospital and propose a way to quantify the domain shift severity. the model is trained to perform binary classiﬁcation of wsis from lymph nodes
of breast cancer patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"we show that our proposed unsupervised metric for quantifying
domain shift correlates best with the changes in performance, in comparison to
multiple baselines. the approach of validating a mil algorithm in a new site
without collecting new labels can greatly reduce the cost and time of quality
detecting domain shift in mil
159
assurance eﬀorts and ensure that the models perform as expected in a variety of
settings. the novel contributions of our work can be summarised as:
1. proposing an unsupervised metric named fr´echet domain distance (fdd)
for quantifying the eﬀects of domain shift in attention-based mil;
2. showing how fdd can help to identify subsets of patient cases for which mil
performance is worse than reported on the in-domain test data;
3."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"to test if
the reduction of the number of features in itself has a positive eﬀect on domain
shift quantiﬁcation, we also compare with k randomly selected patch features. [10] is commonly used to measure similarity
between real and synthetically generated data. inspired by fid, we propose a
metric named fr´echet domain distance (fdd) for evaluating if a model is expe-
riencing a drop in performance on some new dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"for penultimate
layer and mean patch features, we can apply this strategy directly. = 1
k
k
k=1 md,k. now, we can compute means µk
d
and covariance matrices ck
d from mk
d extracted from two datasets d = 1 and
d = 2. [16] is used for model training (770 wsis of
which 293 wsis contain metastases) and in-domain testing (629 wsis of which
289 wsis contain metastases)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"to evaluate
clinically realistic domain shifts, we divided the dataset in two diﬀerent ways,
creating four subsets:
1a. 161 wsis from sentinel node biopsy cases (54 wsis with metastases): a
small shift as it is the same type of lymph nodes as in grand challenge
camelyon data [16].
1b. 141 wsis from axillary nodes dissection cases (57 wsis with metastases):
potentially large shift as some patients have already started neoadjuvant
treatment as well as the tissue may be aﬀected from the procedure of sentinel
lymph node removal.
2a."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"68 wsis with lobular carcinoma (28 wsis with metastases): potentially
large shift as it is a rare type of carcinoma and relatively diﬃcult to diagnose. the datasets of lobular and ductal carcinomas each contain 50 % of wsis
from sentinel and axillary lymph node procedures. the sentinel/axillary division
is motivated by the diﬀering dl prediction performance on such subsets, as
detecting domain shift in mil
161
observed by jarkman et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"our method is intended to avoid requiring
dedicated wsi labelling eﬀorts. we deem that the information needed to do this
type of subset divisions would be available without labelling since the patient
cases in a clinical setting would already contain such information. all datasets
are publicly available to be used in legal and ethical medical diagnostics research."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"in this section, we describe
the experiments we conducted. 4.1
mil training
we trained, with default settings, 10 clam models to classify wsis of breast
cancer metastases using a 10-fold cross-validation (cv) on the training data. the test data was kept the same for all 10 models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"following the con-
clusions of [2] that mcc well represents the full confusion matrix and the fact
that in clinical practice a threshold needs to be set for a classiﬁcation decision,
mcc is used as a primary metric of performance for domain shift analysis while
roc-auc is reported for completeness. whereas extremely large variations in
label prevalence could reduce the reliability of the mcc metric, this is not the
case here as label prevalence is similar (35–45%) in our test datasets. for deep
ensemble [15] we trained 4 additional clam models for each of the 10 cv folds."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"4.2
domain shift quantiﬁcation
as there is no related work on domain shift detection in the mil setting, we
selected methods developed for supervised algorithms as baselines:
– the model’s accumulated uncertainty between two datasets. deep ensemble
[15] (de) and diﬀerence in conﬁdence with entropy [8] (doc) compare the
mean entropy over all data points. de uses an ensemble to estimate better-
calibrated uncertainty than the single model in doc.
– the accumulated conﬁdence of a model across two datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"representation shift [28] (rs)
has shown promising results in detecting domain shift in convolutional neural
networks and it is the method most similar to fdd. however, it is not trivial
which hidden features of mil that are most suitable for this task, and we
evaluate several options (see sect. 2.2) with both methods.
162
m. poceviˇci¯ut˙e et al.
for all possible pairs of camelyon and the other test datasets, and for the
10 cv models, we compute the domain shift measures and compare them to
the observed drop in performance. the eﬀectiveness is evaluated by pearson
correlation and visual investigation of corresponding scatter plots."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"all results
are reported as mean and standard deviation over the 10-fold cv.
5
results
the ﬁrst part of our results is the performance of the wsi classiﬁcation task
across the subsets, summarized in table 1. while showing similar trends, there
is some discrepancy in the level of domain shift represented by the datasets due
to the diﬀerences between the mcc and roc-auc measures. as we deemed mcc to better represent the clinical use situation (see
sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"table
1.
classiﬁcation
performance
reported in mean (standard deviation) of
mcc and roc-auc metrics, computed
over the 10-fold cv models. a thresh-
old for mcc is determined on validation
data. 1. scatter plot of fdd64 against drop
in mcc for all model-dataset combinations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"a ﬁtted line is included for better inter-
pretability. we observe the largest domain shift in terms of mcc on axillary nodes
followed by lobular carcinoma and full brln datasets. there seems to be no
negative eﬀect from processing the sentinel nodes data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"fdd64 outperforms the base-
lines substantially, and has the smallest standard deviation. figure 1 shows how
individual drop in performance of model-dataset combinations are related to the
fdd64 metric. for most models detecting larger drop in performance (> 0.05)
is easier on axillary lymph nodes data than on any other analysed dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"detecting domain shift in mil
163
table 2. pearson correlations between domain shift measure and diﬀerence in per-
formance (mcc metric) of camelyon test set and the other datasets. the mean and
standard deviation values are computed over the 10 cv folds."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"we conclude that in our setup the best and
most reliable performance of domain shift quantiﬁcation is achieved by positive
evidence with fd and k = 64, i.e. fdd64.
164
m. poceviˇci¯ut˙e et al.
6
discussion and conclusion
mil is aﬀected by domain shift. some previous work claim that mil is
more robust to domain shift as it is trained on more data due to the reduced costs
of data annotation [1,17]. we argue that domain shift will still be a factor to
consider as an algorithm deployed in clinical practice is likely to encounter unseen
varieties of data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"however, it may require more eﬀort to determine what type of
changes in data distribution are critical. our results show that domain shift is
present between the wsis from the same hospital (camelyon data) and another
medical centre (brln data). however, as clinically relevant subsets of brln
data are analysed, stark diﬀerences in performance and reliability (indicated
by the standard deviation) are revealed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_16.pdf,"thus, it seems a critical component in domain
shift measurement in attention-based mil is to correctly make use of the atten-
tion scores. from fig. 1 we can see that if we further investigated all model-
dataset combinations that resulted in fdd64 above 0.5, we would detect many
cases with a drop in performance larger than 0.05. however, the drop is easier
to detect on axillary and lobular datasets compared to others."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"endoscopic radio-guided cancer detection and
resection has recently been evaluated whereby a novel tethered laparo-
scopic gamma detector is used to localize a preoperatively injected radio-
tracer. this can both enhance the endoscopic imaging and complement
preoperative nuclear imaging data. however, gamma activity visualiza-
tion is challenging to present to the operator because the probe is non-
imaging and it does not visibly indicate the activity origination on the tis-
sue surface."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"to demon-
strate the eﬀectiveness of this solution, we designed and implemented a
simple regression network that successfully addressed the problem. to
further validate the proposed solution, we acquired and publicly released
two datasets captured using a custom-designed, portable stereo laparo-
scope system. through intensive experimentation, we demonstrated that
our method can successfully and eﬀectively detect the sensing area, estab-
lishing a new performance benchmark."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"code and data are available at
https://github.com/br0202/sensing area detection.git. keywords: laparoscopic image-guided intervention · minimally
invasive surgery · detection of sensing area
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43996-4 25.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43996-4_25
detecting the sensing area of a laparoscopic probe in cancer surgery
261
1
introduction
cancer remains a signiﬁcant public health challenge worldwide, with a new diag-
nosis occurring every two minutes in the uk (cancer research uk1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"however, it is not trivial to determine this using traditional methods due to
1 https://www.cancerresearchuk.org/health-professional/cancer-statistics-for-the-uk. 262
b. huang et al.
poor textural deﬁnition of tissues and lack of per-pixel ground truth depth data. similarly, it is also challenging to acquire the probe pose during the surgery."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"ye et
al. [22] proposed a deep learning framework for surgical scene depth estimation
in self-supervised mode and achieved scalable data acquisition by incorporating
a diﬀerentiable spatial transformer and an autoencoder into their framework. a 3d displacement module was explored in [21] and 3d geometric consistency
was utilized in [8] for self-supervised monocular depth estimation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"recently, fully supervised methods were
summarized in [1] for depth estimation. however, acquiring per-pixel ground
truth depth data is challenging, especially for laparoscopic images, which makes
it diﬃcult for large-scale supervised training [8]. laparoscopic segmentation is another important task in computer-assisted
surgery as it allows for accurate and eﬃcient identiﬁcation of instrument posi-
tion, anatomical structures, and pathological tissue."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"the dual swin transformer u-net
was proposed in [11] to enhance the medical image segmentation performance,
which leveraged the hierarchical swin transformer into both the encoder and the
decoder of the standard u-shaped architecture, beneﬁting from the self-attention
computation in swin transformer as well as the dual-scale encoding design. detecting the sensing area of a laparoscopic probe in cancer surgery
263
although the intermediate depth information was not our ﬁnal aim and can
be bypassed, the 3d surface information was necessary in the intersection point
inference. [3] has been commonly used as the encoder to extract the
image features and geometric information of the scene."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"in particular, in [21],
concatenated stereo image pairs were used as inputs to achieve better results,
and such stereo image types are also typical in robot-assisted minimally invasive
surgery with stereo laparoscopes. hence, stereo image data was also adopted in
this paper. if the problem of inferring the intersection point is treated as a geometric
problem, both data collection and intra-operative registration would be diﬃcult,
which inspired us to approach this problem diﬀerently."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"we note that the standard illumination image from the laparoscopic probe
is also captured with the same setup when the laser module is on. therefore, we
can establish a dataset with an image pair (rgb image and laser image) that
shares the same intersection point ground truth with the laser image (see fig. 2a
and fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"hence, we input these axes to the network as another
branch and randomly sampled points along them to represent the probe. 3
dataset
to validate our proposed solution for the newly formulated problem, we acquired
and publicly released two new datasets. in this section, we introduce the hard-
ware and software design that was used to achieve our ﬁnal goal, while fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"(a)
(b)
(c)
laser spot
laser spot
(d)
left
image
left
image
right
image
right
image
fig. 2. example data. (a) standard illumination left rgb image; (b) left image with
laser on and laparoscopic light oﬀ; same for (c) and (d) but for right images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"the laser module emitted a red laser beam (wavelength 650 nm) that
was visible as a red spot on the tissue surface. we acquired the dataset on a silicone tissue phantom which was 30 × 21 × 8
cm and was rendered with tissue color manually by hand to be visually realistic. the phantom was placed on a rotation stage that stepped 10 times per revolution
to provide views separated by a 36-degree angle."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"at each position, stereo rgb
images were captured i) under normal laparoscopic illumination with the laser
oﬀ; ii) with the laparoscopic light blocked and the laser on; and iii) with the
laparoscopic light blocked and the laser oﬀ. subtraction of the images with laser
on and oﬀ readily allowed segmentation of the laser area and calculation of its
central point, i.e. the ground truth probe axis-surface intersection. all data acquisition and devices were controlled by python and labview
programs, and complete data sets of the above images were collected on visually
realistic phantoms for multiple probe and laparoscope positions. this provided
10 tissue surface proﬁles for a speciﬁc camera-probe pose, repeated for 120 dif-
ferent camera-probe poses, mimicking how the probe may be used in practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"therefore, our ﬁrst newly acquired dataset, named jerry, contains 1200 sets
of images. since it is important to report errors in 3d and in millimeters, we
recorded another dataset similar to jerry but also including ground truth depth
map for all frames by using structured-lighting system [8]—namely the coﬀbee
dataset. these datasets have multiple uses such as:
– intersection point detection: detecting intersection points is an important
problem that can bring accurate surgical cancer visualization."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"in this work, we propose a simple, yet eﬀective regression approach to address
this problem. our approach relies solely on the 2d information and works well
without the need for the laser module after training. furthermore, this sim-
ple methodology facilitated an average inference time of 50 frames per second,
enabling real-time sensing area map generation for intraoperative surgery."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"images with laser on and laparoscopic light oﬀ. the predicted intersection point
is shown in blue and the green point indicates the ground truth, which are further
indicated by arrows for clarity. (color ﬁgure online)
understandable as the red laser spot provides the key information for the seg-
mentation. therefore the network does not have any visual information to make
predictions from images of the gamma probe."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"the
goal was to predict the intersection point pintersect on the surface of the tissue. during the training, the ground truth intersection point position was provided
by the laser source, while during testing the intersection was estimated solely
based on visual information without laser guidance (see fig. unlike the segmentation approach, the intersection
point was directly predicted using a regression network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"the networks were implemented in pytorch [17],
with an input resolution of 896 × 896 and a batch size of 12. we partitioned the
jerry dataset into three subsets, the training, validation, and test set, consisting
of 800, 200, and 200 images, respectively, and the same for the coﬀbee dataset. the learning rate was set to 10−5 for the ﬁrst 300 epochs, then halved until
epoch 400, and quartered until the end of the training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"results using resnet50. grey
color denotes the jerry dataset and blue
color is for coﬀbee dataset (2d errors
are in pixels and 3d errors are in mm). grey color
denotes the jerry dataset and blue color
is for coﬀbee dataset (2d errors are in
pixels and 3d errors are in mm)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"6.9
8.2
16.7 21.3
7.0
3d median
6.0
5.9
7.1
5.3
6.2
5
results
quantitative results on the released datasets are shown in table 1 and table 2
with diﬀerent backbones for extracting image features, resnet and vit. for
the 2d error on two datasets, among the diﬀerent settings, the combination of
resnet and mlp gave the best performance with a mean error of 70.5 pixels
268
b. huang et al.
and a standard deviation of 56.8."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_25.pdf,"similarly, the sampled 50 principal points on the probe
axis were better processed using the simple mlp rather than using a recurrent
procedure lstm. it is worth noting that the results from stereo inputs exceeded
those from mono inputs, which can be attributed to the essential 3d information
included in the stereo image pairs. for the 3d error, the resnet backbone still gave generally better performance
than the vit backbone while under the resnet backbone, lstm and mlp gave
competitive results and they are all in sub-milimeter level."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf,"basal cell carcinoma (bcc) is a prevalent and increasingly
diagnosed form of skin cancer that can beneﬁt from automated whole
slide image (wsi) analysis. however, traditional methods that utilize
popular network structures designed for natural images, such as the
imagenet dataset, may result in reduced accuracy due to the signiﬁ-
cant diﬀerences between natural and pathology images. in this paper,
we analyze skin cancer images using the optimal network obtained by
neural architecture search (nas) on the skin cancer dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf,"furthermore, unlike traditional unilaterally aug-
mented (ua) methods, the proposed supernet skin-cancer net (sc-net)
considers the fairness of training and alleviates the eﬀects of evalua-
tion bias. we use the sc-net to fairly treat all the architectures in the
search space and leveraged evolutionary search to obtain the optimal
architecture for a skin cancer dataset. our experiments involve 277,000
patches split from 194 slides."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf,"(a) region of interest (roi) extrac-
tion and patch generation, and (b) patch detection and wsi classiﬁcation. [7–9] typically employs models like
inception net and resnet, designed for natural images like those in the imagenet
dataset. the signiﬁcant variance in pathology and natural images can compro-
mise these models’ accuracy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf,"module (a) extracts roi from
wsi, creating 224 × 224 patches. module (b) uses these patches to train and search
optimal structure within a supernet via an evolutionary algorithm, yielding dataset
predictions. [14]. section 2.2
provides further details about the supernet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf,"a balanced evolutionary algorithm
is then used to select the optimal structure from the search space, with the
candidate structures’ performance evaluated using mini-batch patch data. we
evaluate the searched architectures on the skin cancer dataset. 2.1
one-shot channel number search
to extract an optimal architecture γ ∈ g from a vast search space g, a weight-
sharing strategy [15–17] is used to prevent training from scratch."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf,"the search
leverages a supernet s with weights w, with each path γ inheriting weights from
w. this makes one-shot nas a two-step optimization process: supernet training
and architecture search. the original dataset is typically split into training dt
and validation datasets dv. the weights w of the supernet s are trained by
uniformly sampling the network width d and optimizing the sub-network with
weights wd ⊂ w. the optimization function is deﬁned as follows:
w∗ = arg min
wd∈w
e
d∈u(d)[[lt(wd; s, d, dt]])
(1)
where u (d) is a uniform distribution of network widths, e is the expected value
of random variables, and lt is the training loss function."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf,"acc(w, d, dv) = 1
2 (acc(sl, d; dv) + acc(sr, d; dv))
(9)
3
experiments
3.1
experiment settings
table 1. generated dataset split
bcc-positive bcc-negative
training wsi patch
wsi patch
118
132,981 37
90,291
testing
wsi patch
wsi patch
30
31,651
9
22,838
the dataset, comprised of 194 skin slides acquired from the southern sun pathol-
ogy laboratory, includes 148 bcc cases and 46 other types (common nevus,
detection of basal cell carcinoma in whole slide images
269
scc), all manually annotated by a dermatopathologist. bcc slides served as
positive samples and the rest as negatives."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf,"these slides were scanned at ×20
magniﬁcation with a 0.44 µm pixel size using a leica aperio at2 scanner. the
patient data were separated between training and testing to prevent overlap. details are shown in table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf,"these models, initialized
from a zero-mean gaussian with standard deviation σ = 0.001, were trained for
200 epochs with a batch size of 256. training used the adam optimizer with a
dynamic learning rate reduction strategy, starting with a learning rate of 5e-5
following a cosine schedule.
table 2. performance comparison on skin cancer dataset
type
model
flops
parameters
acc
se
sp
f1
auc
wsi analysis-related
tian et al. we performed a search
on resnet50 and mobilenetv2 models, compared against original resnet50
(ori resnet50) and mobilenetv2 (ori mobilenetv2) models as baselines."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf,"comparison with related methods. to ensure a fair comparison on our
dataset, we selected several papers in the ﬁeld of pathological image analysis,
such as [9,22,23], as well as others using the ua principle, such as [18,24].
evaluation metrics. our model was evaluated on: (1) accuracy (acc): per-
centage of correct classiﬁcations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_26.pdf,"(3) speciﬁcity (sp): proportion of true negatives identiﬁed. (4)
f1 score (f1): precision and recall’s harmonic mean, indicating label alignment. (5) auc: roc curve area, reﬂecting the false/true positive rate trade-oﬀ.
as shown in table 2, the s resnet50 model outperformed in all metrics,
showing 4.8%, 4.5%, 5.4%, 4.7% and 4.7% improvements in accuracy, sensitivity,
speciﬁcity, f1 score, and auc, respectively, over ori resnet50, and surpassing
270
h. xu et al.
table 3. performance of searched models with diﬀerent searching methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"cervical cancer is a signiﬁcant health burden worldwide, and
computer-aided diagnosis (cad) pipelines have the potential to improve
diagnosis eﬃciency and treatment outcomes. however, traditional cad
pipelines have limitations due to the requirement of a detection model
trained on a large annotated dataset, which can be expensive and time-
consuming. they also have a clear performance limit and low data uti-
lization eﬃciency."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"to address these issues, we introduce a two-stage
detection-free pipeline, incorporating pooling transformer and moco pre-
training strategies, that optimizes data utilization for whole slide images
(wsis) while relying solely on sample-level diagnosis labels for training. the experimental results demonstrate the eﬀectiveness of our approach,
with performance scaling up as the amount of data increases. overall, our
novel pipeline has the potential to fully utilize massive data in wsi classi-
ﬁcation and can signiﬁcantly improve cancer diagnosis and treatment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"cao et al. [1] improved
the detection performance by incorporating clinical knowledge and attention
mechanism into their cell detection model of attfpn. wei et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"these methods have achieved good results through continuous improvement
on the detection-based pipeline, but there are some common drawbacks. first,
they are not able to get rid of their reliance on detection models, which means
they have a high need for expensive detection data labeling to train the detec-
tion model. cervical cancer cell detection datasets involve labeling individual
and small bounding boxes in a large number of cells."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"it often requires multi-
ple experienced pathologists to annotate [15], which is very time-consuming and
labor-intensive. second, the widely used detection-based pipeline has not fully
utilized the massive information in wsis. a wsi is typically large (sized of
about 20000 × 20000 pixels)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"finally, many existing methods focus on detecting
and classifying individual cells. the tendency to neglect eﬀective integration of
the overall information across the entire wsi results in poor performance in
sample-level classiﬁcation.
to address the aforementioned issues, we propose a detection-free pipeline
in this paper, which does not rely on any detection model. instead, our pipeline
requires only sample-level diagnosis labels, which are naturally available in clin-
ical scenarios and thus get rid of additional image labeling."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"the two stages
in our pipeline adopt the same network design (i.e., encoder + pooling trans-
detection-free pipeline
245
former), which makes our solution friendly to develop and to use. we also adopt
contrastive learning to eﬀectively utilize the massive information in wsis when
training the encoder for classiﬁcation. as a summary, our pipeline surpasses pre-
vious detection-based methods and achieves state-of-the-art performance with
large-scale training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"for feasibility of computation, we crop
a wsi into mutiple images. the cropped images are passed through the coarse-grained
and ﬁne-grained stages, where only sample-level diagnosis labels of wsis, instead of
any additional manual labeling, are required for training. two-stage pipeline with attention guided selection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"for the coarse-grained stage, after passing the resized local images through
encoder and pooling transformer, we obtain a rough prediction result at the
sample level. we then use the cross-entropy (ce) loss to minimize the diﬀerence
between the predicted wsi label and the ground truth. in addition, we calculate
the attention score to identify the local image inputs that are most likely to yield
positive reading."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"[6], we also perform pre-
training for ﬁne-grained encoder on a large scale of pathology images. generally,
large-scale pre-training usually requires a massive dataset and a suitable loss
function. for data, wsi naturally has the advantage of having a large amount
of training data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"using this method, we can pre-train a feature encoder in an
unsupervised manner and initialize it into our encoder for the ﬁne-grained stage. 248
m. cao et al.
3
experiment and results
dataset and experimental setup. in this study, we have collected 5384
cervical cytopathological wsi by 20x lens, each with 20000 × 20000 pixels, from
our collaborating hospitals."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"among them, there 2853 negative samples, and 2531
positive samples (962 ascus, and 1569 high-level positive samples). all wsis
only have diagnosis labels at the sample level, without annotation boxes at the
cell level. and all sample labels are strictly diagnosed according to the tbs [16]
criterion by a pathologist with 35 years of clinical experience."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"in this section, we experiment to compare
our method with popular state-of-the-art (sota) methods, which are all fully
supervised and detection-based. to the best of our knowledge, there are few
good methods to train cervical cancer classiﬁcation models in weakly supervised
or unsupervised learning ways. no methods can achieve the detection-free goal
either."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"all the detection-based methods are evaluated in the following way. first,
we label a dataset with cell-level bounding boxes to train a detection model. the detection dataset has 3761 images and 7623 cell-level annotations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"on the other hand, the result implies that our coarse-grained task has
replaced the role of cell detection in early works. thus, we conclude that a
detection-free pipeline
249
detection model trained with an expensive annotated dataset is not necessary
to build a cad pipeline for cervical abnormality. in this section, we experiment to demonstrate the eﬀective-
ness of all the proposed parts in our pipeline."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_24.pdf,"left of fig. 3, the traditional detection-based method has quickly encountered
a saturation bottleneck as the amount of data increases. although our method
initially has poorer performance, it has shown an impressive growth trend."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"however, the similarity between tumors and background and also severe shadow
noises in ultrasound images make accurate segmentation of breast tumor chal-
lenging. in this paper, we propose a large pre-trained model for breast tumor seg-
mentation, with robust performance when applied to new datasets. speciﬁcally,
our model is built upon unet backbone with deep supervision for each stage of
the decoder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"besides using dice score, we also design discriminator-based loss
on each stage of the decoder to penalize the distribution dissimilarity from multi-
scales. our proposed model is validated on a large clinical dataset with more than
10000 cases, and shows signiﬁcant improvement than other representative models. besides, we apply our large pretrained model to two public datasets without ﬁne
tuning, and obtain extremely good results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"[14] raised a deeply-supervised encoder-decoder network, which is connected
through a series of nested and dense skip pathways to reduce semantic gap between fea-
ture maps. in [15], a multi-scale selection and multi-channel fusion segmentation model
was built, which gathers global information from multiple receptive ﬁelds and integrates
multi-level features from different network positions for accurate pancreas segmenta-
tion. oktay et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"pei et al. [19] introduced channel and position attention
module into deep learning neural network to obtain contextual information for colorec-
tal tumors segmentation in ct scans. however, although these proposed models have
achieved satisfactory results in different medical segmentation tasks, their performances
are limited for breast tumor segmentation in ultrasound images due to the low image
contrast and blurry tissue boundary."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"to address these challenges, we present, to the best of our knowledge, the ﬁrst
work to adopt multi-scale features collected from large set of clinical ultrasound images
for breast tumor segmentation. the main contributions of our work are as follows: (1)
we propose a well-pruned simple but effective network for breast tumor segmentation,
which shows remarkable and solid performance on large clinical dataset; (2) our large
pretrained model is evaluated on two additional public datasets without ﬁne-tuning and
shows extremely stabilized improvement, indicating that our model has outstanding
generalizability and good robustness against multi-site data data. 2
method
we demonstrate the architecture of our proposed network in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"it is based on the
unet [20] backbone. in order to collect multi-scale rich information for tumor tissues,
we propose to use gan [21]-based multi-scale deep supervision. in particular, we apply
similarity constraint for each stage of the unet decoder to obtain consistent and sta-
ble segmentation maps."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"the ﬁrst constraint is used to enhance prediction
similarity to the standard ones, for preliminary segmentation. the second constraint is used to
capture data distribution to maintain consistency in high-dimensional space for map reﬁnement. (2)
where pcn(truth) denotes the distribution of original samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"cnθcn(g) represents the
probability for the input of cn coming from the original dataset.
intheimplementation,weupdatethesegmentationnetworkandallthediscriminators
alternatingly in each iteration until both the generator and discriminators are converged. 92
m. li et al.
3
experiments
3.1
dataset and implementation details
we collected 10927 cases for this research from yunnan cancer hospital. each scan is
with resolution of 1 × 1 mm2 and size of 512 × 480."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"the breast tumors of each case
are delineated by three experienced experts. five-fold cross validation is performed on
the dataset in all experiments to verify our proposed network. in order to com-
prehensively evaluate segmentation efﬁciency of our model, dice similarity coefﬁcient
(dsc), precision, recall, jaccard, and root mean squared error (rmse) are used as
evaluation metrics in this work."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"we use adam optimizer to train the framework with an
initial learning rate of 10−4. the comparison exper-
iments are carried on a large-scale clinical breast ultrasound dataset, and the quantitative
results are reported in table 1. it is obvious that our proposed model achieves the opti-
mal performance compared with other segmentation models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"2.
the probability maps predicted by our model are more consistent with the ground truth,
especially in the tiny structures which are difﬁcult to capture. this veriﬁes the superior
ability of the proposed model in maintaining detailed edge information compared with
state-of-the-art methods.
fig. 2. segmentation results of ﬁve subjects obtained from different models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"four
groups of frameworks (stage i, stage ii, stage iii and stage iv) are designed, with the
94
m. li et al.
numerals denoting the level of deep supervision counting from the last deconvolutional
layer. we test these four frameworks on the in-house breast ultrasound dataset, and verify
their segmentation performance using the same ﬁve evaluation criteria. the evaluation
metrics from all cases are presented by the scatter plots in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_9.pdf,"all these comparison results verify the superiority
of deep supervision for breast tumor segmentation in ultrasound images. speciﬁcally, dataset 1 and
dataset 2 are used as testing data to evaluate the generalization performance of the
models trained on our own dataset without ﬁne tuning, and the corresponding results
are shown in table 3. promising performance demonstrates outstanding generalization
ability of our large pre-trained model, with a dsc score of 81.35% and a recall of
80.96% on dataset 1, and a dsc score of 77.16% and a recall of 93.22% on dataset 2.
table 3."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"in the reverse process, it removes the noise from the orig-
inal gaussian noise in multiple steps with the well-trained noise predictor and
ﬁnally outputs the predicted dose distribution map. to ensure the accuracy of the
prediction, we further design a structure encoder to extract anatomical information
from patient anatomy images and enable the noise predictor to be aware of the
dose constraints within several essential organs, i.e., the planning target volume
and organs at risk. extensive experiments on an in-house dataset with 130 rectum
cancer patients demonstrate the superiority of our method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"deep
learning
1
introduction
radiotherapy, one of the mainstream treatments for cancer patients, has gained notable
advancements in past decades. for promising curative effect, a high-quality radiotherapy
plan is demanded to distribute sufﬁcient dose of radiation to the planning target volume
(ptv) while minimizing the radiation hazard to organs at risk (oars). to achieve this,
radiotherapy plans need to be manually adjusted by the dosimetrists in a trial-and-error
manner, which is extremely labor-intensive and time-consuming [1, 2]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"besides the above unet-
based frameworks, song et al. [16] to excavate con-
textual information from different scales, thus obtaining accuracy improvements in the
dose prediction of rectum cancer. mahmood et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"following the idea
of multi-task learning, tan et al. [8] utilized isodose line and gradient information to
promote the performance of dose prediction of rectum cancer. to ease the burden on the
delineation of ptv and oars, li et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"these
high-frequency features formed by ray penetration reveal the ray directions and dose
attenuation with the aim of killing the cancer cells while protecting the oars as much
as possible, which are critical for radiotherapy. consequently, exploring an automatic
method to generate high-quality predictions with rich high-frequency information is
important to improve the performance of dose prediction.
fig. 1. instances from a rectum cancer patient."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"[20] has veriﬁed its remarkable potential in modeling
complex image distributions in some vision tasks [21–23]. unlike other dl models, the
diffusion model is trained without any extra assumption about target data distribution,
thus evading the average effect and alleviating the over-smoothing problem [24]. figure 1
(4) provides an example in which the diffusion-based model predicts a dose map with
shaper and clearer boundaries of ray-penetrated areas. therefore, introducing a diffusion
model to the dose prediction task is a worthwhile endeavor."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"in this
procedure, a noise predictor is trained to predict the noise added in the corresponding step
of the forward process. to further ensure the accuracy of the predicted dose distribution
for both the ptv and oars, we design a dl-based structure encoder to extract the
anatomical information from the ct image and the segmentation masks of the ptv and
oars. such anatomical information can indicate the structure and relative position of
organs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"overall, the contributions of this paper can be concluded as follows: (1) we propose
a novel diffusion-based model for dose prediction in cancer radiotherapy to address
the over-smoothing issue commonly encountered in existing dl-based dose prediction
methods. to the best of our knowledge, we are the ﬁrst to introduce the diffusion model
for this task. (2) we introduce a structure encoder to extract the anatomical information
available in the ct images and organ segmentation masks, and exploit the anatomical
information to guide the noise predictor in the diffusion model towards generating more
precisepredictions.(3)theproposeddiffdpisextensivelyevaluatedonaclinicaldataset
consisting of 130 rectum cancer patients, and the results demonstrate that our approach
outperforms other state-of-the-art methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"meanwhile, y ∈ rh×w×1 is the corresponding dose
distribution map for x. concretely, the forward process produces a sequence of noisy
images {y0, y1, . . . , yt}, y0 = y by gradually adding a small amount of noise to y in
t steps with the noise increased at each step and a noise predictor f is constructed
to predict the noise added to yt−1 by treating yt, anatomic information from x and
embedding of step t as input. to obtain the anatomic information, a structure encoder
g is designed to extract the crucial feature representations from the structure images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"[25] which contains a forward process and a reverse process. by uti-
lizing both processes, the diffdp model can progressively transform the gaussian noise
into complex data distribution. in the forward process, the diffdp model employs the markov chain
to progressively add noise to the initial dose distribution map y0 ∼ q(y0) until the
ﬁnal disturbed image yt becomes completely gaussian noise which is represented as
yt ∼ n(yt | 0, i)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"− αt
√1 − γt
εt,θ

+

1 − αtzt,
(8)
where zt ∼ n(0, i) is a random noise sampled from normal gaussian distribution. more
derivation processes can be found in the original paper of diffusion model [25].
2.2
structure encoder
vanilla diffusion model has difﬁculty preserving essential structural information and
produce unstable results when predicting dose distribution maps directly from noise
with a simple condition mechanism. to address this, we design a structure encoder g
that effectively extracts the anatomical information from the structure images guiding
the noise predictor to generate more accurate dose maps by incorporating extracted
structural knowledge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"firstly, yt is encoded
into feature maps through a convolutional layer. then, these two feature maps are fused
by element-wise addition, allowing the structure information in x to be transferred to the
noise predictor. the following two down-sampling operations retain the addition opera-
tion to complete information fusion, while the last three use a cross-attention mechanism
to gain similarity-based structure guidance at deeper levels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"the parameter t is set to 1000. additionally,
the noise intensity is initialized to 1e−2 and decayed to 1e−4 linearly along with the
increase of steps.
3
experiments and results
dataset and evaluations. we measure the performance of our model on an in-house
rectum cancer dataset which contains 130 patients who underwent volumetric modulated
arc therapy (vmat) treatment at west china hospital."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_19.pdf,"furthermore, we display the visualization comparison in fig. 4. as we can see, the
proposed model achieves the best visual quality with clearer and sharper high-frequency
details (as indicated by red arrows). furthermore, the error map of the proposed is the
darkest, suggesting the least disparity compared with the ground truth."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"at the same time, our proposed method is partially guided by ran-
domly synthesized abnormalities. we conduct experiments on three pub-
lic and one in-house dataset, and demonstrate that our method outper-
forms existing methods in abnormality classiﬁcation, segmentation, and
localization tasks. additionally, reconstructed normal mammograms can
provide insights toward better interpretable visual cues for clinical diag-
nosis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"developing artiﬁcial intelligence (ai) for abnor-
mality detection is of great signiﬁcance for reducing the workload of radiologists
and facilitating early diagnosis [21]. besides using the data-driven manner, to
achieve accurate diagnosis and interpretation of the ai-assisted system output,
it is essential to consider mammogram domain knowledge in a model-driven
fashion. [12], the asymmetry of bilateral
breasts is a crucial clinical factor for identifying abnormalities."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"the question of “what the bi-mg would look like if they were symmetric?”
is often considered when radiologists determine the symmetry of bi-mg. it can
provide valuable diagnostic information and guide the model in learning the
diagnostic process akin to that of a human radiologist. none of these studies is able to reconstruct a normal pixel-
level symmetric breast in the model design."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"(2) we propose synthesis to simulate the asymmetry using normal
pair of views to guide the model for providing normal symmetric breast views
and to indicate the abnormal regions in an accurate supervised fashion. (3) we
demonstrate the robustness of our approach on four mammogram datasets for
classiﬁcation, segmentation, and localization tasks. 2
methodology
in this study, the “asymmetric” refers to the visual diﬀerences on perception
level that can arise between the left and right breasts due to any abnormality,
including both benign and malignant lesions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"here, x ∈
rh×w represents a mammogram with the size of h × w, xr and xl correspond
to the right and left view respectively. yr, yl, yasy ∈ {0, 1} are binary labels,
indicating abnormality for each side, and the asymmetry of paired bi-mg. the
framework of our disasymnet is illustrated in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"then the features are fed into the ψasyt. unlike other mv transformer methods [1,16] that use only cross-attention
(ca), our asymmetric transformer employs self-attention (sa) and ca in par-
allel to aggregate information from both self and contralateral sides to enhance
the side-by-side comparison. this is motivated by the fact that radiologists com-
monly combine unilateral (identifying focal suspicious regions according to tex-
ture, shape, and margin) and bilateral analyses (comparing them with symmet-
ric regions in the contralateral breasts) to detect abnormalities in mammog-
raphy [6]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"while in the ca, we replace the key and value
vectors with those from the contralateral features, f l
ca = ψh=8
mha(f l
q, f r
k, f r
v ) or
f r
ca = ψh=8
mha(f r
q, f l
k, f l
v ). then, the starting feature f, and the attention fea-
tures fsa and fca are concatenated in the channel dimension and fed into the
ffn layers to fuse the information and maintain the same size as f. the trans-
former block is repeated n = 12 times to iteratively integrate information from
bi-mg, resulting in the output feature f r
out, f l
out = ψn=12
asyt (f r, f l). to predict the abnormal probability ˆy of each side, the output features
fout are fed into the abnormal classiﬁer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"some examples are shown in fig. the tumor set t is
collected from real-world datasets. speciﬁcally, to maintain the rule of weakly-
supervised learning of segmentation and localization tasks, we collect the tumors
from the ddsm dataset as t and train the model on the inbreast dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"the loss of the second objective is lrefine as aforementioned. 3
experimental
3.1
datasets
this study reports experiments on four mammography datasets. the inbreast
dataset [7] consists of 115 exams with bi-rads labels and pixel-wise anno-
62
x. wang et al.
tations, comprising a total of 87 normal (bi-rads = 1) and 342 abnormal
(bi-rads ̸= 1) images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"the ddsm dataset [3] consists of 2,620 cases, encom-
passing 6,406 normal and 4,042 (benign and malignant) images with outlines
generated by an experienced mammographer. the vindr-mammo dataset [8]
includes 5,000 cases with bi-rads assessments and bounding box annotations,
consisting of 13,404 normal (bi-rads = 1) and 6,580 abnormal (bi-rads
̸= 1) images. the in-house dataset comprises 43,258 mammography exams from
10,670 women between 2004–2020, collected from a hospital with irb approvals."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"in this study, we randomly select 20% women of the full dataset, comprising 6,000
normal (bi-rads = 1) and 28,732 abnormal (bi-rads ̸= 1) images. due to a
lack of annotations, the in-house dataset is only utilized for classiﬁcation tasks. each dataset is randomly split into training, validation, and testing sets at the
patient level in an 8:1:1 ratio, respectively (except for that inbreast which is
split with a ratio of 6:2:2, to keep enough normal samples for the test)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"then we standardize the image size to 1024 × 512 pixels. for training models,
we employ random zooming and random cropping for data augmentation. we
employ the resnet-18 [2] with on imagenet pre-trained weights as the common
backbone for all methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"the adam optimizer is utilized with an initial learning
rate (lr) of 0.0001, and a batch size of 8. the training process on the inbreast
dataset is conducted for 50 epochs with a lr decay of 0.1 every 20 epochs. for
the other three datasets, the training is conducted separately on each one with 20
epochs and a lr decay of 0.1 per 10 epochs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"all experiments are implemented in
the pytorch framework and an nvidia rtx a6000 gpu (48 gb). the training
takes 3–24 h (related to the size of the dataset) on each dataset.
to assess the performance of diﬀerent models in classiﬁcation tasks, we
calculate the area under the receiver operating characteristic curve (auc) met-
ric. the 95% conﬁdence interval (ci) of auc is estimated using bootstrapping
(1,000 times) for each measure."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"the features from the bi-mg are simply concatenated and passed to
the classiﬁer.
comparison of performance in diﬀerent tasks: for the classiﬁcation
task, the auc results of abnormal classiﬁcation are shown in table 1. our
method outperforms all the single-based and mv-based methods in these classi-
ﬁcation tasks across all datasets. furthermore, the ablation studies demonstrate
the eﬀectiveness of each proposed model component."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"additionally,
our “asyd” only method improves the performance compared to the late-fusion
method, demonstrating that our disentanglement-based self-adversarial learn-
ing strategy can reﬁne classiﬁers and enhance the model’s ability to classify
anomalies and asymmetries. the proposed “synthesis” method further enhances
64
x. wang et al.
table 2. comparison of weakly supervised abnormalities segmentation and localization
tasks on public datasets. 3. eight representative visualizations of normal mammogram reconstruction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_6.pdf,"(color ﬁgure online)
the performance of our proposed method. moreover, we investigate the abil-
ity of diﬀerent methods to classify abnormalities under various percentages
of ddsm, vindr, and in-house datasets. the inbreast dataset was excluded
from this experiment due to its small size."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"however, the diﬃculty in accessing
complete temporal sequences, especially post-contrast images, hinders
segmentation performance, generalization ability and clinical application
of existing methods. in this work, we propose a diﬀusion kinetic model
(dkm) that implicitly exploits hemodynamic priors in dce-mri and
eﬀectively generates high-quality segmentation maps only requiring pre-
contrast images. we speciﬁcally consider the underlying relation between
hemodynamic response function (hrf) and denoising diﬀusion process
(ddp), which displays remarkable results for realistic image generation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"once the dm is pretrained, the latent code estimated
from the dm is simply incorporated into the sm, which enables dkm to
automatically and accurately annotate cancers with pre-contrast images. to our best knowledge, this is the ﬁrst work exploring the relationship
between hrf and ddp for dynamic mri segmentation. we evaluate
the proposed method for tumor segmentation on public breast cancer
dce-mri dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"the source code will be available
on https://github.com/medical-ai-lab-of-jnu/dkm. keywords: deep learning · kinetic representation · dce-mri ·
cancer segmentation · denoising diﬀusion model
1
introduction
dynamic contrast-enhanced magnetic resonance imaging (dce-mri) reveal-
ing tumor hemodynamics information is often applied to early diagnosis and
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43901-8_10
diﬀusion kinetic model for cancer segmentation
101
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"speciﬁcally, baranchuk et al. [9] explores the intermediate activations
from the networks that perform the markov step of the reverse diﬀusion process
and ﬁnd these activations can capture semantic information for segmentation. however, the applicability of ddpm to medical image segmentation are still
limited."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"by designing a net-
work architecture to eﬀectively transmute pre-contrast images into post-contrast
images, the network should acquire hemodynamic inherent in hrf that can be
used to improve segmentation performance. inspired by the fact that ddpm
generates images from noisy input provided by the parameterized gaussian pro-
cess, this work aims to exploit implicit hemodynamic information by a diﬀusion
process that predict post-contrast images from noisy pre-contrast images. specif-
ically, given the pre-contrast and post-contrast images, the latent kinetic code is
learned using a score function of ddpm, which contains suﬃcient hemodynamic
characteristics to facilitate segmentation performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"to verify the eﬀectiveness of the latent kinetic code, the
sm adopts a simple u-net-like structure, with an encoder to simultaneously
conduct semantic feature encoding and kinetic code fusion, along with a decoder
to obtain voxel-level classiﬁcation. in this manner, our latent kinetic code can
be interpreted to provide tic information and hemodynamic characteristics for
accurate cancer segmentation. compared to the existing state-of-the-art approaches with complete
sequences, our method yields higher segmentation performance even with pre-
contrast images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"in summary, the main contributions of this work are listed as
follows:
• we propose a diﬀusion kinetic model that implicitly exploits hemodynamic
priors in dce-mri and eﬀectively generates high-quality segmentation maps
only requiring pre-contrast images. • we ﬁrst consider the underlying relation between hemodynamic response
function and denoising diﬀusion process and provide a ddpm-based solu-
tion to capture a latent kinetic code for hemodynamic knowledge. • compared to the existing approaches with complete sequences, the proposed
method yields higher cancer segmentation performance even with pre-contrast
images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"once the dm is trained, the learned kinetic code
diﬀusion kinetic model for cancer segmentation
103
fig. 2. illustration of our method for implicitly exploiting hemodynamic information
from pre-contrast images. the combination of learned kinetic code is an example.
is incorporated into the sm as hemodynamic priors to guide the segmentation
process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"2.1
diﬀusion module
the diﬀusion module is following the denoising diﬀusion probabilistic model
[8,14]. based on the consideration from nonequilibrium thermodynamics, ddpm
approximates the data distribution by learning a markov chain process which
originates from the gaussian distribution. = n(xt;

1 − βtxt−1, βti)
(1)
particularly, a noisy image xt can be directly obtained from the data x0:
q(xt|x0) := n(xt; √¯αtx0, (1 − ¯αt)i)
(2)
where αt := 1 − βt and ¯αt := t
s=1 αs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"formally, a noisy sample can be acquired by:
xt = √¯αtx0 +
√
1 − ¯αtϵ, ϵ ∼ n(0, i)
(4)
where αt := 1−βt and ¯αt := t
s=1 αs. next, we employ the reverse diﬀusion pro-
cess to transform the noisy sample xt to the post-contrast data xk. as thus, the
dm gradually exploits the latent kinetic code by comparing the pre-contrast and
post-contrast images, which contains hemodynamic knowledge for segmentation.
2.2
segmentation module
once pretrained, the dm outputs multi-scale latent kinetic code fdm from inter-
mediate layers, which is fed into the sm to guide cancer segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"to this end, a linear transformation, parametrized by
a weight matrix w, is applied to the latent code fdm, followed by a batch nor-
malization, relu activation layer and concatenation, which can be represented
as follows:
ˆf = c(φ(bn(w ∗ fdm); fsm)
(5)
where ∗ represents 1 × 1 based convolution operation, w is the weight matrix,
bn represents batch normalization, φ represents relu activation function and
c is concatenation operation. in this way, the hemodynamic knowledge can be
incorporated into the sm to capture more expressive representations to improve
segmentation performance. 2.3
model training
to maintain training stability, the proposed dkm adopts a two-step training pro-
cedure for cancer annotation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"in the ﬁrst step, the dm is trained to transform
pre-contrast images into post-contrast images for a latent space where hemo-
dynamic priors are exploited. in particular, the diﬀusion loss for the reverse
diﬀusion process can be formulated as follows:
ldm = et,ϵ,x||ϵθ(xt, t; x0, xk) − ϵ||2
(6)
where ϵθ represents the denoising model that employs an u-net structure, x0
and xk are the pre-contrast and post-contrast images, respectively, ϵ is gaussian
distribution data ∼ n(0, i), and t is a timestep. for a second step, we train the sm that integrates the previously learned
latent kinetic code to provide tumor hemodynamic information for voxel-level
diﬀusion kinetic model for cancer segmentation
105
prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"each mr volume consists of 60 slices
and the size of each slice is 256×256. regarding preprocessing, we conduct zero-
mean unit-variance intensity normalization for the whole volume. we divided
the original dataset into training (70%) and test set (30%) based on the scans."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"ground truth segmentations of the data are provided in the dataset for tumor
annotation. no data augmentation techniques are used to ensure fairness. all approaches are evaluated using 1) dice
similarity coeﬃcient (dsc) and 2) jaccard index (ji)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"the scans are employed for testing. [5]
complete sequence 64.4 ± 2.4
51.5 ± 2.6
dkm (ours)
pre-contrast
71.5 ± 2.5 58.5 ± 2.6
512, 1024 channels for each stage in the sm to capture expressive and suﬃ-
cient semantic information. the sm is optimized by adam with a learning rate
2 × 10−5 and a weight decay 10−6."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"the model is trained for 500 epochs with the
batch size to 1. no data augmentation techniques are used to ensure fairness. imental results demonstrate that the proposed method comprehensively other
models with less scans (i.e., pre-contrast) in testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"our method outperform the hybridnet by 7.1% and 7.0% in
dsc and ji, respectively. it probably due to two aspects: 1) the hemodynamic
knowledge is implicitly exploited by diﬀusion module from pre-contrast images,
which is useful for cancer segmentation. 2) the intermediate activations from
diﬀusion kinetic model for cancer segmentation
107
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"in a word, the proposed framework can produce accurate prediction masks only
requiring pre-contrast images. this is useful when post-contrast data is limited. ablation study: to explore the eﬀectiveness of the latent kinetic code, we ﬁrst
conduct ablation studies to select the optimal setting."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"we attribute it to the denoising diﬀusion model that receives the
noisy input, leading to the noise of shallow features. in contrast, the deep fea-
tures capture essential characteristics to reveal the structural information and
hemodynamic changes of tumors. figure 4 shows visual comparison of segmen-
tation performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_10.pdf,"the above results reveal that incorporation of kinetic code
comfortably outperform the baseline without hemodynamic information. 4
conclusion
we propose a diﬀusion kinetic model by exploiting hemodynamic priors in dce-
mri to eﬀectively generate high-quality segmentation results only requiring pre-
contrast images. our models learns the hemodynamic response function based on
the denoising diﬀusion process and estimates the latent kinetic code to guide the
segmentation task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"although fully-
supervised deep learning-based methods have made signiﬁcant progress,
a large number of labeled images are required to achieve great segmenta-
tion performance. considering that manually labeling all nuclei instances
for a dataset is ineﬃcient, obtaining a large-scale human-annotated
dataset is time-consuming and labor-intensive. therefore, augmenting a
dataset with only a few labeled images to improve the segmentation per-
formance is of signiﬁcant research and application value."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"in the second
step, we train a conditioned diﬀusion model to synthesize histopathology
images based on nuclei structures. the synthetic histopathology images
paired with synthetic instance maps will be added to the real dataset for
this work is supported by chinese key-area research and development program of
guangdong province (2020b0101350001), and the guangdong basic and applied basic
research foundation (2023a1515011464, 2020b1515020048), and the national natural
science foundation of china (no. 62102267, no. 61976250), and the shenzhen science
and technology program (jcyj20220818103001002, jcyj20220530141211024), and
the guangdong provincial key laboratory of big data computing, the chinese uni-
versity of hong kong, shenzhen. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3 57."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14227, pp. https://doi.org/10.1007/978-3-031-43993-3_57
diﬀusion-based data augmentation for nuclei image segmentation
593
training the segmentation model. the experimental results show that by
augmenting 10% labeled real dataset with synthetic samples, one can
achieve comparable segmentation results with the fully-supervised base-
line."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"most of these methods
are fully-supervised so the great segmentation performance usually relies on a
large number of labeled images. however, manually labeling the pixels belonging
to all nucleus boundaries in an image is time-consuming and requires domain
knowledge. in practice, it is hard to obtain an amount of histopathology images
with dense pixel-wise annotations but feasible to collect a few labeled images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"a question is raised naturally: can we expand the training dataset with a small
proportion of images labeled to reach or even exceed the segmentation perfor-
mance of the fully-supervised baseline? intuitively, since the labeled images are
samples from the population of histopathology images, if the underlying distri-
bution of histopathology images is learned, one can generate inﬁnite images and
their pixel-level labels to augment the original dataset. therefore, it is demanded
to develop a tool that is capable of learning distributions and generating new
paired samples for segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"specially, a newly proposed gan-based
method can synthesize labeled histopathology image for nuclei segmentation
[21]. while gans are able to generate high quality images, they are known
for unstable training and lack of diversity in generation due to the adversarial
training strategy. [8] tend to overshadow gans."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"on the testing stage,
the nuclei structures and the corresponding images are generated successively
by the two models. as far as our knowledge, we are the ﬁrst to apply diﬀusion
models on histopathology image augmentation for nuclei segmentation. our contributions are: (1) a diﬀusion-based data augmentation framework
that can generate histopathology images and their segmentation labels from
scratch; (2) an unconditional nuclei structure synthesis model and a condi-
tional histopathology image synthesis model; (3) experiments show that with
our method, by augmenting only 10% labeled training data, one can obtain
segmentation results comparable to the fully-supervised baseline."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"denote a true nuclei structure as y0, which is sampled from real distribution
q(y). to maximize data likelihood, the diﬀusion model deﬁnes a forward and
a reverse process. in the forward process, small amount of gaussian noise are
successively added to the sample y0 in t steps by:
yt =

1 − βtyt−1 +

βtϵt−1, t = 1, ..., t,
(1)
where ϵt ∼ n(0, i) and {βt ∈ (0, 1)}t
t=1 is a variance schedule."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"there are usually two ways to synthesize images constrained
596
x. yu et al.
fig. 2. the proposed diﬀusion-based data augmentation framework. we ﬁrst generate
a nuclei structure with an unconditional diﬀusion model, and then generate images
conditioned on the nuclei structure."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"the two mod-
els can be learned with one neural network. speciﬁcally, pθ(x|y) is trained on
paired data (x0, y0) and pθ(x) can be trained by randomly discarding y (i.e.
y = ∅) with a certain drop rate ∈ (0, 1) so that the model learns uncondi-
tional and conditional generation simultaneously. the noise predictor ϵ′
θ(xt, t, y)
of classiﬁer-free guidance is a combination of the above two predictors:
ϵ′
θ(xt, t, y) = (w + 1)ϵθ(xt, t, y) − wϵθ(xt, t),
(6)
where ϵθ(xt, t) = ϵθ(xt, t, y = ∅), w is a scalar controlling the strength of
classiﬁer-free guidance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"since nuclei
structures and histopathology images have diﬀerent feature spaces, simply con-
catenating or passing them through a cross-attention module [7,15,17] before
entering the u-net will degrade image ﬁdelity and yield unclear correspondence
between synthetic nuclei image and its nuclei structure. inspired by [28], we
embed information of the nuclei structure into feature maps of nuclei image
by the spatially-adaptive normalization (spade) module [25]. in other words,
the spatial and morphological information of nuclei modulates the normalized
diﬀusion-based data augmentation for nuclei image segmentation
597
feature maps such that the nuclei are generated in the right places while the
background is left to be created freely."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"each condresblock consists of spade-
silu-conv which takes both feature map and nuclei structure as inputs. 3
experiments and results
3.1
implementation details
datasets. the monuseg dataset has 44 labeled images of size 1000 × 1000, 30 for
training and 14 for testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"the kumar dataset consists of 30 1000×1000 labeled
images from seven organs of the cancer genome atlas (tcga) database. the
dataset is splited into 16 training images and 14 testing images. paired sample synthesis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"the batch size is set to
be 1. for the diﬀusion process of both steps, we set the total diﬀusion timestep
t to 1000 with a linear variance schedule {β1, ..., βt } following [8].
for monuseg dataset, we generate 512/512/512/1024 synthetic samples for
10%/20%/50%/100% labeled subsets; for kumar dataset, 256/256/256/512 syn-
thetic samples are generated for 10%/20%/50%/100% labeled subsets. the syn-
thetic nuclei structures are generate by the nuclei structure synthesis network
and the corresponding images are generated by the histopathology image synthe-
sis network with the classiﬁer-free guidance scale w = 2."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"3. visualization of synthetic samples. the ﬁrst and second row show selected
patches and corresponding nuclei structures from the 10% labeled subset of monuseg
dataset. the third and fourth row show selected synthetic images and corresponding
nuclei with similar style as the real one in the same column."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"third,
the synthetic nuclei structures and images show great diversity: the synthetic
samples resemble diﬀerent styles of the real ones but with apparent diﬀerences. we then train segmentation models on the four labeled subsets of monuseg
and kumar dataset and corresponding augmented subsets with both real and
synthetic labeled images. with a speciﬁc labeling proportion, say 10%, we name
diﬀusion-based data augmentation for nuclei image segmentation
599
the original subset as 10% labeled subset and the augmented on as 10% aug-
mented subset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"table 1 show the segmentation performances with hover-net. for monuseg
dataset, it is clear that the segmentation metrics drop with fewer labeled images. for example, with only 10% labeled images, dice and aji reduce by 2.4% and
3.1%, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"note that the metrics of 10% augmented subset
are higher than those of 20% augmented subset, which might be attributed to
the indetermination of the diﬀusion model training and sampling. interestingly,
augmenting the full dataset also helps: dice increases by 1.3% and aji increases
by 1.6% compared with the original full dataset. therefore, the proposed aug-
mentation method consistently improves segmentation performance of diﬀerent
labeling proportion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"for kumar dataset, by augmenting 10% labeled subset,
aji increases to a level comparable with that using 100% labeled images; by
augmenting 20% and 50% labeled subset, ajis exceed the fully-supervised base-
line. these results demonstrate the eﬀectiveness of the proposed augmentation
method that we can achieve the same or higher level segmentation performance
of the fully-supervised baseline by augmenting a dataset with a small amount of
labeled images. generalization of the proposed data augmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_57.pdf,"table 2
shows the segmentation results with pff-net. for both monuseg and kumar
datasets, all the four labeling proportions metrics notably improve with synthetic
samples. this indicates the generalization of our proposed augmentation method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_50.pdf,"this is performed to
guide radiation to the target and to spare oar from inappropriate irradia-
tion. hence, this manual segmentation step is very time-consuming and must
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43996-4 50. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14228, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_50.pdf,"our contributions are: (i) a dosimetry-aware training loss function
for dl segmentation models, which (ii) yields improved model robustness, and
(iii) leads to improved and safer dosimetry maps. we present results on a clin-
ical dataset comprising ﬁfty post-operative glioblastoma (gbm) patients. in
addition, we report results comparing the proposed loss function, called dose-
segmentation loss (doselo), with models trained with a combination of
binary cross-entropy (bce) and softdice loss functions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_50.pdf,"we refer the reader to [9,12] for further implementation
details. we remark that the dose predictor model was also trained with data
augmentation, so imperfect segmentation masks and corresponding dose plans
are included. this allows us in this study to use the dose predictor to model the
interplay between segmentation variability and dosimetric changes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_50.pdf,"the ﬁnal loss is then,
ltotal = lbce + λldsl,
(4)
dose guidance for radiotherapy-oriented deep learning segmentation
529
where λ is a hyperparameter to weigh the contributions of each loss term. we
remark that during training we use standard data augmentations including spa-
tial transformations, which are also subjected to dose predictions, so the model is
informed about relevant segmentation variations producing dosimetry changes.
3
experiments and results
3.1
data and model training
we divide the descriptions of the two separate datasets used for the dose pre-
diction and segmentation models. dose prediction: the dose prediction model was trained on an in-house
dataset comprising a total of 50 subjects diagnosed with post-operative gbm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_50.pdf,"this reference was generated
on basis of a double arc co-planar vmat plan to deliver 30 times 2 gy while
maximally sparing oars. we divided the dataset into training (35 cases), val-
idation (5 cases), and testing (10 cases). we refer the reader to [9] for further
details.
segmentation models:
to develop and test the proposed approach, we
employed a separate in-house dataset (i.e., diﬀerent cases than those used to
train the dose predictor model) of 50 cases from post-operative gmb patients
receiving standard rt treatment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_50.pdf,"2 for which the standard
bce+softdice was slightly better than the proposed doselo. for case no. 3
the tumor presents a non-convex shape alongside the skull’s parietal lobe, which
was not adequately modeled by the training dataset used to train the segmen-
tation models. indeed, we remark that both models failed to yield acceptable
segmentation quality in this area."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_50.pdf,"case no. 5 (shown in supple-
mentary material) is an interesting case called butterﬂy gbm, which is a rare
type of gbm (around 2% of all gbm cases [3]), characterized by bihemispheric
involvement and invasion of the corpus callosum. in this case, the training data
also lacked characterization for such cases. despite this limitation, we observed
favorable dose distributions with the proposed method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_50.pdf,"as pointed out by [17], segmentation outliers can have a
detrimental eﬀect on rt planning. we also remark that the range of hd values is
in range with values reported by models trained using much more training data
(see [1]), alluding to the possibility that the problem of robustness might not be
directly solvable with more data. dice coeﬃcients did not deviate signiﬁcantly
between the baseline and the doselo models (dsc: 0.713 ± 0.203 (baseline)
vs. 0.697 ± 0.216 (doselo))."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"to overcome these limitations, this article proposes a new app-
roach called dynamic curriculum learning via in-domain uncertainty
(dclu), which is derived from uncertainty estimation. the proposed
approach utilizes a dirichlet distribution classiﬁer to obtain prediction
and uncertainty estimates from the network, which can be used as a met-
ric to quantify the diﬃculty level of the data. an uncertainty-aware sam-
pling pacing function is also introduced to adapt the curriculum accord-
ing to the diﬃculty metric."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"the source code for this approach
will be released at https://github.com/joey2117/dclu. keywords: medical image classiﬁcation · curriculum learning ·
uncertainty estimation
1
introduction
curriculum learning methods in deep learning are inspired by human educa-
tion and involve structuring the training data from easy to hard to teach net-
works progressively. however, developing a good diﬃculty metric or measurer
for curriculum learning is a challenge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"recently, there are two main categories of
approaches to designing the diﬃculty metrics. the ﬁrst is that human experts
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 72.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp. https://doi.org/10.1007/978-3-031-43904-9_72
748
c. li et al.
quantify data diﬃculty based on data characteristics such as complexity [19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"diﬃculty measur-
ers based on loss function tend to select samples with small training losses as
a priority to train the model. however, this type of approach suﬀers from the
uncertainty of quantifying the diﬃculty of data due to insuﬃcient training in
the early stages. in addition, while deep learning models have achieved impres-
sive performance in the medical image analysis ﬁeld, there remain challenges in
measuring and developing a model with low in-domain uncertainty."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"the pipeline of our dclu with the dynamic diﬃculty measurer (ddm) and
uncertainty-aware sampling pacing function (uas). first, ddm provides both uncer-
tainty estimates and predictions for all training data simultaneously in every iteration. then, based on uncertainty estimates, we use uas to sort all data from easy to hard
and select easier samples to update the parameters of the network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"we propose a new approach to address the challenges of curriculum learn-
ing, which we call dynamic curriculum learning via in-domain uncertainty
(dclu). our approach is motivated by two key observations: 1) sample diﬃ-
culty is inﬂuenced by both the complexity of the data and the model’s inability
dclu for medical image classiﬁcation
749
to explain data, related to in-domain uncertainty, and 2) reducing in-domain
uncertainty by improving the learning process can boost model performance. to
estimate in-domain uncertainty, we use a dirichlet distribution classiﬁer, which
provides uncertainty estimates and predictions simultaneously."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"in particular, our dynamic diﬃculty measurer (ddm) generates uncertain-
ties and predictions for each image simultaneously. uncertainties reﬂect the dif-
ﬁculty and we use these as the criteria for data rearrangement. uncertainty
estimation runs at each iteration to ingest the feedback of the current network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"the full process of the
proposed method is shown in fig. we evaluate our method on two medical
image datasets isic 2018 task 3 and chest-xray8 (covid-19). results indicate
that our method outperforms other curriculum learning works."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"2
related work
in-domain uncertainty. in-domain uncertainty represents the uncertainty
associated with inputs extracted from a data distribution equivalent to the
training data distribution [6]. deep learning models can experience in-domain
uncertainty as they lack the necessary in-domain knowledge to interpret in-
domain samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"in-domain uncertainty is caused by two types of uncertainty:
data uncertainty and model uncertainty. data uncertainty represents the com-
plexity of data, which is related to noise and variations in observations [6]. on
the other hand, model uncertainty arises due to shortcomings of the model such
as a poor ﬁt to the training dataset or lack of knowledge [6].
in-domain uncertainty estimate."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"[14], which
750
c. li et al.
applied example-wise training loss at each iteration as the diﬃculty measurer. the drawback of spl is that some easier data appear all the time since spl
tended to select data with lower current losses. jiang et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"in
addition, kong et al. [12] presented a linear combination of the current model loss
and prior knowledge to adapt the diﬃculty measurer of the current model. in our
work, we want to build a dynamic diﬃculty measurer through the uncertainty
estimates generated by each iteration of the network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"this means our approach
does not require the prior knowledge provided by the pre-trained model and is
able to update the curricula during the training process. 3
method
3.1
overview
in dclu, we randomly input data into the dynamic diﬃculty measurer (ddm)
at the ﬁrst epoch to obtain the uncertainty for each data point. next, we apply
the uncertainty-aware sampling pacing function (uas) to present the training
data to the ddm in order of the ascending uncertainties to synchronously pro-
duce predictions and new uncertainty estimates."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"additionally, our pacing
function selects a fraction of easier samples and learns a parameter vector to
update the network. with the training process progressing, the proportion of
selected samples increases until it eventually comprises the entire dataset. the
pseudo-code for our method is given in the appendix.
3.2
dynamic diﬃculty measurer
the diﬃculty measurer is used to measure the diﬃculty of the data to decide
on the order of the training data, which is a crucial component of curriculum
learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"(3)
thus, ddm can generate both predictions and in-domain uncertainty estimates
for each sample simultaneously. to be speciﬁc, in-domain uncertainty estimates
include data and model uncertainty. [6]. inspired by this phenomenon, we employ in-domain uncertainty to measure
the diﬃculty of data at each iteration."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"the method can achieve a dynamic curriculum. 3.3
uncertainty-aware sampling pacing function
the pacing function is based on using the diﬃculty measurer to determine how
training examples (data) are fed into the network during the training process. our
dynamic diﬃculty measurer (ddm) can provide diﬃculty scores for all data at
each iteration."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"to address
this problem, we propose the uncertainty-aware sampling pacing function (uas)
consisting of two modules: the reorder and sampling modules. within the reorder
module, uas sorts all training data from easy to hard according to ascending
uncertainties from ddm at the last epoch and sends them into the network to
yield predictions and new uncertainty estimates. the sampling module speciﬁes
a fraction of easier samples to update to the network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"these
weights α will be applied to the objective function, which allows the parameters
learned from the speciﬁed examples to update the network. we increase the frac-
tion exponentially in each epoch until it eventually comprises the entire dataset. in our work, we implement uas through two approaches."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"firstly, uas (expo-
nential) incorporates both the reorder and sampling modules, prompting the net-
work to prioritize learning easier examples during the initial stages of training and
then gradually learn more diﬃcult examples. in each epoch, uas (exponential)
752
c. li et al.
ﬁrst utilizes the reorder module to sort all training data from easy to hard. then,
it applies the sampling module to select a fraction of easier examples to update
the network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,", λt =
min(1, t/50) is the annealing parameter, t is the current training epoch and w
is weights of the network. 4
experiment
4.1
dataset and experimental setup
dataset. [18]. isic 2018 task
3 dataset has 10,015 training images and 194 images for validation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"it aims to
classify 7 skin cancer types including melanoma, melanocytic nevus, basal cell
carcinoma, actinic keratosis, benign keratosis dermatoﬁbroma and vascular
lesion. we employ training and validation data from
isic 2018 task 3 dataset as our training and test sets. chest-xray 8 (covid-
19) dataset is randomly divided into two parts, with 80% of the images being
the training set and 20% of the images being the test set.
evaluation metrics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"the total epoch is 50.
4.2
experimental results
comparison with state-of-the-art. [12] adjusts the diﬃculty measurer by
incorporating the prior knowledge and feedback from the current model. the
results in table 1 show that our method (ddm) with uas (full) outperforms
both datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"our method with uas (exponential) performs better than other
methods on chest-xray 8 (covid-19) and has the second best performance
on isic 2018 task 3. [12]
83.17
37.64
72.77
64.13
ours (exponential) 82.93
39.26
81.7
73.69
ours (full)
85.1
40.58
83.04
82.03
754
c. li et al.
due to the fact that our objective function is constructed based on the dirichlet
distribution and the presence of class imbalance within both datasets. when the
samples are selected by uas (exponential) at the early training stage, samples
from smaller numbered classes may not be chosen which leads to the optimiza-
tion may tend to fall into local extrema."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"our approach clearly outperforms fcl,
demonstrating the necessity to design a dynamic curriculum. compared with loss
function-based curriculum learning (spl, spcl and adaptive cl), our meth-
ods obtain better performance, showing the importance of using uncertainty
estimates as the criterion for diﬃculty measurement of data. furthermore, our
method is more eﬀective than spl at an early training stage."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"details of the comparison
with other curriculum learning methods are referred to in the appendix. table 2. ablation analysis of our method on chest-xray 8 (covid-19) dataset. method
ddm exp single-step linear uas accuracy
vanilla
71.88
fcl
✓
72.32
ours
✓
✓
81.25
ours
✓
✓
80.36
ours
✓
✓
77.23
ours with full uas(w/o l2) ✓
✓
82.14
ours with full uas(w l2)
✓
✓
83.04
ablation study."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"moreover, in order to demonstrate
that the order generated by ddm is robust, we have conducted experiments
on diﬀerent backbones, details of which can be found in the appendix. both single-step
and linear pacing functions divide the entire data set into subsets and put them
into the model progressively. the diﬀerence between the two is in the number
of subsets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_72.pdf,"the results in table 2
show that uas is more suited to ddm than the other pacing functions. when
the other three pacing functions are used, new samples added to the current
subset will disrupt the existing order of data when the size of the subset is
increased. it causes the network to have to relearn and reorder the new subset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"skin image datasets often suﬀer from imbalanced data distri-
bution, exacerbating the diﬃculty of computer-aided skin disease diag-
nosis. some recent works exploit supervised contrastive learning (scl)
for this long-tailed challenge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"a balanced-hybrid-proxy loss is designed to exploit rela-
tions between samples and proxies with diﬀerent classes treated equally. taking both “imbalanced data” and “imbalanced diagnosis diﬃculty”
into account, we further present a balanced-weighted cross-entropy loss
following curriculum learning schedule. experimental results on the clas-
siﬁcation of imbalanced skin lesion data have demonstrated the superior-
ity and eﬀectiveness of our method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"as computer-aided diagnosis matures, recent advances
with deep learning techniques such as cnns have signiﬁcantly improved the per-
formance of skin lesion classiﬁcation [7,8]. however, as data-hungry approaches,
deep learning models require large balanced and high-quality datasets to meet the
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 23.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43895-0_23
ecl: class-enhancement contrastive learning
245
sample
enhanced proxy
anchor sample
anchor proxy
many class
medium class
few class
aggregation
separation
(a) supervised contrastive learning  
(b) class-enhancement contrastive learning
over-treatment 
equal-treatment 
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"accuracy and robustness requirements in applications, which is hard to suﬃce due
to the long-tailed occurrence of diseases in the real-world. long-tailed problem is
usually caused by diﬀerences in incidence rate and diﬃculties in data collection. some diseases are common while others are rare, making it diﬃcult to collect bal-
anced data [13]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"this will cause the head classes to account for the majority of the
samples and the tail classes only have small portions. thus, existing public skin
datasets usually suﬀer from imbalanced problems which then results in class bias
of classiﬁer, for example, poor model performance especially on tail lesion types.
to tackle the challenge of learning unbiased classiﬁers with imbalanced data,
many previous works focus on three main ideas, including re-sampling data [1,
18], re-weighting loss [2,15,22] and re-balancing training strategies [10,23]. despite the great results achieved, these methods either
manually interfere with the original data distribution or improve the accuracy
of minority classes at the cost of reducing that of majority classes [12,13]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"(2) scl loss focuses more on
optimizing the head classes with much larger gradients than tail classes, which
means tail classes are all pushed farther away from heads [24]. (3) most methods
246
y. zhang et al.
only consider the impact of sample size (“imbalanced data”) on the classiﬁcation
accuracy of skin diseases, while ignoring the diagnostic diﬃculty of the diseases
themselves (“imbalanced diagnosis diﬃculty”). to address the above issues, we propose a class-enhancement contrastive
learning (ecl) method for skin lesion classiﬁcation, diﬀerences between scl
and ecl are illustrated in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"we propose a novel hybrid-proxy model to generate proxies for enhancing
diﬀerent classes with a reversed imbalanced strategy, i.e., the fewer samples in
a class, the more proxies the class has. these learnable proxies are optimized
with a cycle update strategy that captures original data distribution to mitigate
the quality degradation caused by the lack of minority samples in a mini-batch. the new loss treats all classes equally
and utilizes sample-to-sample, proxy-to-sample and proxy-to-proxy relations to
improve representation learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"our contributions can be summarized as follows: (1) we propose an ecl
framework for long-tailed skin lesion classiﬁcation. information of classes are
enhanced by the designed hybrid-proxy model with a cycle update strategy. (2)
we present a balanced-hybrid-proxy loss to balance the optimization of each
class and leverage relations among samples and proxies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"(3) a new balanced-
weighted cross-entropy loss is designed for an unbiased classiﬁer, which considers
both “imbalanced data” and “imbalanced diagnosis diﬃculty”. (4) experimental
results demonstrate that the proposed framework outperforms other state-of-the-
art methods on two imbalanced dermoscopic image datasets and the ablation
study shows the eﬀectiveness of each element. 2
methods
the overall end-to-end framework of ecl is presented in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"∈ {1, 2, ..., n p
c }, c ∈ {1, 2, ..., c}}, c is the class number, pc
k ∈ rd is
the k-th proxy vector of class c, and n p
c is the proxy number in this class. since
samples in a mini-batch follow imbalanced data distribution, these proxies are
designed to be generated in a reversed imbalanced way by giving more represen-
tative proxies of tail classes for enhancing the information of minority samples. let us denote the sample number of class c as nc and the maximum in all classes
as nmax."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"as we know, a gradient descent algorithm will generally be executed to
update the parameters after training a mini-batch of samples. however, when
dealing with an imbalanced dataset, tail samples in a batch contribute little to
the update of their corresponding proxies due to the low probability of being
sampled. so how to get better representative proxies?"
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"speciﬁcally, we introduce
the gradient accumulation method into the training process to update proxies
asynchronously. the proxies are updated only after a ﬁnished epoch that all data
has been processed by the framework with the gradients accumulated. − lr ∗ gradt
θ// update parameters θ of model
11
φ ← φ − t
t lr ∗ gradt
φ // update parameters φ of p
12
if e > e2 then
13
f e = v alidate(model, xval)
a strategy, tail proxies can be optimized in a view of whole data distribution,
thus playing better roles in class information enhancement."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"− 1

sk∈{zc∪pc}
exp(si · sk/τ)
(3)
where bc means the sample number of class c in a batch, τ is the temperature
parameter. in addition, we further deﬁne zc and pc as a subset with the label c
of z and p respectively. the average operation in the denominator of balanced-
hybrid-proxy loss can eﬀectively reduce the gradients of the head classes, making
an equal contribution to optimizing each class."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"sample-
to-sample, proxy-to-sample and proxy-to-proxy relations in the proposed loss
have the potential to promote network’s representation learning. moreover, as
the skin datasets are often small, richer relations can eﬀectively help form a
high-quality distribution in the embedding space and improve the separation of
features.
2.3
balanced-weighted cross-entropy loss
taking both “imbalanced data” and “imbalanced diagnosis diﬃculty” into con-
sideration, we design a curriculum schedule and propose balanced-weighted
cross-entropy loss to train an unbiased classiﬁer. the training phase are divided
into three stages."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"the ﬁnal loss is given by loss = λlbhp +μlbw ce
where λ and μ are the hyperparameters which control the impact of losses. 3
experiment
3.1
dataset and implementation details
dataset and evaluation metrics. the 2018
250
y. zhang et al.
(a) classifier branch-ce (isic2018)
(b) dual branch-bwce+bhp (isic2018)
(c) classifier branch-ce (isic2019)
(d) dual branch-bwce+bhp (isic2019)
minority
majority
minority
majority
minority
majority
minority
majority
predicted labels
true labels
predicted labels
predicted labels
predicted labels
true labels
true labels
true labels
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"3. the results of confusion matrix illustrate that ecl obtains great performance
on most classes especially for minority classes. dataset consists of 10015 images in 7 classes while a larger 2019 dataset provides
25331 images in 8 classes. the imbalanced factors α = nmax
nmin of the two datasets
are all > 50 (isic2018 58.30 and isic2019 53.87), which means that skin lesion
classiﬁcation suﬀers a serious imbalanced problem."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"the hyperparameters e1, e2, τ, λ, and μ are
set to 20, 50, 0.01, 1, and 2 respectively. we use the default data augmentation
strategy on imagenet in [9] as t1 for classiﬁcation branch. and for cl branch,
we add random grayscale, rotation, and vertical ﬂip in t1 as t2 to enrich the data
representations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"moreover, mwnl and scl have been veriﬁed to perform well in the skin disease
classiﬁcation task. to ensure fairness, we re-train all methods by rerun their
released codes on our divided datasets with the same experimental settings. ecl: class-enhancement contrastive learning
251
table 1. comparison results on isic2018 and isic2019 datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"the results are shown in table 1. it can be seen that ecl has
a signiﬁcant advantage with the highest level in most metrics on two datasets. noticeably, our ecl outperforms other imbalanced methods by great gains, e.g.,
2.56% in pre on isic2018 compared with scl and 4.33% in f1 on isic2019
dataset compared with tsc."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"3, which illustrate that ecl has signiﬁcantly improved
most of the categories, from minority to majority. table 2. ablation study on isic2019 dataset. to further verify the eﬀectiveness of the designs in ecl, we
conduct a detailed ablation study shown in table 2 (the results on isic2018 are
shown in supplementary material table s2)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"first, we directly move the con-
trastive learning (cl) branch and replaced the balanced-weighted cross-entropy
252
y. zhang et al.
(bwce) loss with cross-entropy (ce) loss. we can see from the results that
adding cl branch can signiﬁcantly improve the network’s data representation
ability with better performance than only adopting a classiﬁer branch. and our
bwce loss can help in learning a more unbiased classiﬁer with an improvement
of 2.7% in f1 compared to ce in dual branch setting."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"then we train the ecl
w/o cycle update strategy. the overall performance of the network has declined
compared with training w/ the strategy, indicating that this strategy can bet-
ter enhance proxies learning through the whole data distribution. in the end,
we also set the proxies’ number of diﬀerent classes equal to explore whether
the classiﬁcation ability of the network is improved due to the increase in the
number of proxies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_23.pdf,"with more proxies, metrics ﬂuctuate and do not increase sig-
niﬁcantly. however, the result of using proxies generated by reversed balanced
way in hybrid-proxy model (hpm) outperforms equal proxies in nearly all met-
rics, which proves that giving more proxies to tail classes can eﬀectively enhance
and enrich the information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"we incorporate a group multi-axis hadamard product attention module
(ghpa) and a group aggregation bridge module (gab) in a lightweight
manner. the ghpa groups input features and performs hadamard prod-
uct attention mechanism (hpa) on diﬀerent axes to extract pathological
information from diverse perspectives. the gab eﬀectively fuses multi-
scale information by grouping low-level features, high-level features, and
a mask generated by the decoder at each stage."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"in short, compared
to the transfuse, our model achieves superior segmentation performance
while reducing parameter and computation costs by 494x and 160x,
respectively. moreover, to our best knowledge, this is the ﬁrst model
with a parameter count limited to just 50kb. our code is available at
https://github.com/jcruan519/ege-unet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14223, pp. 1. (a) and (b) respectively show the visualization of comparative experimental
results on the isic2017 and isic2018 datasets. the x-axis represents the number of
parameters (lower is better), while y-axis represents miou (higher is better)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"[5] has pioneered a serial fusion of cnn and vit for medical image
segmentation. [26] employs a dual-path structure, utilizing cnn and
vit to capture local and global information, respectively. [8] utilizes
a hybrid hierarchical architecture, eﬃcient bidirectional attention, and semantic
maps to achieve global multi-scale feature fusion, combining the strengths of
cnn and vit."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"therefore, in this study, we propose ege-unet, a lightweight skin lesion
segmentation model that achieves state-of-the-art while signiﬁcantly reducing
parameter and computation costs. additionally, to our best knowledge, this is
the ﬁrst work to reduce parameter to approximately 50kb. to be speciﬁc, ege-unet leverages two key modules: the group multi-axis
hadamard product attention module (ghpa) and group aggregation bridge
ege-unet: an eﬃcient group enhanced unet
483
module (gab)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"[7] have shown
promise, owing to the multi-head self-attention mechanism (mhsa). mhsa
divides the input into multiple heads and calculates self-attention in each head,
which allows the model to obtain information from diverse perspectives, integrate
diﬀerent knowledge, and improve performance. nonetheless, the quadratic com-
plexity of mhsa enormously increases the model’s size."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"however, it is worth noting that we perform
hpa on diﬀerent axes in diﬀerent groups, which helps to further obtain infor-
mation from diverse perspectives. on the other hand, for gab, since the size
and shape of segmentation targets in medical images are inconsistent, it is essen-
tial to obtain multi-scale information [19]. therefore, gab integrates high-level
and low-level features with diﬀerent sizes based on group aggregation, and addi-
tionally introduce mask information to assist feature fusion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"a clear comparison
of ege-unet with others is shown in fig. 1.
in summary, our contributions are threefold: (1) ghpa and gab are pro-
posed, with the former eﬃciently acquiring and integrating multi-perspective
information and the latter accepting features at diﬀerent scales, along with an
auxiliary mask for eﬃcient multi-scale feature fusion. (2) we propose ege-
unet, an extremely lightweight model designed for skin lesion segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"the encoder is composed of six stages,
each with channel numbers of {8, 16, 24, 32, 48, 64}. while the ﬁrst three
stages employ plain convolutions with a kernel size of 3, the last three stages
utilize the proposed ghpa to extract representation information from diverse
perspectives. in contrast to the simple skip connections in unet, ege-unet
incorporates gab for each stage between the encoder and decoder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"however,
ege-unet: an eﬃcient group enhanced unet
485
fig. the architecture of group aggregation bridge module (gab).
utilizing simple hpa alone is insuﬃcient to extract information from multiple
perspectives, resulting in unsatisfactory results. motivated by the multi-head
mode in mhsa, we introduce ghpa based on hpa, as illustrated in algorithm
1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"for the last group, we only use dw
on the feature map. finally, we concatenate the four groups along the channel
dimension and apply another dw to integrate the information from diﬀerent
perspectives. note that all kernel size employed in dw are 3.
group aggregation bridge module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"for each group of fused features,
the mask is concatenated. next, dilated convolutions [25] with kernel size of 3
and diﬀerent dilated rates of {1, 2, 5, 7} are applied to the diﬀerent groups, in
order to extract information at diﬀerent scales. finally, the four groups are con-
catenated along the channel dimension, followed by the application of a plain
convolution with the kernel size of 1 to enable interaction among features at
diﬀerent scales."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"486
j. ruan et al.
loss function. [27] is employed to calculate the loss function
for diﬀerent stages, in order to generate more accurate mask information. our
loss function can be expressed as eqs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"λi is the weight
for diﬀerent stage. 3
experiments
datasets and implementation details. [2,6], containing 2150 and 2694 dermoscopy images, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"the images are normalized and
resized to 256 × 256. we apply various data augmentation, including horizontal
ﬂipping, vertical ﬂipping, and random rotation. [13] is utilized as the
optimizer, initialized with a learning rate of 0.001 and the cosineannealinglr
[12] is employed as the scheduler with a maximum number of iterations of 50
and a minimum learning rate of 1e-5."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"a total of 300 epochs are trained with
a batch size of 8. to evaluate our method, we employ mean intersection over
union (miou), dice similarity score (dsc) as metrics, and we conduct 5 times
and report the mean and standard deviation of the results for each dataset. the comparative experimental results presented in
table 1 reveal that our ege-unet exhibits a comprehensive state-of-the-art per-
formance on the isic2017 dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"in comparison to other lightweight models, ege-unet
surpasses unext-s with a miou improvement of 1.55% and a dsc improvement
of 0.97%, while exhibiting parameter and computation reductions of 17% and
72% of unext-s. furthermore, ege-unet outperforms malunet with a miou
improvement of 1.03% and a dsc improvement of 0.64%, while reducing param-
eter and computation to 30% and 85% of malunet. for the isic2018 dataset,
the performance of our model also outperforms that of the best-performing
model. besides, it is noteworthy that ege-unet is the ﬁrst lightweight model
ege-unet: an eﬃcient group enhanced unet
487
table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"the baseline utilized in our work is
referenced from malunet [19], which employs a six-stage u-shaped architecture
488
j. ruan et al.
fig. 4. qualitative comparisons on the isic2018 dataset.
with symmetric encoder and decoder components. each stage includes a plain
convolution operation with a kernel size of 3, and the number of channels at
each stage is set to {8, 16, 24, 32, 48, 64}."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_46.pdf,"table 2(c) illustrates the ablations for gab. initially, we
omit the mask information, and miou metric even drops below 79%, thereby
conﬁrming once again the critical role of mask information in guiding feature
fusion. furthermore, we substitute the dilated convolutions in gab with plain
convolutions, which also leads to a reduction in performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"however, recent research has revealed that deep
neural networks for skin lesion recognition may overly depend on disease-
irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor
generalization in unseen environments. to address this issue, we pro-
pose a novel domain generalization method called epvt, which involves
embedding prompts into the vision transformer to collaboratively learn
knowledge from diverse domains. concretely, epvt leverages a set of
domain prompts, each of which plays as a domain expert, to capture
domain-speciﬁc knowledge; and a shared prompt for general knowledge
over the entire dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"to facilitate knowledge sharing and the interac-
tion of diﬀerent prompts, we introduce a domain prompt generator that
enables low-rank multiplicative updates between domain prompts and
the shared prompt. a domain mixup strategy is additionally devised to
reduce the co-occurring artifacts in each domain, which allows for more
ﬂexible decision margins and mitigates the issue of incorrectly assigned
domain labels. experiments on four out-of-distribution datasets and six
diﬀerent biased isic datasets demonstrate the superior generalization
ability of epvt in skin lesion recognition across various environments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"keywords: skin lesions · prompt · domain generalization · debiasing
1
introduction
skin cancer is a serious and widespread form of cancer that requires early detec-
tion for successful treatment. computer-aided diagnosis systems (cad) using
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_24.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. the training data is split into ﬁve domains: clean, rulers, hairs, air pockets,
and dark corners."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"as illustrated in fig. 1, we deﬁne the domain
labels based on the types of artifacts present in the training images, which can
provide environment-aware prior knowledge reﬂecting a range of noisy contexts. by doing this, we can develop a dg algorithm to learn the generalized and
robust features from diverse domains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"previous dg algorithms learning domain-invariant features from source
domains have succeeded in natural image tasks [2,17,19], but cannot directly
apply to medical images, in particular skin images, due to the vast cross-domain
diversity of skin lesions in terms of shapes, colors, textures, etc. as each domain
contains ad hoc intrinsic knowledge, learning domain-invariant features is highly
challenging. [7,24,32],
exploiting multiple learnable domain experts (e.g., batch norm statistic, auxil-
iary classiﬁers, etc.) to capture domain-speciﬁc knowledge from diﬀerent source
domains individually."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"still, two signiﬁcant challenges remain. first, previous
work only exploits some weak experts, like the batch norm, to capture knowledge,
which naturally hampers the capability of capturing essential domain-speciﬁc
knowledge. second, previous methods such as [30] focused on learning domain
knowledge independently while overlooking the rich cross-domain information
that all domain experts can contribute collectively for the target domain predic-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"2. the overview of our environment-aware prompt vision transformer (epvt). on the one hand, inspired by the emerging prompt learning techniques that
embed prompts into a model for adaptation to diverse downstream tasks [12,26,
31], we construct diﬀerent prompt vectors to strengthen the learning of domain-
speciﬁc knowledge for adaptation to diverse domains. [8] is adopted to fully model the
relationship between image tokens and prompt vectors."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"the prompt generator enables multiple domain
prompts to work collaboratively and beneﬁt from each other for generalization to
unknown domains. additionally, we devise a domain mixup strategy to resolve
the problem of co-occurring artifacts in dermoscopic images and mitigate the
resulting noisy domain label assignments. our contributions can be summarized as: (1) we resolve an artifacts-derived
biasing problem in skin cancer diagnosis using a novel environment-aware prompt
learning-based dg algorithm, epvt; (2) epvt takes advantage of a vit-
based domain-aware prompt learning and a novel domain prompt generator to
improve domain-speciﬁc and cross-domain knowledge learning simultaneously;
(3) a domain mixup strategy is devised to reduce the co-artifacts speciﬁc to
dermoscopic images; (4) extensive experiments on four out-of-distribution skin
datasets and six biased isic datasets demonstrate the outperforming general-
ization ability and robustness of epvt under heterogeneous distribution shifts."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"we will illustrate its details in the following sections. 2.1
domain-speciﬁc prompt learning with vision transformer
to enable the pre-trained vision transformer (vit) to capture knowledge from
diﬀerent domains, as shown in fig. 2a, we deﬁne a set of m learnable domain
prompts produced by a domain prompt generator (introduced in sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"through optimizing, each prompt becomes a domain expert only responsible
for the images from its own domain. by the self-attention mechanism of vit,
the model can eﬀectively capture domain-speciﬁc knowledge from the domain
prompt tokens.
2.2
cross-domain knowledge learning
to facilitate eﬀective knowledge sharing across diﬀerent domains while maintain-
ing its own parameters of each domain prompt, we propose a domain prompt
generator, as depicted in fig. our approach is inspired by model adaptation
and multi-task learning techniques used in natural language processing [13,26].
aghajanyan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"to this end, we decompose each p m into a hadamard
product between a randomly initialized shared prompt p ∗ and a rank-one matrix
pk obtained from two randomly initialized learnable vectors uk and vk, which
is:
p m = p ∗ ⊙ pk
where
pk = uk · vt
k
(2)
epvt: environment-aware prompt vision transformer
253
where p m represents the domain-speciﬁc prompt, computed by hadamard prod-
uct of p ∗ and pk. here, p ∗ ∈ rs×d is utilized to learn general knowledge, with s
and d representing the dimensions of the prompt vector and feature embedding
respectively. on the other hand, pk is computed using domain-speciﬁc trainable
vectors: uk ∈ rs and vk ∈ rd."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"these vectors capture domain-speciﬁc information
in a low-rank space. the decomposition of domain prompts into rank-one sub-
spaces ensures that the model eﬀectively encodes domain-speciﬁc information. by using the hadamard product, the model can eﬃciently leverage cross-domain
knowledge for target domain prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"however, a non-trivial issue arises due to the possible co-
occurrence of diﬀerent artifacts from other domains within each domain. instead of assign-
ing a hard prediction label (“0” or “1”) to each image, in each batch, we mix
every image using two randomly selected images from two diﬀerent domains. this allows us to learn a ﬂexible margin relative to both domains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"we then
apply the cross-entropy loss to the corresponding labels of bot images, as shown
in fig. = λxk
i +(1−λ)xq
j; xk
i and xq
j are samples from two diﬀerent domains
k and q, and yk
i and yq
j are the corresponding labels. this strategy can overcome
the challenge of ambiguous domain labels in dermoscopic images and improve
the performance of our model.
2.4
optimization
so far, we have introduced lmixup in eq. 3 for optimizing our model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"addi-
tionally, the inferece process is the same as the simulated inference process and
our ﬁnal prediction will be conditioned on the adapted prompt padapted. to ensure that the adapter learns the correct linear correlation between the
domain prompts and the target image, we use the domain label from source
domains to directly supervise the weights wm. λ( 1
m
m

m=1
1
m (lce(wm
m, 1) +

t̸=m
lce(wm
t , 0))
(5)
where ˆfm(x) is the obtained feature map conditioned on the adapted prompt
padapted, and h is the classiﬁcation head."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"(1) out-
of-distribution evaluation: the task is to evaluate the model on test sets that con-
tain diﬀerent artifacts or attributes compared to the training set. [6] dataset, following the split of [3]. we use
the artifacts annotations from [3] and divide the training set of isic2019 into ﬁve
groups: dark corner, hair, gel bubble, ruler, and clean, with 2351, 4884, 1640, 672,
and 2796 images, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"it’s worth noting that isic2019, derm7pt-
dermoscopic, and ph2 are dermoscopic images, while derm7pt-clinical and
pad are clinical images. (2) trap set debiasing: we train and test our epvt
with its baseline on six trap sets [3] with increasing bias levels, ranging from 0
(randomly split training and testing sets from the isic2019 dataset) to 1 (the
highest bias level where the correlation between artifacts and class label is in
the opposite direction in the dataset splits). more details about these datasets
and splits are provided in the complementary material."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"the batch size is 130, and the length
of the prompt is 10. we resize the input image to a size of 224 × 224 and adopt
the standard data augmentation like random ﬂip, crop, rotation, and color jitter. an early stopping with the patience of 22 is set and with a total of 60 epochs
for ood evaluation and 100 epochs for trap set debiasing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"all experiments are
conducted on a single nvidia rtx 3090 gpu.
out-of-distribution evaluation: table 1 presents a comprehensive compar-
ison of our epvt algorithm with existing domain generalization methods. the
results clearly demonstrate the superiority of our approach, with the best perfor-
mance on three out of four ood datasets and remarkable improvements over the
erm algorithm, especially achieving 4.1% and 8.9% improvement on the pad
and ph2 datasets, respectively. although some algorithms may perform simi-
larly to our model on one of the four datasets, none can consistently match the
performance of our method across all four datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"3. (a) deibiasing evaluation (b) domain distance (c) domain weights
0.1%, showing that simply combining prompt does not very helpful for domain
generalization. when we combine the adapter, the model’s average performance
improves by 1.37%, but it performs worse than erm on pad dataset. subse-
quently, we added domain mixup and domain prompt generator to the model,
resulting in signiﬁcant further improvements in the model’s average performance
by 1.32% and 1.69%, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"trap set debiasing: in fig. 3a, we present the performance of the erm base-
line and our epvt on six biased isic2019 datasets. each point on the graph
represents an algorithm that is trained and tested on a speciﬁc bias degree split."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"the graph shows that the erm baseline performs better than our epvt when
the bias is low (0 and 0.3). however, this is because erm relies heavily on spu-
rious correlations between artifacts and class labels, leading to overﬁtting on the
training set. as the bias degree increases, the correlation between artifacts and
class labels decreases, and overﬁtting the train set causes the performance of
erm to drop dramatically on the test set with a signiﬁcant distribution diﬀer-
ence."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"in contrast, our epvt exhibits greater robustness to diﬀerent bias levels. notably, our epvt outperforms the erm baseline by 9.4% on the bias 1 dataset. prompt weights analysis: to verify whether our model has learned the
correct domain prompts for target domain prediction, we analyze and plot the
results in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"3b and 3c. firstly, we extract the features of each domain from
our training set and extract the feature from one target dataset, derm7pt-clin. [9] between each domain and the target
dataset using the extracted feature, representing the domain distance between
epvt: environment-aware prompt vision transformer
257
them."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_24.pdf,"3c; it shows that our model assigns the highest weight
to the “dark corner” group, as the domain distance between “dark corner” and
derm7pt-clin is the closest, as shown in fig. this suggests that they share
the most similar domain information. further, the “clean” group is assigned the
smallest weight as the domain distance between them is the largest, indicating
that their domains are signiﬁcantly diﬀerent and contain less useful information
for target domain prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"one task in diagnosing
lyme disease is lesion segmentation, i.e., separating benign skin from
lesions, which can not only help clinicians to focus on lesions but also
improve downstream tasks such as disease classiﬁcation. however, it is
challenging to segment lyme disease lesions due to the lack of well-
segmented, labeled lyme datasets and the nature of lyme, e.g., the
typical bull’s eye lesion and its closeness to normal skin. in this paper,
we design a simple yet novel data preprocessing and alteration method,
called edgemixup, to help segment lyme lesions on imbalanced training
datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"[1] yet some
improvements still remain to be addressed, importantly in areas that allow both
algorithmic performance and fairness [2], and in certain medical applications that
promise to signiﬁcantly lessen morbidity and mortality. early detection of skin
lesions is such an endeavor as it can aid in identifying infectious diseases with
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8_36.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43901-8_36
edgemixup
375
cutaneous manifestations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"however, while lyme disease lesion segmentation is intu-
itively simple, it is challenging due to the following reasons. first, there lacks
of a well-segmented dataset with manual labels on lyme disease. on one hand,
some datasets—such as ham10000 [10] and isbi challenges [11]—have manual
annotated segmentations for diseases like melanoma, but they do not have lyme
disease lesions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"on the other hand, some datasets—such as groh et al. [12]—have
lyme disease and skin tone and classiﬁcation labels, but not segmentation. second, the segmentation of lyme lesion is itself challenging due to the nature
of em pattern."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"speciﬁcally, a typical lyme lesion exhibits a bull’s eye pattern
with one central redness and one outer circle, which is diﬀerent from darkness
lesion in cancer-related skin disease like melanoma. furthermore, clinical data
collected for training is usually imbalanced in some properties, e.g., more sam-
ples with light skins compared with dark skins. [17], usually suﬀer
from relatively low performance and reduced fairness [2,18,19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"in this paper, we present the ﬁrst lyme disease dataset that contains labeled
segmentation and skin tones. our lyme disease dataset contains two parts: (i)
a classiﬁcation dataset, composed of more than 3,000 diseased skin images that
are either obtained from public resources or clinicians with patient-informed con-
sent, and (ii) a segmentation dataset containing 185 samples that are manually
annotated for three regions—i.e., background, skin (light vs. dark), and lesion—
conducted under clinician supervision and institutional review boards (irb)
approval. secondly, we design a simple yet novel data preprocessing and alternation
method, called edgemixup, to improve lyme disease segmentation and diagno-
sis fairness on samples with diﬀerent skin-tones."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"[22].
2
motivation
in this section, we motivate the design of edgemixup by showing that added
lesion boundary helps a dl model focus more on the lesion part instead of other
features such as skin or background. note that not all skin disease datasets are
carefully processed either due to the large amount of work required or the scarcity
of data samples collected, e.g., sd-198 [23] contains samples that are taken under
variant environments. speciﬁcally, we train two resnet-34 models using the same
dataset with and without edgemixup for a classiﬁcation task of skin disease."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"1b and 1c. the reason is that a legacy
diagnosis has no information about lesion and does not know where to locate its
focus, thus easily gets distracted by ﬁngers instead of the lesion pattern.
3
method
in this section, we ﬁrst give the deﬁnition for model fairness, and we then
describe the design of edgemixup for the purpose of de-biasing in fig. 2 and
edgemixup
377
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"if
there exists a model f such that m(f(xst1), y) = m(f(xst2, y)), we consider it
perfectly fair for st1 and st2 skin-tone samples. edgemixup improves model fairness on light and dark skin samples in both
segmentation and classiﬁcation tasks, and it has two major components: (i) edge
detection using mixup, and (ii) data preprocessing and alteration for downstream
tasks. more speciﬁcally, our proposed edge detection has two parts: initial edge
detection and iterative improvement."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"note that the initial
edge detection is irrelevant to the sample size of a particular subpopulation, thus
improving the fairness. that is, even if the original dataset is imbalanced, as long
as one sample from a subpopulation exists, the color range of the sample’s lesion
is considered in the initial detection. iterative edge improvement: edgemixup includes iterative edge improve-
ment in the training phase of our segmentation model to further improve model
utility."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"the high-level idea is that when the lesion is restricted in a small
edgemixup
379
table 1. annotated segmentation and classiﬁcation dataset characteristics, broken
down by ita-based skin tones (light skin/ dark skin) and disease types. besides, edgemixup calculates a linear combination of original image and lesion
boundary, i.e., by assigning the weight of original image as α and lesion bound-
ary as 1 − α. figure 3 shows the edge-mixed-up images for diﬀerent iterations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"edgemixup removes more skin areas after each iteration and gradually gets
close to the real lesion at the third iteration. 4
datasets
we present two datasets: (i) a dataset collected and annotated by us (called
skin), and (ii) a subset of sd-198 [23] with our annotation (called sd-sub). first,
we collect and annotate a dataset with 3,027 images containing three types of
disease/lesions, i.e., tinea corporis (tc), herpes zoster (hz), and erythema
migrans (em)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"we name it as skin-class
for later reference. [23], a benchmark
dataset for skin disease classiﬁcation, as another dataset for both segmentation
and classiﬁcation tasks. note that due to the amount of manual work involved
in annotation, we select those classes based on the number of samples in each
class."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"we compare predicted masks with the manually-annotated ground
truth by calculating the jaccard index, and computing the gap for subpopula-
tions with ls and ds (based on ita). edgemixup, a data preprocessing method,
improves the utility of lesion segmentation in terms of jaccard index compared
with all existing baselines. one reason is that edgemixup preserves skin lesion
information, thus improving the segmentation quality, while attenuating markers
for protected factors."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"note that edgemixup iteratively improves the segmen-
tation results. take our skin-seg dataset for example. we trained our baseline
unet model for three iterations, and the model utility is increased by 0.0468 on
jaccard index while the jgap between subpopulations is reduced by 0.0193."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"[27] and skin lesion [28]
classiﬁcation. as for skin lesion segmentation tasks, few works has been proposed
due to the lack of datasets with ground-truth segmentation masks. [11] to encourage researches studying lesion seg-
mentation, feature detection, and image classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_36.pdf,"the competing two-player optimization
paradigm is applied to maximizing equality of opportunity [32]. as a comparison,
edgemixup is an eﬀective preprocessing approach to debiasing when applied to
skin disease particularly for lyme-focused classiﬁcation and segmentation tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"in this
paper, we present a new method, prime+, for breast cancer risk pre-
diction that leverages prior mammograms using a transformer decoder,
outperforming a state-of-the-art risk prediction method that only uses
mammograms from a single time point. we validate our approach on a
dataset with 16,113 exams and further demonstrate that it eﬀectively
captures patterns of changes from prior mammograms, such as changes
in breast density, resulting in improved short-term and long-term breast
cancer risk prediction. experimental results show that our model achieves
a statistically signiﬁcant improvement in performance over the state-of-
the-art based model, with a c-index increase from 0.68 to 0.73 (p < 0.05)
on held-out test sets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"[27]
have been used to estimate an individual’s risk of developing breast cancer. how-
ever, these models do not perform well enough to be utilized in practical screen-
ing settings [3] and require the collection of data that is not always available. recently, deep neural network based models that predict a patient’s risk score
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"https://doi.org/10.1007/978-3-031-43904-9_38
390
h. lee et al.
directly from mammograms have shown promising results [3,8,9,20,33]. these
models do not require additional patient information and have been shown to
outperform traditional statistical models. when prior mammograms are available, radiologists compare prior exams
to the current mammogram to aid in the detection of breast cancer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"our method is based on a transformer
model that uses attention [30], similar to how radiologists would compare current
and prior mammograms. the method is trained and evaluated on a large and diverse dataset of over
9,000 patients and shown to outperform a model based on state-of-the art risk
prediction techniques for mammography [33]. although previous models such as
lrp-net and radifusion [5,34] have utilized prior mammograms, prime+
sets itself apart by employing an attention mechanism to extract information
about the prior scan."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"2
method
2.1
risk prediction
survival analysis is done to predict whether events will occur sometime in the
future. for medical applications, x typically
represents patient information like age, family history, genetic makeup, and diag-
nostic test results (e.g., a mammogram). if the event has not yet occurred by the
end of the study or observation period, the data is referred to as right-censored
(fig. 1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"a common backbone network extracts features from
the prior and current images, resulting in xprior and xcurr. we ﬁnd that the trans-
former decoder eﬀectively fuses relevant information from xprior and xcurr to produce
xcp c. the base hazard θb and time-dependent hazard prediction heads θu use the
concatenated feature to predict the cumulative hazard function ˆh. (a) illustrates the
interaction between xprior and xcurr in the cross-attention module of the transformer
decoder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"we make use of two additional fully connected layers
to calculate base hazard θb and time-dependent hazard θu, respectively. the predicted cumulative hazard is obtained by adding the base hazard and
time-dependent hazard, according to:
ˆh(t|x) = θb(x) +
t

τ=1
θuτ (x)
(2)
when dealing with right-censored data, we use an indicator function δi(t) to
determine whether the information for sample i at time t should be included in
392
h. lee et al. the loss calculation or not."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"these features are then ﬂattened and fed as input
to the transformer decoder, where multi-head attention is used to ﬁnd infor-
mation related to the current feature in the prior feature. = xcp c ⊕ xcurr, which is
then used by the base hazard network and time-dependent hazard network to
predict the cumulative hazard function ˆh.
enhanced risk prediction with prior images
393
3
experiments
3.1
dataset
we compiled an in-house mammography dataset comprising 16,113 exams
(64,452 images) from 9,113 patients across institutions from the united states,
gathered between 2010 and 2021. each mammogram includes at least one prior
mammogram."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"mammograms were captured
using hologic (72.3%) and siemens (27.7%) devices. we partitioned the dataset
by patient to create training, validation, and test sets. the validation set con-
tains 800 exams (198 cancer, 210 benign, 392 normal) from 400 patients, and
the test set contains 1,200 exams (302 cancer, 290 benign, 608 normal) from
600 patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"all data was de-identiﬁed according to the u.s hhs safe harbor
method. therefore, the data has no phi (protected health information) and
irb (institutional review board) approval is not required.
3.2
evaluation
we make use of uno’s c-index [28] and the time-dependent auc [16]. the c-
index measures the performance of a model by evaluating how well it correctly
predicts the relative order of survival times for pairs of individuals in the dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"we resize the images to 960 × 640 pixels and use a batch size of 96. to
augment the training data, we apply geometric transformations such as vertical
ﬂipping, rotation and photometric transformations such as brightness/contrast
adjustment, gaussian noise, sharpen, clahe, and solarize. empirically, we ﬁnd
that strong photometric augmentations improved the risk prediction model’s
394
h. lee et al.
table 1. ablation analysis on the eﬀectiveness of prior information and transformer
decoder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"our results support this intuition as the performance improvements over the
baseline are much more pronounced for longer term risk (3, 4-year auc) than
short term risk (1 year). the prime and prime+ models, which incorporate
prior mammograms, show high performance for long-term risk prediction (3,
4-year auc), indicating that considering changes in breast over time contain
useful information for breast cancer risk prediction. lastly, we empirically conﬁrm that a transformer decoder eﬀectively models
spatial relations between prior and current mammograms by demonstrating con-
sistent performance improvements of prime+ across both short-term and long-
term risk prediction settings."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"[4] and can
therefore help us understand why the addition of prior images works. mammographic breast density was determined using the breast imaging
reporting and data system (bi-rads) composition classiﬁcation. bi-rads
category a, b are deﬁned as fatty breasts and bi-rads category c, d are
classiﬁed as dense breasts."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_38.pdf,"however, prime+ is able to predict long-term risk
accurately even when a density change has occurred (3-year auc = 0.74 (0.60
to 0.88)), by learning to refer previous exams properly. this demonstrates the
potential usefulness of incorporating past mammogram information into breast
cancer risk prediction models. thus, we believe that incorporating prior exams
is important to identify changes in texture which are important for long term
risk prediction (table 3)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"accurate segmentation of brain tumors in mri images
requires precise detection of the edges. however, this crucial information
has been overlooked by existing methods. in this paper, we introduce the
edge-oriented transformer (eoformer) which speciﬁcally captures and
enhances edge information for brain tumor segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"the cnn structure captures low-
level local features in the image, while the transformer structure estab-
lishes long-range dependencies between features to generate high-level
global features. additionally, the decoder of our approach utilizes two
edge sharpening modules, the edge-oriented sobel and laplacian mod-
ules, which enhance the edge information. we also introduce eﬃcient
attention and re-parameterization techniques that make eoformer com-
putationally eﬃcient."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"physicians manually delineate the tumor regions based on
the varying signal intensities between diseased and normal tissues. this signal
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 32.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. al.
disparity constitutes the edge information in the images, making it essential for
accurate tumor segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"however, the performance of exist-
ing brain tumor segmentation methods are still unsatisfactory, especially for the
segmentation of edges between tumor lesion and normal tissues. considerable advancement has been achieved in the ﬁeld of natural image
segmentation by focusing on the edge information [3,11,18,25], and this idea
is also being applied to medical image segmentation. some methods utilize the
distance-dependent objective functions to generate more accurate edge predic-
tions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"to simplify the model architecture
and reduce inference time, we also introduce the re-parameterization technique
[4,5]. our model has been evaluated on both the publicly brats 2020 dataset
and a private medulloblastoma segmentation dataset. the results demonstrate
that eoformer clearly outperforms the state-of-the-art methods with limited
model parameters and lower flops (see more in supplementary material)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"2
method
figure 1(a) presents an overview of the proposed eoformer architecture, which
comprises two components: (1) an ehe encoder and bottleneck which are used to
capture low-level local features and learn a comprehensive feature representation. (2) a decoder which incorporates edge-oriented modules to enhance the edge
information in features.
2.1
eﬃcient hybrid encoder
the ehe, shown in fig. , comprises four stages, each of which consists
of a feature extraction module and a downsampling module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"+ x
′,
(1)
where the tokenmixeri(·) corresponds to dwconv (i ∈ {0, 1}) and msa
(i ∈ {2, 3}), norm( · ) represents layer normalization, and mlp(·) denotes the
multilayer perceptron. our approach combines the strengths of cnn and trans-
former to create a more powerful encoder that can extract both local and global
information from input data. we address the computational and memory complexity issues that arise from
3d input by replacing the vanilla attention with our extended 3d eﬃcient atten-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"the ﬁrst
branch contains a 3 × 3 × 3 convolution that extracts basic features from the
input. × 1 × 1 × 1 convolution to enhance the interaction between channel
eoformer: edge-oriented transformer for brain tumor segmentation
337
features of x, then utilizes a learnable scaled sobel ﬁlter to extract the 1st-
order diﬀerentiation edge information from x. this ﬁlter is capable of detecting
edges in three directions (i.e. horizontal, vertical, and orthogonal directions), so
it comprises three ﬁlters mx, my, and mz, each of which is represented by a 3
× 3 × 3 array. we then apply a learnable scaling matrix s ∈ rc×1×1×1 to mx, which allows for
dynamic adjustment of the scaling factor in each channel."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"diﬀerent from the sobel ﬁlter that only
extracts edges in the horizontal, vertical, and orthogonal directions, the lapla-
cian ﬁlter can extract edges in all directions. after extracting the 1st-order diﬀer-
entiation edge information, the intermediate features are then fed into the eol
module for extracting the 2nd-order diﬀerentiation edge information. similarly,
the feature f, obtained from the learnable scaling laplacian ﬁlter, and the ﬁnal
output of the eol module, denoted as flap, are deﬁned as:
f = dwconvs·mlap(conv1×1×1(x)),
flap = norm(conv3×3×3(x) + f)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"we introduce the
re-parameterization [4,5] into the edge-oriented modules to boost the segmenta-
tion performance while maintaining high eﬃciency. quantitative comparison on brats 2020 dataset. the ‘∗’ means the flops
we recalculate."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"quantitative comparison on medseg dataset. the brats 2020 dataset [14] consists of mri image data from 369 patients,
with each patient having four modalities (t1, t1ce, t2 and t2-flair) of
skull-striped mri, which are aligned to a standard brain template. the medseg dataset includes mri images of t1, t1ce, t2, and t2 flair
modalities from 255 patients with medulloblastoma."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"the training/validation/test
split ratio is 3:1:1. four-fold cross-validation is performed on this dataset. eoformer: edge-oriented transformer for brain tumor segmentation
339
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"the initial learning rate is 2×10−4. for data
augmentation, we apply image croping, ﬂipping, identity scaling and shifting. the
results are reproduced on our data split."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_32.pdf,"it is worth mentioning that eoformer has the
lowest flops and limited model size. 2 represents the segmentation results
on the brats 2020 and medseg datasets. the visualisation demonstrates that
the eoformer achieves the closest segmentation results to the ground truth."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"mesoscopic ﬂuorescence lifetime imaging (flim) of tissue
ﬂuorophores (i.e., collagen and metabolic co-factors nadh and fad) emission
has demonstrated the potential to demarcate the extent of head and neck cancer
in patients undergoing surgical procedures of the oral cavity and the orophar-
ynx. here, we demonstrate the ﬁrst label-free flim-based classiﬁcation using a
novelty detection model to identify residual cancer in the surgical cavity of the
oropharynx. due to highly imbalanced label representation in the surgical cav-
ity, the model employed solely flim data from healthy surgical cavity tissue for
training and classiﬁed the residual tumors as an anomaly."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"the results indicate that the flim-based classiﬁcation model
can identify residual cancer by directly imaging the surgical cavity, potentially
enabling intraoperative surgical guidance for tors. keywords: tors · positive surgical margin · flim · head and neck oncology
supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43996-4_56. © the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"[9] to inspect psms in the excised specimen. [10, 11].
label-free mesoscopic ﬂuorescence lifetime imaging (flim) has been demonstrated
as an intraoperative imaging guidance technique with high classiﬁcation performance
(auc = 0.94) in identifying in vivo tumor margins at the epithelial surface prior to tumor
excision [12]. flim can generate optical contrast using autoﬂuorescence derived from
tissue ﬂuorophores such as collagen, nadh, and fad."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"[13].
however, ability of label-free flim to identify residual tumors in vivo in the surgical
cavity (deep margins) has not been reported. one signiﬁcant challenge in developing
a flim-based classiﬁer to detect tumor in the surgical cavity is the presence of highly
imbalanced labels. surgeons aim to perform an en bloc resection, removing the entire tumor and a
margin of healthy tissue around it to ensure complete excision."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"therefore, in most cases,
only healthy tissue in left in the cavity. to address the technical challenge of highly
imbalanced label distribution and the need for intraoperative real-time cavity imaging,
we developed an intraoperative flim guidance model to identify residual tumors by
classifying residual cancer as anomalies. our proposed approach identiﬁed all patients
with psm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"flim-based in vivo classiﬁcation of residual cancer
589
fig. 1. overview methodology of the label-free flim-based intraoperative surgical guidance,
data collection, histopathology registration, and data processing. upper panel: overview of devel-
oping the classiﬁcation method to detect residual hnc in the surgical cavity during the tors
procedure."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"lower panel: describes the workﬂow involving generating labels for classiﬁer training
and testing. the labels were derived directly from histopathology, evaluated, and annotated by a
clinical pathologist (dg). each annotated h&e section was registered with the ex vivo and in vivo
flim scan images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"the red annotations correspond to cancer labels, and the green annotations
correspond to healthy. 2
method
as illustrated in fig. 1, the proposed method uses a clinically-compatible flim system
coupled to the da vinci sp transoral robotic surgical platform to scan the surgical cavity
in vivo and acquire flim data. we used the cumulative distribution transform (cdt)
of the ﬂuorescence decay curves extracted from the flim data as the input feature."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"the novelty detection model classiﬁed flim points closer to the healthy distribution as
healthy and further from the healthy distribution as a residual tumor. we implemented
the image guidance by augmenting the classiﬁcation map to the surgical view using the
predictor output and point locations of the scan.
2.1
flim hardware and data acquisition
this study used a multispectral ﬂuorescence lifetime imaging (flim) device to acquire
data [14]. the flim device features a 355 nm uv laser for ﬂuorescence excitation,
which is pulsed at a 480 hz repetition rate."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"390/40 nm attributed to collagen autoﬂuorescence, (2) 470/28 nm to nadh, and (3)
542/50 nm to fad. the flim device includes a 440 nm continuous wave laser that serves as an aiming
beam; this aiming beam enables real-time visualization of the locations where ﬂuores-
cence (point measurements) is collected by generating visible blue illumination at the
location where data is acquired. segmentation of the ‘aiming beam’ allows for flim
data points to be localized as pixel coordinates within a surgical white light image (see
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"localization of these coordinates is essential to link the regions where data is
obtained to histopathology, which is used as the ground truth to link flim optical data
to pathology status [16].
table 1. anatomy, surgical outcome, and tissue label breakdown for the 22 patients
surgical outcome
no. patients
tissue label
no. flim points
clear margin
19
healthy
170,535
psm
3
cancer
2,451
flim data was acquired using the da vinci sp robotic surgical platform."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"finally,
the patient’s surgical cavity was scanned to check for residual tumor. 2.2
patient cohort and flim data labeling
the research was performed under the approval of the uc davis institutional review
board (irb) and with the patient’s informed consent. all patients were anesthetized,
intubated, and prepared for surgery as part of the standard of care."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"after the surgical excision of the tumor, an in vivo flim scan of approximately 90
s was conducted within the patient’s surgical cavity, where the tumor was excised. to
validate optical measurements to pathology labels (e.g., benign tissue vs. residual tumor),
pathology labels from the excision margins were digitally annotated by a pathologist on
each h&e section. the aggregate of h&e sections was correspondingly labeled on
the ex vivo specimen at the cut lines where the tissue specimen was serially sectioned."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"this
process enables the direct validation of flim measurements to the pathology status of
the electrocauterized surgical margins (see table 1). 2.3
flim preprocessing
the raw flim waveform contains background noise, instrument artifacts, and other
types of interference, which need to be carefully processed and analyzed to extract
meaningful information (i.e., the ﬂuorescence signal decay characteristics). to account
for background noise, the background signal acquired at the beginning of each clinical
case was subtracted from the measured raw flim waveform."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"to retrieve the ﬂuorescence
function, we used a non-parametric model based on a laguerre expansion polynomi-
als and a constrained least-square deconvolution with the instrument impulse response
function as previously described [17]. in addition, an snr threshold of ≥50 db was
applied as a ﬁltering criterion to select flim points with good signal quality. 2.4
novelty detection model
the state-of-the-art novelty detection models were comprehensively reviewed in the
literature [18, 19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"the model trained only on the healthy flim points and
use a semi-supervised technique to classify residual tumor from healthy. the gods
is a pairwise complimentary classiﬁer deﬁned by two separating hyperplanes to min-
imize the distance between the two classiﬁers, limiting the healthy flim data within
the smallest volume and maximizing the margin between the hyperplanes and the data,
thereby avoiding overﬁtting while improving classiﬁcation robustness. η − max

w t
2 xi + b2
2
∔
(1)
where w1, w2 are the orthonormal frames,
min
w∈sk
d ,b
is the stiefel manifold, η is the
sensitivity margin, and was set η = 0.4 for our experiments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"the evaluation followed a leave-one-patient-out cross-validation approach. novelty detection model
solely used healthy labels from the in vivo cavity scan for training. the testing data
contained both healthy and residual cancer labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"the gods model reported the highest mean speciﬁcity of 0.78 ± 0.14 and the
lowest standard deviation. the robust covariance model reported the lowest speciﬁcity,
classifying larger portions of healthy tissue in the cavity as a residual tumor; indicat-
ing that the model did not generalize well to the healthy labels. we also observed that
changing the hyper-parameter, such as the anomaly factor, biased the model toward a
single class indicating overﬁtting (see supplementary section fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"bold font: best-performing
model. residual tumor labels are detected by calculating the distance between
the projected data points and the learned subspace. points that are far from the subspace
are classiﬁed as residual tumors."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"we observed that the gods with the flim decay
curves in the cdt space achieve the best classiﬁcation performance compared to other
novelty detection models with a mean accuracy of 0.76 ± 0.02. this is mainly due to the
robustness of the model, the ability to handle high-dimensional data, and the contrast in
the flim decay curves. the contrast in the flim decay curves was further improved
in the cdt space by transforming the flim decay curves to a normalized scale and
improving linear separability."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"this enhances surgical precision for tors procedures otherwise limited to
visual inspection of the cavity, palpation of the excised specimen, and ifsa. the flim-
based classiﬁcation model could help guide the surgical team in real-time, providing
information on the location and extent of cancerous tissue. in context to the standard of care, the proposed residual tumor detection model
exhibits high patient-level sensitivity (sensitivity = 1) in detecting patients with psms."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"surgeons aim to achieve
negative margins, meaning the absence of cancer cells at the edges of the tissue removed
during surgery. the ﬁnding of positive margins from ﬁnal histology would result in
additional surgical resection, potentially impacting the quality of life. combining the
proposed approach and ifsa could lead to an image-guided frozen section analysis to
help surgeons achieve negative margins in a more precise manner."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"2. gods classiﬁcation overlay of in vivo cavity scans of three patients presenting with
residual tumor. the columns represent each patient, and the rows depict the ground truth labels,
thepoint-predictionoverlay,andtheaugmentedsurgicalview.fpr-falsepositiverate,fnr-false
negative rate.
accounted for by the interpolation approach used for the classiﬁer augmentation (refer
to supplementary section fig. on the other hand, false positives spreading across
a larger region are much more complex to interpret."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"we observed a correlation between a larger spread of false
positive predictions associated with a zone of coagulation to a zone of hyperemia. the novelty detection model generalizes to the healthy labels and considers data
falling off the healthy distribution as residual cancer. the flim properties associated
with the healthy labels in the cavity are heterogeneous due to the electrocautery effects."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_56.pdf,"electrocautery effects are mainly thermal and can be observed by the levels of charring in
the tissue. reﬁning the training labels based on the levels of charring could lead to a more
homogeneous representation of the training set and result in an improved classiﬁcation
model with better generalization."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"but, high spatiospectral dimen-
sions make it diﬃcult to perform eﬃcient and eﬀective segmentation. in
this study, in light of information correlation in mhsis, we present a
computationally eﬃcient, plug-and-play space and spectrum factoriza-
tion strategy based on 2d architectures. drawing inspiration from the
low-rank prior of mhsis, we propose spectral matrix decomposition and
low-rank decomposition modules for removing redundant spatiospectral
information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"by plugging our dual-stream strategy into 2d backbones,
we can achieve state-of-the-art mhsi segmentation performances with 3–
13 times faster compared with existing 3d networks in terms of inference
speed. experiments show our strategy leads to remarkable performance
gains in diﬀerent 2d architectures, reporting an improvement up to 7.7%
compared with its 2d counterpart in terms of dsc on a public multi-
dimensional choledoch dataset. code is publicly available at https://
github.com/boxiangyun/dual-stream-mhsi."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"typically, an mhsi is presented as a hypercube, with hundreds
of narrow and contiguous spectral bands in spectral dimension, and thousands
of pixels in spatial dimension (fig. 1(a)). supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8_15.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43901-8_15
factor space and spectrum
153
(b)
posive
negave
spatial dimension 
spatial dimension 
(a)
2d-encode
2d-decoder
pca
mask
(d) pca-2d-net
2d-encoder
2d-decoder
mask
(c) 2d-net
3d-encoder
3d-decoder
mask
spectral-
pooling
(e) 3d-net
2d-encoder
2d-decoder
1d-encoder
spectral
mask
(f) dual-stream
550      700  
850    1000  
spectral correlation
550      700  
850    1000  
0   10  20
30   40   50   60  
spectral bands
0.5  0.6
0.7   0.8   0.9 
values
spatial correlation
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"due to the success of 2-dimensional (2d) deep neural network in natural
images, the simplest way to classify/segment an mhsi is to treat its two spa-
tial dimensions as input spatial dimension, and treat its spectral dimension as
input channel dimension [25] (fig. 1(c)). [1] are usually adopted to aggregate spectral information before
feeding the hsi into 2d networks (fig. 1(d)). these methods are not suitable
for high spatial resolution mhsi, and they may bring noises in spatial features
while reducing spectral dimension."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"the 2d networks are computationally eﬃ-
cient, usually much faster than 3d networks. but, they mix spectral information
after the ﬁrst convolutional layer, making the interband correlations of mhsis
underutilized. building a 3d network usually suﬀers from high computational
complexity, but it is the most straightforward way to learn interpixel and inter-
band correlations of mhsis [23] (fig. 1(e))."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"[28], learning spectral and
spatial features alternatively, utilizes transformer to capture the global spectral
feature. they overlook the low rankness in the spectral domain, which contains
discriminative information for diﬀerentiating targets from the background. high spatiospectral dimensions make it diﬃcult to perform a thorough anal-
ysis of mhsi."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"exploring mhsi’s
low-rank prior can promote the segmentation performance. in this paper, we consider treating spatiospectral dimensions separately and
propose an eﬀective and eﬃcient dual-stream strategy to “factor” the archi-
tecture, by exploiting the correlation information of mhsis. our dual-stream
strategy is designed based on 2d cnns with u-shaped [16] architecture."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"our
spectral encoder block can be formulated by:
x = dwconv(zin), x′ = smd(norm(x))+ x, zout = ffn(norm(x′))+x′,
(1)
where zin ∈ rs×cspe×h×w indicates the input spectral token tensor, and cspe
is the spectral feature dimension. we introduce depth-wise conv (dwconv) for
dynamically integrating redundant spatial information into spectral features to
reduce spatial redundant noises, achieved by setting diﬀerent strides of the con-
volutional kernel. then, we represent long-distance dependencies among spectral
inter-bands as a low-rank completion problem."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"156
b. yun et al.
in the framework shown in fig. 2, spectral information is integrated from
channel dimensions by performing concatenation, after the second and fourth
encoder blocks, to aggregate the spatiospectral features. the reason for this
design is that spectral features are simpler and lack hierarchical structures com-
pared to spatial features, we will discuss more in the experimental section."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"the feature maps obtained from the ensemble can be decoded using lightweight
2d decoders to generate segmentation masks. [31] with 538 scenes and hyperspectral gastric carcinoma (hgc)
dataset [33] (data provided by the author) with 414 scenes, both with high-
quality labels for binary mhsi segmentation tasks. these mhsis are collected
by hyperspectral system with an objective lens of 20x, and wavelengths from
550 nm to 1000 nm for mdc and 450 nm to 750 nm for hgc, resulting in 60 and
40 spectral bands for each scene."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"the size of a single band image in mdc and
hgc are both resized to 256 × 320. following [23,27], we partition the datasets
into training, validation, and test sets using a patient-centric hard split approach
with a ratio of 3:1:1. speciﬁcally, each patient’s data is allocated entirely to one
of the three sets, ensuring that the same patient’s data do not appear in multiple
sets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"the segmentation performance is evalu-
ated using dice-sørensen coeﬃcient (dsc), intersection of union (iou), and
hausdorﬀ distance (hd) metrics, and throughput (images/s) is reported for
158
b. yun et al.
fig. 3. feature redundancy of three methods on mdc dataset. left ﬁgures plot each
feature embedding in ascending order of the number of times they are dominant in the
population (y-axis) and feature dimension (x-axis) on the statistical results of the test
set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"as shown in table 1,
our ablation study shows that spectral agent strategy improves segmentation
performance by more than 2.5% (63.11 vs. 66.05). if we utilize spectral infor-
mation from the spectral stream to assist in the spatial stream, we ﬁnd that
inserting spectral information at l2 and l4 yields a signiﬁcant improvement of
3.7% (69.73 vs. 66.05), while inserting at l4 alone also results in a signiﬁcant
increase of 1.9% in dsc (67.95 vs. 66.05). a slight improvement is observed when
inserting at l2, possibly due to the coarse features of shallow spectral informa-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"therefore, we
adopt a simple and eﬃcient two-layer spectral ﬂow design. replacing the spec-
tral stream with transformer layers results in a 0.96% (70.88 vs. 69.89) lower
dsc, possibly because transformers are diﬃcult to optimize on small datasets. our proposed ld module is crucial, resulting in a 1.12% performance drop in
terms of dsc without it."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"3, the
factor space and spectrum
159
table 2. performance comparison in “mean (std)” in mdc and hcg dataset. the
best results of each comparison are highlighted."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"160
b. yun et al.
comparisons with state-of-the-art mhsi segmentation methods. table 3 shows comparisons on mdc and hgc datasets. we use a lightweight
and eﬃcient resnet34 as the backbone of our dual-stream method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_15.pdf,"our dual-stream strategy, leveraging
low-rank prior of mhsis, is computationally eﬃcient and plug-and-play, which
can be easily plugged into any 2d architecture. we evaluate our approach on
two mhsi datasets. experiments show signiﬁcant performance improvements on
diﬀerent evaluation metrics, e.g., with our proposed strategy, we can obtain over
7.7% improvement in dsc compared with its 2d counterpart."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"apd-net incorporates a novel architectural design that lever-
ages personalized and centralized parameters, along with a ﬁne-tuning
method based on a modiﬁed ga to identify personal characteristics. we validated apd-net on clinical datasets and demonstrated its supe-
rior performance, compared with state-of-the-art approaches. our exper-
imental results showed that apd-net markedly improved personalized
diagnostic accuracy by 9.9% in dermatitis diagnosis, making it a promis-
ing tool for clinical practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"keywords: personalized diagnosis · federate learning · mobile-based
diagnosis · skin cancer
k. lee and h. lee—contributed equally. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_37.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43898-1_37
fine-tuning network in federated learning for personalized skin diagnosis
379
1
introduction
for the past several years, in skin disease diagnosis, deep learning (dl) tech-
niques have been extensively studied, due to its eﬀectiveness and outstanding
diagnostic performance [8,16,19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"for the
development of dl models, a large number of datasets are needed for accurate
model ﬁtting at the training stage. acquiring enormous skin disease datasets,
however, at a single medical site is challenging. as such, the performance of dl
models is often limited, due to a small number of datasets [2,16,20].
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"1. pipeline of adp-net framework for a personalized diagnosis.
to overcome the limitations mentioned above, federated learning (fl) can be
a viable solution in the context of digital healthcare, especially in the covid-
19 pandemic era [15]. in particular, fl enables that the edge devices only share
the gradient of dl models without sharing data, such that fl improves the
data privacy and security. moreover, fl allows to acquire many heterogeneous
images from edge devices at multiple medical sites [1,6,7,10,15]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"for example,
b. mcmahan et al. developed a protocol to average the gradients from decen-
tralized clients, without data sharing [10]. however, dl models in
the fl environment were optimized to deal with datasets from multiple clients;
therefore, while the dl models yielded a generalized prediction capability across
all of the domains involved, the dl models cannot eﬃciently perform personal-
ized diagnosis, which is deemed a weakness of fl. to alleviate this, personalized
fl methodologies have been emerged [12,14,17]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"the ga-
based ﬁne-tuning method improves apd-net on each edge device by adaptively
customizing the optimized dl model. [13], as well as our own datasets. 380
k. lee et al.
experimental results demonstrated that the apd-net yielded outstanding per-
formance, compared with other comparison methods, and achieved adaptively
personalized diagnosis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"• we introduce a customized ga for apd-net, combined with a corresponding
network architecture, resulting in improved personalized diagnostic perfor-
mance as well as faster prediction time. • we provide a new ﬂuorescence dataset for skin disease diagnosis containing
2,490 images for four classes, including eczema, dermatitis, rosacea, and
normal. this dataset is made publicly available for future research in the
ﬁeld."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"therefore, in the dp architecture, one pipeline employs the generalized
parameters (pg) from the fl server, whereas the other pipeline employs the
personalized parameters (p ∗
pk) in the client. 2.2
customized genetic algorithm
in the fl environment, data privacy is achieved by transferring gradients with-
out sharing data. therefore, since the domain of one client is not recognizable by
another client, the domain gap between two clients is not computable."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"(4) after the diagnosis, the gradients
are shared with the fl server, and pg is newly optimized. conﬁgurations of datasets (left) and experiment i (right). (left) the number of skin images in the customized dataset for experiment
ii."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"(right) summary of key features of the classiﬁcation models. [14]
✓
ours
fl + da
✓
✓
✓
✓
3
experimental results
3.1
experimental setup
dataset. to evaluate the performance and feasibility of apd-net, we used
three public datasets, including 7pt, isic, and ham, and detailed descriptions
for datasets are illustrated in table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"furthermore, in this work, we collected
skin images through the tertiary referral hospital under the approval of the
institutional review board (irb no. 1908-161-1059) and obtained images with
the consent of the subjects according to the principles of the declaration of
helsinki from 51 patients and subjects. the dataset included four categories,
384
k. lee et al.
fig. 3. re-sampling the distribution of images in each client for experiment i
table 3."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"to assess the performance of the pro-
posed network as well as compared networks, two distinct fl environments were
considered: (1) an fl simulation environment to evaluate the performance of
apd-net and (2) a realistic fl environment to analyze the feasibility of apd-
net. the images in all skin datasets
were subsequently re-grouped, as illustrated in fig. 3. in contrast, for experi-
ment ii, we utilized a custom dataset for a realistic fl environment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"the highest values are highlighted as bold, and the second
values are underlined.
fig. 6. (left) confusion matrix of apd-net with the customized dataset in terms of
four-classes classiﬁcation. (right) correlation between similarity score (ﬁtness func-
tion) and accuracy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_37.pdf,"386
k. lee et al.
3.3
experiment ii. to verify the feasibility of apd-net, the performance of apd-net was evaluated
using the customized datasets acquired from our devices for adaptively person-
alized diagnosis. the performance of apd-nets was compared against the other
dl models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"intra-operative ultrasound (ius) has been adopted
to provide real-time images to track brain shift, and inter-modal (i.e.,
mri-ius) registration is often required to update the pre-surgical plan. quality control for the registration results during surgery is important
to avoid adverse outcomes, but manual veriﬁcation faces great challenges
due to diﬃcult 3d visualization and the low contrast of ius. automatic
algorithms are urgently needed to address this issue, but the problem was
rarely attempted."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"during the surgery, brain tissue deformation (called brain shift) can
occur due to various causes, such as gravity, drug administration, and pressure
change after craniotomy. while modern magnetic resonance imaging (mri) tech-
niques can provide rich anatomical and physiological information with various
contrasts (e.g., fmri) for more elaborate pre-surgical planning, intra-operative
mri that can track brain shift requires a complex setup and is costly. in contrast,
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"this can greatly enhance the safety and outcomes
of the surgical procedure by allowing maximum brain tumor removal while avoid-
ing eloquent regions [3]. however, as the true underlying tissue deformation is
unknown due to the 3d nature of the surgical data and the time constraint,
real-time manual inspection of mri-ius registration results is challenging and
error-prone, especially for precision-sensitive neurosurgery. therefore, algorithms
that can detect and quantify unreliable inter-modal medical image registration
results are highly beneﬁcial."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"with high eﬃciency, machine, and deep learning
techniques have been proposed to allow automatic grading and dense estima-
tion of medical image registration errors. early endeavors on this topic primar-
ily relied on hand-crafted features, including information theory-based metrics
[5–10]. unfortunately, so far, error grading and estimation in
inter-contrast/modal registration have rarely been explored, despite the partic-
ular demand in surgical applications."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"although their algorithm performed well in simulated
cases, the results on real clinical scans still required improvements. [13], a recent alter-
native dl technique to self-attention [14] for encoding contextual information,
and uncertainty estimation. we call our method focalerrornet, which has three
main novelties."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"second, we incorporated uncertainty estimation using monte carlo (mc)
dropouts [15] to oﬀer assurance for error regression. [12] using
real clinical data and showed excellent results. [16], which has pre-operative
focalerrornet for inter-modal registration error estimation
691
mri, and ius scans at diﬀerent surgical stages from 23 subjects who underwent
low-grade glioma resection surgeries."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"however,
since the true brain shift model is impossible to obtain, we followed the strategy
of creating silver ground truths for image alignment [9,12], upon which simulated
misalignment is augmented in the ius to build and test our dl model. to create
the silver registration ground truths, we used the homologous landmarks between
mri and ius in the resect dataset to perform landmark-based 3d b-spline
nonlinear registration to register ius to the corresponding mri for all 22 cases. to tackle the limited ﬁeld of view (fov) in ius, we cropped the t2flair mri
to the same fov of the ius, which was resampled to a 0.5 × 0.5 × 0.5 mm3
resolution.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"after misalignment augmentation
on the previously co-registered ius, matching pairs of 3d image patches of size
33 × 33 × 33 voxels were taken from both the ius volume and the correspond-
ing mri. as ius has limited fov and may contain no anatomical features, to
ensure that the patches we extracted contain useful information (e.g. to avoid the
dark background) in ius, we focused on acquiring patches centered around the
692
s. salari et al.
anatomical landmark locations available through the resect database. since
b-spline transformation oﬀers a displacement vector at each voxel of the ius
volume, we directly considered the norm of the vector as the simulated registra-
tion error at the associated voxel."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"2.2
network architecture
we proposed a novel 3d neural network, named focalerrornet, based on the
recent focal modulation networks [13] that was originally proposed for 2d vision
tasks to estimate the registration error between mri and ius patches. with a
similar goal as the vision transformer (vit), the focal modulation network was
designed to model contextual information in images. it incorporates three main
elements to achieve the goal: 1) focal contextualization that comprises a stack of
depth-wise convolutional layers to account for long- to short-range dependencies,
2) gated aggregation to collect contexts into a modulator for individual query
tokens, and 3) element-wise aﬃne transformation to inject the modulator into
the query."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"in the architecture of focalerrornet (see fig. 2), all layers contain two
focal modulator blocks, where two depth-wise convolutional layers focally extract
contexts around each voxel, selectively aggregate and inject them into the query,
and pass the information to the next block. we designed the focalerrornet as
a resnet-like variant of the focal modulation network to better encode rele-
vant features across the input image and ensure a better gradient ﬂow."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"the baseline 3d cnn [9,12] with the added mc dropout layer. 2.3
uncertainty quantiﬁcation
for registration error regression in surgical applications, knowledge regarding
the reliability of the automated results is instrumental for the safety and well-
being of the patients. uncertainty estimation has gained popularity in probing
the trustworthiness and credence of dl algorithms."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"for our experiments, we arbitrarily split the subjects into training, valida-
tion, and test sets with the proportion of 60%, 20%, and 20%, respectively. to
prevent information leakage, we ensured that each patient was included in only
one of the split sets. for model training, we adopted the adam optimization
with a learning rate of 5 × 10−5 and a batch size of 64."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"for the loss function, we
used mean squared error (mse) to minimize the diﬀerence between the predicted
mri-ius registration error and the ground truths. furthermore, in addition to
the transformation augmentation, we also included additional data augmenta-
tion, including random noise addition and random image ﬂipping on training sets
694
s. salari et al.
to mitigate overﬁtting and increase the model’s generalizability. to assess our
proposed focalerrornet, we compared it against a 3d cnn [9,12] (see fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"3)
that was employed for medical image registration error regression. the two dl
models were trained with the same dataset and procedure, and their prediction
accuracies, measured as the absolute error between the predicted and ground
truths mis-registration on the test set were compared with two-sided paired-
samples t-tests to conﬁrm the superiority of the proposed method, in addition
to correlations between their estimated and ground truth errors. to validate the
proposed uncertainty estimation method, we calculated the correlation between
the uncertainty measure and absolute error of focalerrornet, and the correla-
tion between the uncertainty and mutual information between mri and ius,
which is often used to measure the information overlap in multi-modal registra-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"5. left to right: scatter plots and correlations between true registration error vs.
predicted registration error, uncertainty metric vs. absolute error, and mutual infor-
mation vs. uncertainty metric for the 3d cnn [9,12].
3.2
validation of the uncertainty evaluation
we obtained correlations of 0.70 (p < 1e-4) and 0.34 (p < 1e-4) between esti-
mated uncertainty and prediction error for focalerrornet and the baseline 3d
cnn, respectively. additionally, the uncertainty vs. mutual information uncer-
tainties was assessed at –0.67 (p < 1e-4) for our proposed method and –0.18 (p <
1e-3) for the baseline. to allow better visual comparisons, the associated scatter
plots are illustrated in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"the main reason for the observed
performance decline is due to the reduction in suﬃcient image features in ius.
however, despite these challenges, we saw an acceptable outcome from focaler-
rornet (absolute error = 1.28 mm or ∼1 voxel in clinical mris). 696
s. salari et al.
4
discussion
in image-guided interventions, there is an urgent need for automatic assessment
of image registration quality. multi-modal registration quality evaluation poses
major challenges due to three main factors."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"finally, compared with classiﬁcation,
regression tasks tend to be more error-prone for deep learning algorithms. to
tackle these challenges, we employed 3d focal modulation with depth-wise con-
volution to encode contextual information for the image pair. compared with the
vit and its variants, focal modulation allows a more lightweight setup, which
could be desirable for 3d data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"although we admit that residual errors still
remain after landmark-based b-spline nonlinear alignment, this approach has
been adopted in diﬀerent prior studies, considering the residual landmark reg-
istration error is fairly low (mtre of 0.0008 ± 0.0010mm). although simulated
ultrasound has been used to provide a perfect alignment with mris, the ﬁdelity
of the simulated results is still suboptimal, and this may explain the under-
performance of the previous technique in real clinical data [12]. to ensure the
performance of our focalerrornet, we opted to regress the mean registration
error of image patches than simplistic error grades or voxel-wise error maps."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_66.pdf,"these signify a robust performance of the focalerrornet. one limitation of our work lies in the limited patient data, as public ius datasets
are scarce, while the settings and properties of us scanners can vary, potentially
aﬀecting the dl model designs. therefore, we created random deformations for
patch-wise error estimation, and will further explore data-eﬃcient approaches
for registration error assessment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"here we propose a novel focal transformer-based
image segmentation architecture to eﬀectively and eﬃciently extract local
visual features and global context from ct images. additionally, we design
an auxiliary boundary-induced label regression task coupled with the main
prostate segmentation task to address the unclear boundary issue in ct
images. we demonstrate that this design signiﬁcantly improves the quality
of the ct-based prostate segmentation task over other competing meth-
ods, resulting in substantially improved performance, i.e., higher dice sim-
ilarity coeﬃcient, lower hausdorﬀ distance, and average symmetric sur-
face distance, on both private and public ct image datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"as a result, precise prostate seg-
mentation in ct images becomes a crucial step, as it helps to ensure that the
radiation doses are delivered eﬀectively to the tumor tissues while minimizing
harm to the surrounding healthy tissues. due to the relatively low spatial resolution and soft tissue contrast in ct
images compared to mri images, manual prostate segmentation in ct images
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 57. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14222, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"https://doi.org/10.1007/978-3-031-43898-1_57
focalunetr
593
can be time-consuming and may result in signiﬁcant variations between operators
[10]. despite good progress,
these methods often have limitations in capturing long-range relationships and
global context information [2] due to the inherent bias of convolutional opera-
tions. [2] adapts vit to medical image segmentation
tasks by connecting several layers of the transformer module (multi-head sa)
to the fcn-based encoder for better capturing the global context information
from the high-level feature maps."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"unetr [6] and siwnunetr
[20] are transformer architectures extended for 3d inputs. as reported by [20], even pre-trained with a massive amount of medical
data using self-supervised learning, the performance of prostate segmentation
task using high-resolution and better soft tissue contrast mri images has not
been completely satisfactory, not to mention the lower-quality ct images. addi-
tionally, the unclear boundary of the prostate in ct images derived from the
low soft tissue contrast is not properly addressed [7,22]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"second, we also address the challenge of unclear boundaries speciﬁc to ct images
by incorporating an auxiliary task of contour regression. third, our methodology
advances state-of-the-art performance via extensive experiments on both real-
world and benchmark datasets. 2
methods
2.1
focalunetr
our focalunetr architecture (fig. 1) follows a multi-scale design similar to
[6,20], enabling us to obtain hierarchical feature maps at diﬀerent stages."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"in the sub-window pooling step, an
input feature map x ∈ rd×h′′×w ′′ is split into a grid of sub-windows with size
{sl
w, sl
w}, followed by a simple linear layer f l
p to pool the sub-windows spatially. the pooled feature maps at diﬀerent levels l provide rich information at both
ﬁne-grained and coarse-grained, where xl = f l
p(ˆx) ∈ r
d× h′′
slw
× w ′′
slw , and ˆx =
reshape(x) ∈ r
(d× h′′
slw
× w ′′
slw
)×(sl
w×sl
w). after obtaining the pooled feature maps
xll
1 , we calculate the query at the ﬁrst level and key and value for all levels using
three linear projection layers fq, fk, and fv:
q = fq(x1), k = {kl}l
1 = fk({x1, . . ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"this auxiliary
task is achieved by attaching another convolution head after the extracted fea-
ture maps at the ﬁnal stage (see fig. the boundary-aware contour, or the
induced boundary-sensitive label, is generated by considering pixels near the
boundary of the prostate mask. the
resulting contour is a heatmap in the form of a heatsum function [11]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"the optimal
setting of λ1 = λ2 = 0.5 is determined by trying diﬀerent settings. as far as we know,
the amos dataset is the only publicly available ct dataset including prostate
ground truth. we randomly split the private dataset with 280 scans for training,
40 for validation, and 80 for testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"the amos dataset has 200 scans for training
and 100 for testing [9]. although the amos dataset includes the prostate class,
it mixes the prostate (in males) and the uterus (in females) into one single class
labeled pro/ute. we ﬁlter out ct scans missing the pro/ute ground-truth
segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"all experiments are conducted in pytorch,
and each model is trained on a single gpu. we interpolate all ct scans into
an isotropic voxel spacing of [1.0 × 1.0 × 1.5] mm for both datasets. houndsﬁeld
unit (hu) range of [−50, 150] is used and normalized to [0, 1]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"for 2d models, we ﬁrst slice each voxel
patch in the axial direction into 64 slices of 128 × 128 images for training and
stack them back for evaluation. for the private dataset, we train models for 200
epochs using the adamw optimizer with an initial learning rate of 5e−4. an
exponential learning rate scheduler with a warmup of 5 epochs is applied to the
optimizer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"we
use random ﬂip, rotation, and intensity scaling as augmentation transforms with
probabilities of 0.1, 0.1, and 0.2, respectively. we also tried using 10% percent of
amos training set as validation data to ﬁnd a better training parameter setting
and re-trained the model with the full training set. however, we did not get
improved performance compared with directly applying the training parameters
learned from tuning the private dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"we report the dice similarity coef-
ﬁcient (dsc, %), 95% percentile hausdorﬀ distance (hd, mm), and average
symmetric surface distance (assd, mm) metrics.
3.2
experiments
comparison with state-of-the-art methods. quantitative performance comparison on the private and amos datasets
with a mean (standard deviation) for 3 runs with diﬀerent seeds. an asterisk (*) denotes
the model is co-trained with the auxiliary contour regression task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"transunet and swin-unet are the
only methods that are pre-trained on imagenet. detailed information regarding
the number of parameters, flops, and average inference time can be found in
the supplementary materials. quantitative results are presented in table 1, which shows that the proposed
focalunetr, even without co-training, outperforms other fcn and trans-
former baselines (2d and 3d) in both datasets for most of the metrics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"sequently, the models may struggle to provide accurate predictions for this spe-
ciﬁc portion of the uterus. thus, the overall performance of focalunetr is
overshadowed by this challenge, resulting in only moderate improvement over
the baselines on the amos dataset. however, the performance margin signiﬁ-
cantly improves when using the real-world (private) dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"in summary, these
observations indicate that incorporating focalunetr and multi-task training
focalunetr
599
fig. datasets
with an auxiliary contour regression task can improve the challenging ct-based
prostate segmentation performance. qualitative results of several representative methods are visualized in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"all methods perform well for relatively
easy cases (1st row in fig. 3), but the focalunetrs outperform the other meth-
ods. for more challenging cases (rows 2–4 in fig. 3), such as unclear boundaries
and mixed pro/ute labels, focalunetrs still perform better than other
methods. additionally, the focalunetrs are less likely to produce false posi-
tives (see more in supplementary materials) for ct images without a foreground
ground truth, due to the focal sa mechanism that enables the model to cap-
ture global context and helps to identify the correct boundary and shape of the
prostate."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_57.pdf,"overall, the focalunetrs demonstrate improved segmentation capa-
bilities while preserving shapes more precisely, making them promising tools for
clinical applications. to better examine the eﬃcacy of the auxiliary task for
focalunetr, we selected diﬀerent settings of λ1 and λ2 for the overall loss
600
c. li et al.
function ltol on the private dataset. the results (table 2) indicate that as the
value of λ2 is gradually increased and that of λ1 is correspondingly decreased
(thereby increasing the relative importance of the auxiliary contour regression
task), segmentation performance initially improves."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"foundation models have exhibited remarkable success in var-
ious applications, such as disease diagnosis and text report generation.
to date, a foundation model for endoscopic video analysis is still lack-
ing. in this paper, we propose endo-fm, a foundation model speciﬁcally
developed using massive endoscopic video data. first, we build a video
transformer, which captures both local and global long-range dependen-
cies across spatial and temporal dimensions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"second, we pre-train our
transformer model using global and local views via a self-supervised
manner, aiming to make it robust to spatial-temporal variations and
discriminative across diﬀerent scenes. to develop the foundation model,
we construct a large-scale endoscopy video dataset by combining 9 pub-
licly available datasets and a privately collected dataset from baoshan
branch of renji hospital in shanghai, china. our dataset overall consists
of over 33k video clips with up to 5 million frames, encompassing var-
ious protocols, target organs, and disease types."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"with experiments on 3 diﬀerent types of down-
stream tasks, including classiﬁcation, segmentation, and detection, our
endo-fm surpasses the current state-of-the-art (sota) self-supervised
pre-training and adapter-based transfer learning methods by a signiﬁcant
margin, such as vcl (3.1% f1, 4.8% dice, and 5.5% f1 for classiﬁcation,
segmentation, and detection) and st-adapter (5.9% f1, 9.6% dice, and
9.9% f1 for classiﬁcation, segmentation, and detection). code, datasets,
and models are released at https://github.com/med-air/endo-fm. keywords: foundation model · endoscopy video · pre-train
1
introduction
foundation models pre-trained on large-scale data have recently showed suc-
cess in various downstream tasks on medical images including classiﬁcation [9],
z. wang and c. liu—equal contributions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14228, pp. however, medical data have various imag-
ing modalities, and clinical data collection is expensive. it is arguable that a
speciﬁc foundation model trained on some certain type of data is useful at the
moment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"[20,21], involves pre-training on large-scale
image-text pairs and relies on large language models to learn cross-modality
features. however, since clinical routines for endoscopy videos typically do not
involve text data, a pure image-based foundation model is currently more fea-
sible. to this end, we develop a video transformer, based on vit b/16 [8],
containing 121m parameters, which serves as the foundation model backbone
for our video data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"we note that a similarly scaled foundation model in recent
work [33] based on swin unetr [11] with 62m parameters has been success-
fully employed for ct scans. this would indicate that our video transformer
could have suﬃcient capacity to model the rich spatial-temporal information of
endoscopy videos.
to learn rich spatial-temporal information from endoscopy video data [12],
our endo-fm is pre-trained via a self-supervised manner by narrowing the gap
between feature representations from diﬀerent spatial-temporal views of the same
video. these views are generated to address the variety of context informa-
tion and motions of endoscopy videos."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"under this scheme, the student is trained to predict (match)
the teacher’s output in the latent feature space. in other words, given two spatial-
temporal aware views from the same video, one view processed by the teacher is
predicted by another one processed by the student to learn the spatial-temporal
information. therefore, designing eﬀective and suitable matching strategies for
diﬀerent spatial-temporal views from the same endoscopy video is important."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"in this paper, we propose endo-fm, a novel foundation model designed
for endoscopic video analysis. first, we build a video transformer based on
vit [8] to capture long-range spatial and temporal dependencies, together with
dynamic spatial-temporal positional encoding designed for tackling input data
with diverse spatial sizes and temporal frame rates. second, endo-fm is pre-
trained under a teacher-student scheme via spatial-temporal matching on diverse
video views."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"this enables endo-fm to learn spatial-temporal invariant
(to view, scale, and motion) features that are transferable across diﬀerent endo-
scopic domains and disease types while retaining discriminative features that are
speciﬁc to each context. we construct a large-scale endoscopic video dataset by
foundation model for endoscopy video analysis
103
fig. 1. illustration of our proposed endo-fm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"we build a video transformer model and
design a self-supervised pre-training approach. combining 9 public and a new private collected dataset from baoshan branch of
renji hospital in shanghai, china, with over 33k video clips with up to 5 million
frames. our pre-trained endo-fm can be easily applied to various downstream
tasks by serving as the backbone."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"such dynamic strategy ensures that the learned positional
encoding is suitable for downstream tasks with diverse input sizes. 2.2
self-supervised pre-train via spatial-temporal matching
considering the diﬃculties of tackling the context information related with
lesions, tissues, and dynamic scenes in endoscopic data, we pre-train endo-fm
to be robust to such spatial-temporal characteristics. [6], the pre-training is designed in a teacher-student scheme,
where the student is trained to match the teacher’s output."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"diﬀerent from image-based pre-training [33], our
video-oriented pre-training is designed to capture the relationships between dif-
ferent spatial-temporal variations. speciﬁcally, the context information presented
in diﬀerent frames of the same endoscope video can vary under two key factors:
1) the proportion of tissue and lesions within the frame, and 2) the presence
or absence of lesion areas. to address these, we employ a cross-view matching
foundation model for endoscopy video analysis
105
approach where the target global views processed by the teacher ({pt
v i
g}g
i=1) are
predicted from the online local views processed by the student ({ps
v j
l }l
j=1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"the speeds and ranges of motion
can vary greatly across diﬀerent videos, making it diﬃcult to train a model
that is eﬀective across a wide range of dynamic scenarios. the previous model
[27] learned from clips with ﬁxed frame rate can not tackle this issue, as clips
sampled with various frame rates contain diﬀerent motion context information
(e.g., fast v.s. slow scene changing) and diﬀer in nuanced tissue and lesions. to address this challenge, our approach involves motion modeling during pre-
training under dynamic endoscope scenes by predicting a target global view (pt
v ig)
processed by the teacher from another online global view (ps
v kg) processed by the
student."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"except for the challenges posed by the issues of size proportion, existence,
and dynamic scenes in sect. 2.2, we have also observed that the appearance of
106
z. wang et al.
table 1. details of all pre-train and downstream datasets used in this work. [2]
uab
29
612
colonoscope polyp
kumc [15]
kansas
53
19832
colonoscope adenoma, hyperplasia
summary
3 providers
335
506005
2 protocols
4 diseases
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"the training batch size
is 12 with adamw [17] optimizer (learning rate 2e−5, weight decay 4e−2). the
pre-training is ﬁnished with 30 epochs with a cosine schedule [16].
3
experiment
3.1
datasets and downstream setup
we collect all possible public endoscope video datasets and a new one from
baoshan branch of renji hospital for pre-training. as shown in table 1, these
public datasets are provided by world-wide research groups [5,13,18,19,30] and
previous endovis challenge [23], covering 3 endoscopy protocols and 10+ types
of diseases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"we evaluate our pre-trained endo-fm on three downstream
tasks: disease diagnosis (polypdiag [32]), polyp segmentation (cvc-12k [2]), and
detection (kumc [15]). the detailed information of three downstream datasets
is shown in table 1. the example frames of the 10 datasets are shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"moreover, our endo-fm outperforms all
sota methods, with +3.1% f1, +4.8% dice, and +5.5% f1 boosts for the
3 downstream tasks over the second-best. such signiﬁcant improvements are
beneﬁted from our speciﬁc spatial-temporal pre-training designed for endoscopy
videos to tackle the complex context information and dynamic scenes. mean-
while, endo-fm requires less pre-training time than sota pre-training meth-
ods, except the lighter but much worse st-adapter [24].
3.3
analytical studies
without loss of generality, we conduct ablation studies on polyp diagnosis task
from 3 aspects: 1) components analysis of our pre-training method; 2) varying
combinations of global and local views in spatial-temporal matching; 3) varying
the construction of global and local views.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"we can
learn that both spatial and temporal sampling for local views can help improve
the performance and their combination produces a plus, yielding +4.3% f1
improvement. furthermore, our proposed dynamic matching scheme boosts the
performance to 89.7%, demonstrating the importance of capturing the motion
related context information from dynamic scenes. additionally, the performance
is further improved with video augmentations from 89.7% to 90.7%."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_10.pdf,"these improvements stem from the spatial-temporal change invariant and
cross-video discriminative features learned from the diverse endoscopy videos. 4
conclusion and discussion
to the best of our knowledge, we develop the ﬁrst foundation model, endo-
fm, which is speciﬁcally designed for analyzing endoscopy videos. endo-fm
is built upon a video transformer to capture rich spatial-temporal information
and pre-trained to be robust to diverse spatial-temporal variations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf,"extensive experiments demonstrate the supe-
rior performance of freeseed and its dual-domain counterpart over the
state-of-the-art sparse-view ct reconstruction methods. source code is
made available at https://github.com/masaaki-75/freeseed.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 24. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14229, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf,"lowering the dose of ct
scans has been widely adopted in clinical practice to address this issue, following
the “as low as reasonably achievable” (alara) principle in the medical com-
munity [9]. sparse-view ct is one of the eﬀective solutions, which reduces the
radiation by only sampling part of the projection data for image reconstruction. nevertheless, images reconstructed by the conventional ﬁltered back-projection
(fbp) present severe artifacts, thereby compromising their clinical value."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf,"in recent years, the success of deep learning has attracted much attention in
the ﬁeld of sparse-view ct reconstruction. existing learning-based approaches
mainly include image-domain methods [2,4,18] and dual-domain ones [7,13,16],
both involving image post-processing to restore a clean ct image from the
low-quality one with streak artifacts. for the image post-processing, residual
learning [3] is often employed to encourage learning the artifacts hidden in
the residues, which has become a proven paradigm for enhancing the perfor-
mance [2,4,6,16]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf,"with these novel designs, we introduce a simple
yet eﬀective model termed frequency-band-aware and self-guided network
(freeseed), which enhances the reconstruction by modeling the pattern of arti-
facts from a frequency perspective and utilizing the artifact to restore the details. freeseed achieves promising results with only image data and can be further
enhanced once the sinogram is available.
252
c. ma et al. our contributions can be summarized as follows: 1) a novel frequency-band-
aware network is introduced to eﬃciently capture the pattern of global artifacts
in the fourier domain among diﬀerent sparse-view scenarios; 2) to promote the
restoration of heavily corrupted image detail, we propose a self-guided artifact
reﬁnement network that ensures targeted reﬁnement of the reconstructed image
and consistently improves the model performance across diﬀerent scenarios; and
3) quantitative and qualitative results demonstrate the superiority of freeseed
over the state-of-the-art sparse-view ct reconstruction methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf,"(color ﬁgure online)
2
methodology
2.1
overview
given a sparse-view sinogram with projection views nv, let is and if denote
the directly reconstructed sparse- and full- view images by fbp, respectively. in
this paper, we aim to construct an image-domain model to eﬀectively recover is
with a level of quality close to if. the proposed framework of freeseed is depicted in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf,"2. overview of the proposed freeseed. despite the eﬀectiveness, a simple fourier unit in ffc could still preserve
some low-frequency information that may interfere with the learning of arti-
facts, which could fail to accurately capture the banded pattern of the fea-
tures of sparse-view artifacts in the frequency domain. to this end, we propose
to incorporate learnable band-pass attention maps into ffc."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf,"the pseudo-code for the training process and
the exploration on the selection of α can be found in our supplementary material. once the training is complete, seednet can be dropped and the prediction is done
by freenet.
frequency-band-aware and self-guided network for sparse-view ct
255
2.5
extending freeseed to dual-domain framework
dual-domain methods are eﬀective in the task of sparse-view ct reconstruction
when the sinogram data are available. [7], where the resulting
dual-domain counterpart shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf,"we refer the readers to lin et al. [7] for more information. [8], which contains 5,936 ct slices in 1 mm
image thickness from 10 anonymous patients, where a total of 5,410 slices from
9 patients, resized to 256 × 256 resolution, are randomly selected for training
and the 526 slices from the remaining one patient for testing without patient
overlap."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_24.pdf,"in general,
freeseed successfully restores the tiny clinical structures (the spines in the ﬁrst
row, and the ribs in the second row) while achieving more comprehensive artifact
removal (see the third row). note that when the sinogram data are available,
dual-domain counterpart freeseeddudo gains further improvements, showing the
great ﬂexibility of our model.
3.3
ablation study
table 2 presents the eﬀectiveness of each component in freeseed, where seven
variants of freeseed are: (1) fbpconv upon which freenet is built (baseline); (2)
freenet without band-pass attention maps nor seednet guidance lmask (baseline
+ fourier); (3) fbpconv trained with lmask (baseline + seednet); (4) freenet
frequency-band-aware and self-guided network for sparse-view ct
257
trained without lmask (freenet); (5) freenet trained with simple masked loss
l1+mask = ∥(af − 	a) ⊙ (1 + m)∥2 (freenet1+mask); (6) freenet trained with
lmask using ℓ1 norm (freeseedℓ1); and (7) freenet trained with lmask using ℓ2
norm, i.e., the full version of our model (freeseed). by comparing the ﬁrst two rows of table 2, we ﬁnd that simply applying ffc
provides limited performance gains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"automated detection of gallbladder cancer (gbc) from
ultrasound (us) images is an important problem, which has drawn
increased interest from researchers. however, most of these works use
diﬃcult-to-acquire information such as bounding box annotations or
additional us videos. in this paper, we focus on gbc detection using
only image-level labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"however, our analysis reveals that it is diﬃ-
cult to train a standard image classiﬁcation model for gbc detection. this is due to the low inter-class variance (a malignant region usually
occupies only a small portion of a us image), high intra-class variance
(due to the us sensor capturing a 2d slice of a 3d object leading to large
viewpoint variations), and low training data availability. we posit that
even when we have only the image level label, still formulating the prob-
lem as object detection (with bounding box output) helps a deep neural
network (dnn) model focus on the relevant region of interest."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"early
diagnosis can signiﬁcantly improve the survival rate [14]. non-ionizing radiation,
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 20.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14220, pp. 206–215, 2023.
https://doi.org/10.1007/978-3-031-43907-0_20
gall bladder cancer detection from us images
207
low cost, and accessibility make us a popular non-invasive diagnostic modality for
patients with suspected gall bladder (gb) aﬄictions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"such bounding box anno-
tations surrounding the pathological regions are time-consuming and require an
expert radiologist for annotation. this makes it expensive and non-viable for
curating large datasets for training large dnn models. in another recent work, [5]
has exploited additional unlabeled video data for learning good representations
for downstream gbc classiﬁcation and obtained performance similar to [3] using
a resnet50 [13] classiﬁer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"the reliance of both sota techniques on additional
annotations or data, limits their applicability. on the other hand, the image-level
malignancy label is usually available at a low cost, as it can be obtained readily
from the diagnostic report of a patient without additional eﬀort from clinicians. instead of training a classiﬁcation pipeline, we propose to solve an object
detection problem, which involves predicting a bounding box for the malignancy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"the motivation is that, running a classiﬁer on a focused attention/ proposal
region in an object detection pipeline would help tackle the low inter-class and
high intra-class variations. however, since we only have image-level labels avail-
able, we formulate the problem as a weakly supervised object detection (wsod)
problem. [6,9], we choose to use transform-
ers in our model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"these methods primarily rely on training a clas-
siﬁcation pipeline and later generating activation heatmaps using attention and
208
s. basu et al.
fig. 2. samples from the gbcu [3] and kvasir-seg [17] datasets. four images from
each of the disease and non-disease classes are shown on the left and right, respectively.
disease locations are shown by drawing bounding boxes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"inspired by the success of the multiple instance learning (mil) paradigm for
weakly supervised training on medical imaging tasks [20,22], we train a detec-
tion transformer, detr, using the mil paradigm for weakly supervised malignant
region detection. at inference, we
use the predicted instance labels to predict the bag labels. our experiments val-
idate the utility of this approach in circumventing the challenges in us images
and detecting gbc accurately from us images using only image-level labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"although mil and self-supervised instance learning has
been used for cnns [24], such a pipeline has not been used for transformer-
based detection models. – we formulate the gbc classiﬁcation problem as a weakly supervised object
detection problem to mitigate the eﬀect of low inter-class and large intra-class
variances, and solve the diﬃcult gbc detection problem on us images without
using the costly and diﬃcult to obtain additional annotation (bounding box)
or video data. – our method provides a strong baseline for weakly supervised gbc detection
and localization in us images, which has not been tackled earlier."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"[3] consisting of 1255 image samples from 218 patients. the dataset contains 990 non-malignant (171 patients) and 265 malignant (47
patients) gb images (see fig. 2 for some sample images)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"the dataset contains
image labels as well as bounding box annotations showing the malignant regions. note that, we use only the image labels for training. we report results on 5-fold
cross-validation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"3. overview of the proposed weakly supervised detr architecture. the location
information in the object queries learned by the class-agnostic detr ensures generation
of high-quality proposals. the mil framework uses the proposal embeddings generated
at the class-aware branch."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"[17] dataset consisting of 1000 white light colonoscopy images show-
ing polyps (see fig. since kvasir-seg does not contain any control images,
we add 600 non-polyp images randomly sampled from the polypgen [1] dataset. since the patient information is not available with the data, we use random
stratiﬁed splitting for 5-fold cross-validation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"[6] architectures utilize a resnet [13] backbone
to extract 2d convolutional features, which are then ﬂattened and added with a
positional encoding, and fed to the self-attention-based transformer encoder. the
decoder uses cross-attention between learned object queries containing positional
embedding, and encoder output to produce output embedding containing the
class and localization information. the number of object queries, and the decoder
210
s. basu et al.
output embeddings is set to 100 in detr."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"we use a
coco pre-trained class-agnostic detr as proposal generator. the learned object
queries contain the embedded positional information of the proposal. class-
agnostic indicates that all object categories are considered as a single object
class, as we are only interested in the object proposals."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"we
ﬁnally use the mil-based instance classiﬁcation with the self-supervised instance
learning over the ﬁnetuning branch. for gbc classiﬁcation, if the model generates
bounding boxes for the input image, then we predict the image to be malignant,
since the only object present in the data is the cancer. mil setup: the decoder of the ﬁne-tuning detr generates r d-dimensional
output embeddings."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"the
instance supervision for the ﬁrst layer is obtained from the mil head. suppose
ˆyn ∈ rr×(nc+1) is the pseudo-labels of the instances. an instance (pj) is labelled
1 if it overlaps with the highest-scoring instance by a chosen threshold."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"other
ablations related to the hyper-parameter sensitivity is given in supplementary
fig. classiﬁcation performance: we compare our model with the standard cnn-
based and transformer-based classiﬁers, sota wsod-based classiﬁers, and sota
classiﬁers using additional data or annotations (table 3). our method beats the
sota weakly supervised techniques and achieves 1.2% higher sensitivity for gbc
detection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_20.pdf,"method
acc.
spec. however, even without these addi-
tional annotations/ data, our method reaches 86.1% detection sensitivity. the
results for polyp classiﬁcation are reported in table 4."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"the mask strategy is randomly masking a ﬁxed-
length contiguous subsequence of patch embeddings of a wsi. finally,
we combine the classiﬁcation tokens of paired modalities and propose
a triplet learning module to learn high-order relevance and discrimina-
tive patient-level information. after pre-training, a simple ﬁne-tuning
can be adopted to obtain the classiﬁcation results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"keywords: multimodal learning · whole slide image classiﬁcation
1
introduction
pathological image-omic analysis is the cornerstone of modern medicine and
demonstrates promise in a variety of diﬀerent tasks such as cancer diagnosis
and prognosis [12]. with the recent advance of digital pathology and sequencing
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2_49.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43987-2_49
gene-induced multimodal pre-training for image-omic classiﬁcation
509
technologies, modern cancer screening has jointly incorporated genomics and
histology analysis of whole slide images (wsis)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"(1) the gigapixel wsis, which generally yield 15,000 foreground patches
during pre-processing, make attention-based backbones [6] hard to extract pre-
cise image (wsi)-level representations. (2) learning features from genomics data
which have tens of thousands of genes make models such as transformer [16]
impractical to use due to its quadratic computation complexity. (3) image-omic
feature fusion [2,3] may fail to model high-order relevance and the inherent struc-
tural characteristics of each modality, making the fusion less eﬀective."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"a big domain gap also hampers their usage in leverag-
ing the structural characteristic of tumor micro-environment and genomic assay. recently, the literature corpus has proposed some methods for accomplishing
speciﬁc image-omic tasks via kronecker product fusion [2] or co-attention map-
ping between wsis and genomics data [3]. but, the kronecker product overly
concerns feature interactions between modalities while ignoring high-order rele-
vance, w.r.t. decision boundaries across multiple samples, which is critical to clas-
siﬁcation tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"our mpm only needs
to recover the masked patch embeddings in a ﬁxed-length subsequence rather
than processing all patches from wsis. furthermore, to model the high-order
relevance of the two modalities, we combine cls tokens of paired image and
genomic data to form uniﬁed representations and propose a triplet learning mod-
ule to diﬀerentiate patient-level positive and negative samples in a mini-batch. it is worth mentioning that although our uniﬁed representation fuses features
from the whole gene expression cohort and partial wsis in a mini-batch, we
510
t. jin et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"then we use two modality-speciﬁc encoders to capture unimodal features. two pre-
training objectives are considered: 1) building triplets by concatenated cls tokens of
each modality and enhancing the discriminability according to category relations, and
2) reconstructing the missing patch embeddings by its adjacent patches.
can still learn high-order relevance and discriminative patient-level information
between these two modalities in pre-training thanks to the triplet learning mod-
ule. in addition, note that our proposed method is diﬀerent from self-supervised
pre-training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"and
inter-class variation. with the training process going on, complete information
from wsis can be integrated and the fused multimodal representations with
high discrimination will make it easier for the classiﬁer to ﬁnd the classiﬁcation
hyperplane. experimental results demonstrate that our gimp achieves signiﬁ-
cant improvement in accuracy than other image-omic competitors, and our mul-
timodal framework shows competitive performance even without pre-training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"+ 1)
tokens per group. then the prepared tokens are fed to a vanilla multi-head
self-attention (msa) block to extract intra-group information. after that, we
model cross-group interactions by another msa layer on the global scale with
the locally learned group tokens and a ﬁnal classiﬁcation token clsge ∈ rd.
finally, groupmsa could learn dense semantics from the genomic data cohort."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"in wsis, the foreground patches are spatially con-
tiguous, which means the adjacent patches have similar feature embeddings. thus, we propose a masked patch modeling (mpm) pre-training strategy that
masks random patch embeddings from a ﬁxed-length contiguous subsequence
hmpm =

hj | hj ∈ r1024l+i
j=i in hp and reconstruct the invisible information. the ﬁxed subsequence length l is empirically set to 6,000 and the sequences
shorter than l are duplicated to build mini batches."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"[·] is the indicator function.
gene-induced triplet learning. the transformer-based backbones in the
classiﬁcation task require the cls token to be able to extract accurate global
information, which is even more important yet diﬃcult in wsis due to the long
sequence challenge. in addition, in order to construct the mini-batch, the sub-
sequences we intercept in the mpm pre-training phase may not be suﬃciently
representative of the image-level characteristics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"we use a simple multi-layer perceptron
(mlp) head to map clspat to the ﬁnal class predictions ˆp, which can be
written as ˆp = softmax(mlp(clspat)). 3
experiments
3.1
experimental setup
datasets. we verify the eﬀectiveness of our method on the caner genome
atlas (tcga) non-small cell lung cancer (nsclc) dataset, which contains two
cancer subtypes, i.e., lung squamous cell carcinoma (lusc) and lung adeno-
carcinoma (luad)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"after pre-processing [11], the patch number extracted from
wsis at 20× magniﬁcation varies from 485 to 148,569. we collect correspond-
ing rna-seq fpkm data for each patient and the length of the input genomic
sequence is 60,480. among 946 image-omic pairs, 470 of them belong to luad
and 476 cases are lusc."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"we randomly split the data into 567 for training, 189
for validation and 190 for testing. the pre-training process of all algorithms is con-
ducted on the training set, without any extra data augmentation. note that our
genetic encoder, groupmsa, is fully supervised pre-trained on unimodal genetic
data to accelerate convergence and it is frozen during gimp training process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"all
experiments are conducted on a single nvidia geforce rtx 3090.
3.2
comparison between gimp and other methods
we conduct comparisons between gimp and three competitors under diﬀer-
ent settings. firstly, we compare our proposed patch aggregator with the cur-
rent state-of-the-art deep mil models on unimodal tcga-nsclc dataset, i.e.,
only pathological wsis are included as input. as shown in table 1, our proposed
patch aggregator outperforms all the compared attention based multiple instance
learning baselines in classiﬁcation accuracy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"[23],
three popular multimodal pre-training algorithms in medical text-image classiﬁ-
cation task. we can observe in the table that, our gimp raises acc from 91.05%
to 99.47% on tcga-nsclc dataset. [3], three inﬂuential image-omic classiﬁcation architectures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"we further explore why gimp works by insightful interpretation of the pro-
posed method with t-sne visualisation. figure 2 shows the feature mixtureness
of pre-trained clspat extracting global information on training set. compari-
gene-induced multimodal pre-training for image-omic classiﬁcation
515
table 2. ablation study on tcga lung cancer dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"aggregator groupmsa triplet mpm
acc. ✓
snn [7]
0.9684
✓
✓
0.9737
✓
✓
0.9579
✓
✓
0.9263
✓
✓
✓
0.9526
✓
✓
✓
✓
0.9974
son between fig. 2 (a) and (b) indicates that the addition of the genomic data is
indispensable in increasing the inter-class distance and reducing the intra-class
distance, which conﬁrms our motivation that gene-induced multimodal fusion
could model high-order relevance and yield more discriminative representations. 2 (c) and (d), clspat with gimp pre-trained are well sepa-
rated between luad and lusc, i.e., gimp pays more attention to the category-
related feature distribution and could extract more discriminative patient-level
features during triplet learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_49.pdf,"“aggrega-
tor + groupmsa + triplet” means gimp only combines the cls tokens of
each modality and calculates triplet loss during pre-training. we can observe a
performance drop without mpm module, e.g., from 99.47% to 95.26%, which
demonstrates that local pathological information is equally critical as high-order
relevance. 516
t. jin et al.
4
conclusion
in this paper, we propose a novel multimodal pre-training method to exploit
the complementary relationship of genomic data and pathological images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_21.pdf,"to this end, a novel geometric framework for microbubble local-
ization via ellipse intersections is proposed to overcome existing beam-
forming limitations. we present a benchmark comparison based on a
public dataset for which our geometric ulm outperforms existing base-
line methods in terms of accuracy and robustness while only utilizing a
portion of the available transducer data. keywords: ultrasound · microbubble · localization · microscopy ·
geometry · parallax · triangulation · trilateration · multilateration ·
time-of-arrival
1
introduction
ultrasound localization microscopy (ulm) has revolutionized medical imaging
by enabling sub-wavelength resolution from images acquired by piezo-electric
transducers and computational beamforming."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_21.pdf,"however, the necessity of beam-
forming for ulm remains questionable. our work challenges the conventional
assumption that beamforming is the ideal processing step for ulm and presents
an alternative approach based on geometric reconstruction from time-of-arrival
(toa) information. the discovery of ulm has recently surpassed the diﬀraction-limited spatial
resolution and enabled highly detailed visualization of the vascularity [8]. ulm
borrows concepts from super-resolution ﬂuorescence microscopy techniques to
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5_21."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_21.pdf,"however, the conventional approach for ulm involves using computational
beamformers, which may not be ideal for mb localization. for example, a recent
study has shown that ultrasound image segmentation can be learned from radio-
frequency data and thus without beamforming [13]. beamforming techniques
have been developed to render irregular topologies, whereas mbs exhibit a uni-
form geometric structure, for which ulm only requires information about its
spatial position."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_21.pdf,"to this end, we propose an alternative approach for ulm, outlined in fig. 1,
that entirely relies on time-diﬀerence-of-arrival (tdoa) information, omitting
beamforming from the processing pipeline for the ﬁrst time. we demonstrate a
novel geometry framework for mb localization through ellipse intersections to
overcome limitations inherent to beamforming."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"due to this heterogeneity, a general-
purpose cancer detection model can be built using unsupervised learning
anomaly detection models. while prior work in this ﬁeld has showcased
the eﬃcacy of abnormality detection methods (e.g. transformer-based),
these have shown signiﬁcant vulnerabilities to diﬀerences in data geom-
etry. changes in image resolution or observed ﬁeld of view can result in
inaccurate predictions, even with signiﬁcant data pre-processing and aug-
mentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"we propose a new spatial conditioning mechanism that enables
models to adapt and learn from varying data geometries, and apply it
to a state-of-the-art vector-quantized variational autoencoder + trans-
former abnormality detection model. we showcase that this spatial condi-
tioning mechanism statistically-signiﬁcantly improves model performance
on whole-body data compared to the same model without conditioning,
while allowing the model to perform inference at varying data geometries. 1
introduction
the use of machine learning for anomaly detection in medical imaging analysis
has gained a great deal of traction over previous years."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"most recent approaches
have focused on improvements in performance rather than ﬂexibility, thus lim-
iting approaches to speciﬁc input types – little research has been carried out to
generate models unhindered by variations in data geometries. often, research
assumes certain similarities in data acquisition parameters, from image dimen-
sions to voxel dimensions and ﬁelds-of-view (fov). these restrictions are then
carried forward during inference [5,25]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"this strong assumption can often be
complex to maintain in the real-world and although image pre-processing steps
can mitigate some of this complexity, test error often largely increases as new
data variations arise. this can include variances in scanner quality and reso-
lution, in addition to the fov selected during patient scans. usually training
data, especially when acquired from diﬀering sources, undergoes signiﬁcant pre-
processing such that data showcases the same fov and has the same input
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_29."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14220, pp. https://doi.org/10.1007/978-3-031-43907-0_29
geometry-invariant abnormality detection
301
dimensions, e.g. by registering data to a population atlas. whilst making the
model design simpler, these pre-processing approaches can result in poor gener-
alisation in addition to adding signiﬁcant pre-processing times [11,13,26]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"given
this, the task of generating an anomaly detection model that works on inputs
with a varying resolution, dimension and fov is a topic of importance and the
main focus of this research. unsupervised methods have become an increasingly prominent ﬁeld for auto-
matic anomaly detection by eliminating the necessity of acquiring accurately
labelled data [4,7] therefore relaxing the stringent data requirements of medical
imaging. this approach consists of training generative models on healthy data,
and deﬁning anomalies as deviations from the deﬁned model of normality during
inference."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"however, novel unsupervised
anomaly detectors based on autoregressive transformers coupled with vector-
quantized variational autoencoders (vq-vae) have overcome issues associated
with autoencoder-only methods [21,22]. in [22], the authors explore the advan-
tage of tractably maximizing the likelihood of the normal data to model the
long-range dependencies of the training data. the work in [21] takes this method
a step further through multiple samplings from the transformer to generate a
non-parametric kernel density estimation (kde) anomaly map."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"this would happen even in the case that a scan’s original fov was
restricted [17].
as such, we propose a geometric-invariant approach to anomaly detection,
and apply it to cancer detection in whole-body pet via an unsupervised anomaly
detection method with minimal spatial labelling. through adapting the vq-
vae transformer approach in [21], we showcase that we can train our model on
data with varying ﬁelds of view, orientations and resolutions by adding spatial
conditioning in both the vq-vae and transformer. furthermore, we show that
the performance of our model with spatial conditioning is at least equivalent to,
and sometimes better, than a model trained on whole-body data in all testing
scenarios, with the added ﬂexibility of a “one model ﬁts all data” approach."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"speciﬁcally, a vq-vae plus a transformer are jointly used to learn the proba-
bility density function of 3d pet images as explored in prior research [21,22,24].
302
a. patel et al.
fig. 1. flowchart showcasing traditional data pipelines for developing machine learning
models in medical imaging (top) vs. the reduced pipeline for our approach (bottom)
2.1
vector-quantized variational autoencoder
the vq-vae model provides a data-eﬃcient encoding mechanism—enabling 3d
inputs at their original resolution—while generating a discrete latent representa-
tion that can trivially be learned by a transformer network [20]. the vq-vae
is composed of an encoder that maps an image x ∈ rh×w ×d onto a com-
pressed latent representation z ∈ rh×w×d×nz where nz is the latent embedding
vector dimension."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"using the vq-vae, we can
obtain a discrete representation of the latent space by replacing the codebook
elements in zq with their respective indices in the codebook yielding ziq. to
model the imaging data, we require the discretized latent space ziq to take the
form of a 1d sequence s, which we achieve via a raster scan of the latent. the
transformer is then trained to maximize the log-likelihoods of the latent tokens
sequence in an autoregressive manner."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"the performer
used in this work corresponds to a decoder transformer architecture with 14
layers, each with 8 heads, and an embedding dimension of 256. similarly the
embedding dimension for the ct data and the spatial conditioning data had an
embedding dimension of 256. see appendix b for implementation details."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"we can then
reshape the “healed” sequence back into its 3d quantized representation to feed
into the vq-vae to generate a healed reconstruction xr without anomalies. in this work, abnormalities are deﬁned as deviations between the distribution
of “healed” reconstructions and the observed data, measured using a kernel den-
sity estimation (kde) approach. t. in each
resampling, the transformer outputs the likelihood for every possible token at
position i. based on these probabilities, we can create a multinomial distribution
showcasing the probability of each token."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"our kde implementation used 60 samples for each
anomalous token in s, followed by ﬁve decodings with dropout, yielding 300
“healed” reconstructions that are then used to calculate the kde.
3
method
3.1
vq-vae spatial conditioning
to date, there has been little research on generating autoencoder models capable
of using images of varying sizes and resolutions (i.e. the input tensor shape to
a autoencoder is assumed to be ﬁxed). although fully convolutional models can
304
a. patel et al.
ingest images of varying dimensions, we have found that using training data with
varying resolutions resulted in poor auto-encoder reconstructions. [19] as a mechanism to account for some
level of spatial awareness, an approach which has been applied to various tasks
in medical imaging scenarios with ranging levels of success [1,18]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"for example, two whole-
body images with large diﬀerences in voxel-size will have coordconv channels
from 0–1 along each axis, thus conveying the notion of spatial resolution to the
network. we found when training the vq-vae model on data with varying reso-
lutions and dimensions that reconstructions showcased unwanted and signiﬁcant
artifacts, while by adding the coordconv channels this issue was not present
(see appendix c for examples). [0, 1] where 0 is the upper leg, and 1 is the neck."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"in that case, we can
contract this range to represent the area displayed in the image (fig. 2). in doing
so, we convey information about the fov to the vq-vae through coordconv
layers. note that while the proposed model assumes only translation and scale
changes between samples, it can be trivially extended to a full aﬃne mapping of
the coordinate system (including rotations/shearing between samples)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"for the implementation of the coordconv layer, these channels are
added once to the original input image and at the beginning of the vq-vae
decoder, concatenated to the latent space, using the same value ranges but at a
lower resolution given the reduced spatial dimension of the latent space.
3.2
transformer spatial conditioning
numerous approaches have used transformers in the visual domain [7,8]. given
that transformers work natively on 1d sequences, the spatial information in
images is often lost. while various works have aimed to convey the spatial infor-
mation of the original image when projected onto a 1d sequence [14,28], we
require our spatial positioning to encode both where in the image ordering a
token belongs, and where the token belongs in the context of the whole body."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"for reference, this mechanism can be visualised in fig. 3.
3.3
data
for this work we leveraged whole-body pet/ct data from diﬀerent sources to
explore the eﬃcacy of our approach for varying image geometries. 211 scans from
nsclc radiogenomics [2,3,10,16] combined with 83 scans from a proprietary
dataset constitute our lower resolution dataset with voxel dimensions of 3.6 ×
3.6×3 mm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"from this, we split the data to give 210 training samples, 34 validation
and 50 testing. our higher resolution dataset uses autopet [10,15] (1014 scans)
with voxel dimensions of 2.036 × 2.036 × 3 mm. from this, 850 scans are used
for training, 64 for validation and 100 for testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"all baseline models work in a single space with constant dimensions, obtained
by registering the autopet images to the space of the nsclc dataset. for evaluation, we use four testing sets: a lower resolution set derived from
both the nsclc and the private dataset; a higher resolution set from autopet;
306
a. patel et al.
fig. 3. pipeline for transformer training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"pet and ct are encoded to generate a
discrete latent space. coordconv layers are used to generate the spatial conditionings
that are added to the ct conditioning and fed to the transformer via cross-attention
a testing set with random crops of the same nsclc/private testing dataset and
ﬁnally a testing set that has been rotated through 90◦ using the high resolution
testing data. as the cropped and rotated dataset cannot be fed into the baseline
models, we pad the images to the common image sizing before inference."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"in addition, we calculate the area under the precision-recall curve
(auprc) as a suitable measure for segmentation performance under class imbal-
ance. we additionally showcase the performance of the classic vq-vae + trans-
former approach trained on whole-body data only (without the proposed spatial
conditioning), as well as the proposed coordconv model trained with varying
image geometries but without the transformer spatial conditioning to explicitly
showcase the added contribution of both spatial conditionings. the full results
are presented in table 1 with visual examples shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"4. we can observe
that the addition of spatial conditioning improves performance even against the
same model without conditioning trained on whole-body data (mann whitney
u test, p < 0.01 on high resolution and p < 0.001 on cropped data for dice
and auprc). for cropped data, models trained on whole-body data fail around
cropping borders, as showcased in fig. 4. this is not the case for the models
trained on varying geometries."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"note that the vq-vae + transformer trained on
varying geometries still shows adequate performance, highlighting the resilience
of the transformer network to varying sequence lengths without any form of
spatial conditioning. however, by adding the transformer spatial conditioning,
we see improvements across all test sets (most signiﬁcantly on cropped data and
the rotated data p < 0.001) for both evaluation metrics. for the rotated data,
we see little performance degradation in the conditioned model thanks to the
spatial conditioning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"308
a. patel et al.
5
conclusion
detection and segmentation of anomalous regions, particularly for cancer
patients, is essential for staging, treatment and intervention planning. gener-
ally, the variation scanners and acquisition protocols can cause failures in mod-
els trained on data from single sources. in this study, we proposed a system
for anomaly detection that is robust to variances in geometry."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_29.pdf,"not only does
the proposed model showcase strong and statistically-signiﬁcant performance
improvements on varying image resolutions and fov, but also on whole-body
data. through this, we demonstrate that one can improve the adaptability and
ﬂexibility to varying data geometries while also improving performance. such
ﬂexibility also increases the pool of potential training data, as they dont require
the same fov."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"they are directly computed from the graph properties and
its connected components with graph-based methods. experimental results on
lung (83 cts from 19 patients) and liver (77 cects from 18 patients) datasets
with more than two scans per patient yielded an individual lesion change class
accuracy of 98% and 85%, and identiﬁcation of patterns of lesion change with
an accuracy of 96% and 76%, respectively. highlighting unusual lesion labels
and lesion change patterns in the graph helps radiologists identify overlooked or
faintly visible lesions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"currently, scans are acquired every 2–12 months
according to the patient’s characteristics, disease stage, and treatment regime. the scan
interpretation consists of identifying lesions (primary tumors, metastases) in the affected
supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9_11. © the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"the novelties of this paper are: 1) identiﬁcation and formalization of longitudinal
lesion matching and patterns of lesion changes in ct in a graph-theoretic framework;
2) new classiﬁcation and detection of changes of individual lesions and lesion patterns
based on the properties of the lesion changes graph and its connected components;
3) a simultaneous lesion matching method with more than two scans; 4) graph-based
methods for the detection of changes in individual lesions and patterns of lesion changes. experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18 patients)
datasets show that our method yields high classiﬁcation accuracy. to the best of our knowledge, ours is the ﬁrst method to perform longitudinal lesion
matching and lesion changes pattern detection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"these methods assume that organs and lesions undergo minor changes, are very sen-
sitive to registration errors, and cannot handle complex lesion changes. [13–16]
with an 84–96% accuracy on the deeplesion dataset [14]. they are susceptible to major
changes in the lesion appearance and do not handle complex lesion changes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"1. longitudinal study of a patient with liver metastases (color overlays): three consecutive (a–
c) illustrative slices of unregistered cect scans acquired at times ti; matching colors correspond
to matching lesions; (d) lesion changes graph: nodes correspond to lesions vi
j where j is the lesion
number at time ti (dotted rectangle); edges correspond to lesion matches (consecutive straight,
non-consecutive curved). the individual lesion changes labels are shown below each node. the
graph has three undirected connected components ccm (red, brown, green), corresponding to three
lesion changes patterns, single_p, merged_p, linear_p."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"graph-theoretic automatic lesion tracking and detection
109
the method inputs the scans and the organ and lesion segmentations in each scan. its outputs are the lesion matchings, the labels of the changes in individual lesions, and
the patterns of the lesion changes. the method is a pipeline of four steps: 1) pairwise
deformable registration of each prior scan, organ and lesion segmentations, with the
most recent (current) scan as in [3]; 2) overlap-based lesion matching; 3) construction of
the lesion change graph from the individual lesion segmentations and lesion matches; 4)
detection of changes in individual lesions and patterns of lesion changes from the graph
properties and from analysis of its connected components.
2.1
problem formalization
let s =

s1, . . ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"in this setup, connected components correspond to matched
lesions and their pattern of evolution over time (fig. 1d). we deﬁne seven mutually exclusive individual lesion change labels for lesion vi
j in
scan si based on the vertex in- and out-degrees (fig. 2). i < l ≤ n; 1) lone: a lesion present in scan si and absent
in all previous scans sk and subsequent scans sl; 2) new: a lesion present in scan si and
absent in all previous scans sk; 3) disappeared: a lesion present in scan si and absent
in all subsequent scans sl; 4) unique: a lesion present in scan si and present as a single
lesion in a previous scan sk and/or in a subsequent scan sl; 5) merged: a lesion present in
scan si and present as two or more lesions in a previous scan sk; 6) split: a lesion present
in scan si and present as two or more lesions in a subsequent scan sl; 7) complex: a
lesion present as two or more lesions in at least one previous scan sk and at least one
subsequent scan sl."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"i > 1, it is also labeled new. we deﬁne ﬁve patterns of lesion changes based on the properties of the connected
components ccm of g and on the labels of lesion changes: 1) single_p: a connected
component ccm =

vi
j
	
consisting of a single lesion labeled as lone, new, disappeared;
2) linear_p: a connected component consisting of a single earliest vertex vﬁrst
j
(can
be new), a single latest vertex vlast
l
(can be disappeared) connected by a sequence. ﬁrst < k ≤ n, one or more labeled
split; 5) complex_p: all other connected components."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"2. (a) individual lesion change classes of a vertex vi
j deﬁned by its in- and out-degrees
din(vi
j), dout(vi
j). illustrative node pattern: node (circle), edges (arrows); (b) patterns of lesion
changes deﬁned by node labels and connected component properties and illustrative patterns. the changes in individual lesions and the detection and classiﬁcation of patterns
of lesion changes consist of constructing a graph whose vertices are the corresponding
lesion in the scans, computing the graph consecutive and non-consecutive edges that
correspond to lesion matchings, computing the connected components of the resulting
graph, and assigning an individual lesion change label to each vertex and a lesion change
pattern label to each connected component according to the categories above."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"the changes in individual lesions, patterns of lesion changes, and lesion changes
graph serve as the basis for individual lesion tracking, which consists of following the
path from the lesion in the most recent scan backwards to its origins in earlier scans and
recordingthemerged,splitandcomplexlesionchangeslabels.summarizinglongitudinal
studies and queries can also be performed with graph-based algorithms. 3
experimental results
we evaluated our method with two studies on retrospectively collected patient datasets
that were manually annotated by an expert radiologist. dataset: lung and liver ct studies were retrospectively obtained from two medical
centers (hadassah univ hosp jerusalem israel) during the routine clinical examination
of patients with metastatic disease."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"each patient study consists of at least 3 scans. lesions in both datasets were annotated by an expert radiologist, yielding a total of
1,178 lung and 800 liver lesions, with a mean of 14.2 ± 19.1 and 10.4 ± 7.9 lesions/scan
(lesions with <20 voxels were excluded). ground-truth lesion matching graphs and
lesion changes labeling were produced by running the method on the datasets and then
having the radiologist review and correct the resulting node labels and edges."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"the
settings of the parameters were: dilation distance d = 1 mm, overlap percentage p =
10%, number of iterations r = 5 and 7, and centroid maximum distance δ = 17 and
23 mm for the lungs and liver lesions, respectively. we compared the computed and ground-truth lesion changes graphs with two met-
rics: 1) lesion changes classiﬁcation accuracy, which is the % of correct computed labels
from the ground truth labels; 2) lesion matching precision and recall based on the pres-
ence/absence of computed vs. ground truth edges. the precision and recall deﬁnitions
were adapted so that wrong or missed non-consecutive edges are counted as true positive
when there is a path between their vertices in either the ground-truth or the computed
graph."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"table 1 summarizes the results. the distribution of lesion changes labels for
dlungs (1,178 lesions) is unique 785 (67%), new 215 (18%), lone 109 (9%), dis-
appeared 51 (4%), merged 12 (1%), split 6 (1%), complex 0 (0%) with class accuracy
≥ 96% for all except split (66%). it is unique 450 (56%),
graph-theoretic automatic lesion tracking and detection
113
new 185 (23%), lone 45 (6%), disappeared 77 (10%), merged 27 (3%), split 18 (2%),
complex 1 (0.05%) with class accuracy ≥ 81% for all except disappeared (71%) and
split (67%)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"for each non-consecutive
edge connecting lesions vi
j, vk
l , he analyzed the corresponding region in the skipped
scans sj at tj ∈ ]ti, tk[ for possible missed lesions. for the dlungs dataset, 25 visible
and 5 faintly visible or surmised to be present unmarked lesions were found for 27 non-
consecutive edges. for the dliver dataset, 20 visible and 21 faintly visible or surmised
to be present unmarked lesions were found for 25 non-consecutive edges."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_11.pdf,"moreover, he found that 14 and 16 lesions initially
labeled as lone, had been wrongly classiﬁed: for these lesions he found 15 and 21
114
b. di veroli et al.
previously unmarked matching lesions in the next or previous scans. in total, 45 and
62 missing lesions were added to the ground truth dlungs and dliver datasets,
respectively. these hard-to-ﬁnd ground-truth false negatives (3.7%, 7.2% of all lesions)
may change the radiological interpretation and the disease status."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"interactive segmentation reduces the annotation time of
medical images and allows annotators to iteratively reﬁne labels with
corrective interactions, such as clicks. while existing interactive mod-
els transform clicks into user guidance signals, which are combined with
images to form (image, guidance) pairs, the question of how to best
represent the guidance has not been fully explored."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"we propose an adaptive gaussian heatmap
guidance signal that utilizes the geodesic distance transform to dynami-
cally adapt the radius of each heatmap when encoding clicks. we conduct
our study on the msd spleen and the autopet datasets to explore the
segmentation of both anatomy (spleen) and pathology (tumor lesions). our results show that choosing the guidance signal is crucial for inter-
active segmentation as we improve the performance by 14% dice with
our adaptive heatmaps on the challenging autopet dataset when com-
pared to non-interactive models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"this brings interactive models one step
closer to deployment in clinical workﬂows. code: https://github.com/
zrrr1997/guiding-the-guidance/.
keywords: interactive segmentation · comparative study · click
guidance
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 61.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14222, pp. https://doi.org/10.1007/978-3-031-43898-1_61
638
z. marinov et al.
1
introduction
deep learning models have achieved remarkable success in segmenting anatomy
and lesions from medical images but often rely on large-scale manually annotated
datasets [1–3]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"this is challenging when working with volumetric medical data
as voxelwise labeling requires a lot of time and expertise. interactive segmen-
tation models address this issue by utilizing weak labels, such as clicks, instead
of voxelwise annotations [5–7]. the clicks are transformed into guidance sig-
nals, e.g., gaussian heatmaps or euclidean/geodesic distance maps, and used
together with the image as a joint input for the interactive model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"we address these challenges with the following contributions:
1. [2] datasets and vary various hyperparameters. we show which param-
eters are essential to tune for each guidance and suggest default values."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"for each volume, n clicks
are iteratively sampled from over- and undersegmented predictions of the model
as in [16] and represented as foreground and background guidance signals. we
implemented our experiments with monai label [23] and will release our code. we trained and evaluated all of our models on the openly available
autopet [1] and msd spleen [2] datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"we discard the 513 tumor-free patients, leaving us with 501 volumes. we also only use pet data for our experiments. the pet volumes have a voxel
size of 2.0 × 2.0 × 2.0mm3 and an average resolution of 400 × 400 × 352 voxels.
2.3
hyperparameters: experiments
we keep these parameters constant for all models: learning rate = 10−5, #clicks
n = 10, dice cross-entropy loss [24], and a ﬁxed 80–20 training-validation split
(dtrain/dval)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"note that this metric depends on the volume size and hardware
setup. ∗our maximum measurement tmax is shorter than 1 second
(m4) consistent
improve-
ment
ratio of clicks c+ that improve the dice score to the total number of
validation clicks:
|c+|
n·|dval|, where n = 10 and dval is the validation dataset
(m5) ground-
truth
overlap
overlap of the guidance g with the ground-truth mask m: |m∩g|
|g|
. [11] model for each (σ, θ) pair and set p = 100% and
the input adaptor to concat to constrain the parameter space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"and their dice scores. ∗same for both datasets. geodesic
maps exhibit lower dice scores for small σ < 5 and achieve the best performance
for σ = 5 on both datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"1e) indicate that the best performance is achieved by
simply concatenating the guidance signal with the input volume. this holds true
for both datasets and the diﬀerence in performance is substantial. (h4) probability of interaction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"the circles next to each
metric represent the ranking of the guidances (sorted top-to-bottom). autopet [1] are diﬀerent, the ﬁve metrics follow the same trend on both
datasets. (m1) initial and (m2) final dice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"overall, all guidance signals improve
their initial-to-ﬁnal dice scores after n clicks, with autopet [1] showing a gap
between disks/heatmaps and distance-based signals. moreover, geodesic-based
signals have lower initial scores on both datasets and require more interactions. (m3) consistent improvement."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"heatmaps, disks, and edt have
a signiﬁcantly higher overlap with the ground truth compared to geodesic-
based signals, particularly on autopet [1]. gdt incorporates the changes in
voxel intensity, which is not a strong signal for lesions with weak boundaries in
autopet [1], resulting in a smaller overlap with the ground truth. the guidances
are ranked in the same order in (m3) and in (m4) for both datasets. thus, a
good overlap with the ground truth can be associated with precise corrections."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_61.pdf,"disks are
the most eﬃcient signal, ﬁlling up spheres with constant values, while heatmaps
are slightly slower due to applying a gaussian ﬁlter over the disks. distance
transform-based guidances are the slowest on both datasets due to their com-
plexity, but all guidance signals are computed in a reasonable time (<1 s).
guiding the guidance: a comparative analysis of user guidance signals
645
adaptive heatmaps: results. varying (h1)–(h4) and examining (m1)–
(m5), we ﬁnd disks/heatmaps as the best signals, but with inﬂexibility near
edges due to their ﬁxed radius (fig. 1a))."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"besides, we design a lightweight densely con-
nected transformer (dct) block to replace the standard transformer
block, thus signiﬁcantly reducing computational complexity. we con-
duct extensive experiments on two public multimodal datasets, heck-
tor21 and pi-cai22. the experimental results show that our proposed
method outperforms the existing state-of-the-art methods while having
lower computational complexity."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14223, pp. https://doi.org/10.1007/978-3-031-43901-8_66
h-denseformer
693
(pet) are beneﬁcial to represent morphological and metabolic information of
tumors, respectively. in clinical practice, multimodal registered images, such as
pet-ct images and magnetic resonance (mr) images with diﬀerent sequences,
are often utilized to delineate tumors to improve accuracy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"recently, multimodal tumor segmentation has attracted the interest of many
researchers. with the emergence of multimodal datasets (e.g., brats [25] and
hecktor [1]), various deep-learning-based multimodal image segmentation
methods have been proposed [3,10,13,27,29,31]. overall, large eﬀorts have been
made on eﬀectively fusing image features of diﬀerent modalities to improve seg-
mentation accuracy. according to the way of feature fusion, the existing methods
can be roughly divided into three categories [15,36]: input-level fusion, decision-
level fusion, and layer-level fusion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"in contrast, [35] and [21]
propose a solution based on decision-level fusion. the core idea is to train an
independent segmentation network for each data modality and fuse the results
in a speciﬁc way. these approaches can bring much extra computation at the
same time, as the number of networks is positively correlated with the number
of modalities."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"to this end, we propose an eﬃcient multimodal tumor segmentation solu-
tion named hybrid densely connected network (h-denseformer). first, our
method leverages transformer to enhance the global contextual information
of diﬀerent modalities. second, h-denseformer integrates a transformer-based
multi-path parallel embedding (mpe) module, which can extract and fuse mul-
timodal image features as a complement to naive input-level fusion structure."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"finally, we design a lightweight, densely connected transformer (dct)
module to replace the standard transformer to ensure performance and com-
putational eﬃciency. extensive experimental results on two publicly available
datasets demonstrate the eﬀectiveness of our proposed method. 2
method
2.1
overall architecture of h-denseformer
figure 1 illustrates the overall architecture of our method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"finally, the
segmentation network generates multi-scale outputs, which are used to calculate
deep supervision loss as the optimization target. 2.2
multi-path parallel embedding
many methods [5,10,15] have proved that decoupling the feature representa-
tion of diﬀerent modalities facilitates the extraction of high-quality multimodal
features. inspired by this, we design a multip-path parallel embedding (mpe)
module to enhance the representational ability of the network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"the independence of the diﬀerent paths allows mpe to han-
dle an arbitrary number of input modalities. besides, the introduction of the
transformer provides the ability to model global contextual information. [1, 2, ..., c], p = 16
and l = 128 denote the path size and embedding feature length respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"to preserve more details, we set the maximum downsampling
factor to 8. the multi-level multimodal features from mpe are fused in a bit-
wise addition way to enrich the semantic information. the decoder is used to
restore the resolution of the features, consisting of deconvolutional and convolu-
tional layers with skip connections to the encoder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"this approach can improve the convergence speed
and performance of the network. [1] and pi-cai221. hecktor21 is a dual-
modality dataset for head and neck tumor segmentation, containing 224 pet-
ct image pairs. each pet-ct pair is registered and cropped to a ﬁxed size
of (144,144,144)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"after standard resampling and center cropping, all images have a size of
(24,384,384). we randomly select 180 samples for each dataset as the training
set and the rest as the independent test set (44 cases for hecktor21 and 40
cases for pi-cai22). speciﬁcally, the training set is further randomly divided
into ﬁve folds for cross-validation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"we also use an early stopping
strategy with a tolerance of 30 epochs to ﬁnd the best model within 100 epochs. online data augmentation, including random rotation and ﬂipping, is performed
to alleviate the overﬁtting problem. 1 https://pi-cai.grand-challenge.org/.
698
j. shi et al.
3.3
overall performance
table 2. comparison with existing methods on independent test set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"for hecktor21, 3d h-denseformer achieves a dsc of 73.9%, hd95
of 8.1mm, and ji of 62.5%, which is a signiﬁcant improvement (p < 0.01) over
3d u-net [7], unetr [16], and transbts [31]. it is worth noting that the per-
formance of hybrid models such as unetr is not as good as expected, even
worse than 3d u-net, perhaps due to the small size of the dataset. moreover,
compared to the champion solution of hecktor20 proposed by iantsen et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_66.pdf,"in particular, the degradation after removing the mpe also con-
table 4. ablation study of 3d h-denseformer, w/o denotes without. hd95 (mm) ↓ ji (%) ↑
3d h-denseformer w/o mpe
72.1 ± 0.8
10.8 ± 1.1
60.4 ± 0.8
3d h-denseformer w/o dct
70.7 ± 1.8
11.9 ± 1.9
58.6 ± 2.1
3d h-denseformer w/o ds loss 72.2 ± 0.9
10.2 ± 1.0
60.1 ± 1.2
3d h-denseformer
73.9 ± 0.5 8.1 ± 0.6
62.5 ± 0.5
700
j. shi et al.
ﬁrms that decoupling the feature expression of diﬀerent modalities helps obtain
higher-quality multimodal features and improve segmentation performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf,"speciﬁcally, we
ﬁrst design an auto contrast enhancement (ace) module to adjust
the vessel contrast dynamically. then, we propose a cross-scale non-
local block (csnb) to eﬀectively fuse multi-scale features by utilizing
both local and global semantic information. experimental results show
that our approach achieves better pulmonary vessel segmentation out-
comes compared to other state-of-the-art methods, demonstrating the
eﬃcacy of the proposed ace and csnb module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf,"most of
these methods employed manual features to segment peripheral intrapulmonary
vessels. however, for vessel segmentation, the widely used models, such as
u-net and its variants, limit their segmentation accuracy on low-contrast small
vessels due to the loss of detailed information caused by the multiple down-
sampling operations. accordingly, zhou et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf,"wang et al. [13] replaced the
original skip connections with transformer blocks to better merge the multi-scale
contextual information. for this task, cui et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf,"also, we propose a cross-scale non-local block (csnb) to replace the
skip connections in vanilla u-net [11] structure for the aggregation of multi-scale
feature maps. it helps to form local-to-global information connections to enhance
vessel information at the feature-level, and address the complex scale variations
of pulmonary vessels. 2
method
the overview of the proposed method is illustrated in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf,"first, the ace module
is developed to enhance the contrast of vessels in the original ct images for the
following vessel segmentation network. after that, we introduce the csnb mod-
ule to make the network pay more attention to multi-scale vessel information in
the latent feature space. 2.1
auto contrast enhancement
in non-contrast ct images, the contrast between pulmonary vessels and the
surrounding voxels is pretty low."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf,"as illus-
trated in fig. 2, the csnb works as the information bridge between encoders
and decoders while also ensuring the feasibility of experiments involving large
3d data. speciﬁcally, the i1 ∼ i4 are the inputs of csnb, and o1 ∼ o4 are the
outputs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf,"within the csnb, there are three levels of modiﬁed anbs, we denote
them as anb-h (anb-head) and anb-p (anb-post). for the two anbs in
each level, the anb-h has two input feature maps, and the lower-level feature
maps (denoted as fl) contain more ﬁne-grained information than the higher-
level feature maps (denoted as fh). we use fh to generate embedding q, while
embeddings k and v are derived from fl."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf,"that is, each level of csnb can fuse the feature
maps from its corresponding level with the fused feature maps from all of the
above lower levels. thereby, the response of multi-scale vessels can be enhanced.
3
experiments and results
dataset and evaluation metrics. we use a total of 160 non-contrast ct
images with the inplane size of 512 × 512, where the slice number varies from
217 to 622."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf,"our experiments are implemented using pytorch
framework and trained using a single nvidia-a100 gpu. we pre-process the
data by truncating the hu value to the range of [–900, 900] and then linearly
scaling it to [–1, 1]. in the training stage, we randomly crop sub-volumes with
the size of 192 × 192 × 64 near the lung ﬁeld, and then the cropped sub-volumes
are augmented by random horizontal and vertical ﬂipping with a probability of
0.5."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_53.pdf,"for a fair comparison,
we use the same hyper-parameter settings and dice similarity coeﬃcient loss
across all experiments. in particular, we use the same data augmentation, no
post-processing scheme, adam optimizer with an initial learning rate of 10−4,
and train for 800 epochs with a batch size of 4. in our experiments, we use a
two-step optimization strategy: 1) ﬁrst, train the ace module with the basic
u-net; 2) integrate the trained ace module and a new csnb module into the
u-net, and ﬁx the parameters of ace module when training this network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf,"in computation pathology, the pyramid structure of gigapixel
whole slide images (wsis) has recently been studied for capturing vari-
ous information from individual cell interactions to tissue microenviron-
ments. this hierarchical structure is believed to be beneﬁcial for cancer
diagnosis and prognosis tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf,"however, most previous hierarchical wsi
analysis works (1) only characterize local or global correlations within
the wsi pyramids and (2) use only unidirectional interaction between
diﬀerent resolutions, leading to an incomplete picture of wsi pyramids.
to this end, this paper presents a novel hierarchical interaction graph-
transformer (i.e., higt) for wsi analysis. with graph neural network
and transformer as the building commons, higt can learn both short-
range local information and long-range global representation of the wsi
pyramids. considering that the information from diﬀerent resolutions is
complementary and can beneﬁt each other during the learning process,
we further design a novel bidirectional interaction block to establish com-
munication between diﬀerent levels within the wsi pyramids."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf,"finally, we
aggregate both coarse-grained and ﬁne-grained features learned from dif-
ferent levels together for slide-level prediction. we evaluate our methods
on two public wsi datasets from tcga projects, i.e., kidney carcinoma
(kica) and esophageal carcinoma (esca). experimental results show
that our higt outperforms both hierarchical and non-hierarchical state-
of-the-art methods on both tumor subtyping and staging tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf,"keywords: wsi analysis · hierarchical representation · interaction ·
graph neural network · vision transformer
z. guo and w. zhao: contributed equally to this work. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 73.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43987-2_73
756
z. guo et al.
1
introduction
histopathology is considered the gold standard for diagnosing and treating many
cancers [19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf,"such graph-
transformer architecture has also been introduced into wsi analysis [15,20] to
mine the thorough global and local correlations between diﬀerent image patches. however, current graph-transformer-based wsi analysis models only consider
the representation learning under one speciﬁc magniﬁcation, thus ignoring the
rich multi-resolution information from the wsi pyramids. diﬀerent resolution levels in the wsi pyramids contain diﬀerent and comple-
mentary information [3]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf,"[3] proposed an inheritable vit
framework to model wsi at diﬀerent resolutions. whereas these methods only
characterize local or global correlations within the wsi pyramids and use only
unidirectional interaction between diﬀerent resolutions, leading to insuﬃcient
capability to model the rich multi-resolution information of the wsi pyramids.
in this paper, we present a novel hierarchical interaction graph-transformer
framework (i.e., higt) to simultaneously capture both local and global informa-
tion from wsi pyramids with a novel bidirectional interaction module. speciﬁ-
cally, we abstract the multi-resolution wsi pyramid as a heterogeneous hierar-
chical graph and devise a hierarchical interaction graph-transformer architec-
ture to learn both short-range and long-range correlations among diﬀerent image
patches within diﬀerent resolutions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf,"to reduce the tremendous computation and memory
cost, we further adopt the eﬃcient pooling operation after the hierarchical gnn
part to reduce the number of tokens and introduce the separable self-attention
mechanism in hierarchical interaction vit modules to reduce the computation
burden. the extensive experiments with promising results on two public wsi
datasets from tcga projects, i.e., kidney carcinoma (kica) and esophageal
carcinoma (esca), validate the eﬀectiveness and eﬃciency of our framework
on both tumor subtyping and staging tasks. the codes are available at https://
github.com/hku-medai/higt."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf,"finally, a 1 × 1 convolution and
mean operation followed by a linear projection are employed to produce the
slide-level prediction. 3
experiments
datasets and evaluation metrics. we assess the eﬃcacy of the proposed
higt framework by testing it on two publicly available datasets (kica and
esca) from the cancer genome atlas (tcga) repository."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf,"the datasets are
described below in more detail:
– kica dataset. the kica dataset consists of 371 cases of kidney carcinoma,
of which 279 are classiﬁed as early-stage and 92 as late-stage. for the tumor
typing task, 259 cases are diagnosed as kidney renal papillary cell carcinoma,
while 112 cases are diagnosed as kidney chromophobe.
– esca dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf,"[3],
they were introduced with a hierarchical graph neural network and hierar-
chical transformer architecture, respectively. the results for esca and kica
datasets are summarized in table 1 and table 2, respectively. overall, our model
achieves a content result both in auc and acc of classifying the wsi, and
especially in predicting the more complex task (i.e. staging) compared with the
sota approaches."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_73.pdf,"finally, the ablation analysis
results show that all of these modules we used actually improved the prediction
eﬀect of the model to a certain extent. table 3. ablation analysis on kica dataset. 2. computational analysis of our framework and some selected sota methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"contrastive learning has gained popularity due to its robust-
ness with good feature representation performance. however, cosine
distance, the commonly used similarity metric in contrastive learning,
is not well suited to represent the distance between two data points,
especially on a nonlinear feature manifold. inspired by manifold learn-
ing, we propose a novel extension of contrastive learning that leverages
geodesic distance between features as a similarity metric for histopathol-
ogy whole slide image classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"to reduce the computational over-
head in manifold learning, we propose geodesic-distance-based feature
clustering for eﬃcient contrastive loss evaluation using prototypes with-
out time-consuming pairwise feature similarity comparison. the eﬃcacy
of the proposed method is evaluated on two real-world histopathology
image datasets. results demonstrate that our method outperforms state-
of-the-art cosine-distance-based contrastive learning methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"owing to the huge size of a wsi, the conventional wsi
classiﬁcation process consists of patch decomposition and per-patch classiﬁca-
tion, followed by the aggregation of per-patch results using multiple instance
learning (mil) for the ﬁnal per-slide decision [7]. mil constructs bag-of-features
(bof) that eﬀectively handles imperfect patch labels, allowing weakly super-
vised learning using per-slide labels for wsi classiﬁcation. although mil does
not require perfect per-patch label assignment, it is important to construct good
feature vectors that are easily separated into diﬀerent classes to make the clas-
siﬁcation more accurate."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"recently, contrastive learning has demonstrated its robustness in the repre-
sentational ability of the feature extractor, which employs self-supervised learn-
ing with a contrastive loss that forces samples from the same class to stay closer
in the feature space (and vice versa). [3] introduced the utilization of
data augmentation and a learnable nonlinear transformation between the feature
embedding and the contrastive loss to generally improve the quality of feature
embedding. [6] employed a dynamic dictionary along with a momentum
encoder in the contrastive learning model to serve as an alternative to the super-
vised pre-trained imagenet model in various computer vision tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"the main motivation of this work is to extend the current contrastive learn-
ing to represent the nonlinear feature manifold inspired by manifold learning. owing to the manifold distribution hypothesis [8], the relative distance between
high-dimensional data is preserved on a low-dimensional manifold. isomap [12]
is a well-known manifold learning approach that represents the manifold struc-
ture by using geodesic distance (i.e., the shortest path length between points on
the manifold)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"[4]
employed the geodesic distance computed using the dijkstra algorithm on the k-
nearest neighbor graph to measure the correlation between the original samples
deep manifold contrastive learning
685
and then further divided each class into sub-classes to deal with the problems of
high spectral dimension and channel redundancy in the hyperspectral images. however, this method captured the nonlinear data manifold structure on the
original data (not on the feature vectors) only once at the beginning stage,
which is not updated in the further training process. in this study, we propose a hybrid method that combines manifold learn-
ing and contrastive learning to generate a good feature extractor (encoder) for
histopathology image classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"– we demonstrate that the proposed method outperforms other state-of-the-
art (sota) methods with a much smaller number of sub-classes without
complicated prototype assignment (e.g., hierarchical clustering). to the best of our knowledge, this work is the ﬁrst attempt to leverage manifold
geodesic distance in contrastive learning for histopathology wsi classiﬁcation. 2
method
the overview of our proposed model is illustrated in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"inter-subclass loss linter is proposed to make the sub-classes from a diﬀerent
class far apart from one another. linter = 1
j
j

j=1
(△ − d(f(qa
j ), p b))
(2)
d(y, z) = max{sup
y∈y
d(y, z), sup
z∈z
d(z, y )}
(3)
where f(qa
j ) is a set of patch features in batch j from class a, p b is a set of
prototypes from the sub-classes of class b, and △ is a positive margin between
classes on data manifold. t ∈ y to the subset r ⊆ y ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"these
bags are fed into a classiﬁer with two layers of multiple perceptron layers (512
neurons) and a softmax layer and then trained with a binary cross-entropy loss. after the classiﬁcation, majority voting is applied to the predicted labels of the
bags to derive the ﬁnal predicted label for each wsi.
688
j. w. tan and w.-k. jeong
3
result
3.1
datasets
we tested our proposed method on two diﬀerent tasks: (1) intrahepatic cholan-
giocarcinomas(ihccs) subtype classiﬁcation and (2) liver cancer type classiﬁca-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"ihccs can be further categorized
into small duct type (sdt) and large duct type (ldt). using gene mutation
information as prior knowledge, we collected wsis with wild kras and mutated
idh genes for use as training samples in sdt, and wsis with mutated kras
and wild idh genes for use in ldt. the rest of the wsis were used as test-
ing samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"the
result shown in the tables is the average result from 10 iterations of the mil
classiﬁcation model. 3.3
experimental results
the performance of diﬀerent models from two diﬀerent datasets is reported in
this section. for the baseline model, we chose the pre-trained vgg16 feature
extractor with an mil classiﬁer, which is the same as our proposed model except
that the encoder is retrained using the proposed loss."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"note that our method only used 20 sub-classes but outperformed pcl (using
2300 sub-classes) by 4% and hcsc (using 112 sub-classes) by 5% in accuracy. table 1. classiﬁcation performance on ihccs subtype and liver cancer type dataset. precision, rec.: recall, f1: f1 score, na: not applicable)
method prototype number ihcc subtype
liver cancer type
acc.
prec."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"two scatter plots are
t-sne projections of feature vectors from the encoders trained using geodesic
distance and cosine distance, respectively. red dots represent sdt samples and
blue dots represent ldt samples from the ihccs dataset (corresponding histol-
ogy thumbnail images are shown on the right). in this example, all eight cases
are correctly classiﬁed by the method using geodesic distance while all cases are
incorrectly classiﬁed by the method using cosine distance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_66.pdf,"one limitation of
the proposed method is the extra computation time for graph generation and
pairwise distance computation using the dijkstra algorithm. in the future, we
plan to optimize the algorithm and apply our method to other datasets and
tasks, such as multi-class classiﬁcation problems and natural image datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"in this work, we propose a novel mil method for patholog-
ical image analysis with integrated instance-level and bag-level supervi-
sion (termed iib-mil). more importantly, to overcome the weakly super-
vised nature of mil, we design a label-disambiguation-based instance-
level supervision for mil using prototypes and conﬁdence bank to
reduce the impact of noisy labels. extensive experiments demonstrate
that iib-mil outperforms state-of-the-art approaches in both bench-
marking datasets and addressing the challenging practical clinical task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"the code is available at https://github.com/tencentailabhealthcare/
iib-mil. keywords: computational pathology · multi-instance learning · label
disambiguation · prototype · conﬁdence bank
q. ren and y. zhao—equally-contributed authors. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 54."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"in the mil setting, the
entire wsi is regarded as a bag and tiled patches are instances. the primary
challenge of mil arises from its weakly supervised nature, i.e. only the bag-
level label for the entire wsi is provided, while labels for individual patches
are usually unavailable. although mil-based methods have shown impressive
potential in solving a wide range of pathological image analysis tasks including
cancer grading and subtype diagnosis [23], prognosis prediction [18], genotype-
related tasks such as gene mutation prediction [4], etc., it is still an open question
regarding learning an informative and eﬀective representation of the entire wsi
for down-streaming task based on mil architecture."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"nonetheless, bag-level mil needs to
learn informative embeddings of instances and adjust the contributions of these
instance embeddings to generate the bag representation simultaneously, which
faces the risk of obtaining a suboptimal model given the limited training samples
in practice. the instance-level mil, however, faces the problem of noisy labels,
which is caused by the common strategy of assigning the wsi labels to patches
and the fact that there are lots of patches irrelevant to the wsi labels [3,6].
considering these conventional mil methods usually utilize either bag-level
or instance-level supervision, leading to suboptimal performance. then we propose to combine bag-level and instance-level supervision
to improve the performance of mil."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"the co-supervision design also makes the mil to be a
562
q. ren et al.
multi-task learning framework, where the bag-level supervision channel works
to globally summarise the wsi for prediction and the instance-level supervision
channel can locally identify key relevant patches. the detailed contributions can
be summarized as follows:
1) we propose a novel mil method for pathological image analysis that leverages
a specially-designed residual transformer backbone and organically integrates
both transformer-based bag-level and label-disambiguation-based instance-
level supervision for performance enhancement. 2) we develop a label-disambiguation module that leverages prototypes and con-
ﬁdence bank to tackle the weakly supervised nature of instance-level super-
vision and reduce the impact of assigned noisy labels.
3) the proposed framework outperforms state-of-the-art (sota) methods on
public datasets and in a practical clinical task, demonstrating its superiors
in wsi analysis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"besides, ablation studies illustrate the superiority of our
co-supervision design compared to using only one type of supervision.
fig. 1. (a) overall framework of the iib-mil.(b) the detailed diagram of the label-
disambiguation-based instance-level supervision. (details are given in section: 2.1)
iib-mil: integrated instance-level
563
2
method
2.1
overview
the overall framework of the proposed iib-mil is shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"1. similar to
previous works [27], iib-mil ﬁrst transforms input huge-size wsi to a set of
patch embeddings to simplify the following learning task using a pre-trained
encoder, i.e. eﬃcientnet-b0. then a specially-designed residual transformer
backbone works to calibrate the obtained patch embeddings and encode the con-
text information and correlation of patches. after that, iib-mil utilizes both
a transformer-based bag-level and a label-disambiguation-based instance-level
supervision to cooperatively optimize the model, where the bag-level loss is cal-
culated referring to the wsi labels, while the instance loss is calculated referring
to pseudo patch labels calibrated by the label-disambiguation module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"since
bag-level supervision channel is trained to globally summarise information of all
patches for prediction, the bag-level outputs are used as the ﬁnal predictions
during the test stage.
2.2
problem formulation
assume there is a set of n wsis denoted by s = {s1, s2, ..., sn}. each wsi si
has a wsi-level label yi ∈ {1, ..., c}, where c represents category number. in
each si, there exist mi tiled patches without patch-level labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"our
proposed iib-mil comprehensively integrates obtained embeddings {ei,j, ...} to
generate accurate wsi classiﬁcation. 2.3
backbone network
before bag-level and instance-level supervision channels, we design a residual
transformer backbone t(·) : rk → rd to calibrate the obtained patch embed-
dings and encode the context information and correlation of patches. t(·) maps
patch embeddings {ei,j, ...} to a lower-dimensional feature space, denoted as
{xi,j, ...}, where xi,j = t(ei,j), xi,j ∈ rd is the calibrated embedding, t(·) is
composed of transformer layers and skip connections (details are given in the
supplementary.)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"2.4
instance-level supervision
at the core of instance-level supervision is the label disambiguation module,
which serves to rectify the imprecise labels that have been assigned to patches. it
comprises prototypes and a conﬁdence bank, takes instance features and instance
classiﬁer predictions as inputs, and generates soft labels as outputs (fig. 1 (b)). the prototypes, denoted as p ∈ rc×d, are initialized with all-zero vectors
and employ momentum-based updates using selected instance features x with
564
q. ren et al.
the highest probability probinst of belonging to their corresponding categories."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"prototype labels z are determined based on the proximity of patch features to
the prototypes. conﬁdence b ∈ rn×m×c is initialized with all wsi labels and
uses momentum-based updates with z. detailed steps are summarized as follows:
step 1: obtain the instance classiﬁer output. the instance-level clas-
siﬁer, denoted as finst(·), takes xi,j ∈ rd as input and outputs the predicted
instance probability probinst
i,j
∈ rc, as:
probinst
i,j
= softmax(finst(xi,j)),
(1)
the probability that xi,j is predicted as class c is denoted as probinst
i,j,c ∈ r1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"then
we obtain the predicted probability of wsi si as:
probbag
i
= softmax(fbag(a(xi))). (7)
the bag-level loss function is given by:
lbag = −
n

i=1
probbag
i
· log(yi),
(8)
where yi ∈ rc is the label of wsi si.
2.6
training
in the training phase, we employ a warm-up strategy in which we update only
the prototypes and do not update the conﬁdence bank during the ﬁrst few
epochs. l = lbag + λlinst,
(9)
where λ is the hyperparameter that controls the relative importance of the two
losses."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"3
experiments
3.1
dataset
we evaluate our model with three datasets. (1) luad-gm dataset: the objec-
tive is to predict the epidermal growth factor receptor (egfr) gene mutations
in patients with lung adenocarcinoma (luad) using 723 whole slide image
(wsi) slices, where 47% of cases have egfr mutations. (2) tcga-nsclc and
tcga-rcc datasets: cancer type classiﬁcation is performed using the cancer
genome atlas (tcga) dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"the tcga-nsclc dataset comprised two sub-
types, lung squamous cell carcinoma (lusc) and lung adenocarcinoma (luad),
while the tcga-rcc dataset included three subtypes: renal chromophobe cell
carcinoma (kich), renal clear cell carcinoma (kirc), and renal papillary cell
carcinoma (kirp). 3.2
experiment settings
the dataset was randomly split into three parts: training, validation, and testing,
with 60%, 20%, and 20% of the samples, respectively. wsis were preprocessed
by cropping them into 1120 × 1120 patches, without overlap."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"the results, in table 2, indicate that all of the designed components,
iib-mil: integrated instance-level
567
fig. 2. (a)t-sne plot of the patch features obtained from the backbone;(b) example
heatmaps of iib-mil on wsis with known efgr mutation labels.
including the label disambiguation module, instance-level supervision, and bag-
level supervision, contribute to the success of iib-mil. we also investigated the
impact of the warm-up epoch number and found that selecting an appropriate
value, such as warmup = 10, can lead to better model performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_54.pdf,"the numbers displayed within
each group represent the average likelihood of the egfr mutation predicted
by the patches. with the help of the label-disambiguation-based instance-level
supervision, iib-mil can identify highly positive and negative related patches
to the wsi-label, i.e., the cyan-blue group and yellow group. double-checked
by pathologists, we ﬁnd that the cyan-blue group consists of patches from lung
adenocarcinoma and the yellow group consists of patches from the squamous
cells."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"to achieve conditional
sampling from the implicit policy, we devise a forward-process guided
action inference strategy that corrects the state mismatch. we collected
a private esd video dataset with 1032 short clips to validate our method. experimental results demonstrate that our solution outperforms sota
imitation learning methods on our formulated task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14228, pp. https://doi.org/10.1007/978-3-031-43996-4_47
imitation learning for surgical dissection trajectory prediction
495
keywords: imitation learning · surgical trajectory prediction ·
endoscopic submucosal dissection · surgical data science
1
introduction
despite that deep learning models have shown success in surgical data science to
improve the quality of surgical intervention [20–22], such as intelligent workﬂow
analysis [7,13] and scene understanding [1,28], research on higher-level cognitive
assistance for surgery still remains underexplored. [9,24,29], which is challenging yet
crucial for ensuring surgical safety."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"first, the decision of
dissection trajectories is complicated and depends on numerous factors such as
safety margins surrounding the tumor. [27]. to date, there is still
no work on data-driven solutions to predict such dissection trajectories, but we
argue that it is possible to reasonably learn this skill from expert demonstrations
based on video data. imitation learning has been widely studied in various domains [11,16,18]
with its good ability to learn complex skills, but it still needs adaptation and
improvement when being applied to learn dissection trajectory from surgical
data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"[4–6,12] suﬀer from intensive computa-
tions due to reliance on the langevin dynamics, which leads to a slow training
process. in addition, the model performance can be sensitive to data distribution
and the noise in training data would result in unstable trajectory predictions. in this paper, we explore an interesting task of predicting dissection trajec-
tories in esd surgery via imitation learning on expert video data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"to eﬀectively model the surgeon’s
behaviors and handle the large variation of surgical scenes, we leverage implicit
modeling to express expert dissection skills. to address the limitations of inef-
ﬁcient training and unstable performance associated with ebm-based implicit
policies, we formulate the implicit policy using an unconditional diﬀusion model,
which demonstrates remarkable ability in representing complex high-dimensional
data distribution for videos. subsequently, to obtain predictions from the implicit
policy, we devise a conditional action inference strategy with the guidance of
forward-diﬀusion, which further improves the prediction accuracy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"results show
that our method achieves superior performances in diﬀerent contexts of surgical
scenarios compared with representative popular imitation learning methods. 2
method
in this section, we describe our approach idiﬀ-il, which learns to predict the
dissection trajectory from expert video data using the implicit diﬀusion policy. an overview of our method is shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,", it},
it ∈ rh×w ×3 and the output is an action distribution of a sequence of 2d
coordinates a = {yt+1, yt+2, ..., yt+n}, yt ∈ r2 indicating the future dissection
trajectory projected to the image space. in order to obtain the demonstrated dissection trajectories from the expert
video data, we ﬁrst manually annotate the dissection trajectories on the video
frame according to the moving trend of the instruments observed from future
frames, then create a dataset d = {(s, a)i}m
i=0 containing m pairs of video clip
(state) and dissection trajectory (action). to precisely predict the expert dissection behaviors and eﬀectively learn gen-
eralizable features from the expert demonstrations, we use the implicit model as
our imitation policy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"= max
θ
e(s,a)∼d[log pθ(s, a)].
(1)
in this regard, the imitation of surgical dissection decision-making is converted
to a distribution approximation problem. 2.2
training implicit policy as diﬀusion models
approximating the joint state-action distribution in eq. 1 from the video demon-
stration data is challenging for previous ebm-based methods. to address the
learning of implicit policy, we rely on recent advances in diﬀusion models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"by rep-
resenting the data using a continuous thermodynamics diﬀusion process, which
can be discretized into a series of gaussian transitions, the diﬀusion model is
498
j. li et al.
able to express complex high-dimensional distribution with simple parameter-
ized functions. in addition, the diﬀusion process also serves as a form of data
augmentation by adding a range of levels of noise to the data, which guarantees
a better generalization in high-dimensional state space. as shown in fig. 1 (a), the diﬀusion model comprises a predeﬁned forward
diﬀusion process and a learnable reverse denoising process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"the forward pro-
cess gradually diﬀuses the original data x0 = (s, a), to a series of noised data
{x0, x1, · · · , xt } with a gaussian kernel q(xt|xt−1), where t denotes the dif-
fusion step. in the reverse process, the data is recovered via a parameterized
gaussian pθ(xt−1|xt) iteratively. with the reverse process, the joint state-action
distribution in the implicit policy can be expressed as:
pθ(x0) =

x1:t
pθ(x0:t ) =

x1:t
p(xt )

pθ(xt−1|xt) = epθ(x1:t )pθ(x0|x1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"ϵa∥ + γ∥ϵs
θ(xt, t) − ϵs∥],
(3)
where ϵs and ϵa are sampled from n(0, i s), n(0, i a) respectively. to better
process features from video frames and trajectories of coordinates, we employ a
variant of the unet as the implicit diﬀusion policy network, where the trajectory
information is fused into feature channels via mlp embedding layers. then the
trajectory noise is predicted by an mlp branch at the bottleneck layer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"2.3
conditional sampling with forward-diﬀusion guidance
since the training process introduced in sect. 2.2 is for unconditional generation,
the conventional sampling strategy through the reverse process will predict ran-
dom trajectories in expert data. an intuitive way to introduce the condition into
the inference is to input the video clip as the condition state s∗ to the implicit
diﬀusion policy directly, then only sample the action part."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"the deterministic action
can be obtained by taking the most probable samples during the reverse process. 3
experiments
3.1
experimental dataset and evaluation metrics
dataset. we evaluated the proposed approach on a dataset assembled from 22
videos of esd surgery cases, which are collected from the endoscopy centre of
the prince of wales hospital in hong kong."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"the remaining 290 clips
(consisting of 970 frames) were used for testing. first, to study how the model performs on data within
the same surgical context as the training data, we deﬁne a subset, referred as
to the “in-the-context” testing set, which consists of consecutive frames selected
from the same cases as included in the training data. second, to assess the
model’s ability to generalize to visually distinct scenes, we created an “out-of-
the-context” testing set that is composed of video clips sampled from 2 unseen
surgical cases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"the sizes of these two subsets are 224 and 66 clips, respectively.
evaluation metrics. quantitative results on the in-the-context and the out-of-the-context data
in metrics of ade/fde/fd. values in parentheses denote video-clip wise standard
deviation. besides, we also use the fr´echet distance (fd) metric, to indicate
the geometrical similarity between two temporal sequences."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"this exhibits
the limitations of ebm-based methods in learning visual representations from
complex endoscopic scenes. the superior results achieved by our method demon-
strate the eﬀectiveness of the diﬀusion model in learning the implicit policy from
the expert video data. in addition, our method can learn generalizable dissection
skills by exhibiting a lower standard deviation of the prediction errors compared
to the bc, which severely suﬀers from over-ﬁtting to the training data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"the implicit modeling makes a more signiﬁcant contribution in
predicting within the “in-the-context” scenes, suggesting that the implicit model
excels at capturing subtle changes in surgical scenes. while our method improves
marginally compared with the explicit form on the “out-of-the-context” data,
exhibiting a slighter over-ﬁtting with a lower standard deviation. we also investigated the necessity of the
forward-diﬀusion guidance in conditional sampling for prediction accuracy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"3, the implicit diﬀusion policy
beneﬁts more from the forward-diﬀusion guidance in the “in-the-context” scenes,
achieving an improvement of 0.33 on ade. when encountered with the unseen
scenarios in “out-of-the-context” data, the performance improvement of such
inference strategy is marginal.
value of synthetic data. since the learned implicit diﬀusion policy is capable
of generating synthetic expert dissection trajectory data, which can potentially
reduce the expensive annotation cost."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"we randomly generated 9k video-trajectory
pairs by unconditional sampling from the implicit diﬀusion policy. then, we
train the bc model with diﬀerent data, the pure expert data (real), synthetic
data only (synt) and the mixed data with the real and the synthetic (mix). the table in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_47.pdf,"3 shows the synthetic data is useful as the augmented data for
downstream task learning. 3. left: ablation study of key method components; middle: visualization of
reverse processes of unconditional/conditional sampling from implicit policy; right:
performance of bc trained with synthetic data v.s. our method on ade."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"besides, diﬀerent from existing models that used cnns or lstms to
exploit tumor enhancement patterns on dynamic contrast-enhanced ct
imaging, we improved the extraction of dynamic tumor-related texture
features in multi-phase contrast-enhanced ct by fusing local and global
features using cnn and transformer modules, further enhancing the fea-
tures extracted across multi-phase ct images. we extensively evaluated
and compared the proposed method with existing methods in the multi-
center (n = 4) dataset with 1,070 patients with pdac, and statistical
h. dong—work was done during an internship at alibaba damo academy. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 24.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"how-
ever, current clinical markers such as larger tumor size and high carbohydrate
antigen (ca) 19-9 level may not be suﬃcient to accurately tailor neoadjuvant
treatment for patients [19]. therefore, multi-phase contrast-enhanced ct has a
great potential to enable personalized prognostic prediction for pdac, lever-
aging its ability to provide a wealth of texture information that can aid in the
development of accurate and eﬀective prognostic models [2,10].
previous studies have utilized image texture analysis with hand-crafted fea-
tures to predict the survival of patients with pdacs [1], but the representational
fig. 1. two examples of spatial information between vessel (orange region) and tumor
(green region)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"however,
pdacs diﬀer signiﬁcantly from the tumors in these studies. [14].
therefore, focusing solely on the texture information of the tumor itself may
not be eﬀective for the prognostic prediction of pdac. it is necessary to incor-
porate tumor-vascular involvement into the feature extraction process of the
prognostic model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"furthermore, to capture the tumor enhancement patterns across multi-phase ct
images, we are the ﬁrst to combine convolutional neural networks (cnn) and
transformer [4] modules for extracting the dynamic texture patterns of pdac
and its surroundings. this approach takes advantage of the visual transformer’s
adeptness in capturing long-distance information compared to the cnn-only-
based framework in the original approach. by incorporating texture information
between pdac, pancreas, and peripancreatic vessels, as well as the local tumor
information captured by cnn, we aim to improve the accuracy of our prognostic
prediction model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"in this study, we make the following contributions: (1) we propose a novel
approach for aiding survival prediction in pdac by introducing a learnable
neural distance that explicitly evaluates the degree of vascular invasion between
the tumor and its surrounding vessels. (2) we introduce a texture-aware trans-
former block to enhance the feature extraction approach, combining local and
global information for comprehensive texture information. we validate that the
cross-attention is utilized to capture cross-modality information and integrate it
with in-modality information, resulting in a more accurate and robust prognos-
tic prediction model for pdac."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"2.1
texture-aware vision transformer: combination of cnn
and transformer
recently, self-attention models, speciﬁcally vision transformers (vits [4]), have
emerged as an alternative to cnns in survival prediction [15,25]. [13], aims to combine both
local information (such as pdac texture) and global information (such as the
relationship between pdac and the pancreas). this approach is diﬀerent from
previous methods that rely solely on either cnn-based or transformer-based
backbones, focusing only on local or global information, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"2. an overview of the proposed method. the texture-aware transformer cap-
tures texture information among pdac, pancreas and vessels around pancreas with
our proposed texture-aware transformer block and a cross-attention block to fusion
cross-modality features. the structure-aware block extracts the structure relationship
between pdac and four related vessels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"therefore, we obtain three outputs from the transformer block with the input
of these phases, denoted as f1
o, f2
o, f3
o ∈ rd×c, resulting in the concatenated
output fo ∈ rd×3c. instead of directly fusing the outputs as in previous work, we employ a 3-way
cross-attention block to extract cross-modality information from these phases. i, j ≤ (k + 1)c,
k = 0, 1, 2,
0
otherwise,
(1)
here, q, k, v are the query, key, and value matrices, respectively, obtained
by linearly projecting the input ft
o ∈ r3c×d."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"the cross-modality output fcross
and in-modality output ft
o are then concatenated and passed through an average
pooling layer to obtain the ﬁnal output feature of the texture branch, denoted
as ft ∈ rct. 2.2
neural distance: positional and structural information
between pdac and vessels
the vascular involvement in patients with pdac aﬀects the resectability and
treatment planning [5]. tal vein and splenic vein (pvsv), superior mesenteric artery (sma), superior
mesenteric vein (smv), and truncus coeliacus (tc)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"(4)
secondly, we regard the entire sets ˆvc and ˆpc as sequences and calculate the
distance using a 2-way cross-attention block (similar to eq. 1) to build a neural
distance based on the 3d spatial coordinates of each point:
dθ(ˆv, ˆp) = crossattention(ˆvc, ˆpc),
ˆvc, ˆpc ∈ rk×3. (5)
neural distance allows for the ﬂexible assignment of weights to diﬀerent points
and is able to ﬁnd positional information that is more suitable for pdac prog-
nosis prediction. in addition to neural distance, we use the 3d-cnn model
introduced in [22] to extract the structural relationship between pdac and the
vessels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"speciﬁcally, we concatenate each pdac-vessel pair xv
s ∈ r2×h×w ×d,
where v ∈{pvsv, smv, sma, tc} and obtain the structure feature fs ∈ rcs. finally, we concatenate the features extracted from the two components and
apply a fully-connected layer to predict the survival outcome, denoted as oos,
which is a value between 0 and 1. to optimize the proposed model, we use the
negative log partial likelihood as the survival loss [9].
3
experiments
dataset. in this study, we used data from shengjing hospital to train our
method with 892 patients, and data from three other centers, including guang-
dong provincial people’s hospital, tianjin medical university and sun yat-
sen university cancer center for independent testing with 178 patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"we also reported the survival auc, which esti-
mates the cumulative area under the roc curve for the ﬁrst 36 months. implementation details: we used nested 5-fold cross-validation and aug-
mented the training data by rotating volumetric tumors in the axial direction
and randomly selecting cropped regions with random shifts. we also set the out-
put feature dimensions to ct = 64 for the texture-aware transformer, cs = 64
for the structure extraction and k = 32 for the neural distance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"we ﬁrst evaluated the performance of our proposed texture-
aware transformer (tat) by comparing it with the resnet18 cnn backbone
and vit transformer backbone, as shown in table 1. our model leverages the
strengths of both local and global information in the pancreas and achieved the
best result. next, we compared diﬀerent methods for multi-phase stages, includ-
ing lstm, early fusion (fusion), and cross-attention (cross) in our method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_24.pdf,"presented the results in table 1. combining the texture-aware
transformer and regular structure information improved the results from 0.630
to 0.648, as tumor invasion strongly aﬀects the survival of pdac patients. we
also employed a simple 4-variable regression model that used only the chamfer
distance of the tumor and the four vessels for prognostic prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"the proposed de-biased sp modules eﬀectively disentangle latent
race-intrinsic attributes from the survival features, which provides a fair
survival outcome through the survival prediction head. we evaluate our
method using a multimodal pe dataset with time-to-event labels and race
identiﬁcations. the comprehensive results show an eﬀective de-biased per-
formance of our framework on outcome predictions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"keywords: pulmonary embolism · deep survival prediction ·
de-bias learning · multi-modal learning
1
introduction
bias in medicine has demonstrated a notable challenge for providing comprehen-
sive and equitable care. implicit biases can negatively aﬀect patient care, particu-
larly for marginalized populations with lower socioeconomic status [30]. evidence
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_50. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"in an algorithm used to
predict healthcare costs, black patients who received the same health risk scores
as white patients were consistently sicker [21]. using biased data for ai models
reinforces racial inequities, worsening disparities among minorities in healthcare
decision-making [22].
within the radiology arm of ai research, there have been signiﬁcant advances
in diagnostics and decision making [19]. along these advancements, bias in
healthcare and ai are exposing poignant gaps in the ﬁeld’s understanding of
model implementation and their utility [25,26]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"the pulmonary embolism severity index (pesi) is a
well-validated clinical tool based on 11 clinical variables and used for outcome pre-
diction measurement [2]. [7,12,14].
however, one issue with traditional survival analysis is bias from single modal
data that gets compounded when curating multimodal datasets, as diﬀerent
combinations of modes and datasets create with a uniﬁed structure. multimodal
data sets are useful for fair ai model development as the bias complementary
from diﬀerent sources can make de-biased decisions and assessments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"in that
process, the biases of each individual data set will get pooled together, creating
a multimodal data set that inherits multiple biases, such as racial bias [1,15,23]. in addition, it has been found that creating multimodal datasets without any de-
biasing techniques does not improve performance signiﬁcantly and does increase
bias and reduce fairness [5]. overall, a holistic approach to model development
would be beneﬁcial in reducing bias aggregation in multimodal datasets. [4] for bias disentanglement
improves model generalization for fairness [3,6,27].
we developed a pe outcome model that predicted mortality and detected
bias in the output."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"the survival head predicts the outcomes based
on the de-biased survival attributes. 2
bias in survival prediction
this section describes the detail of how we identify the varying degrees of bias
in multimodal information and illustrates bias using the relative diﬀerence in
survival outcomes. we will ﬁrst introduce our pulmonary embolism multimodal
datasets, including survival and race labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"then, we evaluate the baseline sur-
vival learning framework without de-biasing in the various racial groups.
dataset. the pulmonary embolism dataset used in this study from 918 patients
(163 deceased, median age 64 years, range 13–99 years, 52% female), including
3978 ctpa images and 918 clinical reports, which were identiﬁed via retro-
spective review across three institutions. the clinical reports from physicians
that provided crucial information are anonymized and divided into four parts:
medical history, clinical diagnosis, observations and radiologist’s opinion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"[2].
diverse bias of multimodal survival prediction model. we designed a
deep survival prediction (sp) baseline framework for multimodal data as shown
518
z. zhong et al.
in fig. 1, which compares the impact of diﬀerent population distributions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"the
frameworks without de-basing are evaluated for risk prediction in the test set
by performing survival prediction on ctpa images, clinical reports, and clini-
cal variables, respectively. first, we use two large-scale data-trained models as
backbones to respectively extract features from preprocessed images and cleaned
clinical reports. [11] is used as
the backbone model for analyzing imaging risk and extracting information from
multiple slices of volumetric ctpa scans to locate the pe."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"the feature with the
highest pe probability from a patient’s multiple ctpas is considered as the most
pe-related visual representation. next, the gatortron [29] model is employed
to recognize clinical concepts and identify medical relations for getting accurate
patient information from pe clinical reports. [img, text, var]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"for reference, the c-index of
pesi scores is additionally provided for comparative analysis. in table 1 (baseline), we computed the c-index between the predicted risk
of each model and time-to-event labels. when debiasing is not performed, signif-
icant diﬀerences exist among the diﬀerent modalities, with the image modality
exhibiting the most pronounced deviation, followed by text and pesi variables."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"this
redundancy leads to model overﬁtting on race, compromising the fairness of risk
prediction across diﬀerent races. besides, clinical data in the form of text reports
and pesi variables objectively reﬂect the patient’s physiological information and
the physician’s diagnosis, exhibiting smaller race biases in correlation with sur-
vival across diﬀerent races. moreover, the multimodal fusion strategy is found
to be eﬀective, yielding more relevant survival outcomes than the clinical gold
standard pesi scores."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"2, we
present a feature-level de-biased sp module that enhances fairness in survival
de-biased outcome prediction model
519
outcomes by decoupling race attributes, as shown in the lower right of fig. 1.
in the de-biased sp module, ﬁrstly, two separate encoders em
i
and em
c are for-
mulated to embed features f m into disentangled latent vectors for race-intrinsic
attributes zid or race-conﬂicting attributes zsur implied survival information [16]. [31] to
train em
c and cm
c to overﬁt to race label while training em
i
and cm
i
with cross-
entropy (ce) loss."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"[13], which optimizes the cox partial likelihood, is
used to maximize concordance diﬀerentiable and update model weights of the
survival branch. − log

j:yj
t >yi
t
ecsur(zj
sur)
⎞
⎠
(4)
loverall = ldis + λswlsw + λsurlcoxph
(5)
where yt and ye are survival labels including the survival time and the event,
respectively. the weights λsw and λsur are assigned as 0.5 and 0.8, respectively,
to balance the feature disentanglement and survival prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"4
experiment
we validate the proposed de-biased survival prediction frameworks on the col-
lected multi-modality pe data. the data from 3 institutions are randomly split
520
z. zhong et al.
table 1. performance comparison of the proposed de-biased sp framework and base-
line using c-index values on multiple modal outcomes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"it takes in a sliding window of 24
slices at a time, resulting in a window-level prediction that represents the prob-
ability of pe for the current slices [11]. the penet is pre-trained on large-scale
ctpa studies and shows excellent pe detection performance with an auroc
of 0.85 on our entire dataset. the 2048 dimensional features from the last convo-
lution with the highest probability of pe, are designated as the imaging features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"the
mlps with 3 hidden layers are used to encode image and text features, and
another mlps with 2 layers encodes the features of pesi variables. a fully
connected layer with sigmoid activation acts as a risk classiﬁer cm
sur (zm
sur) for
survival prediction, where zm
sur is the feature encoded from single modal data. for training the biased and de-biased sp modules, we collect data from one
modality as a batch with synchronized batch normalization."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"experiments are conducted
on an nvidia gv100 gpu.
de-biased outcome prediction model
521
fig. 2. tsne visualizations of the features from multimodal data. based on the com-
parison between the id features and others, it is observed that the clusters containing
race obtained from the same class are more compact."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"the results indicate the eﬀectiveness of the proposed de-biasing in mitigating
race inequity. the results also prove the observations for the diﬀerent biases
present in diﬀerent modalities, especially in the ctpa images containing more
abundant race-related information. it also explains the limited eﬀectiveness of
de-biasing the clinical results, which contain less racial identiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"2. we observe the disentanglement in the visualization of the id features
zid, while the survival features zsur eliminate the race bias. the lack of appar-
ent race bias observed in both the original features and those encoded in the
baseline can be attributed to the subordinate role that id features play in the
multimodal information. the kaplan-meier (k-m) survival curve [14], as shown
in fig. 3, is used to compare the survival prediction between high-risk and low-
risk patient groups."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_50.pdf,"as shown in table 2, the diﬀerent training settings show signiﬁcant diﬀerences
in survival prediction performance across modalities. the swapping augmenta-
tion provides a strong bias correction eﬀect for image data with obvious bias.
for clinical data, the resampling generally improves performance in most cases. overall, multimodal fusion approaches are eﬀective in all training settings, and
the coxph model can actively learn the optimal combination of multimodal
features to predict survival outcomes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"the displacement estimation step of ultrasound elastogra-
phy (use) can be done by optical ﬂow convolutional neural networks
(cnn). even though displacement estimation in use and computer
vision share some challenges, use displacement estimation has two dis-
tinct characteristics that set it apart from the computer vision coun-
terpart: high-frequency nature of rf data, and the physical rules that
govern the motion pattern. the high-frequency nature of rf data has
been well addressed in recent works by modifying the architecture of
the available optical ﬂow cnns."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"recently, physically inspired constraint for unsuper-
vised regularized elastography (picture) has been introduced which
incorporates the physical laws of deformation by introducing a regu-
larized loss function. picture tries to limit the range of the lateral
displacement by the feasible range of poisson’s ratio and the estimated
high-quality axial displacement. despite the improvement, the regular-
ization was only applied during the training phase."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"the reﬁnement
optimization methods are embedded into the diﬀerent pyramid levels of
the network architecture to improve the estimate. our results on exper-
imental phantom and in vivo data show that the proposed method sub-
stantially improves the estimated displacements. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_45.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"https://doi.org/10.1007/978-3-031-43907-0_45
468
a. k. z. tehrani and h. rivaz
keywords: ultrasound elastography · convolutional neural
networks · physically inspired constraint · known operator · poisson’s
ratio
1
introduction
ultrasound elastography (use) provides information related to the stiﬀness of
the tissue. ultrasound (us) data before and after the tissue deformation (which
can be caused by an external or internal force) are collected and compared
to calculate the displacement map, indicating each individual sample’s relative
motion. the strain is computed by taking the derivative of the displacement
ﬁelds."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"unsupervised and semi-supervised
training methods have been proposed, which enable the networks to use real us
images for training [1,14,17]. the proposed networks have achieved high-quality
axial strains. in contrast to axial strain, lateral strain, which is highly required
in poisson’s ratio imaging and elasticity reconstruction, has a poor quality due
to the low sampling frequency, limited motion and lack of carrier signal in the
lateral direction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"recently, physically inspired constraint in unsupervised regularized elastog-
raphy (picture) has been proposed [5]. this method aims to improve lat-
eral displacement by exploiting the high-quality axial displacement estimation
and the relation between the lateral and axial strains deﬁned by the physics of
motion. despite the substantial improvement, the regularization is only applied
during the training phase."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"the core idea is that some known operations (for example
inversion of a matrix) are embedded inside the networks to simplify the training
and improving the generalization ability of the network. the known operator can
be viewed as the prior knowledge related to the physics of the problem. [7].
in this paper, we aim to embed two lateral displacement reﬁnement algo-
rithms in the cnns to improve the lateral strains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"we then introduce our method for incorporating
known operators into our deep model and outline our unsupervised training
technique. we then present the training and test datasets and ﬁnish the section
by demonstrating the network architecture.
2.1
picture
let εx denote axial (x = 1), lateral (x = 2), and out-of-plane (x = 3) strains. assuming linear elastic, isotropic, and homogeneous material that can move
freely in the lateral direction, the lateral strain can be obtained from the axial
strain and the poisson’s ratio by ε2 = −v × ε."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"lvs, where λvs is the weight
of the smoothness loss. picture loss is added to the data and smoothness
losses of unsupervised training. 2.2
known operators
the known operators are added to the network in the inference mode only due to
the high computational complexity of unsupervised training (outlined in the next
section)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"it is worth highlighting
that known operators oﬀer a compelling alternative to regularization. while the
latter involves adjusting trained weights based on the training data and keep-
ing them ﬁxed during testing, the former relies on iterative reﬁnement that is
adaptable to the test data and does not require any learnable weights.
2.3
unsupervised training
we followed a similar unsupervised training approach presented in [5] for both
picture and kpicture methods. λv lv
(6)
known operators for ultrasound elastography
471
algorithm 1: poisson’s ratio clipper
input : lateral displacement wl, axial displacement wa, vemin,vemax, iteration
output: reﬁned lateral displacement wref
1 wref ← wl
2 for q ← 1 to iteration do
3
e22 ← ∂wl
∂l
// gradient in lateral direction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"the network is iterative
with 5 pyramid levels. the known operators are added after optical ﬂow estimation,
and reﬁne the estimated lateral displacement in each pyramid level (added from level
3) to provide improved lateral displacement to the next pyramid level.
where ld denotes photometric loss which is obtained by comparing the pre-
compressed and warped compressed rf data, ls is smoothness loss in both
axial and lateral directions. λs and λv specify the weights of the smoothness
loss and picture loss, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"the young’s mod-
ulus of the experimental phantom was 20 kpa and contains several inclusions
with young’s modulus of higher than 40 kpa. this data is available online at
http://code.sonography.ai in [16]. in vivo data was collected at johns hopkins hospital from patients with liver
cancer during open-surgical rf thermal ablation by a research antares siemens
system using a vf 10-5 linear array with the sampling frequency of 40 mhz and
the center frequency of 6.67 mhz."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"the institutional review board approved the
study with the consent of the patients. we selected 600 rf frame pairs of this
dataset for the training of the networks. two well-known metrics of contrast to noise ratio (cnr) and strain ratio
(sr) are utilized to evaluate the compared methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"we made the network’s weight trained using both picture and spic-
ture methods publicly available online at http://code.sonography.ai. we also
employed a similar hyper-parameters and training schedule for experimental
phantom and in vivo data.
3.2
results and discussions
the lateral strains of ultrasound rf data collected from three diﬀerent loca-
tions of the tissue-mimicking breast phantom are depicted in fig. 2, and the
quantitative results are given in table 1. visual inspection of fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"for example, the inclusion borders in sample 2 are much more
clearly visible. the strain images obtained by kpicture have a much higher
quality than those of picture. furthermore, kpicture has the highest qual-
ity strain images among the compared methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"kpicture further limits the epr values; only a small number of samples
are outside of the physically plausible range. the lateral strain results of in vivo data are depicted in fig. 3 (b), and axial
strains are given in the supplementary materials (the quality of axial strains is
high in all methods)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_45.pdf,"their performance on anisotropic materials can be investigated by
experiments on anisotropic tissues such as muscles. furthermore, 3d imaging
data can be collected from 2d arrays to have information in out-of-plane direc-
tion to be able to formulate known operators and picture loss for anisotropic
tissues. it should be noted that after incorporating the known operators, the inference
time of the network increased from an average of 195 ms to 240 ms (having 10
iterations for algorithm 1 and 100 iterations for algorithm 2)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"thirdly, to improve
the distinction between the object and the background, we apply con-
ditional encoding to enhance the intermediate features with the original
image encoding. to objectively validate the proposed method, we com-
pared state-of-the-art deep learning model on the 2015 miccai gland
segmentation challenge (glas) dataset and the colorectal adenocarci-
noma gland (crag) dataset. the experimental results show that our
method improves the accuracy of segmentation and proves the eﬃcacy
of the method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"hence, automated methods
for glandular instance segmentation hold signiﬁcant value in clinical practice.
fig. 1. (a–b) example images from the crag dataset. (c–d) example images from
the glas dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"however, these methods may face diﬃculties in capturing diﬀerent cell shapes
and distinguishing tightly positioned gland boundaries. limitations arise from
image scaling and cropping, leading to information loss or distortion, resulting
in ineﬀective boundary recognition and over-/under-segmentation. to overcome
these limitations, we aim to perform gland instance segmentation to accurately
identify the target location and prevent misclassiﬁcation of background tissue."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"[9] has gained popularity as eﬃcient generative
models [16]. in the task of image synthesis, diﬀusion model has evolved to achieve
state-of-the-art performance in terms of quality and mode coverage compared
with gan [32]. [4] treats the object detection task as a generative task on
the bounding box space in images to handle projection detection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"(3) to enhance object-background diﬀerentiation, we utilize conditional
encoding to augment intermediate features with the original image encoding. this method eﬀectively integrates the abundant information from the original
image, thereby enhancing the distinction between the objects and the surround-
ing background. [6] (as shown in fig. 1), and the experiment
results demonstrate the eﬃcacy of the method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"the image decoder
based on a diﬀusion model incorporates the original image features as conditions to
enhance the intermediate features. to preserve multi-scale information, we introduce a
mask branch that operates on fmask. by applying convolutions with weights assigned
from ﬁlters to fmask, we obtain instance masks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"[30], which typically uses
two markov chains divided into two phases: a forward diﬀusion process and
a reverse denoising process. the components of diﬀusion model are a learning
reverse process called pθ(zt−1|zt) that creates samples by converting noise into
samples from q(z0) and a forward diﬀusion process called q(zt|zt−1) that gradu-
ally corrupts data from some target distribution into a normal distribution. ∈ (0, 1), t ∈ {1, ..., t} determines the amount of noise
that is introduced at each stage."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"our image decoder is based on diﬀusion model, which can be viewed as a
noise-to-gt denoising process. in this setting, the data samples consist of a set
of bounding boxes represented as z0, where z0 is a set of n boxes. the neural network fθ(zt, t) is trained to predict z0 from the zt based on
the corresponding image x. in addition, to achieve complementary information
by integrating the segmentation information from zt into the original image
encoding, we introduce conditional encoding, which uses the encoding features
of the current step to enhance its intermediate features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"[22] to predict masks in our study. in this stage, we use the mask branch to fuse the diﬀerent scale information of
the fpn and output the mask feature fmask. the diﬀusion process decodes roi
features into local masks, and multi-scale features can be supplemented with
more detailed information for predicting global masks to compensate for the
detail lost in the diﬀusion process, and we believe that instance masks require
a larger perceptual domain because of the higher demands on instance edges."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"in this work, we
chose γ = 5 to balance these two losses. 3
experiments and results
we presented the segmentation results of our model compared to the ground
truth in fig. 3, and provided both qualitative and quantitative evaluations that
validate the eﬀectiveness of our proposed network for gland instance segmenta-
tion.
data and evaluation metrics: we evaluated the eﬀectiveness of the pro-
posed model on two datasets: the glas dataset and the crag dataset. the
glas dataset comprises 85 training and 80 testing images, divided into 60 images
in test a and 20 images in test b."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"we have adopted vahadane method for stain normal-
ization [1]. furthermore, to enhance the training dataset and mitigate the risk
of overﬁtting, we employed random combinations of image ﬂipping, translation,
gaussian blur, brightness variation, and other augmentation techniques. we assessed the segmentation results using three metrics from the glas chal-
lenge: (1) object f1, which measures the accuracy of detecting individual glands,
instance-aware diﬀusion model for gland segmentation
667
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"image decoder, mask branch and mask fcn head are trained end-to-
end. we trained on the glas and crag datasets in a python 3.8.3 environment
on ubuntu 18.04, using pytorch 1.10 and cuda 11.4. 10−5 and the weight
decay as 10−4."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"[6], the gcsba-net [25], and the mpcnn [19]. table 1
provides an overview of the average performance of these models. our proposed model demonstrated a enhancement in performance, surpass-
ing the second-best method on both test a and test b datasets. speciﬁcally,
on test a, we observed an improvement of 0.006, 0.01, and 1.793 in object
f1, object dice, and object hausdorf."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"although test b presented a more challenging task due to
the presence of complex morphology in the images, our proposed model demon-
strated accurate segmentation in all cases. the experimental results highlighted
the eﬀectiveness of our approach in improving the accuracy of gland instance
segmentation.
668
m. sun et al.
results on the crag dataset: the proposed model was additionally evalu-
ated on the crag dataset by comparing it against the gcsba-net, doubleu-
net, dse model, mild-net, and dcan. the average performance of these
models is shown in table 2."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"our experimental results demonstrate that our pro-
posed method achieves superior performance, with improvements of 0.017, 0.012,
and 4.026 for object f1, object dice, and object hausdorﬀ, respectively, com-
pared to the second-best method. these results demonstrate the eﬀectiveness of
our method in segmenting diﬀerent datasets. ablation studies: our network utilizes the mask branch and conditional
encoding to enhance performance and segmentation quality."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"the mask branch is responsible for multi-scale feature extraction and
fusion with the backbone network, as well as reﬁning the image decoder’s out-
put. without the mask branch, direct usage of original image features lacks
multi-scale information and results in less accurate segmentation. conditional
encoding is employed to establish a connection between input image features
table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"the s represents the
score, r represents the rank and rank sum refers to the sum of rank for each evaluation
metric. the experimental results on the crag dataset. [2]
0.736 5
0.794 6
218.76
6
17
instance-aware diﬀusion model for gland segmentation
669
and the diﬀusion model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_64.pdf,"similarly, by utilizing conditional encod-
ing, we observed an improvement of 0.048, 0.034, 0.052 in object f1, and 0.026,
0.042, 0.057 in object dice, while object hausdorﬀ decreased by 6.771, 8.115,
12.141 on glas test a, glas test b, and crag, respectively.
table 3. the ablation study results on the crag and glas datasets demonstrate the
impact of diﬀerent modules on performance. the mask branch module contributes to
multi-scale feature extraction, while the conditional encoding module establishes the
connection between input image features and the diﬀusion model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"advanced deep learning methods are required to address this
need for explainability and high performance. in this work, we investigate
whether additional information available during the training process can
be used to create an understandable and powerful model. we propose
an innovative solution called proto-caps that leverages the beneﬁts of
capsule networks, prototype learning and the use of privileged informa-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"keywords: explainable ai · capsule network · prototype learning
1
introduction
deep learning-based systems show remarkable predictive performance in many
computer vision tasks, including medical image analysis, and are often compa-
rable to human performance. however, the complexity of this technique makes
it challenging to extract model knowledge and understand model decisions. this
limitation is being addressed by the ﬁeld of explainable ai, in which signiﬁcant
progress has been made in recent years."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"our work proves this once again by providing a powerful and explainable
solution for medical image classiﬁcation. a promising approach for interpretability is the use of privileged informa-
tion, i.e. information that is only available during training [19,20]. besides using
the additional knowledge to improve performance, it can also help to increase
explainability, as has already been shown using the lidc-idri dataset [3]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"shen et al. [16] used the attributes with a
hierarchical 3d cnn approach, demonstrating the potential of using this priv-
ileged information. lalonde et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"the goal is to ﬁnd embedded prototypes (i.e. examples) that
best separate the images by their classes [5]. this idea has been applied to var-
ious methods, such as unsupervised learning [13], few- and zero-shot learning
[17,18,22], as well as for capsule networks [21], however without the use of privi-
leged information. a successful approach is prototypical models with case-based
reasoning, which justify their prediction by showing prototypical training exam-
ples similar to the input instance [4,12]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"it is up to the user to
guess which features of the image regions are relevant to the network and are
exempliﬁed by the prototypes. our method addresses the limitations of privileged information-based and
prototype-based explanation by combining case-based visual reasoning through
exemplary representation of high-level attributes to achieve explainability and
high-performance. the proposed method is an image classiﬁer that satisﬁes
explainable-by-design with two elements: first, decisive intermediate results of a
high-performance cnn are trained on human-deﬁned attributes which are being
predicted during application."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"second, the model provides prototypical natural
images to validate the attribute prediction. in addition to the enhanced explain-
ability oﬀered by the proposed approach, to our knowledge the proposed method
outperforms existing studies on the lidc-idri dataset. proto-caps: interpretable medical image classiﬁcation
437
the main contributions of our work are:
– a novel method that, for the ﬁrst time to our knowledge, combines privileged
information and prototype learning to provide increased explanatory power
for medical classiﬁcation tasks.
– a prototype network architecture based on a capsule network that leverages
the beneﬁts of both techniques.
– an explainable solution outperforming state-of-the-art explainable and non-
explainable methods on the lidc-idri dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"during the training, a combined loss
function encourages a training sample to be close to a prototype of the correct
attribute class and away from prototypes dedicated to others, similar to existing
approaches [6]. randomly initialized, the prototypes are a representative subset
of the training dataset for each attribute after the training. for this, a cluster
cost reduces the euclidean distance of a sample’s capsule vector oa to the nearest
prototype vector pj of group pas which is dedicated to its correct attribute score."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"nodules were also scored according to their characteristics with respect to pre-
deﬁned attributes, namely subtlety (diﬃculty of detection, 1-extremely subtle,
5-obvious), internal structure (1-soft tissue, 4-air), pattern of calciﬁcation (1-
popcorn, 6-absent), sphericity (1-linear, 5-round), margin (1-poorly deﬁned, 5-
sharp), lobulation (1-no lobulation, 5-marked lobulation), spiculation (1-no spic-
ulation, 5-marked spiculation), and texture (1-non-solid, 5-solid). the pylidc
framework [7] is used to access and process the data. the mean attribute anno-
tation and the mean and standard deviation of the malignancy annotations are
calculated."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"to ensure comparability with previous work [8,9,11],
the main metric used is within-1-accuracy, where a prediction within one score
is considered correct. five-fold stratiﬁed cross-validation was performed using
10 % of the training data for validation and the best run of three is reported. 440
l. gallée et al.
the algorithm was implemented using the pytorch framework version 1.13 and
cuda version 11.6."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"besides pure performance, the eﬀect of reduced availability of attribute anno-
tations was investigated. this was done by using attribute information only for
a randomly selected fraction of the nodules during the training.
to investigate the eﬀect of prototypes on the network performance, an abla-
tion study was performed. three networks were compared: proto-caps (pro-
posed) including learning and applying prototypes during inference, proto-
capsw/o use where prototypes are only learned but ignored for inference, and
proto-capsw/o learn using the proposed architecture without any prototypes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"table 1 shows the results of our experiments compared to other
state-of-the-art approaches, with results taken from original reports. the accu-
racy of the proposed method exceeds previous work in both the malignancy and
almost all attribute predictions, while modelling all given attributes.
table 2 lists the results obtained when only fractions of the training samples
come with attribute information. the experiments indicate that the performance
of the given approach is maintained up to a fraction of 10 %."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"using no attribute
annotations at all, i.e. no privileged information, achieves a similar performance,
but results in a loss of explainability, as the high-level features extracted in the
capsules are not understandable to humans. this result suggests that privileged
proto-caps: interpretable medical image classiﬁcation
441
information here leads to an increase in interpretability for humans by providing
attribute predictions and prototypes without interfering with the model perfor-
mance.
fig. 2. one correct and two wrongly predicted examples with exemplary attribute
prototypes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"proto-caps: interpretable medical image classiﬁcation
443
the experiments demonstrate that it outperforms state-of-the-art methods that
provide less explainability. our data reduction studies show that the proposed
solution is robust to the number of annotated examples, and good results are
obtained even with a 90% reduction in privileged information. this opens the
door for application to other datasets by reducing the additional annotation
overhead."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_42.pdf,"while we did see a reduction in performance with too few labels, our
results suggest that this is mainly due to inhomogeneous coverage of individ-
ual attribute values. in this respect, it would be interesting to ﬁnd out how a
speciﬁc selection of the annotated samples, e.g. with extremes, aﬀects the accu-
racies, especially since our results show that the overall performance is robust
even when the attributes are not explicitly trained, i.e. without additional priv-
ileged information. another area of research would be to explore other types of
privileged information that require less extra annotation eﬀort, such as medi-
cal reports, to train the attribute capsules."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"then,
the augmented ct is generated by fusing the labeled vascular tree and
the non-contrasted intraoperative ct. our method is trained and val-
idated on porcine data, achieving an average dice score of 0.81 on the
predicted vessel tree instead of 0.51 when a medical expert segments the
non-contrasted ct. in addition, vascular labels can also be transferred to
provide additional information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"to address this challenge, two main strategies could be considered: image
fusion and image processing techniques. image fusion typically relies on the
estimation of rigid or non-rigid transformations between 2 images, to bring into
the intraoperative image structures of interest only visible in the preoperative
data. this process is often described as an optimization problem [9,10] which
can be computationally expensive when dealing with non-linear deformations,
making their use in a clinical workﬂow limited."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"2. the neural network takes as input the preoperative vessel map (vm) and the
intraoperative ncct, and outputs the intraoperative vessel map (vm) from which we
extract the deformed vascular tree. finally, the augmented ct is created by fusing the
segmented image and labels with the intraoperative ncct. 294
s. el hadramy et al.
2.1
vessel map extraction
we call vessel map (vm) the region of interest deﬁning the vascular tree in
the ncct."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"in practice, the acquisition
protocols limit the shift between the ncct and cct acquisitions, and only a
few sequential dilation operations are needed to ensure we capture the true vessel
ﬁngerprint in the ncct image. note that the resulting vessel map is not a binary
mask, but a subset of the image limited to the volume covered by the vessels.
2.2
data augmentation
the preoperative mpcect provides a couple of registered ncct and cct
images. this is obviously not suﬃcient for training purposes, as they do not rep-
resent the possible soft tissue deformation that may occur during the procedure."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"the neural network takes as input the preop-
erative vessel map and the intraoperative ncct, and outputs the intraoperative
vessel map. our network learns to ﬁnd the image features (or vessel ﬁngerprint)
present in the vessel map, in a given ncct assuming the knowledge of its geom-
etry, topology, and the distribution of contrast from the preoperative mpcect. the architecture of our network is illustrated in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"3. our neural network uses a four-path encoder-decoder architecture and takes
as input a two-channel image corresponding to the intraoperative ncct image con-
catenated with the preoperative vessel map. the output is the intraoperative vessel
map.
2.4
augmented ct
once the network has been trained on the patient-speciﬁc preoperative data, the
next step is to augment and visualize the intraoperative ncct. this is done in
3 steps:
– the dilatation operations introduced in sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"the augmented vessels are
displayed in green to ensure the clinician is aware this is not a true cct
image (see fig. 5). – it is also possible to add anatomical labels to the intraoperative augmented
ct to further assist the clinician. to achieve this objective, we compute a
graph data structure from the preoperative segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"nodes and edges correspond respectively to vessel tree bifurcations
and branches. we use the graph structure to associate each anatomical label
(manually deﬁned) with a strahler [6] graph ordering. the same process is
applied to the predicted intraoperative segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"4. this ﬁgure illustrates the diﬀerent stages of the pipeline adopted to generate the
vm and show how the vessel tree topology is retrieved from the predicted intraoperative
vm by computing a displacement ﬁeld between the preoperative vm and the predicted
vm. this ﬁeld is applied to the preoperative segmentation to get the intraoperative
one.
3
results and discussion
3.1
dataset and implementation details
to validate our approach, 4 couples of mpcect abdominal porcine images
were acquired from 4 diﬀerent subjects. for a given subject, each couple cor-
responds to a preoperative and an intraoperative mpcect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"these images are
then cropped and down-sampled to 256 × 256 × 256, and the voxels intensities
are scaled between 0 and 255. finally, we extract the vm from each mpcect
sample and apply 3 dilation operations, which demonstrated the best perfor-
mance in terms of prediction accuracy and robustness on our data. [25] and others do not ﬁt
our problem since they do not include the ncct images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"aiming at a patient-
speciﬁc prediction, we only train on a “subject” at a time. for a given subject, we
generate 100 displacement ﬁelds using the data augmentation strategy explained
above with 50 voxels for the control points spacing in the three spatial directions
intraoperative ct augmentation for needle-based liver interventions
297
and a standard deviation of 5 voxels for the normal distributions. the resulting
deformation is applied to the preoperative mpcect and its corresponding
vm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"being a commonly used metric
for segmentation problems, dice aligns the nature of our problem as well as the
clinical impact of our solution. we ha performed tests on 4 diﬀerent (porcine)
data sets. results are reported in table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"an example of a subject intraoperative augmented ct is illustrated in
fig. 5, where the three images correspond respectively to the initial non injected
ct, the augmented ct without and with labels. figure 6 illustrates the results
of our method for subject 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"the middle image shows
the augmented ct with the predicted vessel tree (in green). the rightmost image shows
the augmented image with anatomical labels transferred from the preoperative image
segmentation and labelling. (color ﬁgure online)
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"we show 3 diﬀerent views of the
intraoperative vessel prediction (in orange), the ground truth (in green) and the pre-
operative vessels (in grey). (color ﬁgure online)
qualitative assessment: to further demonstrate the value of our method,
we have asked two clinicians to manually segment the ncct images in the
intraoperative mpcect data. their results (mean and standard deviation) are
reported in table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"3.3
ablation study and additional results
vessel map: we have removed the vm from the network input to demonstrate
its impact on our results. using the data of the subject 1, a u-net was trained
to segment the vessel tree of the intraoperative ncct image. the network only
managed to segment a small portion of the main portal vein branch."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"we also studied the inﬂuence of the diﬀusion kernel applied to
the initial segmentation. we have seen, on our experimental data, that 3 dilation
operations were suﬃcient to compensate for the possible motion between ncct
and cct acquisitions.
comparison with voxelmorph: the problem that we address can be seen
from diﬀerent angles. in particular, we could attempt to solve it by register-
ing the preoperative ncct to the intraoperative one and then applying the
intraoperative ct augmentation for needle-based liver interventions
299
resulting displacement ﬁeld to the known preoperative segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_28.pdf,"however,
state-of-the-art registration methods such as voxelmorph [7] and others do not
necessarily guarantee a diﬀeomorphic [8] displacement ﬁeld that ensures the
continuity of the displacement ﬁeld inside the parenchyma where the intensity
is quite homogeneous on the ncct. to assess this assumption, a voxelmorph1
network was trained on the subject 1 of our porcine data sets. we trained the
network with both mse and smoothness losses during 100 epochs and given a
batch of size 4. results are illustrated below in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"whole slide image (wsi) classiﬁcation remains a challenge
due to their extremely high resolution and the absence of ﬁne-grained
labels. presently, wsi classiﬁcation is usually regarded as a multiple
instance learning (mil) problem when only slide-level labels are avail-
able."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"therefore, existing methods usually train them
separately, or directly skip the training of the embedder. such schemes
hinder the patch embedder’s access to slide-level semantic labels, result-
ing in inconsistency within the entire mil pipeline. to overcome this issue,
we propose a novel framework called iteratively coupled mil (icmil),
which bridges the loss back-propagation process from the bag-level classi-
ﬁer to the patch embedder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"the reﬁned embedder then generates better instance represen-
tations for achieving a more accurate bag-level classiﬁer. by coupling the
patch embedder and bag classiﬁer at a low cost, our proposed framework
enables information exchange between the two modules, beneﬁting the
entire mil classiﬁcation model. we tested our framework on two datasets
using three diﬀerent backbones, and our experimental results demonstrate
consistent performance improvements over state-of-the-art mil methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"the code is available at: https://github.com/dootmaan/icmil. deep
learning
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 45.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. the typical pipeline of traditional mil methods on wsis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"patch-
based classiﬁcation is a common solution to this problem [3,8,24]. it predicts the
slide-level label by ﬁrst predicting the labels of small, tiled patches in a wsi. this approach allows for the direct application of existing image classiﬁcation
models, but requires additional patch-level labeling."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"unfortunately, patch-level
labeling by histopathology experts is expensive and time-consuming. therefore,
many weakly-supervised [8,24] and semi-supervised [3,5] methods have been pro-
posed to generate patch-level pseudo labels at a lower cost. however, the lack
of reliable supervision directly hinders the performance of these methods, and
serious class-imbalance problems could arise, as tumor patches may only account
for a small portion of the entire wsi [12]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"due to the
high computational cost, end-to-end training of the feature extractor and bag
classiﬁer is prohibitive, especially for high-resolution wsis. as a result, many
methods focus solely on improving a(·) or f(·), leaving g(·) untrained on the wsi
dataset (as shown in fig. however, the domain shift between wsi and nat-
ural images may lead to sub-optimal representations, so recently there have been
methods proposed to ﬁne-tune g(·) using self-supervised techniques [4,12,21] or
weakly-supervised techniques [10,13,23] (as shown in fig. 2(c))."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"(d) our pro-
posed icmil which can bridge the loss back-propagation process from f(·) to g(·) by
iteratively coupling them during training.
to address the challenges mentioned above, we propose a novel mil frame-
work called icmil, which can iteratively couple the patch feature embedding pro-
cess with the bag-level classiﬁcation process to enhance the eﬀectiveness of mil
training (as illustrated in fig. 2(d)). [9,16,25], we aim to bridge the loss back-propagation process from f(·) to g(·) to
improve g(·)’s ability to perceive slide-level labels. speciﬁcally, we propose to use
the bag-level classiﬁer f(·) to initialize an instance-level classiﬁer f ′(·), enabling
f(·) to use the category knowledge learned from bag-level features to determine
each instance’s category."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"this framework ﬁne-tunes the patch embedder
based on the bag-level classiﬁer, and the reﬁned embeddings, in turn, help train
a more accurate bag-level classiﬁer. (2) we propose a teacher-student approach
to achieve eﬀective and robust knowledge transfer from the bag-level classiﬁer
f(·) to the instance-level representation embedder g(·). (3) we conduct extensive
experiments on two datasets using three diﬀerent backbones and demonstrate
the eﬀectiveness of our proposed framework."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"[6] pre-trained on imagenet [19] (step 1⃝ in fig. 3). subsequently, this f(·) is
considered as the initialization of a hidden instance classifer f ′(·), generating
pseudo-labels for each instance-level representation. this operation is feasible
when the bag-level representations aggregated by a(·) are in the same hidden
space as the instance representations, and most aggregation methods (e.g., max
pooling, attention-based) satisfy this condition since they essentially make linear
combinations of instance-level representations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"3), of which the detailed implementation is pre-
sented in sect. after this, g(·) is ﬁne-tuned for the speciﬁc wsi dataset,
which allows it to generate improved representations for each instance, thereby
enhancing the performance of f(·). moreover, with a better f(·), we can use the
iterative coupling technique again, resulting in further performance gains and
mitigation to the distribution inconsistencies between instance- and bag-level
embeddings."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"otherwise, a(·) may
lead to larger diﬀerence between the decision boundaries of bag-level classifer f(·)
and instance-level classiﬁer f ′(·), which may cause icmil taking more time to
converge.
therefore, in our experiments, we choose to use the attention-based instance
aggregation method [9] which has been widely used in many of the existing
iteratively coupled multiple instance learning
471
fig. 4. a schematic view of the proposed teacher-student alike model for label propa-
gation from f(·) to g(·) (mainly in step 2⃝), and its position in icmil pipeline. for a bag that contains k instances, attention-based
aggregation method ﬁrstly learns an attention score for each instance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"obviously, h
and hk remains in the same hidden space, satisfying the prerequisite of icmil. 2.3
label propagation from bag classiﬁer to embedder
we propose a novel teacher-student model for accurate and robust label propaga-
tion from f(·) to g(·). the model’s architecture is depicted in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"we then train a student patch embedding network, g′(·), to learn category knowl-
edge from the teacher. for a given patch input x, the teacher generates the
corresponding pseudo label, while the student receives an augmented image x′
and attempts to generate a similar prediction to that of the teacher through a
consistency loss lc. this loss function is deﬁned as:
472
h. wang et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"the overall
loss function for this step is lc + αlw, with α set to 0.5 in our experiments. this dataset consists of a total of 399
wsis, with 159 normal and 111 metastasis wsis for the training set, and the
remaining 129 for test. although patch-level labels are oﬃcially provided in
camelyon16, they were not used in our experiments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"the second dataset is a private hepatocellular carcinoma (hcc) dataset col-
lected from sir run run shaw hospital, hangzhou, china. this dataset com-
prises a total of 1140 valid tumor wsis scanned at 40× magniﬁcation, and the
objective is to identify the severity of each case based on the edmondson-steiner
(es) grading. the ground truth labels are binary classes of low risk and high
risk, which were provided by experienced pathologists.
3.2
implementation details
for camelyon16, we tiled the wsis into 256×256 patches on 20× magniﬁcation
using the oﬃcial code of [25], while for the hcc dataset the patches are 384×384
on 40× magniﬁcation following the pathologists’ advice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"the instance embed-
ding process was the same of [16], which means for each patch, it would be
iteratively coupled multiple instance learning
473
table 1. results of ablation studies on camelyon16 with ab-mil.
(a) ablation study on the icmil iteration times
icmil iterations
0
0.5
1
1.5
2
2.5
3
auc
85.4
88.8
90.0
89.7
90.5
90.4
90.0
f1
78.0
79.4
80.5
80.1
82.0
80.7
81.7
acc
84.5
85.0
86.6
86.0
85.8
86.9
86.6
(b) loss propagation
method
na¨ıve
ours
auc
88.5
90.0
f1
78.8
80.5
acc
83.9
86.6
table 2. comparison with other methods on camelyon16 and hcc datasets, where
† indicates the corresponding camelyon16 results are cited from [25]. best results are
in bold, while the second best ones are underlined."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"from
table 1(a), we can learn that as the number of icmil iteration increases, the
performance will also go up until reaching a stable point. since the number of
instances is very large in wsi datasets, we empirically recommend to choose
to run icmil one iteration for ﬁne-tuning g(·) to achieve the balance between
performance gain and time consumption. from table 1(b), it is shown that our
474
h. wang et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"only one
iteration of icmil is used to achieve the right ﬁgure. teacher-student-based method outperforms the na¨ıve “pseudo label generation”
method for ﬁne-tuning g(·), which demontrates the eﬀectiveness of introducing
the learnable instance-level classiﬁer f ′(·).
comparison with other methods. experimental results are presented in
table 2."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"it proves that a more suitable patch embedding can
greatly enhance the overall mil classiﬁcation framework. when used with the
state-of-the-art mil method dtfd-mil, icmil further increases its perfor-
mance on camelyon16 by 0.5% auc, 2.1% f1, and 1.6% acc.
results on the hcc dataset also proves the eﬀectiveness of icmil, despite
the minor diﬀerence on the relative performance of baseline methods. mean pool-
ing performs better on this dataset due to the large area of tumor in the wsis
(about 60% patches are tumor patches), which mitigates the impact of average
pooling on instances."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"also, the performance diﬀerences among diﬀerent vanilla
mil methods tends to be smaller on this dataset since risk grading is a harder
task than camelyon16. in this situation, the quality of instance representations
plays a crucial role in generating more separable bag-level representations. as a
result, after applying icmil on the mil baselines, these methods all gain great
performance boost on the hcc dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_45.pdf,"4
conclusion
in this work, we propose icmil, a novel framework that iteratively couples
the feature extraction and bag classiﬁcation stages to improve the accuracy of
mil models. icmil leverages the category knowledge in the bag classiﬁer as
pseudo supervision for embedder ﬁne-tuning, bridging the loss propagation from
classiﬁer to embedder. we also design a two-stream model to eﬃciently facilitate
such knowledge transfer in icmil."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"existing methods for interpretability of model predictions
are largely based on technical insights and are not linked to clinical
context. we use the question of predicting response to radiotherapy
in colorectal cancer patients as an exemplar for developing prediction
models that do provide such contextual information and therefore can
eﬀectively support clinical decision making. there is a growing body of
evidence that about 30% of colorectal cancer patients do not respond to
radiotherapy and will need alternative treatment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"a graph neural network is trained to achieve this joint
prediction task, which subsequently provides novel interpretability maps
to aid clinicians in their cancer treatment decision making process. our
model is trained and validated on two private rectal cancer datasets. interpretability
supported by cancer research uk.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_73.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"our methodology proposes a novel and disease relevant approach to a more
interpretable model that eﬀectively supports a diagnostic task. pathologists and
oncologists can use this information to inspect the validity of the prediction result
and interrogate key aspects of the spatial biology that is critical for patient man-
agement. ultimately, this type of information that is not available today will
help to characterise interactions between the tumour and the host tissue and
therefore help to support choice of therapy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"for computational reasons, all images are split into patches of size
256 × 256 pixels. in order to have a common feature set all the way up to the
last layer of the gnn, individual patches should be represented by morphologi-
cal features that are label-agnostic. this last layer of the gnn then splits into
three branches to predict response to radiotherapy, the cms4 subtype classiﬁ-
cation for crc, and epithelial tissue regions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"this way we can guarantee the
common latent features and derivation across branches, maintaining the contex-
tual importance of each branch. the dino framework [4] uses a self-distillation
training approach, using data augmentation to locally crop the patches and train
with a local-global student-teacher approach. [6], representing each
patch with 384 features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"each graph-level prediction
is derived from the corresponding branch node predictions, by applying pooling
and dropout. we train and validate our methods on two retrospective rectal cancer
datasets, grampian and aristotle. both cohorts received standard chemora-
diotherapy of pelvic irradiation (45–50.4gy in 25 fractions over 5 weeks) with
capecitabine 900mg/m2."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"pathological complete response, which we use as a tar-
get outcome here, was derived from histopathological assessment from post-
treatment resections. the cms labels for this data are derived from three diﬀerent transcriptomic
versions (single cohort, combined cohort correcting batch eﬀects and combined
cohort including 2036 cases run with the same platform), in order to gener-
ate robust classiﬁcations. in all cases the cms call was calculated using the
cmsclassiﬁer random forest and single sample predictor [9]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"final cms calls
are based on matching calls between the three transcriptomic versions. despite
our eﬀorts to minimise the noise from rna sequencing, we still expect a certain
level of noise in our ground truth data, which we discuss in the results section. the epithelial labels for each graph node are calculated from epithelial masks
for each wsi."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"we use these masks in our analysis to ﬁlter out background and irrelevant
tissue from the images. grampian and aristotle are used in both training and
validation, with a 70/30% training-validation split, keeping any wsis from a
single patient in the same dataset. we predict complete response to radiother-
apy against all other responses, such as partial response and no response."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"we address this imbalance
in the supplementary materials. there are 365 slides total in our dataset, from
249 patients. 3
experiments
implementation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"[1] with compactness of
20, setting the number of segments for each wsi as half the mean size of the
wsi. prior to ﬁtting the graph model we normalize the node features relative
to the whole dataset. we train our graph model for 30 epochs using adam
joint prediction in colorectal cancer biopsies
763
optimizer with learning rate 0.001 and weight decay 0.0001."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"we evaluate the best validation epoch by ﬁnding the best
mean auc across the three prediction branches. we run the whole pipeline on
four folds with diﬀerent random data splits for training and validation. the code
for this research will be made available upon request."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"each prediction uses an optimised threshold value determined
from the validation set in order to round the output probabilities to a binary prediction. we use weighted metrics due to the class imbalance in our dataset. despite the noise in our reference data used for training, our model
achieves good performance in terms of mean auc scores on all three prediction
branches of our model, predicting complete response to radiotherapy (rt) with
0.819 auc, cms4 with 0.819 auc and epithelial tissue at the node level with
0.760 auc across folds."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"further metrics are provided in table 1. the prediction
performance of the model could be improved by utilising a larger training dataset
and performing more exhaustive parameter searches, however the current per-
formance of the model is suﬃcient to demonstrate the impact of this approach. the predicted response to radiotherapy can now be viewed in the context of dis-
ease biology as captured by cms4."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"we ﬁnd that predicting these outcomes individually in a single branch model,
particularly with response to radiotherapy, can result in slightly higher auc
joint prediction in colorectal cancer biopsies
765
scores, but we consciously make this trade-oﬀ in order to provide better inter-
pretability of the model predictions. the focus of this research is not to achieve
the best possible metrics, but to develop robust methods which can add context
and explanation to clinical black box deep learning model predictions, with the
view to ease clinical translation of such models.
to explore the eﬀects of the noisy cms4 ground truth labels, we remove from
our dataset any wsis classiﬁed as ‘unmatched’ for the cms call, which for the
main results of this paper we deﬁned as ‘not cms4’. removing this data and
rerunning our analysis improved our predictions for cms4 by +0.06 auc, and
reduced our response to radiotherapy and epithelial predictions by −0.02 and
−0.01 respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"the results can be found in the supplementary materials. these small changes indicate that the noise in our data does not degrade the
performance of our classiﬁer, reinforcing it as a robust and accurate model.
4
conclusion
by setting the prediction of response to therapy in context with disease biol-
ogy and spatial organisation of the tissue we are providing a novel approach
for enhancing the interpretablity of complex prediction tasks. these results do
not only enhance the interpretability, they also provide new ways to utilise large
retrospective clinical trial cohorts for which no additional molecular data is avail-
able."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_73.pdf,"by cross-referencing these prediction maps with our prior
understanding of cancer biology, this approach can help to establish trust in the
prediction model and also help to identify potential failure cases. this work relies on access to well annotated clinical trial samples which will
limit our ability to include more data for training and testing. in future, we plan
to use these methods to help better characterise tumour-stromal interactions of
the tissue."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"while deep learning based nuclei
segmentation methods yield excellent results, they rely on a large amount
of annotated images; however, annotating nuclei from histology images
is tedious and time-consuming. to get rid of labeling burden completely,
we propose a label-free approach for nuclei segmentation, motivated
from one pronounced yet omitted property that characterizes histology
images and nuclei: intra-image self similarity (iiss), that is, within
an image, nuclei are similar in their shapes and appearances. first, we
leverage traditional machine learning and image processing techniques
to generate a pseudo segmentation map, whose connected components
form candidate nuclei, both positive or negative."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"then,
we ﬁlter the candidates based on a custom-designed index that roughly
measures if a candidate contains multiple nuclei. the remaining candi-
dates are used as pseudo labels, which we use to train a u-net to dis-
cover the hierarchical features distinguish nuclei pixels from background. finally, we apply the learned u-net to produce ﬁnal nuclei segmenta-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"we validate the proposed method on the public dataset monuseg. experimental results demonstrate the eﬀectiveness of our design and,
to the best of our knowledge, it achieves the state-of-the-art per-
formances of label-free segmentation on the benchmark monuseg
dataset with a mean dice score of 79.2%. keywords: label-free · nuclei segmentation · pseudo label
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"(color ﬁgure
online)
1
introduction
nuclei segmentation is a fundamental step in histology image analysis. in recent
advances, with a large amount of labeled data, fully-supervised learning meth-
ods can easily achieve reasonable results [1–5]. however, accurate pixel-level
annotation of nuclei is not always accessible for segmentation labeling is a labor-
intensive and time-consuming procedure."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"methods to relieve the high depen-
dency on the accurate annotations of nuclei are highly needed. unsupervised learning (ul) methods achieved great success in the data
dependency problem for nuclei segmentation, which learns from the structural
properties in the data without any manual annotations. based on the character
of these methods, we can group them into two categories: the traditional ul
methods and the deep learning ul methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"moreover, due to the heavily
rely on preset parameters, these traditional methods also show weak robustness. therefore, some researchers [11–15] resort to deep ul segmentation models to
better utilize both pixel value and shape information and develop a robust app-
roach. the common and eﬀective way is to employ image clustering by maximiz-
ing mutual information between image and predicted labels to distinguish fore-
ground and background regions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"ji et al. [12] propose the invariant information
clustering. while reasonable results are obtained, these deep clustering-based
label-free nuclei segmentation using intra-image self similarity
675
methods still suﬀer diﬃculties: (i) poor segmentation of the regions between
adjacent nuclei."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"deep clustering-based methods experience diﬃculties in dealing with
these regions due to the lack of supervision. (ii) underutilization of intra-image
self similarity (iiss) information. as shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"1, in terms of value, shape
and texture, nuclei show a similar appearance within the same image but vary
greatly among diﬀerent images1. this phenomenon oﬀers valuable information
for networks to use but the current clustering models do not take this into
account. to address the above issues and motivated by the iiss property, we hereby
propose a novel self-similarity-driven segmentation network (ssimnet) for
unsupervised nuclei segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"as shown in fig. 2, instead of designing com-
plex discriminative network architectures, our framework derives knowledge from
the iiss property to aid the segmentation. speciﬁcally, we obtain candidate
nuclei with some unsupervised image processing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"hence, we ﬁl-
ter the candidates based on a custom-designed index that roughly measures if a
candidate contains multiple nuclei. the remaining candidates are used as pseudo
labels, which we use to train a u-net (aka ssimnet) to discover the hierarchical
features that distinguish nuclei pixels from the background. finally, we apply
the learned ssimnet to produce the ﬁnal nuclei segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"to validate the eﬀectiveness of our method, we conduct extensive experiments
on the monuseg dataset [16,17] based on ten existing unsupervised segmentation
methods [9,11–15,18–20]. our method outperforms all comparison methods with
an average dice score of 0.792 and aggregated jaccard index of 0.498 on the
monuseg dataset which is close to the supervised method. 2
method
as shown in fig. 2, our ssimnet aims at unsupervised segmentation of nuclei
from histology images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"speciﬁcally, by using a matrix factorization on hema-
toxylin and eosin (h&e) stained histology images, we get the hematoxylin chan-
nel image for clustering, active contour reﬁning and softening to generate the
ﬁnal soft candidate label. then according to the designed unsupervised evalua-
tion metric driven from the iiss property, an ssimnet is trained with highly-
rated soft pseudo labels and corresponding original patches. last, while testing
on the test image, to adapt the network to learn nucleus similarity within the
same image, we ﬁne tune the network with soft pseudo labels of some patches
in current test images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"we transform it into cielab color space
and invoke the fuzzy c-means method (fcm) with 2 clusters to obtain the
candidate foreground pixels. to reduce the noise in clustering results, we use
active contour method as a smoothing operation to get hard candidate labels:
p = {pi}n
i=1 = {activecontour(fcm(it
i ))}n
i=1
(3)
label-free nuclei segmentation using intra-image self similarity
677
label smoothing. + 1] = b[k] + p[k]
2 · a
(4)
where k represents the kth epoch erosion of the connected component, b[k] is
the conﬁdence score of pixels eroded in the kth epoch and b[0] = 0.5 as the
initial condition, p[k] means number of pixels eroded in the kth epoch, and a
is the area of the connected component."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"as a terminal condition, we set the
termination of erosion when b[k] > 0.975. (4), we obtain our soft
candidate labels ˜p from p.
2.2
data puriﬁcation and ssimnet learning
so far, soft candidate labels ˜pi have been acquired for each image is
i . however, it
is common that adjacent nuclei are merged into one candidate due to imperfect
staining and imaging conditions, which violate the iiss property."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"we sample k patches with overlap from original image
is
i . the sampled results are expressed as patch tissue x = {xi}n·k
i=1 and patch
label y = {yi}n·k
i=1 . we design the unsupervised shape measure index (usmi)
and calculate it using the algorithm in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"based on thresholding the
usmi, we obtain pairs (yi, ui)n·k
i=1 . note that the smaller usmi is, the more the
pseudo label conforms to prior knowledge. sorting these pairs by usmi from
the smallest to largest, only maintain the ﬁrst α%(0 < α < 100) of data pairs
as ( ˜
x = {xu(i)}α·n·k
i=1
, ˜y = {yu(i)}α·n·k
i=1
)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"figure 3(right) shows a separation
of candidates into two groups (yellow and blue) with a typical yellow patch
containing merger nuclei and a blue patch containing isolated nuclei. to further separate possible adjacent nuclei in a blue patch, we follow [23]
to construct the voronoi label as in fig. 2 by setting the center of connected
component as 1, constructing voronoi diagram, setting voronoi edge as 0, and
ignoring other pixels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"(1 − λ)lce(f(˜x), ˜z),
(5)
where lbce is the binary cross-entropy loss and lce is the cross-entropy loss. also, we can obtain tissue patches and corresponding pseudo labels for each
image in the test set termed as setk = ( ˜
x t
k , ˜yt
k , ˜zt
k ). before evaluation, we
ﬁrst ﬁne tune our network f using setk for several epochs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"as shown in the
ablation study, this operation is simple but eﬀective. and this ﬁne tuning process
can help the network capture the size and shape information in the current test
slice. 678
l. chen et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"yellow (or
cyan) points denote the samples whose usmi is greater (or less) than the threshold. (color ﬁgure online)
3
experiments
3.1
datasets and settings
monuseg. multi-organ nuclei segmentation [16,17] (monuseg) is used to
evaluated our ssimnet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"all the
methods were trained for 150 epochs on monuseg and 200 epochs on cpm17
each time and experimented with an initial learning rate of 5e−5 and a decay
of 0.98 per epoch. our experiment repeated ten times on monuseg dataset and
only once on cpm17 dataset for an augmented convenience. specially for our
ssimnet training, we set α = 70% for data puriﬁcation and λ = 0.9 for loss
in training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"moreover, we ﬁne tune the network with only ﬁve epochs for each
image on test set with optimizer parameter saved in checkpoint.
label-free nuclei segmentation using intra-image self similarity
679
table 1. performance of the nuclei segmentation on monuseg dataset. the best results
are highlighted in bold and the second best underlined."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"secondly, while the
recall of all comparison methods is higher than precision, our ssimnet’s recall
(0.772) is lower than precision (0.820) and also lower than the state-of-the-art
method’s recall (0.834). the reason lies in that our method considers mining as
strong prior knowledge from tissue slice itself, which renders a tighter constraint
on our model, leading the model to predict a lower conﬁdence in the easily-
confused region. moreover, figure 4 shows the visualization of two test slice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"it
also conforms the eﬀectiveness of our method on eliminating the model confusion
in the region between adjacent nuclei and the ability in capturing nuclei shape. besides, we conduct an additional comparison experiment based on cpm17
dataset to demonstrate the generalization of our method. as shown in table 2,
our method again achieves the top performances."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"green,
yellow and red colors refer to the true positive, the false positive and the false negative
predictions. (color ﬁgure online)
table 2. performance of the nuclei segmentation on cpm17 dataset. the best results
are highlighted in bold and the second best underlined."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_65.pdf,"[11]
82.0
65.4
72.1
46.7
our ssimnet w/o ﬁnetune 85.4
80.7
81.2
49.3
our ssimnet
85.8
80.5
81.6
49.8
3.3
ablation study
we perform ablation studies by disabling each component to the ssimnet frame-
work to evaluate their eﬀectiveness. as shown in table 3, each component in our
ssimnet can bring diﬀerent degrees of improvement, which shows that all of the
label softening, data puriﬁcation and ﬁnetuning process are signiﬁcant parts of
our ssimnet and play an indispensable role in achieving superior performance. table 3. ablation study on ssimnet using monuseg dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"vision transformer (vit) models have demonstrated a
breakthrough in a wide range of computer vision tasks. however, com-
pared to the convolutional neural network (cnn) models, it has been
observed that the vit models struggle to capture high-frequency compo-
nents of images, which can limit their ability to detect local textures and
edge information. as abnormalities in human tissue, such as tumors and
lesions, may greatly vary in structure, texture, and shape, high-frequency
information such as texture is crucial for eﬀective semantic segmentation
tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"to address this limitation in vit models, we propose a new tech-
nique, laplacian-former, that enhances the self-attention map by adap-
tively re-calibrating the frequency information in a laplacian pyramid.
more speciﬁcally, our proposed method utilizes a dual attention mech-
anism via eﬃcient attention and frequency attention while the eﬃcient
attention mechanism reduces the complexity of self-attention to linear
while producing the same output, selectively intensifying the contribu-
tion of shape and texture features. furthermore, we introduce a novel
eﬃcient enhancement multi-scale bridge that eﬀectively transfers spatial
information from the encoder to the decoder while preserving the fun-
damental features. we demonstrate the eﬃcacy of laplacian-former on
multi-organ and skin lesion segmentation tasks with +1.87% and +0.76%
dice scores compared to sota approaches, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"our implemen-
tation is publically available at github. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 70.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43898-1_70
laplacian-former
737
keywords: deep learning · texture · segmentation · laplacian
transformer
1
introduction
the recent advancements in transformer-based models have revolutionized the
ﬁeld of natural language processing and have also shown great promise in a wide
range of computer vision tasks [5]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"[22],
the segmentation performance in low-resolution stages was improved. despite
these advances, there remain some limitations in these methods such as com-
putationally ineﬃciency (e.g., transunet model), the requirement of a heavy
cnn backbone (e.g., hiformer), and the lack of consideration for multi-scale
information. these limitations have resulted in less eﬀective network learning
results in the ﬁeld of medical image segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"[12] besides exploring the eﬃcient transformer [25] counterpart to diminish
the parameter overﬂow of vision transformers, applies a non-invertible down-
sampling operation on input blocks transformer to reduce the parameters. former [24] is a pure transformer-based pipeline that comprises a double atten-
tion module to capture locally ﬁne-grained attention and interaction with dif-
ferent units in a dilated manner through its mechanism.
drawbacks of transformers: recent research has revealed that traditional
self-attention mechanisms, while eﬀective in addressing local feature discrepan-
cies, have a tendency to overlook important high-frequency information such
as texture and edge details [21]. this is especially problematic for tasks like
tumor detection, cancer-type identiﬁcation through radiomics analysis, as well
as treatment response assessment, where abnormalities often manifest in texture."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"the eﬃcient attention mech-
anism reduces the complexity of self-attention to linear while producing the same
output. the frequency attention mechanism is modeled using a laplacian pyra-
mid to emphasize each frequency information’s contribution selectively. then,
a parametric frequency attention fusion strategy to balance the importance of
shape and texture features by recalibrating the frequency features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"these two
attention mechanisms work in parallel. ➋ we also introduce a novel eﬃcient
enhancement multi-scale bridge that eﬀectively transfers spatial information
from the encoder to the decoder while preserving the fundamental features. ➌ our method not only alleviates the problem of the traditional self-attention
mechanism mentioned above, but also it surpasses all its counterparts in terms
of diﬀerent evaluation metrics for the tasks of medical image segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"2.1
eﬃcient enhancement transformer block
in medical imaging, it is important to distinguish diﬀerent structures and tis-
sues, especially when tissue boundaries are ill-deﬁned. this is often the case for
accurate segmentation of small abnormalities, where high-frequency information
plays a critical role in deﬁning boundaries by capturing both textures and edges.
inspired by this, we propose an eﬃcient enhancement transformer block that
incorporates an eﬃcient frequency attention (ef-att) mechanism to capture
contextual information of an image while recalibrating the representation space
within an attention mechanism and recovering high-frequency details. our eﬃcient enhancement transformer block ﬁrst takes a layernorm (ln)
from the input x."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"2.
2.2
eﬃcient frequency attention (ef-att)
the traditional self-attention block computes the attention score s using query
(q) and key (k) values, normalizes the result using softmax, and then multiplies
the normalized attention map with value (v):
s(q, k, v) = softmax

qkt
√dk

v,
(1)
where dk is the embedding dimension. one of the main limitations of the dot-
product mechanism is that it generates redundant information, resulting in
unnecessary computational complexity. shen et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"wang et al. [21] explored another major limitation of the self-attention mech-
anism, where they demonstrated through theoretical analysis that self-attention
operates as a low-pass ﬁlter that erases high-frequency information, leading to a
loss of feature expressiveness in the model’s deep layers. authors found that the
softmax operation causes self-attention to keep low-frequency information and
loses its ﬁne details."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"the process begins by extracting (l + 1) gaussian representations from
the encoded feature using diﬀerent variance values of the gaussian function:
gl(x) = x ∗
1
σl
√
2π e
− i2+j2
2σ2
l ,
(3)
where x refers to the input feature map, (i, j) corresponds to the spatial loca-
tion within the encoded feature map, the variable σl denotes the variance of
the gaussian function for the l-th scale, and the symbol ∗ represents the con-
volution operator. the pyramid is then built by subtracting the l-th gaussian
function (gl) output from the (l + 1)-th output (gl − gl+1) to encode fre-
quency information at diﬀerent scales. the laplacian pyramid is composed of
multiple levels, each level containing distinct types of information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"hence, we present frequency attention that involves multiplying the key
and value of each level (xl) to calculate the attention score and then fuses the
resulting attention scores of all levels using a fusion module, which performs
summation. the resulting attention score is multiplied by query (q) to obtain
the ﬁnal frequency attention result, which subsequently concatenates with the
eﬃcient attention result and applies the depth-wise convolution with the kernel
size of 2×1×1 in order to aggregate both information and recalibrate the feature
map, thus allowing for the retrieval of high-frequency information. thus, we introduce the eﬃcient enhancement
multi-scale bridge as an alternative to simply concatenating the features from
the encoder and decoder layers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"the proposed bridge, depicted in fig. 1, deliv-
ers spatial information to each decoder layer, enabling the recovery of intricate
details while generating output segmentation masks. in this approach, we aim
742
r. azad et al.
to calculate the eﬃcient attention mechanism for each level and fuse the multi-
scale information in their context; thus, it is important that all levels’ embedding
dimension is of the same size."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"each ct scan consists of 85 ∼ 198 slices of the in-plane size of 512 × 512
and has annotations for eight diﬀerent organs. we followed the same prefer-
ences for data preparation analogous to [5]. table 1. comparison results of the proposed method on the synapse dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"blue
indicates the best result, and red indicates the second-best. 3. segmentation results of the proposed method on the synapse dataset. our
laplacian-former shows ﬁner boundaries (high-frequency details) for the region of the
stomach and less false positive prediction for the pancreas."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_70.pdf,"skin lesion segmentation: table 2a shows the comparison results of our pro-
posed method, laplacian-former, against leading methods on the skin lesion seg-
mentation benchmark. our approach outperforms other competitors across most
evaluation metrics, indicating its excellent generalization ability across diﬀerent
datasets. [15] and pure transformer-based methods such as swin-unet [4]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"in medical image analysis, imbalanced noisy dataset classiﬁ-
cation poses a long-standing and critical problem since clinical large-
scale datasets often attain noisy labels and imbalanced distributions
through annotation and collection. current approaches addressing noisy
labels and long-tailed distributions separately may negatively impact
real-world practices."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"additionally, the factor of class hardness hinder-
ing label noise removal remains undiscovered, causing a critical neces-
sity for an approach to enhance the classiﬁcation performance of noisy
imbalanced medical datasets with various class hardness. to address this
paradox, we propose a robust classiﬁer that trains on a multi-stage noise
removal framework, which jointly rectiﬁes the adverse eﬀects of label
noise, imbalanced distribution, and class hardness. the proposed noise
removal framework consists of multiple phases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"multi-environment risk
minimization (mer) strategy captures data-to-label causal features for
noise identiﬁcation, and the rescaling class-aware gaussian mixture
modeling (rcgm) learns class-invariant detection mappings for noise
removal. extensive experiments on two imbalanced noisy clinical datasets
demonstrate the capability and potential of our method for boosting the
performance of medical image classiﬁcation. keywords: imbalanced data · noisy labels · medical image analysis
1
introduction
image classiﬁcation is a signiﬁcant challenge in medical image analysis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"https://doi.org/10.1007/978-3-031-43987-2_30
learning robust medical image classiﬁer by minimizing invariant risk
307
fig. (a) and (b) are conﬁdence distributions of clean and noisy
data on the majority class and the minority class, respectively (c) is the relationship
between class rate and f1 score among diﬀerent classes. time-consuming and expensive."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"besides, pruning clean and balanced datasets
require a large amount of crucial clinical data, which is insuﬃcient for large-scale
deep learning. therefore, we focus on a more practical yet unexplored setting for
handling imbalanced medical data with noisy labels, utilizing all available low-
cost data with possible noisy annotations. noisy imbalanced datasets arise due
to the lack of high-quality annotations [11] and skewed data distributions [18]
where the number of instances largely varies across diﬀerent classes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"besides,
the class hardness problem where classiﬁcation diﬃculties vary for diﬀerent cat-
egories presents another challenge in removing label noise. due to diﬀerences
in disease epidemicity and collection diﬃculty, rare anomalies or anatomical
features render diseases with low epidemicity easier to detect. [12,23,24] fail to jointly address these scenarios, leading to inadequate
classiﬁcation outcomes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"existing approaches for non-ideal medical image classiﬁcation can be sum-
marized into noisy classiﬁcation, imbalanced recognition, and noisy imbalanced
identiﬁcation. noisy classiﬁcation approaches [3,7,23] conduct noise-invariant
learning depending on the big-loss hypothesis, where classiﬁers trained with
clean data with lower empirical loss aid with de-noising identiﬁcation. however,
imbalanced data creates diﬀerent conﬁdence distributions of clean and noisy data
in the majority class and minority class as shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"1, which invalidates the
big-loss assumption [3,4]. imbalanced recognition approaches [9,15,21] utilize
augmented embeddings and imbalance-invariant training loss to re-balance the
long-tailed medical data artiﬁcially, but the disturbance from noisy labels leads
to uncasual feature learning, impeding the recognition of tail classes. noisy long-
tailed identiﬁcation technique [25] has achieved promising results by addressing
noise and imbalance concerns sequentially."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"the main contributions of our work include: 1) we decom-
pose the negative eﬀects in practical medical image classiﬁcation, 2) we minimize
308
j. li et al. the invariant risk to tackle noise identiﬁcation inﬂuenced by multiple factors,
enabling the classiﬁer to learn causal features and be distribution-invariant, 3)
a re-scaling class-aware gaussian mixture modeling (cgmm) approach is pro-
posed to distinguish noise labels under various class hardness, 4) we evaluate our
method on two medical image datasets, and conduct thorough ablation studies
to demonstrate our approach’s eﬀectiveness. 2
method
2.1
problem formulation
in the noisy imbalanced classiﬁcation setting, we denote a medical dataset as
{(xi, yi)}n
i=1 where yi is the corresponding label of data xi and n is the total
amount of instances."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"here yi may be noisy. further, we split the dataset accord-
ing to class categories. then, we have {dj}m
j=1, where m is the number of classes;
dj denotes the subset for class j. in each subset containing nj samples, the
data pairs are expressed as {(xj
i, yj
i )}nj
i=1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"furthermore, the
impact of hardness eﬀects has not been considered in previous studies, which
adds an extra dimension to noise removal. in essence, the fundamental idea
of noisy classiﬁcation involves utilizing clean data for classiﬁer training, which
determines the importance of noise identiﬁcation and removal. to address these
issues, we propose a mapping correction approach that combines independent
noise detection and removal techniques to identify and remove noise eﬀectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"following [25], we minimize the invariant risk [2] across multi-environment
for independent detector learning. by assuming that the robust classiﬁer per-
forms well on every data distribution, we solve the optimizing object by ﬁnding
the optima to reduce the averaged distance for gradient shift:
310
j. li et al.
min
hθ:x→z
fφ:z→y

ε∈etr
l(fφ ◦ hθ)
s.t. ∈ etr,
(3)
where ε represents an environment (distribution) for classiﬁer fφ and backbone
hθ; and l denotes the empirical loss for classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"since the incorrect map-
ping is not caused by feature representation, the backbone hθ is ﬁxed during
the optimization. (4)
ideally, the noise removal process is distribution-invariant if data is uniformly
distributed w.r.t. classes. by the law of large numbers, all constructed distribu-
tions should be symmetric according to the balanced distribution to obtain a
uniform expectation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"in practice, all environments
are established from the training set with the same class categories. 2.4
rescaling class-aware gaussian mixture
existing noise labels learning methods [1,13] cluster all sample loss or conﬁ-
dence scores with beta mixture model or gaussian mixture model into noisy
and clean distributions. from the perspective of clustering, deﬁnite and immense
gaps between two congregate groups contribute to more accurate decisions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"how-
ever, in medical image analysis, an overlooked mismatch exists between class
hardness and diﬃculty in noise identiﬁcation. this results in ineﬀectiveness of
global cluster methods in detecting label noises across all categories. to resolve
the challenge, we propose a novel method called rescaling class-aware gaussian
mixture modeling (rcgm) which clusters each category data independently
by ﬁtting conﬁdence scores qij from ith class into two gaussian distributions
as pn
i (xn|μn, σn) and pc
i(xc|μc, σc)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"=

k∈{c,n}
αikpk
i

qij | μk
i , σk
i

,
(5)
which produces more accurate and independent measurements of label quality. rather than relying on the assumption that conﬁdence distributions of training
samples depend solely on their label quality, rcgm solves the eﬀect of class
hardness in noisy detection by individually clustering the scores in each category. this overcomes the limitations of global clustering methods and signiﬁcantly
enhances the accuracy of noise identiﬁcation even when class hardness varies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"learning robust medical image classiﬁer by minimizing invariant risk
311
instead of assigning a hard label to the potential noisy data as [8] which also
employs a class-speciﬁc gmm to cluster the uncertainty, we further re-scale the
conﬁdence score of class-wise noisy data. let xij be the jth in class i, then its
probability of having a clean label is:
γij = αikpc
i (qij | μc
i, σc
i )
pm
i (qij)
,
(6)
which is then multiplied by a hyperparameter s if the instance is predicted as
noise to reduce its weight in the ﬁnetuning. τ
(7)
2.5
overall learning framework for imbalanced and noisy data
in contrast to two-stage noise removal and imbalance classiﬁcation techniques,
our approach applies a multi-stage protocol: warm-up phases, noise removal
phases, and ﬁne-tuning phases as shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"a few epochs by assuming that g only remembers
clean images with less empirical loss. in the noise removal phases, we learn class-
invariant probability distributions of noisy-label eﬀect with mer and remove
class hardness impact with rcgm. finally, in the ﬁne-tuning phases, we apply
mixup technique [13,25,26] to rebuild a hybrid distribution from noisy pairs
and clean pairs by:
ˆxkl := αklxk + (1 − αkl)xl,
∀xk, xl ∈ d
ˆykl := αklyk + (1 − αkl)yl,
∀yk, yl ∈ d
(8)
where αkl := v(xk)
v(xl) denotes the balanced scale; and {(ˆxkl, ˆykl)} are the mixed
clean data for classiﬁer ﬁne-tuning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"[12] and ce loss are the ﬁne-tuning loss functions. 3
experiment
3.1
dataset and evaluation metric
we evaluated our approach on two medical image datasets with imbalanced class
distributions and noisy labels. [22], is a dermato-
scopic image dataset for skin-lesion classiﬁcation with 10,015 images divided into
seven categories."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"it contains a training set with 7,007 images, a validation set
with 1,003 images, and a testing set with 2,005 images. following the previous
noisy label settings [25], we add 20% noise to its training set by randomly ﬂip-
ping labels. [29], is a histopathology image
dataset manually annotated into four cancer categories by three pathological
312
j. li et al.
table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"to emulate imbalanced scenarios, we prune the class sizes of the training
set into an imbalanced distribution as [5]. consequently, chaoyang dataset
consists of a training set with 2,181 images, a validation set with 713 images,
and a testing set with 1,426 images, where the validation and testing sets have
clean labels. the imbalanced ratios [12] of ham10000 and chaoyang are 59
and 20, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"[25], nl+sqrt-rs, gce+sqrt-
rs, gce+focal). we train all approaches under the same data augmenta-
tions and network architecture. table 1 exhibits the overall comparison of all
approaches."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"then, we compare our approach to state-of-the-art meth-
ods of the noisy (dividemix), imbalanced (fcd), and noisy long-tailed (h2e)
methods. our framework achieves improvements in all metrics on both datasets,
demonstrating the rationality of the assumption and the eﬀectiveness of our
framework.
fig. 3. ablation analysis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"[12] as our baseline. figure 3a and 3b show that only using mer
or rcgm achieves better performance than our strong baseline on both datasets. for example, mer achieves 5.37% and 1.15% improvements on ham10000 and
chaoyang, respectively, demonstrating the eﬀectiveness of our noise removal
314
j. li et al.
techniques."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"further, our multi-stage noise removal technique outperforms single
mer and rcgm, revealing that the decomposition for noise eﬀect and hard-
ness eﬀect works on noisy imbalanced datasets. we ﬁnd that the combination
of mer and rcgm improves more on chaoyang dataset. this is because
chaoyang has more possible label noise than ham10000 caused by the high
annotating procedure."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_30.pdf,"3c, we observe the accuracy trends are as the
scale increases and achieve the peak around 0.6. it indicates the re-scaling pro-
cess for noise weight deduction contributes to balancing the feature learning and
classiﬁcation boundary disturbance from the mixture of noisy and clean data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf,"we demonstrate excellent accuracy on phantom
imagery. remarkably, the watertight prior combined with illumination
decline, allows to complete the reconstruction of unseen portions of the
surface with acceptable accuracy, paving the way to automatic quality
assessment of cancer screening explorations, measuring the global per-
centage of observed mucosa. early detection is crucial
for a good prognosis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf,"despite the existence of other techniques, such as vir-
tual colonoscopy (vc), optical colonoscopy (oc) remains the gold standard for
colonoscopy screening and the removal of precursor lesions. unfortunately, we do
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 48. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14229, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf,"the light
source is co-located with the camera and close to the surface, which results in
a strong correlation between pixel brightness and distance to the camera. in
this paper, we show that, far from being a handicap, this correlation is a key
information for neural network self-supervision. neus training selects a pixel from an image and samples points along its
projecting ray."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf,"earlier methods [3] have reported similar accu-
racies but only on very few synthetic images and on short sections of the colon. by contrast, we can handle much longer ones and provide a broad evaluation in
a real dataset (c3vd) over multiple sequences. this makes us the ﬁrst to show
accurate results of extended 3d watertight surfaces from monocular endoscopy
images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf,"in addition, they propose a positional encoding for loca-
tion x and direction d, which allows high-frequency details in the reconstruction. neural implicit surfaces (neus) were introduced in [25] to improve
the quality of nerf representation modelling watertight surfaces. 0

where φs(x) =
1
1 + e−sx
(2)
the sdf formulation makes it possible to estimate the surface normal as
n = ∇f(x)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf,"furthermore, it is close to the surfaces to be modeled. cos (θ) g
1/γ
(3)
where le is the radiance emitted by the light source to the surface point, that
was modeled and calibrated in the endomapper dataset [1] according to the
sls model from [16]. the bidirectional reﬂectance distribution function (brdf)
determines how much light is reﬂected to the camera, and the cosine term
cos (θ) = −d · n weights the incoming radiance with respect to the surface
normal n. equation (3) also takes into account the camera gain g and gamma
correction γ.
3.1
using illumination decline as a depth cue
the neus formulation of sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf,"an
adaptive gain factor g is applied by the endoscope’s internal logic and gamma
correction is also used to adapt to non-linear human vision, achieving better
contrast perception in mid tones and dark areas. endoscope manufacturers know
the post-processing logic of their devices, but this information is proprietary and
not available to users. again, gamma correction can be calibrated assuming it is
constant [3], and the gain change between successive images can be estimated,
for example, by sparse feature matching."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf,"all these factors must be taken into account during network training. thus,
our photometric loss is computed using a normalized image:
i′ =
 iγ
leg
1/γ
(5)
4
experiments
we validate our method on the c3vd dataset [4], which covers all diﬀerent sec-
tions of the colon anatomy in 22 video sequences. this dataset contains sequences
recorded with a medical video colonoscope, olympus evis exera iii cf-hq190l."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf,"in an operational setting, we could use a structure-from-motion approach such
as colmap [21] or a slam technique such as [8], which have been shown to
work well in endoscopic settings. the gain values were easily estimated from the
dataset itself. for vignetting, we use the calibration obtained from a colonoscope
of the same brand and series from the endomapper dataset [1].
during training, we follow the neus paper approach of using a few informa-
tive frames per scene, as separated as possible, by sampling each video uniformly."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_48.pdf,"once the network is trained, we can extract
triangulated meshes from the reconstruction. since the c3vd dataset comprises
a ground-truth triangle mesh, we compute point-to-triangle distances from all
the vertices in the reconstruction to the closest ground-truth triangle. in the ﬁrst rows of table 1, we report median (medae), mean (mae), and
root mean square (rmse) values of these distances for all vertices seen in at
least one image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"it uses a mask transformer to jointly segment
and classify each lesion with improved anchor queries and a foreground-
enhanced sampling loss. it also has an image-wise classiﬁer to eﬀectively
aggregate global information and predict patient-level diagnosis. a large-
scale multi-phase dataset is collected containing 939 tumor patients and
810 normal subjects."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"early detection and accurate diagnosis of liver tumors may improve overall
partially supported by the national natural science foundation of china (grant
82071885), basic research projects of liaoning provincial department of education
(ljkmz20221160), the national youth talent support program of china, and science
and technology innovation talent project in shenyang (rc210265). supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 8.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43904-9_8
liver tumor screening and diagnosis in ct
73
patient outcomes, in which imaging plays a key role [11]. computed tomog-
raphy (ct) is one of the most important imaging modalities for liver tumors."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"such an approach will be helpful to discover asymptomatic
incidental tumors [12] from routine nc ct scans indicated for general diagnos-
tic purposes at no additional cost and radiation exposure. after an incidental
tumor is found, the patient may undergo further imaging examination such as
a multi-phase dce ct for diﬀerential diagnosis [11], which can provide useful
discriminative information such as the vascularity of lesions and the pattern of
contrast agent enhancement [19]. liver is largest solid organ in body and is the
site of many tumor types [11]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"many researchers have developed algorithms to automatically segment [1,9,
13,15,23] or classify [19,21,25] liver tumors in ct to help radiologists improve
their accuracy and eﬃciency. [9,13] and lesion edge information [15]. lits only has single-phase cts
(venous phase)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"such models may generate false positives
in real-world screening scenario when facing diverse tumor-free images. we col-
lect a large-scale dataset with both tumor and non-tumor subjects, where the
non-tumor subjects includes not only healthy ones, but also patients with various
diﬀuse liver diseases such as steatosis and hepatitis to improve the robustness of
the algorithm. meanwhile, we learn tumor segmentation and classiﬁ-
cation with one network using an instance segmentation framework [3]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"inspired by them, we propose a novel end-to-end framework
named pixel-lesion-patient network (plan) for lesion segmentation and classi-
ﬁcation, as well as patient classiﬁcation. it contains three branches with bottom-
up cooperation: the segmentation map from the pixel branch helps to initialize
the lesion branch, which is an improved mask transformer aiming to segment and
classify each lesion; the patient branch aggregates information from the whole
image and predicts image-level labels of each lesion type, with regularization
terms to encourage consistency with the lesion branch. we collected a large-scale multi-phase dataset containing 810 non-tumor sub-
jects and 939 tumor patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"our codes will be made public upon institutional approval. [3,4,17,22]. diﬀerent from traditional fully-
convolutional segmentators [8] that predict a class label for each pixel, mask
transformers predict a class label and a binary mask for each object. [3] as an example."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"[10].
lesion branch and foreground-enhanced sampling loss. similar to
mask2former, the lesion branch predicts a binary mask and a class label for
each query, see fig. 1. mask2former calculates its segmentation loss on k sam-
pled pixels instead of on the whole image, which is shown to both improve
accuracy and reduce gpu memory usage [3]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"for example,
diagnosing the subject as normal, benign, or malignant will result in completely
diﬀerent treatments [24]. intuitively, we can also infer patient-level labels from
segmentation results by checking if there is any lesion in the predicted mask. however, certain tumors are often related to signs outside the tumor, e.g. hepa-
tocellular carcinoma and cirrhosis, cholangiocarcinoma and bile duct dilatation,
etc."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"we equip plan with a dedicated patient branch to aggregate such global
information to make better patient-level prediction. since one patient can have
multiple liver tumors of diﬀerent types, in our problem, we give each image
several hierarchical binary labels. labels suggest the existence of c ﬁne-grained types of tumors."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"then,
we compute the l2 loss between them: lconsist = ∥˜p − ˜c∥2. the overall loss of plan is listed in eq. 1, where lpixel is the combined cross-
entropy (ce) and dice loss for the pixel branch as in nnu-net [8]; llesion-class
is the ce loss [3] for lesion classiﬁcation in the lesion branch; llesion-mask is
the combined ce and dice loss [3] for binary lesion segmentation in the lesion
branch with the foreground-enhanced sampling strategy; lpatient is the binary
ce loss for the multi-label classiﬁcation task in the patient branch. l = λ1lpixel + λ2cllesion-class + λ2mllesion-mask + λ3lpatient + λ4lconsist."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"(1)
3
experiments
data. our dataset contains 810 normal subjects and 939 patients with liver
tumors. each normal subject has a non-contrast (nc) ct, while each patient
has a dynamic contrast-enhanced (dce) ct scan with nc, arterial, and venous
phases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"we train two separate networks for nc and dce cts. in the former setting,
both normal and patient data are used and randomly split into 1149 training,
100 validation, and 500 testing. in the latter one, only patient data are used
with 641 training, 100 validation, and 200 testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"each ct is resampled to 0.7×0.7×5mm in spacing. we ﬁrst train an nnu-net on public datasets to segment liver and surround-
ing organs (gallbladder, hepatic vein, spleen, stomach, and pancreas), and then
crop the liver region to train plan. 78
k. yan et al.
table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"[3],
is also adapted to 3d for comparison. for the baselines, patient-level labels are
inferred from their predicted masks by counting lesion pixels. as displayed in
table 1, plan achieves the best accuracy on all tasks, especially in nc pre-
liminary diagnosis tasks, which demonstrates the eﬀectiveness of its dedicated
patient branch that can explicitly aggregate features from the whole image.
lesion and pixel-level results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_8.pdf,"in
summary, our results are superior or comparable to existing works. table 4. ablation study on nc data. fes loss: foreground enhanced sampling loss."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"we perform unsupervised disentan-
glement of latent clinical signatures and leverage time-distance scaled
self-attention to jointly learn from clinical signatures expressions and
chest computed tomography (ct) scans. our classiﬁer is pretrained on
2,668 scans from a public dataset and 1,149 subjects with longitudinal
chest cts, billing codes, medications, and laboratory tests from ehrs
of our home institution. evaluation on 227 subjects with challenging
spns revealed a signiﬁcant auc improvement over a longitudinal mul-
timodal baseline (0.824 vs 0.752 auc), as well as improvements over
a single cross-section multimodal scenario (0.809 auc) and a longitu-
dinal imaging-only scenario (0.741 auc)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"taken together, these ﬁndings suggest that
learning across both time and multiple modalities is important in biomedical
predictive modeling, especially spn diagnosis. however, such an approach that
scales across longitudinal multimodal data from comprehensive representations
of the clinical routine has yet to be demonstrated [24].
related work. directly learning from routinely collected electronic health
records (ehrs) is challenging because observations within and between modali-
ties can be sparse and irregularly sampled."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"due to the importance of time
dynamics in spn classiﬁcation, we use the time interval between samples to
scale self-attention with the intuition that recent observations are more impor-
tant to attend to than older observations. compared with imaging-only and a
baseline that aggregates longitudinal data into bins, our approach allowed us to
incorporate additional modalities from routinely collected ehrs, which led to
improved spn classiﬁcation. 2
methods
latent clinical signatures via probabilistic independence."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"we transformed each
variable to a longitudinal curve at daily resolution, estimating the variable’s
instantaneous value for each day [18]. we used smooth interpolation for contin-
uous variables [4] or a continuous estimate of event density per time for event
data. previous work used gaussian process inference to compute both types
of curves [16,17]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"we use an ica model to estimate a linear decomposition of the observed
curves from the ehr-pulmonary cohort to independent latent sources, or clinical
signatures. formally, we have dataset dehr-pulmonary = {lk | k = 1, . . . , n} with
longitudinal curves denoted as lk = {li|i = 1, . ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"we
set t = 3 and added a ﬁxed padding embedding to represent missing items in
the sequence. embeddings that incorporate positional and segment information
are computed for each item in the sequence (fig. 1, right). patches pro-
posed by a pretrained spn detection model
[21]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"we will refer to this approach as tdsig. following [5,19,32], we intuit that if medical
data is sampled as a cross-sectional manifestation of a continuously progressing
longitudinal multimodal transformer integrating imaging
653
phenotype, we can use a temporal emphasis model (tem) emphasize the impor-
tance of recent observations over older ones. additionally, self-attention is masked
for padded embeddings, allowing our approach to scale with varying sequence
lengths across subjects."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"finally, we implemented single cross-sectional
versions of tdimage, tdcode2vec, and tdsig, csimage, cscode2vec, and
cssig respectively, using the scan date closest to the lung malignancy diag-
nosis for cases or spn date for controls. all baselines except csimage, which
654
t. z. li et al.
table 1. breakdown of modalities, size, and longitudinality of each dataset. icd billing codes,
med: medications, lab: laboratory tests.
employed a multi-layer perceptron directly after the convolutional embedding,
used the same architecture and time-distance self-attention as tdsig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"the trans-
former encoders in this study were standardized to 4 heads, 4 blocks, input token
size of 320, multi-layer perception size of 124, self-attention weights of size 64. this work was supported by pytorch 1.13.1, cuda 11.7.
3
experimental setup
datasets. this study used an imaging-only cohort from the nlst [28] and
three multimodal cohorts from our home institution with irb approval (table 1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"we randomly
sampled from the control group to obtain a 4:6 case control ratio. next, ehr-
pulmonary was the unlabeled dataset used to learn clinical signatures in an
unsupervised manner. we searched all records in our ehr archives for patients
who had billing codes from a broad set of pulmonary conditions, intending to
capture pulmonary conditions beyond just malignancy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"finally, image-ehr-spn was a
subset of image-ehr with the inclusion criteria that subjects had a billing code
for an spn and no cancer of any type prior to the spn. we labeled malignant
cases as those with a lung malignancy billing code occurring within three years
after any scan and only used data collected before the lung malignancy code. all
data within the ﬁve-year period were used for controls."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"clinical signatures over a binned embedding strategy. cross-sectional embed-
ded billing codes did not signiﬁcantly improve performance over images alone
(cscode2vec vs csimage, p = 0.56), but adding clinical signatures did (cssig vs
csimage, p < 0.01; tdsig vs tdimage, p < 0.01) and the greatest improvement
in longitudinal data over single cross sections occurred when clinical signatures
were included.
for control subjects, tdsig correctly/incorrectly reclassiﬁed 40/18 from
tdcode2vec, 54/8 from tdimage, 12/18 from cssig, 104/7 from cscode2vec,
and 125/5 from csimage. for case subjects, tdsig correctly/incorrectly reclas-
siﬁed 13/10 from tdcode2vec, 17/8 from tdimage, 12/2 from cssig, 23/16
from cscode2vec, and 29/16 from csimage (fig. 2)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"we evaluated on clinically-billed spns, meaning that
clinicians likely found these lesions diﬃcult enough to conduct a clinical workup. in this setting, we found that adding clinical context increased the performance
gap between longitudinal data and single cross-sections. our clinical signatures
incorporated longitudinality and additional modalities to build a better repre-
sentation of clinical context than binned embeddings."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_62.pdf,"we release our implemen-
tation at https://github.com/masilab/lmsignatures. the lack of longitudinal multimodal datasets has long been a limiting factor
[24] in conducting studies such as ours. one of our contributions is demonstrating
training strategies in a small-dataset, incomplete-data regime."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"low-dose computer tomography (ldct) has been widely
used in medical diagnosis yet suﬀered from spatial resolution loss and
artifacts. numerous methods have been proposed to deal with those
issues, but there still exists drawbacks: (1) convolution without guid-
ance causes essential information not highlighted; (2) features with
ﬁxed-resolution lose the attention to multi-scale information; (3) sin-
gle super-resolution module fails to balance details reconstruction and
noise removal. therefore, we propose an ldct image super-resolution
network consisting of a dual-guidance feature distillation backbone for
elaborate visual feature extraction, and a dual-path content communica-
tion head for artifacts-free and details-clear ct reconstruction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"specif-
ically, the dual-guidance feature distillation backbone is composed of a
dual-guidance fusion module (dgfm) and a sampling attention block
(sab). the dgfm guides the network to concentrate the feature repre-
sentation of the 3d inter-slice information in the region of interest (roi)
by introducing the average ct image and segmentation mask as comple-
ments of the original ldct input. meanwhile, the elaborate sab utilizes
the essential multi-scale features to capture visual information more rel-
ative to edges."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"furthermore, the heads
with the same function share the parameters so as to eﬃciently improve
the reconstruction performance by reducing the amount of parameters. the experiments compared with 6 state-of-the-art methods on 2 public
datasets prove the superiority of our method. the code is made available
at https://github.com/neu-szy/dual-guidance_ldct_sr."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"keywords: low-dose computed tomography · image denoising ·
image super-resolution · deep learning
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14229, pp. [18] and cancer screening [28]. to
balance the high image quality and low radiation damage compared to normal-
dose ct (ndct), numerous algorithms have been proposed for ldct super-
resolution [3,4]. image post-processing super-resolution (sr) methods
could be divided into 3 categories: interpolated-based methods [16,25], model-
based methods [13,14,24,26] and learning-based methods [7–9,17]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"huang et al. [11] introduced a deep alternating network (dan) which
estimated the degradation kernels and corrected those kernels iteratively and
reconstructed results following the inverse process of the estimated degradation.
more recently, aiming at improving the quality of medical images further, huang
et al. [12] ﬁrst composited degradation model proposed for radiographs and pro-
posed attention denoising super-resolution generative adversarial network (aid-
srgan) which could denoise and super-resolve radiographs simultaneously."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"avg ct is the average image among adjacent ct slices
of each patient.
munication. our contributions are as follows: (1) we design a dual-guidance
fusion module (dgfm) which could fuse the 3d ct information and roi guid-
ance by mutual attention to make full use of ct features and reconstruct clearer
textures and sharper edges. (2) we propose a sampling attention block (sab)
which consists of sampling attention module (sam), channel attention module
(cam) and elaborate multi-depth residual connection aiming at the essential
multi-scale features by up-sampling and down-sampling to leverage the features
in ct images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"2
method
2.1
overall architecture
the pipeline of our proposed method is shown in fig. we ﬁrst calculate the
average ct image of adjacent ct slices of each patient to provide the 3d spatial
structure information of ct volume. meanwhile, the roi mask is obtained by
a pre-trained segmentation network to guide the network to concentrate on the
focus area or tissue area."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"dual-guidance feature distillation backbone. to decrease the redundant
computation and make full use of the above-mentioned extra information, we
design a dual-guidance feature distillation backbone consisting of a dual-guidance
fusion module (dgfm) and sampling attention block(sab). firstly, we use a 3 × 3 convolutional layer to extract the shallow features of
the three input images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"mean-
while, prj(·) is the projection function, softmax[·] means the softmax function
and fi are the output features of the i-th dgfm. the dgfm helps the backbone
to focus on the roi and tiny structural information by continuously introducing
additional guidance information. furthermore, to take advantage of the multi-scale information which is essen-
tial for obtaining the response matrix containing the connections between dif-
ferent levels of features, as shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"to deal with this problem, we develop a dual-
path architecture by introducing the shared denoising head into sr task where
the parameters of sr heads and denoising heads in diﬀerent paths are shared
respectively. two paths are designed to process the deep features extracted from
our backbone: (1) the sr path transfers the deep features to those with high-
frequency information and reconstructs the sr result, and (2) the denoising
102
j. chi et al.
path migrates the deep features to those without noise and recovers the clean
result secondly. especially, the parameters of those two paths are shared and
optimized by multiple supervised strategy simultaneously."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"∥igt − ifinal∥1
(3)
where, igt is the ground truth, bi(·) means bicubic interpolation, ∥·∥1 represents
the l1 norm and λ1, λ2 are the weight parameters for adjusting the losses. 3
experiments
3.1
datasets and experiment setup
datasets. [5], are used for both training and testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"we choose 1663 ct images from 16 patients for training, 226
ct images from 2 patients for validation and 185 ct images from 2 patients for
testing. similarly, the pancreas dataset is used for pancreas segmentation
which consists of 19328 512 × 512 ct ﬁles from 82 patients. we choose 5638
ct images from 65 patients for training, 668 ct images from 8 patients for
validation and 753 ct images from 9 patients for testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"all experiments are implemented on ubuntu 16.04.12
with an nvidia rtx 3090 24g gpu using pytorch 1.8.0 and cuda 11.1.74. we augment the data by rotation and ﬂipping ﬁrst and then randomly crop
them to 128 × 128 patches. adam optimizer with β1 = 0.9 and β2 = 0.99
is used to minimize the target function."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"104
j. chi et al.
fig. 2. qualitative results on the 3d-ircadb dataset with the scale factor of 2. (a)
is the hr image and its red rectangle region displays the liver and its lateral issues."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"(b) to (h) are the reconstruction results by diﬀerent methods. [10].
figure 2 shows the qualitative comparison results on the 3d-ircadb dataset
with the scale factor of 2. all methods enhance the image quality to diﬀer-
ent extents compared with bicubic interpolation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"the results of dan, spsr and aid-srgan suﬀers from
the artifacts. jdnsr blurs the issue structural information, e.g. the edges of
liver and bone. for the inferior vena cava, portal vein, and gallbladder within
the kidney, realsr restores blurred details and textures though it could recover
clear edges of calciﬁcations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"therefore, our method reconstructs more detailed
results than other methods. table 2 shows the quantitative comparison results of diﬀerent state-of-the-
art methods with two scale factors on two datasets. for the 3d-ircadb
and pancreas datasets, our method outperforms the second-best meth-
ods 1.6896/0.0157 and 1.7325/0.0187 on psnr/ssim with the scale factor
of 2 respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"similarly, our method outperforms the second-best methods
low-dose ct image super-resolution network
105
fig. 3. qualitative results on pancreas dataset with the scale factor of 4. (a) is the
hr image and its red rectangle region shows the pancreas and kidney."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"those quantitative superiorities conﬁrm our qualitative observa-
tions.
106
j. chi et al.
4
conclusion
in this paper, we propose an ldct image sr network with dual-guidance fea-
ture distillation and dual-path content communication. facing the existing prob-
lem that reconstructed results suﬀer from residual artifacts, we design a dual-
guidance feature distillation backbone which consists of dgfm and sab to
extract deep visual information. especially, the dgfm could fuse the average
ct image to take the advantage of the 3d spatial information of ct volume and
the segmentation mask to focus on the roi, which provides pixel-wise shallow
information and deep semantic features for our backbone."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_10.pdf,"then, our shared heads mechanism reconstructs the deep features obtained by
our backbone to satisfactory results. the experiments compared with 6 state-of-
the-art methods on 2 public datasets demonstrate the superiority of our method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"to reduce false posi-
tives, we identify three challenges: (1) unlike natural images, a malignant
mammogram typically contains only one malignant ﬁnding; (2) mam-
mography exams contain two views of each breast, and both views ought
to be considered to make a correct assessment; (3) most mammograms
are negative and do not contain any ﬁndings. in this work, we tackle the
three aforementioned challenges by: (1) leveraging sparse r-cnn and
showing that sparse detectors are more appropriate than dense detectors
for mammography; (2) including a multi-view cross-attention module
to synthesize information from diﬀerent views; (3) incorporating multi-
instance learning (mil) to train with unannotated images and perform
breast-level classiﬁcation. the resulting model, m&m, is a multi-view
and multi-instance learning system that can both localize malignant
ﬁndings and provide breast-level predictions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"in particular, the high
y.n.truong vu and d. guo—equal contribution. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_75.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43904-9_75
m&m: a multi-view and mil sparse detector
779
(a) free response operating characteris-
tic (froc) curves on ddsm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"to illustrate the distribution shift, we train
four popular dense detectors using a standard setup that includes only annotated malig-
nant and benign cases [1,13,16]. [7], a large dataset with a signiﬁ-
cant proportion of negatives (table 1), for training and evaluation. across all dense mod-
els, there is a large performance drop in the clinically representative setting that includes
negative images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"however, excluding negative
images from training and evaluation leads to a distribution shift since negative
images are abundant in clinical practice. concretely, the false positive rate is
low for a typical evaluation data distribution but much higher for a clinically-
representative data distribution, as shown in fig. in this work, we tackle these challenges and propose a multi-view and multi-
instance learning system, m&m."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"m&m is an end-to-end system that detects
malignant ﬁndings and provides breast-level classiﬁcation. to achieve these goals,
m&m leverages three components: (1) sparse r-cnn to replace dense anchors
with a set of sparse proposals; (2) multi-view cross-attention to synthesize
780
y. n. truong vu et al.
information from two views and iteratively reﬁne the predictions, and (3) multi-
instance learning (mil) to include negative images during training. ultimately,
each component contributes to our goal of reducing false positives."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"with mil, m&m improves the recall at 0.1 fp/image by 12.6% (fig. 4). , m&m can provide breast-level classiﬁcation predictions, achieving
aucs of more than 0.88 on four diﬀerent datasets (table 3). 2
m&m: a multi-view and mil system
2.1
sparse r-cnn with dual classiﬁcation heads
the sparsity of malignant ﬁndings calls into question the use of dense detectors."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"yet, a model
generalizes poorly if these negative images are dropped during training (fig. 1b). since image- and breast-level labels are available, we adopt an mil module
to include images without bounding boxes during training. to compute image-
and breast-level scores, we leverage the proposal malignancy logits mi (eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"= 1 − n
k=1(1 − x[k]) to the
malignancy probabilities pi = sigmoid(mi) ∈ rn. next, as cc and mlo views
oﬀer complimentary information on a breast, we obtain breast-level malignancy
score by averaging the image-level scores across these views. we apply cross-entropy losses limage and lbreast at the image and breast
level for all training samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"[7], which is funded
by cancer research uk. we split the data into train/val/test with an 80:10:10
ratio at the patient level; (2) inhouse-a: an evaluation dataset collected from
m&m: a multi-view and mil sparse detector
783
table 1. we report the number of breasts in each dataset, bro-
ken down by 3 categories: malignant, benign, and negative."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"δ denotes the ap gap
between evaluating with and without negative images. 2.2 for more details on the
inhouse datasets). we followed the
methods by [3,5,13,16] to split the test set; (5) cbis-ddsm: a curated subset
of ddsm [9]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"we only include breasts that have one cc view and one mlo
view. dataset statistics are reported in table 1.
metrics. we report average precision with intersection over union from 0.25
to 0.75."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"apmb denotes average precision on the set of annotated malignant and
benign images. ap denotes average precision when all data is included. we
report free response operating characteristic (froc) curves and recalls at var-
ious fp/image (r@t)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"quantitative classiﬁcation evaluation. (a) on three private datasets, we use
two open-sourced mammography classiﬁers as baselines [23,25]. all models were trained
only on optimam."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"m&m (87% r@0.5) outperforms all recent sota with the same test split,
including 2022 sota [33] (83% r@0.5), by at least 4%. table 3a reports m&m’s breast-level and exam-level
classiﬁcation results on optimam and the two inhouse datasets. [25] as baselines since they are open-sourced classiﬁers developed
for mammography."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_75.pdf,"0.08–0.12 exam auc when evaluated on inhouse-a and inhouse-b. in compar-
ison, m&m has smaller performance gaps of 0.02 on inhouse-a and 0.04 on
inhouse-b. similar observations for other classiﬁers, such as eﬃcientnet, are
reported in the appendix. table 3b compares m&m with recent literature reporting on the public cbis-
ddsm dataset. [14] by 0.08 and 0.14 breast auc, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"despite its clinical utility, medical image segmentation (mis)
remains a daunting task due to images’ inherent complexity and variabil-
ity. vision transformers (vits) have recently emerged as a promising solu-
tion to improve mis; however, they require larger training datasets than
convolutional neural networks. to overcome this obstacle, data-eﬃcient
vits were proposed, but they are typically trained using a single source
of data, which overlooks the valuable knowledge that could be leveraged
from other available datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"na¨ıvly combining datasets from diﬀerent
domains can result in negative knowledge transfer (nkt), i.e., a decrease
in model performance on some domains with non-negligible inter-domain
heterogeneity. in this paper, we propose mdvit, the ﬁrst multi-domain
vit that includes domain adapters to mitigate data-hunger and combat
nkt by adaptively exploiting knowledge in multiple small data resources
(domains). further, to enhance representation learning across domains, we
integrate a mutual knowledge distillation paradigm that transfers knowl-
edge between a universal network (spanning all the domains) and auxil-
iary domain-speciﬁc network branches."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"our code is available at
https://github.com/siyi-wind/mdvit. keywords: vision transformer · data-eﬃciency · multi-domain
learning · medical image segmentation · dermatology
1
introduction
medical image segmentation (mis) is a crucial component in medical image
analysis, which aims to partition an image into distinct regions (or segments)
that are semantically related and/or visually similar. this process is essential
for clinicians to, among others, perform qualitative and quantitative assessments
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 43."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"this enables
a vit to achieve improved segmentation performance compared to traditional
convolutional neural networks (cnns) on plenty of segmentation tasks [16]. how-
ever, due to the lack of inductive biases, such as weight sharing and locality, vits
are more data-hungry than cnns, i.e., require more data to train [31]. [17] in brain mri."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"as each dataset alone is too small
to properly train a vit, the challenge becomes how to eﬀectively leverage the
diﬀerent datasets. related works on mitigating vits’ data-hunger or multi-domain adaptive
learning. u (universal) implies a model spans multiple domains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"f means the model’s
size at inference time remains ﬁxed even when more domains are added. sharing knowl-
edge by transferring knowledge from a cnn [31] or pertaining vits on multiple
related tasks and then ﬁne-tuning on a down-stream task [37]; increasing data via
augmentation [34]; and non-supervised pre-training [8]. nevertheless, one notable
limitation in these approaches is that they are not universal, i.e., they rely on
separate training for each dataset rather than incorporate valuable knowledge
from related domains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"as a result, they can incur additional training, inference,
and memory costs, which is especially challenging when dealing with multiple
small datasets in the context of mis tasks. to the best of our knowledge, multi-domain uni-
versal models have not yet been investigated for alleviating vits’ data-hunger. given the inter-domain heterogeneity resulting from variations in imaging
protocols, scanner manufacturers, etc."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"however, those
mat techniques are built based on cnn rather than vit or are scalable, i.e., the
models’ size at the inference time increases linearly with the number of domains. to address vits’ data-hunger, in this work, we propose mdvit, a novel ﬁxed-
size multi-domain vit trained to adaptively aggregate valuable knowledge from
multiple datasets (domains) for improved segmentation. in particular, we intro-
duce a domain adapter that adapts the model to diﬀerent domains to mitigate
negative knowledge transfer caused by inter-domain heterogeneity."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"besides, for
better representation learning across domains, we propose a novel mutual knowl-
edge distillation approach that transfers knowledge between a universal network
(spanning all the domains) and additional domain-speciﬁc network branches. we summarize our contributions as follows: (1) to the best of our knowledge,
we are the ﬁrst to introduce multi-domain learning to alleviate vits’ data-hunger
when facing limited samples per dataset. (2) we propose a multi-domain vit,
mdvit, for medical image segmentation with a novel domain adapter to coun-
teract negative knowledge transfer and with mutual knowledge distillation to
enhance representation learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"(3) the experiments on 4 skin lesion segmenta-
tion datasets show that our multi-domain adaptive training outperforms separate
and joint training (st and jt), especially a 10.16% improvement in iou on the
skin cancer detection dataset compared to st and that mdvit outperforms
state-of-the-art data-eﬃcient vits and multi-domain learning strategies.
2
methodology
let x ∈ rh×w ×3 be an input rgb image and y ∈ {0, 1}h×w be its ground-
truth segmentation mask. training samples {(x, y )} come from m datasets,
each representing a domain. we aim to build and train a single vit that per-
forms well on all domain data and addresses the insuﬃciency of samples in any
of the datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"we ﬁrst introduce our baseline (base), a vit with hierarchi-
cal transformer blocks (fig. 1-a). our proposed mdvit extends base with 1)
a domain adapter (da) module inside the factorized multi-head self-attention
(mhsa) to adapt the model to diﬀerent domains (fig. 1-b,c), and 2) a mutual
knowledge distillation (mkd) strategy to extract more robust representations
across domains (fig. 1-d). we present the details of mdvit in sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"mdvit
451
fig. 1. overall architecture of mdvit, which is trained on multi-domain data by
optimizing two types of losses: lseg and lmkd. mdvit extends base (a) with da
inside factorized mhsa (b), which is detailed in (c), and mkd (d)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"to reduce computational complexity, fol-
lowing [19], we add two and one cnn layer before and after transformer blocks,
respectively, enabling the 1st transformer block to process features starting from
a lower resolution: h
4 × w
4 . [33,39], to clearly
evaluate the eﬃcacy of multi-domain learning in mitigating vits’ data-hunger.
2.1
mdvit
mdvit consists of a universal network (spanning m domains) and m auxiliary
network branches, i.e., peers, each associated with one of the m domains. the
universal network is the same as base, except we insert a domain adapter
(da) in each factorized mhsa to tackle negative knowledge transfer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"similarly,
our intuition of inserting the da into mhsa is to enable the diﬀerent heads to
have varied perspectives across domains. rather than manually designate each
head to one of the domains, guided by a domain label, mdvit learns to focus on
the corresponding features from diﬀerent heads when encountering a domain. da
contains two steps: attention generation and information selection (fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"after that, similar to [20], we
calculate attention for each head: ah = ψ(w hd) ∈ rk, h = 1, 2, ...h, where ψ
is a softmax operation across heads and w h ∈ rk× k
r . information selection adaptively selects information from diﬀerent heads. [uh
1, uh
2, ..., uh
k] ∈ rn×k from the hth head, we utilize
ah to calibrate the information along the channel dimension: ˜uh
k = ah
k · uh
k.
mutual knowledge distillation (mkd): distilling knowledge from domain-
speciﬁc networks has been found beneﬁcial for universal networks to learn more
robust representations [21,40]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"moreover, mutual learning that transfers knowl-
edge between teachers and students enables both to be optimized simultane-
ously [15]. to realize these beneﬁts, we propose mkd that mutually transfers
knowledge between auxiliary peers and the universal network. 1-d, the
mth auxiliary peer is only trained on the mth domain, producing output ˆy
m,
whereas the universal network’s output is ˆy ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"similar to [21], we utilize a sym-
metric dice loss lam
mkd = dice( ˆy , ˆy
m) as the knowledge distillation loss. each
peer is an expert in a certain domain, guiding the universal network to learn
domain-speciﬁc information. the universal network experiences all the domains
and grasps the domain-shared knowledge, which is beneﬁcial for peer learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"speciﬁcally, multi-level features from the encoding
transformer blocks (fig. 1-a) go through an mlp layer and an up-sample oper-
ation to unify the channel dimension and resolution to h
4 × w
4 , which are then
concatenated with the feature involving domain-shared information from the
mdvit
453
table 2. segmentation results comparing base, mdvit, and sota methods. we
report the models’ parameter count at inference time in millions (m)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"[25], which contain
454
s. du et al.
fig. 2. visual result comparison of mdvit, base and sota data-eﬃcient mis vits
in st and jt training paradigms on four datasets. the green and red contours present
the ground truth and segmentation results, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"(color ﬁgure online)
2594, 1300, 206, and 200 samples, respectively. to facilitate a fairer performance
comparison across datasets, as in [4], we only use the 1212 images from dmf
that exhibited similar lesion conditions as those in other datasets. we perform
5-fold cross-validation and utilize dice and iou metrics for evaluation as [33]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"implementation details: we conduct 3 training paradigms: separate (st),
joint (jt), and multi-domain adaptive training (mat), described in sect. 1,
to train all the models from scratch on the skin datasets. images are resized
to 256 × 256 and then augmented through random scaling, shifting, rotation,
ﬂipping, gaussian noise, and brightness and contrast changes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"the hidden dimensions of the cnn bridge and auxiliary peers
are 1024 and 512. but at the expense of diminished performance on larger datasets (isic
and dmf). this is expected given the non-negligible inter-domain heterogeneity
between skin lesion datasets, as found by bayasi et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"the above results
demonstrate that shared knowledge in related domains facilitates training a vit
on small datasets while, without a well-designed multi-domain algorithm, caus-
ing negative knowledge transfer (nkt) due to inter-domain heterogeneity, i.e.,
the model’s performance decreases on other datasets. meanwhile, mdvit ﬁts
all the domains without nkt and outperforms base in st by a large margin;
signiﬁcantly increasing dice and iou on scd by 6.4% and 10.16%, showing
that mdvit smartly selects valuable knowledge when given data from a certain
domain. additionally, mdvit outperforms base in jt across all the domains,
with average improvements of 0.82% on dice and 1.4% on iou."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"mdvit
455
table 3. ablation studies of mdvit and experiments of da’s plug-in capability. kd
means general knowledge distillation, i.e., we only transfer knowledge from auxiliary
peers to the universal network. d or b refers to using deeplabv3’s decoder or base’s
decoding layers as auxiliary peers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"as illustrated in table 2-a,b,c, these sota
models are superior to base in st. this is expected since they are designed
to reduce data requirements. nevertheless, in jt, these models also suﬀer from
nkt: they perform better than models in st on some datasets, like scd,
and worse on others, like isic."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"more segmentation results are presented in the supplementary material. though
bat and transfuse in st have better results on some datasets like isic, they
require extra compute resources to train m models as well as an m-fold increase
in memory requirements. the above results indicate that domain-shared knowl-
edge is especially beneﬁcial for training relatively small datasets such as scd."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_43.pdf,"in table 2-d, base† confronts nkt, which
lowers the performance on dmf compared with base in st, whereas mdvit†
not only addresses nkt but also outperforms base† on average dice and iou.
456
s. du et al.
ablation studies and plug-in capability of da: we conduct ablation
studies to demonstrate the eﬃcacy of da, mkd, and auxiliary peers. b reveals that using one-direction knowledge distillation (kd) or either of the
critical components in mdvit, i.e., da or mkd, but not together, could not
achieve the best results. [9] (4.7m) or base’s decoding layers (10.8m)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"radi-
ologists usually rely on a series of multi-phase contrast-enhanced com-
puted tomography (cect) scans done during follow-up visits to perform
early detection of the potential crlm. these scans form unique ﬁve-
dimensional data (time, phase, and axial, sagittal, and coronal planes
in 3d ct). most of the existing deep learning models can readily han-
dle four-dimensional data (e.g., time-series 3d ct images) and it is not
clear how well they can be extended to handle the additional dimension
of phase."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"our
code is available at https://github.com/xueyangliosu/mpbd-lstm. keywords: colorectal cancer liver metastasis · liver cancer
prediction · contrast-enhanced ct scan · bi-directional lstm
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 37. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"colorectal cancer liver metastases (crlm)
have therefore become one of the major focuses in the medical ﬁeld. patients
with colorectal cancer typically undergo contrast-enhanced computed tomogra-
phy (cect) scans multiple times during follow-up visits after surgery for early
detection of crlm, generating a 5d dataset. in addition to the axial, sagit-
tal, and coronal planes in 3d ct scans, the data comprises contrast-enhanced
multiple phases as its 4th dimension, along with diﬀerent timestamps as its 5th
dimension."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"[13] use
convolutional neural networks (cnn) to capture spatial features and long
short-term memory (lstm) to process temporal features. some other models,
such as simvp [4], replace lstms with cnns but still have the capability of
processing spatiotemporal information. these models can be adapted for classi-
ﬁcation tasks with the use of proper classiﬁcation head."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"however, all these methods have only demonstrated their eﬀectiveness
towards 3d/4d data (i.e., time-series 2d/3d images), and it is not clear how
to best extend them to work with the 5d cect data. part of the reason is
due to the lack of public availability of such data. when extending these mod-
els towards 5d cect data, some decisions need to be made, for example: 1)
what is the most eﬀective way to incorporate the phase information?"
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"[12]
shows uni-directional lstm works well on natural videos while several other
works show bi-directional lstm is needed in certain medical image segmenta-
tion tasks [2,7]. in this paper, we investigate how state-of-art deep learning models can be
applied to the crlm prediction task using our 5d cect dataset. we evaluate
the eﬀectiveness of bi-directional lstm and explore the possible method of
incorporating diﬀerent phases in the cect dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"[12] with
a bi-directional lstm and a multi-plane structure. mpbd-lstm
381
2
dataset and methodology
2.1
dataset
fig. 1. representative slices from 3d ct images of diﬀerent patients in our dataset,
at a/v phases and timestamps t0, t1, t2 (cropped to 256 × 256 for better view)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"characreristics of our dataset
cohort # of positive cases # of negative cases total cases positive rate
1st
60
141
201
0.299
2nd
9
59
68
0.132
total
69
200
269
0.257
when patients undergo cect scans to detect crlm, typically three phases
are captured: the unenhanced plain scan phase (p), the portal venous phase
(v), and the arterial phase (a). the p phase provides the basic shape of the
liver tissue, while the v and a phases provide additional information on the
liver’s normal and abnormal blood vessel patterns, respectively [10]. professional
radiologists often combine the a and v phases to determine the existence of
metastases since blood in the liver is supplied by both portal venous and arterial
routes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"– patients have two or more times of cect scans. – we already determined whether or not the patients had liver metastases
within 2 years after the surgery, and manually labeled the dataset based
on this.
– no potential focal infection in the liver before the colorectal radical surgery.
– no metastases in other organs before the liver metastases.
– no other malignant tumors. our retrospective dataset includes two cohorts from two hospitals."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"ct images are collected with the following
acquisition parameters: window width 150, window level 50, radiation dose 120
kv, slice thickness 1 mm, and slice gap 0.8 mm. all images underwent manual
quality control to exclude any scans with noticeable artifacts or blurriness and
to verify the completeness of all slices. additional statistics on our dataset are
presented in table 1 and examples of representative images are shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"yv,t0 is the output of this 3d-lstm module after processed by σ. (color
ﬁgure online)
mpbd-lstm
383
2.2
methods
numerous state-of-the-art deep learning models are available to eﬀectively pro-
cess 4d data. in this paper, we will evaluate some of the most popular ones:
1) saconvlstm, introduced by lin et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"[12], integrates 3d cnns into lstm
cells to capture both short- and long-term temporal relations. they used 3d-
cnns to handle the 3d data at each timestamp and lstms to compute
information at diﬀerent timestamps.
3) predrnn-v2, introduced by wang et al. [4], introduced by gao et al., uses cnn as the translator instead of
lstm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"all of these models need to be modiﬁed to handle 5d cect datasets. a straight-
forward way to extend them is simply concatenating the a phase and v phase
together, thus collapsing the 5d dataset to 4d. however, such an extension may
not be the best way to incorporate the 5d spatiotemporal information, because
the positional information of the same ct slice in diﬀerent phases would be lost."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"after this, the output yv,t0 is passed into
the bi-directional lstm module in the next layer and viewed as input for this
module. figure 2(a) illustrates how mpbd-lstm uses these 3d-lstm building
blocks to handle the multiple phases in our ct scan dataset. [12] with the same hyperparameters."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"we ﬁrst use three 3d-cnn
encoders (not displayed in fig. 2(a)) as introduced in e3d-lstm to extract the
features. each encoder is followed by a 3d-lstm stack (the “columns”) that
processes the spatiotemporal data for each timestamp. the stacks are bidirec-
tionally connected, as we described earlier, and consist of two layers of 3d-lstm
modules that are connected by their hidden states."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"when the spatiotemporal
dataset enters the model, it is divided into smaller groups based on timestamps
and phases. the 3d-lstm stacks process these groups in parallel, ensuring that
the ct slices from diﬀerent phases are processed independently and in order,
preserving the positional information. after the computation of the 3d-lstm
modules in each plane, we use an average function to combine the output hidden
states from both planes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"it modiﬁes
e3d-lstm by using bi-directional connected lstms to enhance communication
between diﬀerent timestamps, and a multi-plane structure to simultaneously
process multiple phases. 3
experiments
3.1
data augmentation and selection
we selected 170 patients who underwent three or more cect scans from our
original dataset, and cropped the images to only include the liver area, as shown
in fig. 1. among these cases, we identiﬁed 49 positive cases and 121 negative
cases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"for data augmentation, we randomly rotated the images
from −30◦ to 30◦ and employed mixup [17]. we applied the same augmentation
technique consistently to all phases and timestamps of each patient’s data. [18] to uniformly select 64 slices."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"for
each slice, the dimension was 256 × 256 after cropping. we used the a and
v phases of cect for our crlm prediction task since the p phase is only
relevant when tumors are signiﬁcantly present, which is not the case in our
dataset. h × w), where t is the number of timestamps, p is the number
of diﬀerent phases, d is the slice depth, h is the height, and w is the width.
3.2
experiment setup
as the data size is limited, 10-fold cross-validation is adopted, and the ratio of
training and testing dataset is 0.9 and 0.1, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"as this is a classiﬁcation
task, we evaluate all models’ performance by their auc scores. [9]
0.721
predrnn-v2 [14] 0.765
simvp [4]
0.662
mpbd-lstm
0.790
table 2 shows the auc scores of all models tested on our dataset. additional
data on accuracy, sensitivity speciﬁcity, etc. can be found in the supplementary
material."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"after this, we performed an ablation study to assess the eﬀectiveness of
the bi-directional connection. by replacing the bi-directional connection with
a uni-directional connection, the mpbd-lstm model’s performance decreased
to 0.768 on the original dataset. this result indicates that the bi-directional
connection is crucial for computing temporal information eﬀectively, and its
inclusion is essential for achieving high performance in mpbd-lstm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"all 3 timestamps
0.790
mpbd-lstm
387
ablation study on timestamps and phases. we conducted ablation stud-
ies using ct images from diﬀerent timestamps and phases to evaluate the
eﬀectiveness of time-series data and multi-phase data. the results, as shown
in table 4, indicate that mpbd-lstm achieves auc scores of 0.660, 0.676, and
0.709 if only images from timestamps t0, t1, and t2 are used, respectively.
these scores suggest that predicting crlm at earlier stages is more challenging
since the features about potential metastases in ct images get more signiﬁcant
over time."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"additionally, mpbd-lstm obtains auc scores
of 0.653 and 0.752 on single a and v phases, respectively. these results suggest
that the v phase is more eﬀective when predicting crlm, which is consistent
with medical knowledge [15]. however, both of these scores are lower than the
result of combining two phases, indicating that a multi-phase approach is more
useful."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_37.pdf,"with similar conﬁdence in the two cases, the error is likely
due to the relatively smaller liver size of patient c. beyond this case, we ﬁnd
that small liver size is also present in most of the false negative cases. a possible
explanation would be that smaller liver may provide less information for accurate
prediction of crlm. how to eﬀectively address inter-patient variability in the
dataset, perhaps by better fusing the 5d features, requires further research from
the community in the future."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"we train
a multi-channel quantitative image translation model to decompose an
x-ray image into projections of ct of individual muscles to infer the lean
muscle mass and muscle volume. we propose the object-wise intensity-
sum loss, a simple yet surprisingly eﬀective metric invariant to muscle
deformation and projection direction, utilizing information in ct and
x-ray images collected from the same patient. while our method is basi-
cally an unpaired image-to-image translation, we also exploit the nature
of the bone’s rigidity, which provides the paired data through 2d-3d
rigid registration, adding strong pixel-wise supervision in unpaired train-
ing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"through the evaluation using a 539-patient dataset, we showed that
the proposed method signiﬁcantly outperformed conventional methods. the average pearson correlation coeﬃcient between the predicted and
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_47.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. we
believe our method opened up a new musculoskeletal diagnosis method
and has the potential to be extended to broader applications in multi-
channel quantitative image translation tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"figure 1 illustrates the meaning of our
ﬁne-grained muscle analysis and its challenges. the contribution of this paper
is three-fold: 1) proposal of the object-wise intensity-sum (owis) loss, a simple
yet eﬀective metric invariant to muscle deformation and projection direction, for
quantitative learning of the absolute volume and lean mass of the muscles, 2)
proposal of partially aligned training utilizing the aligned (paired) dataset for
the rigid object for the pixel-wise supervision in an unpaired image translation
task, 3) extensive evaluation of the performance using a 539-patient dataset. mskdex
499
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"2. overview of the proposed mskdex. [21], and projection,
embedding information of volume and mass. a decomposition model was trained using
gan loss and proposed gc loss chain, owis loss, and bone loss to decompose an
x-ray image into drrs whose intensity sum derives the metric of volume and mass.
2
method
2.1
dataset preparation
figure 2 illustrates the overview of the proposed mskdex."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"= eix − gc(ix, v (g(ix)w v drr))
(1)
mskdex
501
to maintain the structure consistency between an x-ray image and decomposed
drr, where g(ix)w v drr is the decomposed wvdrr. however, we do not
apply reconstruction gc loss for vdrr and mdrr because of lacking atten-
uation coeﬃcient information. +gc(st(g(ix)w v drr
i
), g(ix)mdrr
i
)

(2)
to chain the structural constraints from wvdrr to vdrr and mdrr, where
the g(ix)w v drr
i
, g(ix)v drr
i
, and g(ix)mdrr
i
are i-th object image of the
decomposed wvdrr, vdrr, and mdrr, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"intensity sum consistency. unlike general images, our drrs embedded
speciﬁc information so that the intensity sum represents physical metrics (mass
and volume). furthermore, the conventional method did not utilize the paired
information of an x-ray image and drr (obtained from the same patient)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"+ lis(mdrr).
partially aligned training. a previous study [15] suggested that supervision
by the aligned (paired) data can improve the quantitative translation. [21] to align the pelvis and femur drrs
with the paired x-ray images for partially aligned training to improve overall
performance, including muscle metrics estimation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"3
experiments and results
the automatic segmentation results of 552 cts were visually veriﬁed, and 13
cases with severe segmentation failures were omitted from our analysis, resulting
in 539 cts. four-fold cross-validation was performed, i.e., 404 or 405 training
data and 134 or 135 test data per fold. the baseline of our experiment was the
vanilla cyclegan with the reconstruction gc loss proposed in [17]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"from 3d ct images with three metrics, pearson correlation coeﬃcient (pcc),
intra-class correlation coeﬃcient (icc), and mean absolute error (mae). addi-
tionally, we evaluated the image quality of predicted drrs of the bones by com-
paring them with the aligned drrs using peak-signal-noise-ratio (psnr) and
structural similarity index measure (ssim). implementation details are described
in supplemental materials."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"504
y. gu et al.
ablation study. we performed ablation studies to investigate the impact of
proposed owis loss and the use of aligned bones using 404 training and 135 test
data. the re-weighting parameter λis of 0, 10, and 1000 with and without the
partially aligned training lb was tested."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"more detailed results are shown in supplemental materials. performance comparison between the conventional and proposed methods in
a cross-validation study using 539 data. lean muscle mass and bone mass estimation accuracy (pcc)
method glu."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"the prediction of muscles overlapped
with the pelvis in the x-ray image can leverage the strong pixel-wise supervision
by the aligned pelvis’s drr, which can be considered as a type of calibration. our future works are the validation with a large-scale dataset and extension to
the decomposition into a larger number of objects.
acknowledgement. the research in this paper was funded by mext/jsps kak-
enhi (19h01176, 20h04550, 21k16655)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_47.pdf,"ieee trans. med. hiasa, y., et al.: cross-modality image synthesis from unpaired data using cycle-
gan. zhu, j.-y., park, t., isola, p., efros, a.a.: unpaired image-to-image translation
using cycle-consistent adversarial networks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"the attention-based multi-modal fusion module uses cross-
attention and self-attention to extract modality-invariant features and
modality-speciﬁc features in parallel. in addition, we design an object-
level temporal aggregation (ota) module that can automatically ﬁlter
low-quality features and eﬃciently integrate temporal information from
multiple frames to improve the accuracy of tumor diagnosis. tal results on a multicenter dataset show that the proposed framework
outperforms the single-modal models and the competing methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"directly concatenation and addition were
the most common methods, such as [3,4,12]. these simple operations might not
highlight essential information from diﬀerent modalities. weight-based fusion
methods generally used an importance prediction module to learn the weight
of each modality and then performed sum, replacement, or exchange based on
the weights [7,16,17,19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"nevertheless, we prove in our experiments that these attention-
based methods may have the potential risks of entangling features of diﬀerent
modalities. in practice, experienced radiologists usually utilize dynamic information on
tumors’ blood supply in ceus videos to make diagnoses [8]. previous researches
have proved that temporal information is eﬀective in improving the performance
of deep learning models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"chen et al. [2] showed that ceus videos can provide
more detailed blood supply information of tumors allowing a more accurate
breast lesion diagnosis than static us images. in this work, we propose a novel multi-modal us video fusion network
(muvf-yolox) based on ceus videos for renal tumor diagnosis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"our main
contributions are fourfold. (1) to the best of our knowledge, this is the ﬁrst deep
learning-based multi-modal framework that integrates both b-mode and ceus-
mode information for renal tumor diagnosis using us videos. (2) we propose an
attention-based multi-modal fusion (amf) module consisting of cross-attention
and self-attention blocks to capture modality-invariant and modality-speciﬁc fea-
tures in parallel."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"the above two stages are trained successively. we
ﬁrst perform a strong data augmentation to train the network for tumor detec-
tion and classiﬁcation on individual frames. after that, the ﬁrst stage model is
switched to the evaluation mode and predicts the label of each frame in the video
clip."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"finally, we train the ota module to aggregate the temporal information
for precise diagnosis. 2.2
dual-attention strategy for multimodal fusion
using complementary information between multi-modal data can greatly
improve the precision of detection. therefore, we propose a novel amf mod-
ule to fuse the features of diﬀerent modalities."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"famf is the output of the amf module. 2.3
video-level decision generation
in clinical practice, the dynamic changes in us videos provide useful information
for radiologists to make diagnoses. therefore, we design an ota module that
aggregates single-frame renal tumor detection results in temporal dimension for
diagnosing tumors as benign and malignant."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"the
features are ﬁnally picked out from the cls_conv and reg_conv layers guided
by the positions of the top 30 grid cells. let fcls = {cls1, cls2, ...clsl} and
freg = {reg1, reg2, ...regl} denote the above obtained high-quality features
from l frames. after feature selection, we aggregate the features in the temporal
dimension by time attention."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"+ fcls
(6)
after temporal feature aggregation, ftemp is fed into a multilayer perceptron
head to predict the class of tumor. 3
experimental results
3.1
materials and implementations
we collect a renal tumor us dataset of 179 cases from two medical centers,
which is split into the training and validation sets. we further collect 36 cases
from the two medical centers mentioned above (14 benign cases) and another
center (fujian provincial hospital, 22 malignant cases) to form the test set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"2)
and the other two centers, which raises the complexity of the task but can better
verify our method’s generalization ability. more than two radiologists with ten
years of experience manually annotate the tumor bounding box and class label at
the frame level using the pair annotation software package (https://www.aipair. each case has 40–50
labeled frames, and these frames cover the complete contrast-enhanced imaging
cycle."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"the number of cases and annotated frames is summarized in table 1.
weights pre-trained from imagenet are used to initialize the swin-
transformer backbone. data augmentation strategies are applied synchronously
to b-mode and ceus-mode images for all experiments, including random rota-
tion, mosaic, mixup, and so on. all models are trained for 150 epochs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"a muvf network for renal tumor diagnosis
647
table 1. the details of our dataset. number of cases in brackets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"however, “ca+sa” (row 6 in table 2) obtains inferior
performance than “ca” (row 5 in table 2). we conjecture that connecting the two
attention modules in series leads to the entanglement of modality-speciﬁc and
modality-invariant information, which would disrupt the model training. on the
contrary, the “ca//sa” method, combining two attention modules in parallel,
enables the model to capture and digest modality-speciﬁc and modality-invariant
features independently."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"meanwhile,
increasing the sampling interval tends to decrease the performance (row 4 and
row 5 in table 3). it indicates that continuous inter-frame information is beneﬁ-
cial for renal tumor diagnosis. compared to the single-modal
models, directly concatenating multi-modal features (row 3 in table 4) improves
a muvf network for renal tumor diagnosis
649
ap50 and ap75 by more than 15%."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"they score higher ap in the valida-
tion set but lower one in the test set than “concatenate”. this may be because
the generated weights are biased to make similar decisions to the source domain,
thereby reducing model generalization in the external data. moreover, cmf only
highlights similar features between two modalities, ignoring that each modal-
ity contains some unique features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_62.pdf,"[9] fail to outperform weight-based models. on the contrary, our amf
module prevents information entanglement by conducting cross-attention and
self-attention blocks in parallel. it achieves ap50 = 82.8, ap75 = 60.6 in the val-
idation set and ap50 = 79.5, ap75 = 39.2 in the test set, outperforming all com-
peting methods while demonstrating superior generalization ability."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"however, the eﬀectiveness of deep neural net-
works is often limited by the lack of interpretability and the need for
signiﬁcant amount of manual annotations. to address these issues, we
present a novel approach by leveraging both gaze data and multi-view
data for mammogram classiﬁcation. the gaze data of the radiologist
serves as a low-cost and simple form of coarse annotation, which can pro-
vide rough localizations of lesions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"we also develop a pyramid loss better
ﬁtting to the gaze-supervised process. moreover, considering many stud-
ies overlooking interactive information relevant to diagnosis, we accord-
ingly utilize transformer-based attention in our network to mutualize
multi-view pathological information, and further employ a bidirectional
fusion learning (bfl) to more eﬀectively fuse multi-view information. experimental results demonstrate that our proposed model signiﬁcantly
improves both mammogram classiﬁcation performance and interpretabil-
ity through incorporation of gaze data and cross-view interactive infor-
mation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"radiologists typically focus
on areas with breast lesions during mammogram reading [11,22], which provides
valuable guidance. we propose using real-time eye tracking information from
radiologists to optimize our model. by using gaze data to guide model training,
we can improve model interpretability and performance [24].
radiologists’ eye movements can be automatically and unobtrusively
recorded during the process of reading mammograms, providing a valuable source
of data without the need for manual labeling."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"the determination of the benign or malignant nature
of masses is largely dependent on the smoothness of their edges [13]. the gaze
data can guide the model’s attention towards the malignant masses. microcalci-
ﬁcations are small calcium deposits which exhibit irregular boundaries on mam-
mograms [9]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"radiologists need to magnify mammograms
to diﬀerentiate between benign scattered calciﬁcations and clustered calciﬁca-
tions, the latter of which are more likely to be malignant and necessitate further
diagnosis. leveraging gaze data can guide the model to locate malignant calci-
ﬁcations. in this work, we propose a novel diagnostic model, namely mammo-net,
which integrates radiologists’ gaze data and interactive information between
cc-view and mlo-view to enhance diagnostic performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"our model is designed for single-
breast cases. mammo-net extracts multi-view features and utilizes transformer-
based attention to mutualize information [21]. furthermore, there are diﬀerences
70
c. ji et al.
between multi-view mammograms of the same patient, arising from variations
in breast shape and density."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"• we propose a novel breast cancer diagnosis model, namely mammo-net. this
model employs transformer-based attention to mutualize information and uses
bfl to integrate task-related information to make accurate predictions. • we demonstrate the eﬀectiveness of our approach through experiments using
mammography datasets, which show the superiority of mammo-net."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"our proposed method employs bfl to eﬀectively fuse multi-view informa-
tion to improve diagnostic accuracy. additionally, by integrating gaze data from
radiologists, our proposed model is able to generate more precise attention maps. the fusion network combines multi-view feature representations using a stack of
linear-activation layers and a fully connected layer, resulting in a classiﬁcation
output.
2.2
gaze supervision
in this module, we utilize cam to calculate the attention map for the network
by examining gradient-based activations in back-propagation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"1. mammo-net consists of two components: a multi-view classiﬁcation network
(upper half) and an attention consistency module (lower half). the classiﬁcation net-
work interacts multi-view information, while the attention consistency module provides
positional supervision. 72
c. ji et al.
square error (mse) between the attention maps generated by the radiologist
and the model at each level of the gaussian pyramid."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"this allows the model to
mimic the attention of radiologists and enhance diagnostic performance. moreover, the pyramid representation enables the model to learn from the
important pathological regions on which radiologists are focusing, without the
need for precise pixel-level information. layernorm is also employed to address
the issue of imprecise gaze data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"this reduces noise in the consistency process
by performing consistency loss only in the regions where radiologist spent most
time. 2.3
interactive information
transformer-based mutualization model. we use transformer-based
attention to mutualize information from the two views at the level of the spatial
feature map."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"for each attention head, we compute embeddings for the source
and target pixels. our model does not utilize positional encoding, as it encodes
the relative position of each pixel and is not suitable for capturing information
between diﬀerent views of mammograms [21]. the target view feature maps are
transformed into q, the source view feature maps are transformed into k, and
the original source feature maps are transformed into v ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"(4)
bidirectional fusion learning. to enable the fusion network to retain more
of the shared features between the two views and ﬁlter out noise, we pro-
pose to use bfl to learn a fusion representation that maximizes the cross-view
mutual information. the optimization target is to generate a fusion representa-
tion i from multi-view representations pv, where v ∈ {cc, mlo}."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"maximizing
the dissimilarity between diﬀerent patient mammograms enhances the model’s
robustness. in short, we require the fusion representation i to reversely reconstruct multi-
view representations pv so that more view-invariant information can be passed
to i. by aligning the prediction n(i) to pv, we enable the model to decide how
much information it should receive from each view. the overall loss function for this module is the sum of the losses deﬁned for
each view:
lbf l = lcc
i + lmlo
i
."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"in conclusion, we have proposed a total
of three loss functions to guide the model training: lbce, lbf l, and lp yramid. + μlbf l.
(8)
3
experiments and results
3.1
datasets
mammogram dataset. our experiments were conducted on cbis-ddsm
[12] and inbreast [16]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"the cbis-ddsm dataset contains 1249 exams that have
been divided based on the presence or absence of masses, which we used to
perform mass classiﬁcation. the inbreast dataset contains 115 exams with both
masses and micro-calciﬁcations, on which we performed benign and malignant
classiﬁcation. we split the inbreast dataset into training and testing sets in a
7:3 ratio."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"it is worth noting that the oﬃcial inbreast dataset does not provide
image-level labels, so we obtained these labels following shen et al. eye gaze dataset. eye movement data was collected by reviewing all cases in
inbreast using a tobii pro nano eye tracker."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"74
c. ji et al.
3.2
implementation details
we trained our model using the adam optimizer [10] with a learning rate of
10−4 (partly implemented by mindspore). to overcome the problem of limited
data, we employed various data augmentation techniques, including translation,
rotation, and ﬂipping. to address the problem of imbalanced classes, we utilized
a weighted loss function that assigns higher weights to malign cases in order
to balance the number of benign and malign cases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"[25] as our evaluation metrics,
and we selected the ﬁnal model based on the best validation auc. considering
the relatively small size of our dataset, we used resnet-18 as the backbone of
our network. 3.3
results and analysis
table 1. ablation study of key components of mammo-net, and comparison of diﬀer-
ent models in terms of auc and acc."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"“bfl” denotes “bidirectional fusion learning”,
and “ra” denotes “radiologist attention”. dataset
model
auc
acc
cbis-ddsm lopez et al. [26]
0.859
0.791
mlo-view
0.663
0.716
cc-view
0.650
0.704
cross-view
0.762
0.755
cross-view+bfl
0.786
0.812
cross-view+ra
0.864
0.830
cross-view+bfl+ra (mammo-net) 0.889 0.849
mammo-net for multi-view mammogram classiﬁcation
75
performance comparison."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"we also compare our model with other methods that use eye movement
supervision as shown in table 1. [23] proposed a resnet-based
model with class activation mapping guided by eye gaze data. we developed a
multi-view model using this approach for a fair comparison, and found that our
method performed better."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"in contrast, our method does not rely on gaze input during inference
stage. figure 2 illustrates the visualization of our proposed model on
three representative exams from the inbreast dataset that includes masses, cal-
ciﬁcations, and a combination of both. for each exam, we present gaze heat
maps generated from eye movement data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"the results of the visualization demonstrate that the model’s capability in
localizing lesions becomes more precise when radiologist attention is incorpo-
rated in the training stage. the pyramid loss improves the model’s robustness
even when the radiologist’s gaze data is not entirely focused on the breast. this
intuitively demonstrates the eﬀectiveness of training the model with eye-tracking
supervision.
ablation study."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_7.pdf,"this shows the beneﬁts of adapting the model to mimic the radi-
ologist’s decision-making process.
4
conclusion and discussion
in this paper, we have developed a breast cancer diagnosis model to mimic the
radiologist’s decision-making process. to achieve this, we integrate gaze data as
a form of weak supervision for both lesion positioning and interpretability of
the model. we also utilize transformer-based attention to mutualize multi-view
information and further develop bfl to fully fuse multi-view information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"however, existing methods that rely on
end-to-end learning paradigms, which directly input images and output
segmentation maps, often struggle with extremely hard boundaries, such
as those found in lesions of particularly small or large sizes. this lim-
itation arises because the receptive ﬁeld and local context extraction
capabilities of any ﬁnite model are inevitably limited, and the acqui-
sition of additional expert-labeled data required for larger models is
costly. motivated by the impressive advances of diﬀusion models that
regard image synthesis as a parameterized chain process, we introduce
a novel approach that formulates skin lesion segmentation as a bound-
ary evolution process to thoroughly investigate the boundary knowledge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"second, we propose an evolution uncertainty-based
fusion strategy to reﬁne the evolution results and yield more precise lesion
boundaries. we evaluate the performance of our model on two popular
skin lesion segmentation datasets and compare our model to the latest
cnn and transformer models. our results demonstrate that our model
outperforms existing methods in all metrics and achieves superior per-
formance on extremely challenging skin lesions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"the proposed approach
has the potential to signiﬁcantly enhance the accuracy and reliability
of skin lesion segmentation, providing critical information for diagnosis
and treatment. all resources will be publicly available at https://github.
com/jcwang123/mbdiﬀ.
keywords: skin lesion segmentation · diﬀusion model
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8_41. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14223, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,", current models for skin lesion segmentation are still struggling with
extremely challenging cases, which are often encountered in clinical practice. while some approaches aim to optimize the model architecture by incorporating
local and global contexts and multi-task supervision, and others seek to improve
performance by collecting more labeled data and building larger models, both
strategies are costly and can be limited by the inherent complexity of skin lesion
boundaries. therefore, we propose a novel approach that shifts the focus from
merely segmenting lesion boundaries to predicting their evolution."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"secondly, we have implemented an
evolution uncertainty-based fusion strategy, which takes into account the uncer-
tainty of diﬀerent initializations to reﬁne the evolution results and obtain more
precise lesion boundaries. we evaluate our model on two popular skin lesion
segmentation datasets, isic-2016 and ph2 datasets, and ﬁnd that it performs
signiﬁcantly better than existing models. 2
method
the key objective of mb-diﬀ is to improve the representation of ambiguous
boundaries by learning boundary evolution through a cascaded series of steps,
rather than a single step."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"however, simply averaging the predicted identities
from multiple evolutions is not eﬀective, as the used mse loss without activa-
tion constrains the predicted identities to be around 0 or 1, unlike the sigmoid
function which would limit the identities to a range between 0 and 1. therefore,
432
j. wang et al.
table 1. comparison of skin lesion segmentation with diﬀerent approaches on the isic-
2016 and ph2 datasets. the averaged scores of both sets are presented respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"finally, the segmentation map is
generated as y∗ = (n
i=1 y∗,i) ≥ τ. 3
experiment
3.1
datasets and evaluation metrics
datasets: we use two publicly available skin lesion segmentation datasets from
diﬀerent institutions in our experiments: the isic-2016 dataset and the ph2
dataset. the isic-2016 dataset [8] is provided by the international skin imaging
collaboration (isic) archive and consists of 900 samples in the public training
set and 379 samples in the public validation set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"evaluation metrics: to comprehensively compare the segmentation results,
particularly the boundary delineations, we employ four commonly used metrics
to quantitatively evaluate the performance of our segmentation methods. these
metrics include the dice score, the iou score, average symmetric surface dis-
tance (assd), and hausdorﬀ distance of boundaries (95−th percentile; hd95).
to ensure fair comparison, all labels and predictions are resized to (512×512)
before computing these scores, following the approach of a previous study [18].
3.2
implementation details
for the diﬀusion model hyper-parameters, we use the default settings of the plain
diﬀusion model, which can be found in the supplementary materials. regarding
medical boundary diﬀusion model
433
false negatives
true positves
false postives
image
u-net++
ca-net
transunet
mb-diff
(ours)
gt
transfuse
xbound-
former
medseg
diff
uncertainty
(ours)
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"2. visual comparison of our method and the sotas. the ﬁrst three rows are
samples from the isic-2016 validation set and the last three rows are from the ph2
dataset. we highlight the small lesions using dotted boxes in the third row."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"the training parameters, we resize all images to (256 × 256) for eﬃcient memory
utilization and computation. we use a set of random augmentations, including
vertical ﬂipping, horizontal ﬂipping, and random scale change (limited to 0.9 ∼
1.1), to augment the training data. we set the batch size to 4 and train our model
for a total of 200,000 iterations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"[22], a recently released diﬀusion-based model, which we re-trained
for 200,000 steps to ensure a fair comparison. the quantitative results are shown in table 1, which reports four evaluation
scores for two datasets. though the parameters of cnns and transformers are
selected with the best performance on isic-2016 validation set and the param-
eters of our method are selected by completing the 200,000 iterations, mb-diﬀ
still achieves the 1.18% iou improvement and 0.7% dice improvement."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"3. detailed analysis of our method, including the ablation analysis (a) and the
comparison to the other diﬀusion-based method (b, c). moreover, our method shows a larger improvement in generalization
performance on the ph2 dataset, indicating its better ability to handle new data. we present a visual comparison of challenging samples in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_41.pdf,"it is evident that most regions
with high uncertainties correspond to false predictions. this information can be
used to guide human reﬁnement of the segmentation in practical applications,
ultimately increasing the ai’s trustworthiness. 3.4
detailed analysis of the evolution
in this subsection, we make a comprehensive analysis to investigate the perfor-
mance of each component in our method and compare it to the diﬀusion-based
model, medsegdiﬀ. the results of our ablation study are presented in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"survival prediction is crucial for cancer patients as it provides early
prognostic information for treatment planning. recently, deep survival models
based on deep learning and medical images have shown promising performance
for survival prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"in view of this, we propose a merging-
diverging learning framework for survival prediction from multi-modality images. this framework has a merging encoder to fuse multi-modality information and a
diverging decoder to extract region-speciﬁc information. in the merging encoder,
we propose a hybrid parallel cross-attention (hpca) block to effectively fuse
multi-modality features via parallel convolutional layers and cross-attention trans-
formers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"our framework
is demonstrated on survival prediction from pet-ct images in head and neck
(h&n) cancer, by designing an x-shape merging-diverging hybrid transformer
network (named xsurv). our xsurv combines the complementary information
in pet and ct images and extracts the region-speciﬁc prognostic information
in pt and mln regions. extensive experiments on the public dataset of head
and neck tumor segmentation and outcome prediction challenge (hecktor
2022) demonstrate that our xsurv outperforms state-of-the-art survival prediction
methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"© the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43987-2_39
merging-diverging hybrid transformer networks
401
1
introduction
head and neck (h&n) cancer refers to malignant tumors in h&n regions, which is
among the most common cancers worldwide [1]. survival prediction, a regression task
that models the survival outcomes of patients, is crucial for h&n cancer patients: it pro-
vides early prognostic information to guide treatment planning and potentially improves
the overall survival outcomes of patients [2]. multi-modality imaging of positron emis-
sion tomography – computed tomography (pet-ct) has been shown to beneﬁt sur-
vival prediction as it offers both anatomical (ct) and metabolic (pet) information about
tumors [3, 4]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"nevertheless, we identiﬁed that
existing deep survival models still have two main limitations. firstly, existing deep survival models are underdeveloped in utilizing complemen-
tary multi-modality information, such as the metabolic and anatomical information in
pet and ct images. [17, 18] or rely on early fusion (i.e., concatenating
multi-modality images as multi-channel inputs) to combine multi-modality informa-
tion [11, 14–16, 19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"[20, 21], where multi-modality fea-
tures were extracted by multiple independent encoders with resultant features fused. however, early fusion has difﬁculties in extracting intra-modality information due to
entangled (concatenated) images for feature extraction, while late fusion has difﬁcul-
ties in extracting inter-modality information due to fully independent feature extraction. recently, tang et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"however, the performance of this method heavily relies on using tumor
segmentation masks as inputs, which limits its generalizability. secondly, although deep survival models have advantages in performing end-to-end
survival prediction without requiring tumor masks, this also incurs difﬁculties in extract-
ing region-speciﬁc information, such as the prognostic information in primary tumor
402
m. meng et al.
(pt) and metastatic lymph node (mln) regions. however, most of them only considered pt segmentation and ignored the
prognostic information in mln regions [11, 24–26]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"[16] performed survival
prediction with joint pt-mln segmentation and achieved one of the top performances
in hecktor 2022. however, this method extracted entangled features related to both
pt and mln regions, which incurs difﬁculties in discovering the prognostic information
in pt-/mln-only regions. in this study, we design an x-shape merging-diverging hybrid transformer network
(named xsurv, fig. 1) for survival prediction in h&n cancer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"our technical contributions in xsurv are three folds:
(i) we propose a merging-diverging learning framework for survival prediction. this
framework is specialized in leveraging multi-modality images and extracting region-
speciﬁc information, which potentially could be applied to many survival prediction
tasks with multi-modality imaging. (ii) we propose a hybrid parallel cross-attention
(hpca) block for multi-modality feature learning, where both local intra-modality and
global inter-modality features are learned via parallel convolutional layers and cross-
attention transformers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"the
convolution operations are realized using successive convolutional layers with residual
connections, while the cross-attention operations are realized using swin transformer
[27] where the input xin (from the same encoder branch) is projected as q and the input
xcross (from the other encoder branch) is projected as k and v. in addition, conv blocks
perform the same convolution operations as hpca blocks but discard cross-attention
operations; hpsa blocks share the same overall architecture with hpca blocks but
perform self-attention within the input xin (i.e., the xin is projected as q, k and v). conv
and hpsa blocks are used ﬁrst and then followed by hpca blocks, which enables the
xsurv to learn both intra- and inter-modality information. in this study, we set nconv,
nself , and ncross as 1, 1, and 3, as this setting achieved the best validation results (refer
to the supplementary materials)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"the idea of adopting convolutions and transformers in parallel has been explored
for segmentation [28], which suggests that parallelly aggregating global and local infor-
mation is beneﬁcial for feature learning. in this study, we extend this idea to multi-
modality feature learning, which parallelly aggregates global inter-modality and local
intra-modality information via hpca blocks, to discover inter-modality interactions
while preserving intra-modality characteristics. 404
m. meng et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"2(b),
rag blocks generate three softmax-activated spatial attention maps αpt, αmln, and
αb that correspond to pt, mln, and background regions. these attention maps are
computed based on the contextual information provided by the gating signals gpt and
gmln (whicharetheoutputsoftheformerconvblocksintheptandmlnbranches).the
attention maps αpt and αmln are multiplied with the features xskip that are propagated
from skip connections, which spatially diverge the features xskip into pt- and mln-
related features xpt and xmln. different from the vanilla attention gate (ag) block
[29], rag blocks leverage the gating signals from two decoder branches and generate
mutually exclusive (softmax-activated) attention maps."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"in addition, clinical indicators (e.g., age, gender) also can be integrated by
the coxph model. [7], while the testing dataset was excluded as its ground-truth labels are not released. each patient underwent pretreatment pet/ct and has clinical indicators."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"recurrence-
free survival (rfs), including time-to-event in days and censored-or-not status, was
provided as ground truth for survival prediction, while pt and mln annotations were
provided for segmentation. the patients from two centers (chum and chuv) were
used for testing and other patients for training, which split the data into 386/102 patients
in training/testing sets. we trained and validated models using 5-fold cross-validation
within the training set and evaluated them in the testing set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"the learning rate was set as 1e−4 initially and then reset to 5e−5 and 1e−5
at the 4,000th and 8,000th training iteration. data augmentation was applied in real-time
during training to minimize overﬁtting, including random afﬁne transformations and
random cropping to 112 × 112 × 112 voxels. validation was performed after every 200
training iterations and the model achieving the highest validation result was preserved."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"survival prediction and segmenta-
tion were evaluated using concordance index (c-index) and dice similarity coefﬁcient
(dsc), which are the standard evaluation metrics in the challenges [6, 7, 35]. we also performed two ablation studies on the encoder and decoder separately: (i)
we replaced hpca/hpsa blocks with conv blocks and compared different strategies to
combinepet-ctimages.(ii)weremovedragblocksandcompareddifferentstrategies
to extract pt/mln-related information. table 1. comparison between xsurv and state-of-the-art survival prediction methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_39.pdf,"this ﬁnding is consistent with wang et al. [19]’s study, which suggests that
early and late fusion cannot effectively leverage the complementary information in pet-
ct images. as we have mentioned, early and late fusion have difﬁculties in extracting
intra- and inter-modality information, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"[19,25], and treatment
planning [27]. [3,9,14,17].
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 59.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. [8], level set methods [22]) or explicitly
represent them as a ordered set of landmarks or correspondence points (aka point
distribution models, pdms)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"various correspondence gener-
ation methods exist, including non-optimized landmark estimation and paramet-
ric and non-parametric correspondence optimization. non-optimized methods
manually label a reference shape and warp the annotated landmarks using reg-
istration techniques [10,16,18]. [26], while group-wise non-parametric approaches
ﬁnd correspondences by considering the variability of the entire cohort during
the optimization process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"in this paper, we introduce mesh2ssm1, a deep learning method that
addresses the limitations of traditional and deep learning-based ssm approaches. mesh2ssm leverages unsupervised, permutation-invariant representation learn-
ing to learn the low dimensional nonlinear shape descriptor directly from mesh
data and uses the learned features to generate a correspondence model of the
population. mesh2ssm also includes an analysis network that operates on the
learned correspondences to obtain a data-driven template point cloud (i.e., tem-
plate point cloud), which can replace the initial template, and hence reducing
the bias that could arise from template selection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"[13,21] operating
on the learned correspondence points and trained end-to-end with correspon-
dence generation network. this vae branch serves two purposes: (a) serves
as a shape analysis module for the non-linear shape variations and (b) learns
a data-speciﬁc template from the latent space of the correspondences that is
fed back to the correspondence generation network. to motivate the need for the mesh feature encoder and study the eﬀect of the
template selection, we considered the box-bump dataset, a synthetic dataset of
3d shapes of boxes with a moving bump."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"moreover, flowssm fails to identify the correct mode
of variation, the horizontal movement of the bump as the primary variation,
which can also be inferred by comparing the compactness curves in fig. 1.c.
mesh2ssm performs best when the template is a medoid shape, which makes
the case for learning a data-speciﬁc template. since mesh2ssm model uses an
autoencoder, inference on unseen meshes only requires a single forward pass (1 s
per sample); flowssm requires re-optimization, increasing the inference time
drastically and require a convergence criteria to determine the best number of
iterations per sample (0.15 s for one iterations per sample)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"correspondence is established since the same template is
deformed to all the samples. 2.2
analysis
the mesh2ssm model also consists of an analysis branch that acts as a shape
analysis module to capture non-linear shape variations identiﬁed by the learned
correspondences {ci}n
i=1 and also learns a data-informed template from the
620
k. iyer and s. y. elhabian
latent space of correspondences to be fed back into the correspondence gen-
eration network during training. this branch uses one network module:
shape variation autoencoder (sp-vae): the vae [13,21] is a latent
variable model parameterized by an encoder φ, decoder θ, and the prior p(zp) ∼
n(0, i)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"after the burn-in stage, alter-
nate optimization of the correspondence and analysis module begins. during the
alternate optimization phase, we generate the data-informed template from the
latent space of sp-vae at regular intervals. the learned data-informed template
is used in the correspondence generation module in the subsequent epochs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"the segmentations were isotropi-
cally resampled, smoothed, centered, and converted to meshes with roughly 2000
vertices. although the dgcnn mesh autoencoder used in mesh2ssm does not
require the same number of vertices, uniformity across the dataset makes it com-
putationally eﬃcient; hence, we pad the smallest mesh by randomly repeating
the vertices (akin to padding image for convolutions). the samples were ran-
domly divided, with 218 used for training, 26 for validation, and 27 for testing.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"figure 3
shows the top three pca modes of variations identiﬁed by mesh2ssm and
flowssm. similar to the observations made box-bump dataset, flowssm is
aﬀected by the choice of the template, and the modes of variation diﬀer as the
template changes. on the other hand, pdm predicted by mesh2ssm identiﬁes
the same primary modes consistently."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"compactness measures
622
k. iyer and s. y. elhabian
fig. 4. (a) shape statistics of pancreas dataset: compactness (higher is better), gen-
eralization (lower is better), and speciﬁcity (lower is better). (b) mesh2ssm learned
template across epochs for pancreas dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"generalization measures the average sur-
face distance between all test shapes and their reconstructions, and speciﬁcity
measures the distance between randomly generated pca samples. figure 4.a
shows the metrics for the pancreas dataset. mesh2ssm outperforms flowssm
in all three metrics, despite using only 256 correspondence points compared to
flowssm’s ∼2000 vertices."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"the model can learn
correct deformations in the correspondence generation module and identify the
correct mean shape in the latent space of sp-vae in the analysis module. using
the analysis module of mesh2ssm, we visualized the top three modes of varia-
tion identiﬁed by sorting the latent dimensions of sp-vae based on the stan-
dard deviations of the latent embeddings of the training dataset. variations are
generated by perturbing the latent representation of a sample in three direc-
tions, resulting in non-linear modes such as changes in the size and shape of the
pancreas head and narrowing of the neck and body."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"additionally, the surface to surface distance of the mesh reconstructions
(using the correspondences in mesh2ssm and deformed meshes in flowssm)
was included. for the pancreas dataset with the medoid as the initial template,
mesh2ssm with the template feedback produced more precise models.
3.2
limitations and future scope
as ssm is included a part of diagnostic clinical support systems, it is crucial to
address the drawbacks of the models. like most deep learning models, perfor-
mance of mesh2ssm could be aﬀected by small dataset size, and it can produce
overconﬁdent estimates."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_59.pdf,"the
use of an autoencoder for meaningful feature extraction of meshes to learn the
pdm provides a versatile and scalable framework for ssm. [13,21] analysis module helps in mitigating bias and
capturing non-linear characteristics of the data. the method is demonstrated to
have superior performance in identifying shape variations using fewer parameters
on synthetic and clinical datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf,"while previous works focused on lesion detection in still images [25]
and oﬄine videos [9,11,22], this paper explores real-time ultrasound video lesion
detection. real-time lesion prompts can assist radiologists during scanning, thus
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2_1. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf,"we plug the ntca module into a basic real-time detector to
form ultradet. experiments on cva-bus dataset [9] demonstrate that ultra-
det, with real-time inference speed, signiﬁcantly outperforms previous works,
reducing about 50% fps at a recall rate of 0.90. our contributions are four-fold."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf,"(3)
we conduct extensive experiments to demonstrate the proposed ultradet sig-
niﬁcantly outperforms the previous state-of-the-arts. (4) we release high-quality
labels of the cva-bus dataset [9] to facilitate future research. 2
related works
real-time video object detection is typically achieved by single-frame
detectors, often with temporal information aggregation modules."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf,"[5,8,16,21] use only intra-frame information, detr-based detec-
tors [20,26] and faster r-cnn-based detectors [1,2,7,14,23,28] are also widely
utilized in video object detection. they aggregate temporal information by min-
ing inter-object relationships without considering ntc. [10] can assist radiologists in clinical practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf,"thus
their performances are far from satisfactory. for the ﬁrst time, we use inverse optical ﬂow to
guide temporal context information extraction. 3
method
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf,"the yellow and green frames are sampled as
context frames, and their feature maps are inputs of the ntca module. (color ﬁgure
online)
in real-time video lesion detection, given the current frame it and a sequence of
t previous frames as {iτ}t−1
τ=t−t , the goal is to detect lesions in it by exploiting
the temporal information in previous frames as illustrated in fig. [14], and a
temporal relation head [2]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf,"4
experiments
4.1
dateset
cva-bus dateset. we use the open source cva-bus dataset that consists
of 186 valid videos, which is proposed in cva-net [9]. we split the dataset
into train-val (154 videos) and test (32 videos) sets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf,"in the test split, there are 3849 frames
with 32 lesions. we focus on the lesion detection task and do not utilize the
benign/malignant classiﬁcation labels provided in the original dataset.
8
h. yu et al.
high-quality labels. the bounding box labels provided in the original cva-
bus dataset are unsteady and sometimes inaccurate, leading to jiggling and
inaccurate model predictions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf,"we provide a new version of high-quality labels
that are re-annotated by experienced radiologists. we reproduce all baselines
using our high-quality labels to ensure a fair comparison. visual comparisons
of two versions of labels are available in supplementary materials."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf,"mining negative temporal contexts to suppress fps
9
4.3
implementation details
ultradet settings. we use flownets [3] as the ﬁxed flownet in iof align and
share the same ﬁnding with previous works [4,12,13] that the flownet trained
on natural datasets generalizes well on ultrasound datasets. we set the pooling
stride in the flownet to 4, the number of ultradet head layers l = 2, the
number of previous frames t = 15 and tctxt = 2, and the number of proposals is
16."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_1.pdf,"level aggregation provides no performance gains. this conclusion agrees with
radiologists’ skills to focus more on local regions instead of global information. mining negative temporal contexts to suppress fps
11
5
conclusion
in this paper, we address the clinical challenge of real-time ultrasound lesion
detection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"in spite
of the huge size of whole slide images, the number of individual slides is often
rather small, leading to a small number of labeled samples. to improve train-
ing, we propose and investigate novel data augmentation strategies for multiple
instance learning based on the idea of linear and multilinear interpolation of fea-
ture vectors within and between individual whole slide images. based on state-
of-the-art multiple instance learning architectures and two thyroid cancer data
sets, an exhaustive study was conducted considering a range of common data
augmentation strategies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"whereas a strategy based on to the original mixup app-
roach showed decreases in accuracy, a novel multilinear intra-slide interpolation
method led to consistent increases in accuracy. keywords: histopathology · data augmentation · mixup · multiple instance
learning
1
motivation
whole slide imaging is capable of effectively digitizing specimen slides, showing both
the microscopic detail and the larger context, without any signiﬁcant manual effort. due
to the enormous resolution of the whole slide images (wsis), a classiﬁcation based
on straight-forward convolutional neural network architectures is not feasible."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"while the majority
of methods rely on separate learning stages, also end-to-end approaches have been pro-
posed [3,14]. in spite of the large amount of data, the number of labeled samples in mil
(represented by the number of individual, globally labelled wsis) is often small and/or
imbalanced [6]. general data augmentation strategies, such as rotations, ﬂipping, stain
augmentation and normalization and afﬁne transformations, are applicable to increase
the amount of data [15]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"all of these methods are performed in the image domain. here, we consider feature-level data augmentation directly applied to the representation
extracted using a convolutional neural network. for example, li et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"[21], which is
referred to as mixup. this method was originally proposed as data agnostic approach
which also shows good results if applied to image data [2,4,16]. variations were pro-
posed, to be applied to latent representations [17] as well as to balance data sets [6]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"due to the structure of mil training data, we identiﬁed several options to perform
interpolation-based data augmentation. the main contribution of this work is a set of novel data augmentation strategies for
mil, based on the interpolation of patch descriptors. inspired by the (linear) mixup
approach [21], we investigated several ways to translate this idea to the mil setting.
beyond linear interpolation, we also deﬁned a more ﬂexible and novel multilinear app-
roach."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"this strategy
is highly efﬁcient during training since the features are only computed once (per patch)
and for augmentation only simple arithmetic operations are applied to the (smaller)
feature vectors. image-based data augmentation strategies (such as stain-augmentation,
rotations or deformations) can be combined easily with the feature-based approaches
but require individual feature extraction during training. however, to avoid the curse of
meta-parameters and thereby experiments these methods are not considered here."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"xj , where xi and xj are randomly
sampled raw input feature vectors. corresponding labels y′ are generated such that
y′ = α · yi + (1 − α) · yj , where yi and yj are the corresponding one-hot label
encodings. the weight α is drawn from a uniform distribution between 0 and 1.
a single input (corresponding to a wsi) of a mil approach with a separate feature
extraction stage [10] can be expressed as a p-tupel x = (x1, ..., xp ) with xi being the
feature vector of an individual patch and p being the number of patches per wsi."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"however,
there are several options to adapt the basic idea to the changed setting.
fig. 1. overview of the proposed feature-based data augmentation approaches. in the case of
inter-mixup (a), a linear combination was applied on the pairs of wsi descriptors with a ran-
domly selected weight factor."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"since the new synthetic descriptors are
individually generated in each epoch, there is no beneﬁt if the number of extracted wsi
480
m. gadermayr et al.
descriptors is increased. we ﬁx this number to the number of wsis in the training data
set, in order to keep the number of training iterations per epoch consistent. two different conﬁgurations are considered."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"firstly, we investigate the interpolation
between wsis of the same class (v1). secondly, interpolation between all wsis is
performed, which also includes the interpolation between the labels (v2). the random values, α, v and w are selected individually for
each individual wsi and each epoch."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"the thereby
obtained vector tupel (x1′, ..., xp ′) ﬁnally represents the synthetic wsi-based image
descriptor. besides performing combinations for each wsi during training, selective
interpolation can be useful to keep real samples within the training data. this can be
easily achieved by choosing (x1′, ..., xp ′) with a chance of β and (x1, ..., xp ) oth-
erwise."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"since the method represents a state-of-the-art approach,
it further serves as well-performing baseline. in instance-based mil, the information
per patch is ﬁrst condensed to a single scalar value, representing the classiﬁcation per
patch. finally, all of these patch-based values are aggregated."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"[7], but utilize a pre-trained
network instead. speciﬁcally, we applied a resnet18 pre-trained on the image-net chal-
lenge data, due to the high performance in previous work on similar data [5]. resnet18
was assessed as particularly appropriate due to the rather low dimensional output (512
mixup-mil: novel data augmentation for multiple instance learning
481
dimensions). we actively decided not to use a self-supervised contrastive learning app-
roach [10] as feature extraction stage since invariant features could interfere with the
effect of data augmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"ran-
dom sampling corresponds to the random selection of patches (feature vectors) from
each wsi. thereby the amount of investigated data per wsi is reduced with the beneﬁt
of increasing the variability of the data. in the experiments, we adjust the sample ratio
q between the patch-based features for training and testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"this differentiation is crucial, due to the different treatment options, in particular with
respect to the extent of surgical resection of the thyroid gland [19]. the data set utilized
in the experiments consists of 80 wsis overall. one half (40) of the data set consists of
frozen and the other half (40) of parafﬁn sections [5]), representing the different modal-
ities."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"the mean and median age of patients at the date of dissection was
47 and 50 years, respectively. the data set comprised 13 male and 27 female patients,
corresponding to a slight gender imbalance. they were labeled by an expert pathologist
with over 20 years experience."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"2. mean overall classiﬁcation accuracy and standard deviation obtained with each individual
combination. the columns represent the frozen (left) and parafﬁn data set (right). the top row
(a) shows the baseline scores of embedding-based, instance-based and 3 combinations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"and within wsis (intra-mixup). mixup-mil: novel data augmentation for multiple instance learning
483
the data set was randomly separated into training (80 %) and test data (20 %). the
whole pipeline, including the separation, was repeated 32 times to achieve representa-
tive scores."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"for each patch, we checked that at least 75 % of the area was covered with tissue (green
color channel) in order to exclude empty areas [5]. to obtain a representation indepen-
dent of the wsi size, we extracted 1024 patches with a size of 256×256 pixel per wsi,
resulting in 1024 patch-descriptors per wsi [5]. [10]. data and source code
are publicly accessible via https://gitlab.com/mgadermayr/mixupmil. we use the refer-
ence implementation of the dual-stream mil approach [10]. to obtain further insight
into the feature distribution, we randomly selected patch descriptor pairs and computed
the euclidean distances."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"in detail, we selected 10,000 pairs (a) from different classes,
(b) from different wsis (similar and dissimilar classes), (c,d) from the same class and
different wsis, and (e) from the same wsi.
3
results
figure 2 shows the mean overall classiﬁcation accuracy and standard deviations
obtained with each individual combination. the columns represent the frozen (left) and
parafﬁn data set (right). the top row (a) shows the baseline scores of embedding-based,
instance-based and the 3 combinations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"subﬁgure (b) show the scores obtained with
baseline data augmentation for embedding-based and dual-stream mil. subﬁgure (c)
shows the scores obtained with interpolation between patches between (inter-mixup)
and within wsis (intra-mixup). without data augmentation, scores between 0.49 and
0.72 were obtained for frozen and scores between 0.41 and 0.81 for the parafﬁn data
set. to limit the number of ﬁgures and due to the fact that instance-based mil showed
weak scores only, in the following part the focus is on embedding-based and combined-
mil (2/2) only."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"with baseline data augmentation, scores between 0.69 and 0.73 were
achieved for the frozen and between 0.78 and 0.83 for the parafﬁn data set. inter-mixup
exhibited scores up to 0.71 for the frozen and up to 0.79 for the parafﬁn data set. intra-
mixup showed average accuracy up to 0.78 for the frozen and up to 0.84 for the parafﬁn
data set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"in the
intra-wsi setting, a mean distance of 134.8 was obtained. based on the used common
box plot variation (whiskers length is less than 1.5× the interquartile range), a large
number of data points was identiﬁed as outliers. however, these points are not consid-
ered as real outliers, but occur due to the asymmetrical data distribution (as indicated
by the violin plot in the background)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"3. analysis of the distributions of the patch descriptor distances between (a) patches from
different classes, (b) randomly selected patches from different wsis, (c,d) patches from the same
class and different wsis (for both classes, pc and fn) and (e) patches within the wsis. 4
discussion
in this work, we proposed and examined novel data augmentation strategies based on
the idea of interpolations of feature vectors in the mil setting. instance-based mil
did not show any competitive scores."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"in our analysis, we focused on the embedding-based con-
ﬁguration and on the balanced combined approach (referred to as 2/2). with the baseline
data augmentation approaches, the maximum improvements were 0.03, and 0.02 for the
frozen, and 0.01, and 0.05 for the parafﬁn data set. the inter-mixup approach did not
show any systematic improvements."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"independently of the chosen strategy (v1, v2),
concerning the combination within or between classes, we did not notice any positive
trend. the multilinear intra-mixup method, however, exhibited the best scores for 3
out of 4 combinations and the best overall mean accuracy for both, the frozen and the
parafﬁn data set. also a clear trend with increasing scores in the case of an increasing
ratio of augmented data (β) is visible."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"the analysis of the distance distributions between
patch representations conﬁrmed that, the variability between wsis is clearly larger than
the variability within wsis. in addition, the results showed that the variability between
mixup-mil: novel data augmentation for multiple instance learning
485
classes is, on patch-level, not clearly larger than the variability within a class. obvi-
ously variability due to the acquisition outweigh any disease speciﬁc variability."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"we expect that stain nor-
malization methods (but not stain augmentation) could be utilized to align the different
wsis to provide a more appropriate basis for inter-wsi interpolation. with regard to
the different data sets, we noticed a stronger, positive effect in case of the frozen section
data set. this is supposed to be due to the clearly higher variability of the frozen sec-
tions corresponding with a need for a higher variability in the training data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"we also
noticed a stronger effect of the solely embedding-based architecture (also showing the
best overall scores). we suppose that this is due to the fact that the additional loss of the
dual-stream architecture exhibits a valuable regularization tool to reduce the amount of
needed training data. with the proposed intra-mixup augmentation strategy, this effect
diminishes, since the amount and quality of training data is increased."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"this is supposedly due to the
high variability between the wsis compared to a rather low variability within the wsis. in the future, additional experiments will be conducted including stain normalization
methods and larger benchmark data sets to provide further insights. this work was partially funded by the county of salzburg (no. fhs2019-
10-kiamed)
references
1. buddhavarapu, v.g., jothi, a.a.: an experimental study on classiﬁcation of thyroid
histopathology images using transfer learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"in: proceedings of the international con-
ference on medical image computing and computer assisted intervention (miccai), pp. 519–528 (2020)
4. dabouei, a., soleymani, s., taherkhani, f., nasrabadi, n.m.: supermix: supervising the
mixing data augmentation. in: proceedings of the ieee/cvf conference on computer
vision and pattern recognition (cvpr), pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"https://
github.com/binli123/dsmil-wsi
11. li, z., et al.: a novel multiple instance learning framework for covid-19 severity assessment
via data augmentation and self-supervised learning. rymarczyk, d., borowa, a., tabor, j., zielinski, b.: kernel self-attention for weakly-
supervised image classiﬁcation using deep multiple instance learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"shao, z., et al.: transmil: transformer based correlated multiple instance learning for
whole slide image classiﬁcation. in: advances in neural information processing systems
(neurips), vol. 34, pp. sharma, y., shrivastava, a., ehsan, l., moskaluk, c.a., syed, s., brown, d.: cluster-to-
conquer: a framework for end-to-end multi-instance learning for whole slide image classi-
ﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"in: proceedings of the medical imaging with deep learning conference (midl),
pp. tellez, d., et al.: quantifying the effects of data augmentation and stain color normalization
in convolutional neural networks for computational pathology. thulasidasan, s., chennupati, g., bilmes, j.a., bhattacharya, t., michalak, s.: on mixup
training: improved calibration and predictive uncertainty for deep neural networks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_46.pdf,"in: proceedings of the conference on medical image computing and
computer assisted intervention (miccai), pp. xi, n.m., wang, l., yang, c.: improving the diagnosis of thyroid cancer by machine learning
and clinical data. zhang, h., et al.: dtfd-mil: double-tier feature distillation multiple instance learning for
histopathology whole slide image classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"we leverage a low-
resolution (lr) latent space for motion correction, followed by super-
resolution reconstruction, compensating for imaging artefacts caused by
respiratory motion and spontaneous bowel movements. this alleviates
the need for semantic knowledge about the intestines and paired data. both are examined through variations of our proposed approach and we
compare them to conventional, model-based, and learning-based mc and
sr methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"we investigate this hypothesis by evalu-
ating a downstream task, automatically scoring ibd in the area of the
terminal ileum on the reconstructed images and show evidence that our
method does not suﬀer a synthetic domain bias. keywords: abdominal mr · motion correction · super-resolution ·
deep learning
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 12.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43999-5_12
122
w. zhang et al.
1
introduction
inﬂammatory bowel disease (ibd) is a relatively common, but easily overlooked
disease."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"this limits the assessment
of the complete volume in 3d. given these problems, we aim to develop a novel
method that can perform both motion correction (mc) and super-resolution
(sr) to improve the quality of 3d ibd mri and to support accurate interpre-
tation and diagnosis. motion can cause multiple issues for mr acquisition."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"as a result, 3d bowel motion can lead
to intra- and inter-plane corruptions [6], e.g., slice misregistration, slice proﬁle
eﬀects, and anisotropic spatial resolution. sr can be used to enhance these scans,
but conventional methods often struggle with this type of anisotropic data or
may unintentionally hide signiﬁcant imaging ﬁndings. manual correction
or enhancement of these volumes is not feasible."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"(2) second, volumes are corrected by incorporating latent features in the lr
domain. the complementary spatial information from unpaired quality images
is exploited via cycle regularisation to provide an explicit constraint. third,
we conduct extensive evaluations on 200 subjects from a uk crohn’s disease
study, and a public abdominal mri dataset with realistic respiratory motion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"(3) experimental evaluation and analysis show that our mocosr is able to
generate high-quality mr images and performs favourably against other, alter-
mocosr
123
native methods. furthermore, we explore conﬁdence in the generated data and
improvements to the diagnostic process. (4) experiments with existing models
for predicting the degree of small bowel inﬂammation in crohn’s disease patients
show that mocosr can retain diagnostically relevant features and maintain the
original hr feature distribution for downstream image analysis tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"[10–12] are commonly used to model a variety of complex struc-
tures while preserving details. [16,17] rely on paired data to learn the mapping and degradation processes,
which is not acceptable in real-world scenarios where data are mismatched. [18]
utilizes cyclic consistency structures to address unpaired degradation adaptation
in brain sr, however abdominal data would be more complicated and suﬀer
from motion corruption."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"recent studies on sr joint with
other tasks (e.g., reconstruction, denoising) have demonstrated improvements in
the lr space [11,19,20]. for this purpose, we utilize a cycle consistency frame-
work to handle unpaired data and joint tasks.
automated evaluation of ibd. in the ﬁeld of machine learning and gastroin-
testinal disease, [21] used random forests to segment diseased intestines, which is
the ﬁrst time that image analysis support has been applied to bowel mri."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"in [22] residual networks focused on
the terminal ileum to detect crohn’s disease. in this case, quality reconstruction
data is extremely important for the detection of relevant structures. 2
method
problem formulation and preliminaries: in 3d sr, the degradation pro-
cess is modeled with: ilr = d(ihr; ki, ↓s)+n, d() represents the downsampling
with the blur kernel ki, scaling factor s, and noise n. in this work, we propose
the motion corruption term m, which operates on lr latent space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"and our
mc-sr model can be reﬁned to
ilr = m(d(ihr; ki, ↓s); z) + n
(1)
mocosr concept: as shown in fig. 1, our multi-task framework consists of
three parts: a pair of corrupted lr (clr) encoder and sr decoder on corrupted
lr input, a pair of quality sr (qlr) encoder and learned lr (llr) decoder on
hr input and two task-speciﬁc discriminators. for the ﬁrst two pairs, individual
features are extracted and scaling is applied to provide our network with the
ability to handle multiple tasks at diﬀerent scales."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"1. during training, the input comprises corrupted mri with motion artifacts and
blurring eﬀects. during inference, only the ﬁrst pair of lr encoder eclr and sr
decoder dsr will be utilized to generate high-quality, motion-free, and super-resolved
images. a quality hr volume y is fed ﬁrst into the qhr encoder in order to obtain
a quality latent feature map zq upon which the corrupted lr can be trained."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"a pair of task-speciﬁc
discriminators is used to improve the performance of each task-related encoder
and decoder. loss functions: rather than aiming to reconstruct motion-free and hr images
in high dimensional space with paired data, we propose to regularize in low
dimensional latent space to obtain a high quality lr feature that can be used
for upscaling. + ladv( ˜y )
(4)
the corresponding two task-speciﬁc discriminators ldmc and ldsr for dis-
criminating between corrupted and quality images followed are used for the
mocosr
125
purpose of staged reconstruction zq and ˆy of mc at latent space and sr at
spatial space, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"network architecture:
in the paired encoder-decoder structure, we devel-
oped a 3d global and local residual blocks (glrb) with local residual mod-
ules (lrm) in fig. the glrb is designed to extract local fea-
tures and global structural information at 3d level, and then construct blocks
for multi-scale features connected to the output of the ﬁrst layer. the residual
output is then added to the input using a residual connection to obtain a staged
output."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"the
decoders are employed with the upsampling prior to the convolution layers. 3
experiments
data degradation: we use 64 × 64 × 64 patches. for downsampling and the
degradation associated with mri scanning, (1) gaussian noise with a standard
deviation of 0.25 was added to the image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"3. (a) the proposed pmrm sequence of respiratory simulation for motion corrup-
tion for crohn’s diseases. (b) the staged reconstruction process involving application
of pmrm, mc result, and ground truth (gt) on tcga-lihc dataset. the mc
matrix is calculated and applied on hr for visual comparison with simulated motion
perturbations directly."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"(c) the mocosr results on clinical ibd data without gt. tcga-lihc data set, abdominal mri:
a total of 237 mr exams
are collected from the cancer imaging archive liver hepatocellular carci-
noma (tcga-lihc). the data contains 908 mri series from 97 patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"we applied simulated motion to tcga mri-abdomen series to generate the
motion-corrupted dataset with respiration-induced shift. ibd data set, inﬂammatory bowel disease:
mri sequences obtained
include axial t2 images, coronal t2 images and axial postcontrast mri data
on a philips achieva 1.5 t mr scanner. abdominal 2d-acquired images exhibit
motion shifts between slices and ﬁbrillation artefacts due to the diﬃculty of
holding one’s breath/body movement and suppressing random organ motion for
extended periods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"mocosr
127
table 1. ibd data set: the upper shows mri data acquisition parameters. the lower
shows the number of patients in each severity level of inﬂammation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"mocosr achieves the best per-
formance among all evaluation metrics. cmrsr and mresr cannot guarantee
the output quality of mapping from hr back to lr, resulting in poor perfor-
mance on complex 3d bowel data. representatives for the qualitative evaluation
are shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"if features deﬁning crohn’s disease are hidden by the method,
this would aﬀect disease scoring. we use a classiﬁer with an attention mecha-
nism similar to [22], trained on hr raw data. our evaluation is based on the
average possibility of normal and abnormal small bowel inﬂammation on mri."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"the degree of small bowel inﬂammation on abnormal mris was classiﬁed by
radiologists as mild, moderate or severe. this outcome was compared against
the results of the data constructed from diﬀerent sr methods. complete results including lr degraded image, sr image reconstructed by
mresr, cmrsr, and mocosr, are shown in table 3."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"we tracked and quanti-
ﬁed the changes by performing a signiﬁcance evaluation (t-test) based on p-values
< 0.05. the ideal sr data can achieve classiﬁcation results as close as possible to
hr data with lower requirements. our method obtains similar small-scale atten-
uation results on both healthy and abnormal samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"cmrsr
makes the predicted probability much lower than that of hr.
discussion: according to the sensitivity analysis and comparison results, our
mocosr method shows superior results compared to the forward adversarial
reconstruction algorithms and encoder-decoder structures. combining multi-
scale image information in the feature space of diﬀerent resolution image domains
yields better results than inter-domain integration. the cycle consistency net-
work splits the diﬀerent resolution spaces and latent space, which facilitates the
ﬂexibility of the neural network to customize the mc according to the speciﬁc
purpose and ensures consistency of the corrected data with the unpaired data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"this leads to a distribution shift in the samples, which makes
the disease prediction biased as shown in fig. the data generated by ours can
fig. 5. distribution relationships between reconstructed 3d bowel data and original
hr data in a downstream crohn’s disease diagnosis task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"(b)
the comparison method results in the loss and concealment of discriminative features. (c) incorrectly reconstructed data misleads shifts in the distribution of sr data, which
aﬀects downstream task results. mocosr
129
table 3."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_12.pdf,"mocosr has
a negligible eﬀect on the downstream classiﬁcation task as shown by high p-values in
contrast to lr, mresr, and cmrsr which produce signiﬁcantly lower performance. the present sensitivity study is lim-
ited to the automatic classiﬁcation from single domain and down-stream task
framework, and future extensions will explore model-based and learning segmen-
tation tasks across data domains and acquisitions. 4
conclusion
mocosr is a dl-based approach to reconstruct high-quality sr mri."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"existing unsupervised semantic segmenta-
tion methods encounter a huge challenge on gland images. they either
over-segment a gland into many fractions or under-segment the
gland regions by confusing many of them with the background.
to overcome this challenge, our key insight is to introduce an empirical
cue about gland morphology as extra knowledge to guide the segmen-
tation process. to this end, we propose a novel morphology-inspired
method via selective semantic grouping."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"we ﬁrst leverage the empirical
cue to selectively mine out proposals for gland sub-regions with variant
appearances. then, a morphology-aware semantic grouping module is
employed to summarize the overall information about glands by explic-
itly grouping the semantics of their sub-region proposals. in this way,
the ﬁnal segmentation network could learn comprehensive knowledge
about glands and produce well-delineated and complete predictions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"our
method exceeds the second-best counterpart by over 10.56% at miou. with the
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 27.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. 1. (a): example of a gland and its gland border and interior epithelial tissues."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"[18]) instead of pixel-level annotations to train a gland segmentation net-
work. however, these weak annotations are still laborious and require expert
knowledge [37]. [10,29] to design annotation-free
methods for gland segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"gland borders typically consist of dark-colored cells, whereas the interior epithe-
lial tissues contain cells with various color distributions that may closely resem-
ble those non-glandular tissues in the background. as such, the e2e clustering
methods tend to blindly cluster pixels with similar properties and confuse many
gland regions with the background, leading to under-segment results.
to tackle the above challenges, our solution is to incorporate an empir-
ical cue about gland morphology as additional knowledge to guide
gland segmentation. the cue can be described as: each gland is comprised
of a border region with high gray levels that surrounds the interior epithelial tis-
sues."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"to begin, we leverage the empirical
cue to selectively mine out proposals for the two gland sub-regions with vari-
ant appearances. then, considering that our segmentation target is the gland,
we employ a morphology-aware semantic grouping module to summarize the
semantic information about glands by explicitly grouping the semantics of the
sub-region proposals. in this way, we not only prioritize and dedicate extra atten-
tion to the target gland regions, thus avoiding under-segmentation; but also
exploit the valuable morphology information hidden in the empirical cue, and
force the segmentation network to recognize entire glands despite the excessive
variance among the sub-regions, thus preventing over-segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"a segmentation network. meantime, a morphology-aware semantic grouping
(msg) module is used to summarize the overall information about glands from
their sub-region proposals. more details follow in the subsequent sections."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"we
ﬁrst obtain a normalized feature map fi for xi from a shallow encoder f with
3 convolutional layers, which can be expressed as fi = ∥f (xi)∥2. we train the
encoder in a self-supervised manner, and the loss function l consists of a typical
self-supervised loss lss, which is the cross-entropy loss between the feature map
fi and the one-hot cluster label ci = arg max (fi), and a spatial continuity loss
lsc, which regularizes the vertical and horizontal variance among pixels within
a certain area s to assure the continuity and completeness of the gland border
regions (see fig. 1 in the supplementary material)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"l = lmsgo + λvlmsgv ,
(6)
where λv (set to 1) is the coeﬃcient. the
glas dataset contains 165 h&e-stained histopathology patches extracted from
16 wsis. the crag dataset owns 213 h&e-stained histopathology patches
extracted from 38 wsis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"the crag dataset has more irregular malignant
glands, which makes it more diﬃcult than glas, and we would like to empha-
size that the results on crag are from the model trained on glas without
retraining.
morphology-inspired ugs via selective semantic grouping
287
fig. 3. visualization of predictions on glas (left) and crag dataset(right). black
denotes glandular tissues and white denotes non-glandular tissues (more in the sup-
plementary material)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"the segmentation performance is
progressively improved as the involvement of msg for variation & msg for omission. +14.30%
unsupervised methods in table 1 are obtained using the same backbone trained
with the corresponding pseudo-labels. the code is available at https://github.
com/xmed-lab/mssg."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"3.3
comparison with state-of-the-art methods
we compare our mssg with multiple approaches with diﬀerent supervision
settings in table 1. on the glas dataset, the end-to-end clustering methods
(denoted by “∗”) end up with limited improvement over a randomly initialized
network. our mssg, on the contrary, achieves signiﬁcant advances."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_27.pdf,"moreover,
mssg surpasses all other unsupervised counterparts, with a huge margin of
10.56% at miou, compared with the second-best unsupervised counterpart. on
crag dataset, even in the absence of any hints, mssg still outperforms all
unsupervised methods and even some of the fully-supervised methods. addi-
tionally, we visualize the segmentation results of mssg and its counterpart (i.e.,
sgscn [2]) in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"speciﬁcally, each head is trained
to minimize a weighted cross-entropy loss, but the weights are diﬀer-
ent among the diﬀerent branches. we show that the resulting averaged
predictions can achieve excellent calibration without sacriﬁcing accuracy
in two challenging datasets for histopathological and endoscopic image
classiﬁcation. our experiments indicate that multi-head multi-loss clas-
siﬁers are inherently well-calibrated, outperforming other recent cali-
bration techniques and even challenging deep ensembles’ performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"code to reproduce our experiments can be found at https://github.com/
agaldran/mhml_calibration. keywords: model calibration · uncertainty quantiﬁcation
1
introduction and related work
when training supervised computer vision models, we typically focus on improv-
ing their predictive performance, yet equally important for safety-critical tasks
is their ability to express meaningful uncertainties about their own predictions
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_11. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14222, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"in the context of machine learning, we often distinguish two types of uncer-
tainties: epistemic and aleatoric [13]. brieﬂy speaking, epistemic uncertainty
arises from imperfect knowledge of the model about the problem it is trained
to solve, whereas aleatoric uncertainty describes ignorance regarding the data
used for learning and making predictions. for example, if a classiﬁer has learned
to predict the presence of cancerous tissue on a colon histopathology, and it is
tasked with making a prediction on a breast biopsy it may display epistemic
uncertainty, as it was never trained for this problem [21]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"broadly speaking, one can attempt to promote calibration during training,
by means of a post-processing stage, or by model ensembling.
training-time calibration. these techniques often rely on correctly tuning a hyper-parameter controlling the
trade-oﬀ between discrimination ability and conﬁdence, and can easily achieve
better calibration at the expense of decreasing predictive performance [22].
examples of medical image analysis works adopting this approach are diﬀerence
between conﬁdence and accuracy regularization [20] for medical image diag-
nosis, or spatially-varying and margin-based label smoothing [14,27], which
extend and improve label smoothing for biomedical image segmentation tasks. [6,15] have been proposed to correct over or under-
conﬁdent predictions by applying simple monotone mappings (ﬁtted on a held-
out subset of the training data) on the output probabilities of the model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"examples of applying ensembling in medical image computing include [17,24].
in this work we achieve model calibration by means of multi-head models
trained with diverse loss functions. in this sense, our approach is closest to
some recent works on multi-output architectures like [21], where a multi-branch
cnn is trained on histopathological data, enforcing specialization of the diﬀer-
ent heads by backpropagating gradients through branches with the lowest loss. compared to our approach, ensuring correct gradient ﬂow to avoid dead heads
requires ad-hoc computational tricks [21]; in addition, no analysis on model
calibration on in-domain data or aleatoric uncertainty was developed, focusing
instead on anomaly detection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"∈ [0, 1]k by a softmax operation p = σ(z), where
pj = ezj/ 
i ezi. if the label of x was y ∈ {1, ..., k}, we can measure the error
associated to prediction p with the cross-entropy loss lce(p, y) = − log(py). we now wish to implement a multi-head ensemble model like the one shown
in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"we modify the weights ωm between branches
to achieve more diverse gradients during training. where y is a one-hot representation of the label y.
from eq. (1) we see that the gradient in branch m will be scaled depending
on how much probability mass pm
y is placed by f m on the correct class relative
to the total mass placed by all heads."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"this
can be quantiﬁed by the expected calibration error (ece), given by:
ece =
n

s=1
|bs|
n |acc(bs) − conf(bs)|,
(5)
where 
s bs form a uniform partition of the unit interval, and acc(bs), conf(bs)
are accuracy and average conﬁdence (maximum softmax value) for test samples
predicted with conﬁdence in bs.
in practice, the ece alone is not a good measure in terms of practical usabil-
ity, as one can have a perfectly ece-calibrated model with no predictive power
[29]. a binary classiﬁer in a balanced dataset, randomly predicting always one
class with c = 0.5 + ϵ conﬁdence, has a perfect calibration and 50% accuracy. [9] that capture both discrimination abil-
ity and calibration: a model must be both accurate and calibrated to achieve a
low psr value."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"finally, we show as summary
metric the average rank when aggregating rankings of ece, nll, and accuracy. 3
experimental results
we now describe the data we used for experimentation, carefully analyze per-
formance for each dataset, and end up with a discussion of our ﬁndings.
3.1
datasets and architectures
we conducted experiments on two datasets: 1) the chaoyang dataset1, which
contains colon histopathology images. it has 6,160 images unevenly distributed in
4 classes (29%, 19%, 37%, 15%), with some amount of label ambiguity, reﬂecting
high aleatoric uncertainty."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"as a consequence, the best model in the original
reference [32], applying speciﬁc techniques to deal with label noise, achieved
an accuracy of 83.4%. 2) kvasir2, a dataset for the task of endoscopic image
classiﬁcation. the annotated part of this dataset contains 10,662 images, and it
represents a challenging classiﬁcation problem due a high amount of classes (23)
and highly imbalanced class frequencies [2]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"we call this model 2hml
(2 heads-multi loss)); 3) finally, we increase the number of heads to four, and
we refer to this model as 4hml. [30], and using the
1 https://bupt-ai-cz.github.io/hsa-nrl/.
2 https://datasets.simula.no/hyper-kvasir/.
114
a. galdran et al.
dca loss [20]. we analyze the impact of temperature scaling [10] in appendix a.
what we expect to see: multi-head multi-loss models should
achieve a better calibration (low ece) than other learning-based meth-
ods, ideally approaching deep ensembles calibration."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"finally we would ideally
observe improved performance as we increase the diversity (comparing
2hsl to 2hml) and as we add heads (comparing 2hml to 4hml). we report the results on the chaoyang dataset. overall,
accuracy is relatively low, since this dataset is challenging due to label ambiguity,
and therefore calibration analysis of aleatoric uncertainty becomes meaningful
here."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"overall, we can see a pattern: multi-loss
multi-head models appear to be extremely well-calibrated (low ece and nll
values) without sacriﬁcing accuracy, and as we diversify the losses and increase
the number of heads we tend to improve calibration. results on the chaoyang dataset with diﬀerent architectures and strategies. for each model, best and second best ranks are marked."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_11.pdf,"deep
ensembles again reach the highest accuracy and excellent calibration. inter-
estingly, methods that smooth labels (ls, mbls, mixup) show a strong
degradation in calibration and their ece is often twice the ece of the
baseline sl1h model. we attribute this to class imbalance and the large
number of categories: smoothing labels might be ineﬀective in this sce-
nario."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"breast cancer (bc) is one of the most common cancers iden-
tiﬁed globally among women, which has become the leading cause of
death. multi-modal pathological images contain diﬀerent information for
bc diagnosis. hematoxylin and eosin (h&e) staining images could reveal
a considerable amount of microscopic anatomy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"the pre-trained model
could be performed on multiple relevant tasks (ihc reconstruction and
ihc classiﬁcation). the experiments on two datasets (herohe chal-
lenge and bci challenge) show state-of-the-art results. the patholog-
ical process is usually the golden standard approach for bc diagnosis, which
relies on leveraging diverse complementary information from multi-modal data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"1. illustration of diﬀerent multi-modal pre-training methods. the wsis, genetic
and clinical data from a patient could be used for isomorphic data pre-training. pairs
of h&e and ihc staining wsis are used for heterogeneous data pre-training in our
method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"according to the data format, there are two main multi-modal
pre-training approaches, as shown in fig. 1. one is based on isomorphic data,
such as vision-language pre-training [5] and vision-speech-text pre-training [3]. the other is based on heterogeneous data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"[2] proposed multi-
mae to pre-train models with intensity images, depth images, and segmentation
maps. in the ﬁeld of medical image analysis, it is widely recognized that using
multi-modal data can produce more accurate diagnoses than using single-modal
data. however, the development of multi-modal pre-training methods has been
limited due to the scarcity of paired multi-modal data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"most methods focus on
chest x-ray vision-language pre-training [8,11]. to our best knowledge, there is
no work for multi-modal pre-training based on pathological heterogeneous data. in this paper, we propose a multi-modal pre-training method based on
masked autoencoders for bc downstream tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf," we propose a multi-modal pre-training via masked autoencoders mmp-
mae for bc diagnosis. to our best knowledge, this is the ﬁrst pre-training
work based on multi-modal pathological data.  we evaluate the proposed method on two public datasets as herohe chal-
lenge and bci challenge, which shows that our method achieves state-of-the-
art performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"the remained
patches are fed into the modal-fusion encoder to get the corresponding tokens. then we use the mixed attention module to extract intra-modal and inter-modal
complementary information. finally, the modal-speciﬁc tokens are fed into the
modal-speciﬁc decoders to reconstruct the original h&e and her2 images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"the intra-modal attention is the
original transformer block, and there is no interaction between two modalities. we
replace the mhsa with mhca in the inter-modal attention to learn complementary
information. and a feedforward network (ffn)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"[19] as the aggregator
in our training process. 3
experimental results
3.1
datasets
acrobat challenge. [27] provides h&e wsis and matched ihc wsis
(er, pr, her2, and ki67), which consists of 750 training cases, 100 valida-
tion cases, and 300 testing cases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"[24] and with 4 nvidia a100 ten-
sor core gpus. we pre-train our mmp-mae on the acrobat dataset with
adamw [17] and the learning rate of 1e−4. the batch size of pre-training is 1024
and it takes about 30 h for 100 epochs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"we use the learning
rate decay strategy for stable training. peak signal to noise ratio (psnr) and
structural similarity (ssim) are used as the evaluation indicators for the quality
of the her2 generated images. in the her2 status prediction task, we use 1 gpu with a batch size of 1 (wsi
level)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"3.3
method comparison
her2 staining image generation. three methods on bci datasets are
compared in our experiments, as shown in table 1. cyclegan is a representa-
tive unsupervised method, which doesn’t need paired images for training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"5. visualization of mmp-mae generation results on the acrobat dataset. the
region in the red box shows our mmp-mae could learn the semantic information from
the adjacent area. (color ﬁgure online)
cyclegan focuses more on style transformation, and it is diﬃcult to match the
cell-level information in detail."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"our mmp-mae further improves
the performance, which achieves higher psnr by 1.60, and ssim by 0.007. the
visualization on the acrobat dataset also shows our model could learn the
modality-related information, as shown in fig. 5.
her2 status prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_44.pdf,"we compare our method with the top ﬁve methods
reported in herohe challenge review [9]. most of these methods use the multi-
network ensemble strategy and extra datasets. team macaroon uses the came-
lyon dataset [4] for tumor classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"whole slide image (wsi) classiﬁcation is an essential task in com-
putational pathology. despite the recent advances in multiple instance learning
(mil) for wsi classiﬁcation, accurate classiﬁcation of wsis remains challeng-
ing due to the extreme imbalance between the positive and negative instances in
bags, and the complicated pre-processing to fuse multi-scale information of wsi.
to this end, we propose a novel multi-scale prototypical transformer (mspt)
for wsi classiﬁcation, which includes a prototypical transformer (pt) module
and a multi-scale feature fusion module (mffm). the pt is developed to reduce
redundant instances in bags by integrating prototypical learning into the trans-
former architecture."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"it substitutes all instances with cluster prototypes, which are
then re-calibrated through the self-attention mechanism of transformer. there-
after, an mffm is proposed to fuse the clustered prototypes of different scales,
which employs mlp-mixer to enhance the information communication between
prototypes. the experimental results on two public wsi datasets demonstrate
that the pro-posed mspt outperforms all the compared algorithms, suggesting its
potential applications."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"however, wsis are extremely
large in the size and lack of pixel-level annotations, making it difﬁcult to adopt the
traditional supervised learning methods for wsi classiﬁcation [4].
to address this issue, multiple instance learning (mil) has been successfully applied
to the wsi classiﬁcation task as a weakly supervised learning problem [5–7]. in this
context, a wsi is considered as a bag, and the cropped patches within the slide are the
supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2_58. © the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"these clustering-based mil algorithms can signiﬁcantly
reduce the redundant instances, and thereby improving the training efﬁciency for wsi
classiﬁcation.however,itisdifferentfork-meanstospecifytheclusternumberaswellas
the initial cluster centers, and different initial values may lead to different cluster results,
thus affecting the performance of mil. besides, affected by the feature extractor, the
clustering-based mil algorithms may ignore the most important instances that contain
critical diagnostic information. therefore, it is necessary to develop a method that can
fully exploit the potential complementary information between critical instances and
prototypes to improve representation learning of prototypes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"however, since the number of patches at each
resolution is quite different, it requires complex pre-processing to spatially align feature
vectors of patches in different resolutions. therefore, it is signiﬁcant to develop an
efﬁcient and effective patch aggregation strategy to learn multi-scale information from
wsis. in this work, we propose a multi-scale prototypical transformer (mspt) for wsi
classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"the speciﬁcally developed pt uses a
clustering algorithm to extract instance prototypes from the bags, and then re-calibrates
these prototypes at each scale with the self-attention mechanism in transformer [19]. mffm is designed to effectively fuse multi-scale information of wsis, which utilizes
the mlp-mixer [20] to learn effective representations by aggregating the multi-scale
prototypes generated by the pt. the mlp-mixer adopts two types of mlp layers to
allow information communication in different dimensions of data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"it can effectively re-calibrate the cluster prototypes as well as
reduce the computational complexity of the transformer. 2) a new multi-scale feature fusion module (mffm) is developed based on the mlp-
mixer to enhance the information communication among phenotypes. it can effec-
tively capture multi-scale information in wsi to improve the performance of wsi
classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"2
method
2.1
mil problem formulation
mil is a typical weakly supervised learning method, where the training data consists
of a set of bags, and each bag contains multiple instances. the goal of mil is to learn
a classiﬁer that can predict the label of a bag based on the instances in it. in binary
classiﬁcation, a bag can be marked as negative if all in-stances in the bag are negative,
otherwise, the bag is labeled as positive with at least one positive instance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"in the mil
setting, a wsi is considered as a bag and the numerous cropped patches in wsi are
regarded as instances in the bag. xi =

ij
i
j=n
j=1,
(1)
where xi denotes a patient, yi the label of xi, ij
i is the j-th instance of xi, n is the number
of patients and n is the number of instances. 2.2
multi-scale prototypical transformer (mspt)
the overall architecture of mspt is shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"[21] is used to
extract features from each patch. the learned multi-scale features are then fed into the
proposed mspt, which consists of a pt and an mffm, to re-calibrate cluster prototypes
at each scale and fuse multi-scale information of wsi. finally, a wsi-level classiﬁer is
trained to predict the bag label."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"it is essen-
tial to try different initializations and choose the one with the lowest error. however,
the wsi dataset generally has a long sequence of instances, which makes the clustering
algorithms computationally expensive and slow down as the size of the bag increases. to solve the issue above, we propose to apply the self-attention (sa) mechanism in
transformer to re-calibrate these cluster prototypes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,", akn] in amap. these attention scores are
then weighted to xbag to update the pk ∈ r1×dk for completing the calibration of the
clustering prototypes ˆp ∈ rk×dk.
606
s. ding et al.
as mentioned above, existing clustering-based mil methods use the k-means clus-
tering to identify instances prototypes in the bag, where the most important instances
that contain the key semantic information may be ignored. on the contrary, our pt can
efﬁciently use all the instances to update the cluster prototypes multiple times."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"to fuse the output clustered proto-
types at different scales in mspt, we proposed an mffm, which consists of an mlp-
mixer and a gated attention pooling (gap). the mlp-mixer is used to enhance the
information communication of the prototype representation, and the gap is used to get
the wsi-level representation for wsi classiﬁcation. as shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"token-mixing mlp is a cross-location operation to mix all
prototypes, whilechannel-mixingmlpis apre-locationoperationtomixfeatures of each
prototype. thus, mlp-mixer allows the information communication between different
prototypes and prototype features to learn superior representation through information
aggregation. the structure of mffm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"speciﬁcally, the procedure of mffm is described as follows:
we ﬁrst perform the feature concatenation operation on the multi-scale output
clustering prototypes

ˆp20×, ˆp10×, ˆp5×

to construct a feature pyramid
⌣p:
concat

ˆp20×, ˆp10×, ˆp5×

→
⌣p ∈ rk×3dk
(3)
where dk is the feature vector dimension of the prototypes. r1×dout is the class label probability of the bag, and dout is the number of
classes. 3
experiments and results
3.1
datasets
to evaluate the effectiveness of mspt, we conducted experiments on two public dataset,
namely camelyon16 [24] and tcga-nsclc."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"after pre-processing, a total of 2.4 million patches at
×20 magniﬁcation, 0.56 million patches at ×10 magniﬁcation, and 0.16 million patches
at ×5 magniﬁcation, with an average of about 5900, 1400, and 400 patches per bag. the
tcga-nsclc dataset includes two sub-types of lung cancer, i.e., lung squamous cell
carcinoma (tgca-lusc) and lung adenocarcinoma (tcga-luad). we collected
a total of 854 diagnostic slides from the national cancer institute data portal (https://
portal.gdc.cancer.gov)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"we selected accuracy (acc) and area under curve
(auc) as evaluation metrics. for camelyon16 dataset, we reported the results of the
ofﬁcial testing set. for tcga-nsclc, we conducted ﬁve cross-validation on the 854
slides, and the results are reported in the format of mean ±sd (standard deviation)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"608
s. ding et al.
3.3
implementation details
for the feature extractor, we employed the simclr encoder trained by lee et al. [9]
for the camelyon16 and tcga datasets. but [9] only trained simclr encoders at 20×
and 5× magniﬁcation, to align with that setting, we used the same settings to train the
simclr encoder at 10× magniﬁcation on both datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"[11]; 4) the clustering-based algorithm remix
[16].
table 1. comparison results on the camelyon16 and tcga datasets. table 1 shows the comparison results on the camelyon16
and tcga-nsclc datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"in camelyon16, it can be found that the proposed
mspt outperforms all the compared algorithms with the best accuracy of 0.9536, and
auc of 0.9869. compared to other algorithms, mspt improves at least 0.78%, and
1.07% on classiﬁcation acc and auc, indicating the effectiveness of mffm to learn
the multi-scale information of wsis. in addition, pt achieves the best classiﬁcation
results in the single-resolution methods and outperforms remix on all indices, which
proves pt can effectively re-calibrate the clustering prototypes.
in tcga-nsclc, the proposed mspt algorithm again outperforms all the com-
pared algorithms on all indices."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"moreover, mspt improves at least
0.78% and 1.03%, respectively, on the corresponding indices compared with all other
algorithms. 3.5
ablation study
to evaluate the contribution of pt and mffm in the proposed mspt, we further
conducted a series of ablation studies.
investigation of the number of prototypes in pt. to evaluate the effectiveness of
the pt, we ﬁrst changed the number of prototypes k in the range of {1, 2, 4, 8, 16, 32} to
get the optimal k for each dataset. then, the following two variants were compared with
pt: (1) full-bag: the ﬁrst variant was only trained on all the instances; (2) prototype-bag:
the second variant was only trained on the cluster prototypes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"as shown in fig. 3, the horizontal axes denote the number of prototypes, and the ver-
tical axes denote the classiﬁcation accuracy. in the camelyon16 dataset, the performance
of both pt and prototype-bag increases with the increase of k value, and achieves the
best results with k = 16. in the tcga-nsclc dataset, pt always outperforms the full-
bag and prototype-bag."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_58.pdf,"[8] on the cluster prototypes for each
magniﬁcation, and then added them. table 2 gives the results on the camelyon16 and tcga-nsclc datasets. compared
with other multi-scale variants, the proposed mspt improves acc by at least 0.78%
and 0.85% on camelyon16 and tcga-nsclc, respectively, which proves that the
mlp-mixer in mffm can effectively enhance the information communication among
phenotypes and their features, thus improving the performance of feature aggregation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"in addition, we impose optional super-
vision during training by leveraging tens of anatomical landmarks that
can be extracted automatically. we train our approach at large scale
with more than 50,000 computed tomography (ct) scans and validate
it on two diﬀerent applications: 1) tracking of generic lesions based on
the deeplesion dataset, including liver tumors, lung nodules, enlarged
lymph-nodes, for which we report highest matching accuracy of 92%,
with localization accuracy that is nearly 10% higher than the state-of-
the-art; and 2) tracking of lung nodules based on the nlst dataset
for which we achieve similarly high performance. in addition, we include
an error analysis based on expert radiologist feedback, and discuss next
steps as we plan to scale our system across more applications."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"based
on longitudinal imaging for a given patient it requires establishing which lesions
are corresponding (i.e., same lesion, observed at diﬀerent timepoints), which
lesions have disappeared and which are new compared to prior scanning. this
information can be leveraged to assess treatment response, e.g., by analyzing the
evolution of size and morphology for a given tumor [1], but also for adaptation
of (re-)treatment radiotherapy plans that take into account new tumors. in practice, the development of automatic and reliable lesion tracking solu-
tions is hindered by the complexity of the data (over diﬀerent modalities), the
absence of large, annotated datasets, and the diﬃculties associated with lesion
identiﬁcation (i.e., varying sizes, poses, shapes, and sparsely distributed loca-
tions)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"to increase the system robustness and emulate the clini-
cian’s reading strategies, we propose to use multi-scale embeddings to enable the
system to progressively reﬁne the ﬁne-grained location. in addition, as imaging
oﬀers contextual information about the human body that is naturally consis-
tent, we design the model to beneﬁt from biologically-meaningful points (i.e.,
anatomical landmarks). the reasoning behind this strategy is that simple data
augmentation methods cannot faithfully model inter-subject variability or pos-
sible organ deformations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"with these two strategies, the goal is to
ensure a high degree of robustness in the computation of the lesion matching
across diﬀerent lesion sizes and varying anatomies. furthermore, a signiﬁcant
focus and contribution of our research is the experimental study at a very large
scale: we (1) train a pixel-wise self-supervised system using a very large and
diverse dataset of 52,487 ct volumes and (2) evaluate on two publicly available
datasets. notably, one of the datasets, nlst, presents challenging cases with
68% of lesions being very small (i.e., radius < 5 mm)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"[5] uses a
self-supervised anatomical embedding model (sam) to create semantic embed-
dings for each image pixel, avoiding the detection step. training exclusively on
augmented paired data prevents sam from accurately representing anatomical
changes and deformations that occur over time. this can inﬂuence the contex-
tual information of a pixel, which in turn impacts the pixel-wise embeddings
on which the similarity-based tracker depends."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"the
problem of lesion tracking can be formulated as ﬁnding the optimal transforma-
tion that maps p1 to its corresponding location, p2, in i2.
3.2
training stage
let d = {x1, x2, ..., xn} be a set of n unpaired and unlabeled 3d-ct volumes. as shown in fig. 1, given an image x ∈ rd×h×w from the training dataset d,
we randomly select two overlapping 3d patches (anchor and query), namely xa
and xq. to create synthetic paired data that mimics appearance changes across
diﬀerent images, we apply random data augmentation (i.e., random spatial and
intensity-related transformations) to the content of xa and xq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"∥f iq∥2
(2)
finally, we combine the multi-scale similarity maps through summation and
select the voxel with the highest similarity as the matching point in the query
volume. 4
experiments
4.1
datasets and setup
datasets: we train the universal and ﬁne-grained anatomical point matching
model using an in-house ct dataset (variousct). the training dataset contains
52,487 unlabeled 3d ct volumes capturing various anatomies, including chest,
head, abdomen, pelvis, and more."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"[8] and the national lung screening trial (nlst)
dataset [12]. [11] medical imag-
ing dataset, containing 3891 pairs of lesions with information on their location
and size. the dataset covers various types of lesions across diﬀerent organs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"for nlst, we randomly selected
a subset of 1045 test images coming from 420 patients with up to 3 studies. a
certiﬁed radiologist annotated the testing data by identifying the location and
size of the pulmonary nodules, resulting in a total of 825 paired annotations. we
evaluate lesion tracking in both directions, from baseline to follow-up and from
follow-up to baseline [8]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"the isotropic resolution of all ct
volumes is adjusted to 2mm through bilinear interpolation. system training: our learning model is implemented in pytorch and uses
the torchio library [13] for medical data manipulation and augmentation. we employ a u-net-based encoder-decoder architecture [2] that utilizes an
inﬂated 3d resnet-18 [3,4] as its encoder, which extends all 2d convolutions
578
a. vizitiu et al.
table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"the model is trained with adamw optimizer [6] for 64 epochs using an early
stopping strategy with a patience of 5 epochs, a batch size of 8 augmented 3d
paired patches of 32 × 96 × 96, and a learning rate of 0.0001. for data augmentation, we apply random cropping, scaling, rotation, and
gaussian noise injections. a windowing approach that covers the intensity ranges
of lungs and soft tissues is used to scale ct intensity values to [−1, 1]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"[8], denoted
with cpm@10 mm. the nlst testing dataset has a distinctive feature wherein
nodules are relatively small, 68% of annotated lesions have a radius of less than
5 mm (compared to 6% in dls dataset). to ensure that such small nodules are
not missed during evaluation, we relax the minimum distance requirement and
consider a distance of 6 mm as a permissible matching error."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"these include the deep lesion tracker
(dlt) and its variants [8], as well as registration-based trackers [15,16,21] and
appearance-based trackers via detector learning [17–20]. [5] with images
from variousct dataset. table 2. results on the nlst dataset related to the tracking of lung nodules."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"when imposing a maximum distance limit
of 10 mm between the ground truth and prediction, our method increases perfor-
mance by 1.46%, showing the importance of the multi-scale approach in lesion
fig. 2. examples of lesion matching results on the dls testing dataset. we denote the
ground-truth points using green markers in both the baseline and follow-up images,
whereas the predicted points are indicated by red markers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"qualita-
tive examples are shown in fig. 2.
on the nlst dataset, our proposed method obtains a center point matching
accuracy of 92.12% (table 2). in the case of longitudinal lung nodule tracking
(fig. 3), it is more frequent to observe signiﬁcant changes in size and density."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"our system was robust to the axial rotations of the
scans and the increasing size of the nodule and correctly established the correspondence. 4. examples of lesion matching results on clinically challenging cases from nlst
testing dataset: (a) bronchiectasis with mucus plugging adjacent to the nodule, (b) spic-
ulated nodule in a setting of interstitial lung disease, (c), (d) small nodule progressed
and increased signiﬁcantly in size. the green and red markers denote the ground-truth
and predicted lesion location."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"(color ﬁgure online)
multi-scale self-supervised learning for longitudinal lesion tracking
581
5
conclusion
in conclusion, this paper presents an eﬀective method for longitudinal lesion
tracking based on multi-scale self-supervised learning. the method is generic,
it does not require expert annotations or longitudinal data for training and
can generalize to diﬀerent types of tumors/organs/modalities. the multi-scale
approach ensures a high degree of robustness and accuracy for small lesions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_55.pdf,"in addition, we aim to expand to
more applications, e.g., treatment monitoring for brain cancer using mri. disclaimer: the concepts and information presented in this paper/presentation
are based on research results that are not commercially available. future com-
mercial availability cannot be guaranteed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf,"[9]) mainly aim to ﬁnd the key instances (e.g., patches and
tissues) for wsi representation and decision-making, which prefers the needle-in-
a-haystack tasks, e.g., cancer diagnosis, cancer subtyping, etc. to handle cancer
survival prediction, some researchers integrated some attribute priors into the
network design [5,26]. [5] treated the wsi as point
cloud data, and the patch-level adjacent relationship of wsi is learned by a
graph convolutional network (gcn). however, the ﬁxed-size patches cropped
from wsi mainly contain single-level biological entities (e.g., cells), resulting
in limited structural information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf,"therefore, wsi-based cancer survival prediction still remains a chal-
lenging task. in summary, to better capture the prognosis-related information in wsi,
two technical key points should be fully investigated: (1) an analysis strategy
to mine more comprehensive and in-depth prior of wsis, and (2) a promising
learning network to explore the contextual interactions of pathological com-
ponents. to this end, this paper presents a novel multi-scope analysis driven
learning framework, called hierarchical graph transformer (hgt), to perti-
nently resolve the above technical key points for more reliable and interpretable
w. hou and y. he—contributed equally to this work."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf,"however, the conventional patch-level anal-
ysis is diﬃcult to meet this requirement. therefore, it is essential to extract and
combine higher-level topology information for better wsi representation. 748
w. hou et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf,"[10], etc.
tissue graph convolutional layer. third, based on the spatial assignment
matrix aspa, the learned patch-level features can be aggregated to the tissue-level
hierarchical graph transformer
749
features which contain the information of necrosis, epithelium, etc. [aspa]tv
′
patch,
(2)
where [·]t denote the matrix transpose operation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf,"formally, the feature embed-
ding of each tissue in space d is deﬁned as the mean feature embeddings of the
patches within the tissue. and then, the pathological component label of each
tissue is determined as the component closest to the euclidean distance of the
tissue in space d. the semantic assignment matrix between segmented tissues
and pathological components is denoted as asem ∈ rm×k."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf,"finally, the representation of the regression token at the output layer of the
transformer, i.e., [p ′
out]0, is served as the predicted survival risk o.
750
w. hou et al.
loss function and training strategy. ⎞
⎠ ,
(6)
where δi denote the censorship of i-th patient, o(i) and o(j) denote the survival
output of i-th and j-th patient in a batch, respectively.
3
experiments
3.1
experimental settings
dataset. in this study, we used a colorectal cancer (crc)(385 cases)
cohort collected from co-operated hospital to evaluate the proposed method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf,"generally, most mil methods, i.e., deepsets, abmil, dsmil,
transmil mainly focus on a few key instances for prediction, but they do
not have signiﬁcant advantages in cancer prognosis. furthermore, due to the
large size of crc dataset and relatively high model complexity, patch-gcn and
transmil encountered a memory overﬂow when processing the crc dataset,
which limits their clinical application. deepattnmisl has a certain semantic
perception ability for patch, which achieves better performance in lihc cohort."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf,"our
method achieves higher ci and relatively low p-value (< 0.05) of km analysis
on both three cancer cohorts, which consistently outperform the sotas and
baselines. in addition, the feature aggregation of the lower levels (i.e., patch and
tissue) are guided by the priors, and the mhsa is only executed on pathological
components, resulting in high eﬃciency even on the crc dataset. experimental results of ci. results not signiﬁcantly worse than the best
(p-value > 0.05, two-sample t-test) are shown in bold."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf,"“-” denotes that the algorithm cannot be executed in
this cohort due to memory overﬂow. w/o transformer
0.592 ± 0.010
0.647 ± 0.002
0.616 ± 0.005
ours
hgt
0.607 ± 0.004 0.657 ± 0.003 0.646 ± 0.003
752
w. hou et al.
3.3
interpretability of the proposed framework
we selected the crc dataset for further interpretable analysis, as it is one of the
leading causes of mortality in industrialized countries, and its prognosis-related
factors have been widely studied [3,8]. background (back); debris (deb); lymphocytes (lym); mucus
(muc); muscle (mus); normal colon mucosa (norm); stroma (str); tumor
(tum)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf,"figure 3 shows the original image, spatial topology, proportion and
fig. 2. km analysis of second best sota method and our proposed framework for
diﬀerent datasets. all the patients across the ﬁve test folds are combined and analysis
here."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_72.pdf,"hierarchical graph transformer
753
biological meaning of pathological components, and its contextual interactions
of a typical case from crc cohort. it can be seen that the interaction between
component 1 (tum) and component 9 (str) has gained the highest attention
of the network, which is consistent with the existing knowledge [3,8]. moreover,
there is also concentration of interaction in some other interactions, which may
potentially imply some new biomarkers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"speciﬁcally,
we propose a hierarchical multi-task multi-instance learning framework
to jointly predict histology and molecular markers. moreover, we pro-
pose a co-occurrence probability-based label correction graph network
to model the co-occurrence of molecular markers. lastly, we design an
inter-omic interaction strategy with the dynamical conﬁdence constraint
loss to model the interactions of histology and molecular markers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"our
experiments show that our method outperforms other state-of-the-art
methods in classifying diﬀuse glioma, as well as related histology and
molecular markers on a multi-institutional dataset. keywords: diﬀuse glioma · digital pathology · multi-task learning ·
muti-label classiﬁcation
1
introduction
diﬀuse glioma is the most common and aggressive primary brain tumors in
adults, accounting for more deaths than any other type [7]. pathology diagnosis
is the gold standard for diﬀuse glioma but is usually time-consuming and highly
depends on the expertise of senior pathologists [13]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"[15], namely digital pathology,
promise to oﬀer rapid diagnosis and aid precise treatment. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_52.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43990-2_52
552
x. wang et al.
recently, deep learning has achieved success in diagnosing various tumors
[2,21]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"moreover, multiple molecular markers are needed for classifying cancers, due
to complex tumor biology. to reﬂect real-world clinical scenarios, we formu-
late predicting multiple molecular markers as a multi-label classiﬁcation (mlc)
task. previous mlc methods have successfully modeled the correlation among
labels [12,22]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"despite success, when applied
to predicting multiple molecular markers, most existing methods may ignore
the co-occurrence of molecular markers, which have intrinsic associations [23]. hence, we propose a co-occurrence probability-based, label-correlation graph
(cplc-graph) network to model the co-occurrence of molecular markers, i.e.,
intra-omic relationship. lastly, we focus on modeling the interaction between molecular markers and
histology."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"particularly, we design a dynamical conﬁdence constraint (dcc) loss that con-
strains the model to focus on similar areas of wsis for both tasks. to the best of
our knowledge, this is the ﬁrst attempt to classify diﬀuse gliomas via modeling
the interaction of histology and molecular markers. our main contributions are: (1) we propose a multi-task multi-instance learn-
ing framework to jointly predict molecular markers and histology and ﬁnally
classify diﬀuse glioma, reﬂecting the new paradigm of pathology diagnosis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"(3) we design a dcc learning strategy to model the
554
x. wang et al.
inter-omic interaction between histology and molecular markers for glioma clas-
siﬁcation. 2
preliminaries
database: we use publicly available tcga gbm-lgg dataset [6]. following
[15], we remove the wsis of low quality or lack of labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"totally, we include
2,633 wsis from 940 cases, randomly split into training (2,087 wsis of 752
cases), validation (282 wsis of 94 cases) and test (264 wsis of 94 cases) sets. all the wsis are crop into patches of size 224 px × 224 px at 0.5 µm px−1.
training labels: original lables for genomic markers and histology of wsis
are obtained from tcga database [6]. according to the up-to-date who cri-
teria [14], we generate the classiﬁcation labels for each case as grade 4 glioblas-
toma (deﬁned as idh widetype), oligodendroglioma (deﬁned as idh mutant and
1p/19q co-deletion), grade 4 astrocytoma (deﬁned as idh mutant, 1p/19q non
co-deletion with cdkn homdel or nmp), or low-grade astrocytoma (other
cases)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"of note, the ﬁnal diagnosis of diﬀuse gliomas is generated
via a decision tree-based logical function with the input of predicted molecular
markers and histology, consistent with the up-to-date who criteria. 3.1
hierarchical multi-task multi-instance learning
to extract global information from input {xi}n
1 , we propose a hierarchical
multi-task multi-instance learning (hmt-mil) framework for both histology and
molecular marker predictions. note for wsis with patch number< n,
we adopt a biological repeat strategy for dimension alignment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"multi-task learning for classifying diﬀuse glioma
555
fig. 2. pipelines of cplc-graph network and lc loss.
3.2
co-occurrence probability-based, label-correlation graph
in predicting molecular markers, i.e., idh, 1p/19q and cdkn, existing mlc
methods based on label correlation may ignore the co-occurrence of the labels. in
the genomic marker prediction module, we proposed a co-occurrence probability-
based, label-correlation graph (cplc-graph) network and a label correlation
(lc) loss for intra-omic modeling of the co-occurrence probability of the three
markers.
1) cplc-graph network: cplc-graph (fig. 2) is deﬁned as g = (v, e),
where v indicates the nodes, while e represents the edges."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"in this paper, we aim to classify diﬀuse gliomas
under up-to-date diagnosis criteria, via jointly learning the tasks of molecular
marker prediction and histology classiﬁcation. inputting histology wsis, our
model incorporates a novel hmt-mil framework to extract global information
for both predicting both molecular markers and histology. we also design a
cplc-graph network and a dcc loss to model both intra-omic and inter-omic
interactions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"kingma, d.p., ba, j.: adam: a method for stochastic optimization. li, x., wu, h., li, m., liu, h.: multi-label video classiﬁcation via coupling atten-
tional multiple instance learning with label relation graph. liang, s., et al.: clinical practice guidelines for the diagnosis and treatment of
adult diﬀuse glioma-related epilepsy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"louis, d.n., et al.: the 2021 who classiﬁcation of tumors of the central nervous
system: a summary. lu, m.y., williamson, d.f., chen, t.y., chen, r.j., barbieri, m., mahmood,
f.: data-eﬃcient and weakly supervised computational pathology on whole-slide
images. shao, z., bian, h., chen, y., wang, y., zhang, j., ji, x., et al.: transmil: trans-
former based correlated multiple instance learning for whole slide image classiﬁca-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"volante, m., lam, a.k., papotti, m., tallini, g.: molecular pathology of poorly
diﬀerentiated and anaplastic thyroid cancer: what do pathologists need to know?
endocr. pathol. xing, x., chen, z., zhu, m., hou, y., gao, z., yuan, y.: discrepancy and gradient-
guided multi-modal knowledge distillation for pathological glioma grading. deep learning-based six-type classiﬁer for lung cancer and mimics
from histopathological whole slide images: a retrospective study."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_52.pdf,"zhang, l., wei, y., fu, y., price, s., schönlieb, c.b., li, c.: mutual contrastive
low-rank learning to disentangle whole slide image representations for glioma grad-
ing. zhang, y., luo, l., dou, q., heng, p.a.: triplet attention and dual-pool contrastive
learning for clinic-driven multi-label medical image classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"the medical imaging community generates a wealth of data-
sets, many of which are openly accessible and annotated for speciﬁc
diseases and tasks such as multi-organ or lesion segmentation. current
practices continue to limit model training and supervised pre-training
to one or a few similar datasets, neglecting the synergistic potential
of other available annotated data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"we propose multitalent, a method
that leverages multiple ct datasets with diverse and conﬂicting class
deﬁnitions to train a single model for a comprehensive structure seg-
mentation. our results demonstrate improved segmentation performance
compared to previous related approaches, systematically, also compared
to single-dataset training using state-of-the-art methods, especially for
lesion segmentation and other challenging structures. we show that mul-
titalent also represents a powerful foundation model that oﬀers a supe-
rior pre-training for various segmentation tasks compared to commonly
used supervised or unsupervised pre-training baselines."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"the code and model weights will be published here: https://github.com/
mic-dkfz/multitalent. multitask learning ·
transfer learning · foundation model · partially labeled datasets
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_62.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43898-1_62
multitalent: a multi-dataset approach to medical image segmentation
649
1
introduction
the success of deep neural networks heavily relies on the availability of large
and diverse annotated datasets across a range of computer vision tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"thus, depending on the task, only a bare minimum of images and
target structures is usually annotated. this results in a situation where a zoo
of partially labeled datasets is available to the community. recent eﬀorts have
resulted in a large dataset of >1000 ct images with >100 annotated classes
each, thus providing more than 100,000 manual annotations which can be used
for pre-training [30]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"focusing on such a dataset prevents leveraging the poten-
tially precious additional information of the above mentioned other datasets that
are only partially annotated. integrating information across diﬀerent datasets
potentially yields a higher variety in image acquisition protocols, more anatom-
ical target structures or details about them as well as information on diﬀerent
kinds of pathologies. consequently, recent advances in the ﬁeld allowed utilizing
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"1. (a) usually only a few classes are annotated in publicly available datasets. b)
diﬀerent groundtruth label properties can generate contradicting class predictions. for
example, the heart annotation of dataset 11 diﬀers from the heart annotation of dataset
10, which causes the aorta of dataset 11 to overlap with the heart of dataset 10."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"in
contrast to dataset 11, in dataset 7 the aorta is also annotated in the lower abdomen. c) instead of training one network for each dataset, we introduce a method to train
one network with all datasets, while retaining dataset-speciﬁc annotation protocols. [5,27] and penalizing overlapping predictions
by taking advantage of the fact that organs are mutually exclusive [7,28]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"some
other methods only predicted one structure of interest for each forward pass by
incorporating the class information at diﬀerent stages of the network [4,22,31]. chen et al. trained one network with a shared encoder and separate decoders for
each dataset to generate a generalized encoder for transfer learning [2]. however,
most approaches are primarily geared towards multi-organ segmentation as they
do not support overlapping target structures, like vessels or cancer classes within
an organ [6,8,12,23]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"so far, all previous methods do not convincingly leverage
cross-dataset synergies. as liu et al. pointed out, one common caveat is that
many methods force the resulting model to average between distinct annota-
tion protocol characteristics [22] by combining labels from diﬀerent datasets for
the same target structure (visualized in fig. hence, they all fail to reach
segmentation performance on par with cutting-edge single dataset segmentation
methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"2) it retains diﬀerent annotation protocol characteristics for
the same target structure and 3) allows for overlapping target structures with
diﬀerent level of detail such as liver, liver vessel and liver tumor. overall, mul-
titalent can include all kinds of new datasets irrespective of their annotated
target structures. multitalent can be used in two scenarios: first, in a combined multi-dataset
(md) training to generate one foundation segmentation model that is able to
predict all classes that are present in any of the utilized partially annotated
datasets, and second, for pre-training to leverage the learned representation of
this foundation model for a new task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"inter-
estingly, the beneﬁts of multitalent are particularly notable for more diﬃcult
classes and pathologies. in comparison to an ensemble of single dataset solutions,
multitalent comes with shorter training and inference times. additionally, at the example of three challenging datasets, we demonstrate
that ﬁne-tuning multitalent yields higher segmentation performance than train-
ing from scratch or initializing the model parameters using unsupervised pre-
training strategies [29,33]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"it also surpasses supervised pretrained and ﬁne-tuned
state-of-the art models on most tasks, despite requiring orders of magnitude less
annotations during pre-training. 2
methods
we introduce multitalent, a multi dataset learning and pre-training method,
to train a foundation medical image segmentation model. it comes with a novel
multitalent: a multi-dataset approach to medical image segmentation
651
dataset and class adaptive loss function."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"the proposed network architecture
enables the preservation of all label properties, learning overlapping classes and
the simultaneous prediction of all classes. [1, i], is assigned to one class c ∈ c(k), where c(k) ⊆ c is
the label set associated to dataset d(k). even if classes from diﬀerent datasets
refer to the same target structure we consider them as unique, since the exact
annotation protocols and labeling characteristics of the annotations are unknown
and can vary between datasets: c(k) ∩ c(j) = ∅, ∀k ̸= j. this implies that the
network must be capable of predicting multiple classes for one voxel to account
for the inconsistent class deﬁnitions.
2.2
multitalent
network modiﬁcations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"we employ three diﬀerent network architectures,
which are further described below, to demonstrate that our approach is applica-
ble to any network topology. to solve the label contradiction problem we decou-
ple the segmentation outputs for each class by applying a sigmoid activation
function instead of the commonly used softmax activation function across the
dataset. but it has indepen-
dent segmentation head parameters θc for each class."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"the sigmoid probabilities
for each class are deﬁned as ˆyc = f(x, θ, θc). this modiﬁcation allows the net-
work to assign multiple classes to one pixel and thus enables overlapping classes
and the conservation of all label properties from each dataset. consequently, the
segmentation of each class can be thought of as a binary segmentation task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"hence, the loss signal for each class prediction does not
depend on the number of other classes within the batch. this compensates for
the varying number of annotated classes in each dataset. otherwise, the magni-
tude of the loss e.g. for the liver head from d1 (2 classes) would be much larger
as for d7 (13 classes)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"we implemented our approach in the nnu-net framework
[13]. however, the automatic pipeline conﬁguration from nnu-net was not used
in favor of a manually deﬁned conﬁguration that aims to reﬂect the peculiarities
of each of the datasets, irrespective of the number of training cases they contain. we manually selected a patch size of [96, 192, 192] and image spacing of 1mm in
plane and 1.5mm for the axial slice thickness, which nnu-net used to automati-
cally create the two cnn network topologies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"for the swinunetr, we adopted
the default network topology. multi-dataset training setup. detailed information
about the datasets, can be found in the appendix in table 3 and fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"we increased the batch size to 4 and
the number of training epochs to 2000 to account for the high number of train-
ing images. to compensate for the varying number of training images in each
dataset, we choose a sampling probability per case that is inversely proportional
to √n, where n is the number of training cases in the corresponding source
dataset. apart from that, we have adopted all established design choices from
nnu-net to ensure reproducibility and comparability."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"transfer learning setup. we used the btcv (small multi organ dataset
[19]), amos (large multi organ dataset [16]) and kits19 (pathology dataset
[11]) datasets to evaluate the generalizability of the multitalent features in a
pre-training and ﬁne tuning setting. naturally, the target datasets were excluded
multitalent: a multi-dataset approach to medical image segmentation
653
from the respective pre-training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"finally, we
continued with the standard nnu-net training schedule. 2.3
baselines
as a baseline for the multitalent, we applied the 3d u-net generated by the
nnu-net without manual intervention to each dataset individually. furthermore,
we trained a 3d u-net, a resenc u-net and a swinunetr with the same
network topology, patch and batch size as our multitalent for each dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"we used the same patch
size, image spacing, batch size and number of epochs as for the multitalent
training. as unsupervised baseline for the cnns, we pre-trained the networks
on the multi-dataset collection based on the work of zhou et al. (model gen-
esis [33]). finally, for the swinunetr architecture, we compared the utility
of the weights from our multitalent with the ones provided by tan et al. who
performed self-supervised pre-training on 5050 ct images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"to ensure fair comparability, we did not scale
up any models. despite using gradient checkpointing, the swinunetr models
requires roughly 30 gb of gpu memory, compared to less than 17 gb for the
cnns.
3
results
multi-dataset training results are presented in fig. 2. in general, the convo-
lutional architectures clearly outperform the transformer-inspired swinunetr."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"multitalent improves the performance of the purely convolutional architectures
(u-net and resenc u-net) and outperforms the corresponding baseline models
that were trained on each dataset individually. since a simple average over all
classes would introduce a biased perception due to the highly varying numbers
of images and classes, we additionally report an average over all datasets. for
example, dataset 7 consists of only 30 training images but has 13 classes, whereas
654
c. ulrich et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"diﬃcult
classes are those for which the default nnu-net has a dice below 75. the same color
indicates the same architecture and the pattern implies training with multiple datasets
using multitalent. the mean dices are written on the figure."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"table 4 in the appendix pro-
vides all results for all classes. averaged over all datasets, the multitalent gains
1.26 dice points for the resenc u-net architecture and 1.05 dice points for the u-
net architecture. compared to the default nnu-net, conﬁgured without manual
intervention for each dataset, the improvements are 1.56 and 0.84 dice points."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"both class groups, but espe-
cially the cancer classes, experience notable performance improvements from
multitalent. for the oﬃcial btcv test set in table 1, multitalent outperforms
all related work that have also incorporated multiple datasets during training,
proving that multitalent is substantially superior to related approaches. the
advantages of multitalent include not only better segmentation results, but also
considerable time savings for training and inference due to the simultaneous pre-
diction of all classes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"the training is 6.5 times faster and the inference is around
13 times faster than an ensemble of models trained on 13 datasets. transfer learning results are found in table 2, which compares the ﬁne-
tuned 5-fold cross-validation results of diﬀerent pre-training strategies for three
diﬀerent models on three datasets. the multitalent pre-training is highly ben-
eﬁcial for the convolutional models and outperforms all unsupervised baselines."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"although multitalent was trained with a substantially lower amount of manually
multitalent: a multi-dataset approach to medical image segmentation
655
annotated structures (˜3600 vs. ˜105 annotations), it also exceeds the supervised
pre-training baseline. especially for the small multi-organ dataset, which only
has 30 training images (btcv), and for the kidney tumor (kits19), the multi-
talent pre-training boosts the segmentation results. in general, the results show
that supervised pre-training can be beneﬁcial for the swinunetr as well, but
pre-training on the large totalsegmentator dataset works better than the md
pre-training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"for the amos dataset, no pre-training scheme has a substantial
impact on the performance. we suspect that it is a result of the dataset being
saturated due to its large number of training cases. the resenc u-net pre-
trained with multitalent, sets a new state-of-the-art on the btcv leaderboard1
(table 1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"oﬃcial btcv test set leaderboard results, dice and 95% hausdorﬀ distance. * indicates usage of multiple datasets. we submitted both a 5-fold cv ensemble and a
single model to improve comparability to related methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"method
# models
avg. [32]
single model
84.97
18.47
multitalent resenc u-net*
single model
88.82
16.35
multitalent resenc u-net*
5-fold ensemble 88.91
14.68
resenc u-net (pre-trained multitalent*) 5-fold ensemble 89.07
15.01
4
discussion
multitalent demonstrates the remarkable potential of utilizing multiple pub-
licly available partially labeled datasets to train a foundation medical segmen-
tation network, that is highly beneﬁcial for pre-training and ﬁnetuning various
segmentation tasks. multitalent surpasses state-of-the-art single-dataset models
and outperforms related work for multi dataset training, while retaining conﬂict-
ing annotation protocol properties from each dataset and allowing overlapping
classes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"in the transfer
learning setting, the feature representations learned by multitalent boost seg-
mentation performance and set a new state-of-the-art on the btcv leaderboard. the nature of multitalent imposes no restrictions on additional datasets, which
1 assuming that no additional private data from the same data domain has been used. 656
c. ulrich et al.
table 2."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"p
kidney dice p
tumor dice p
swinunetr
from scratch
74.27
86.04
87.69
46.56
org. 84.92
0.03
89.81
0.16
96.89
0.04 84.01
0.12
allows including any publicly available datasets (e.g. amos and totalsegmen-
tator). this paves the way towards holistic whole body segmentation model that
is even capable of handling pathologies.
acknowledgements."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"med3d: transfer learning for 3d medical image anal-
ysis. arxiv:1904.00625 (2019)
3. clark, k., et al.: the cancer imaging archive (tcia): maintaining and operating
a public information repository. imaging 26, 1045–1057 (2013)
4. dmitriev, k., kaufman, a.e.: learning multi-class segmentations from single-class
datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"ieee trans. med. ms-kd: multi-organ segmen-
tation with multiple binary-labeled datasets. arxiv:2108.02559 (2021)
7. fidon, l., et al.: label-set loss functions for partial supervision: application to fetal
brain 3d mri parcellation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"in: de bruijne, m., et al. (eds.) miccai 2021. learning
from partially overlapping labels: image segmentation under annotation shift. https://doi.org/10.1007/978-3-030-87722-4_12
multitalent: a multi-dataset approach to medical image segmentation
657
9."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"in: proceedings of the ieee/cvf winter conference on applications of computer
vision (wacv), pp. heller, n., et al.: the kits19 challenge data: 300 kidney tumor cases with clini-
cal context, ct semantic segmentations, and surgical outcomes. huang, r., zheng, y., hu, z., zhang, s., li, h.: multi-organ segmentation via co-
training weight-averaged models from few-organ datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"segthor: segmentation of tho-
racic organs at risk in ct images. cai multi-atlas labeling beyond the cranial vault-workshop and challenge (2015). li, h., zhou, j., deng, j., chen, m.: automatic structure segmentation for radio-
therapy planning challenge (2019)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"liu, j., et al.: clip-driven universal model for organ segmentation and tumor detec-
tion. liu, p., zheng, g.: context-aware voxel-wise contrastive learning for label eﬃcient
multi-organ segmentation. medical image computing and computer assisted intervention – miccai
2022."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"roth, h.r., et al.: deeporgan: multi-level deep convolutional networks for auto-
mated pancreas segmentation. roulet, n., slezak, d.f., ferrante, e.: joint learning of brain lesion and anatomy
segmentation from heterogeneous datasets. in: proceedings of the 2nd interna-
tional conference on medical imaging with deep learning (2019)
28. shi, g., xiao, l., chen, y., zhou, s.k.: marginal loss and exclusion loss for partially
supervised multi-organ segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_62.pdf,"30. wasserthal, j., meyer, m., breit, h.c., cyriac, j., yang, s., segeroth, m.:
totalsegmentator: robust segmentation of 104 anatomical structures in ct images.
arxiv:2208.05868 (2022)
31. dodnet: learning to segment multi-organ and
tumors from multiple partially labeled datasets. prior-aware neural network for partially-supervised multi-organ
segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"therefore, magnetic resonance (mr)
imaging has been recommended to enhance the segmentation of soft tis-
sue oars in the han region. based on our two empirical observations
that deformable registration of ct and mr images of the same patient
is inherently imperfect and that concatenating such images at the input
layer of a deep learning network cannot optimally exploit the information
provided by the mr modality, we propose a novel modality fusion mod-
ule (mfm) that learns to spatially align mr-based feature maps before
fusing them with ct-based feature maps. the proposed mfm can be
easily implemented into any existing multimodal backbone network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"fur-
thermore, the evaluation on a clinically realistic scenario with the missing
mr modality shows that mfm outperforms other state-of-the-art mul-
timodal approaches. keywords: multimodal segmentation · head and neck ·
organs-at-risk · computed tomography · magnetic resonance ·
nnu-net
1
introduction
head and neck (han) cancer is a prevalent type of cancer [3] with a yearly inci-
dence of above 1 million cases and prevalence of above 4 million cases worldwide,
accounting for around 5% of all cancer sites [17]. radiotherapy (rt) is a standard
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 71.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. [21]. to
optimize radiation dose distribution, accurate three-dimensional (3d) segmen-
tation of target volumes and oars is required."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"this
naturally poses a question of whether automatic oar segmentation can beneﬁt
from the mr image modality. our study therefore aims to evaluate the impact of
mr integration on the quality and robustness of automatic oar segmentation
in the han region, therefore contributing to the growing body of research on
multimodal methods for medical image analysis. a literature review by zhang et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"similar conclusions were reached in a review of multimodal seg-
mentation methods in the medical imaging community by zhou et al. most
methods implement either early or late fusion, however, the layer fusion strat-
egy was identiﬁed as a better choice, since dense connections among layers can
exploit more complex and complementary information to enhance training. the
highlight is hyperdensenet, a dual-path 3d network proposed by dolz et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"consequently, our primary
focus is the paired multimodal segmentation problem, including the missing
modality scenario.
motivation. when segmenting oars in the han region for the purpose of rt
planning, a multimodal segmentation model that can leverage the information
from ct and mr images of the same patient might be beneﬁcial compared to
separate single-modal models. firstly, as intuition suggests, such a model would
rely on the ct image for bone structures and on the mr image for soft tissues,
and therefore improve the overall segmentation quality by exploiting the comple-
mentary information from both modalities."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"secondly, a multimodal model would
facilitate cross-modality learning by extracting knowledge from one and applying
that knowledge to the other modality, potentially improving the segmentation
accuracy. several studies indicated that such an approach is feasible, for exam-
ple, for improving video classiﬁcation by training a model on an auxiliary audio
reconstruction task [12], or for audio-based detection by using the multimodal
knowledge distillation concept, where teacher networks trained on rgb, depth
and thermal images improve a student network trained only on audio data [20].
finally, from the dl infrastructure maintenance perspective, it is easier to main-
tain a single model that can handle both modalities than two separate models
for each modality. however, clinical practice diﬀers considerably from theory,
meaning that a number of considerations must be taken into account."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"firstly,
although mr image acquisition is recommended, it is not always feasible due to
time constraints, scanner occupancy and ﬁnancial aspects. consequently, auto-
matic oar multimodal segmentation is required to handle the missing modality
scenario, and provide a similar segmentation quality as a single-modality system. secondly, because ct and mr images are not acquired simultaneously and with
the same acquisition parameters (e.g. resolution), there is an inherent misalign-
ment between both modalities."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"to tackle these considerations, we propose a mechanism named
modality fusion module (mfm) that can generally be applied to any network
architecture that learns features from multiple modalities, and shows promising
performance also in the missing modality scenario. the advantages of the pro-
posed mfm are the following: 1) it enables the spatial alignment of fms from
one with fms from the other modality to further reduce errors that persist after
deformable registration of input images, and enrich the fms to improve the ﬁnal
oar segmentation, 2) it signiﬁcantly improves the performance of the missing
modality scenario compared to other baseline fusion approaches, and 3) it per-
forms well also on single modality out-of-distribution data, therefore facilitating
cross-modality learning and contributing to better model generalizability. 2
methods
backbone architecture."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"[9], who introduced a spatial transformer network (stn)
that learns to infer transformation parameters in a single forward pass, and then
uses them to transform images and/or fms. the fundamental idea is that stn
can learn meaningful features that are spatially invariant to characteristics of
the input data, without the need for extra supervision, thereby enhancing task
multimodal ct and mr segmentation of han oars
749
fig. the proposed backbone architecture is based on nnu-net but with separate
encoders for the computed tomography (ct) and magnetic resonance (mr) image,
and with the proposed modality fusion module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"we evaluate the performance of the proposed mfm
nnu-net against three baseline networks: 1) a single modality nnu-net trained
only on ct images, 2) a nnu-net trained on concatenated ct and mr image
pairs, and 3) a model with separate encoders for both modalities, but with a
simple concatenation along the channel axis instead of the proposed mfm. [23].
3
experiments and results
image datasets. the han-seg dataset comprises ct and t1-weighted mr images of
56 patients, which were deformably registered with the simpleelastix registra-
tion tool, and corresponding curated manual delineations of 30 oars (for details,
please refer to [14])."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"although only a subset of images is publicly available1 due
to the ongoing han-seg challenge2, both the publicly available training as well
as the privately withheld test images were used in our 4-fold cross-validation
experiments. on the other hand, to evaluate the generalization ability of our
method, we also conducted experiments on the ct-only pddca dataset (for
details, please refer to [15]), from which we collected 15 images from the oﬀ-
and on-site test sets of the corresponding challenge for our evaluation. as this
dataset is widely used for evaluating the performance of automatic han oar
segmentation methods, it serves as a valuable benchmark for comparison with
other state-of-the-art methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"multimodal ct and mr segmentation of han oars
751
fig. 2. the results in terms of the dice similarity coeﬃcient (dsc) for the han-seg
(left) and pddca (right) dataset.
fig. the results in terms of the 95th-percentile hausdorﬀ distance (hd95) for
the han-seg (left) and pddca (right) dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"note that the maml model, which is composed of two u-nets, had a
considerably higher number of parameters. to address the challenge of a rela-
tively small dataset, we adopted a 4-fold cross-validation strategy without using
any external training images. all models were trained until convergence, i.e.
when the validation loss plateaued, and we selected the model with the best
validation loss for inference.
results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"we also performed analysis of statisti-
cal signiﬁcance by applying paired sample t-tests with the bonferroni correction,
presented with bars on top of the box plots (non-signiﬁcant: ns (p > 0.05), sig-
niﬁcant: ∗ (0.01 < p < 0.05), ∗∗ (0.001 < p < 0.01), ∗ ∗ ∗ (0.0001 < p < 0.001)
and ∗ ∗ ∗∗ (p < 0.0001)). 4
discussion
in this study, we evaluated the impact on the quality and robustness of auto-
matic oar segmentation in the han region caused by the incorporation of the
mr modality into the segmentation framework. we devised a mechanism named
mfm and combined it with nnu-net as our backbone segmentation network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_71.pdf,"missing modality scenario. the overall good performance on the han-seg
dataset suggests that all models are close to the maximal performance, which is
bounded by the quality of reference segmentations. however, the performance
on the pddca dataset that consists only of ct images allows us to test how
the models handle the missing modality scenario and perform on an out-of-
distribution dataset, as images from this dataset were not used for training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"the success of large-scale pre-trained vision-language models
(vlm) has provided a promising direction of transferring natural image
representations to the medical domain by providing a well-designed
prompt with medical expert-level knowledge. however, one prompt has
diﬃculty in describing the medical lesions thoroughly enough and con-
taining all the attributes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"in this paper,
we propose an ensemble guided fusion approach to leverage multiple
statements when tackling the phrase grounding task for zero-shot lesion
detection. extensive experiments are conducted on three public medical
image datasets across diﬀerent modalities and the detection accuracy
improvement demonstrates the superiority of our method. keywords: vision-language models · lesion detection · multiple
prompts · prompt fusion · ensemble learning
1
introduction
medical lesion detection plays an important role in assisting doctors with the
interpretation of medical images for disease diagnosing, cancer staging, etc.,
which can improve eﬃciency and reduce human errors [9,19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"current object
detection approaches are mainly based on supervised learning with abundant
well-paired image-level annotations, which heavily rely on expert-level knowl-
edge. as such, these supervised approaches may not be suitable for medical
lesion detection due to the laborious labeling. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_28.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"therefore, sub-
stituting conventional object detection with vlms is possible and necessary. the
vlms are ﬁrst pre-trained to learn universal representations via large-scale unla-
belled data and can be eﬀectively transferred to downstream tasks. for example,
a recent study [15] has demonstrated that the pre-trained vlms can be used for
zero-shot medical lesion detection with the help of well-designed prompts."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"this prompt needs reﬁning to cover all
the features of the target as much as possible. in addition, each
keyword in a single lengthy prompt cannot take eﬀect equally as we expect,
where the essential information can be ignored. this problem motivates us to
study alternative approaches with multiple prompt fusion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"glip [11] uniﬁes phrase grounding and object detection tasks,
demonstrating outstanding transfer capability. in addition, vild [7] is proposed
for open-vocabulary object detection taking advantage of the rich knowledge
learned from clip [4] and text input. as pointed out by a review [3], ensemble learning meth-
ods achieve better performance by producing predictions based on extracted fea-
tures and fusing via various voting mechanisms."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"for example, a selective ensem-
ble of classiﬁer chains [13] is proposed to reduce the computational cost and the
storage cost arose in multi-label learning [12] by decreasing the ensemble size. undeed [23], a semi-supervised classiﬁcation method, is presented to increase
the classiﬁer accuracy on labeled data and diversity on unlabeled data simulta-
neously. [22] is also
proposed to generate basic clustering partitions with prior knowledge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"(2)
3.3
ensemble learning based fusion
although the syntax based fusion approach is simple and suﬃcient, it is restricted
by the form of text descriptions which may cause ambiguity in the fused prompt
during processing. moreover, the fused prompts are normally too long that the
model could lose proper attention to the key information, resulting in extremely
unstable performance (results shown in sect. therefore, in this subsection, we further explore fusion approaches based
on ensemble learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"we ﬁnd in our preliminary experiments
that direct concatenation of the candidates is not satisfactory and eﬀective, since
simply integration hardly screens out the bad predictions. in addition, the can-
didate, e.g., cij ∈ ci, carries richer information that can be further utilized, such
multiple prompt fusion for zero-shot lesion detection
287
as central coordinate xj and yj, region size wj and hj, category label, and pre-
diction conﬁdence score. therefore, we consider step-wise clustering mechanisms
using the above information to screen out the implausible candidates based on
clustering ensemble learning [3].
another observation in our preliminary experiments is that most of the can-
didates distribute near the target if the prompt description matches better with
the object."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"accordingly, we rearrange these
candidates ci into a new set c for the subsequent fusion process. [21], and blood cell detection dataset bccd to validate our proposed app-
roach for zero-shot medical lesion detection. for the experiments, we use the
glip-t variant [11] as our base pre-trained model and adopt two metrics for
the grounding evaluation, including average precision (ap) and ap50."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"[20] for compar-
isons. [11]
with single prompt and other fusion baselines across all datasets. the ﬁrst three
rows in table 1 represent the results of single prompt by only providing shape,
color, and location information, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"[17] and our method on cvc-300 under 10-shot settings. table 2 shows that our method outperforms yolov5, which indicates fully-
supervised models such as yolo may not be suitable for medical scenarios
where a large labeled dataset is often not available. [25] method to generate prompts."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"zero-shot v.s. 10-shot results. dataset
isic 2016 cvc-300
zero-shot 19.8
36.1
10-shot
38.2
50.2
multiple prompt fusion for zero-shot lesion detection
289
fig. 2. fine-tuning v.s. zero-shot results on the isic 2016 dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"here we present part of the single prompts used in the experiments
for illustration. the misclassiﬁcation problem in some of the single prompts is corrected
(i.e., malignant to benign) on the ﬁrst dataset. for all datasets, the candidate boxes
are more precise and associated with higher conﬁdence scores."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"as shown in table 3 and fig. 2, with the same group
of multiple prompts, the accuracy of ﬁne-tuned model has increased almost twice
as much as that of zero-shot, further demonstrating the eﬀectiveness of our
290
m. guo et al.
table 4. results with diﬀerent fusion strategies.
strategy
dataset
isic 2016
cvc-300
bccd
ap
ap50
ap
ap50
ap
ap50
equally
16.8
25.2
30.8
40.4
12.5
21.6
category
13.2
20.4
30.8
40.4
15.3
24.9
attribute
19.8
30.9
36.1
47.9
15.8
32.6
method in both settings. therefore, we can conclude that the pre-trained glip
model has the ability to learn a reasonable alignment between textual and visual
modalities in medical domains.
visualizations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"on the contrary, our approach consistently gives a better prediction that defeats
all single prompts with a relatively proper description, yet syntax based fusion
relies too much on the format and content of inputs, which results in great
variance and uninterpretability. the step-wise clustering mechanism based on
ensemble learning enables our method to exploit multi-dimensional information
besides visual features. in addition, the key components in our proposed app-
roach are unsupervised, which also enhances stability and generalization.
comparison among fusion strategies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"the ﬁrst strategy is to process each
prompt equally, which is the most convenient and suitable in any situation. ing prompts by category is speciﬁcally for multi-category datasets to ﬁrst gather
the prompts belonging to the same category and make further fusion. similarly,
fusing by attribute is to fuse the candidates, whose prompts are describing the
same attribute."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_28.pdf,"on the contrary, it is
possible to neglect this distribution when fusing each prompt equally. as shown in table 5, we perform ablation studies on three
datasets. our approach has three key components, i.e., location cluster, size clus-
ter and prediction corrector."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"nuclei seg-
mentation, for instance, is an important task for diagnosing diﬀerent
cancers. however, training deep learning models for nuclei segmentation
requires large amounts of annotated data, which is expensive to col-
lect and label. this necessitates explorations into generative modeling of
histopathological images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"in this work, we use recent advances in con-
ditional diﬀusion modeling to formulate a ﬁrst-of-its-kind nuclei-aware
semantic tissue generation framework (nasdm) which can synthesize
realistic tissue samples given a semantic instance mask of up to six dif-
ferent nuclei types, enabling pixel-perfect nuclei localization in generated
samples. these synthetic images are useful in applications in pathology
pedagogy, validation of models, and supplementation of existing nuclei
segmentation datasets. we demonstrate that nasdm is able to syn-
thesize high-quality histopathology images of the colon with superior
quality and semantic controllability over existing generative methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"https://doi.org/10.1007/978-3-031-43987-2_76
nasdm: nuclei-aware histopathology image generation
787
be used to generate histopathology images with speciﬁc characteristics, such as
visual patterns identifying rare cancer subtypes [4]. as such, generative models
can be sampled to emphasize each disease subtype equally and generate more
balanced datasets, thus preventing dataset biases getting ampliﬁed by the mod-
els [7]. generative models have the potential to improve the pedagogy, trustwor-
thiness, generalization, and coverage of disease diagnosis in the ﬁeld of histology
by aiding both deep learning models and human pathologists."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"synthetic datasets
can also tackle privacy concerns surrounding medical data sharing. addition-
ally, conditional generation of annotated data adds even further value to the
proposition as labeling medical images involves tremendous time, labor, and
training costs. [8]
have achieved tremendous success in conditional and unconditional generation
of real-world images [3]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"however,
gans suﬀer from problems of frequent mode collapse and overﬁtting their dis-
criminator [29]. it is also challenging to capture long-tailed distributions and
synthesize rare samples from imbalanced datasets using gans. more recently,
denoising diﬀusion models have been shown to generate highly compelling images
by incrementally adding information to noise [8]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"semantic image synthesis is a task
involving generating diverse realistic images from semantic layouts. gan-based
semantic image synthesis works [20,24,25] generally struggled at generating high
quality and enforcing semantic correspondence at the same time. to this end,
a semantic diﬀusion model has been proposed that uses conditional denoising
diﬀusion probabilistic model and achieves both better ﬁdelity and diversity [27]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"[5] to demonstrate our framework. this dataset con-
sists of histology image regions of colon tissue from six diﬀerent data sources
at 20× objective magniﬁcation. the images are accompanied by full segmenta-
tion annotation for diﬀerent types of nuclei, namely, epithelial cells, connective
tissue cells, lymphocytes, plasma cells, neutrophils, and eosinophils."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"a gener-
ative model trained on this dataset can be used to eﬀectively synthesize the
colonic tumor micro-environments. the dataset contains 238 image regions, with
an average size of 1055 × 934 pixels. as there are substantial visual variations
across images, we construct a representative test set by randomly sampling a
7.5% area from each image and its corresponding mask to be held-out for test-
ing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"the test and train image regions are further divided into smaller image
patches of 128 × 128 pixels at two diﬀerent objective magniﬁcations: (1) at 20×,
the images are directly split into 128 × 128 pixels patches, whereas (2) at 10×,
we generate 256 × 256 patches and resize them to 128 × 128 for training. to use
the data exhaustively, patching is performed with a 50% overlap in neighboring
patches. as such, at (1) 20× we extract a total of 54,735 patches for training
and 4,991 patches as a held-out set, while at (2) 20× magniﬁcation we generate
12,409 training patches and 655 patches are held out.
3.2
stain normalization
a common issue in deep learning with h&e stained histopathology slides is the
visual bias introduced by variations in the staining protocol and the raw mate-
rials of chemicals leading to diﬀerent colors across slides prepared at diﬀerent
labs [1]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"in this work, we use the structure
preserving color normalization scheme introduce by vahadane et al. [26] to trans-
form all the slides to match the stain distribution of an empirically chosen slide
from the training dataset. 3.3
conditional denoising diﬀusion probabilistic model
in this section, we describe the theory of conditional denoising diﬀusion prob-
abilistic models, which serves as the backbone of our framework."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"original noise ϵ and prediction ˆϵ are used to compute the loss in (4). diﬀusion model aims to maximize the likelihood pθ(x0 | y), where data x0 is
sampled from the conditional data distribution, x0 ∼ q(x0 | y), and y represents
the conditioning signal. a diﬀusion model consists of two intrinsic processes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"[8], we can
790
a. shrivastava and p. t. fletcher
randomly sample timestep t during training and use the expectation et,x0,y,ϵ to
estimate lvlb and optimize parameters θ. the denoising neural network can be
parameterized in several ways, however, it has been observed that using a noise-
prediction based formulation results in the best image quality [8]. (4)
note that the above loss function provides no signal for training σθ(xt, y, t)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"overall, the loss is a weighted summation of
the two objectives described above as follows:
lhybrid = lsimple + λlvlb. (5)
3.4
conditioning on semantic mask
nasdm requires our neural network noise-predictor ϵθ(xt, y, t) to eﬀectively
process the information from the nuclei semantic map. for this purpose, we
leverage a modiﬁed u-net architecture described in wang et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"in addition, we also concatenate a mask comprising of the
edges of all nuclei to further demarcate nuclei instances. 3.5
classiﬁer-free guidance
to improve the sample quality and agreement with the conditioning signal, we
employ classiﬁer-free guidance [10], which essentially ampliﬁes the conditional
distribution using unconditional outputs while sampling. during training, the
conditioning signal, i.e., the semantic label map, is randomly replaced with a
null mask for a certain percentage of samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"we then per-
form quantitative and qualitative assessments to demonstrate the eﬃcacy of our
nuclei-aware semantic histopathology generation model. in all following exper-
iments, we synthesize images using the semantic masks of the held-out dataset
at the concerned objective magniﬁcation. we then compute fr´echet inception
distance (fid) and inception score (is) metrics between the synthetic and real
images in the held-out set.
4.1
implementation details
our diﬀusion model is implemented using a semantic unet architecture
(sect. 3.4), trained using the objective in (5)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"quantitative assessment: we report the performance of our method using
fr´echet inception distance (fid) and inception score (is) with the metrics reported in
existing works. (-) denotes that corresponding information was not reported in original
work. ∗note that performance reported for best competing method on the colon data
is from our own implementation, performances for both this and our method should
improve with better tuning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"as seen in fig. 2, increase
in guidance scale initially results in better image quality as more detail is added
to visual structures of nuclei. however, with further increase, the image quality
degrades as the model overemphasizes the nuclei and staining textures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"note that we only train on a
subset on 20× mag. to keep the size of the training data constant. 4.4
quantitative analysis
to the best of our knowledge, ours is the only work that is able to synthesize
histology images given a semantic mask, making a direct quantitative compari-
son tricky."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"we use 30 patches, 17 synthetic and 13 real for this review. we have two experts
assess the overall medical quality of the patches as well as their consistency with
the associated nuclei masks on likert scale. the survey used for the review can
be found on a public google survey1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"5
conclusion and future works
in this work, we present nasdm, a nuclei-aware semantic tissue generation
framework. we demonstrate the model on a colon dataset and qualitatively
1 https://forms.gle/1dladk9xkhp6fwmy6. 794
a. shrivastava and p. t. fletcher
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_76.pdf,"4. qualitative review: compiled results from a pathologist review. we have
experts assess patches for, their overall medical quality (left), as well as, their consis-
tency with the associated mask (right). we observe that the patches generated by the
model do better on all metrics and majority are imperceptible from real patches.
and quantitatively establish the proﬁciency of the framework at this task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"respiratory correlated cone beam computed tomography
(4dcbct) is a technique used to address respiratory motion artifacts
that aﬀect reconstruction quality, especially for the thorax and upper-
abdomen. 4dcbct sorts the acquired projection images in multiple
respiratory correlated bins."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"using a funda-
mental property of the fdk reconstruction algorithm, and prior results
from the literature, we prove mathematically the ability of the method
to work and specify the underlying assumptions. we apply the method to a public dataset and to an in-house dataset
and show that it matches the performance of a supervised approach and
outperforms it when measurement noise is present in the data. 4dcbct ·
deep learning · unsupervised learning
1
introduction
radiotherapy (rt) is one of the cornerstones of cancer patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"image
guided rt (igrt) is a technique to capture the anatomy of the day using in
room imaging in order to align the treatment beam with the tumor location [1]. cone beam ct (cbct) is the most widely used imaging modality for igrt.
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 46.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43999-5_46
482
s. papa et al.
a major challenge especially for cbct imaging of the thorax and upper-
abdomen is the respiratory motion that introduces blurring of the anatomy,
reducing the localization accuracy and the sharpness of the image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"this limits the number of projections available and results in
view-aliasing [16]. additionally, the projections are aﬀected by stochastic mea-
surement noise caused by the ﬁnite imaging dose used, which further degrades
the quality of the reconstruction even when all projections are used. several traditional methods based on iterative reconstruction algorithms and
motion compensation techniques are used to reduce view-aliasing in 4dcbcts
[7,10,11,14,15]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"the second is to apply noise2inverse directly to all the
projections. in this case, the motion artifacts that blur the image will appear
again, as noise2inverse requires averaging the sub-reconstructions to obtain a
clean reconstruction. we propose noise2aliasing to address these limitations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"the method can
be used to provably train models to reduce both view-aliasing artifacts and
stochastic noise from 4dcbcts in an unsupervised way. training deep learning
models for medical applications often needs new data. this was not the case for
noise2aliasing, and historical clinical data suﬃced for training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"we validated our method on publicly available data [15] against a supervised
approach [6] and applied it to an internal clinical dataset of 30 lung cancer
patients. we explore diﬀerent dataset sizes to understand their eﬀects on the
reconstructed images. 2
theoretical background
in this section, we will introduce the concepts and the notation necessary to
understand the method and the choices made during implementation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"}, the fdk reconstruction
algorithm r, and the noisy projections ˜y = ax+ϵ with ϵ mean-zero element-wise
independent noise. ∈ d
be the input-target pairs in dataset d of reconstructions using disjoint subsets of
noisy projections. let l be the expected mse over d with respect to a function
f : rdv → rdv and the previously-described input-target pairs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"(9)
this is suﬃcient to reduce stochastic noise but we need to further manipulate this
expression to address view aliasing. [ej2∼j2(ˆxj1|˜xj2 = z)] ,
(10)
now assume that ˆxj1 is the clean reconstruction that is consistent with the
observed noisy reconstruction z obtained from each disjoint subset j2, then:
f ∗(z) = ej1∼j1(ˆxj1). (11)
finally, we use the property of the fdk from eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"6:
f ∗(z) = ej1∼j1(ˆxj1) = ˆx.
(12)
⊓⊔
noise2aliasing
485
3.1
design choices based on the proposition
the proposition guided the choice of reconstruction method to be fdk and the
design of the subset selection method from considerations that are now explained. equation 12 holds true only when the same underlying clean reconstruction
ˆx can be determined from the noisy reconstruction using any subset from a par-
tition of the projections j . this means that, in our dataset, we should have
at our disposal reconstructions of the same underlying volume x using disjoint
subsets of projections."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"when the projections are selected with the same sampling pattern
as the one used in respiratory-correlated reconstructions, then the view-aliasing
artifacts display will have the same pattern as the ones present in the 4dcbcts.
compared to previous work, to obtain the additional eﬀect of reducing projec-
tion noise, the respiratory-uncorrelated reconstructions must use non-overlapping
subsets of projections. coincidentally, a previously proposed subset selection
method utilized for supervised aliasing reduction ﬁts all these requirements and
will, therefore, be used in this work [4].
4
experiments
first, we used the spare varian dataset to study whether noise2aliasing can
match the performance of the supervised baseline and if it can outperform it
when adding noise to the projections. then, we use the internal dataset to explore
the requirements for the method to be applied to an existing clinical dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"given two volumes
(x, y), the training pairs (xi(k), yi(k)) are the same i-th slice along the k-th dimen-
sion of each volume chosen to be the axial plane. the datasets used in this study are two:
1. the spare varian dataset was used to provide performance results on pub-
licly available patient data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"training is performed over 4
patients while 1 patient is used as a test set. the hyperparameters are opti-
mized over the training dataset. 486
s. papa et al.
2. an internal dataset (irb approved) of 30 lung cancer patients’ 4dcbcts
from 2020 to 2022, originally used for igrt, with 25 patients for training and
5 patients for testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"the scans are 4 min 205◦ scans with 120kev source and
512 × 512 sized detector, using elekta linacs. the data were anonymized
prior to analysis. projection noise was added using the poisson distribution to the spare varian
dataset to evaluate the ability of the unsupervised method to reduce it."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"[17] all the metrics are deﬁned between the output of the neural network and
a 3d (cb)ct scan. for the spare varian dataset, we use the rois deﬁned
provided [15] and used the 3d reconstruction using all the projections available
as a ground truth. for the internal dataset, we deformed the planning ct to each
of the phases reconstructed using the fdk algorithm and evaluate the metric
over only the 4dcbct volume boundaries.
5
results and discussion
spare varian."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"from the qualitative evaluation of the methods in
fig. 1, noise2aliasing matches the visual quality of the supervised approach on
the low-noise dataset on both soft tissue and bones. the metrics in table 1 show
mean and standard deviation across all phases for a single patient."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"noise2aliasing and the supervised method produce very similar images
in the low-noise case. with noisy data, the supervised method tends to re-create the
noise seen during training. table
1. metrics for the comparison between fdk, supervised method, and
noise2aliasing (n2a)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"overall, using more
patients results in better noise reduction and sharper reconstructions (see fig. 2),
488
s. papa et al.
fig. 2. reconstruction using noise2aliasing with diﬀerent-sized datasets. with fewer
patients, the model is more conservative and tends to keep more noise, but also smudges
the interface between tissues and bones."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"however, the model
also tends to remove small anatomical structures as high-frequency objects that
cannot be distinguished from the noise. when applied to a clinical dataset, noise2aliasing beneﬁts from more
patients being included in the dataset, however, qualitatively good performance
is already achieved with 5 patients. no additional data collection was required
and the method can be applied without major changes to the current clinical
practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"6
conclusion
we have presented noise2aliasing, a method to provably remove both view-
aliasing and stochastic projection noise from 4dcbcts using an unsupervised
deep learning method. we have empirically demonstrated its performance on
a publicly available dataset and on an internal clinical dataset. noise2aliasing
noise2aliasing
489
outperforms a supervised approach when stochastic noise is present in the pro-
jections and matches its performance on a popular benchmark."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_46.pdf,"noise2aliasing
can be trained on existing historical datasets and does not require changing
current clinical practices. the method removes noise more reliably when the
dataset size is increased, however further analysis is required to establish a good
quantitative measurement of this phenomenon. as future work, we plan to study
noise2aliasing in the presence of changes in the breathing frequency and ampli-
tude between patients and during a scan."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"active learning (al) is an eﬀective approach to select the
most informative samples to label so as to reduce the annotation cost. existing al methods typically work under the closed-set assumption,
i.e., all classes existing in the unlabeled sample pool need to be clas-
siﬁed by the target model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"in this paper, we formulate this scenario as an open-set al
problem and propose an eﬃcient framework, openal, to address the
challenge of querying samples from an unlabeled pool with both target
class and non-target class samples. experiments on ﬁne-grained classiﬁ-
cation of pathology images show that openal can signiﬁcantly improve
the query quality of target class samples and achieve higher performance
than current state-of-the-art al methods. code is available at https://
github.com/miccaiif/openal."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"keywords: active learning · openset · pathology image classiﬁcation
1
introduction
deep learning techniques have achieved unprecedented success in the ﬁeld of
medical image classiﬁcation, but this is largely due to large amount of annotated
data [5,18,20]. however, obtaining large amounts of high-quality annotated data
is usually expensive and time-consuming, especially in the ﬁeld of pathology
image processing [5,12–14,18]. therefore, a very important issue is how to obtain
the highest model performance with a limited annotation budget."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"existing al methods cannot accurately
distinguish whether the samples are from the target classes or not, thus querying a
large number of non-target samples and wasting the annotation budget, while our
method can accurately query samples from the target categories. (color ﬁgure online)
active learning (al) is an eﬀective approach to address this issue from a
data selection perspective, which selects the most informative samples from an
unlabeled sample pool for experts to label and improves the performance of the
trained model with reduced labeling cost [1,2,9,10,16,17,19]. [11]. figure 1 shows an al
scenario for pathology image classiﬁcation in an open world, which is very com-
mon in clinical practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"for example, in the cell classiﬁcation task, only patches of tumor, lymphatic and
normal cells need to be labeled and classiﬁed by the target model. since the non-
target patches are not necessary for training the classiﬁer, labeling them would
waste a large amount of budget. we call this scenario in which the unlabeled pool
consists of both target class and non-target class samples open-set al problem."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"[11] proposed the ﬁrst al algorithm for open-set anno-
tation in the ﬁeld of natural images. they ﬁrst trained a network to detect target
class samples using a small number of initially labeled samples, and then mod-
eled the maximum activation value (mav) distribution of each sample using a
gaussian mixture model [15] (gmm) to actively select the most deterministic
openal
5
target class samples for labeling. although promising performance is achieved,
their detection of target class samples is based on the activation layer values
of the detection network which has limited accuracy and high uncertainty with
small initial training samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"in this stage, we measure
the uncertainty of all unlabeled samples in the candidate set using the classiﬁer
trained with the target class samples labeled in previous iterations, and select
the samples with the highest model uncertainty as the ﬁnal selected samples in
this round of query. after the second stage, the queried samples are sent for
annotation, which includes distinguishing target and non-target class samples
and giving a ﬁne-grained label to every target class sample. after that, we train
the classiﬁer again using all the ﬁne-grained labeled target class samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"we conducted two experiments with diﬀerent matching ratios (ratio of the
number of target class samples to the total number of samples) on a public 9-class
colorectal cancer pathology image dataset. the experimental results demonstrate
that openal can signiﬁcantly improve the query quality of target class samples
and obtain higher performance with equivalent labeling cost compared with the
current state-of-the-art al methods. to the best of our knowledge, this is the
ﬁrst open-set al work in the ﬁeld of pathology image analysis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"iterative queries are
performed to query a ﬁxed number of samples in each iteration, and the objec-
tive is to select as many target class samples as possible from pu in each query,
while selecting as many informative samples as possible in the target class sam-
ples. each queried sample is given to experts for labeling, and the experts will
give ﬁne-grained category labels for target class samples, while only giving a
“non-target class samples” label for non-target class samples. 6
l. qu et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"after scoring all the unlabeled samples, we select the top
ε% samples with the smallest scores to form the candidate set. in this paper, we
empirically take twice the current iterative labeling budget (number of samples
submitted to experts for labeling) as the sample number of the candidate set. below, we give the deﬁnitions of sti and swi.
distance-based feature distribution modeling."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"the deﬁnitions of these two values are slightly diﬀerent,
and we ﬁrst present the calculation of sti, followed by that of swi. for all labeled target class samples from previous iterations, their ﬁne-grained
labels are known, so we represent these samples as diﬀerent clusters in the feature
space according to their true class labels, where a cluster is denoted as cl
t (t =
1, . . . next, we calculate the score sti for zu
i using the mahalanobis distance
(md) according to eq. 2. md is widely used to measure the distance between a
point and a distribution because it takes into account the mean and variance of
the distribution, which is very suitable for our scenario."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"it
can be seen that sti is essentially the minimum distance of the unlabeled sample
xu
i to each target class cluster. for all the queried non-target class samples from previous iterations, since
they do not have ﬁne-grained labels, we ﬁrst use the k-means algorithm to cluster
their features into w classes, where a cluster is denoted as cl
w (w = 1, . . 8
l. qu et al.
w is set to 9 in this paper."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"therefore, we calculate
the entropy of the model for the samples in the candidate set and select 50% of
them with the highest entropy as the ﬁnal samples in the current iteration. the dataset contains a
total of 100,000 patches of pathology images with ﬁne-grained labeling, with
nine categories including adipose (adi 10%), background (back 11%), debris
(deb 11%), lymphocytes (lym 12%), mucus (muc 9%), smooth muscle (mus
14%), normal colon mucosa (norm 9%), cancer-associated stroma (str 10%),
and colorectal adenocarcinoma epithelium (tum, 14%). to construct the open-
set datasets, we selected three classes, tum, lym and norm, as the target
classes and the remaining classes as the non-target classes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"we measure the ﬁnal performance of each al method using the accuracy
of the ﬁnal classiﬁer on the test set of target class samples. [20], of which only lfosa [11] is designed for open-set al. for all al
methods, we randomly selected 1% of the samples to label and used them as
the initial labeled set for model initialization. it is worth noting that the initial
labeled samples contain target class samples as well as non-target class samples,
but the non-target class samples are not ﬁne-grained labeled."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"c. ablation
study of openal under a 33% matching ratio.
fig. 4. a. cumulative sampling ratios of our openal for the target classes lym,
norm, and tum across querynums 1-7 on the original dataset (under 33% matching
ratio). b. cumulative sampling ratios of lfosa on the original dataset (under 33%
matching ratio)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"c. cumulative sampling ratios of our openal on a newly-constructed
more imbalanced setting for the target classes lym (6000 samples), norm (3000
samples), and tum (9000 samples). on the original dataset with a 33% matching ratio, as shown in fig. addi-
tionally, we visualize the cumulative sampling ratios of the lfosa method on
the same setting in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_1.pdf,"it can be seen that the distance modeling of both the target class samples
and the non-target class samples is essential in the ftss strategy, and missing
either one results in a decrease in performance. although the miss strategy does
not signiﬁcantly facilitate the selection of target class samples, it can eﬀectively
help select the most informative samples among the samples in the candidate
set, thus further improving the model performance with a limited labeling bud-
get. in contrast, when the samples are selected based on uncertainty alone, the
performance decreases signiﬁcantly due to the inability to accurately select the
target class samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_58.pdf,"finally, we demonstrate that the oce measurements can be used to
eﬀectively discriminate the tissue mimicking phantoms. [4]. diﬀerent imaging modalities
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43996-4 58.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43996-4_58
608
r. mieling et al.
can be used to detect the biomechanical response to an external load for the
characterization of cancerous tissue, e.g., ultrasound, magnetic resonance and
optical coherence elastography (oce)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_58.pdf,"finally, we consider how the obtained
elasticity estimates can be used for the classiﬁcation of both materials. 2
methods
in the following, we ﬁrst present our oce needle probe and outline data process-
ing for elasticity estimates. we then present an experimental setup for simulating
friction and bulk displacement and describe the conducted surface and deep tis-
sue indentation experiments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_58.pdf,"the two materi-
als (mat. a and mat. b) display a young’s modulus of 53.4 kpa and 112.3 kpa,
respectively. reference elasticity is determined by unconﬁned compression exper-
iments of three cylindrical samples for each material according to eq. 1, using
force and position sensor data (see supplementary material). the young’s modu-
lus is obtained by linear regression for the combined measurements of each mate-
rial."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_58.pdf,"three
surface measurements with ﬁxed samples and seven deep tissue indentations with
simulated friction and bulk displacement. for each indentation, we place the nee-
dle in front of the surface or deep tissue interface and acquire oct data while
driving the needle for 3 mm (fig. 1). as the beginning of the needle movement
might not directly correspond to the beginning of sample indentation, we eval-
uate oce measurements only if the estimated tip force is larger than 50 mn.
to further ensure that measurements occur within the pre-rupture deformation
phase [6,15], only samples below 20 % local strain are considered."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf,"positron emission tomography (pet) is an advanced nuclear
imaging technique with an irreplaceable role in neurology and oncol-
ogy studies, but its accessibility is often limited by the radiation haz-
ards inherent in imaging. to address this dilemma, pet enhancement
methods have been developed by improving the quality of low-dose pet
(lpet) images to standard-dose pet (spet) images. however, previ-
ous pet enhancement methods rely heavily on the paired lpet and
spet data which are rare in clinic."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf,"et al.
alzheimer’s disease [8]. [19].
to reduce the radiation hazards, besides upgrading imaging hardware, design-
ing advanced pet enhancement algorithms for improving the quality of low-dose
pet (lpet) images to standard-dose pet (spet) images is a promising alter-
native. in recent years, many enhancement algorithms have been proposed to
improve pet image quality."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf,"subsequently, with the devel-
opment of deep learning, the end-to-end pet enhancement networks [9,14,21]
were proposed and achieved signiﬁcant performance improvement. but these
supervised methods relied heavily on the paired lpet and spet data that are
rare in actual clinic due to radiation exposure and involuntary motions (e.g., res-
piratory and muscle relaxation). [17]
were developed to overcome this limitation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf,"however, these methods still require
lpet to train models, which contradicts with the fact that only spet scans
are conducted in clinic. fortunately, the recent glowing diﬀusion model [6] provides us with the idea
for proposing a clinically-applicable pet enhancement approach, whose train-
ing only relies on spet data. generally, the diﬀusion model consists of two
reversible processes, where the forward diﬀusion adds noise to a clean image
until it becomes pure noise, while the reverse process removes noise from pure
noise until the clean image is recovered."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf,"[20], to replace the gaussian
noise in the diﬀusion process for avoiding the introduction of details that are not
existing in pet images, and 3) designing ct-guided cross-attention to incorpo-
rate additional ct images into the inverse process for helping the recovery of
structural details in pet. our work had three main features/contributions: i) proposing a clinically-
applicable unsupervised pet enhancement framework, ii) designing three tar-
geted strategies for improving the diﬀusion model, including pet image com-
pression, poisson diﬀusion, and ct-guided cross-attention, and iii) achieving
better performance than state-of-the-art methods on the collected pet datasets. 2
method
the framework of upete is illustrated in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf,"similar to [10,18], we adopt an autoencoder (e and d) to compress the
3d pet images into a lower dimensional but more compact space. the crucial
aspects of this process is to ensure that the latent representation contains the
necessary and representative information for the input image. to achieve this, we
train the autoencoder by a combination of perceptual loss [24] and patch-based
adversarial loss [5], instead of simple voxel-level loss such as l2 or l1 loss."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf,"quantitative results of ablation
analysis, in terms of psnr and ssim. 2. generalizability to dose changes.
3
experiments
3.1
dataset
our dataset consists of 100 spet images for training and 30 paired lpet and
spet images for testing. [25], and 20 paired
chest-abdomen images are collected by list mode of the scanner with 256 mbq of
[18f]-fdg injection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf,"speciﬁcally, the spet images are reconstructed by using
the 1200 s data between 60–80 min after tracer injection, while the corresponding
lpet images are simultaneously reconstructed by 120 s data uniformly sampled
from 1200 s data. as a basic data preprocessing, all images are resampled to voxel spacing of
2 × 2 × 2 mm3 and resolution of 256 × 256 × 160, while their intensity range is
normalized to [0, 1] by min-max normalization. for increasing the training sam-
ples and reducing the dependence on gpu memory, we extract the overlapped
patches of size 96 × 96 × 96 from every whole pet image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf,"speciﬁcally, the average improvement in psnr and ssim on spet estimation
are 1.554 db and 0.005, respectively. this suggests that our upete can generate
promising results without relying on paired data, demonstrating its potential for
clinical applications. qualitative comparison: in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_1.pdf,"(color ﬁgure online)
overall, these pieces of evidence demonstrate the superiority of our upete over
state-of-the-art methods. 3.4
generalization evaluation
we further evaluate the generalizability of our upete to tracer dose changes by
simulating poisson noise on spet to produce diﬀerent doses for lpet, which
is a common way to generate noisy pet data [20]. notably, we do not need to
retrain the models since they have been trained in sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"recently, weakly supervised nuclei segmentation methods
using only points are gaining attention, as they can ease the tedious
labeling process. however, most methods often fail to separate adjacent
nuclei and are particularly sensitive to point annotations that deviate
from the center of nuclei, resulting in lower accuracy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"next, segmentation pre-
dictions are used to repeatedly generate pseudo oﬀset maps that indi-
cate the most likely nuclei center. finally, an expectation maximization
(em) based process iteratively reﬁnes initial point labels based on the
oﬀset map predictions to ﬁne-tune our framework. experimental results
show that our model consistently outperforms state-of-the-art methods
on public datasets regardless of the point annotation accuracy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"keywords: weakly supervised nuclei segmentation · instance
segmentation · point reﬁnement · oﬀset map · geodesic distance
1
introduction
nuclei segmentation in histopathology images is an important task for cancer
diagnosis and immune response prediction [1,13,18]. while several fully super-
vised deep learning approaches to segment nuclei exist [2,6,8,9,19,25], labeling
s. nam and j. jeong—equal contribution. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_51.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"https://doi.org/10.1007/978-3-031-43907-0_51
pronet for nuclei instance segmentation
529
thousands of instances are tedious and the ambiguous nature of nuclei bound-
aries requires high-level expert annotators. [5,10,15,20,23,28] have emerged as an attractive
alternative using cheap and inexact labels e.g., center point annotations. as
point labels alone do not provide suﬃcient foreground information, it is com-
mon to use euclidean distance-based voronoi diagrams and k-means clustering
[7] to generate pseudo segmentation labels for training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"however, since euclidean
distance-based schemes only use distance information while ignoring color, they
often fail to capture nuclei shape information; resulting in inadequate boundary
delineation between adjacent nuclei. moreover, prior methods [17,21,22] typi-
cally assume that point labels are located precisely at the center of the nuclei. in real-world scenarios, point annotation locations may shift from nuclei centers
as a result of the expert labeling process, leading to a lower performance after
model training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"the proposed model consists of three modules responsible for
binary segmentation, boundary delineation, and instance separation. to train
the binary segmentation module, we generate pseudo binary segmentation masks
using geodesic distance-based voronoi labels and cluster labels from point anno-
tations. geodesic distance provides more precise nuclei shape information than
previous euclidean distance-based schemes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"to train the oﬀset map module, we
generate pseudo oﬀset maps by computing the oﬀset distance between binary
segmentation pixel predictions and the point label. the oﬀset information facil-
itates precise delineation of the boundaries between adjacent nuclei. [4] algorithm-based process to reﬁne point labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"the contributions of this paper are as follows: (1) we propose an end-to-
end weakly supervised segmentation model that simultaneously predicts binary
mask, oﬀset map, and center map to accurately identify and segment nuclei. (2) by utilizing geodesic distance, we produce more detailed voronoi and clus-
ter labels that precisely delineate the boundary between adjacent nuclei. (3) we
introduce an em algorithm-based reﬁnement process to encourage model robust-
ness on center-shifted point labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"it consists of an encoder and three modules
for binary segmentation, oﬀset map and center map prediction. to train oﬀset map
and center map modules(blue lines), pseudo labels are generated using point label and
predicted binary segmentation mask(green lines). during inference, the instance map,
obtained by predicted oﬀset map and center map, is multiplied with predicted binary
mask to produce instance segmentation prediction(orange lines)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"the fea-
ture maps are further processed through a series of residual units (rus) and
attention units (aus) to predict a binary segmentation mask ˆb, an oﬀset map
ˆo, and a center map ˆc. the rus are employed to maintain feature information
so that subsequent modules can reuse the features from early-stage modules. in
contrast, the aus are used to reﬁne the features of initial modules by using the
predictions of later modules."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"in particular, the aus use the point predictions to
reﬁne the features in the oﬀset module, and the oﬀset predictions to reﬁne the
features in the binary module. in the training stage, we ﬁrst generate a voronoi label v and a cluster label k
along the green lines in fig. 1 to train the segmentation module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"finally, the instance segmentation output ˆs is obtained
by ˆb × i.
fig. 2. visualization of cluster label on cpm17(left) and monuseg(right). (a) input
image; (b) ground truth; (c) the cluster labels generated by euclidean distance, and
(d) those by geodesic distance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"the green, red, and black colors are foreground, back-
ground, and ignored, respectively. (color ﬁgure online)
2.1
loss functions using pseudo labels
segmentation loss. we generate v and k to train the binary segmentation
module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"in [21], v was generated based on euclidean distance between points
without considering color information. as a result, the voronoi boundaries are
often created across nuclei instances, and the oﬀset map’s quality was limited. [3,24] by computing
distances di between all center points pi ∈ p and pixels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"to train the module we employ a focal
loss, commonly used in point detection problems. if c(x, y) = 1
(1 − c(x, y))β( ˆc(x, y))αlog(1 − ˆc(x, y))
otherwise,
(3)
where np denotes the number of point labels. we set the focal loss hyper-
parameters α = 2 and β = 4 following [14,29]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"λblb + λolo + λclc, where λb, λo and λc denote loss weights. 2.2
reﬁnement via expectation maximization algorithm
training with nuclei (center) shifted point labels can lead to blurry center map
predictions (see fig. this in turn limits model optimization and it’s ability
to distinguish objects, resulting in poor adjacent nuclei segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"to address
this, we propose an em based center point reﬁnement process. instead of the
standard ﬁxed-point label based model optimization, we alternatively optimize
both model parameters and point labels. in the e-step, we update the center of each nucleus according to ˆo."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"images were normalized and cropped to 300×300. monuseg is a multi-organ
nuclei segmentation dataset consisting of 30 h&e stained images (1000×1000)
extracted from seven diﬀerent organs. we used 16 images (4 images from the
breast, liver, kidney, and prostate) as training and 14 images (2 images from each
breast, liver, kidney, prostate, bladder, brain, and stomach) as testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"for a fair
comparison, images were pre-processed before training/testing i.e., normalized
and cropped to 250×250 patches following the setting used in [17].
534
s. nam et al.
to make point labels, we use the center point of full mask annotations. for
a realistic scenario, we generate shifted point label. [11] using a learning rate of 1e-4, weight decay
of 3e-2, and batch size of 4."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"we used a nvidia rtx a5000 gpu and
pytorch version 1.7.1.
table 1. performance comparison of nuclei segmentation on two public datasets. shift
indicates the number of pixels point annotations deviate from the nuclei center."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"as opposed to the dice score,
aji is key when evaluating adjacent nuclei separation in instance segmentation
tasks. on cpm17, our method outperformed the prior approach by a large mar-
gin of +3.4% in dice and +7.2% in aji when the point label is located at the
nuclei center. more importantly, our approach surpassed prior approaches by
a substantial margin when the shift exists."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"we conducted ablation studies to assess the impact of
the oﬀset regression module, geodesic distance, and point reﬁnement process
(table 2). when the binary segmentation module is combined only with the cen-
ter map module without the oﬀset module, the model could separate nuclei only
trained by the ideal label. on the other hand, since there was no reﬁnement pro-
cess due to the absence of the oﬀset map, inaccurate points extracted from the
center map are obtained in the real-world scenario."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"we also demonstrate that
labels with geodesic distance help improve overall performance. this is because
536
s. nam et al.
it creates conﬁdent labels and more decent divides the boundaries between nuclei. finally, using the full set of modules along with a complete instance map, the
model was able to separate adjacent nuclei with precise boundaries, ultimately
reporting higher scores."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_51.pdf,"these ﬁndings validate the utility of the center map
and oﬀset map modules i.e., they synergistically facilitate precise instance delin-
eation and nuclei boundary prediction. the geodesic distance and reﬁnement
process also improved the accuracy by contributing to more accurate pseudo
labels. especially, most variants show a signiﬁcant drop in performance when
the annotations shift was over 4 pixels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"in clinical practice,
the contextual structure of nodules and the accumulated experience of
radiologists are the two core elements related to the accuracy of identi-
ﬁcation of benign and malignant nodules. contextual information pro-
vides comprehensive information about nodules such as location, shape,
and peripheral vessels, and experienced radiologists can search for clues
from previous cases as a reference to enrich the basis of decision-making. in this paper, we propose a radiologist-inspired method to simulate the
diagnostic process of radiologists, which is composed of context pars-
ing and prototype recalling modules."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"the prototype recalling module utilizes prototype-based learning to con-
dense previously learned cases as prototypes for comparative analysis,
which is updated online in a momentum way during training. building
on the two modules, our method leverages both the intrinsic characteris-
tics of the nodules and the external knowledge accumulated from other
nodules to achieve a sound diagnosis. to meet the needs of both low-
dose and noncontrast screening, we collect a large-scale dataset of 12,852
and 4,029 nodules from low-dose and noncontrast cts respectively, each
with pathology- or follow-up-conﬁrmed labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"experiments on several
datasets demonstrate that our method achieves advanced screening per-
formance on both low-dose and noncontrast scenarios. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9 20.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43904-9_20
200
j. zhang et al.
1
introduction
lung cancer screening has a signiﬁcant impact on the rate of mortality associ-
ated with lung cancer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"particularly, the evaluation of nodule (i.e., 8–30mm)
malignancy is recommended in the guidelines [13].
fig. 1. in pare, a nodule is diagnosed
from two levels: ﬁrst parsing the contex-
tual information contained in the nodule
itself, and then recalling the previously
learned nodules to look for related clues. one of the major challenges of
lung nodule malignancy prediction is
the quality of datasets [6]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"for example, shao et al. [16] collated a
pathological gold standard dataset of
990 ct scans. another issue is most of
the studies focus on ldct for malig-
nancy prediction [10]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"to achieve the nodule-level prediction, xie et al. [24] introduced
a knowledge-based collaborative model that hierarchically ensembles multi-view
predictions at the decision level for each nodule. liu et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"[17] eﬀectively improved the malignancy prediction accuracy by using a
transfer learning and semi-supervised strategy. despite their advantages in rep-
resentation learning, these methods do not take into account expert diagnostic
knowledge and experience, which may lead to a bad consequence of poor general-
ization. we believe a robust algorithm should be closely related to the diagnosis
experience of professionals, working like a radiologist rather than a black box."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"2. overview architecture of our proposed pare model.
abbreviated as pare. at the intra-level, the contextual information of the
nodules provides clues about their shape, size, and surroundings, and the inte-
gration of this information can facilitate a more reliable diagnosis of whether
they are benign or malignant. motivated by this, we ﬁrst segment the context
structure, i.e., nodule and its surroundings, and then aggregate the context
information to the nodule representation via the attention-based dependency
modeling, allowing for a more comprehensive understanding of the nodule itself."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"this is similar to how radiologists rely on their accumulated experience
in clinical practice. thus, the model is expected to have the ability to store and
recall knowledge, i.e., the knowledge learned can be recorded in time and then
recalled as a reference for comparative analysis. to achieve this, we condense the
learned nodule knowledge in the form of prototypes, and recall them to explore
potential inter-level clues as an additional discriminant criterion for the new
case."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"for the ncct, we annotate over 4,029 nodules from
over 2,565 patients from our collaborating hospital. experimental results on sev-
eral datasets demonstrate that our method achieves outstanding performance on
both ldct and ncct screening scenarios. our contributions are summarized as follows: (1) we propose context parsing
to extract and aggregate rich contextual information for each nodule."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"(2) we
condense the diagnostic knowledge from the learned nodules into the prototypes
and use them as a reference to assist in diagnosing new nodules. (3) we curate the
largest-scale lung nodule dataset with high-quality benign/malignant labels to
fulﬁll both ldct and ncct screening needs. (4) our method achieves advanced
malignancy prediction performance in both screening scenarios (0.931 auc), and
exhibits strong generalization in external validation, setting a new state of the
art on lungx (0.801 auc)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"202
j. zhang et al.
2
method
figure 2 illustrates the overall architecture of pare, which consists of three
stages: context segmentation, intra context parsing, and inter prototype recall-
ing. we now delve into diﬀerent stages in detail in the following subsections.
2.1
context segmentation
the nodule context information has an important eﬀect on the benign and malig-
nant diagnosis. therefore, we use a u-like net-
work (unet) to parse the semantic mask m for the input image patch x, thus
allowing subsequent context modeling of both the nodule and its surrounding
structures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"speciﬁcally, each voxel of m belongs to {0 : background, 1 : lung, 2 :
nodule, 3 : vessel, 4 : trachea}. this segmentation process allows pare to gather
comprehensive context information that is crucial for an accurate diagnosis. for
the diagnosis purpose, we extract the global feature from the bottleneck of unet
as the nodule embedding q, which will be used in later diagnostic stages."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"speciﬁcally, the context mask is tokenized into a set of sequences via
the overlapped patch embedding. the input image is also split into patches and
then embedded into the context tokens to keep the original image information. besides, positional encoding is added in a learnable manner to retain location
information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"here
g is the number of context tokens, and d represents the embedding dimension. then we perform the self-attention modeling on these tokens simultaneously,
called self context attention (sca), to aggregate context information into the
nodule embedding. the nodule embedding token at the output of the last sca
block serves as the updated nodule representation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"it allows the nodule embedding to selectively
attend to the most relevant parts of prototype sequences. the state of query at
the output of the last cpa module servers as the ﬁnal nodule representation to
predict its malignancy label, either “benign” (y = 0) or “malignant” (y = 1). updating prototype online: the prototypes are updated in an online man-
ner, thereby allowing them to adjust quickly to changes in the nodule represen-
tations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"the in-house cohort was retrospectively collected
from 2,565 patients at our collaborating hospital between 2019 and 2022. unlike
nlst, this dataset is noncontrast chest ct, which is used for routine clinical
care. segmentation annotation: we provide the segmentation mask for our
in-house data, but not for the nlst data considering its high cost of pixel-level
labeling."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"the in-house test set has 1,437 (1,298 benign and 139
malignant) nodules from 452 patients. [2] challenge dataset, which is usually used for external validation
in previous work [6,11,24]. lungx contains 83 (42 benign and 41 malignant)
nodules, part of which (13 scans) were contrast-enhanced."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"more details can be found
in the ablation. due to the lack of manual annotation of nodule masks for the
nlst dataset, we can only optimize the segmentation task using our in-house
dataset, which has manual nodule masks. 3.2
experiment results
ablation study: in table 1, we investigate the impact of diﬀerent conﬁgura-
tions on the performance of pare on the validation set, including transformer
layers, number of prototypes, embedding dimension, and deep supervision."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"our pare method surpasses all other methods on both
nlst and in-house test sets. moreover, by utilizing the ensemble of multiple
deep supervision heads, the overall auc is further improved to 0.931 on both
datasets. [2] as an external test
to evaluate the generalization of pare."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_20.pdf,"results in fig. 3 reveal that our method achieves
performance comparable to that of radiologists.
generalization on ldct and ncct: our model is trained on a mix of
ldct and ncct datasets, which can perform robustly across low-dose and
regular-dose applications. we compare the generalization performance of the
models obtained under three training data conﬁgurations (ldct, ncct, and
a combination of them)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"in this study, we pro-
pose a multimodal transformer (pathomics) integrating pathology
and genomics insights into colon-related cancer survival prediction. we
emphasize the unsupervised pretraining to capture the intrinsic interac-
tion between tissue microenvironments in gigapixel whole slide images
(wsis) and a wide range of genomics data (e.g., mrna-sequence,
copy number variant, and methylation). after the multimodal knowl-
edge aggregation in pretraining, our task-speciﬁc model ﬁnetuning could
expand the scope of data utility applicable to both multi- and single-
modal data (e.g., image- or genomics-only)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"we evaluate our approach on
both tcga colon and rectum cancer cohorts, showing that the proposed
approach is competitive and outperforms state-of-the-art studies. finally,
our approach is desirable to utilize the limited number of ﬁnetuned sam-
ples towards data-eﬃcient analytics for survival outcome prediction. the
code is available at https://github.com/cassie07/pathomics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"[24].
diﬀerent cancer genotypes are translated into pathological phenotypes that
could be assessed by pathologists [24]. high-resolution pathological images have
proven their unique beneﬁts for improving prognostic biomarkers prediction via
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 60. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"in a broader context, assessing cancer
prognosis is essentially a multimodal task in association with pathological and
genomics ﬁndings. therefore, synergizing multimodal data could deepen a cross-
scale understanding towards improved patient prognostication. supervised studies [5–7] have allowed
multimodal data fusion among image and non-image biomarkers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"alternatively, the co-
attention transformer [6] could capture the genotype-phenotype interactions
for prognostic prediction. yet these supervised approaches are limited by fea-
ture generalizability and have a high dependency on data labeling. to alle-
viate label requirement, unsupervised learning evaluates the intrinsic similar-
ity among multimodal representations for data fusion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"for example, integrating
image, genomics, and clinical information can be achieved via a predeﬁned unsu-
pervised similarity evaluation [4]. to broaden the data utility, the study
[28]
leverages the pathology and genomic knowledge from the teacher model to guide
the pathology-only student model for glioma grading. from these analyses, it is
increasingly recognized that the lack of ﬂexibility on model ﬁnetuning limits the
data utility of multimodal learning. meanwhile, the size of multimodal medical
datasets is not as large as natural vision-language datasets, which necessitates
the need for data-eﬃcient analytics to address the training diﬃculty.
to tackle above challenges, we propose a pathology-and-genomics multimodal
framework (i.e., pathomics) for survival prediction (fig. 1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"we summarized
our contributions as follows. (1) unsupervised multimodal data fusion. our unsupervised pretraining exploits the intrinsic interaction between morpho-
logical and molecular biomarkers (fig. 1a)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"(2) flexible modality
ﬁnetuning. a key contribution of our multimodal framework is that it com-
bines beneﬁts from both unsupervised pretraining and supervised ﬁnetuning data
fusion (fig. 1b). as a result, the task-speciﬁc ﬁnetuning broadens the dataset
usage (fig 1b and c), which is not limited by data modality (e.g., both single-
and multi-modal data)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"(3) data eﬃciency with limited data size. our
approach could achieve comparable performance even with fewer ﬁnetuned data
(e.g., only use 50% of the ﬁnetuned data) when compared with using the entire
ﬁnetuning dataset. 624
k. ding et al.
2
methodology
overview."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"figure 1 illustrates our multimodal transformer framework. our
method includes an unsupervised multimodal data fusion pretraining and a
supervised ﬂexible-modal ﬁnetuning. 1a, in the pretraining, our unsu-
pervised data fusion aims to capture the interaction pattern of image and
genomics features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"next, as seen in fig. 1b
and c, our approach enables three types of ﬁnetuning modal modes (i.e., multi-
modal, image-only, and genomics-only) towards prognostic prediction, expanding
the downstream data utility from the pretrained model.
fig. 1. workﬂow overview of the pathology-and-genomics multimodal transformer
(pathomics) for survival prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"in (a), we show the pipeline of extracting image
and genomics feature embedding via an unsupervised pretraining towards multimodal
data fusion. in (b) and (c), our supervised ﬁnetuning scheme could ﬂexibly handle mul-
tiple types of data for prognostic prediction. with the multimodal pretrained model
backbones, both multi- or single-modal data can be applicable for our model ﬁnetuning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"to aggregate patient-wise
multimodal feature embedding from the group-wise representations, as shown
in fig. 1a, we propose a pathology-and-genomics multimodal model containing
two model streams, including a pathological image and a genomics data stream. in each stream, we use the same architecture with diﬀerent weights, which is
updated separately in each modality stream."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"due to the domain
gap between image and molecular feature heterogeneity, a proper design of
multimodal fusion is crucial to advance integrative analysis. in the pretrain-
ing stage, we develop an unsupervised data fusion strategy by decreasing the
mean square error (mse) loss to map images and genomics embeddings into
the same space. ideally, the image and genomics embeddings belonging to the
same patient should have a higher relevance between each other."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"in this way, the
pretrained model is trained to map the paired image and genomics embeddings
to be closer in the latent space, leading to strengthen the interaction between
diﬀerent modalities. lfusion = argmin 1
p
p

p=1
((ip
embedding − gp
embedding)2)
(4)
in the single modality ﬁnetuning, even if we use image-only data, the model is
able to produce genomic-related image feature embedding due to the multimodal
knowledge aggregation already obtained from the model pretraining. as a result,
our cross-modal information aggregation relaxes the modality requirement in the
ﬁnetuning stage."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"during the ﬁnetuning, we update the model parameters using a log-likelihood
loss for the discrete-time survival model training [6](see appendix 2). 3
experiments and results
datasets. all image and genomics data are publicly available."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"we
cropped each wsi into 512 × 512 non-overlapped patches. we removed the samples without the corresponding
genomics data or ground truth of survival outcomes. finally, we included 426
patients of tcga-coad and 145 patients of tcga-read."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"experimental settings and implementations. we implement two types of
settings that involve internal and external datasets for model pretraining and
ﬁnetuning. as shown in fig 2a, we pretrain and ﬁnetune the model on the same
dataset (i.e., internal setting)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"then, we implement four-fold cross-validation on the
pathology-and-genomics multimodal transformer for survival prediction
627
fig. 2. dataset usage. in a, we use tcga-coad dataset for model pretraining, ﬁne-
tuning, and evaluation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"in b, we use tcga-coad dataset for model pretraining. then,
we use tcga-read dataset to ﬁnetune and evaluate the pretrained models. training set for pretraining, ﬁnetuning, and hyperparameter-tuning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"the test set
is only used for evaluating the best ﬁnetuned models from each cross-validation
split. for the external setting, we implement pretraining and ﬁnetuning on the
diﬀerent datasets, as shown in fig 2b; we use tcga-coad for pretraining;
then, we only use tcga-read for ﬁnetuning and ﬁnal evaluation. we imple-
ment a ﬁve-fold cross-validation for pretraining, and the best pretrained models
are used for ﬁnetuning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"[26]) and mil multimodal-based models (mcat [6],
porpoise [7]). we follow the same data split and processing, as well as the
identical training hyperparameters and supervised fusion as above. notably,
there is no need for supervised ﬁnetuning for the baselines when using tcga-
coad (table 1), because the supervised pretraining is already applied to the
training set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"628
k. ding et al.
results. in table 1, our approach shows improved survival prediction per-
formance on both tcga-coad and tcga-read datasets. compared with
supervised baselines, our unsupervised data fusion is able to extract the
phenotype-genotype interaction features, leading to achieving a ﬂexible ﬁnetun-
ing for diﬀerent data settings."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"with the multimodal pretraining and ﬁnetuning,
our method outperforms state-of-the-art models by about 2% on tcga-coad
and 4% tcga-read. we recognize that the combination of image and mrna
sequencing data leads to reﬂecting distinguishing survival outcomes. remark-
ably, our model achieved positive results even using a single-modal ﬁnetuning
when compared with baselines (more results in appendix 3.1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"we show that with a single-modal ﬁnetuning strategy, the
model could generate meaningful embedding to combine image- and genomic-
related patterns. in addition, our model reﬂects its eﬃciency on the limited ﬁne-
tuning data (e.g., 75 patients are used for ﬁnetuning on tcga-read, which
are only 22% of tcga-coad ﬁnetuning data). in table 1, our method could
yield better performance compared with baselines on the small dataset across
the combination of images and multiple types of genomics data.
table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"3. ablation study. in (a) and (b), we evaluate the model eﬃciency by using fewer
data for model ﬁnetuning on tcga-coad and tcga-read. we show the average
c-index of baselines, the detailed results are shown in the appendix 3.2.
ablation analysis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"we verify the model eﬃciency by using fewer amounts of
ﬁnetuning data in ﬁnetuning. for tcga-coad dataset, we include 50%, 25%,
and 10% of the ﬁnetuning data. for the tcga-read dataset, as the number
of uncensored patients is limited, we use 75%, 50%, and 25% of the ﬁnetuning
data to allow at least one uncensored patient to be included for ﬁnetuning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"as
shown in fig. 3a, by using 50% of tcga-coad ﬁnetuning data, our approach
achieves the c-index of 64.80%, which is higher than the average performance
of baselines in several modalities. similarly, in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"3b, our model retains a good
performance by using 50% or 75% of tcga-read ﬁnetuning data compared
with the average of c-index across baselines (e.g., 72.32% versus 64.23%). for
evaluating the eﬀect of cross-modality information extraction in the pretraining,
we kept supervised model training (i.e., the ﬁnetuning stage) while removing
the unsupervised pretraining. the performance is lower 2%-10% than ours on
multi- and single-modality data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"in addition, we replaced our unsupervised loss with cosine similarity loss; our
approach outperforms the setting of using cosine similarity loss by 3%-6%. 4
conclusion
developing data-eﬃcient multimodal learning is crucial to advance the survival
assessment of cancer patients in a variety of clinical data scenarios. we demon-
strated that the proposed pathomics framework is useful for improving the
survival prediction of colon and rectum cancer patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_60.pdf,"importantly, our app-
roach opens up perspectives for exploring the key insights of intrinsic genotype-
phenotype interactions in complex cancer data across modalities. our ﬁnetuning
630
k. ding et al.
approach broadens the scope of dataset inclusion, particularly for model ﬁnetun-
ing and evaluation, while enhancing model eﬃciency on analyzing multimodal
clinical data in real-world settings. in addition, the use of synthetic data and
developing a foundation model training will be helpful to improve the robustness
of multimodal data fusion [11,15]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"in current pathology image classiﬁcation, methods mostly
rely on patch-based multi-instance learning (mil), which only consid-
ers the relationship between patches and slides. however, in clinical
medicine, doctors use slide-level labels to summarize patient-level labels
as a diagnostic result, indicating the involvement of three levels of patch,
slide, and patient in actual pathology image analysis, which we refer to
as the multi-level multi-instance learning (ml-mil) problem. to address
this issue, we propose a novel and general framework called patients and
slides are equal (p&sre), inspired by the doctor’s diagnostic process of
repeatedly conﬁrming labels at the patient and slide level."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"in this frame-
work, we treat patients and slides as instances at the same level and use
transformers and attention mechanisms to build connections between
them. this allows for interaction between patient-level and slide-level
information and the correction of their respective features to achieve bet-
ter classiﬁcation performance. we evaluate our method on two datasets
using two state-of-the-art mil methods as baselines."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"our method provides a simple and eﬀective solution
to the common problem of ml-mil in medical clinical scenarios and has
broad potential applications. keywords: multiple instance learning · multi-level labels ·
pathology images · transformer
f. li, m. wang and b. huang—contribute equally to this work. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"https://doi.org/10.1007/978-3-031-43904-9_7
64
f. li et al.
1
introduction
pathological image analysis is a vital area of research within medical image
analysis, focused on utilizing computer technology to aid doctors in diagnosing
and treating diseases by analyzing pathological tissue slide images [5]. advance-
ments in pathological image analysis have been made in early cancer diagnosis,
tumor localization, and grading, and treatment planning [3,10]. [2] is the primary analysis method used, which involves analyzing tasks
based on slide labels and patches. despite this, the clinical pathological analysis
presents certain challenges and complexities, with the ultimate diagnosis relying
on patients rather than slides."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"speciﬁcally, in clinical problems of pathological image analysis, doctors usu-
ally summarize patient-level labels based on slide labels as the diagnostic results
[1,6]. for example, for the pathological discrimination diagnosis task of intesti-
nal tuberculosis(itb) and crohn’s desease(cd), the categories of postoperative
slides are divided into three types (normal, cd, itb), and doctors will summa-
rize the binary results of patients (itb or cd) based on slide-level labels [6]. therefore, as shown in
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"the ﬁrst
method is to directly average the prediction values of slides or take the maxi-
mum prediction value [9]. this method is relatively simple, but the information
exchange between slides is not fully utilized, which may lead to errors in the sum-
mary result. the second method is to treat slide-patient as a new mil problem
according to the traditional mil thinking, where slides are regarded as instances
and patient labels as bags."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"although this method seems reasonable, the number
of patients is usually relatively small, and deep learning models usually require a
large amount of data for training. therefore, the insuﬃcient number of samples
at the slide-patient level may make it diﬃcult for the model to learn enough
information.
to address the multi-level multi-instance learning (ml-mil) problem in med-
ical ﬁeld, we propose a novel framework called patients and slides are equal
(p&sre). inspired by the iterative labeling process in medical diagnosis, this
framework treats patients and slides as instances at the same level and uses
transformers and attention mechanisms to build connections between them."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"1. description and solutions for the ml-mil problem. slide-level feature vectors; then, at the slide-patient level, we use self-attention
mechanisms to combine the slides of the same patient into patient-level feature
vectors, and treat these patient-level feature vectors together with all slide-level
feature vectors of the same patient as instances at the same level, which are
inputted into transformers for feature interaction and prediction of patient- and
slide-level labels. our method can eﬀectively solve the problem of diﬃcult train-
ing due to the scarcity of samples at the highest level in ml-mil, and can be
integrated into two state-of-the-art methods to further improve performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"our contributions include:
1) proposing a novel general framework to address the unique “patch-slide-
patient” ml-mil problem in the medical ﬁeld. before this, no other frame-
work had directly tackled this speciﬁc problem, making our proposal a
ground-breaking step in the application of ml-mil in healthcare;
2) proposing a simple yet highly eﬀective method that leverages self-attention
mechanisms and transformer models to enhance the interaction between slide
and patient information. this innovative approach not only improves the clas-
siﬁcation performance at the patient level but also at the slide level, show-
casing its eﬀectiveness and versatility;
3) conducting extensive experiments on two separate datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"the second part is the patient-slide level mil, which
generates patient-level features using attention mechanism and interacts the fea-
tures with transformer. to enhance readability, we ﬁrst provide the following
symbolization for ml-mil: for a patient xi, it has a patient-level classiﬁcation
label yi. for patient xi, there may exist ni slides si={sj|j=1 to ni}, where the
classiﬁcation label for each slide sj is denoted as zj."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"patient-level feature generation based on self-attention. doctors usu-
ally select certain key slides for careful observation and information aggrega-
tion during diagnosis, similar to the self-attention mechanism. therefore, we
directly use a fully connected (fc) layer to integrate the feature-level features
into patient-level features vi through attention mechanism, serving as patient
instances."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"after doctors summarize the patient-level
results, they typically review the slides to double-check the diagnosis results. this
patient-slide feature interaction (psfi) naturally lends itself to the construction
of a transformer, and information exchange and integration between slides and
patient level are bidirectional. thus, self-attention is more ideal for this purpose
than other kinds of attention (such as cross-attention or doctors’ attention)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"by
using the self-attention-based transformer structure, each input token is treated
equally (i.e., viewed as the same instance level), and tokens can interact exten-
sively with each other, enabling mutual correction between patients and slides
and even between slides. speciﬁcally, we merge the slide feature set {hj} and
the patient feature vi into the input tokens t in
i
= {h1, h2, ..., hni, vi} = {t},
68
f. li et al.
and then input them into a multi-layer transformer through self-attention and
feed-forward neural network layers to obtain the interaction information between
slides and output tokens t out
i
:
βk,l = softmax(w qtk
t (w ktl)/
√
d)
(4)
tk =
ni+1

l=1
βk,lw v tl
(5)
t′
k = relu(tkw r + b1)w o + b2
(6)
where d is the dimension of the token, and tk and tl come from t in
i . βk,l
is multi-head attention matrix, and w q, w k, and w v are weight matrices of
query, key, and value, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"during training, we sampled one
patient at a time and pre-extracted their batch-level features for all slides, in
order to save gpu memory. [7] loss function.
3
experiments and results
3.1
dataset and evaluation
cd-itb dataset. cd-itb is a private dataset consisting of 853 slides from 163
patients, with binary patient-level labels of cd or itb in a ratio of 103:60 and
tri-class slide-level labels of cd, itb, and normal slides in a ratio of 436:121:296,
respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"we adopted a patient-level stratiﬁcation approach for 5-fold
cross-validation, with 20% of the training set randomly assigned as the valida-
tion set for each fold. the dataset comprises an average of 2.3k instances per
bag, with the largest bag containing over 16k instances.
camelyon17 dataset. [1] is a publicly dataset, and its train-
ing set comprises 500 slides from 100 breast cancer patients with lymph node
metastases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"the patients are divided into two groups
p&sre: a ml-mil framework for pathological image analysis
69
based on their pn stage, namely lymph node positive and lymph node negative,
in proportions of 24:76, respectively. the data folding method is the same as
the cd-itb dataset. the average number of instances per bag is approximately
6.1k, and the largest bag contains over 23k instances.
metrics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"our p&sre framework improves both abmil and
dsmil methods at both levels. abmil with p&sre improves the f1 score
from 0.565 to 0.579 for the cd-itb dataset and from 0.529 to 0.571 for the
camelyon17 dataset at the slide-level, and improves the f1 score from 0.522
to 0.599 for the cd-itb dataset and from 0.842 to 0.861 for the camelyon17
dataset at the patient-level. therefore, the ablation experiments demonstrate
the eﬀectiveness of p&sre in enhancing the classiﬁcation performance at both
the slide and patient levels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_7.pdf,"based on existing
state-of-the-art mil methods, we then extend the framework to p&sre, which
p&sre: a ml-mil framework for pathological image analysis
71
conducts feature extraction and interaction at the slide-patient level. by intro-
ducing a transformer, the framework enables iterative interaction and correction
of information between patients and slides, resulting in better performance at
both the patient level and slide level compared to existing state-of-the-art algo-
rithms on two validation datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"transfer learning is a critical technique in training deep neu-
ral networks for the challenging medical image segmentation task that
requires enormous resources. with the abundance of medical image data,
many research institutions release models trained on various datasets
that can form a huge pool of candidate source models to choose from. hence, it’s vital to estimate the source models’ transferability (i.e., the
ability to generalize across diﬀerent downstream tasks) for proper and
eﬃcient model reuse."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"due to the large amount of learnable param-
eters in neural networks, suﬃcient annotated training samples are required for
training. however, the labeling process of medical images is tedious and time-
consuming. to address this problem, the common paradigm of transfer learning,
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_64.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"[10,21,30].
compared with the distributed training across multiple centers, there are no
speciﬁc ethical issues or computational design of distributed/federated learning
frameworks with the “pre-train-then-ﬁne-tune” workﬂow. previous works mainly focused on the ﬁne-tuning strategy to eﬀectively adapt
the knowledge from the pre-trained models to target tasks [4,12,19,26]. [18] enable researchers
to experiment across a large number of downstream datasets and tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"however, it has
been observed by recent works [23] that the pre-trained models cannot always
beneﬁt the downstream tasks. when the knowledge is transferred from a less
relevant source, it may not improve the performance or even negatively aﬀect
the intended outcome [24]. a brute-force method is to ﬁne-tune a set of pre-
trained models with target datasets to ﬁnd the optimal one."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"this process is time-
consuming and laborious. however, most of these works
require source information available while medical images have more privacy and
ethical issues and fewer datasets are publicly available than natural images. considering the issues mentioned above, this work focused on source-free
pre-trained model selection for segmentation tasks in the medical image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"as
shown in fig. 1, models pre-trained by upstream data constitute the model bank. the main idea is to directly measure the transferability of the pre-trained mod-
els without fully training based on the downstream/target dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"among the
recent works, leep [15] and its variant [1,13] were developed to utilize the log-
likelihood between the target labels and the predictions from the source model. logme [27] computed evidence based on the linear parameters assumption and
eﬃciently leverages the compatibility between features and labels. gbc [17]
applied the gaussian distribution to each class, and estimate the separability
between classes as the basis for transferability estimation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"transrate [9] evalu-
ated the transferability of models with the compactness and the completeness
of embedding space. [5] contended that discriminability and transfer-
ability are crucial properties of representations and introduce the information
bottleneck theory for transferability estimation. these methods have achieved
promising performance on classiﬁcation and regression tasks without fully con-
sidering the properties of medical image segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"first, unlike classiﬁcation
and regression problems that can use a single n-dimensional feature vector to rep-
resent each image, segmentation problems lack a global semantic representation,
which poses diﬃculties for direct transferability estimation. in addition, most
label-comparison-based methods [9,15,17,27] focus on the relationship between
the embeddings and downstream labels without exploring the eﬀectiveness of the
features themselves. third, medical images face severe class imbalance problems,
with excessive diﬀerences between foreground and background."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"however, exist-
ing algorithms rarely give additional attention to the class imbalance problem. 676
y. yang et al.
fine-tune
performance
model bank 
dnn1
upstream data
downstream data
dnn2
dnn3
pre-train-then-fine-tune process
pre-train
match or not? forward propagation
point sampling
class 1
class 2
class n
feature bank
framework of cc-fv method
feature unit sphere
feature analysis
estimation
estimation
transferability
estimation
source-free model selection problem
fine-tune
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"our main goal is to
predict the performance of models in the model bank after ﬁne-tuning on downstream
tasks without actually ﬁne-tuning. note that the upstream data are not available in
our model selection process. besides, for semantic segmentation tasks, the feature pyramid is critical for
the segmentation output of multi-scale objects while existing works neglect it."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"after ﬁne-tuning,
the performance of mi can be measured with the segmentation metric (e.g. dice
score), which is denoted by pi
s→t in this paper. i
s→t without ﬁne-tuning the model on target datasets. a perfect transferability score should preserve the ordering, i.e. t i
s→t > t j
s→t if
and only if pi
s→t > p j
s→t.
2.2
class consistency with feature variety constraint te method
the transferability of models from a weakly related source domain to a target
domain can be compromised if the domains are not suﬃciently comparable [24]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"this intrigues us about the question of “what kind of models are transferable"". the proposed method is intuitive and straightforward: features extracted by the
pre-trained model should be consistent within the class of the target dataset
while representative and various globally. therefore, class consistency and
feature variety are considered to estimate the transferability between mod-
els and downstream data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"the pre-trained models are trained with speciﬁc pretext
tasks based on the upstream dataset. therefore, features extracted by the pre-
trained models cannot perfectly distinguish the foreground and background of
target data. if the features are generalizable, foreground region features will
likely follow a similar distribution even without ﬁne-tuning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"and σf k
j
and
σf k
j′ are covariance matrices of fk
j and fk
j′. compared to some commonly used
metrics like kl-divergence or bhattacharyya distance [17], wasserstein distance
is more stable during the computation of high-dimensional matrices because it is
unnecessary to compute the determinant or inverse of a high-dimensional matrix,
which can easily lead to an overﬂow in numerical computation. we calculate the
wasserstein distance of the distribution with voxels of the same class in a sample
pair comprised of every two samples in the dataset, and obtained the following
deﬁnition of class consistency ccons
ccons =
1
n(n − 1)
c

k=1

ij
w2(fk
i , fk
j )
(2)
given that 3d medical images are computationally intensive, and prone to
causing out-of-memory problems, in the sliding window inference process for
678
y. yang et al.
each case, we do not concatenate the output of each patch into the ﬁnal predic-
tion result, but directly sample from the patched output and concatenate them
into the ﬁnal sampled feature matrix. in the calculation of class consistency, we
only sample the foreground voxels with a pre-deﬁned sampling number which is
proportional to the voxel number of each class in the image because of the severe
class imbalance problem."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"as a result of learning some trivial solutions, some overﬁtted models
have limited generalization capacity and are diﬃcult to apply to new tasks. we
believe that the essential reason for this phenomenon is that class consistency is
only concerned with local homogeneity of information while neglecting the inte-
gral feature quality assessment. hence we propose the feature variety constraint,
which measures the expressiveness of the features themselves and the uniformity
of their probability distribution."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"besides, we decrease the sampling
ratio in the decoder layer close to the bottleneck to avoid feature redundancy. the ﬁnal transferability of pre-trained model m to dataset t tm→t is
tm→t = 1
d
d

i=1
log
fi
v
ci
cons
(5)
where d is the number of decoder layers used in the estimation. [2] dataset is composed of ten dif-
ferent datasets with various challenging characteristics, which are widely used in
the medical image analysis ﬁeld."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"to evaluate the eﬀectiveness of cc-fv, we con-
duct extensive experiments on 5 of the msd dataset, including task03 liver(liver
and tumor segmentation), task06 lung(lung nodule segmentation), task07 pan-
creas(pancreas and pancreas tumor segmentation), task09 spleen(spleen seg-
mentation), and task10 colon(colon cancer segmentation). all of the datasets
are 3d ct images. the public part of the msd dataset is chosen for our experi-
ments, and each dataset is divided into a training set and a test set at a scale of
80% and 20%."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"since model
selection generally picks the top models and ignores the poor performers, we
assign a higher weight to the good models in the calculation, known as weighted
kendall’s τ. the pearson coeﬃcient also ranges from [-1, 1], and measures how
well the data can be described by a linear equation. the higher the pearson
coeﬃcient, the higher the correlation between the variables."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_64.pdf,"we have
standardized the various metrics uniformly, aiming to observe a positive relationship
between higher performance and higher transferability estimations. 3. visualization of features with same labels using t-sne. points with diﬀerent
colors are from diﬀerent samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"transformer-based multiple instance learning (mil) frame-
work has been proven advanced for whole slide image (wsi) analysis. however, existing spatial embedding strategies in transformer can only
represent ﬁxed structural information, which are hard to tackle the scale-
varying and isotropic characteristics of wsis. moreover, the current mil
cannot take advantage of a large number of unlabeled wsis for training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"moreover, we propose
a position-aware cross-attention (paca) module with a kernel reorien-
tation (kro) strategy, which makes pama able to maintain spatial
integrity and semantic enrichment during the training. we evaluated the
proposed method on a public tcga-lung dataset with 3,064 wsis and
an in-house endometrial dataset with 3,654 wsis, and compared it with
6 state-of-the-art methods. the results of experiments show our pama
is superior to sota mil methods and ssl methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"keywords: wsi representation learning · self-supervised learning
1
introduction
in the past few years, the development of histopathological whole slide image
(wsi) analysis methods has dramatically contributed to the intelligent cancer
diagnosis [4,10,15]. however, due to the limitation of hardware resources, it is
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 69. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp. 714–724, 2023."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"the bias and error generated in each level of the representation
model will accumulate in the ﬁnal decision model. moreover, the vit [6] back-
bone used in hipt is originally designed for nature sense images in ﬁxed sizes
whose positional information is consistent. however, histopathological wsis are
scale-varying and isotropic."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"the positional embedding strategy of vit will bring
ambiguity into the structural modeling. [19] built
hierarchical masks based on local anchors to maintain multi-scale relative dis-
tance information in the training. but these masks are manually deﬁned which is
not trainable and lacked orientation information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"pama can be trained end-to-end from
the local features to the wsi-level representation. moreover, we designed a
position-aware cross-attention mechanism to guarantee the correlation of local-
to-global information in the wsis while saving computational resources. the
proposed approach was evaluated on a public tcga-lung dataset and an in-
716
k. wu et al.
house endometrial dataset and compared with 6 state-of-the-art methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"(2) we pro-
pose a position-aware cross-attention (paca) module with a kernel reorienta-
tion (kro) strategy, which makes the framework able to maintain the spa-
tial integrity and semantic enrichment of slide representation during the self-
supervised training. (3) the experiments on two datasets show our pama can
achieve competitive performance compared with sota mil methods and ssl
methods.
fig. [7] is a successful ssl framework that learns image presentations by recon-
structing the masked image in the original pixel space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"after training, the pre-trained encoder will be
employed as the backbone for various downstream tasks. 2.3
position-aware cross-attention
to preserve the structure information of the tissue, we propose the position-
aware cross-attention (paca) module, which is the core of the encoder and
decoder blocks. the structure of paca is shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"= σ(k(n)w(n)
q
· (x(n)w(n)
k )
t
√de
+ϕd(d(n))+ϕp(p(n)))·(x(n)w(n)
v ), (1)
where wl ∈ rdf ×de, l = q, k, v are learnable parameters with de denoting the
dimension of the head output, σ represents the softmax function, and ϕd and ϕp
are the embedding functions that respectively take the distance and polar angle
as input and output the corresponding trainable embedding values. (2)
the two-way communication makes the patches and anchors timely trans-
mit local information and perceive the dynamic change of global information. the embedding of relative distance and polar angle information helps the model
maintain the semantic and structural integrity of the wsi and meanwhile pre-
vents the wsi representation from collapsing to the local area throughout the
training process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"namely,
it is isotropic. embedding the orientation information with a ﬁxed polar axis
will lead to ambiguities in various slides. to address this problem, we design a kernel reorientation (kro) strategy to
dynamically update the polar axis during the training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"based on the updated polar
axis, we can then amend p(n) to p(n+1). the detailed algorithm is described in
algorithm 1.
position-aware masked autoencoder
719
3
experiments and results
3.1
datasets
we evaluated the proposed method on two datasets, the public tcga-lung and
the in-house endometrial dataset, which are introduced as follows. algorithm 1: kernel reorientation algorithm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"for j in np do
p(n+1)
h,i,j
= p(n)
h,i,j − p(n)
h,i,max;// reorientation. end
end
end
tcga-lung dataset is collected from the cancer genome atlas (tcga)
data portal. the dataset includes a total of 3,064 wsis, which consist of three
categories, namely tumor-free (normal), lung adenocarcinoma (luad), and
lung squamous cancer (lusc),
endometrial dataset includes 3,654 wsis of endometrial pathology, which
includes 8 categories, namely well/moderately/low-diﬀerentiated endometrioid
adenocarcinoma, squamous diﬀerentiation carcinoma, plasmacytoid carcinoma,
clear cell carcinoma, mixed-cell adenocarcinoma, and benign tumor."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"each dataset was randomly divided into training, validation and test sets
according to 6:1:3 while keeping each category of data proportionally. we con-
ducted wsi multi-type classiﬁcation experiments on the two datasets. the val-
idation set was used to perform an early stop."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"during the downstream classiﬁcation task,
720
k. wu et al.
fig. 2. semi-supervised experiments with 10%, 35%, 60% and 85% of labelled data
on the endometrial dataset. solid lines represent ﬁne-tuning results and dotted lines
represent liner probing results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"table 1. ablation study on 35% of labelled endometrial dataset. following the protocol in self-
supervised learning [7], we evaluated the quality of pre-training with the two
approaches: 1) fine-tuning is to train the whole network parameters, including
wsi encoder and classiﬁer; 2) linear probing is to freeze the encoder and only
train the classiﬁer. [7] framework,
which was concatenated with patch tokens."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"we imple-
mented all the models in python 3.8 with pytorch 1.7 and cuda 10.2 and run
the experiments on a computer with 4 gpus of nvidia geforce 2080ti. position-aware masked autoencoder
721
3.3
eﬀectiveness of the wsi representation learning
we ﬁrst conducted experiments on the endometrial dataset to verify the eﬀec-
tiveness of self-supervised learning for wsi analysis under label-limited condi-
tions. the results are shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"[7],
which is referred to as mae+ in fig. 2.
table 2. comparison with weakly-supervised mil and slide-level self-learning study
on the two datasets for sub-type classiﬁcation. these results have demonstrated
the eﬀectiveness of pama in wsi representation pre-training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"and, when removing both the distance and polar angle
embedding, the auc drops by 0.034. these results demonstrate that local and
global spatial information is crucial for pama to learn wsi representations. the results are shown in table 2. overall, pama consistently achieves the best
performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"in comparison with the second-best methods, pama achieves
an increase of 0.015/0.011 and 0.025/0.009 in aucs on tcga and endome-
trial datasets, respectively, by using 35%/100% labeled wsis. moreover, pama
reveals the most robust capacity when reducing the training data from 100% to
35%, with auc decreasing slightly from 0.988 to 0.982 and from 0.851 to 0.829
on the two datasets. [19] are state-of-
the-art methods for histopathological image classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_69.pdf,"it is the main reason that the three methods cannot surpass our
method even with 100% training wsis.
4
conclusion
in this paper, we proposed an eﬀective self-supervised representation learn-
ing framework for wsi analysis. the experiments on two large-scale datasets
have demonstrated the eﬀectiveness of pama in the condition of limited-label. the results have shown superiority to the existing weakly-supervised and self-
supervised mil methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"deep learning-based medical image enhancement methods
(e.g., denoising and super-resolution) mainly rely on paired data and
correspondingly the well-trained models can only handle one type of
task. in this paper, we address the limitation with a diﬀusion model-
based framework that mitigates the requirement of paired data and can
simultaneously handle multiple enhancement tasks by one pre-trained
diﬀusion model without ﬁne-tuning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"however, ct imaging has relatively high radiation doses that can pose a risk of
radiation exposure to patients. [9].
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1_1.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14222, pp. mr imaging, on the other hand, uses a strong magnetic ﬁeld and radio waves
to create detailed images of the body’s internal structures, which can produce
high-contrast images for soft tissues and does not involve ionizing radiation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"[18], which limits its ability to visualize small structures or abnormalities. motivated by the aforementioned, there is a pressing need to improve the
quality of low-dose ct images and low-resolution mr images to ensure that
they provide the necessary diagnostic information. these algorithms are capable of improving image quality, but they have
two signiﬁcant limitations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"first, paired images are required for training,
e.g., low-dose and full-dose ct images; low-resolution and high-resolution mr
images). however, acquiring such paired data is challenging in real clinical sce-
narios. although it is possible to simulate low-quality images from high-quality
images, the models derived from such data may have limited generalization abil-
ity when applied to real data [9,14]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"the training of diﬀusion models requires a signiﬁcant
amount of computational resources and training images. for example, openai’s
improved diﬀusion models [21] took 1600–16000 a100 hours to be trained on
the imagenet dataset with one million images, which is prohibitively expensive. [22,31], but they still rely on paired images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"[12,28], we further introduce
a paradigm for plug-and-play ct and mr image denoising and super-resolution
as shown in fig. notably, it eliminates the need for paired data, enabling
greater scalability and wider applicability than existing paired-image dependent
pre-trained diﬀusion models
5
fig. 1. comparison of (a) the common paired-image dependent paradigm and (b) the
plug-and-play paradigm for medical image enhancement."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"2
method
this section begins with a brief overview of diﬀusion models for image generation
and the mathematical model and algorithm for general image enhancement. we
then introduce a plug-and-play framework that harnesses the strengths of both
approaches to enable unsupervised medical image enhancement.
2.1
denoising diﬀusion probabilistic models (ddpm)
for unconditional image generation
image generation models aim to capture the intrinsic data distribution from
a set of training images and generate new images from the model itself. [11] for unconditional medical image generation, which contains a
diﬀusion (or forward) process and a sampling (or reverse) process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"intuitively, a
denoising network is trained with the mean square loss et,x||ϵ − ϵθ(xt, t)||2. the
sampling process aims to generate a clean image from gaussian noise xt ∼
n(0, i), and each reverse step is deﬁned by:
xt−1 =
1
√αt

xt − 1 − αt
√1 − ¯αt
ϵθ(xt, t)

+ βtz; z ∼ n(0, i). (2)
2.2
image enhancement with denoising algorithm
in general, image enhancement tasks can be formulated by:
y = hx + n,
(3)
where y is the degraded image, h is a degradation matrix, x is the unknown
original image, and n is the independent random noise."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"for instance, in the image denoising task, h is
the identity matrix, and in the image super-resolution task, h is the downsam-
pling operator. = arg min
x ∥y − hx∥2 + r(x),
(4)
where the ﬁrst data-ﬁdelity term keeps the data consistency and the second data-
regularization term r(x) imposes prior knowledge constraints on the solution. = h†y,
(5)
where h† := ht (hht )−1 is the pseudo inverse of the degradation matrix h
and ∥f∥ht h := f t ht hf."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"notably,
the ﬁnal algorithm is equivalent to ddnm [28], but it is derived from diﬀerent
perspectives. 8
j. ma et al.
algorithm 1. pre-trained ddpm for plug-and-play medical image enhancement
require: pre-trained ddpm ϵθ, low-quality image y, degradation operator h
1: initialize xt ∼ n(0, i). project x0|t on the hyperplane y = hx
5:
xt−1 =
√
¯αt−1βt
1−¯αt
ˆx0|t +
√αt(1−¯αt−1)
1−¯αt
xt + βtz, z ∼ n(0, i) // sampling
6: end for
7: return enhanced image x0
3
experiments
dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"we conducted experiments on two common image enhancement tasks:
denoising and sr. to mimic the real-world setting, the diﬀusion models were
trained on a diverse dataset, including images from diﬀerent centers and scan-
ners. the testing set (e.g., mr images) is from a new medical center that has
not appeared in the training set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"experiments show that our model can gen-
eralize to these unseen images. notably, the presented framework elimi-
nates the requirement of paired data. [21] based on the full-dose dataset that contains 5351
images, and the hold-out quarter-dose images were used for testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"the testing
images were downsampled by operator h with factors of 4× and 8× to produce
low-resolution images, and the original images served as the ground truth.
evaluation metrics. [29],
and visual information fidelity (vif) [24], which are widely used measures in
medical image enhancement tasks [9,17].
implementation details. we followed the standard conﬁguration in [21] to
train the diﬀusion model from scratch."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"supple-
mentary fig. 1 (a) visually compares the denoising results, showing that the
presented method eﬀectively removes the noise and preserves the anatomical
details, while other methods either fail to suppress the noise or result in loss of
tissue information. we also used the same pre-trained diﬀusion model for simultaneously denois-
ing and sr by setting h as the downsampling operator."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"our results still outperformed ivlr
and dps in all metrics. dip obtains slightly better scores in psnr for the 4× sr
task and psnr and ssim for the 8× sr tasks, but visualized image quality is
signiﬁcantly worse than our results as shown in supplementary fig. 2, e.g., many
anatomical structures are smoothed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_1.pdf,"this is because perceptual and distortion
qualities are in opposition to each other as theoretically proven in [2]. dip mainly
prioritizes the distortion measures for the noise-free sr tasks while our results
achieve a better trade-oﬀ between the perceptual and distortion quality."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf,"previous polyp segmentation net-
works based on supervised binary masks may have lacked global seman-
tic perception of polyps, resulting in a loss of capture and discrimination
capability for polyps in complex scenarios. to address this issue, we
propose a novel gaussian-probabilistic guided semantic fusion method
that progressively fuses the probability information of polyp positions
with the decoder supervised by binary masks. our probabilistic model-
ing ensemble vision transformer network(petnet) eﬀectively sup-
presses noise in features and signiﬁcantly improves expressive capabilities
at both pixel and instance levels, using just simple types of convolu-
tional decoders."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf,"extensive experiments on ﬁve widely adopted datasets
show that petnet outperforms existing methods in identifying polyp
t. ling and c. wu—equal contributions. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_54.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43990-2_54
petnet improves complex polyp segmentation
573
camouﬂage, appearance changes, and small polyp scenes, and achieves a
speed about 27fps in edge computing devices."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf,"our contributions are
threefold:
• we propose a novel gaussian-probabilistic guided semantic fusion method for
polyp segmentation, which improves the decoder’s global perception of polyp
locations and discrimination capability for polyps in complex scenarios. 574
t. ling et al.
• we evaluate the performance of petnet on ﬁve widely adopted datasets,
demonstrating its superior ability to identify polyp camouﬂage and small
polyp scenes, achieving state-of-the-art performance in locating polyps with
high precision. furthermore, we show that petnet can achieve a speed of
about 27fps in edge computing devices (nvidia jetson orin)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf,"we add a
mta layer to encode the last level features, enhancing the model’s semantic
representation and accelerating the training process [16]. moreover, the encoder
output features are presented as {xe
i }4
i=1 with channels of [2c, 4c, 8c, 16c].
2.3
gaussian-probabilistic modeling group
to incorporate both polyp location probability and surface pattern information
in a progressive manner, we propose the gaussian probabilistic-induced tran-
sition (git) method. this method involves the interaction between a gaussian
auxiliary decoder and multiple binary decoders in a layer-wise fashion, as shown
in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf,"table s1 displays the results of our model’s
training and learning performance. our model achieves comparable performance
to the sota model on the kvasir-seg and clinicdb datasets. notably, our
model yields superior results in false-positive instance evaluation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf,"the generalization results are shown in table 1. we
conduct three unseen datasets to test models’ generalizability. results show that
petnet achieves excellent generalization performance compared with previous
models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf,"diminutive polyps are hard to precisely detect,
while they are the major targets of optical biopsies performed by endoscopists. we selected images from two unseen datasets with 0∼2% polyp labeled area
to perform the test. as shown, petnet demonstrates great strength in both
datasets, which indicates that one of the major advantages of our model lies in
detecting small polyps with lower false-positive rates."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf,"we observe that while the impact of each binary decoder varies, all sub binary
decoders contribute to the overall performance. furthermore, the git method
signiﬁcantly enhances instance-level evaluation without incurring performance
penalty in pixel-level evaluation, especially in unseen datasets. petnet improves complex polyp segmentation
579
table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_54.pdf,"quantitative results of the test datasets endoscene, cvc-colondb and
etis-laribpolypdb. quantitative results of the small polyp detection in etis and cvc-
colondb dataset. small polyps are deﬁned as the polyp area accounts for 0∼2% of the
entire image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"automatic whole slide image
(wsi) classiﬁcation is highly demanded, for the huge burden of dis-
ease control and prevention. however, the wsi-based computer-aided
vcc screening method is still vacant due to the scarce labeled data
and unique properties of candida. candida in wsi is challenging to be
captured by conventional classiﬁcation models due to its distinctive elon-
gated shape, the small proportion of their spatial distribution, and the
style gap from wsis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"our experimental results
demonstrate that our framework achieves state-of-the-art performance. code and example data are available at https://github.com/caijd2000/
miccai2023-vvc-screening. keywords: whole slide image · vulvovaginal candidiasis ·
attention-guided
1
introduction
vulvovaginal candidiasis (vvc) is a type of fungal infection caused by candida,
which results in discomforting symptoms, including itching and burning in the
genital area [4,18]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"in recent years, tct has
become mainstream in cervical disease screening compared to pap smear [8]. however, partially due to
the limited data and annotation, screening for candidiasis is mostly understud-
ied. computer-aided diagnosis for candidiasis through wsi is highly challenging
(see examples in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"(3) the staining
of diﬀerent samples leads to the huge style gap between wsis. while collecting
more candida data may contribute to a more robust network, such eﬀorts are
dwarfed by the inhomogeneity of wsis, which adds to the risk of overﬁtting. all
of the above issues make it diﬃcult for diagnostic models to focus on candida,
thus resulting in poor classiﬁcation performance and generalization capability."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"however, the unique shape and appear-
ance of the candidate incur troubles for cnn-based classiﬁers, whose spatial
ﬁeld of view can be relatively limited. in recent years, vision transformer (vit)
has been widely used in visual tasks for its global attention mechanism [14],
sensitivity to shape information in images [19], and robustness to occlusion [12]. nevertheless, such a transformer can be hard to train for our task, due to the
large image size, huge network parameters, and huge demand for training data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"progressive attention guidance
237
speciﬁcally, we use the pre-trained cnn-based encoder to extract features
for each cropped image. the feature maps extracted after the ﬁrst layer is consid-
ered low-level, which contains ﬁne-grained texture information. on the contrary,
the feature maps extracted from the last layer are high-level, which represents
semantics regarding candida."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"the class token ’cls’ is used for the ﬁnal classiﬁcation. the combined feature
maps can oﬀer more representative information so that the classiﬁer focuses more
on diﬀerent scales to long-range candida. meanwhile, the extra ssa structure is
simple, which causes a low computation burden."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"we take the average
grayscale of attention map ¯
m as a restriction, as shown in the last part of eq. 2.
ltri,lam, and lfocus are combined as lcl to constrain each other and take full
advantage of contrastive learning, as shown in eq. + s(imasked) + ¯
m.
(2)
finally, we use the cross-entropy loss lce to calculate the classiﬁcation loss
with labels. the total loss during training can be expressed as shown in eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"3.
α is a hyper-parameter, set to 0.1. l = lce(s(iaug), labels) + αlcl. (3)
2.4
aggregated classiﬁcation for wsi
with the strategies above, we have built the classiﬁer for all cropped images in a
wsi."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"we
complete the aggregation of the top-k features by the transformer and make the
wsi-level decision by an fc layer in the ﬁnal. 3
experimental results
datasets and experimental setup. our samples were collected by a collab-
orating clinical institute in 2021."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"the ratio of
training and validation is 4:1.
for training of the image-level classiﬁcation model, we use 1940 positive
images (1467 of which are used in detector pre-training) and 2093 negative
images. all images used to pre-train the detector are categorized as training
data here. the rest 473 images are split in 5-fold cross-validation, from which
we collect experimental results and report later."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"(pt: pre-training; ssa: skip self-attention; cl: contrastive loss). dataset-small is balanced with 100
positive wsis and 100 negative wsis. we conduct a 5-fold cross-validation, and
the ratio of training, validation, and testing is 3:1:1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"we further validate upon
an imbalanced dataset-large of 7654 wsis. there are only 140 positive wsis in
this dataset, which is closer to real world. these two wsi-level datasets have no
overlay with the data used to train the above detection and classiﬁcation tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"after pt, the model can focus on
the candida area, edges of cells, and folds that resemble candida, as shown in
fig. after adding the ssa module, more texture information is used to
240
j. cai et al.
fig. 4. (a) the original image, where the green box indicates candida (enlarged in
the left) and the red box shows the prediction of the detection model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"we compare our proposed
method to other methods in the whole slide of cervical disease screening. to save
computation, we did not verify the performance of the methods that performed
too poorly on dataset-small. the detection-based method [23] uses a detection
network to get suspicious candida and classify wsis with average conﬁdence.
resnet trained without our method is the same as the baseline in table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_23.pdf,"comparision of diﬀerent methods for wsi classiﬁcation. our attention-based method brings 6% improvement of
accuracy on data-small compared to other methods with the same wsi-level
method ’threshold’. transformer shows a better capacity of feature aggregation
than other wsi-level classiﬁers, raising the auc on dataset-large to 84.18%."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"automatic nuclei detection and classiﬁcation can produce
eﬀective information for disease diagnosis. most existing methods classify
nuclei independently or do not make full use of the semantic similarity
betweennucleiandtheirgroupingfeatures.inthispaper,weproposeanovel
end-to-end nuclei detection and classiﬁcation framework based on a group-
ingtransformer-basedclassiﬁer.thenucleiclassiﬁerlearnsandupdatesthe
representations of nuclei groups and categories via hierarchically grouping
the nucleus embeddings."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"for
the eﬃciency of the fully transformer-based framework, we take the nucleus
group embeddings as the input prompts of backbone, which helps harvest
grouping guided features by tuning only the prompts instead of the whole
backbone. experimental results show that the proposed method signiﬁ-
cantly outperforms the existing models on three datasets. for example, the
this work was supported in part by the chinese key-area research and development
program of guangdong province (2020b0101350001), in part by the national natural
science foundation of china (no. 62102267, no. 61976250), in part by the guangdong
basic and applied basic research foundation (2023a1515011464, 2020b1515020048),
in part by the shenzhen science and technology program (jcyj20220818103001002,
jcyj20220530141211024), and the guangdong provincial key laboratory of big data
computing, the chinese university of hong kong, shenzhen."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"j. huang and h. li—contribute equally to this work. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3_55.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. it is a challenge to infer
the nucleus types due to the diversity and unbalanced distribution of nuclei."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"a number of methods [7,10,14,23–25,33,34] have been proposed for auto-
matic nuclei segmentation and classiﬁcation. [28] for training to produce dense predictions with expensive pixel-level
labels. in this paper, we aim to obtain the location and category of cells, which
only needs aﬀordable labels of centroids or bounding boxes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"we propose a novel fully transformer-based framework for nuclei detec-
tion and classiﬁcation, by integrating a backbone, a centroid detector, and the
grouping-based classiﬁer. however, the transformer framework has a relatively
large number of parameters, which could cause high costs in ﬁne-tuning the whole
model on large datasets. on the other hand, there exist domain gaps in the patho-
logical images of diﬀerent organs, staining, and institutions, which makes it nec-
essary to ﬁne-tune models to new applications."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"inspired by the prompt tuning methods [13,16,20] which train continuous
prompts with frozen pretrained models for natural language processing tasks,
we propose a grouping prompt based learning strategy for eﬃcient tuning. we
prepend the embeddings of nucleus clusters to the input space and freeze the
entire pre-trained transformer backbone so that these group embeddings act
as prompt information to help the backbone extract grouping-aware features. our contributions are: (1) a prompt-based grouping transformer framework for
end-to-end detection and classiﬁcation of nuclei; (2) a novel grouping prompt
learning mechanism that exploits nucleus clusters to guide feature learning with
low tuning costs; (3) experimental results show that our method achieves the
state-of-the-art on three public benchmarks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"in the
prompt-tuning phase, grouping prompts are added to the input of the backbone
and gtc, while the backbone parameters are frozen. [10] is a colorectal nuclear dataset with three types, consisting
of 41 h&e stained image tiles from 16 colorectal adenocarcinoma whole-slide
images (wsis). the wsis are at 20× magniﬁcation and the size of the slides is
500 × 500."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"[2] algorithm to generate superpixels as instances and split them
into 80/10/30 slides for training/validation/testing. [9] has 291 histology images of colon tissue from six datasets, containing
nearly half a million labeled nuclei in h&e stained colon tissue. the wsis are
at 20× magniﬁcation with an average size of 1,016 × 917 pixels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"the number of grouping prompts g is 64. random crop,
ﬂipping, and scaling are used for data augmentation. our method is trained
with pytorch on a 48 gb gpu (nvidia a100) for 12–24 h (depending on the
dataset size)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"more details are listed in the supplementary material. as shown in table 1, our method exceeds all
1 https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/.
2 https://github.com/topoxlab/dataset-brca-m2c/.
3 https://warwick.ac.uk/fac/cross_fac/tia/data/lizard/.
prompt-based grouping transformer
575
table 1. comparison with existing methods on consep, brca-m2c and lizard. for each dataset, we report the f-score of each class (f k
c ), the mean f-score over
all classes (fc) and the detection f-score (fd)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"[22] by more than 1.5% and 6.4% on fd and
fc, respectively. meanwhile, we conduct t-test on consep dataset for statistical
signiﬁcance test. the details are listed in the supplementary material."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"the visual
comparisons are shown in fig. 4. with the context information from surrounding
nuclei, our method eﬀectively reduces the misclassiﬁcation rate of the lymphocytes
and neutrophil categories (blue and red). 3.3
ablation analysis
the strengths of the grouping transformer based classiﬁer and the
grouping prompts are veriﬁed on consep dataset, as shown in table 2.
prompt-based grouping transformer (pgt) is our proposed detection and clas-
siﬁcation architecture with grouping prompts and the gtc (in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"while the
576
j. huang et al.
fig. the visualization results on consep dataset. (color ﬁgure online)
table 2. ablation study on consep."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"our method surpasses the detached setting by 2.4%
in fd and 3.1% in fc, which suggests that sharing embeddings of groups and
prompts is eﬀective. with a frozen backbone, the performances of ‘w/o pt’ and
‘w/o gtc’ are both dropping, which veriﬁes the strength of the prompt tuning
and the gtc module, respectively.
table 3 shows the eﬀect of diﬀerent numbers of grouping prompts on
consep dataset. when the number of groups is small, the classiﬁcation result
is inferior."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_55.pdf,"* means
p-value ≤0.05. as shown in table 4, we calculate fd of each testing
image as sample data and conduct t-test to obtain p-values on the consep
dataset. the p-values are computed between our method and the others."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"cur-
rent state of the art methods are based on multi-instance learning
schemes (mil), which usually rely on pretrained features to represent the
instances. due to the lack of task-speciﬁc annotated data, these features
are either obtained from well-established backbones on natural images,
or, more recently from self-supervised models pretrained on histopathol-
ogy. however, both approaches yield task-agnostic features, resulting in
performance loss compared to the appropriate task-related supervision, if
available."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"we propose prompt-mil, an mil framework that integrates prompts
into wsi classiﬁcation. prompt-mil adopts a prompt tuning mecha-
nism, where only a small fraction of parameters calibrates the pretrained
features to encode task-speciﬁc information, rather than the conven-
tional full ﬁne-tuning approaches. extensive experiments on three wsi
datasets, tcga-brca, tcga-crc, and bright, demonstrate the
superiority of prompt-mil over conventional mil methods, achieving a
relative improvement of 1.49%–4.03% in accuracy and 0.25%–8.97% in
auroc while using fewer than 0.3% additional parameters."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"compared
to conventional full ﬁne-tuning approaches, we ﬁne-tune less than 1.3%
of the parameters, yet achieve a relative improvement of 1.29%–13.61%
in accuracy and 3.22%–27.18% in auroc and reduce gpu memory
consumption by 38%–45% while training 21%–27% faster. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3_60.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43993-3_60
prompt-mil: boosting mil schemes via prompt tuning
625
keywords: whole slide image classiﬁcation · multiple instance
learning · prompt tuning
1
introduction
whole slide image (wsi) classiﬁcation is a critical task in computational pathol-
ogy enabling disease diagnosis and subtyping using automatic tools."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"these features are then aggregated using
diﬀerent pooling or attention-based operators to provide a wsi-level prediction. imagenet pretrained networks have been widely used as mil feature extractors.
more recently, self-supervised learning (ssl), using a large amount of unlabeled
histopathology data, has become quite popular for wsi classiﬁcation [5,13] as
it outperforms imagenet feature encoders. most existing mil methods do not ﬁne-tune their feature extractor together
with their classiﬁcation task; this stems from the requirement for far larger
gpu memory than is available currently due to the gigapixel nature of wsis,
e.g. training a wsi at 10x magniﬁcation may require more than 300 gb of
gpu memory."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"however, we ﬁnd that conven-
tional ﬁne-tuning approaches, where the entire network is ﬁne-tuned, achieve low
performance. for example, on the bright dataset [2], the accuracy drops more
than 5% compared to the conventional mil approaches. the poor performance
is probably caused by the large network over-ﬁtted to the limited downstream
training data, leading to suboptimal feature representation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"recently, prompts have also been adopted in computer vision and
demonstrated superior performance compared to conventional ﬁne-tuning meth-
ods [10]. prompt tuning performs well even when only limited labeled data is
available for training, making it particularly attractive in computational pathol-
ogy. the process of prompt tuning thus involves providing a form of limited
guidance during the training of downstream tasks, with the goal of minimizing
the discrepancy between feature representations that are fully tuned to the task
and those that are not task-speciﬁc."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"by doing so, only the prompt
parameters together with the classiﬁer, are optimized. this avoids poten-
tial overﬁtting while still injecting task-speciﬁc knowledge into the learned
representations. extensive experiments on three public wsi datasets, tcga-brca, tcga-
crc, and bright demonstrate the superiority of prompt-mil over conven-
tional mil methods, achieving a relative improvement of 1.49%–4.03% in accu-
racy and 0.25%–8.97% in auroc by using only less than 0.3% additional
parameters."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"moreover, compared
to the full ﬁne-tuning approach, our method reduces gpu memory consump-
tion by 38%–45% and trains 21%–27% faster. to the best of our knowledge,
this is the ﬁrst work where prompts are explored for wsi classiﬁcation. while
our method is quite simple, it is versatile as it is agnostic to the mil scheme
and can be easily applied to diﬀerent mil methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"our code is available at
https://github.com/cvlab-stonybrook/promptmil.
2
method
our prompt-mil framework consists of three components: a frozen feature
model to extract features of tissue patches, a classiﬁer that performs an mil
scheme of feature aggregation and classiﬁcation of the wsis, and a train-
able prompt. given a wsi and its label y, the image is tiled into n tissue
prompt-mil: boosting mil schemes via prompt tuning
627
patches/instances {x1, x2, . . . , xn} at a predeﬁned magniﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,", k} is the trainable prompt consisting of k trainable tokens. the classiﬁer g(·) applies an mil scheme to predict the label ˆy and calculate
the loss l as:
l = lcls(ˆy, y) = lcls(g(h), y),
(2)
where the lcls is a classiﬁcation loss. 2.1
visual prompt tuning
the visual prompt tuning is the key component of our framework."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"tissue patches tiled from the input wsi are grouped into separate batches, which are
fed into a frozen feature model f(·) to compute their respective features. the features
are subsequently concatenated into the feature h and a classiﬁer g(·) applies an mil
scheme on h to predict the label and calculate the loss l. (b) structure of the feature
model f(·) with the additional prompt. an input image xi is cropped into w small
patches z1, . ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"the feature model f(·) is frozen
and only the prompt is trainable. 628
j. zhang et al.
where t0
i is the embedding token of zi and t0
z is the collection of such tokens. these tokens t0
z are concatenated with a class token t0
cls and a prompt p: the
class token is used to aggregate information from all other tokens."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"the concatenation is fed
into l layers of the transformer encoders:

t1
z, t1
p , t1
cls

= l1([t0
z, p, t0
cls])
(4)

ti
z, ti
p , ti
cls

= li([ti−1
z
, ti−1
p
, ti−1
cls ]), i = 2, 3, . . , k},
(6)
where pi
j is the jth output prompt token of the ith transformer encoder and ti
p
is the collection of all k such output prompt tokens, which are not trainable. hi = tl
cls."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"[13] as the classiﬁer and binary cross entropy
as the classiﬁcation loss lcls when the task is a tumor sub-type classiﬁcation or
cross entropy otherwise. these datasets were utilized for
both the self-supervised feature extractor pretraining and the end-to-end ﬁne-
tuning (with or without prompts), including the mil component. note that the
testing data were not used in the ssl pretraining."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"tcga-
crc contains 430 diagnostic digital slides of colorectal cancer for a binary clas-
siﬁcation task: chromosomal instability (cin) or genome stable (gs). following
the common 4-fold data split [1,16], we used the ﬁrst three folds for training (236
gs, 89 cin), and the fourth for testing (77 gs, 28 cin). we further split 20%
(65 slides) training data as a validation set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"the cropped
patches (1.24m training, 195k test) were extracted at 10× magniﬁcation. table 1. comparison of accuracy and auroc on three datasets. reported metrics
(in %age) are the average across 3 runs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"for ssl pretraining,
we leveraged the dino framework [4] with the default hyperparameters, but
adjusted the batch size to 256 and employed the global average pooling for token
aggregation. we pretrained separate vit models on the tcga-crc datasets
for 50 epochs, on the bright dataset for 50 epochs, and on the brca dataset
for 30 epochs. for tcga-brca, we used the adamw [17] optimizer with a
learning rate of 1e − 4, 1e − 2 weight decay, and trained for 40 epochs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"we train the full tuning model for 10 more epochs than our prompt training
to allow full convergence. this training strategy is optimized using the validation
datasets. [20] on a nvidia tesla
v100 or a nvidia quadro rtx 8000."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"the computationally intensive full ﬁne-tuning method under-performed con-
ventional mil and prompt-mil. compared to the full ﬁne-tuning method, our
method achieved a relative improvement of 1.29% to 13.61% in accuracy and
3.22% to 27.18% in auroc on the three datasets. due to the relatively small
amount of slide-level labels (few hundred to a few thousands) fully ﬁne tuning
prompt-mil: boosting mil schemes via prompt tuning
631
5m parameters in the feature model might suﬀer from overﬁtting."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"in contrast,
our method contained less than 1.3% of parameters compared to full ﬁne-tuning,
leading to robust training. table 2. comparison of gpu memory consumption and training speed per slide bench-
marked on the bright dataset between the full ﬁne-tuning and our prompt tuning
on four slides with diﬀerent sizes. our prompt method requires far less memory and is
signiﬁcantly faster."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"dataset
tcga-brca
bright
metric
accuracy auroc accuracy auroc
vit-small [27]
91.75
97.03
54.17
76.76
vit-small w/ prompt-mil 92.78
97.53
57.50
78.29
evaluation of time and gpu memory eﬃciency: prompt-mil is an
eﬃcient method requiring less gpu memory to train and running much faster
than full ﬁne-tuning methods. we evaluated the training speed and memory
consumption of our method and compared to the full ﬁne-tuning baseline on
four diﬀerent sized wsis in the bright dataset. as shown in table 2, our
method consumed around 38% to 45% less gpu memory compared to full ﬁne-
tuning and was 21% to 27% faster."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"evaluation on the pathological foundation models: we demonstrated
our prompt-mil also had a better performance when used with the pathologi-
cal foundation model. foundational models refer to those trained on large-scale
pathology datasets (e.g. the entire tcga pan-cancer dataset [28]). in table 3, we showed
that our method robustly boosted the performance on both tcga (the same
632
j. zhang et al.
domain as the foundation model trained on) and bright (a diﬀerent domain)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"for two diﬀerent wsi
classiﬁcation tasks, one token was enough to boost the performance of the conventional
mil schemes. dataset
tcga-brca
bright
#prompt tokens k accuracy auroc accuracy auroc
k = 1
93.47
96.89
64.58
81.31
k = 2
93.13
96.93
60.41
79.74
k = 3
93.47
96.86
59.17
76.75
ablation study: an ablation was performed to study the eﬀect of the number
of trainable prompt tokens on downstream tasks. table 4 shows the accuracy
and auroc of our prompt-mil model with 1, 2 and 3 trainable prompt tokens
(k = 1, 2, 3) on the tcga-brca and the bright datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"on the tcga-
brca dataset, our prompt-mil model with 1 to 3 prompt tokens reported
similar performance. on the bright dataset, the performance of our model
dropped with the increased number of prompt tokens. empirically, this ablation
study shows that for classiﬁcation tasks, one prompt token is suﬃcient to boost
the performance of conventional mil methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_60.pdf,"prompt-mil adopts a prompt tuning mechanism
rather than a conventional full ﬁne-tuning of the entire feature representation. in such a scheme, only a small fraction of parameters calibrates the pretrained
representations to encode task-speciﬁc information, so the entire training can be
performed in an end-to-end manner. we applied our proposed method to three
publicly available datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf,"in this work, we propose a novel radiomics-informed deep-
learning method, ridl, that combines the advantages of deep learning
and radiomic approaches to improve af sub-type classiﬁcation. unlike
existing hybrid techniques that mostly rely on na¨ıve feature concatena-
tion, we observe that radiomic feature selection methods can serve as
an information prior, and propose supplementing low-level deep neural
network (dnn) features with locally computed radiomic features. this
reduces dnn over-ﬁtting and allows local variations between radiomic
features to be better captured."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf,"keywords: atrial fibrillation · radiomics · ct imaging analysis
1
introduction
atrial ﬁbrillation (af) is a cardiac disease characterized by rapid, irregular
heartbeats [4]. the disease can lead to stroke and heart failure, and has a mortal-
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2 15.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. af is classiﬁed as either persistent atrial ﬁbril-
lation (peaf), where abnormal heart rhythms occur continuously for more than
seven days, or paroxysmal atrial ﬁbrillation (paaf), where the heart rhythm
returns to normal within seven days."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf,"in this work, we propose a novel approach to atrial ﬁbrillation sub-type clas-
siﬁcation from ct volumes by integrating radiomic and deep learning methods. we note that textural radiomic features identiﬁed by feature selection methods
can serve as an information prior to supplement low-level features from dnns,
since they are designed to capture low-level context and have predictive power
[23]. to this end, we locally calculate radiomic features based on patches sur-
rounding each voxel, and perform feature fusion with low-level dnn features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf,"and explained in detail
below. our dataset d := {(xi, yi)}n
i=1 includes n samples of input xi and binary
label yi, where 0 indicates paaf and 1 indicates peaf. xi has two channels, one
consisting of the 3d ct volume centered around the left atrium and the other
the binary region-of-interest (roi) mask indicating eat."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf,"(b) feature bank implementation for feature de-
correlation. light blue features indicate features in current training iteration (color
ﬁgure online)
xi. feature selection methodologies such as mutual information (mi), principal
component analysis (pca), or lasso regularization, are then used to iden-
tify predictive features for classiﬁcation [23]. radiomic features are classiﬁed
into shape, ﬁrst-order statistical features, and texture features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf,"conventional statistics such as entropy and correlation are then used to summa-
rize these measures [25], but these tend to be limited in their ability to capture
local heterogeneity, such as the varying textures on the surface of a cancer tumor. although dnn’s are more eﬀective at capturing local variations, they can overﬁt
without suﬃcient data for training [17]. unlike existing works that na¨ıvely concatenate radiomic and deep features
before the classiﬁcation layer [2,19], we observe that textural features selected
through radiomics feature selection algorithms are known to be predictive and
can be used as prior knowledge to improve low-level dnn features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf,"l = lcls + wcorrlcorr + lrec . (6)
3
experiments
3.1
implementation details
dataset. we use a dataset of 172 patients containing 94 paaf and 78 peaf
cases collected from the sun yat-sen memorial hospital in china."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf,"we use ﬁve-fold cross-validation and report average
test performance across folds. cross-validation is implemented by splitting the
dataset into ﬁve equal subsets and using three subsets for training, one subset
for validation, and one subset for testing. a rolling scheme is used such that
diﬀerent validation and test subsets are used for each of the ﬁve folds."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf,"baseline† is a na¨ıve hybrid
implementation using simple feature concatenation. to demonstrate the eﬀec-
tiveness of radiomic feature selection as prior knowledge for feature fusion, we
compare with results from using features discarded by radiomics feature selec-
tion. we randomly select three discarded features to generate local features rl
i as
input whilst keeping other components constant."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_15.pdf,"given the large set of radiomic features, it is possible some discarded fea-
tures may outperform selected features due to diﬀerences in global and local
computation. nevertheless, our results indicate that the radiomic feature selec-
tion process serves as an reasonable information prior. our work is the ﬁrst to
propose fusing locally computed radiomic features with low-level dnn features,
and we leave detailed local feature selection methods to future works."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"image-guided surgery requires fast and accurate registration to align
preoperative imaging and surgical spaces. the breast undergoes large nonrigid
deformations during surgery, compromising the use of imaging data for intra-
operative tumor localization. rigid registration fails to account for nonrigid soft
tissue deformations, and biomechanical modeling approaches like ﬁnite element
simulations can be cumbersome in implementation and computation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"computing a displacement ﬁeld using this method does not
require mesh discretization or large matrix assembly and inversion conventionally
associated with ﬁnite element or mesh-free methods. we solve for the optimal
superposition of regularized kelvinlet functions that achieves registration of the
medical image to simulated intraoperative geometric point data of the breast. we
present registration performance results using a dataset of supine mr breast imag-
ing from healthy volunteers mimicking surgical deformations with 237 individual
targets from 11 breasts."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"this
work focuses speciﬁcally on nonrigid breast registration, although these methods could
be adapted for other soft tissue organs. current guidance technologies for breast conserv-
ing surgery localize a single tumor-implanted seed without providing spatial information
about the tumor boundary. as a result, resections can have several centimeters of tissue
beyond the cancer margin."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"despite seed information and large resections, reoperation
rates are still high (~17%) emphasizing the need for additional guidance technologies
such as computer-assisted surgery systems with nonrigid registration [7].
intraoperative data available for registration is often sparse and subject to data col-
lection noise. image-to-physical registration methods that accurately model an elastic
soft-tissue environment while also complying with intraoperative data constraints is an
active ﬁeld of research. determining correspondences between imaging space and geo-
metric data is required for image-to-physical registration, but it is often an inexact and
ill-posed problem."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"deep learning image registra-
tion methods like voxelmorph have also been used for this purpose [10]. however, these
methods require extensive training data and may struggle with generalizability. other
non-learning image-to-physical registration strategies include [11] which utilized a coro-
tational linear-elastic ﬁnite element method (fem) combined with an iterative closest
point algorithm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"mesh-free methods have been introduced to circumvent
this limitation. the element-free galerkin method is a mesh-free method that requires
only nodal point data and uses a moving least-squares approximation to solve for a
solution [13]. other mesh-free methods are reviewed in [14]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"in this work, we propose an image-to-physical registration method that uses regu-
larized kelvinlet functions as a novel deformation basis for nonrigid registration. reg-
ularized kelvinlet functions are analytical solutions to the equations for linear elasticity
that we superpose to compute a nonrigid deformation ﬁeld nearly instantaneously [15].
346
m. ringel et al.
we utilize “grab” and “twist” regularized kelvinlet functions with a linearized iterative
reconstruction approach (adapted from [12]) that is well-suited for sparse data registra-
tion problems. sensitivity to regularized kelvinlet function hyperparameters is explored
on a supine mr breast imaging dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"[ktwist(r)]×f
(8)
2.2
registration task
for registration, x0 control point positions for k number of total regularized kelvinlets
“grab” and “twist” functions are distributed in a predetermined conﬁguration. then, the
348
m. ringel et al.
f grab and f twist vectors are optimized to solve for a displacement ﬁeld that minimizes
distance error between geometric data inputs. for a predetermined conﬁguration of regularized kelvinlet “grab” and “twist” func-
tions centered at different x0 control point locations, an elastically deformed state can
be represented as the summation of all regularized kelvinlet displacement ﬁelds where
∼u (x) is the superposed displacement vector and k = kgrab + ktwist in eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"by setting x0 locations, εgrab, and εtwist as hyperparameters,
deformation states can be represented by various α vectors with the registration task
being to solve for the optimal α vector. an objective function is formulated to minimize misalignment between the moving
space xmoving and ﬁxed space xﬁxed through geometric data constraints. for the breast
imaging datasets in this work, we used simulated intraoperative data features that real-
istically could be collected in a surgical environment visualized in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"these ﬁducials have known point correspondence. the other two data features are an
intra-ﬁducial point cloud of the skin surface (fig. 2, light blue) and sparse contour sam-
ples of the chest wall surface (fig. 2, yellow). these data features are surfaces that do
not have known correspondence."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"these data feature designations are consistent with
implementations in previous work [16, 17].
for a given deformation state, each data feature contributes to the total error measure. for the point data, the error ei
point for each point i is simply the distance magnitude
between corresponding points in xﬁxed and xmoving space. for the surface data, the error
ei
surface is calculated as the distance from every point i in the xﬁxed point cloud surface
to the closest point in the xmoving surface, projected onto the surface unit normal which
allows for sliding contact between surfaces.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"the optimal
state β is iteratively solved using levenberg-marquardt optimization terminating at
|(β)|<10–12.
(β) =
1
npoint
npoint

i=1
(ei
point)2 +
1
nsurface
nsurface

i=1
(ei
surface)2 + wse(ese)2
(11)
3
experiments and results
in this section, two experiments are conducted. the ﬁrst explores sensitivity to regu-
larized kelvinlet function hyperparameters kgrab, ktwist, εgrab, and εtwist and establishes
optimal hyperparameters in a training dataset of 11 breast deformations. the second
validates the registration method in a breast cancer patient and compares registration
accuracy and computation time to previously proposed methods.
3.1
hyperparameters sensitivity analysis
this dataset consists of supine breast mr images simulating surgical deformations of
11 breasts from 7 healthy volunteers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"the volunteers were then instructed
to raise one arm above their heads, causing deformation of the ipsilateral breast. a second
mr image in the deformed state was acquired to create simulated intraoperative physical
data and to use for validation. this second image was used as the xﬁxed space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"the posterior surface was labeled to inform x0
control point locations. the skin ﬁducials and intra-ﬁducial surface point clouds were
labeled in both images as data features. sparse tracked ultrasound data collection patterns
were projected on the posterior surface for use as the third data feature."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"3.
the registration with the lowest root mean squared error was from conﬁguration 3 kgrab
= 40, εgrab = 0.05, ktwist = 1, εtwist = 0.1. these hyperparameters were used on a
different dataset for validating and comparing the registration method in sect. 3. target error results from regularized kelvinlet functions hyperparameter sweeps."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"a 71-year-old patient with invasive mammary carcinoma in
the left breast was enrolled in a study approved by the institutional review board at
vanderbilt university. skin ﬁducial placement, image acquisition, arm placement, and
preprocessing steps followed the same protocol detailed in sect. the tumor was
segmented in both images by a subject matter expert, and a 3d tumor model was created
to evaluate tumor overlap metrics after registration."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_33.pdf,"image-to-image registration would not be possi-
ble for intraoperative registration in most surgical settings. however, it was included to
regularized kelvinlet functions to model linear elasticity
351
demonstrate accuracy when volumetric imaging data is available, as opposed to sparse
geometric point data as in the surgical application case. the rigid and image-to-physical
registrations were performed on a single thread of a 3.6 ghz amd ryzen 7 3700x
cpu."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"state-of-the-art machine learning models often learn spuri-
ous correlations embedded in the training data. this poses risks when
deploying these models for high-stake decision-making, such as in medical
applications like skin cancer detection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"in the ﬁrst step (1), r2r reveals
model weaknesses by ﬁnding outliers in attributions or through inspec-
tion of latent concepts learned by the model. secondly (2), the responsi-
ble artifacts are detected and spatially localized in the input data, which
is then leveraged to (3) revise the model behavior. concretely, we apply
the methods of rrr, cdep and clarc for model correction, and (4)
(re-)evaluate the model’s performance and remaining sensitivity towards
the artifact."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"this includes various medical
f. pahde and m. dreyer—contributed equally. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 56.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43895-0_56
reveal to revise: an xai life cycle for iterative bias correction of dnns
597
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"firstly, we identify model weaknesses by ﬁnding either outliers in expla-
nations using spray (1a) or suspicious concepts using zoomed-in crp concept visu-
alizations (1b). secondly (2), spray clusters or collecting the top reference samples
allows us to label artifactual samples and to compute an artifact cav, which we use
to model and localize the artifact in latent and input space, respectively. however, the reasoning of these highly
complex and non-linear models is generally not transparent [23,24], and as such,
their decisions may be biased towards unintended or undesired features, poten-
tially caused by shortcut learning [2,9,14,27]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"the ﬁeld of xai brings light into the black boxes of dnns and provides a bet-
ter understanding of their decision processes. as such, local xai methods reveal
(input) features that are most relevant to a model, which, for image data, can
be presented as heatmaps. in contrast, global xai methods (e.g., [12,14]) reveal
general prediction strategies employed or features encoded by a model, which is
necessary for the identiﬁcation and understanding of systematic (mis-)behavior.
acting on the insights from explanations, various methods have been introduced
to correct for undesired model behavior [31]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"[13,25].
598
f. pahde et al.
to that end, we propose reveal to revise (r2r), an iterative xai life cycle
requiring low amounts of human interaction that consists of four phases, illus-
trated in fig. speciﬁcally, r2r allows to ﬁrst (1) identify spurious model behav-
ior and secondly, to (2) label and localize artifacts in an automated fashion. the
generated annotations are then leveraged to (3) correct and (4) (re-)evaluate the
model, followed by a repetition of the entire life cycle if required."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"in our experiments, we correct model behavior w.r.t. dataset-intrinsic, as
well as synthetic artifacts in a controlled setting. lastly, we showcase the r2r life
cycle through multiple iterations, unveiling and unlearning diﬀerent biases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"[25,29].
however, studying individual predictions is slow and labor-extensive, limiting
its practicability. in contrast, the authors of [2] use spray [14] for the detection
of spurious model behavior and labeling of artifactual samples. in addition to
spray, we suggest to study latent features of the model via crp concept visual-
izations [1] as a tool for more ﬁne-grained model inspection, catching systematic
misbehavior which would not be visible through spray clusters."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"most model correction methods require dense annotations, such as labels for
artifactual samples or artifact localization masks, which are either crafted heuris-
tically or by hand [13,20]. in our r2r framework, we automate the annotation
by following [2] for data labeling through spray outlier clusters, or by collecting
the most representative samples of bias concepts according to crp. the spatial
artifact localization is further automated by computing artifact heatmaps as out-
lined in sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"3.1, thereby considerably easing the step from bias identiﬁcation
to correction. reveal to revise: an xai life cycle for iterative bias correction of dnns
599
existing works for model correction measure the performance on the original
or clean test set, with corrected models often showing an improved generaliza-
tion [13,20]. a more targeted approach for measuring the artifact’s inﬂuence is
the evaluation on poisoned data [25], for which r2r is well suited by using its
localization scheme to ﬁrst extract artifacts and to then poison clean test sam-
ples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"by precisely localizing artifacts, r2r further allows to measure the model’s
attention on an artifact through attribution heatmaps. 3
reveal to revise framework
our reveal to revise (r2r) framework comprises the entire xai life cycle, includ-
ing methods for (1) the identiﬁcation of model bias, (2) artifact labeling and local-
ization, (3) the correction of detected misbehavior, and (4) the evaluation of the
improved model. to that end, we now describe the methods used for r2r."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"1 (bottom left) for band-aid concepts, where irrelevant background
is overlaid with black semi-transparent color. the collection of top-ranked refer-
ence samples for spurious concepts allows us to label artifactual data. explanation outliers through spray."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"following [2,14],
we apply spray by clustering latent attributions computed through lrp. the
spray clusters then naturally allow us to label data containing the bias. we automate artifact localization by training a con-
cept activation vector (cav) hl to model the artifact in latent space of a layer
l, representing the direction from artifactual to non-artifactual samples obtained
from a linear classiﬁer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"the framework consists of two methods, namely
augmentive clarc (a-clarc) and projective clarc (p-clarc). dependent on input x. parameter γ(x) is chosen
such that the activation in direction of the cav is as high as the average value
over non-artifactual or artifactual samples for p-clarc or a-clarc, respectively.
rrr and cdep for correction through prior knowledge. [20] is based on an additional λ-weighted
loss term (besides the cross-entropy loss lce) for neural network training that
aligns the use of features by the model fθ, described by an explanation expθ, to
a deﬁned prior explanation expprior."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"the authors of rrr propose to penalize
the model’s attention on unfavorable artifacts using the input gradient w.r.t. localizing an artifact and class label ytrue. [20] proposes to use cd [17] importance scores β(xs)
for a feature subset xs based on the forward pass instead of gradient to align the
model’s attention."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"reveal to revise: an xai life cycle for iterative bias correction of dnns
601
fig. 2. overview of artifacts with crp visualizations of corresponding concepts
zoomed-in using receptive ﬁeld information (top), input samples (middle), and cropped
out artifacts (bottom) using our artifact localization method. [10] for bone age estimation based on hand radiographs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"see appendix a.1 for additional experiment details. 4.2
revealing and revising spurious model behavior
revealing bias: in the ﬁrst step of the r2r life cycle, we can reveal the use
of several artifacts by the examined models, including the well-known band-aid,
ruler and skin marker [6] and our synthetic clever hans for the isic dataset, as
shown in fig. 2 for vgg-16. here, we show concept visualizations and cropped
out artifacts based on our automatic artifact localization scheme described in
sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"speciﬁcally, we correct for the
band-aid, “l” markings as well as synthetic artifacts. the skin marker and ruler
602
f. pahde et al.
table 1. model correction results for two isic dataset artifacts (band-aid | synthetic). arrows indicate whether low (↓) or high (↑) scores are better with best scores bold."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"we evaluate the eﬀectiveness of model corrections based on two metrics:
the attributed fraction of relevance to artifacts and prediction performance on
both the original and a poisoned test set (in terms of f1-score and accuracy). whereas in the synthetic case, we simply insert the artifact into all samples
to poison the test set, data-intrinsic artifacts are cropped from random artifac-
tual samples using our artifact localization strategy. note that artifacts might
overlap clinically informative features in poisoned samples, limiting the compa-
rability of poisoned and original test performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_57.pdf,"to reveal model bias,
r2r relies on crp and spray. whereas spray automatically points out clever
hans behavior by analyzing large sets of attribution data, crp allows for a ﬁne-
grained investigation of spurious concepts learned by a model. moreover, crp
is ideal for large datasets, as the concept space dimension remains constant."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf,"conventional ac techniques require additionally-acquired com-
puted tomography (ct) or magnetic resonance (mr) images to calculate
attenuation coeﬃcients, which increases imaging expenses, time costs, or
radiation hazards to patients, especially for whole-body scanners. in this
paper, considering technological advances in acquiring more anatomi-
cal information in raw pet images, we propose to conduct attenuation
correction to pet by itself. to achieve this, we design a deep learn-
ing based framework, namely anatomical skeleton-enhanced generation
(aseg), to generate pseudo ct images from non-attenuation corrected
pet images for attenuation correction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf,"both modules are trained
collaboratively with speciﬁc anatomical-consistency constraint to guar-
antee tissue generation ﬁdelity. experiments on four public pet/ct
datasets demonstrate that our aseg outperforms existing methods by
achieving better consistency of anatomical structures in generated ct
images, which are further employed to conduct pet attenuation correc-
tion with better similarity to real ones. this work veriﬁes the feasibility of
generating pseudo ct from raw pet for attenuation correction without
acquising additional images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf,"the associated implementation is available
at https://github.com/yongshengpan/aseg-for-pet2ct. keywords: pet · attenuation correction · ct · image generation
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5_3.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43999-5_3
revealing anatomical structures in pet
25
1
introduction
positron emission tomography (pet) is a general nuclear imaging technique,
which has been widely used to characterize tissue metabolism, protein deposition,
etc."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf,"for training the generative module g2, we further
propose the anatomical-consistency constraint to guarantee the ﬁdelity of tissue
distribution besides general constraints in previous studies. experiments on four
publicly collected pet/ct datasets demonstrate that our aseg outperforms
existing methods by preserving better anatomical structures in generated pseudo
ct images and achieving better visual similarity in corrected pet images. 2
method
we propose the anatomical skeleton enhanced generation (aseg, as illustrated
in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf,"aseg composes of two sequential generative modules {g1, g2}
to deal with them, respectively. g1 devotes itself to decoupling the anatomical
skeleton from nac-pet to provide rough prior information of attenuation coeﬃ-
cients to g2, particularly for lungs and bones that have the most inﬂuential vari-
ances. g2 then devotes to rendering the tissue details in the ct pattern exploit-
ing both the skeleton and nac-pet images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf,"each
encoding block contains a rb and a convolutional layer with strides of 2 × 2 × 2
for downsampling while each decoding block contains an upsampling operation
of 2 × 2 × 2 and a convolutional layer. the kernel size for the input and output
convolutional layers is 7 × 7 × 7 while for others is 3 × 3 × 3. skip connections
are further used locally in rbs and globally between corresponding layers to
empower information transmission. meanwhile, the adversarial network d con-
sists of ﬁve 4 × 4 × 4 convolutional layers with strides of 2 × 2 × 2 for the ﬁrst
four layers and 1 × 1 × 1 for the last layer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf,"(6)
during the inference phase, only the nac-pet image of each input subject
is required, where the pseudo ct image is derived by ˆy ≈ g2(g1(xnac), xnac). [4] (https://www.cancerimagingarchive.net/collections/), where a series
of public datasets with diﬀerent types of lesions, patients, and scanners are
open-access. among them, 401, 108, 46, and 20 samples are extracted from
the head and neck scamorous cell carcinoma (hnscc), non-small cell lung
cancer (nsclc), the cancer genome atlas (tcga) - head-neck squamous
cell carcinoma (tcga-hnsc), and tcga - lung adenocarcinoma (tcga-
luad), respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf,"quantitative analysis of ct. as the most import application of ct that
is to display the anatomical information, we propose to measure the anatom-
ical consistency between the pseudo ct images and actual ct images, where
the dice coeﬃcients on multiple anatomical regions that extracted from the
pseudo/actual ct images are calculated. [15]
to ﬁnely segment the actual and pseudo ct images to multiple anatomical
structures, and compose them to nine independent tissues for simplifying result
30
y. pan et al.
table 1. comparison of pseudo ct images generated by diﬀerent methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf,"as the pseudo ct images gener-
ated from nac-pet are expected to be used in ac, it is necessary to further
evaluate the eﬀectiveness of pseudo ct images in pet ac. because we cannot
access the original scatters [5], inspired by [11], we propose to resort cgan to
simulate the ac process, denoted as acgan and trained on hnscc dataset. the input of acgan is a concatenation of nac-pet and actual ct, while the
output is actual ac-pet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_3.pdf,"under the collaboration of two modules
and speciﬁc anatomical-consistency constraint, our aseg can generate more
reasonable pseudo ct from nac-pet. experiments on a collection of public
datasets demonstrate that our aseg outperforms existing methods by achiev-
ing advanced performance in anatomical consistency. our study support that
aseg could be a promising and lower-cost alternative of ct acquirement for
ac."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"accurate segmentation of polyps is a crucial step in the
eﬃcient diagnosis of colorectal cancer during screening procedures. the prevalent unet-like encoder-decoder frameworks are commonly
employed, due to their capability of capturing multi-scale contextual
information eﬃciently. however, two major limitations hinder the net-
work from achieving eﬀective feature propagation and aggregation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"firstly, the skip connection only transmits a single scale feature to the
decoder, which can result in limited feature representation. secondly, the
features are transmitted without any information ﬁlter, which is inef-
ﬁcient for performing feature fusion at the decoder. to address these
limitations, we propose a novel feature enhancement network that lever-
ages feature propagation enhancement and feature aggregation enhance-
ment modules for more eﬃcient feature fusion and multi-scale feature
propagation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"speciﬁcally, the feature propagation enhancement module
transmits all encoder-extracted feature maps from the encoder to the
decoder, while the feature aggregation enhancement module performs
feature fusion with gate mechanisms, allowing for more eﬀective infor-
mation ﬁltering. the multi-scale feature aggregation module provides
rich multi-scale semantic information to the decoder, further enhanc-
ing the network’s performance. extensive evaluations on ﬁve datasets
demonstrate the eﬀectiveness of our method, particularly on challenging
datasets such as cvc-colondb and etis, where it can outperform the
previous state-of-the-art models by a signiﬁcant margin (3%) in terms of
miou and mdice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"the unet-like model uses skip
connections that transmit only single-stage features. in contrast, our approach utilizes
fpe to propagate features from all stages, incorporating a gate mechanism to regulate
the ﬂow of valuable information. [11] in the ﬁeld of medical image segmenta-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"to
further address the issue of high-level semantics being overwhelmed in the pro-
gressive feature fusion process, we also integrate a feature aggregation enhance-
ment (fae) module that aggregates the outputs of fpe from previous stages at
634
y. su et al.
decoder. moreover, we introduce gate mechanisms in both fpe and fae to ﬁlter
out redundant information, prioritizing informative features for eﬃcient feature
fusion. finally, we propose a multi-scale aggregation (msa) module appended
to the output of the encoder to capture multi-scale features and provide the
decoder with rich multi-scale semantic information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"(2) fpe transmits all encoder-extracted feature maps to the decoder, and fae
combines the output of the last stage at the decoder and multiple outputs
from fpe. msa aggregates multi-scale high-level features from fpes to
provide rich multi-scale information. (3) the proposed method achieves state-of-the-art results in ﬁve polyp segmen-
tation datasets and outperforms the previous cutting-edge approach by a
large margin (3%) on cvc-colondb and etis datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"features with lower
spatial resolution usually contain richer high-level semantics. then, these fea-
tures are transmitted by the feature propagation enhancement module (fpe) to
yield the feature set {c1, c2, c3, c4}, which provides multi-scale information from
all the stages. this is diﬀerent from the skip connection which only transmits the
single-scale features at the present stage."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"i = 1, 2, 3.
(2)
a noteworthy observation is that the gating mechanism has been widely utilized
in both fpe and fae to modulate the transmission and integration of features. by selectively controlling the ﬂow of relevant information, this technique has
shown promise in enhancing the overall quality of feature representations [4,9]. the ﬁnal features o1 are passed through the classiﬁer (i.e., a 1×1 convolutional
layer) to get the ﬁnal prediction result in o. further details on fpe, fae, and
msa will be provided in the following sections.
636
y. su et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"feature propagation enhancement module. in contrast to the traditional
encoder-decoder architecture with skip connections, the fpe aims to transmit
multi-scale information from full stage at the encoder to the decoder, rather than
single-scale features at the current stage. the fpe architecture is illustrated in
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"instead of directly com-
bining the four inputs, fpe applies gate mechanisms to emphasize informative
features. by selectively enhancing useful information and
ﬁltering out irrelevant information, the reference features x assist in identifying
optimal features y at the current level. the output of g is in [0, 1]h×w , which
controls the transmission of informative features from y or helps ﬁlter useless
information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"the fae is a novel approach
that integrates the outputs of the last stages at the decoder with the fpe’s
outputs at both the current and deeper stages to compensate for the high-level
semantics that may be lost in the process of progressive feature fusion. in contrast
to the traditional encoder-decoder architecture with skip connections, the fae
assimilates the output of the present and higher-stage fpes, delivering richer
spatial and semantic information to the decoder. the fae, depicted in fig. 2(c), integrates the outputs of the current and
deeper fpe stages with high-level semantics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"[3], integrates three
highest-level features c2, c3, and c4 from the fpe output. this provides rich
multi-scale information for subsequent feature aggregation in the fae and also
helps to form a coarse localization of polyps under supervision. [8], and
etc. by aiding in forming a coarse location of the polyp and contributing to
improved accuracy and performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"as depicted in fig. 2(d), the msa module
ﬁrst processes these three features separately. c2, which has the highest feature
resolution, is processed with multiple dilated convolutions to capture its multi-
scale information while keeping its spatial resolution unchanged. c3 is processed
with only one dilated convolution due to its higher spatial resolution, while c4 is
not processed since it already contains the richest contextual information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"to
better integrate these three multi-scale high-level features for subsequent fusion,
additional cross-feature fusion operations (i.e., 2 cu layer) are performed in the
msa module. 3
experiments
datasets. following the setting in [2,3,5,10,10,17,19], the model
is trained using a fraction of the images from cvc-clinicdb and kvasir, and its
performance is evaluated by the remaining images as well as those from cvc-t,
cvc-colondb, and etis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"adamw is selected as the optimizer with a weight
decay of 1e-4. we adopt the same data augmentation techniques as uacanet [8],
including random ﬂip, random rotation, and color jittering. in evaluation phase,
we mainly focus on mdice, miou, the two most common metrics in medical image
638
y. su et al.
segmentation, to evaluate the performance of the model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"we compared our proposed
method with previous state-of-the-art methods. according to the experimental
settings, the results on cvc-clinicdb and kvasir demonstrate the learning abil-
ity of the proposed model, while the results on cvc-t, cvc-colondb, and etis
demonstrate the model’s ability for cross-dataset generalization. the experimen-
tal results are listed in table.1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"this shows that our model is second to none
in terms of learning ability, which demonstrates the eﬀectiveness of our model. furthermore, our proposed method demonstrates strong cross-dataset gener-
alization capability on cvc-t, cvc-colondb, and etis datasets, with partic-
ularly good performance on the latter two due to their larger and more represen-
tative datasets. our model outperforms state-of-the-art models by 2.9% mdice
and 3.2% miou on cvc-colondb and 3.5% mdice and 4.0% miou on etis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"our results indicate that the impact of each module on the ﬁnal performance
is considerable, and their combination yields the optimal overall performance. speciﬁcally, across the ﬁve datasets, our proposed model improves the mdice
score by at least 1.4% and up to 3.4% on cvc-t, compared to the baseline. for
miou, the improvements are 1.5% and 3.5% on the corresponding datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"4
conclusion
we introduce a new approach to polyp segmentation that addresses ineﬃcient
feature propagation in existing unet-like encoder-decoder networks. speciﬁcally,
640
y. su et al.
a feature propagation enhancement module is introduced to propagate multi-
scale information over full stages in the encoder, while a feature aggregation
enhancement module is attended at the decoder side to prevent the loss of
high-level semantics during progressive feature fusion. furthermore, a multi-
scale aggregation module is used to aggregate multi-scale features to provide
rich information for the decoder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_61.pdf,"experimental results on ﬁve popular polyp
datasets demonstrate the eﬀectiveness and superiority of our proposed method. speciﬁcally, it outperforms the previous cutting-edge approach by a large mar-
gin (3%) on cvc-colondb and etis datasets. to extend our work, our future
direction focuses on exploring more eﬀective approaches to feature utilization,
such that eﬃcient feature integration and propagation can be achieved even on
lightweight networks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf,"automated detection of cervical abnormal cells from thin-
prep cytologic test (tct) images is essential for eﬃcient cervical abnor-
mal screening by computer-aided diagnosis system. however, the detec-
tion performance is inﬂuenced by noise samples in the training dataset,
mainly due to the subjective diﬀerences among cytologists in annotating
the training samples. besides, existing detection methods often neglect
visual feature correlation information between cells, which can also be
utilized to aid the detection model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf,"then, a pre-trained patch correction network (pcn)
is leveraged to obtain local-scale features and conduct further reﬁne-
ment for these suspicious cell patches. we design a classiﬁcation ranking
loss to utilize reﬁned scores for reducing the eﬀects of the noisy label. furthermore, the proposed roi-correlation consistency loss is computed
between extracted roi features and local-scale features to exploit cor-
relation information and optimize retinanet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf,"[16]
for detection, xception, and patch-based models to boost classiﬁcation. although the above-mentioned attempts can improve the screening perfor-
mance signiﬁcantly, there are several issues that need to be addressed: 1) object
detection methods often require accurate annotated data to guarantee perfor-
mance with robustness and generalization. however, due to legal limitations, the
scarcity of positive samples, and especially the subjectivity diﬀerences between
cytopathologists for manual annotations [20], it is likely to generate noisy sam-
ples that aﬀect the performance of the detection model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf,"however, in clinical practice
pathologists usually examine the target cells by comparing them to the sur-
rounding cells to determine whether they are abnormal. therefore, the visual
feature correlations between the target cells and their surroundings can provide
valuable information to aid the screening process, which also needs to be utilized
when designing the cervical abnormal cell detection network. to address these issues, we propose a novel method for cervical abnormal
cell detection using distillation from local-scale consistency reﬁnement."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf,"note that pcn is frozen and the cervical abnormal cell detection is updated by lrank
and lrcc during training. (pcn), which is designed to exploit the supervised information from the pcn to
reduce the impact of noisy labels and utilize the contextual relationships between
cells. [12] to locate suspicious
cells and crop the top-k suspicious cells into patches."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf,"the pcn is employed to
reﬁne and enhance the retinanet proposal classiﬁer, which is trained from a
large number of patches collected in advance with more excellent classiﬁcation
performance. more speciﬁcally, the input image is processed by the base detector fd(·)
ﬁrstly to obtain the primary proposal information. the proposed pcn fc(·) takes
the top-k patches as inputs, which are cropped from original images according
to the proposal location, denoted as ip = cr(i, p), where cr(·) denotes the crop
function, i and p denote input image and proposal boxes predicted by fd(·),
respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf,"we
then deﬁne the proposed rcc loss as:
lrcc =

1
bk
rc(x) − rr(x)
2
2 ,
(7)
where x is the proposals from the sampled mini-batch, rc(x) and rr(x) are
the correlation matrices computed on x under diﬀerent network. by minimizing
lrcc during the training process, the network could be enhanced to capture the
intrinsic relation between patches, thus helping to extract additional semantic
information from cells. 2.4
optimization
to better optimize the retinanet detector in a reinforced way, we take the fol-
lowing training strategy, which consists of three major stages."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf,"during inference, only the optimized detector is used to output the ﬁnal
detection results without any additional modules. 3
experimental results
3.1
dataset and experimental setup
dataset. for cervical cell detection, our dataset includes 3761 images of 1024×
1024 pixels cropped from wsis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf,"performance comparison with state-of-the-art methods. initially, the
images were randomly assigned to pathologist b or c for initial labeling. later,
the assigned pathologist’s annotations were reviewed and veriﬁed by the other
pathologist."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf,"any discrepancies found were checked and re-labeled by pathologist
a. these images were divided into the training set and the testing set according
to the ratio of 9:1. we also collect a new dataset of 5000 positive and negative
224 × 224 cell patches to train the pcn. the backbone of the suspicious cell detection net-
work is retinanet with resnet-50 [7]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_63.pdf,"by model
660
m. fei et al.
learning, our method can gradually enhance the features of abnormal cell regions
while repressing noise or other suspicious but non-lesion regions. 4
conclusion
in this paper, we integrate a distillation strategy that uses the knowledge learned
from the pre-trained pcn to guide the training of the detection model to min-
imize the eﬀects of noisy labels and explore the feature interaction between
cells. our method constructs retinanet with the pcn module which provides
the reﬁned scores and local-scale features of extracted patches."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"this paper presents a new robust loss function, the t-loss,
for medical image segmentation. the proposed loss is based on the nega-
tive log-likelihood of the student-t distribution and can eﬀectively handle
outliers in the data by controlling its sensitivity with a single parameter. this parameter is updated during the backpropagation process, elimi-
nating the need for additional computation or prior information about
the level and spread of noisy labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"our experiments show that the t-
loss outperforms traditional loss functions in terms of dice scores on two
public medical datasets for skin lesion and lung segmentation. we also
demonstrate the ability of t-loss to handle diﬀerent types of simulated
label noise, resembling human error. our results provide strong evidence
that the t-loss is a promising alternative for medical image segmenta-
tion where high levels of noise or outliers in the dataset are a typical
phenomenon in practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"the project website can be found at https://
robust-tloss.github.io. noisy labels
1
introduction
convolutional neural networks (cnns) and visual transformers (vits) have
become the standard in semantic segmentation, achieving state-of-the-art results
in many applications [1,16,24]. however, supervised training of cnns and vits
requires large amounts of annotated data, where each pixel in the image is labeled
with the category it belongs to."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"in the medical domain, obtaining these anno-
tations can be costly and time-consuming as it requires expertise and domain
m. pouly and a. a. navarini—joint last authorship. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 68.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43898-1_68
robust t-loss for medical image segmentation
715
knowledge that is often scarcely available [6]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"in addition, medical image anno-
tations can be aﬀected by human bias and poor inter-annotator agreement [23],
further complicating the process. despite eﬀorts to obtain labels through auto-
mated mining [31] and crowd-sourcing methods [11], the quality of datasets
gathered using these methods remains challenging due to often high levels of
label noise. for instance, the fitzpatrick 17k dataset, commonly used in dermatology
research, contains non-skin images and noisy annotations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"in a random sample
of 504 images, 5.4% were labeled incorrectly or as other classes [10]. noisy labels are and will continue to be, a problem in medical datasets. this is a concern as label noise has been shown to decrease the accuracy of
supervised models [20,22,35], making it a key area of focus for both research
and practical applications."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"compared
to the ﬁrst two approaches, which may suﬀer from inaccurate estimates of the
noise transition matrix, robust loss functions enable joint optimization of model
parameters and variables related to the noise model and have shown promising
results in classiﬁcation tasks [8,34]. despite these advances, semantic segmenta-
tion with noisy labels is relatively understudied. although previous methods have shown robustness in semantic segmentation,
they often have limitations, such as more hyper-parameters, modiﬁcations to the
network architecture, or complex training procedures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"however, their eﬀectiveness has
not been thoroughly investigated. in this work, we show that several traditional robust loss functions are vul-
nerable to memorizing noisy labels. to overcome this problem, we introduce a
novel robust loss function, the t-loss, which is inspired by the negative log-
likelihood of the student-t distribution."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"the t-loss, whose simplest formulation
features a single parameter, can adaptively learn an optimal tolerance level to
label noise directly during backpropagation, eliminating the need for additional
computations such as the expectation maximization (em) steps.
to evaluate the eﬀectiveness of the t-loss as a robust loss function for medi-
cal semantic segmentation, we conducted experiments on two widely-used bench-
mark datasets in the ﬁeld: one for skin lesion segmentation and the other for lung
segmentation. we injected diﬀerent levels of noise into these datasets that simu-
late typical human labeling errors and trained deep learning models using various
robust loss functions. our experiments demonstrate that the t-loss outperforms
716
a. gonzalez-jimenez et al.
these robust state-of-the-art loss functions in terms of segmentation accuracy and
robustness, particularly under conditions of high noise contamination."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"2 introduces the motivation behind
our t-loss and provides its mathematical derivation. section 3 covers the
datasets used in our experiments, the implementation and training details of
t-loss, and the metrics used for comparison. section 4 presents the main ﬁnd-
ings of our study, including the results of the t-loss and the baselines on both
datasets and an ablation study on the parameter of t-loss."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"(2)
robust t-loss for medical image segmentation
717
the functional form of our loss function for one image is then obtained with the
identiﬁcation y = yi and the approximation µ = fw(xi), and aggregated with
lt = 1
n
n

i=1
− log p(yi|fw(xi), σ; ν). (3)
equation (2) has d(d+1)/2 free parameters in the covariance matrix, which
should be estimated from the data. in the case of images, this can easily be in the
order of 104 or larger, which makes a general computation highly non-trivial and
may deteriorate the generalization capabilities of the model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"to this
end, we reparametrize ν = e˜ν + ϵ where ϵ is a safeguard for numerical stability. [15].
3.1
datasets
the isic 2017 dataset [5] is a well-known public benchmark of dermoscopy
images for skin cancer detection. it contains 2000 training and 600 test images
with corresponding lesion boundary masks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"the images are annotated with lesion
type, diagnosis, and anatomical location metadata. the dataset also includes a
list of lesion attributes, such as size, shape, and color. we resized the images to
256 × 256 pixels for our experiments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"718
a. gonzalez-jimenez et al.
shenzhen [4,13,25] is a public dataset containing 566 frontal chest radio-
graphs with corresponding lung segmentation masks for tuberculosis detection. since there is not a predeﬁned split for shenzhen as in isic, to ensure represen-
tative training and testing sets, we stratiﬁed the images by their tuberculosis and
normal lung labels, with 70% of the data for training and the remaining 30% for
testing. resulting in 296 training images and 170 test images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"all images were
resized to 256 × 256 pixels. without a public benchmark with real noisy and clean segmentation masks,
we artiﬁcially inject additional mask noise in these two datasets to test the
model’s robustness to low annotation quality. this simulates the real risk of
errors due to factors like annotator fatigue and diﬃculty in annotating certain
images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"the morphological transfor-
mations included erosion, dilation, and aﬃne transformations, which respectively
reduced, enlarged, and displaced the annotated area.
3.2
setup
we train a nnu–net [12] as a segmentation network from scratch. to increase
variations in the training data, we augment them with random mirroring, ﬂip-
ping, and gamma transformations. the t-loss was initialized with ˜ν = 0 and
ϵ = 10−8."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"the model is trained using noisy masks. however, by using the ground truth
for the corresponding noisy mask, we can evaluate the robustness of the model
and measure noisy-label memorization. this is done by analyzing the dice score
of the model’s prediction compared to the actual ground truth."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"[9] to compare the diﬀerences between the means of the
dice scores and obtain a p-value. in addition, if the diﬀerence is signiﬁcant, we
1 https://github.com/gaozhitong/sp guided noisy label seg.
robust t-loss for medical image segmentation
719
perform the tukey post-hoc test [14] to determine which means are diﬀerent. we
assume statistical signiﬁcance for p-values of less than p = 0.05 and denote this
with a ⋆.
4
results
4.1
results on the isic dataset
we present experimental results for the skin lesion segmentation task on the isic
dataset in table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"this can
be observed from the training dice scores in fig. 1, where traditional robust
losses overﬁt data in later stages of learning while metrics for the t-loss do not
deteriorate. our method achieves a dice score of 0.788 ± 0.007 even for the most
extreme noise scenario under exam."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"examples of the obtained masks can be seen
in the supplementary material. dice score on the isic dataset with diﬀerent noise ratios. the values refer
to the mean and standard deviation over 3 diﬀerent random seeds for the mean score
over the last 10 epochs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"0.761(6)⋆
720
a. gonzalez-jimenez et al.
fig. the dice score of training set predictions compared to ground truth annotations
during the training process on the isic 2017 dataset for each type of noisy mask with
α = 0.7, β = 0.7. the model memorizes the noisy labels after the ﬁrst ∼20k iterations,
thus negatively aﬀecting the dice score for all losses except the t-loss.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"2. the behavior of ˜ν in the skin lesion segmentation task. left: convergence of ˜ν
with diﬀerent levels of label noise. right: sensitivity of the dice score to the initialization of ˜ν with the same
settings."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"the statistical test results
also support this claim, with the t-loss being signiﬁcantly superior to the other
methods. 4.3
dynamic tolerance to noise
the value of ˜ν is crucial for the model’s performance, as it controls the sensitivity
to label noise. to shed light on this mechanism, we study the behavior of ˜ν during
training for diﬀerent label noise levels and initializations on the isic dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_68.pdf,"as seen in fig. 2, ˜ν dynamically adjusts annotation noise tolerance in the early
stages of training, independently of its initial value. the plots demonstrate that
robust t-loss for medical image segmentation
721
table 2. dice score on the shenzhen dataset with diﬀerent noise ratios. the values
refer to the mean and standard deviation over 3 diﬀerent random seeds for the mean
score over the last 10 epochs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"fully-supervised polyp segmentation has accomplished sig-
niﬁcant triumphs over the years in advancing the early diagnosis of col-
orectal cancer. however, label-eﬃcient solutions from weak supervision
like scribbles are rarely explored yet primarily meaningful and demand-
ing in medical practice due to the expensiveness and scarcity of densely-
annotated polyp data. besides, various deployment issues, including data
shifts and corruption, put forward further requests for model generaliza-
tion and robustness."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"concretely, for the ﬁrst time
in weakly-supervised medical image segmentation, we promote the dual-
branch co-teaching framework by leveraging the intrinsic complemen-
tarity of features extracted from the spatial and spectral domains and
encouraging cross-space consistency through collaborative optimization. furthermore, to produce reliable mixed pseudo labels, which enhance the
eﬀectiveness of ensemble learning, we introduce a novel adaptive pixel-
wise fusion technique based on the entropy guidance from the spatial
and spectral branches. our strategy eﬃciently mitigates the deleterious
eﬀects of uncertainty and noise present in pseudo labels and surpasses
previous alternatives in terms of eﬃcacy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"ultimately, we formulate a
holistic optimization objective to learn from the hybrid supervision of
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 4.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. extensive experiments and evaluation on
four public datasets demonstrate the superiority of our method regarding
in-distribution accuracy, out-of-distribution generalization, and robust-
ness, highlighting its promising clinical signiﬁcance. our code is available
at https://github.com/lofrienger/s2me."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"recently, deep learning has emerged
as a powerful tool in medical image analysis, prompting extensive research into
its potential for polyp segmentation. the eﬀectiveness of deep learning mod-
els in medical applications is usually based on large, well-annotated datasets,
which in turn necessitates a time-consuming and expertise-driven annotation
process. this has prompted the emergence of approaches for annotation-eﬃcient
weakly-supervised learning in the medical domain with limited annotations like
points [8], bounding boxes [12], and scribbles [15]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"besides, scribbles provide
a more robust supervision signal, which can be prone to noise and outliers [5].
hence, this work investigates the feasibility of conducting polyp segmentation
using scribble annotation as supervision. the eﬀectiveness of medical applica-
tions during in-site deployment depends on their ability to generalize to unseen
data and remain robust against data corruption. improving these factors is cru-
cial to enhance the accuracy and reliability of medical diagnoses in real-world
scenarios [22,27,28]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"dual-branch learning has been widely adopted in annotation-eﬃcient learn-
ing to encourage mutual consistency through co-teaching. while existing
approaches are typically designed for learning in the spatial domain [21,25,29,
30], a novel spatial-spectral dual-branch structure is introduced to eﬃciently
leverage domain-speciﬁc complementary knowledge with synergistic mutual
teaching. furthermore, the outputs from the spatial-spectral branches are aggre-
gated to produce mixed pseudo labels as supplementary supervision."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"diﬀerent
from previous methods, which generally adopt the handcrafted fusion strate-
gies [15], we design to aggregate the outputs from spatial-spectral dual branches
with an entropy-guided adaptive mixing ratio for each pixel. consequently, our
incorporated tactic of pseudo-label fusion aptly assesses the pixel-level ambigu-
ity emerging from both spatial and frequency domains based on their entropy
maps, thereby allocating substantially assured categorical labels to individual
pixels and facilitating eﬀective pseudo label ensemble learning. s2me: spatial-spectral mutual teaching and ensemble learning
37
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"spatial-spectral cross-domain consistency is encouraged through mutual
teaching. high-quality mixed pseudo labels are generated with pixel-level guidance from
the dual-space entropy maps, ensuring more reliable supervision for ensemble learning. overall, the contributions of this work are threefold: first,
we devise a spatial-spectral dual-branch structure to leverage cross-space knowl-
edge and foster collaborative mutual teaching."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"to our best knowledge, this is the
ﬁrst attempt to explore the complementary relations of the spatial-spectral dual
branch in boosting weakly-supervised medical image analysis. second, we intro-
duce the pixel-level entropy-guided fusion strategy to generate mixed pseudo
labels with reduced noise and increased conﬁdence, thus enhancing ensemble
learning. lastly, our proposed hybrid loss optimization, comprising scribbles-
supervised loss, mutual training loss with domain-speciﬁc pseudo labels, and
ensemble learning loss with fused-domain pseudo labels, facilitates obtaining
a generalizable and robust model for polyp image segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"[4] to disentangle global patterns across vary-
ing frequency components and derives hybrid feature representation. in addi-
tion, spectrum learning also exhibits advantageous robustness and generaliza-
tion against adversarial attacks, data corruption, and distribution shifts [19]. [14], yet only in the spatial domain."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"this has motivated
us to develop the cross-domain cooperative mutual teaching scheme to leverage
the favorable properties when learning in the spectral space. besides consistency constraints, utilizing pseudo labels as supplementary
supervision is another principle in label-eﬃcient learning [11,24]. [11], ﬁltering out unreliable
pixels [24], and mixing dual-branch outputs [15] following
pmix = α × p1 + (1 − α) × p2, α = random(0, 1),
(1)
where α is the random mixing ratio."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"1,
the spatial and spectral branches take the same training image as the input and
extract domain-speciﬁc patterns. the raw model outputs, i.e., the logits lspa and
lspe, will be converted to probability maps pspa and pspe with softmax normal-
ization, and further to respective pseudo labels ˆyspa and ˆyspe by ˆy = arg max p.
the spatial and spectral pseudo labels supervise the other branch collaboratively
during mutual teaching and can be expressed as
ˆyspa → fspe and ˆyspe → fspa,
(2)
where “→” denotes supervision1. through cross-domain engagement, these two
branches complement each other, with each providing valuable domain-speciﬁc
insights and feedback to the other."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"consequently, such a scheme can lead to bet-
ter feature extraction, more meaningful data representation, and domain-speciﬁc
knowledge transmission, thus boosting model generalization and robustness.
entropy-guided pseudo label ensemble learning. in addition to mutual
teaching, we consider aggregating the pseudo labels from the spatial and spec-
tral branches in ensemble learning, aiming to take advantage of the distinctive
1 for convenience, we omit the input x and model parameters θ. s2me: spatial-spectral mutual teaching and ensemble learning
39
yet complementary properties of the cross-domain features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"considering such property, we propose a novel adaptive strategy to
automatically adjust the mixing ratio for each pixel based on the entropy of its
categorical probability distribution. hence, the mixed pseudo labels are more
reliable and beneﬁcial for ensemble learning. log p(c),
(3)
where c is the number of classes that equals 2 in our task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"(1), we can update the
mixing ratio between the two probability maps pspa and pspe with the weighted
entropy guidance at each pixel location by
ps2 =
hspe
hspa + hspe
⊗ pspa +
hspa
hspa + hspe
⊗ pspe,
(4)
where “⊗” denotes pixel-wise multiplication. ps2 is the merged probability map
and can be further converted to the pseudo label by ˆys2 = arg max ps2 to super-
vise the spatial and spectral branch in the context of ensemble learning following
ˆys2 → fspa and ˆys2 → fspe. (5)
by absorbing strengths from the spatial and spectral branches, ensemble learn-
ing from the mixed pseudo labels facilitates model optimization with reduced
overﬁtting, increased stability, and improved generalization and robustness."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"hybrid loss supervision from scribbles and pseudo labels. besides the
scribble annotations for partial pixels, the aforementioned three types of pseudo
labels ˆyspa, ˆyspe, and ˆys2 can oﬀer complementary supervision for every pixel,
with diﬀerent learning regimes. overall, our hybrid loss supervision is based on
cross entropy loss ℓce and dice loss ℓdice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"[13] ℓpce, which only calculates the loss on the labeled pixels,
for learning from scribbles following
lscrib = ℓpce(lspa, y) + ℓpce(lspe, y),
(6)
where y denotes the scribble annotations. furthermore, the mutual teaching loss
with supervision from domain-speciﬁc pseudo labels is
lmt = {ℓce(lspa, ˆyspe)+ℓdice(pspa, ˆyspe)}



ˆyspe→fspa
+ {ℓce(lspe, ˆyspa)+ℓdice(pspe, ˆyspa)}



ˆyspa→fspe
. , our hybrid loss supervision can be stated as
lhybrid = lscrib + λmt × lmt + λel × lel,
(9)
where λmt and λel serve as weighting coeﬃcients that regulate the relative sig-
niﬁcance of various modes of supervision."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"the hybrid loss considers all possible
supervision signals in the spatial-spectral dual-branch network and exceeds par-
tial combinations of its constituent elements, as evidenced in the ablation study. 3
experiments
3.1
experimental setup
datasets. we employ the sun-seg [10] dataset with scribble annotations for
training and assessing the in-distribution performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"this dataset is based
on the sun database [16], which contains 100 diﬀerent polyp video cases. to
reduce data redundancy and memory consumption, we choose the ﬁrst of every
ﬁve consecutive frames in each case. we then randomly split the data into 70, 10,
and 20 cases for training, validation, and testing, leaving 6677, 1240, and 1993
frames in the respective split."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"[1]
with 1000, 612, and 1537 polyp frames, respectively. these datasets are collected
from diversiﬁed patients in multiple medical centers with various data acquisi-
tion systems. varying data shifts and corruption like motion blur and specular
reﬂections2 pose signiﬁcant challenges to model generalization and robustness."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"as shown in
table 1 and fig. 2, our s2me achieves superior in-distribution performance quan-
titatively and qualitatively compared with other baselines on the sun-seg [10]
dataset. regarding generalization and robustness, as indicated in table 2, our
method outperforms other weakly-supervised methods by a signiﬁcant margin
on three unseen datasets, and even exceeds the fully-supervised upper bound
on two of them4."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"these results suggest the eﬃcacy and reliability of the pro-
posed solution s2me in fulﬁlling polyp segmentation tasks with only scribble
annotations. notably, the encouraging performance on unseen datasets exhibits
promising clinical implications in deploying our method to real-world scenarios. 4 complete results of all four metrics are present in the supplementary materials.
42
a. wang et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"the contour of the ground-truth mask is displayed in black, in
comparison with that of each method shown in diﬀerent colors. table 2. generalization comparison on three unseen datasets. the underlined results
surpass the upper bound."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"table 3. ablation comparison of dual-branch network architectures. results are from
outputs of model-1 on the sun-seg [10] dataset. to ensure the reliability of the mixed
pseudo labels for ensemble learning, we present the pixel-level adaptive fusion
strategy according to entropy maps of dual predictions to balance the strengths
and weaknesses of spatial and spectral branches."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"we decompose the proposed hybrid loss lhybrid
in eq. (9) to demonstrate the eﬀectiveness of holistic supervision from scribbles,
s2me: spatial-spectral mutual teaching and ensemble learning
43
table 4. ablation on the pseudo label
fusion strategies on the sun-seg [10]
dataset. as shown in table 5, our proposed
hybrid loss, involving lscrib, lmt, and lel, achieves the optimal results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_4.pdf,"4
conclusion
to our best knowledge, we propose the ﬁrst spatial-spectral dual-branch net-
work structure for weakly-supervised medical image segmentation that eﬃciently
leverages cross-domain patterns with collaborative mutual teaching and ensem-
ble learning. our pixel-level entropy-guided fusion strategy advances the relia-
bility of the aggregated pseudo labels, which provides valuable supplementary
supervision signals. moreover, we optimize the segmentation model with the
hybrid mode of loss supervision from scribbles and pseudo labels in a holistic
manner and witness improved outcomes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_3.pdf,"this design not only preserves the depth of the network, but also increases the
width of the network. it is beneﬁcial for the network to extract shallow local tex-
ture information and global semantic information. after the feature extraction
phase, a new fusion method named msf fuses all of diﬀerent scale features yi."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_3.pdf,"finally, the features are multiplied by the corresponding
normalized weights and the processed features are added together to generate
the new multi-scale features. mfeblock is very applicable to process histopatho-
logical images of diﬀerent magniﬁcation factors, as it employs convolution and
attention operations to capture local and global image context information and
fuse them well.
2.2
classiﬁcation module
the task of the cf module is to classify the reconstructed sr images. it can
use the method of downsampling to capture multi-scale features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_3.pdf,"in csfblock, the upsampling operations are performed on
shisrcnet
27
the low-resolution features xl to realize consistency with xh dimension. u = xh + up(xl)
then, using gap along the channel dimension to get the global information
s. a fc layer generates a compact feature vector z which guides the feature
selection procedure. and z is reconstructed into two weight vectors a, b of the
same dimension as s through two fc layers, which can be deﬁned as:
z = δ(wcs),
a = waz,
b = wbz
where δ denotes relu and wa, wb, wc, means the weight of the fc layers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_3.pdf,"in the cf module,
we introduce hr images to cf module in the training stage for improving the
performance of cf module and reducing the error caused by the reconstructed
sr images. [16] to alleviate the class imbalanced data problem
of the hr and sr images’ classiﬁcation. [5], the hr and sr of the same images are similar to two
diﬀerent views."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_3.pdf,"in our experiment, λ1, λ2 and λ3 are set to 0.6, 0.3 and 0.1,
respectively. and the temperature parameter in nt − xent loss is set to 0.5.
3
experiment
dataset: this work uses the breast cancer histopathological image database
(breakhis)1 [20]. the images in the dataset have four magniﬁcation factors
1 https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-
breakhis/.
28
l. xie et al.
(40x, 100x, 200x, 400x) and eight breast cancer classes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_3.pdf,"this dataset includes
four distinct histological types of benign breast tumors: adenosis (a), ﬁbroade-
noma (f), phyllodes tumor (pt), and tubular adenona (ta); and four malignant
tumors (breast cancer): carcinoma (dc), lobular carcinoma (lc), mucinous car-
cinoma (mc) and papillary carcinoma (pc). the original dataset is randomly
divided into training set and testing set for each magniﬁcation at a ratio of 7:3
following previous work. implementation details: for all experiments, we conduct 5-fold cross valida-
tion, and report the mean."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_3.pdf,"we use lr histopathological images with size 48 × 48,
96 × 96, 192 × 192 as input for diﬀerent single image sr tasks (x8, x4, x2) and
set batch size to 8. for the corresponding lr and hr images in the training
dataset, the same data augmentation is adopted, such as rotation, color jitter. the model is trained using the adam optimizer [25] with the learning rate set
to 1x10−3."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"numerous
z. yu and t. lin—equal contribution. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_25.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. [25], mutation predic-
tion [32], microsatellite instability prediction [31], and survival prediction from
wsis [2,16]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"[8], a milestone work,
introduces hierarchical pre-training (dino [6]) for the patch-level (256 × 256)
and region-level (4096 × 4096) in a two-stage manner, achieving superior per-
formance on slide-level tasks. [13] uses eﬃcientnet-b0 for image
compression in the ﬁrst stage and then derives multi-task learning on the com-
pressed wsis, which assumes the primary site information, e.g., the organ type,
is always available and can be used as pseudo labels. [35] also pro-
poses a two-stage pre-training framework for wsis using contrastive learning
(simclr [10]), where the diﬀerently subsampled bags1 from the same wsi are
positive pairs in the second stage."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"the aforementioned meth-
ods share the same two-stage pre-training paradigm, i.e., patch-to-region/slide. thus broader context information is preserved to close the gap between pretext
and downstream tasks. however, they are essentially instance discrimination
where only the self-invariance of region/slide is considered, leaving the intra-
and inter-slide semantic structures unexplored."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"the learning objective of dino is self-distillation. taking stage two as an
example, dino distills the knowledge from teacher to student by minimizing the
cross-entropy between the probability distributions of two views at region-level:
lself = ex∼pdh(gt(ˆz), gs(z)),
(1)
where h(a, b) = −a log b, and pd is the data distribution that all regions are
drawn from. the teacher and the student share the same architecture consisting
of an encoder (e.g., vit) and a projection head gt/gs. ˆz and z are the embeddings
of two views at region-level yielded by the encoder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"2.3
slide-level clustering
many histopathologic features have been established based on the morpho-
logic phenotypes of the tumor, such as tumor invasion, anaplasia, necrosis and
mitoses, which are then used for cancer diagnosis, prognosis and the estimation
of response-to-treatment in patients [3,9]. to obtain meaningful representations
of slides, we aim to explore and maintain such histopathologic features in the
latent space. clustering can reveal the representative patterns in the data and has
achieved success in the area of unsupervised representation learning [4,5,24,26].
to characterize the histopathologic features underlying the slides, a straight-
forward practice is the global clustering, i.e., clustering the region embeddings
from all the wsis, as shown in the left of fig. however, the obtained
clustering centers, i.e., the prototypes, are inclined to represent the visual bias
slpd
263
related to staining or scanning procedure rather than medically relevant fea-
tures [33]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"similar operations are applied across other slides, and
then we acquire n groups of prototypes {{cm
n }m
m=1}n
n=1. each group of proto-
types is expected to encode the semantic structure (e.g., the combination of
histopathologic features) of the wsi.
2.4
intra-slide distillation
the self-distillation utilized by hipt in stage two encourages the correspondence
between two views of a region at the macro-scale because the organizations of
cells share mutual information spatially. however, the self-distillation, which
solely mines the spatial correspondences inside the 4096 × 4096 region, cannot
comprehensively understand the histopathologic consistency at the slide-level."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"as shown in fig. , we assume that the representation
z and its assigned prototype c also share mutual information and encourage z
to be closer to c with the intra-slide distillation:
lintra = ex∼pdh (gt(c), gs(z)) ,
(2)
we omit super-/sub-scripts of z for brevity. through eq. 2, we can leverage more
intra-slide correspondences to guide the learning process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"with the proposed set-
to-set distance, we can model the inter-slide correspondences conveniently and
accurately. speciﬁcally, for a region embedding z belonging to the slide w and
assigned to the prototype c, we ﬁrst search the top-k nearest neighbors of w in
the dataset based on the semantic similarity, denoted as { ˆwk}k
k=1. second, we
also obtain the matched prototype pairs {(c, ˆck)}k
k=1 determined by the optimal
permutation, where ˆck is the prototype of ˆwk."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"we believe the performance
can be further improved by tuning this. 3
experimental results
datasets. we conduct experiments on two public wsi datasets2."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"tcga-
nsclc dataset includes two subtypes in lung cancer, lung squamous cell car-
cinoma and lung adenocarcinoma, with a total of 1,054 wsis. tcga-brca
dataset includes two subtypes in breast cancer, invasive ductal and invasive
lobular carcinoma, with a total of 1,134 wsis.
pre-training. we extract 62,852 and 60,153 regions at 20× magniﬁcation from
tcga-nsclc and tcga-brca for pre-training vit4096-256 in stage two."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"following the oﬃcial code of hipt, vit4096-256
is optimized for 100 epochs with optimizer of adamw, base learning rate of 5e-4
and batch size of 256 on 4 gtx3090 gpus. 2 the data is released under a cc-by-nc 4.0 international license. slpd
265
fine-tuning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"we adopt the 10-fold cross validated accuracy (acc.)
and area under the curve (auc) to evaluate the weakly-supervised classiﬁcation
performance. the data splitting scheme is kept consistent with hipt. “mean” leverages the averaged pre-extracted embed-
dings to evaluate knn performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"table 2(#1,2) reports the comparative results, where the slide-level clustering
surpasses the global clustering by 0.6% and 1.8% of auc on nsclc and brca,
which veriﬁes the eﬀectiveness of the former. the inferior performance of the
global clustering is due to the visual bias underlying the whole dataset. table 2. ablation studies of slpd."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_25.pdf,"to
verify the necessity of this distillation method, we turn to another design where
the inter-slide correspondence is explored through two nearest region embeddings
across slides (#3 in table 2). as can be seen, the region-level correspondences
lead to inferior performances, even worse than the baseline (#5 in table 1),
because the learning process is not guided by the slide-level information. number of prototypes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"medical image analysis using deep learning is often challenged
by limited labeled data and high annotation costs. fine-tuning the entire
network in label-limited scenarios can lead to overﬁtting and suboptimal
performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"recently, prompt tuning has emerged as a more promising
technique that introduces a few additional tunable parameters as prompts
to a task-agnostic pre-trained model, and updates only these parameters
using supervision from limited labeled data while keeping the pre-trained
model unchanged. however, previous work has overlooked the importance
of selective labeling in downstream tasks, which aims to select the most
valuable downstream samples for annotation to achieve the best perfor-
mance with minimum annotation cost. to address this, we propose a frame-
work that combines selective labeling with prompt tuning (slpt) to boost
performance in limited labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"in addition, we pro-
pose a diversiﬁed visual prompt tuning strategy to provide multi-prompt-
based discrepant predictions for tesla. we evaluate our method on
liver tumor segmentation and achieve state-of-the-art performance, out-
performing traditional ﬁne-tuning with only 6% of tunable parameters,
also achieving 94% of full-data performance by labeling only 5% of the data. keywords: active learning · prompt tuning · segmentation
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 2.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"https://doi.org/10.1007/978-3-031-43895-0_2
slpt: selective labeling meets prompt tuning
15
1
introduction
deep learning has achieved promising performance in computer-aided diagnosis
[1,12,14,24], but it relies on large-scale labeled data to train, which is challenging
in medical imaging due to label scarcity and high annotation cost [3,25]. specif-
ically, expert annotations are required for medical data, which can be costly and
time-consuming, especially in tasks such as 3d image segmentation. transferring pre-trained models to downstream tasks is an eﬀective solution
for addressing the label-limited problem [8], but ﬁne-tuning the full network
with small downstream data is prone to overﬁtting [16]. [5,18] is emerging from natural language processing (nlp), which introduces
additional tunable prompt parameters to the pre-trained model and updates only
prompt parameters using supervision signals obtained from a few downstream
training samples while keeping the entire pre-trained unchanged."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"by tuning only
a few parameters, prompt tuning makes better use of pre-trained knowledge. it
avoids driving the entire model with few downstream data, which enables it
to outperform traditional ﬁne-tuning in limited labeled data. [5], instead of designing text prompts
and transformer models, we explore visual prompts on convolutional neural
networks (cnns) and the potential to address data limitations in medical imag-
ing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"however, previous prompt tuning research [18,28], whether on language or
visual models, has focused solely on the model-centric approach. for instance,
coop [29] models a prompt’s context using a set of learnable vectors and opti-
mizes it on a few downstream data, without discussing what kind of samples
are more suitable for learning prompts. vpt [13] explores prompt tuning with
a vision transformer, and spm [17] attempts to handle downstream segmen-
tation tasks through prompt tuning on cnns, which are also model-centric."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"however, in downstream tasks with limited labeled data, selective labeling as a
data-centric method is crucial for determining which samples are valuable for
learning, similar to active learning (al) [23]. in al, given the initial labeled
data, the model actively selects a subset of valuable samples for labeling and
improves performance with minimum annotation eﬀort. nevertheless, directly
combining prompt tuning with al presents several problems."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"however, previous al methods [27] did not consider the
existence of prompts or use prompts to estimate sample value. therefore, this paper proposes the ﬁrst framework for selective labeling and
prompt tuning (slpt), combining model-centric and data-centric methods to
improve performance in medical label-limited scenarios. we make three main
contributions: (1) we design a novel feature-aware prompt updater embedded
in the pre-trained model to guide prompt tuning in deep layers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"(2) we propose
16
f. bai et al.
fig. 1. workﬂow of slpt: (1) create an initial label set via the pre-trained model for
unsupervised diversity selection (subplot c step 0). (2) insert a feature-aware prompt
updater (subplot a) into the pre-trained model for prompt tuning with initial labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"(3) use diversiﬁed visual prompt tuning (subplot b) to obtain prompt-based discrepant
predictions. (4) select valuable data by prompt-based uncertainty (subplot c step 1)
and update the prompt-based model accordingly. note: the orange modules are tunable
for prompt tuning, while the gray ones are frozen."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"please zoom in for details. a diversiﬁed visual prompt tuning mechanism that provides multi-prompt-based
discrepant predictions for selective labeling. (3) we introduce the tesla strat-
egy which includes both unsupervised diversity selection via task-agnostic fea-
tures and supervised selection considering prompt-based uncertainty."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"slpt consists of
three components, as illustrated in fig. 1: (a) a prompt-based visual model, (b)
diversiﬁed visual prompt tuning, and (c) tandem selective labeling. speciﬁcally,
with slpt, we can select valuable data to label and tune the model via prompts,
which helps the model overcome label-limited medical scenarios."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"2.1
prompt-based visual model
the pre-trained model, learned by supervised or unsupervised training, is a pow-
erful tool for improving performance on label-limited downstream tasks. fine-
tuning a large pre-trained model with limited data may be suboptimal and prone
to overﬁtting [16]. to overcome this issue, we draw inspiration from nlp [18]
and explore prompt tuning on visual models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"fpus are inserted into the network to update deep prompts and features. 1(a), an fpu receives two inputs, feature map f out
i−1 and prompt pi−1, of
slpt: selective labeling meets prompt tuning
17
the same shape, and updates to fi and pi through two parallel branches. in the
feature branch, f out
i−1 and pi−1 are concatenated and fed into a 1x1 convolution
and fusion module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"drawing inspiration, we propose a
visual prompt tuning approach that associates diverse prompts with discrepant
predictions. to achieve this, we design k diﬀerent data augmentation, heads,
and losses based on corresponding k prompts. by varying hyperparameters, we
can achieve diﬀerent data augmentation strengths, increasing the model’s diver-
sity and generalization."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"y represents
the ground truth and l is the total loss. 2.3
tandem selective labeling
previous studies overlook the critical issue of data selection for downstream
tasks, especially when available labels are limited. to address this challenge, we
propose a novel strategy called tesla."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"tesla consists of two tandem steps:
unsupervised diversity selection and supervised uncertainty selection. the ﬁrst
step aims to maximize the diversity of the selected data, while the second step
aims to select the most uncertain samples based on diverse prompts. step 0: unsupervised diversity selection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"since we do not have any labels
in the initial and our pre-trained model is task-agnostic, we select diverse samples
to cover the entire dataset. to achieve this, we leverage the pre-trained model
to obtain feature representations for all unlabeled data. although these features
are task-independent, they capture the underlying relationships, with similar
samples having closer feature distances."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"[22], which identiﬁes the b samples that best represent the diversity of
the data based on these features. these selected samples are then annotated and
serve as the initial dataset for downstream tasks. step 1: supervised uncertainty selection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"then, we have the divergence score
sd = mean(d), which reﬂects inter-prompts uncertainty. slpt: selective labeling meets prompt tuning
19
in the latter, we evaluate intra-prompts uncertainty by computing the mean
prediction of the prompts and propose to estimate prompt-based gradients as
the model’s performance depends on the update of prompt parameters θp. how-
ever, for these unlabeled samples, computing their supervised loss and gradient
directly is not feasible."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"we calculate our uncertainty score s as follows:
s =
sd
max(sd) ×
sg
max(sg)
(7)
where max(·) ﬁnds the maximum value. we sort the unlabeled data by their
corresponding s values in ascending order and select the top b data to annotate.
3
experiments and results
3.1
experimental settings
datasets and pre-trained model. we conducted experiments on automating
liver tumor segmentation in contrast-enhanced ct scans, a crucial task in liver
cancer diagnosis and surgical planning [1]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"although there are publicly available
liver tumor datasets [1,24], they only contain major tumor types and diﬀer in
image characteristics and label distribution from our hospital’s data. deploying
a model trained from public data to our hospital directly will be problematic. collecting large-scale data from our hospital and training a new model will be
expensive."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"therefore, we can use the model trained from them as a starting
point and use slpt to adapt it to our hospital with minimum cost. we col-
lected a dataset from our in-house hospital comprising 941 ct scans with eight
categories: hepatocellular carcinoma, cholangioma, metastasis, hepatoblastoma,
hemangioma, focal nodular hyperplasia, cyst, and others. it covers both major
and rare tumor types."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"our objective is to segment all types of lesions accurately. we utilized a pre-trained model for liver segmentation using supervised learning
on two public datasets [24] with no data overlap with our downstream task. the
nnunet [12] was used to preprocess and sample the data into 24 × 256 × 256
patches for training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"20
f. bai et al.
table 1. evaluation of diﬀerent tunings on the lesion segmentation with limited data
(40 class-balanced patients). denote precision and recall."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"we integrated 13 fpus behind each upsampling or down-
sampling of nnunet, adding only 2.7m parameters. during training, we set
k = 3 and employed diverse data augmentation techniques such as scale, elas-
tic, rotation, and mirror. to ensure fairness and eliminate model ensemble eﬀects, we
only used the model’s prediction with k = 1 during testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"we used ﬁxed ran-
dom seeds and 5-fold cross-validation for all segmentation experiments.
3.2
results
evaluation of prompt tuning. since we aim to evaluate the eﬃcacy of
prompt tuning on limited labeled data in table 1, we create a sub-dataset of
approximately 5% (40/752) from the original dataset. speciﬁcally, we calculate
the class probability distribution vector for each sample based on the pixel class
in the mask and use coreset with these vectors to select 40 class-balanced sam-
ples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"step 1: supervised uncertainty selection. the
labeling budget for each step is 20 patients. step +∞ refers to fully labeled 752 data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"although spm also out-
performs ﬁne-tuning, our methods outperform spm by 1.18% and save 0.44m
tunable parameters with more eﬃcient fpu. in cases of limited data, ﬁne-tuning
tends to overﬁt on a larger number of parameters, while prompt tuning does
not. the pre-trained model is crucial for downstream tasks with limited data,
as it improves performance by 9.52% compared to learn-from-scratch."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"among
the three partial tuning methods, the number of tuning parameters positively
correlates with the model’s performance, but they are challenging to surpass
ﬁne-tuning.
evaluation of selective labeling. we conducted steps 0 (unsupervised selec-
tion) and 1 (supervised selection) from the unlabeled 752 data and compared our
approach with other competing methods, as shown in table 2. in step 0, without
any labeled data, our diversity selection outperformed the random baseline by
1.86%."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"we also outperformed uncertaingcn by 1.93%. mc
dropout and entropy underperformed in our prompt tuning, likely due to the
diﬃculty of learning such uncertain data with only a few prompt parameters. notably, our method outperformed random sampling by 10.28%."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"it shows that each component plays a critical role in improving performance. 4
conclusions
we proposed a pipeline called slpt that enhances model performance in label-
limited scenarios. with only 6% of tunable prompt parameters, slpt outper-
forms ﬁne-tuning due to the feature-aware prompt updater."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_2.pdf,"moreover, we pre-
sented a diversiﬁed visual prompt tuning and a tesla strategy that combines
unsupervised and supervised selection to build annotated datasets for down-
stream tasks. slpt pipeline is a promising solution for practical medical tasks
with limited data, providing good performance, few tunable parameters, and low
labeling costs. future work can explore the potential of slpt in other domains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"the proposed method can be seamlessly integrated into existing net-
works. extensive experiments on both open-source and in-house datasets
consistently demonstrate the eﬀectiveness of the proposed method over
some cnn and transformer-based segmentation methods. our code is
available at https://github.com/splinterli/sattca."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"this is signiﬁcantly
lower than the mean value of 81.68%. (b): statistics of the number of nodules at
diﬀerent scales in three datasets. the range of nodule diameter corresponding to micro,
small, medium, and mass is (0, 10], (10, 20], (20, 30], [30, ∞), respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"1(a–c), the recall rate of the
large-scale nodule and mass is usually lower than the average level. the main
reason is that the lesion scale in the two public datasets are relatively small,
which matches the fact few patients have very large nodule or mass. this makes
the pulmonary nodule and mass segmentation task resemble a long-tail problem
rather than a mere large scale span problem."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"recently, some click-based lesion segmentation methods [19–21] introduce the
click at the input or feature level and modify the network accordingly, result-
ing in higher accuracy results. yet, the click input does not provide the scale
information of lesions for the network. [24] during testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"additionally, we also propose a
multi-scale input encoder to further address the problem of imbalanced lesion
scales. experimental results on two public datasets and one in-house dataset
demonstrate that the proposed method outperforms existing methods with dif-
ferent backbones. 2
method
2.1
restatement of image segmentation based on click
for pulmonary nodule and mass segmentation, existing methods mostly rely on
regions of interest (roi) obtained by lesion detection networks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"typically, a neural network with weighted parameters θ
is trained to predict the lesion area ˆs = θ(i), with the goal of minimizing the
loss function l(s, ˆs). the stochastic gradient descent (sgd) and the automatic
data acquisition module weight decay (adamw) optimizers are usually used to
optimize the weighted parameters. for each roi input, the center point c of the lesion, which is represented
as pc = ( d
2 , h
2 , w
2 ) in cartesian coordinate system, can be used as a reference
point to assist the network in improving segmentation performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"the subsequent modules can be
based on either cnn or transformer structures. the multi-scale input encoder
allows the network to capture more scale information of the nodules and masses,
thus mitigating the problem of large lesion scale span. 2.3
scale-aware test-time click adaptation
in clinical scenarios, the neural network for assisted diagnosis is generally a pre-
training model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"if the pre-
dicted nodule size is greater than 40 mm, the axial length of mi has a linear
relationship with the side length. to determine the super parametric values for
the mapping function r, we perform cross-validation on three datasets. 2.4
training objective of sattca
we use the foreground range of adaptively adjusted ellipsoid mi to mask ˆsi to
obtain a masked segmentation ˆsm
i ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"then we use mi to adjust the normalization
layer parameters in the network during testing [24]. the test-time loss function
ltt is the weighted sum of the binary cross-entropy loss lbce and the dice loss
with sigmoid ldice of mi and ˆsm
i , and the information entropy loss lent of ˆsi. formally, ltt is given by:
ltt = lbce + σldice + γlent,
(3)
where σ and γ are hyper-parameters set to 0.5 and 1 in all experiments,
respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"the sum of the ﬁrst two equations is referred to as click loss lclick. 3
experiments
3.1
datasets and evaluation protocols
we experiment on two public datasets and one in-house dataset. all three
datasets are divided into training, validation, and test sets using a 7:1:2 ratio."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"686
z. li et al.
table 1. performance of diﬀerent backbones with or without the proposed sattca
and other click-based methods. experiments are conducted with various pulmonary
nodule segmentation datasets using 3d nnunet [7], transbts [25] and nnunet with
multi-scale input encoder (ms) as the backbone. comparative experiments are carried
out with the click-based methods in [19,20] and simple test-time click adaptation on
ms-unet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"[1]: the lidc dataset is a publicly available lung ct image database
containing 1018 scans, developed by the lung image database consortium
(lidc). all pulmonary nodules and masses in the dataset have been annotated
by multiple raters. to generate the ground truth for each nodule and mass,
we combined the segmentation annotations from diﬀerent raters."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"overall, we
selected a total of 1625 nodules and masses that were annotated by more than
three raters from the lidc dataset for the experiment. [16]: the lndb dataset published in 2019, comprises 294 ct scans col-
lected between 2016 and 2018. each ct scan in the dataset has been segmented
by at least one radiologist."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"the nodules included in this dataset are larger than
3 mm. the mean scale of the lesion in lndb dataset is the shortest among the
three datasets. we adopt 1968 nodules and masses from the lndb dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"it incor-
porates long-range dependencies into the traditional cnn structure to achieve a
larger receptive ﬁeld. the experimental results presented in table 1, consistently
demonstrate that the cnn-based network can achieve better results in multi-
scale pulmonary nodule and mass segmentation tasks across all three datasets. this is mainly due to the fact that large receptive ﬁelds may involve background
features that are not conducive to segmentation inference for micro or small
nodules."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_65.pdf,"we further analyze the performance of sattca. firstly, we present the
quantitative comparison in table 2, where we group the nodules and masses in
each dataset at 10 mm intervals and calculate the average segmentation perfor-
mance diﬀerences of the nodules in each scale group. the statistical results show
that the proposed sattca signiﬁcantly improves the recall rate of the segmen-
tation on large nodules and masses."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"however, there is still a large
performance gap between current weakly-supervised methods and fully
supervised learning, leaving room for exploration. in this work, we pro-
pose a novel 3d framework with two consistency constraints for scribble-
supervised multiple abdominal organ segmentation from ct. speciﬁ-
cally, we employ a triple-branch multi-dilated network (tdnet) with
one encoder and three decoders using diﬀerent dilation rates to cap-
ture features from diﬀerent receptive ﬁelds that are complementary to
each other to generate high-quality soft pseudo labels. for more stable
unsupervised learning, we use voxel-wise uncertainty to rectify the soft
pseudo labels and then supervise the outputs of each decoder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"to fur-
ther regularize the network, class relationship information is exploited
by encouraging the generated class aﬃnity matrices to be consistent
across diﬀerent decoders under multi-view projection. experiments on
the public word dataset show that our method outperforms ﬁve exist-
ing scribble-supervised methods. keywords: weakly-supervised learning · scribble annotation ·
uncertainty · consistency
1
introduction
abdominal organ segmentation from medical images is an essential work in clin-
ical diagnosis and treatment planning of abdominal lesions [17]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"https://doi.org/10.1007/978-3-031-43990-2_4
34
m. han et al.
learning methods based on convolution neural network (cnn) have achieved
impressive performance in medical image segmentation tasks [2,24]. however,
their success relies heavily on large-scale high-quality pixel-level annotations
that are too expensive and time-consuming to obtain, especially for multiple
organs in 3d volumes. weakly supervised learning with a potential to reduce
annotation costs has attracted great attention."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"training cnns for segmentation with scribble annotations has been increas-
ingly studied recently. existing methods are mainly based on pseudo label
learning [11,15], regularized losses [10,18,22] and consistency learning [7,13,26]. pseudo label learning methods deal with unannotated pixels by generating fake
semantic labels for learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"for example, luo et al. [15] introduced a network
with two slightly diﬀerent decoders that generate dynamically mixed pseudo
labels for supervision. liang et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"[11] proposed to leverage minimum span-
ning trees to generate low-level and high-level aﬃnity matrices based on color
information and semantic features to reﬁne the pseudo labels. arguing that the
pseudo label learning may be unreliable, tang et al. [22] introduced the condi-
tional random field (crf) regularization loss for image segmentation directly."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"obukhov et al. [18] proposed to incorporate the gating function with crf loss
considering the directionality of unsupervised information propagation. recently,
consistency strategies that encourage consistent outputs of the network for the
same input under diﬀerent perturbations have achieved increasing attentions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"[15] that tend to be over-conﬁdence predictions. for
more stable unsupervised learning, we use voxel-wise uncertainty to rectify the
soft pseudo labels and then impose consistency constraints on the output of each
branch. in addition, we extend the consistency to the class-related information
scribble-based 3d multiple abdominal organ segmentation
35
ℒ
ℒ
uncertainty 
rectified
ℒ
ℒ
image
scribble
1
2
encoder
decoder 
1, 
= 1
decoder 
, 
= 6
decoder 
2, 
= 3
1
2
2
1
ℒ
ℒ
ℒ
−
−
×
∗ ×
×
× (∗)
project in sagittal view
reshape matrix
×
multiply operation
transpose matrix 
(b) class affinity calculation
(a) the proposed tdnet
project in coronal view
project in axial view
class affinity calculation
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"1. overview of the proposed triple-branch multi-dilated network (tdnet) that
uses diﬀerent dilation rates at three decoders. the tdnet is optimized by uncertainty-
weighted soft pseudo label consistency (uspc) using the mixed soft pseudo labels and
multi-view projection-based class-similarity consistency (mpcc). the class aﬃnity
calculation process is shown in (b)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"by equipping with varying dilation rates, the network
can better leverage multi-scale context for dealing with organs at diﬀerent scales. 2) we propose two novel consistency loss functions, i.e., uncertainty-weighted
soft pseudo label consistency (uspc) loss and multi-view projection-based
class-similarity consistency (mpcc) loss, to regularize the prediction from the
pixel-wise and class-wise perspectives respectively, which helps the segmenta-
tion network obtain reliable predictions on unannotated pixels. 3) experiments
results show our proposed method outperforms ﬁve existing scribble-supervised
methods on the public dataset word [17] for multiple abdominal organ seg-
mentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"we introduce a network with one encoder and three decoders with
36
m. han et al.
diﬀerent dilation rates to learn multi-scale features. the decoders’ outputs are
averaged to generate a soft pseudo label that is rectiﬁed by uncertainty and then
used to supervise each branch. to better deal with multi-class segmentation, a
class similarity consistency loss is also used for regularization."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"speciﬁcally, decoders using
convolution with small dilation rates can extract detailed local features but their
receptive ﬁelds are small for understanding a global context. decoders using con-
volution with large dilation rates can better leverage the global information but
may lose some details for accurate segmentation. being the same in the three decoders."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"in addition, the bottleneck’s output features are randomly dropped out before
sending into the auxiliary decoders. the probability prediction maps obtained
by the three decoders are denoted as p1, p2 and p3, respectively.
2.2
pixel-wise and class-wise consistency
uncertainty-weighted soft pseudo label consistency (uspc). as the
three decoders capture features at diﬀerent scales that are complementary to
each other, an ensemble of them would be more robust than a single branch."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"therefore, we take an average of p1, p2, p3 to get a better soft pseudo label
¯p = (p1 + p2 + p3)/3 that is used to supervise each branch during training. however, ¯p may also contain noises and be inaccurate, and it is important
to highlight reliable pseudo labels while suppressing unreliable ones. 
i wi
(1)
where ¯pi refers to the prediction probability at voxel i in ¯p, and ¯pn,i is the
corresponding prediction probability at voxel i in ¯pn."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"wi is the voxel-wise weight based on uncertainty estimation:
wi = e

c ¯
p c
i log( ¯
p c
i )
(2)
where the uncertainty is estimated by entropy. i in the pseudo label. note that a higher
uncertainty leads to a lower weight."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"in addition to using lusp c for pixel-wise
supervision, we consider making consistency on class relationship across the
outputs of the decoders as illustrated in fig. 1. in order to save computing
resources, we project the soft pseudo labels along each dimension and then cal-
culate the aﬃnity matrices, which also strengthens the class relationship infor-
mation learning. we ﬁrst project the soft prediction map of the n-th decoder
pn ∈ rc×d×h×w in axial view to a tensor with the shape of c × 1 × h × w. it
is reshaped into c ×(wh) and multiplied by its transposed version, leading to a
class aﬃnity matrix q′axial
n
∈ rc×c."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"= β · e(−5(1−t/tmax)2) in a similar way. in this way, the model can learn
accurate information from scribble annotations, which also avoids getting stuck
in a degenerate solution due to low-quality pseudo labels at an early stage. 3
experiments and results
3.1
dataset and implementation details
we used the publicly available abdomen ct dataset word [17] for experiments,
which consists of 150 abdominal ct volumes from patients with rectal cancer,
prostate cancer or cervical cancer before radiotherapy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"we aimed to segment seven organs: the liver,
spleen, left kidney, right kidney, stomach, gallbladder and pancreas. following
the default settings in [17], the dataset was split into 100 for training, 20 for
validation and 30 for testing, respectively, where the scribble annotations for
foreground organs and background in the axial view of the training volumes had
been provided and were used in model training. for pre-processing, we cut oﬀ
the hounsﬁeld unit (hu) values with a ﬁxed window/level of 400/50 to focus
on the abdominal organs, and normalized it to [0, 1]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"3.
visualization
of
the
improvement obtained by using
diﬀerent dilation rates and uncer-
tainty rectifying. best viewed in
color.
between our method and the other weakly supervised methods on the word
dataset (word 0014.nii). it can be obviously seen that the results obtained by
our method are closer to the ground truth, with less mis-segmentation in both
slice level and volume level.
3.3
ablation experiment
we then performed ablation experiments to investigate the contribution of
each part of our method, and the quantitative results on the validation set
are shown in table 2, where lusp c(−ω) means using lusp c without pixel-
wise uncertainty rectifying."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"by equipping each decoders with diﬀerent dilation rates,
the model’s performance is further improved, especially in terms of asd and
hd95, which proves our hypothesis that learning features from diﬀerent scales
can improve the segmentation accuracy. replacing lusp c(−ω) with lusp c fur-
ther improved the dsc to 84.21%, and reduced the asd and hd95 by 0.52 mm
and 1.01 mm through utilizing the uncertainty information. visual comparison in
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"scribble-based 3d multiple abdominal organ segmentation
41
4
conclusion
in this paper, we proposed a scribble-supervised multiple abdominal organ seg-
mentation method consisting of a 3d triple-branch multi-dilated network with
two-level consistency constraints. by equipping each decoder with diﬀerent dila-
tion rates, the model leverages features at diﬀerent scales to obtain high-quality
soft pseudo labels. in addition to mine knowledge from unannotated pixels, we
also proposed uspc loss and mpcc loss to learn unsupervised information
from the uncertainty-rectiﬁed soft pseudo labels and class aﬃnity matrix infor-
mation respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_4.pdf,"experiments on a public abdominal ct dataset word
demonstrated the eﬀectiveness of the proposed method, which outperforms
ﬁve existing scribble-based methods and narrows the performance gap between
weakly-supervised and fully-supervised segmentation methods. in the future, we
will explore the eﬀect of our method on sparser labels, such as a volumetric data
with scribble annotations on one or few slices."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"however, manual delineation is labor-
intensive, and automatic segmentation of esophageal gtv is diﬃcult due
to the ambiguous boundary of the tumor. detailed tumor information
naturally exists in the previous stage, however the correlation between
the ﬁrst and second course rt is rarely explored. in this study, we ﬁrst
reveal the domain gap between the ﬁrst and second course rt, and aim
to improve the accuracy of gtv delineation in the second course rt by
incorporating prior information from the ﬁrst course."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"we propose a novel
prior anatomy and rt information enhanced second-course esophageal
gtv segmentation network (artseg). a region-preserving attention
module (ram) is designed to understand the long-range prior knowl-
edge of the esophageal structure, while preserving the regional patterns.
sparsely labeled medical images for various isolated tasks necessitate
eﬃcient utilization of knowledge from relevant datasets and tasks. to
achieve this, we train our network in an information-querying manner."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"h. liao and x. yang are the co-corresponding authors. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2_48.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43990-2_48
512
y. sun et al.
keywords: second course radiotherapy · esophageal gross tumor
volume · data eﬃcient learning · prior anatomical information ·
attention
1
introduction
esophageal cancer is a signiﬁcant contributor to cancer-related deaths glob-
ally [3,15]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"the automatic identiﬁcation of the esophagus presents inherent challenges due to
its elongated soft structure and ambiguous boundaries between it and adjacent
organs [12]. moreover, the automatic delineation of the gtv in the esophagus
poses a signiﬁcant diﬃculty, primarily attributable to the low contrast between
the esophageal gtv and the neighboring tissue, as well as the limited datasets. [18,19].
since the task is challenging, jin et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"[9,10] improve the segmentation accuracy
by incorporating additional information from paired positron emission tomog-
raphy (pet). nevertheless, such approaches require several imaging modalities,
which can be both costly and time-consuming, while disregarding any knowledge
from previous treatment or anatomical understanding. moreover, the correlation
between the ﬁrst and second courses of rt is rarely investigated, where detailed
prior tumor information naturally exists in the previous rt planning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"in this paper, we present a comprehensive study on accurate gtv delin-
eation for the second course rt. we proposed a novel prior anatomy and rt
information enhanced second-course esophageal gtv segmentation network
(artseg). a region-preserving attention module (ram) is designed to eﬀec-
tively capture the long-range prior knowledge in the esophageal structure, while
preserving regional tumor patterns."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"meanwhile, an ideal method for automatic esophageal gtv segmenta-
tion in the second course of rt should consider three key aspects: 1) changes in
tumor volume after the ﬁrst course of rt, 2) the proliferation of cancerous cells
from a tumor to neighboring healthy cells, and 3) the anatomical-dependent
second-course esophageal gtv segmentation
513
fig. our training approach leverages multi-center datasets containing relevant anno-
tations, that challenges the network to retrieve information from e1 using the features
from e2. the decoder d utilizes the prior knowledge obtained from i1 and g1 to gener-
ate the mask prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"nature of gtv on esophageal locations. to achieve this, we eﬃciently exploit
knowledge from multi-center datasets that are not tailored for second-course
gtv segmentation. our training strategy does not speciﬁc to any tasks but
challenges the network to retrieve information from another encoder with aug-
mented inputs, which enables the network to learn from the above three aspects."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"d.
our objective is to predict the esophageal gtv g2 of the second course. it would be advantageous to leverage insights from the ﬁrst course, as it com-
prises comprehensive information pertaining to the tumor in its preceding phase. therefore, the input to encoder e1 consists of the concatenation of i1 and g1 to
encode the prior information (features f d
1 ) from the ﬁrst course, while encoder
e2 embeds both low- and high-level features f d
2 of the local pattern of i2 (fig. 1),
f d
1 = e1(i1, g1), f d
2 = e2(i2), d = 0, 1, 2, 3, 4
(1)
where the spatial shape of f d
1/2 is h
2d × w
2d × d
2d , with 2d+4 channels.
region-preserving attention module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"medical images are sparsely labeled which are isolated by diﬀerent tasks [20],
and are often inadequate. in this study, we use a paired ﬁrst-second course gtv
dataset sp, an unpaired gtv dataset sv, and a public esophagus dataset se. in order to fully leverage both public and private datasets, the training objec-
tive should not be speciﬁc to any tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"as shown
in fig. 1, our strategy is to challenge the network to retrieve information from
augmented inputs in e1 using the features from e2, which can incorporate a wide
range of datasets that are not tailored for second-course gtv segmentation. 3.1
tumor volume variation
the diﬀerences in tumor volume between the ﬁrst and second courses following
an rt treatment can have a negative impact on the state-of-the-art (sota)
learning-based techniques, which will be discussed in sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"in sp, i1
p and i2
p are the ﬁrst and second
course ct images, while g1
p and g2
p are the corresponding gtv annotations. second-course esophageal gtv segmentation
515
3.2
cancer cell proliferation
the paired dataset sp for the ﬁrst and second courses is limited, whereas an
unpaired gtv dataset sv = {iv; gv} can be easily obtained in a standard clini-
cal workﬂow with a substantial amount. sv lacks its counterpart for the second
course, in which iv/gv are the ct image and the corresponding annotation for
gtv. to address this, we apply two distinct randomized augmentations, p1, p2,
to mimic the unregistered issue of the ﬁrst and second course ct."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"(4)
the esophageal tumor can proliferate with varying morphologies into the sur-
rounding tissues. although not paired, sv contains valuable information about
the tumor. challenging the network to query information within gtv will
enhance the capacity to retrieve pertinent information for the tumor positions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"3.3
reliance of gtv on esophageal anatomy
to make full use of the datasets of relevant tasks, we incorporate a public esoph-
agus segmentation dataset, denoted as se = {ie; ge}, where ie/ge represent the
ct images and corresponding annotations of the esophagus structure. by aug-
menting the data as described in eq. (4), se challenges the network to extract
information from the entire esophagus, which enhances the network’s embedding
space with anatomical prior knowledge of the esophagus."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"similarly, data from
the paired sp is also augmented by p1/2 to increase the network’s robustness. in summary, our training strategy is not dataset-speciﬁc or target-speciﬁc,
thus allowing the integration of prior knowledge from multi-center esophageal
gtv-related datasets, which eﬀectively improves the network’s ability to retrieve
information for the second course from the three key aspects stated in sect. 3.
4
experiments
4.1
experimental setup
datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"the paired ﬁrst-second course dataset, sp, is collected from sun yat-
sen university cancer center (ethics approval number: b2023-107-01), com-
prising paired ct scans of 69 distinct patients from south china. we collected
the gtv dataset sv from medmind technology co., ltd., which has ct scans
from 179 patients. for both sp and sv, physicians annotated the esophageal
cancer gtv in each ct."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"(color ﬁgure online)
have esophageal cancer. we randomly split sp into training and test datasets
at the patient-level. the training dataset includes sv, se, and 41 patients from
sp (denoted as strain
p
), while the test dataset comprises 28 patients from sp
(denoted as stest
p
)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"second-course esophageal gtv segmentation
517
4.2
domain gap between the first and second course
as previously mentioned, the volume of the tumors changes after the ﬁrst course
of rt. to demonstrate the presence of a domain gap between the ﬁrst and sec-
ond courses, we train sota methods with datasets strain
p
and sv, by feeding the
data sequentially into the network. we then evaluate the models on stest
p
."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"the
results presented in table 1 indicate a performance gap between gtv segmen-
tation in the ﬁrst and second courses, with the latter being more challenging. notably, the paired ﬁrst-second course dataset stest
p
pertains to the same group
of patients, thereby ensuring that any performance drop can be attributed solely
to diﬀerences in courses of rt, rather than variations across diﬀerent patients. figure 2 illustrates the reduction in the gtv area after the initial course of
rt, where the transverse plane is taken from the same location relative to the
vertebrae (yellow lines)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"this suggests that deep learning-based approaches may not rely solely on the
identiﬁcation of malignant tissue patterns, as doctors do, but rather predict high-
risk areas statistically. therefore, for accurate second-course gtv segmentation,
we need to explicitly propagate prior information from the ﬁrst course using dual
encoders in artseg, and incorporate learning about tumor changes. 4.3
evaluations of second-course gtv segmentation performance
combination of various datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"we ﬁrst utilize a standard artseg (w/o ram) as an ablation
network. when prior information from the ﬁrst course is explicitly introduced
using sp, artseg outperforms other baselines for gtv segmentation in the
second course, which reaches a dsc of 66.73%. however, in fig. 3, it can be
observed that the model failed to accurately track the gtv area along the
esophagus (orange arrows) due to the soft and elongated nature of the esophageal
tissue, which deforms easily during ct scans performed at diﬀerent times."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"therefore, sv
and se improve the network from two distinct aspects and are both valuable. our proposed training strategy fully exploits the datasets sp, sv, and se, and
518
y. sun et al.
table 2. quantitative comparison of gtv segmentation performance in the second
course."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"(best methods in table 1)
fig. the impact of diﬀerent prior knowledge on esophageal tumor detection. net-
works with inadequate knowledge of the esophagus may fail in identifying the tumor
within the esophagus (orange arrows), whereas a limited understanding of tumor mor-
phology can deteriorate the ability to detect the tumor in the adjacent area (blue
arrows)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"our proposed approach, encompassing comprehensive prior knowledge, shows
superior performance. (color
ﬁgure online)
further improve the dsc to 74.54% by utilizing comprehensive knowledge of
both the tumor morphology and esophageal structures. region-preserving
attention
module."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"although
introducing
the
esophageal structural prior knowledge using se can improve the performance
in dsc and asd (table 2), the increase in hsd (38.22 to 47.89 mm; 21.71 to
27.00 mm) indicates that there are outliers far from the ground truth bound-
aries. this may be attributed to the convolution that cannot eﬀectively handle
the long-range knowledge of the esophagus structure. the attention mechanism
can eﬀectively capture the long-range relationship as shown recently in [13]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_48.pdf,"we attribute the drawback is due to the
location-agnostic nature of the operations in mha, where the local regional
correlations are perturbed. to tackle the aforementioned problem, we propose ram which involves the
concatenation of the original features with attention outputs, allowing for the
preservation of convolution-generated regional tumor patterns while eﬀectively
comprehending long-range prior knowledge speciﬁc to the esophagus. finally,
our proposed artseg with ram achieves the best dsc/hsd of 75.26%/19.75
mm, and outperforms its ablations as well as other baselines, as shown in table 2.
limitations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"our framework consists of
two networks: a seg-net that generates segmentation results for membranes and
nuclei, and a tran-net that transforms the segmentation into semantic points. in
this way, the accuracy of the semantic points is closely related to the segmenta-
tion quality. thus, the inconsistency between the semantic points and the point
annotations can be used as effective supervision for cell segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"h. li and z. xu—equally ﬁrst authors. supplementary information the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2_52.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. therefore, there is an
urgent need for precise automatic ihc membrane analysis methods to provide objective
quantitative evidence and improve diagnostic efﬁciency.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"we employ a network named seg-net to segment
the nuclei and membranes separately, followed by a trans-net to convert the segmen-
tation results into semantic points. since the accuracy of semantic points is directly
related to the segmentation results, the segmentation quality can be implicitly super-
vised by the loss between the semantic points and the point annotations, as shown in
fig. 2
to the best of our knowledge, this is the ﬁrst study that using point-level supervision
for membrane segmentation from ihc images, which could signiﬁcantly advance future
segment membranes and nuclei via point-level supervision
541
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"additionally, our method is the ﬁrst to employ point annotations
to simultaneously supervise the segmentation of two objects. extensive experiments
conﬁrm the efﬁcacy of the proposed method, attaining performance that is comparable
to models trained with fully supervised data. 2
related works
deep learning-based segmentation methods have been widely developed for cell nuclei
segmentation from h&e images in recent years, ranging from convolutional neural
networks [5,12,24] to transformer-based architectures [8], resulting in continuously
improved accuracy in nuclei segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"these traditional methods perform inadequately and are vulnerable to the effects of dif-
ferences in staining intensity and abnormal staining impurities. in recent years, a few
fully supervised cell membrane segmentation methods also have emerged [9,19], but
the high cost of data annotation limits their applicability to various membrane staining
image analysis tasks. to reduce the annotation cost of nuclei segmentation in histopathological images,
weakly supervised segmentation training methods have received attention, including:
1) unsupervised cell nuclei segmentation methods represented by adversarial-based
methods [6,7]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"3.
ii
fθ
−→


mi, si

gω
−→ pi ∼= pi,
(1)
where gω is the transition network (tran-net) with parameters ω. gω transforms


mi, si

to semantic points pi ∈ rh×w ×(c+1), it should be noted that 
mi and si
respectively provide the semantic and spatial information to gω for semantic points
prediction, so that the segmentation performance is crucial for gω. so that, by ﬁtting pi
to pi ( pi ∼= pi), the segmentation can be supervised."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"in this study, we create si by
expanding each point annotation to a circle with a radius of 20. thus, to provide seman-
tic information to tran-net for predicting semantic points, the other channel must con-
tain information describing the staining status of the membrane, which in turn decouples
membrane segmentation. because both si and si are single-channel, we employ the naive l1 loss to supervise
the segmentation of the nuclei, as shown in eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"nevertheless, we can also employ the cross-entropy function for point-level supervi-
sion:
ℓpoints
i
= − pi log (pi) . (5)
to enhance the spatial supervision information of the point annotations pi, it is
worth noting that we extended each point annotation to a gaussian circle with a radius
of 5 pixels. = 1 in ℓhinge
i
."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"the
her2 experiment is dedicated to validate segmentation performance, while the pdl1
experiment is utilized to verify the effectiveness of converting segmentation results into
clinically relevant indicators. segment membranes and nuclei via point-level supervision
545
4.1
dataset
we collected 648 her2 and 1076 pdl1 images at 40x magniﬁcation with a resolution
of 1024 × 1024 from wsis. the pdl1 data only has point-level annotations, where
pathologists categorize cells as positive or negative based on membrane staining."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"pixel-level annotations are used for
testing and fully supervised experiments only. we split the data into training and test
sets in a 1:1 ratio and do not use a validation set since our method is trained without
pixel-level annotations. [10] with the
initial learning rate of 5×10−4 and the momentum of 0.9."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"we employ the intersection over union (iou) segmentation metric and pixel-level
f1 score to validate the performance of the proposed method. however, only point-level
annotations are equipped for the pdl1 dataset, we evaluate the segmentation perfor-
mance at the point-level by converting the segmentation into key point predictions, and
the conversion process details are available in the supplementary materials. 4.3
result analysis
her2 results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_52.pdf,"besides, qualitative experimental results can be found in the
supplementary materials. 546
h. li et al.
table 1. comparison of the cell segmentation results on her2 test data. 4. qualitative results on the her2 test set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf,"in this paper, we present a novel framework, which can explicitly
capture protruded regions in kidneys to enable a better segmentation of
kidney tumors. we created a synthetic mask dataset that simulates a
protuberance, and trained a segmentation network to separate the pro-
truded regions from the normal kidney regions. to achieve the segmen-
tation of whole tumors, our framework consists of three networks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf,"given the initial
tumor region mask and the protruded region mask, the last network fuses
them and predicts the ﬁnal kidney tumor mask accurately. the proposed
method was evaluated on a publicly available kits19 dataset, which con-
tains 108 ncct images, and showed that our method achieved a higher
dice score of 0.615 (+0.097) and sensitivity of 0.721 (+0.103) compared
to 3d-unet. to the best of our knowledge, this is the ﬁrst deep learning
method that is speciﬁcally designed for kidney tumor segmentation on
ncct images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf,"our goal is to segment kidney tumors includ-
ing isodensity types on ncct images. to achieve this goal, we create a syn-
thetic dataset, which has separate annotations for normal kidneys and protruded
regions, and train a segmentation network to separate the protruded regions from
the normal kidney regions. in order to segment whole tumors, our framework
consists of three networks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf,"this framework can be extended to other organs (e.g. adrenal
gland, liver, pancreas). 3. verify that the proposed framework achieves a higher dice score compared to
the standard 3d u-net using a publicly available dataset. segmentation of kidney tumors on ncct images
15
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf,"the second protuberance detection network is the same as the base network
except it starts from 8 channels instead of 16. we train this network using syn-
thetic datasets. the details of the dataset and training procedures are described
in sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf,"and as a loss function, we use the dice loss [16] and
the cross-entropy loss equally.
segmentation of kidney tumors on ncct images
17
3.2
step2: training protuberance detection network
in the second step, we train the protuberance detection network alone to separate
protruded regions from the normal kidney masks. here, we only use the cross-
entropy loss and label smoothing with a smoothing factor of ϵ = 0.01.
synthetic dataset. to enable a segmentation of protruded regions only, a sep-
arate annotation of each region is usually required."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf,"however, annotating such
areas is time-consuming and preparing a large number of data is challenging. alternatively, we create a synthetic dataset that mimics a kidney with protru-
sions. the synthetic dataset is created through the following steps:
1. randomly sample a kidney mask without protuberance and a tumor mask."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf,"3. randomly insert the tumor mask into the kidney mask. 4. if both of the following conditions are met, append to the dataset. 
i kiti

i ki
< 0.3,
(1)

i kiti

i ti
< 0.95,
(2)
where ki is a voxel value (0 or 1) in the kidney mask and ti is a voxel value
in the tumor mask."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf,"3.3
step3: end-to-end training with fusion network
in the ﬁnal step, we train the complete network jointly. although our network is
fully diﬀerentiable, since there is no separate annotation for protruded regions
other from the synthetic dataset, we freeze the parameters in protuberance detec-
tion network. the output of the protuberance detection network will likely have more false
positives than the base network since it has no access to the input image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf,"this is
achieved by extracting kidney masks and adjusting the height of each kidney. the ground truth mask contains a kidney label and a kidney tumor label. cysts
are not annotated separately and included in the kidney label on this dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_2.pdf,"during
the training, the images were randomly cropped to a patch size of 128×128×128
voxels. we applied random rotation, random scaling and random noise addition
as data augmentation. during the step2 phase of the training, where we used the synthetic dataset,
we created 10,000 masks using the method from sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"gastroscopic lesion detection (gld) plays a key role in
computer-assisted diagnostic procedures. however, this task is not well
studied in the literature due to the lack of labeled data and the appli-
cable methods. generic detectors perform below expectations on gld
tasks for 2 reasons: 1) the scale of labeled data of gld datasets is far
smaller than that of natural-image object detection datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"2) gastro-
scopic images exhibit distinct diﬀerences from natural images, which are
usually of high similarity in global but high diversity in local. such char-
acteristic of gastroscopic images also degrades the performance of using
generic self-supervised or semi-supervised methods to solve the labeled
data shortage problem using massive unlabeled data. in this paper, we
propose self- and semi-supervised learning (ssl) for gld tailored for
using massive unlabeled gastroscopic images to enhance gld tasks per-
formance, which consists of a hybrid self-supervised learning (hsl)
method for backbone pre-training and a prototype-based pseudo-label
generation (ppg) method for semi-supervised detector training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"the
hsl combines patch reconstruction with dense contrastive learning to
boost their advantages in feature learning from massive unlabeled data. the pgg generates pseudo-labels for unlabeled data based on similarity
to the prototype feature vector to discover potential lesions and avoid
introducing much noise. moreover, we contribute the ﬁrst large-scale
gld datasets (lgldd), which contains 10,083 gastroscopic images
with 12,292 well-annotated boxes for four-category lesions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"x. zhang and k. yin—contribute equally to this paper. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_9.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43904-9_9
84
x. zhang et al.
keywords: gastroscopic lesion detection · self-supervised backbone
pre-training · semi-supervised detector training
1
introduction
gastroscopic lesion detection (gld) plays a key role in computer-assisted
diagnostic procedures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"some appearances of lesions are quite rare and can only be observed in a
few patients. generic self-supervised backbone pre-training or semi-supervised
detector training methods can solve the ﬁrst challenge for natural images but its
eﬀectiveness is undermined for gastroscopic images due to the second challenge.
self-supervised backbone pre-training methods enhance object detection
performance by learning high-quality feature representations from massive unla-
belled data for the backbone. the mainstream self-supervised backbone pre-
training methods adopt self-supervised contrast learning [3,4,7,9,10] or masked
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"1. pipeline of self- and semi-supervised learning (ssl) for gld. ssl
consists of a hybrid self-supervised learning (hsl) method and a prototype-based
pseudo-label generation (ppg) method. hsl combines patch reconstruction with
dense contrastive learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"[3,4,7,9] can
learn discriminative global feature representations, and [10] can further learn
discriminative local feature representations by extending contrastive learning
to dense paradigm. however, these methods usually cannot grasp enough local
detailed information. on the other hand, masked image modeling is expert in
extracting local detailed information but is weak in preserving the discriminabil-
ity of feature representation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"therefore, both types of methods have their own
weakness for gld tasks. semi-supervised object detection methods [12,14,16,17,20,22,23] ﬁrst use
detectors trained with labeled data to generate pseudo-labels for unlabeled data
and then enhance object detection performance by regarding these unlabeled
data with pseudo-labels as labeled data to train the detector. current pseudo-
label generation methods rely on the objectiveness score threshold to generate
pseudo-labels, which makes them perform below expectations on gld, because
the characteristic of gastroscopic lesions makes it diﬃcult to set a suitable thresh-
old to discover potential lesions meanwhile avoiding introducing much noise."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"the main challenge for this goal is the characteristic
of gastroscopic lesions. intuitively, such a challenge requires local feature rep-
resentations to contain enough detailed information, meanwhile preserving dis-
criminability. enlightened by this, we propose the self- and semi-supervised
learning (ssl) framework tailored to address challenges in daily clinical prac-
tice and use massive unlabeled data to enhance gld performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"ssl over-
comes the challenges of gld by leveraging a large volume of unlabeled gastro-
scopic images using self-supervised learning for improved feature representations
and semi-supervised learning to discover and utilize potential lesions to enhance
performance. speciﬁcally, it consists of a hybrid self-supervised learning
(hsl) method for self-supervised backbone pre-training and a prototype-based
pseudo-label generation (ppg) method for semi-supervised detector training. [10] with the patch recon-
struction to inherit the advantages of discriminative feature learning and grasp
the detailed information that is important for gld tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"the ppg generates
pseudo-labels based on the similarity to the prototype feature vectors (formu-
lated from the feature vectors in its memory module) to discover potential lesions
from unlabeled data, and avoid introducing much noise at the same time. more-
over, we propose the ﬁrst large-scale gld datasets (lgldd), which contains
10,083 gastroscopic images with 12,292 well-annotated lesion bounding boxes
of four categories of lesions (polyp, ulcer, cancer, and sub-mucosal tumor). in summary, our contributions include:
– a self- and semi-supervise learning (ssl) framework to leverage massive
unlabeled data to enhance gld performance.
– a large-scale gastroscopic lesion detection datasets (lgldd)
86
x. zhang et al.
– experiments on lgldd demonstrate that ssl can bring signiﬁcant enhance-
ment compared with baseline methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"the ground truth is the corresponding patches vi = {vi1, vi2, ..., vis2}
of the input view. lh = lg + λdld + λrlr
where λd and λr are the weights of ld and lr and are set to 1 and 2.
2.2
prototype-based pseudo-label generation method
we propose the prototype-based pseudo-label generation method (ppg) to
discover potential lesions from unlabeled gastroscopic data meanwhile avoid
introducing much noise to further enhance gld performance. speciﬁcally, ppg
adopts a memory module to remember feature vectors of the representative
lesions as memory and generates prototype feature vectors for each class based
on the memories stored."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"to preserve the representativeness of the memory and
further the prototype feature vectors, ppg designs a novel memory update
strategy. in semi-supervised learning, ppg generates pseudo-labels for unlabeled
data relying on the similarity to the prototype feature vectors, which achieves a
better balance between lesion discovery and noise avoidance. memory module stores a set of lesion feature vectors as
memory."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"to maintain stability, we start updating the memory and calculating its
loss after ﬁxed epochs, and only the positive sample feature vector can be selected
to update the memory. ppg proposes to generate pseudo-labels based on
the similarity between the prototype feature vectors and the feature vector of
potential lesions. otherwise omits it."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"we invite 10 senior doctors to annotate them from the unlabeled
endoscopic images. to preserve the annotation quality, doctors can refer to the
diagnosis reports, and each lesion is annotated by a doctor and checked by
another. finally, they annotates 12,292 lesion boxes in 10,083 images after going
through about 120,000 images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"the train/val split of
lgmdd is 8,076/2,007. the other data serves as unlabeled data. evaluation metrics : we use standard object detection metrics to evaluate the
gld performance, which computes the average precision (ap) under multiple
intersection-of-union (iou) thresholds and then evaluate the performance using
the mean of aps (map) and the ap of some speciﬁc iou threshold."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"ppg can bring extra 0.9ap and 0.9ap
enhancement for centernet and fasterrcnn respectively. we conduct extra experiments based on faster rcnn
to further analyze the eﬀect of diﬀerent parameter settings on lgldd.
1) reconstruction loss weight λr is designed to balance the losses of con-
trastive learning and the reconstruction, which is to balance the discriminability
and the detailed information volume of local feature representations. as illus-
trated in table 2.a, only suitable λr can fully boost the detection performance.
2) objectiveness score threshold τu: we compare ppg with objective-
ness score-based pseudo-label generation methods with diﬀerent τu (table 2.b)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"the objectiveness score threshold controls the quality of pseudo-labels. a) a
self- and semi-supervised learning for gastroscopic lesion detection
91
low threshold generates noisy pseudo-labels, leading to reduced performance (-
0.6/-0.2 ap at thresholds 0.5/0.6). b) a high threshold produces high-quality
pseudo-labels but may miss potential lesions, resulting in only slight performance
improvement (+0.3 ap at threshold 0.7)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_9.pdf,"(endo21 challenge consists of 4 sub-tasks and
only the sub-task 2 train/test split is available according to the [2]). experi-
mental results in table 2.d show that ssl can bring signiﬁcant improvements to
publicly available datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"unsupervised
domain
adaptation
(uda)
has
become
increasingly popular in imaging-based diagnosis due to the challenge of
labeling a large number of datasets in target domains. without labeled
data, well-trained deep learning models in a source domain may not per-
form well when applied to a target domain."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"uda allows for the use of
large-scale datasets from various domains for model deployment, but it
can face diﬃculties in performing adaptive feature extraction when deal-
ing with unlabeled data in an unseen target domain. to address this, we
propose an advanced test-time ﬁne-tuning uda framework designed to
better utilize the latent features of datasets in the unseen target domain
by ﬁne-tuning the model itself during diagnosis. our proposed frame-
work is based on an auto-encoder-based network architecture that ﬁne-
tunes the model itself."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"this helps the framework to converge to a local mini-
mum that is better-suited for the target domain, allowing for improved
performance in domain adaptation tasks. to evaluate our framework, we
carried out experiments on uda segmentation tasks using breast can-
cer datasets acquired from multiple domains. our experimental results
demonstrated that our framework achieved state-of-the-art performance,
outperforming other competing uda models, in segmenting breast can-
cer on ultrasound images from an unseen domain, which supports its
clinical potential for improving breast cancer diagnosis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"the primary
limitation is that dl models require a large number of training samples to
achieve accurate predictions [8,24]. yet, acquiring large training datasets and
their corresponding labels, especially from a cohort of patients, can be costly
or even infeasible, which poses a signiﬁcant challenge in developing a dl model
with high performance [7]. second, even when large-scale datasets are available
through collaborative research from multiple sites, dl models trained on such
datasets may yield sub-optimal solutions due to domain gaps caused by diﬀer-
ences in images acquired from diﬀerent sites [20]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"example solu-
tions include transfer learning- and style transfer-based methods. this is due to sensitive privacy issues in patients’ data,
particularly in collaborative research, which restricts access to labels from diﬀer-
ent domains. [16,33], aiming to generate semi-predictions (pseudo-labels)
in target domains ﬁrst, followed by producing accurate predictions using the
pseudo-labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"one critical limitation of pseudo-label-based uda is the possi-
bility of error accumulation due to mispredicted pseudo-labels. this can lead to
signiﬁcant degradation of the performance of dl models, as errors can compound
and become more pronounced over time [17,25].
to alleviate the problem of pseudo-label-based uda, in this work, we propose
an advanced uda framework based on self-supervised da with a test-time ﬁne-
tuning network. test-time adaptation methods have been developed [4,11,13,23]
to improve the learning of knowledge in target domains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"with our framework,
we are able to identify the best-performing parameters that result in improved
performance in da tasks. • our framework is eﬀective at preserving privacy, since it carries out da using
only pre-trained network parameters, without transferring any patient data. • we applied our framework to the task of segmenting breast cancer from ultra-
sound imaging data, demonstrating its superior performance over competing
uda methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"the main task is the segmentation task,
(h◦dseg◦e)(x). in predicting segmentation labels in the target domain (t ), dft
is also involved in the main task, and the ﬁnal prediction after the ﬁne-tuning is
542
k. lee et al.
provided by

h ◦ (dseg ⊕ dft) ◦ e

(x), where ⊕ is the concatenation operation. in the pretext task, e, a decoder for a generator, dgen, and a discriminator, c,
are involved."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"the pretext task aims to generate synthetic images, (dgen ◦ e)(t). note that dgen and dseg share the same parameters to enable knowledge trans-
fer. however, since the headers of image reconstruction and generating segmen-
tation mask are diﬀerent (diﬀerent output), a new header incorporating df t
and dseg is devised and leverages the outputs of two decoders."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"besides, dgen =
df t is ﬁne-tuned during the ﬁne-tuning step, and the df t learns the knowledge
of the input domain via image reconstruction. two distinct knowledge (informa-
tion) from dseg and df t enable the network to utilize target domain knowledge
and predict precise predictions. pre-training in source domain."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"additionally, ds
seg = ds
gen.
fine-tuning in target domain. since the pre-trained model is likely to
produce imprecise predictions in t , the model should learn domain knowledge
about t . to this end, in the pretext task, for self-supervised learning, the model
is ﬁne-tuned in t to generate synthetic images identical to the input images as
below:
θp
t = argmin
θp
t

t
lgan

(ds
gen ◦ es)(t), t

⇒
θp
t ⊇ es ∪ ds→t
gen
,
(2)
where only dgen is ﬁne-tuned to achieve memory eﬃciency and to decrease the
ﬁne-tuning time, and ds
gen is ﬁne-tuned as ds→t
gen
."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"furthermore, since dt
ft is ﬁne-tuned in t
self-supervised domain adaptive segmentation of breast cancer
543
fig. 2. illustration of the local minimum of the source (a) and target (b) domains and
parameter ﬂuctuation (c)
using knowledge distillation, it can provide domain-speciﬁc information for t . as a result, the predictions made by the ﬁne-tuned model in t are jointly con-
strained by the expectations of ds
seg and dt
ft."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"[18], which
are considered to be diﬀerent domains. all three databases contain ultrasound
imaging data and segmentation masks for breast cancer, with the masks labeled
as 0 (background) and 1 (lesion) using a one-hot encoding. the bus database
consists of 163 images along with corresponding labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"the busi database con-
tains 780 images, with 133 images belonging to the normal class and having
labels containing only 0 values. the buv database originally consists of ultra-
sound videos, providing a total of 21,702 frames. while the database also provides
labels for the detection task, we processed these labels as segmentation masks
using a region growing method [15]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"as the
evaluation metrics, dice coeﬃcient (d. coef), prauc, which is an area under a
precision-recall curve, and cohen kappa (κ) were employed [30]. our experimen-
tal set-ups included: (i) individual databases were used to assess the baseline
segmentation performance (appendix); (ii) the domain adaptive segmentation
performance was assessed using the three databases, where two databases were
regarded as the source domain, and the remaining database was regarded as the
target domain; and (iii) the ablation study was carried out to evaluate the pro-
posed network architecture along with the randomized re-initialization method.
3.2
comparison analysis
since all compared dl models show similar d. coef, only uda performance is
comparable as a control in our experiments. in this experiment, two databases
were used for training, and the remaining database was used for testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"for
instance, bus in fig. 3 illustrates the bus database was used for testing, and
self-supervised domain adaptive segmentation of breast cancer
545
fig. 3. comparison analysis of our framework and comparison models: performance
comparison table (left) and box-and-whisker plot (right)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"area
under the precision-recall curve (pr-auc) values were reported. the other two databases of busi and buv were used for training. 3 and
4 show quantitative results, and fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"5 shows the sample segmentation results. unlike the experiment using the individual database, u-net, fusionnet, and
mib-net showed signiﬁcantly inferior scores due to domain gaps. in contrast,
uda methods of cbst and ct-net showed superior scores, compared with
others, and the scores were not strongly reduced, compared with the experiment
with the single database."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"546
k. lee et al.
fig. 5. segmentation results by ours and comparison models on each database.
fig. 6. illustration of feature maps: style loss comparison (left) and a t-sne plot of
generated images by diﬀerent decoders (right)
3.3
ablation study
in order to assess the eﬀectiveness of each of the proposed modules, includ-
ing the parameter ﬂuctuation and ﬁne-tuning methods, the ablation study was
carried out."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"additionally, the generated images
by decoders, including ds
seg, dﬂ
seg, and ds→t
seg
in s and t are plotted with t-
sne, where the short distance represents the similar features [19]. the generated
images became similar to t in order of ds
seg, dﬂ
seg, and ds→t
seg
, which conﬁrmed
the eﬀectiveness of the ﬁne-tuning method in terms of knowledge distillation. additionally, the parameters were successfully re-positioned from the local min-
imum in s by parameter ﬂuctuation, which was conﬁrmed by the distances from
s to ds
gen and dﬂ
gen.
4
discussion and conclusion
in this work, we proposed a dl-based segmentation framework for multi-domain
breast cancer segmentation on ultrasound images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"to address
this issue, we introduced a novel self-supervised da network for breast cancer
segmentation in ultrasound images. in particular, we proposed a test-time ﬁne-
tuning network to learn domain-speciﬁc knowledge via knowledge distillation by
self-supervised learning. since uda is susceptible to error accumulation due to
imprecise pseudo-labels, which can lead to degraded performance, we employed
a self-supervised learning-based pretext task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_52.pdf,"this approach enabled our framework to eﬃciently
ﬁne-tune the network in the target domain and achieve better segmentation
performance. experimental results, carried out with three ultrasound databases
from diﬀerent domains, demonstrated the superior segmentation performance of
our framework over other competing methods. additionally, our framework is
well-suited to a scenario in which access to source domain data is limited, due
to data privacy protocols."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"state-of-the-art object detection and segmentation meth-
ods for microscopy images rely on supervised machine learning, which
requires laborious manual annotation of training data. here we present
a self-supervised method based on time arrow prediction pre-training
that learns dense image representations from raw, unlabeled live-cell
microscopy videos."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"we show that the resulting dense representations capture
inherently time-asymmetric biological processes such as cell divisions on
a pixel-level. we furthermore demonstrate the utility of these represen-
tations on several live-cell microscopy datasets for detection and segmen-
tation of dividing cells, as well as for cell state classiﬁcation. our method
outperforms supervised methods, particularly when only limited ground
truth annotations are available as is commonly the case in practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"we
provide code at https://github.com/weigertlab/tarrow. the resulting datasets can consist of terabytes
of raw videos that require automatic methods for downstream tasks such as clas-
siﬁcation, segmentation, and tracking of objects (e.g. cells or nuclei). current
state-of-the-art methods rely on supervised learning using deep neural networks
that are trained on large amounts of ground truth annotations [6,25,31]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"recently,
self-supervised representation learning (ssl) has emerged as a promising app-
roach to alleviate this problem [1,3]. in ssl one ﬁrst deﬁnes a pretext task
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43993-3_52.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. 1. a) example frames from two live-cell microscopy videos."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"however, to the best of our knowl-
edge, ssl via time arrow prediction has not yet been studied in the context of
live-cell microscopy. concretely our contributions are: i) we introduce the time
arrow prediction pretext task to the domain of live-cell microscopy and propose
the tap pre-training scheme, which learns dense representations (in contrast to
only image-level representations) from raw, unlabeled live-cell microscopy videos,
ii) we propose a custom (permutation-equivariant) time arrow prediction head
that enables robust training, iii) we show via attribution maps that the repre-
sentations learned by tap capture biologically relevant processes such as cell
divisions, and ﬁnally iv) we demonstrate that tap representations are beneﬁcial
for common image-level and pixel-level downstream tasks in live-cell microscopy,
especially in the low training data regime. z = f(x) ∈ rc×h×w from
single images x ∈ rh×w (cf. fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"to that end, we
randomly sample from each sequence i pairs of smaller patches x1, x2 ∈ rh×w
from the same spatial location but consecutive time points x1 ⊂ it, x2 ⊂ it+1. we next ﬂip the order of each pair with equal probability p = 0.5, assign it the
corresponding label y (forward or backward) and compute dense representations
z1 = f(x1) and z2 = f(x2) with z1, z2 ∈ rc×h×w via a fully convolutional
feature extractor f. =
e˜zt
i ·˜zj/τ
c
j=1 e˜zt
i ·˜zj/τ
(2)
here ˜z ∈ rc×2hw denotes the stacked features z ﬂattened across the non-channel
dimensions, and τ is a temperature parameter."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"throughout the experiments
we use λ = 0.01 and τ = 0.2. and x2 ⊂ it+δt, which we empirically found to work better for
datasets with high frame rate. in other words, h should be equivari-
ant wrt."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"the last layer hl includes an additional global average pooling
along the spatial dimensions to yield the ﬁnal logits ˆy ∈ r2.
augmentations: to avoid overﬁtting on artiﬁcial image cues that could be
discriminative of the temporal order (such as a globally consistent cell drift,
or decay of image intensity due to photo-bleaching) we apply the following aug-
mentations (with probability 0.5) to each image patch pair x1, x2: ﬂips, arbitrary
rotations and elastic transformations (jointly for x1 and x2), translations for x1
and x2 (independently), spatial scaling, additive gaussian noise, and intensity
shifting and scaling (jointly+independently). 3
experiments
3.1
datasets
to demonstrate the utility of tap for a diverse set of specimen and microscopy
modalities we use the following four diﬀerent datasets:
hela. the dataset consists of four videos with
overall 368 frames of size 1100 × 700."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"3b), imaged by ﬂuorescence microscopy every 4 min [27,28]. the dataset
consists of a single video with 1200 frames of size 1600×1200. drosphila
melanogaster
pupal
wing
expressing
ecad::gfp
(cf. fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"3a), imaged by spinning disk confocal microscopy every 5 min [4,20]. the dataset consists of three videos with overall 410 frames of size 3900 × 1900. we use δt = 1.
self-supervised dense representation learning
541
yeast."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"s. cerevisiae cells (cf. fig. the dataset consists of ﬁve videos with overall 600 frames
of size 1024 × 1024. for each dataset we heuristically choose δt to roughly correspond to the time
scale of observable biological processes (i.e. larger δt for higher frame rates).
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"total training time for a single tap model is roughly 8h
on a single gpu. tap is implemented in pytorch. 3.3
time arrow prediction pretraining
we ﬁrst study how well the time arrow prediction pretext task can be solved
depending on diﬀerent image structures and used data augmentations. to
that end, we train tap networks with an increasing number of augmentations
on hela and compute the tap classiﬁcation accuracy for consecutive image
patches x1, x2 that contain either background, interphase (non-dividing) cells,
or mitotic (dividing) cells."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"2a, the accuracy on background
regions is approx. 50% irrespective of the used augmentations, suggesting the
absence of predictive cues in the background for this dataset. in contrast, on
regions with cell divisions the accuracy reaches almost 100%, conﬁrming that
542
b. gallusser et al.
tap is able to pick up on strong time-asymmetric image features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"interestingly,
the accuracy for regions with non-dividing cells ranges from 68% to 80%, indi-
cating the presence of weak visual cues such as global drift or cell growth. when
using more data augmentations the accuracy decreases by roughly 12% points,
suggesting that data augmentation is key to avoid overﬁtting on confounding
cues. next we investigate which regions in full-sized videos are most discriminative
for tap."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"to that end, we apply a trained tap network on consecutive full-
sized frames x1, x2 and compute the dense attribution map of the classiﬁcation
logits y wrt. to the tap representations z via grad-cam [23]. 3 we
show example attribution maps on top of single raw frames for three diﬀerent
datasets. strikingly, the attribution maps highlight only a few distributed, yet
highly localized image regions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"insets show the top six
most discriminative regions and their temporal context (± 2 timepoints). note that
across all datasets almost all regions contain cell divisions. best viewed on screen."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"using the permutation-equivariant head alleviated this problem and enabled a
consistent loss decrease already from the beginning of training. 3.4
downstream tasks
we next investigate whether the learned tap representations are useful for com-
mon supervised downstream tasks, where we especially focus on their utility in
the low training data regime. first we test the learned representations on two
image-level classiﬁcation tasks, and later on two dense segmentation tasks.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"we show results of three runs per model, # of params in parenthesis.
mitosis classiﬁcation on flywing: since tap attribution maps strongly
highlight cell divisions, we consider predicting mitotic events an appropriate
ﬁrst downstream task to evaluate tap. to that end, we generate a dataset of
97k crops of size 2 × 96 × 96 from flywing and label them as mitotic/non-
mitotic (16k/81k) based on available tracking data [20]. we train tap networks
on flywing and use a small resnet architecture (≈ 5m params) that is trained
from scratch as a supervised baseline."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"in fig. 4a we show average precision (ap)
on a held-out test set while varying the amount of available training data. as
expected, the performance of the supervised baseline drops substantially for low
amounts of training data and surprisingly is already outperformed by a linear
classiﬁer (100 params) on top of tap representations (e.g. 0.90 vs. 0.77 for 76
labeled crops). training a small resnet on ﬁxed tap representations consistently
outperforms the supervised baseline even if hundreds of annotated cell divisions
are available for training (e.g. 0.96 vs. 0.95 for 2328 labeled crops with ∼ 400 cell
divisions), conﬁrming the value of tap representations to detect mitotic events."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"cell state classiﬁcation on mdck: next we turn to the more challenging
task of distinguishing between cells in interphase, prometaphase and anaphase
from mdck. this dataset consists of 4800 crops of size 80 × 80 that are labeled
with one of the three classes (1600 crops/class). again we use a resnet as
544
b. gallusser et al.
supervised baseline and report in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"4b test classiﬁcation accuracy for varying
amount of training data. as before, both a linear classiﬁer as well as a resnet
trained on ﬁxed tap representations outperform the baseline especially in the
low data regime, with the latter showing better or comparable results across the
whole data regime (e.g. 0.90 vs. 0.83 for 117 annotated cells). additionally, we
ﬁnetune the pretrained tap feature extractor for this downstream task, which
slightly improves the results given enough training data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"mitosis segmentation on flywing: we now apply tap on a pixel-level down-
stream task to fully exploit that the learned tap representations are dense. we
use the same dataset as for flywing mitosis classiﬁcation, but now densely
label post-mitotic cells. we predict a pixel-wise probability map, threshold it at
0.5 and extract connected components as objects."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"the baseline model
is a u-net trained from scratch. training a u-net on ﬁxed tap representa-
tions always outperforms the baseline, and when only using 3% of the train-
ing data it reaches similar performance as the baseline trained on all available
labels (0.67 vs. 0.68, fig. 5a). interestingly, ﬁne-tuning tap only slightly outper-
forms the supervised baseline for this task even for moderate amounts of training
data, suggesting that ﬁxed tap representations generalize better for limited-size
datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"emerging bud detection on yeast: finally, we test tap on the challenging
task of segmenting emerging buds in phase contrast images of yeast colonies. we train tap networks on yeast and generate a dataset of 1205 crops of size
5 × 192 × 192 where we densely label yeast buds in the central frame (deﬁned
self-supervised dense representation learning
545
as buds that appeared less than 13 frames ago) based on available segmenta-
tion data [17]. we evaluate all methods on held out test videos by interpreting
the resulting 2d+time segmentations as 3d objects and computing the f1 score
using an iou threshold of 0.25."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"the baseline model is again a u-net trained from
scratch. surprisingly, training with ﬁxed tap representations performs slightly
worse than the baseline for this dataset (fig. 5b), possibly due to cell density
diﬀerences between tap training and test videos."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"we show
that tap uncovers sparse time-asymmetric biological processes and events in raw
unlabeled recordings without any human supervision. furthermore, we demon-
strate on a variety of datasets that the learned features can substantially reduce
the required amount of annotations for downstream tasks. although in this
work we focus on 2d+t image sequences, the principle of tap should generalize
to 3d+t datasets, for which dense ground truth creation is often prohibitively
expensive and therefore the beneﬁts of modern deep learning are not fully tapped
into."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"we leave this to future work, together with the application of tap to cell
tracking algorithms, in which accurate mitosis detection is a crucial component.
acknowledgements. we thank albert dominguez (epfl) and uwe schmidt for
helpful comments, natalie dye (pol dresden) and franz gruber for providing the
flywing dataset, benedikt mairhörmann and kurt schmoller (helmholtz munich)
for providing additional yeast training data, and alan lowe (ucl) for providing the
mdck dataset. m.w. and b.g. are supported by the epfl school of life sciences
elisir program and carigest sa.
references
1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"elife 4, e07090 (2015)
5. gidaris, s., singh, p., komodakis, n.: unsupervised representation learning by
predicting image rotations. in: iclr, openreview.net (2018)
546
b. gallusser et al.
6. greenwald, n.f., miller, g., moen, e., kong, a., kagel, a., et al.: whole-cell
segmentation of tissue images with human-level performance using large-scale data
annotation and deep learning. 40(4), 555–565 (2021)
7. han, h., dmitrieva, m., sauer, a., tam, k.h., rittscher, j.: self-supervised
voxel-level representation rediscovers subcellular structures in volume electron
microscopy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"misra, i., zitnick, c.l., hebert, m.: shuﬄe and learn: unsupervised learning using
temporal order veriﬁcation. padovani, f., mairhörmann, b., falter-braun, p., lengefeld, j., schmoller, k.m.:
segmentation, tracking and cell cycle analysis of live-cell imaging data with cell-
acdc. padovani, f., mairhörmann, b., lengefeld, j., falter-braun, p., schmoller, k.:
cell-acdc: segmentation, tracking, annotation and quantiﬁcation of microscopy
imaging data (dataset)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8/paper_52.pdf,"21(5),
558–565 (2011)
self-supervised dense representation learning
547
27. ulicna, k., vallardi, g., charras, g., lowe, a.: mdck cell tracking reference
dataset. https://rdr.ucl.ac.uk/articles/dataset/cell_tracking_reference_dataset/
16595978
28."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"self-supervised learning (ssl) has led to important break-
throughs in computer vision by allowing learning from large amounts
of unlabeled data. as such, it might have a pivotal role to play in
biomedicine where annotating data requires a highly specialized exper-
tise."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"in this work, we study the use of
a leading ssl framework, namely masked siamese networks (msns), for
endoscopic video analysis such as colonoscopy and laparoscopy. to fully
exploit the power of ssl, we create sizable unlabeled endoscopic video
datasets for training msns. these strong image representations serve
as a foundation for secondary training with limited annotated datasets,
resulting in state-of-the-art performance in endoscopic benchmarks like
surgical phase recognition during laparoscopy and colonoscopic polyp
characterization."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"additionally, we achieve a 50% reduction in annotated
data size without sacriﬁcing performance. thus, our work provides evi-
dence that ssl can dramatically reduce the need of annotated data in
endoscopy. keywords: artiﬁcial intelligence · self-supervised learning ·
endoscopy video analysis
1
introduction
endoscopic operations are minimally invasive medical procedures which allow
physicians to examine inner body organs and cavities."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"it is used to diagnose and treat a variety of conditions,
including ulcers, polyps, tumors, and inﬂammation. over 250 million endoscopic
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_55.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43904-9_55
570
r. hirsch et al.
procedures are performed each year globally and 80 million in the united states,
signifying the crucial role of endoscopy in clinical research and care."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"a cardinal challenge in performing endoscopy is the limited ﬁeld of view which
hinders navigation and proper visual assessment, potentially leading to high
detection miss-rate, incorrect diagnosis or insuﬃcient treatment. yet the success of such ai systems heavily
relies on acquiring annotated data which requires experts of speciﬁc knowledge,
leading to an expensive, prolonged process. in the last few years, self-supervised
learning (ssl [5–8]) has been shown to be a revolutionary strategy for unsu-
pervised representation learning, eliminating the need to manually annotate vast
quantities of data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"demonstrating performance on-par with the top results reported
in the literature. yet, the power of ssl lies in large data regimes. through extensive experiments, we
ﬁnd that scaling the data size necessitates scaling the model architecture, lead-
ing to state-of-the-art performance in surgical phase recognition of laparoscopic
procedures, as well as in polyp characterization of colonoscopic videos."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,", the proposed approach exhibits robust generalization, yielding better
performance with only 50% of the annotated data, compared with standard
supervised learning using the complete labeled dataset. this shows the potential
to reduce signiﬁcantly the need for expensive annotated medical data. 2
background and related work
there exist a wide variety of endoscopic applications."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"phase recognition in surgical videos is an
important task that aims to improve surgical workﬂow and eﬃciency. apart
from measuring quality and monitoring adverse event, this task also serves in
facilitating education, statistical analysis, and evaluating surgical performance.
self-supervised learning for endoscopy
571
furthermore, the ability to recognize phases allows real-time monitoring and
decision-making assistance during surgery, thus improving patient safety and
outcomes. [17,18,32]; however, they typically
require large labelled training datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"as an alternative, ssl methods have
been developed [12,28,30], however, these are early-days methods that based
on heuristic, often require external information and leads to sub-optimal perfor-
mance. a recent work [27] presented an extensive analysis of modern ssl tech-
niques for surgical computer vision, yet on relatively small laparoscopic datasets. optical polyp characterization."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"in recent years,
various ai systems have been developed to this end [1,19]. however, training such
automatic optical biopsy systems relies on a large body of annotated data, while
ssl has not been investigated in this context, to the best of our knowledge. [5–8], relying on two
key factors: (i) eﬀective algorithms for unsupervised learning and (ii) training
on large-scale datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"here, we ﬁrst describe masked siamese networks [2],
our chosen ssl framework. additionally, we present our large-scale data collec-
tion (see fig. 2). through extensive experiments in sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"4, we show that training
msns on these substantial datasets unlocks their potential, yielding eﬀective rep-
resentations that transfer well to public laparoscopy and colonoscopy datasets. [2] have set a new state-of-the-art among ssl methods on the imagenet
benchmark [29], with a particular focus on the low data regime. this is of great
interest for us since our downstream datasets are typically of small size [32,33]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"furthermore, to prevent representation collapse and
encourage the model to fully exploit the prototypes, a mean entropy maximiza-
tion (me-max) regularizer [2,22] is added, aiming to maximize the entropy h(¯pa)
of the average prediction across all the anchor views ¯pa ≜
1
mb
b
i=1
m
m=1 pa
i,m.
thus, the overall training objective to be minimized for both θa and q is where
λ > 0 is an hyperparameter and the gradients are computed only with respect
to the anchor predictions pa
i,m (not the target predictions pt
i). applying msns
on the large datasets described below, generates representations that serve as a
strong basis for various downstream tasks, as shown in the next section.
3.2
private datasets
laparoscopy. we compiled a dataset of laparoscopic procedures videos exclu-
sively performed on patients aged 18 years or older."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"the dataset consists of 7,877
videos recorded at eight diﬀerent medical centers in israel. the dataset predom-
inantly consists of the following procedures: cholecystectomy (35%), appendec-
tomy (20%), herniorrhaphy (12%), colectomy (6%), and bariatric surgery (5%). the remaining 21% of the dataset encompasses various standard laparoscopic
operations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"further details are given in the supplementary materials.
colonoscopy. we have curated a dataset comprising 13,979 colonoscopy videos
of patients aged 18 years or older. these videos were recorded during standard
colonoscopy procedures performed at six diﬀerent medical centers between the
years 2019 and 2022."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"using this model,
we obtained bounding boxes around the detected polyps. to ensure high-quality
self-supervised learning for endoscopy
573
data, we ﬁltered out detections with conﬁdence scores below 0.5. for each frame,
we cropped the bounding boxes to generate individual images of the polyps."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"1. schematic of masked siamese networks. 2. data samples. bottom: colonoscopy.
4
experiments
in this section, we empirically demonstrate the power of ssl in the context
of endoscopy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"senior surgeons annotated
each frame to one out of seven phases. [33]: a uniﬁed dataset of
155 colonoscopy videos (37,899 frames) with labeled polyp classes (hyperplastic
or adenoma) and bounding boxes. we use the provided detections to perform
binary classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"we explore large scale ssl pretraining for endoscopy
videos. table 1 compares the results of pretraining with diﬀerent datasets (pub-
lic and private) and model sizes. we pretrain the models with msn and then
report their downstream performances."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"we present results for the cholecystec-
tomy phase recognition task based on ﬁne-tuned models and for the optical
polyp characterization task based on linear evaluation, due to the small size of
the public dataset. as baselines, we report fully-supervised resnet50 results,
trained on public datasets. we ﬁnd that replacing resnet50 with vit-s, despite
comparable number of parameters, yields sub-optimal performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"phase recognition per-video
results improve by 1.3 points when using the msn pretraining, while polyp char-
acterization improve by 2.2 points. importantly, we see that the performance gap
becomes prominent when using the large scale private datasets for ssl pretrain-
ing. here, per-frame and per-video phase recognition performances improve by
6.7% and 8.2%, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"when using the private colonoscopy dataset the
macro f1 improves by 11.5% compared to the fully supervised baseline. notice
that the performance improves with scaling both model and private data sizes,
demonstrating that both factors are crucial to achieve optimal performance. next, we examine the beneﬁts of using msns to improve
downstream performance in a low-shot regime with few annotated samples.
1 for reproducibility purposes, code and model checkpoints are available at https://
github.com/royhirsch/endossl.
self-supervised learning for endoscopy
575
table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"we see that 50% random masking (i.e. we keep 98 tokens out of
196 for the global view) and using 4 local views gives the best of performance. we study the eﬀect of data augmentation. [7], hence, it is important to re-evaluate
these choices for medical images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_55.pdf,"table 2. ablation study of diﬀerent design choices (default setting is highlighted). a) number of prototypes
d) data augmentation
f) avoiding collapse."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"a typical
colonoscopy cade detects a polyp in a single frame and does not track it
through the video sequence. yet, many downstream tasks including polyp
characterization (cadx), quality metrics, automatic reporting, require
aggregating polyp data from multiple frames. in this work we propose
a robust long term polyp tracking method based on re-identiﬁcation by
visual appearance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"it is well known
that many polyps go unnoticed during colonoscopy [22]. the success of polyp detector sparkled the
development of new cad tools for colonoscopy, including polyp characterization
(cadx, or optical biopsy), extraction of various quality metrics, and automatic
reporting. many of those new cad applications require aggregation of all avail-
able data on a polyp into a single uniﬁed entity."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"for example, one would expect
higher accuracy for cadx when it analyzes all frames where a polyp is observed. clustering polyp detections into polyp entities is a prerequisite for computing
such quality metrics as polyp detection rate (pdr) and polyps per colonoscopy
(ppc), and for listing detected polyps in a report. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_57.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"we, on the other hand, follow an early fusion
approach by building a joint representation for the whole sequence. [23] to leverage the attention paradigm for non-
uniform weighing and “knowledge exchange” between tracklet frames. we extensively test the proposed method on hundreds of colonoscopy videos
and evaluate the contribution of method components using an ablation study."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"the vectors of diﬀerent views of the
same polyp are placed closer, and of diﬀerent polyps away from each other [11]. a straightforward approach to train such model is supervised learning, which
requires forming a large collection of polyp image pairs, manually labeled as
same/not same polyp [1]. such annotation turned out to be inaccurate and
expensive."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"in addition, ﬁnding hard negative pairs is especially challenging, as
images of two randomly sampled polyps are usually very dissimilar. moreover,
self-supervised techniques using extensive unannotated datasets has exhibited
substantial advantages within the medical domain [12]. hence, we turn to simclr [5], a contrastive self-supervised learning tech-
nique, which requires no manual labeling."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"a commonly used practice is to compute
single frame embedding (for each view) and fuse them [8,21], e.g. by averaging. the downside of those simple techniques is that they treat every frame in the
same way, including bad quality, repeating, non-informative views. we postulate
that learning a joint embedding of multiple views in an end-to-end manner will
produce a better representation of the visual properties of a polyp, by allowing
“knowledge exchange” between the tracklet frames."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"the second assesses the impact of reid on polyp
classiﬁcation accuracy. 3.1
reid standalone evaluation
dataset. we use 22,283 colonoscopy videos, split into training (21,737) and
test (546) sets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"2.
the tracking algorithm might produce short and uninformative tracklets as
well as outliers. the following clean up steps were performed on the training set:
we ﬁltered out tracklets shorter than 1 s or having less than 15 high conﬁdence
detections, as deﬁned in [27], and took only the longest tracklet from every
procedure. the thresholds were determined using analysis of the training set
tracklets distribution."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"grouping polyp frames into a tracklet, to be fed into the cadx, is usually
done by a spatio-temporal tracker [2]. longer tracklets provide more information
for polyp classiﬁcation. here, we investigate if the proposed reid model, used to group disjoint
tracklets of the same polyp, can increase the accuracy of cadx.
data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"finally, for reid, we merge disjoint tracklets by their appearance
using the reid model. by construction, tracklets generated by methods (2) and
(3) are subsets of the corresponding manually annotated gt tracklet, and are
assigned its polyp classiﬁcation label. a visualization of the resulting tracklets
using diﬀerent grouping methods is provided in supplementary fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"4. the num-
ber of resulting tracklets in the test set for each grouping method and polyp
labels distribution are summarized in table 3.
table 3. cadx test data distribution and fragmentation rate (fr). grouping
tracklets fr
adenoma adenoma fr non-adenoma non-adenoma fr
annotation
608
1.0
464
1.0
144
1.0
tracking
3161
5.20 2537
5.47
624
4.33
tracking+reid 1023
1.68 813
1.75
210
1.46
we ran the cadx model on tracklets generated by the 3 grouping methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_57.pdf,"the results are
summarized in table 4. the result on the manually annotated data is the accu-
racy upper-bound and is brought as a reference point. one can see that the
reid based approach signiﬁcantly improves the cadx accuracy compared to
the tracking-based grouping.
598
y. intrator et al.
table 4."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"firstly, we propose a multi-attention tri-branch net-
work (mtnet) that consists of an encoder and a three-branch decoder,
with each branch using a diﬀerent attention mechanism that calibrates
features in diﬀerent aspects to generate diverse outputs. secondly, we
introduce cross decoder knowledge distillation (cdkd) between the
three decoder branches, allowing them to learn from each other’s soft
labels to mitigate the negative impact of incorrect pseudo labels in train-
ing. additionally, uncertainty minimization is applied to the average
prediction of the three branches, which further regularizes predictions
on unlabeled images and encourages inter-branch consistency."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"the code is available at https://github.com/hilab-git/cdma. keywords: semi-supervised learning · knowledge distillation ·
attention · uncertainty
1
introduction
automatic segmentation of tumor lesions from pathological images plays an
important role in accurate diagnosis and quantitative evaluation of cancers. recently, deep learning has achieved remarkable performance in pathological
image segmentation when trained with a large and well-annotated dataset [6,13,
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"however, obtaining dense annotations for pathological images is challenging
and time-consuming, due to the extremely large image size (e.g., 10000 × 10000
pixels), scattered spatial distribution, and complex shape of lesions. semi-supervised learning (ssl) is a potential technique to reduce the anno-
tation cost via learning from a limited number of labeled data along with a large
amount of unlabeled data. existing ssl methods can be roughly divided into two
categories: consistency-based [9,14,23] and pseudo label-based [2] methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"[7] proposed
to encourage the predictions of auxiliary decoders and a main decoder to be
consistent under perturbed hierarchical features. pseudo label-based methods
typically generate pseudo labels for labeled images to supervise the network [4].
since using a model’s prediction to supervise itself may over-ﬁt its bias, chen
et al. net+ [19] utilized multiple decoders with diﬀerent upsampling strategies
to obtain slightly diﬀerent outputs, and each decoder’s probability output was
sharpened to serve as pseudo labels to supervise the others."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"unlike mc-net+ [19]
that is based on diﬀerent upsampling strategies, our mtnet uses diﬀerent atten-
tion mechanisms in three decoder branches that calibrate features in diﬀerent
aspects to obtain diverse and complementary outputs. secondly, inspired by the
observation that smoothed labels are more eﬀective for noise-robust learning
found in recent studies [10,22], we propose a cross decoder knowledge distil-
lation (cdkd) strategy to better leverage the diverse predictions of unlabeled
images. in cdkd, each branch serves as a teacher of the other two branches using
soft label supervision, which reduces the eﬀect of noise for more robust learn-
ing from inaccurate pseudo labels than argmax [2] and sharpening-based [19]
pseudo supervision in existing methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"in addition, we apply an
uncertainty minimization-based regularization to the average probability pre-
diction across the decoders, which not only increases the network’s conﬁdence,
but also improves the inter-decoder consistency for leveraging labeled images. the contribution of this work is three-fold: 1) a novel framework named
cdma based on mtnet is introduced for semi-supervised pathological image
segmentation, which leverages diﬀerent attention mechanisms for generating
diverse and complementary predictions for unlabeled images; 2) a cross decoder
knowledge distillation method is proposed for robust and eﬃcient learning from
noisy pseudo labels, which is combined with an average prediction-based uncer-
tainty minimization to improve the model’s performance; 3) experimental results
show that the proposed cdma outperforms eight state-of-the-art ssl methods
on the public digestpath dataset [3].
fig. 1. our cdma for semi-supervised segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"three decoder branches use
diﬀerent attentions to obtain diverse outputs. cross decoder knowledge distillation
(cdkd) is proposed to better deal with noisy pseudo labels, and an uncertainty min-
imization is applied to the average probability prediction of the three branches. lsup
is only for labeled images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"2
methods
as illustrated in fig. 1, the proposed cross distillation of multiple attentions
(cdma) framework for semi-supervised pathological image segmentation con-
sists of three core modules: 1) a tri-branch network mtnet that uses three
diﬀerent attention mechanisms to obtain diverse outputs, 2) a cross decoder
knowledge distillation (cdkd) module to reduce the eﬀect of noisy pseudo
labels based on soft supervision, and 3) an average prediction-based uncertainty
minimization loss to further regularize the predictions on unlabeled images. it can calibrate the feature maps for better performance by
paying more attention to the important spatial positions or channels with only a
few extra parameters."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"for an input image, the logit predictions obtained by the three branches are
denoted as zca, zsa and zcsa, respectively. after using a standard softmax
operation, their corresponding probability prediction maps are denoted as pca,
psa and pcsa, respectively.
574
l. zhong et al.
2.2
cross decoder knowledge distillation (cdkd)
since the three branches have diﬀerent decision boundaries, using the predictions
from one branch as pseudo labels to supervise the others would avoid each branch
over-ﬁtting its bias. however, as the predictions for unlabeled training images
are noisy and inaccurate, using hard or sharpened pseudo labels [2,19] would
strengthen the conﬁdence on incorrect predictions, leading the model to overﬁt
the noise [10,22]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"kl( ˜pcsa, ˜psa)
(4)
where kl() is the kullback-leibler divergence function. note that the gradient
of lcsa
kd
is only back-propagated to the csa branch, so that the knowledge is
distilled from the teachers to the student. similarly, the kd losses for the ca
and sa branches are denoted as lca
kd
and lsa
kd , respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"i is the average probability
for class c at pixel i. note that when lum for a pixel is close to zero, the average
probability for class c of that pixel is close to 0.0 (1.0), which drives all the
decoders to predict it as 0.0 (1.0) and encourages inter-decoder consistency. finally, the overall loss function for our cdma is:
l = lsup + λ1lcdkd + λ2lum
(7)
where lsup = (lcsa
sup
+ lca
sup + lsa
sup)/3 is the average supervised learning loss
for the three branches on the labeled training images, and the supervised loss
for each branch calculates the dice loss and cross entropy loss between the
probability prediction (pcsa, pca and psa) and the ground truth label. λ1 and
λ2 are the weights of lcdkd and lum respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"the green regions are lesions. 3
experiments and results
dataset and implementation details. we used the public digestpath data-
set [3] for binary segmentation of colonoscopy tumor lesions from whole slide
images (wsi) in the experiment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"the
batch size was 16 (8 labeled and 8 unlabeled patches). for data augmentation,
we adopted random ﬂipping, random rotation, and random gaussian noise. for
inference, only the csa branch was used due to the similar performance of the
three branches after converge and the increased inference time of their ensemble,
and no post-processing was used."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"dice similarity coeﬃcient (dsc) and jaccard
index (ji) were used for quantitative evaluation. table 1. comparison between diﬀerent ssl methods on the digestpath dataset. ∗
denotes p-value < 0.05 (signiﬁcance level) when comparing the proposed cdma with
the others under t-test hypothesis testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_55.pdf,"it obtained an average dsc of 65.02% and 68.61% under the two annotation
ratios respectively. the proposed lcdkd was compared with two variants: lcdkd
(argmax) and lcdkd (t=1) that represent using hard pseudo labels and standard
probability output obtained by softmax for cdkd respectively. table 2 shows
that our lcdkd obtained an average dsc of 68.84% and 71.49% under the two
annotation ratios respectively, and it outperformed lcdkd (argmax) and lcdkd
(t=1), demonstrating that our cdkd based on softened probability prediction
is more eﬀective in dealing with noisy pseudo labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"breast lesion segmentation in ultrasound (us) videos is essen-
tial for diagnosing and treating axillary lymph node metastasis. however,
the lack of a well-established and large-scale ultrasound video dataset
with high-quality annotations has posed a persistent challenge for the
research community. to overcome this issue, we meticulously curated a
us video breast lesion segmentation dataset comprising 572 videos and
34,300 annotated frames, covering a wide range of realistic clinical sce-
narios."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"we also devise a localization-based
contrastive loss to reduce the lesion location distance between neighbor-
ing video frames within the same video and enlarge the location distances
between frames from diﬀerent ultrasound videos. our experiments on
our annotated dataset and two public video polyp segmentation datasets
demonstrate that our proposed fla-net achieves state-of-the-art perfor-
mance in breast lesion segmentation in us videos and video polyp segmen-
tation while signiﬁcantly reducing time and space complexity. our model
and dataset are available at https://github.com/jhl-det/fla-net."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"however,
this task is challenging due to several factors, including blurry lesion boundaries,
inhomogeneous distributions, diverse motion patterns, and dynamic changes in
lesion sizes over time [12].
table 1. statistics of existing breast lesion us videos datasets and the proposed
dataset. #videos: numbers of videos."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"bbox: whether provide segmentation mask
annotation. bm: whether provide lesion classiﬁcation label (benign or malignant). pa:
whether provide axillary lymph node (aln) metastasis label (presence or absence)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"dataset
year # videos # af
bbox mask bm pa
li et al. [12] 2022 188
25,272
✓
×
✓
×
ours
2023 572
34,300 ✓
✓
✓
✓
the work presented in [10] proposed the ﬁrst pixel-wise annotated benchmark
dataset for breast lesion segmentation in us videos, but it has some limitations. although their eﬀorts were commendable, this dataset is private and contains
only 63 videos with 4,619 annotated frames."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"the small dataset size increases
the risk of overﬁtting and limits the generalizability capability. in this work, we
collected a larger-scale us video breast lesion segmentation dataset
with 572 videos and 34,300 annotated frames, of which 222 videos contain aln
metastasis, covering a wide range of realistic clinical scenarios. please refer to
table 1 for a detailed comparison between our dataset and existing datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"although the existing benchmark method dpstt [10] has shown promis-
ing results for breast lesion segmentation in us videos, it only uses the ultra-
sound image to read memory for learning temporal features. however, ultrasound
images suﬀer from speckle noise, weak boundaries, and low image quality. thus,
there is still considerable room for improvement in ultrasound video breast lesion
segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"additionally, we devise a contrastive loss to enhance the breast lesion location
shifting more attention to breast lesion segmentation in ultrasound videos
499
fig. 1. examples of our ultrasound video dataset for breast lesion segmentation. similarity of video frames within the same ultrasound video and to prohibit loca-
tion similarity of diﬀerent ultrasound videos."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"the experimental results unequiv-
ocally showcase that our network surpasses state-of-the-art techniques in the
realm of both breast lesion segmentation in us videos and two video polyp
segmentation benchmark datasets (fig. 1). 2
ultrasound video breast lesion segmentation dataset
to support advancements in breast lesion segmentation and aln metastasis
prediction, we collected a dataset containing 572 breast lesion ultrasound videos
with 34,300 annotated frames. table 1 summarizes the statistics of existing breast
lesion us video datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"nine experienced pathologists were invited to manually annotate breast lesions
at each video frame. unlike previous datasets [10,12], our dataset has a reserved
validation set to avoid model overﬁtting. the entire dataset is partitioned into
training, validation, and test sets in a proportion of 4:2:4, yielding a total of
230 training videos, 112 validation videos, and 230 test videos for comprehensive
benchmarking purposes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"moreover, apart from the segmentation annotation, our
dataset also includes lesion bounding box labels, which enables benchmarking
breast lesion detection in ultrasound videos. more dataset statistics are available
in the supplementary. 3
proposed method
figure 2 provides a detailed illustration of the proposed frequency and localiza-
tion feature aggregation network (fla-net)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"2, our ffa block takes three features (ft ∈ rc×h×w, ft−1 ∈ rc×h×w, and
ft−2 ∈ rc×h×w) as input. to integrate the three input features and extract rel-
evant information while suppressing irrelevant information, our ffa block ﬁrst
employs a fast fourier transform (fft) to transform the three input features
into the spectral domain, resulting in three corresponding spectral domain fea-
tures ( ˆft ∈ cc×h×w, ˆft−1 ∈ cc×h×w, and ˆft−2 ∈ cc×h×w), which capture the
frequency information of the input features. note that the current spectral fea-
tures ( ˆft, ˆft−1, and ˆft−2) are complex numbers and incompatible with the neural
shifting more attention to breast lesion segmentation in ultrasound videos
501
layers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"therefore we concatenate the real and imaginary parts of these com-
plex numbers along the channel dimension respectively and thus obtain three
new tensors (xt ∈ r2c×h×w, xt−1 ∈ r2c×h×w, and xt−2 ∈ r2c×h×w) with dou-
ble channels. afterward, we take the current frame spectral-domain features
xt as the core and fuse the spatial-temporal information from the two auxil-
iary spectral-domain features (xt−1 and xt−2), respectively. speciﬁcally, we ﬁrst
group three features into two groups ({xt, xt−1} and {xt, xt−2}) and develop
a channel attention function ca(·) to obtain two attention maps."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"we empirically set weights λ1 = λ2 = λ3 = 1.
4
experiments and results
implementation details. [6] on the imagenet dataset, while the remaining components
of our network were trained from scratch. prior to inputting the training video
frames into the network, we resize them to 352×352 dimensions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"to ensure a fair and equitable compar-
ison, we acquire the segmentation results of all nine compared methods by utiliz-
ing either their publicly available implementations or by implementing them our-
selves. additionally, we retrain these networks on our dataset and ﬁne-tune their
network parameters to attain their optimal segmentation performance, enabling
accurate and meaningful comparisons. the quantitative results of our network and the
nine compared breast lesion segmentation methods are summarized in table 2.
analysis of the results reveals that, in terms of quantitative metrics, video-based
methods generally outperform image-based methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"speciﬁcally, our fla-net improves the dice score from 0.762 to
0.789, the jaccard score from 0.659 to 0.687, the f1-score result from 0.799 to
0.815, and the mae score from 0.036 to 0.033.
504
j. lin et al.
table 4. quantitative comparison results on diﬀerent video polyp segmentation
datasets. for more quantitative results please refer to the supplementary material."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"4.3
generalizability of our network
to further evaluate the eﬀectiveness of our fla-net, we extend its application
to the task of video polyp segmentation. [2] and cvc-612-v [3]. table 4 showcases the dice, iou, sα, eφ, and
mae results achieved by our network in comparison to state-of-the-art methods
on these two datasets. our method demonstrates clear superiority over state-of-
the-art methods in terms of dice, iou, eφ, and mae on both the cvc-300-tv
and cvc-612-v datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_48.pdf,"speciﬁcally, our method enhances the dice score from
0.840 to 0.874, the iou score from 0.745 to 0.789, the eφ score from 0.921 to
0.969, and reduces the mae score from 0.013 to 0.010 for the cvc-300-tv
dataset. similarly, for the cvc-612-v dataset, our method achieves improve-
ments of 0.012, 0.014, 0.019, and 0 in dice, iou, eφ, and mae scores, respec-
tively. although our sα results (0.907 on cvc-300-tv and 0.920 on cvc-612-v)
take the 2nd rank, they are very close to the best sα results, which are 0.909
on cvc-300-tv and 0.923 on cvc-612-v. hence, the superior metric results
obtained by our network clearly demonstrate its ability to accurately segment
polyp regions more eﬀectively than state-of-the-art video polyp segmentation
methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf,"the network ﬁrst utilizes the pseudo-masks generated using the extreme
points to train itself, by minimizing a contrastive loss, which encourages
the network to learn more representative features for cancerous voxels. then the trained network ﬁne-tunes itself by using a similarity-aware
propagation learning (simple) strategy, which leverages feature similar-
ity between unlabeled and positive voxels to propagate labels. finally the
network retrains itself by employing the pseudo-masks generated using
previous ﬁne-tuned network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf,"early studies focused on image
processing based approaches by conducting graph-cut segmentation [29] or ana-
lyzing low-level hand-crafted features [1,11,19]. these methods may encounter
the issue of high computational complexity when analyzing volumetric data,
and most of them require manual interactions. recently, deep-learning-based
methods have been applied to analyze breast mri."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf,"how-
ever, the geometric prior used in [6] can not be an appropriate strategy for the
segmentation of lesions with various shapes. to our knowledge, currently only
one weakly-supervised work [18] has been proposed for breast mass segmentation
in dce-mri. this method employed three partial annotation methods including
single-slice, orthogonal-slice (i.e., 3 slices) and interval-slice (∼6 slices) to allevi-
ate the annotation cost, and then constrained segmentation by estimated volume
using the partial annotation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf,"the ﬁne-tune
is conducted by using a similarity-aware propagation learning (simple) strat-
egy to update the pseudo-masks for the subsequent retrain. we evaluate our
method on a collected dce-mri dataset containing 206 subjects. experimental
results show our method achieves competitive performance compared with fully
supervision, demonstrating the eﬃcacy of the proposed simple strategy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf,"let f and θ be network and its
parameters, respectively. (1)
moreover, supervised contrastive learning is employed to encourage voxels
of the same label to gather around in feature space. it ensures the network to
learn discriminative features for each category."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf,"we propose to ﬁne-tune the entire network using the pre-trained
weights as initialization. the ﬁne-tune follows the simple strategy which eval-
uates the similarity between unlabeled voxels and positive voxels to propagate
labels to unlabeled voxels. speciﬁcally, n positive voxels are randomly sampled
as the referring voxel."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf,"then the network is ﬁne-tuned using the partial cross
entropy loss same as in the initial train stage. the loss function lfinetune is
formulated as:
lfinetune = lpce − w ·

s(k)>αn
log(f(x; θ)(k)),
(5)
572
y. zhong and y. wang
where w is the weighting coeﬃcient that controls the inﬂuence of the pseudo
labels. to reduce the inﬂuence of possible incorrect label propagation, pseudo
labels for unlabeled voxels are valid only for the current iteration when they are
generated."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf,"after the ﬁne-tune completed, the network generates binary pseudo-masks
for every training data, which are expected to be similar to the ground-truths
provided by radiologists. finally the network is retrained from random initial-
ization by minimizing the cross entropy loss with the binary pseudo-masks.
3
experiments
dataset. we evaluated our method on an in-house breast dce-mri dataset
collected from the cancer center of sun yat-sen university."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf,"all cancerous regions and extreme points were manually annotated by an
experienced radiologist via itk-snap [26] and further conﬁrmed by another
radiologist. we randomly divided the dataset into 21 scans for training and the
remaining scans for testing1. before training, we resampled all volumes into the
same target spacing 0.600×0.600×1.000 mm3 and normalized all volumes as zero
mean and unit variance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_54.pdf,"the ploy learning policy was also
used. for the simple strategy, we set n = 100, λ = 0.96, α = 0.96, w = 0.1.
1 we have tried diﬀerent amount of training data to investigate the segmentation
performance of the fully-supervised network. the results showed that when using 21,
42, 63 scans for training, the dice results changed very little, within 0.3%."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"the proposed method can capture the local
coherence of adjacent images by optical ﬂow, which yields signiﬁcant
improvements in the precision and stability of the constructed images. we evaluate our proposed method on real datasets and the experimental
results suggest that it can outperform existing state-of-the-art recon-
struction approaches signiﬁcantly. keywords: ct reconstruction · low-dose · generative adversarial
networks · local coherence · optical ﬂow
1
introduction
computed tomography (ct) is one of the most widely used technologies in
medical imaging, which can assist doctors for diagnosing the lesions in human
internal organs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"due to harmful radiation exposure of standard-dose ct, the low
dose ct is more preferable in clinical application [4,6,34]. however, when the
dose is low together with the issues like sparse-view or limited angles, it becomes
quite challenging to reconstruct high-quality ct images. the high-quality ct
images are important to improve the performance of diagnosis in clinic [27]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"https://doi.org/10.1007/978-3-031-43999-5_50
solving low-dose ct reconstruction via gan with local coherence
525
where xr ∈ rd denotes the unknown ground-truth picture, y ∈ rm denotes
the received measurement, and δ is the noise. the problem of ct reconstruction is to recover
xr from the received y.
solving the inverse problem of (1) is often very challenging if there is no
any additional information. if the forward operator t is well-posed and δ is
neglectable, we know that an approximate xr can be easily obtained by directly
computing t −1(y)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"[11] can produce serious detrimental artifact. therefore, most of existing
approaches usually incorporate some prior knowledge during the reconstruc-
tion [14,17,26]. ∥p denotes the p-norm and r(x) denotes the penalty item from some
prior knowledge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"the idea of optical ﬂow has also
been used for tracking the organs movement in medical imaging [16,20,33]. how-
ever, to the best of our knowledge, there is no work considering gans with using
optical ﬂow to capture neighbor slices coherence for low dose 3d ct reconstruc-
tion. 526
w. liu and h. ding
in this paper, we propose a novel optical ﬂow based generative adversarial
network for 3d ct reconstruction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"we introduce the “local coherence” by characterizing the correlation of con-
secutive ct images, which plays a key role for suppressing the artifacts. 2. together with the local coherence, our proposed generative adversarial net-
works (gans) can yield signiﬁcant improvement for texture quality and sta-
bility of the reconstructed images. 3. to illustrate the eﬃciency of our proposed approach, we conduct rigorous
experiments on several real clinical datasets; the experimental results reveal
the advantages of our approach over several state-of-the-art ct reconstruc-
tion methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"i ≤ n, we want to reconstruct its ground truth image xr
i as the eq. before
performing the reconstruction in the generator g, we apply some prior knowledge
in physics and run ﬁlter backward projection on the measurement yi in eq. (1)
to obtain an initial recovery solution si. usually si contains signiﬁcant noise com-
paring with the ground truth xr
i ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"the flownet is
an autoencoder architecture with extraction of features of two input frames to
learn the corresponding ﬂow, which is consist of 6 (de)convolutional layers for
both encoder and decoder.
discriminator. the discriminator d assigns the label “1” to real standard-
dose ct images and “0” to generated images. the goal of d is to maximize the
separation between the distributions of real images and generated images:
1 if i = 1, n(si) = {s2}; if i = n, n(si) = {sn−1}."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"the discriminator includes 3 residual blocks, with 4 convolutional
layers in each residual block.
generator. we use the generator g to reconstruct the high-quality ct image
for the ground truth xr
i from the low-dose image si. the generated image is
obtained by
xg
i = g(si, w(n(xg
i )));
n(xg
i ) = g(n(si)),
(6)
where w(·) is the warping operator."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"lpixel and ladv are designed to recover global structure, and lpercept is
utilized to incorporate additional texture details into the reconstruction process. 530
w. liu and h. ding
4
experiment
datasets. first, our proposed approaches are evaluated on the “mayo-clinic
low-dose ct grand challenge” (mayo-clinic) dataset of lung ct images [19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"[36] for testing. we randomly select 4
patients with 1827 slices from the dataset. the simulation process is identical to
that of mayo-clinic."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"for
both two measures, the higher the better.
results. table 1 presents the results on the mayo-clinic dataset, where the ﬁrst
row represents diﬀerent parameter settings (i.e., the number of uniform views nv,
the number of detectors nd and the standard deviation of gaussian noise σ) for
simulating low-dose sinograms. our proposed approach gan-lc consistently
outperforms the baselines under almost all the low-dose parameter settings."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"fbpconvnet has
solving low-dose ct reconstruction via gan with local coherence
531
table 1. experimental results for mayo-clinic dataset. the value in ﬁrst row of the
table represents nv, nd and σ for simulating low-dose sinograms, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"the value in ﬁrst row of the table
represents nv, nd and σ for simulating low-dose sinograms, respectively. to evaluate the stability and generalization of our
model and the baselines trained on mayo-clinic dataset, we also test them on
the rider dataset. the results are shown in table 2."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"to illustrate the reconstruction performances more clearly, we also show the
reconstruction results for testing images in fig. 3. we can see that our network
can reconstruct the ct image with higher quality. due to the space limit, the
experimental results of diﬀerent views nv and more visualized results are placed
in our supplementary material."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"532
w. liu and h. ding
fig. 3. reconstruction results on mayo-clinic dataset. the sparse view setting of sino-
grams is nv = 200, nd = 400 and σ = 2.0."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_50.pdf,"by considering the inherent
continuity of human body, local coherence can be captured through optical ﬂow,
which is small deformations and structural diﬀerences between consecutive ct
slices. the experimental results on real datasets demonstrate the advantages
of our proposed network over several popular approaches. in future, we will
evaluate our network on real-world ct images from local hospital and use the
reconstructed images to support doctors for the diagnosis and recognition of lung
nodules."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf,"our model can avoid the challenging issues
in mainstream networks, such as the mode collapses in gans or align-
ment between posterior distributions in aes. in conclusion, staindiff
suﬃces to increase the stain style transfer quality, where the training is
straightforward and the model is simpliﬁed for real-world clinical deploy-
ment. keywords: diﬀusion probabilistic model · histology stain transfer
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf,"(b) we specify the consistency cycles to impose the regular-
ization. parameterized by the markov chain, the forward process
in staindiff follows the vanilla ddpm by perturbing the histology images
gradually with gaussian noise, until all structures and morphological context
information are lost. formally, given a histology image xa
0 with respect to the
stain style domain a, a transition kernel q progressively generates a sequence of
t latent variables xa
1 , xa
2 , · · · , xa
t thorough the following equation:
q(xa
t |xa
t−1) = n(xa
t ;

1 − βtxa
t−1, βti),
(1)
where n(·) denotes the gaussian distribution, i is the identity matrix."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf,"(1) to
derive xa
s . choosing the optimal value for s is important, as a large s (e.g., s = t)
leads to the loss of the contextual and structural information; while a small
valued s (e.g., s = 1) fails to inject suﬃcient noise for staindiff to transfer
style. an ideal range for s is a small subset from [1, t] that is centered by 1
2t.
consequently, in this work, we ﬁx s in the range of [s1, s2] with s1 = 2
5t and
s2 = 3
5t. afterwards, the latent variable xa
s is transferred into the corresponding
latent space with respect to stain style b with auxiliary transform network gb,
which gives us ˜xb
s = gb(xa
s )."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf,"this modiﬁcation allows us
to use staindiff for stain normalization without any other adjustments to the
inference process. 3
experiments
datasets. evaluations of staindiff are conducted on two datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf,"(1)
dataset-a: mitos-atypia 14 challenge1. this dataset aims to measure the
style transfer performance on 284 histology frames. each slide is digitized by
two diﬀerent scanners, resulting in stain style variations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf,"[33] as the evaluation metrics. (2) dataset-b: the cancer genome atlas
(tcga). all experiments are implemented in python 3.8.13 with
pytorch 1.12.1 on two nvidia geforce rtx 3090 gpu cards with 24gib of
1 https://mitos-atypia-14.grand-challenge.org."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf,"the superiority of our diﬀusion model to
gans and aes in histology stain style transfer is quantitatively reﬂected in
table 1. speciﬁcally, on dataset-a, staindiff can surpass its counterparts with
a large margin regarding all three metrics. notably, staindiff achieves the
highest ssim of 0.717 and fsim of 0.753, which improves the state-of-the-art
cl-staingan [15] by 0.016 and 0.019 respectively, without reliance on time-
costly self-supervised pre-training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf,"4.
evaluations on stain normalization. table 2 presents the comparison
results of the downstream classiﬁcation task, where the histology images in
dataset-b are normalized using diﬀerent methods. consequently, it yields the
superiority of staindiff in terms of stain normalization is model-agnostic.
ablation study."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_53.pdf,"table 1 and 2 show that incorporating a self-ensemble scheme
can both boost the performance of staindiff, and bring down the variations,
demonstrating its eﬀectiveness in stabilizing the stain transfer and normaliza-
tion. to further investigate the eﬀect of ensemble number m, we conduct ablation
on dataset-a. experimentally, the fsim when m = 1, 5, 10, 15, 20, 50 are 0.742,
0.749, 0.753, 0.756, 0.759, 0.759 respectively. while a slight performance gain
can be achieved with higher m values than 10, the ensemble becomes more time-
consuming, as the cost time is linear to m. it implies an optimal m should be
selected as a trade-oﬀ between the performance and computational time, such
as 10 in this work."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf,"third, structuregnet handles out-of-plane deformation without
requiring any 3d reconstruction thanks to a recursive plane selection. we evaluate the quantitative performance of structuregnet for head
and neck cancer between 3d ct scans and 2d histopathological slides,
enabling pixel-wise mapping of low-quality radiologic imaging to gold-
standard tumor extent and bringing biological insights toward homoge-
nized clinical guidelines. additionally, our method can be used in radia-
tion therapy by mapping 3d planning ct into the 2d mr frame of the
treatment day for accurate positioning and dose delivery."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf,"keywords: multimodal · registration · 2d-3d · histopathology ·
radiology
1
introduction
2d-3d registration refers to the highly challenging process of aligning an input 2d
image to its corresponding slice inside a given 3d volume [4]. it has received growing
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5 73.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14229, pp. https://doi.org/10.1007/978-3-031-43999-5_73
772
a. leroy et al.
attention in medical imaging due to the various contexts where it applies, like image
fusion between 2d real-time acquisitions and either pre-operative 3d images for
guided interventions or reference planning volumes for patient positioning in radi-
ation therapy (rt)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf,"onepromisingsolutionistorelyonrigidstructuresthataresupposedlymorerobust
duringthepreparation.structuralinformationtoguideimageregistrationhasbeen
studiedwiththehelpofsegmentationsintothetrainingloop[11],orbylearningnew
image representations for reﬁned mapping [12].
in this paper, we propose to leverage the structural features of tissue and
more particularly the rigid areas to guide the registration process with two dis-
tinct contributions: (1) a cascaded rigid alignment driven by stiﬀ regions and
coupled with recursive plane selection, and (2) an improved 2d/3d deformable
motion model with distance ﬁeld regularization to handle out-of-plane deforma-
tion. to our knowledge, no previous study proposed 2d/3d registration com-
bined with structure awareness. we also use the cyclegan for image translation
and direct monomodal signal comparison [25]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf,"because soft tissues undergo too large out-of-plane deformations, we
leverage the rigid structures which are supposed not to be distorted or shrunk
during the histological process. we extract their segmentation masks mct, mh
for both modalities (see preprocessing in sect. 3), concatenate and fed them into
an encoder followed by a fully connected layer that outputs six transformation
parameters (3 rotations, 3 translations)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf,"(3)
finally, we add two sources of regularization. soft tissues away from bones
and cartilage are more subject to shrinkage or disruption, so we harness the
information from the cartilage segmentation mask of ct to generate a distance
transform map δ deﬁned as δ(v) = minm∈mct ||v−m||2. it maps each voxel v
of ct to its distance with the closest point m to the rigid area mct ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf,"776
a. leroy et al.
fig. 3. deformable 2d-3d registration pipeline, made of two encoders and a shared
decoder, with regularization applied on the displacement ﬁeld φ thanks to the distance
map from ct.
3
experiments
dataset and preprocessing. our clinical dataset consists of 108 patients for
whom were acquired both a pre-operative h&n ct scan and 4 to 11 wsis
after laryngectomy (with a total amount of 849 wsis)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf,"we ended up with images of size 256×256
(×64 for 3d ct) of 1 mm isotropic grid space. we split the dataset patient-wise
into three groups for training (64), validation (20), and testing (24). to demon-
strate the performance of our model on another application, we also retrieved
the datasets from [14] for pelvis 3d ct/2d mr."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf,"finally, to diﬀerentiate
the latter contributions, we tested two ablation studies: without the cascaded
rigid mapping or without the distance ﬁeld control. according to the mr/ct
application in rt, we compared our model against the state-of-the-art results of
msv-regsynnet which were computed on the same dataset. 4
results
4.1
modality translation
three samples from the test set are displayed in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_73.pdf,"concerning the gpu
runtime, with a 3-step cascade for initialization, the inference remains in a sim-
ilar time scale to baseline methods and performs mapping in less than 3s. we
also compared against msv-regsynnet on its own validation dataset for gener-
alization assessment: we yielded comparable results for the ﬁrst cohort and sig-
niﬁcantly better ones for the second, which proves that structuregnet behaves
well on other modalities and that the structure awareness is an essential asset for
better registration, as pelvis is a location where organs are moving. visuals of
registration results are displayed in the supplementary material."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"comprehensive experiments on histopathology image segmenta-
tion demonstrate that sdt achieves state-of-the-art performance. 1
introduction
instances with complex shapes arise in many biomedical domains, and their
morphology carries critical information. for example, the structure of gland
tissues in microscopy images is essential in accessing the pathological stages for
cancer diagnosis and treatment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"these instances, however, are usually closely
in touch with each other and have non-convex structures with parts of varying
widths (fig. 1a), posing signiﬁcant challenges for existing segmentation methods. in the biomedical domain, most methods [3,4,13,14,22] ﬁrst learns interme-
diate representations and then convert them into masks with standard segmen-
tation algorithms like connected-component labeling and watershed transform. these representations are not only eﬃcient to predict in one model forward
pass but also able to capture object geometry (i.e., precise instance bound-
ary), which are hard for top-down methods using low-resolution features for
z. lin—currently aﬃliated with amazon alexa."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"a fcn maps the
image into the energy space to minimize the loss. (b) inference phase: we threshold the
sdt to generate skeleton segments, which is processed into seeds with the connected
component labeling. finally, the watershed transform algorithm takes the seeds and
the reversed sdt energy to yield the masks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"[5] with a resnet [6] backbone
to directly learn the sdt energy without additional targets (fig. 3, training
phase). we also add a coordconv [10] layer before the 3rd stage in the backbone
network to introduce spatial information into the segmentation model. target sdt generation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"there is an inconsistency problem in object skeleton
generation: part of the complete instance skeleton can be diﬀerent from the
skeleton of the instance part (fig. 4). some objects may touch the image border
due to either a restricted ﬁeld of view (fov) of the imaging devices or spatial data
augmentation like the random crop. if pre-computing the skeleton, we will get
local skeleton (fig. 4c) for objects with missing masks due to imaging restrictions,
and partial skeleton (fig. 4b) due to spatial data augmentation, which causes
ambiguity."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"instance extraction from sdt. in the sdt energy map, all boundary pix-
els share the same energy value and can be processed into segments by direct
thresholding and connected component labeling, similar to dwt [1]. however,
since the prediction is never perfect, the energy values along closely touching
boundaries are usually not sharp and cause split-errors when applying a higher
threshold or merge-errors when applying a lower threshold.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"so that all pixels with
the predicted energy bigger than θ are labeled as skeleton pixels. we ﬁrst perform
connected component labeling of the skeleton pixels to generate seeds and run the
watershed algorithm on the reversed energy map using the seeds as basins (local
optima) to generate the ﬁnal segmentation. we also follow previous works [4,22]
and reﬁne the segmentation by hole-ﬁlling and removing small spurious objects."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"the diversity of object
appearance, size, and shape makes the task challenging. skeleton-aware distance transform
535
dataset and evaluation metric. [17] that contains colored light microscopy images of tissues with a
wide range of histological levels from benign to malignant."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"for the instance-level f1 score, an iou threshold of 0.5 is used to
decide the correctness of a prediction.
methods in comparison. [20], which use
multiple fcn models to select informative training samples from the dataset. with the same training settings as our sdt, we also report the performance of
skeleton with scales (ss) and traditional distance transform (dt)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"[22], our sdt unambiguously separates closely
touching objects while preserving the structure of complicated masks. [13], our model infers the sdt energy of instance masks from
a global structure perspective instead of boundary that relies on relatively local pre-
dictions, which produces high-quality masks.
training and inference. since the training data is relatively limited due to
the challenges in collecting medical images, we apply pixel-level and spatial-level
augmentations, including random brightness, contrast, rotation, crop, and elastic
transformation, to alleviate overﬁtting."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"the same settings are applied to dt. at
inference time, we apply argmax to get the corresponding bin index of each pixel
and transform the energy value to the original data range. finally, we apply the
watershed-based instance extraction rule described in sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_51.pdf,"we
do not quantize the scales as dt and sdt since even ground-truth scales can
yield masks unaligned with the instance boundary with quantization.
results. our sdt framework achieves state-of-the-art performance on 5 out of
6 evaluation metrics on the gland segmentation dataset (table 1). with the bet-
ter distinguishability of object interior and boundary, sdt can unambiguously
separate closely touching instances (fig. 5, ﬁrst two rows), performs better than
previous methods using object boundary representations [4,22]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf,"medical image synthesis is a challenging task due to the
scarcity of paired data. several methods have applied cyclegan to lever-
age unpaired data, but they often generate inaccurate mappings that shift
theanatomy.thisproblemisfurtherexacerbatedwhentheimagesfromthe
sourceandtargetmodalitiesareheavilymisaligned.recently,currentmeth-
ods have aimed to address this issue by incorporating a supplementary seg-
mentation network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf,"keywords: unpaired ct synthesis · structural consistency
1
introduction
magnetic resonance imaging (mri) and computed tomography (ct) are two
commonly used cross-sectional medical imaging techniques. mri and ct pro-
duce diﬀerent tissue contrast and are often used in tandem to provide comple-
mentary information. while mri is useful for visualizing soft tissues (e.g. muscle,
acknowledgement: this study was supported by channel 7 children’s research foun-
dation of south australia incorporated (crf)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf,"https://doi.org/10.1007/978-3-031-43999-5_6
structure-preserving synthesis: maskgan for unpaired mr-ct translation
57
fig. 1. visual results (row 1) and the error map (row 2) between the ground-truth
and synthetic ct on pediatric dataset. (a) input mri and the paired ct."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf,"unfortunately, ct imaging
exposes patients to ionizing radiation, which can damage dna and increase
cancer risk [9], especially in children and adolescents. given these issues, there
are clear advantages for synthesizing anatomically accurate ct data from mri. despite the superior perfor-
mance, supervised methods require a large amount of paired data, which is
prohibitively expensive to acquire."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf,"due to
the lack of direct constraints on the synthetic outputs, cyclegan [20] struggles
to preserve the anatomical structure when synthesizing ct images, as shown in
fig. the structural distortion in synthetic results exacerbates when data
from the two modalities are heavily misaligned, which usually occurs in pediatric
scanning due to the rapid growth in children. recent unsupervised methods impose structural constraints on the synthe-
sized ct through pixel-wise or shape-wise consistency."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf,"moreover, in contrast to previous meth-
ods that train a separate shape extractor, our maskgan uses a shared encoder
for mask and content generators, as illustrated in fig. 2. our design embeds
the extracted shape knowledge into the content generator, thus improving the
structural consistency of the synthetic contents. cycle shape consistency loss."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf,"(5)
the ﬁnal loss for maskgan is the sum of three loss objectives weighted by the
corresponding loss coeﬃcients: l = lgan + λmasklmask + λshapelshape. 3
experimental results
3.1
experimental settings
data collection. we collected 270 volumetric t1-weighted mri and 267 thin-
slice ct head scans with bony reconstruction performed in pediatric patients
under routine scanning protocols1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf,"we then
resampled the volumetric scans to the same resolution of 1.0 × 1.0 × 1.0 mm3. the dataset comprises brain mr and ct volumes from 262 subjects. 13 mri-
ct volumes from the same patients that were captured less than three months
apart are registered using rigid registration algorithms."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf,"in contrast, our proposed maskgan pre-
serves shape-wise consistency and produces the smoothest synthetic ct. [4,14], pediatric datasets are easily misaligned due to children’s
rapid growth between scans. under this challenging setting, unpaired image syn-
thesis can have non-optimal visual results and ssim scores."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_6.pdf,"we compare the performance
of our approach with shape-cyclegan [4] using deformed masks that simulate
human errors during annotation. to alter object shapes, we employ random
elastic deformation, a standard data augmentation technique [10] that applies
random displacement vectors to objects. the level of distortion is controlled
by the standard deviation of the normal distribution from which the vectors
are sampled."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"multiple instance learning is an ideal mode of analysis for
histopathology data, where vast whole slide images are typically anno-
tated with a single global label. in such cases, a whole slide image is
modelled as a collection of tissue patches to be aggregated and classi-
ﬁed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"patient selection for such
l. fillioux and j. boyd—these authors contributed equally to this work. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 57.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43907-0_57
state space models in digital pathology
595
treatment regimes is based principally on the assessment of tissue biopsies and
the characterisation of the tumor microenvironment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"state space models are designed to eﬃciently model long sequences, such as
the sequences of patches that arise in wsi mil. in this paper, we present the
ﬁrst use of state space models for wsi mil. extensive experiments on three
publicly available datasets show the potential of such models for the processing
of gigapixel-sized images, under both weakly and multi-task schemes. moreover,
comparisons with other commonly used mil schemes highlight their robust per-
formance, while we demonstrate empirically the superiority of state space models
in processing the longest of wsi sequences with respect to commonly used mil
methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"a max pooling layer
merges the ssm layer outputs into a single vector, which is projected by a ﬁnal
linear layer and softmax to give the class probabilities ˆy. the model is trained
according to,
lmil = − 1
m
m

m=1
log ˆycm,
(6)
where ˆycm denotes the probability corresponding to cm, the slide-level label of
the sequence corresponding to the mth of m whole slide images. 3.3
multitask training
one advantage of processing an entire slide as a sequence is the ease with
which additional supervision may be incorporated, when available."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"a patch-level
ground truth creates the opportunity for multitask learning, which can enhance
the representations learned for slide-level classiﬁcation. as an extension of our
base model in eq. 6, we train a multitask model to jointly predict a slide-level
and patch-level labels. prior to the max pooling layer of the base model, an
additional linear layer is applied to each sequence token, yielding l additional
model outputs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"our implementation is publicly available2. [16] is a dataset that consists of resections of lymph nodes,
where each wsi is annotated with a binary label indicating the presence of
tumour tissue in the slide, and all slides containing tumors have a pixel-level
annotation indicating the metastatic region. in multitask experiments, we use
this annotation to give each patch a label indicating local tumour presence."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"in our experiments, the average patch sequence length arising from
camelyon16 is 6129 (ranging from 127 to 27444). tcga-luad is a tcga lung adenocarcinoma dataset that contains 541
wsis along with genetic information about each patient. as a mil task, we chose the
task of predicting the patient mutation status of tp53, a tumor suppressor gene
that is highly relevant in oncology studies."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"the average sequence length is 10557
(ranging from 85 to 34560). tcga-rcc is a tcga dataset for three kidney cancer subtypes (denoted
kich, kirc, and kirp). it consists of 936 wsis (121 kich, 518 kirc, and
297 kirp)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"4.2
results
multiple instance learning results. we evaluate our method on each
dataset by accuracy and area under receiver operating characteristic curve
(auroc). for multiclass classiﬁcation, these were computed in a one-versus-rest
manner."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"table 1 summarises the comparison between our proposed model and base-
lines. for the camelyon16 dataset, our method performs on par with trans-
mil and the clam models, while it clearly outperforms the other methods. similarly, in the tcga-luad dataset the proposed model achieves comparable
performance with both clam models, while outperforming transmil and the
other methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"we note that tcga-luad proves to be a more challenging
2 https://github.com/mics-lab/s4 digital pathology. 600
l. fillioux et al.
dataset for all models. moreover, our method outperforms clam models on the
tcga-rcc dataset, while reporting very similar performance with respect to
transmil."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"for our proposed method, we report both models with the
diﬀerent state dimensions (ours (ssm32)) and (ours (ssm128)). compared
table 1. comparison of accuracy and auroc on three datasets camelyon16,
tcga-luad, tcga-rcc, and on average. all metrics in the table are the average
of 10 runs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"most modiﬁcations had very little impact on auroc, but a
more signiﬁcant impact can be seen on the accuracy of the model. models a and
b show that stacking multiple ssm layers results in lower accuracy, which was
observed over all three datasets, while models c and d show that modifying the
state dimension of the ssm module can have an impact on the accuracy. the
optimal state space dimension varies depending on the dataset."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"multitask learning results. we explored the ability of our model to combine
slide- and patch-level information on the cameylon16 dataset. we compared
our model with the best performing model on camelyon16, transmil."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"we observe that all
accuracies and auroc increase compared with those reported in table 1. this
indicates that the use of patch-level annotations complements the learning of the
slide-level label. we furthermore observe that our model outperforms transmil
when combining slide- and patch-level annotations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"we map the sequence of out-
put probabilities to their slide coordinates giving a heatmap localising metastasis
(see supplementary material). table 3. ablation study for the diﬀerent ssm components on the tcga-rcc dataset. best results in bold."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_57.pdf,"best results in bold. in order to highlight the inherent
ability of ssm models to eﬀectively model long sequences, we performed an
experiment on only the largest wsis of the tcga-rcc dataset. indeed, this
dataset contains particularly long sequences (up to 62235 patches at 20x)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"this gives transformers the ability to
learn long-range dependencies and stronger modeling capacities. although they,
e.g. swinunetr, achieve state-of-the-art (sota) results on some benchmarks,
the lack of inductive bias makes transformers harder to train, requires much
more training data, and are sensitive to training recipes. in many clinical scenar-
ios and challenges, transformers can still have inferior performances than sota
cnns like nnunet."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"in this paper, we enhance the swi-
nunetr with convolutions, which results in a surprisingly stronger backbone,
the swinunetr-v2, for 3d medical image segmentation. it achieves top per-
formance on a variety of benchmarks of different sizes and modalities, including
the whole abdominal organ dataset (word), miccai flare2021 dataset,
msd pancreas dataset, msd prostate dataset, and msd lung cancer dataset,
all using the same training recipe (https://github.com/project-monai/research-
contributions/tree/main/swinunetr/btcv, our training recipe is the same as
that by swinunetr) with minimum changes across tasks. keywords: swin transformer · convolution · hybrid model · medical image
segmentation
1
introduction
medical image segmentation is a core step for quantitative and precision medicine."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"the self-attention mechanism enables learning long-
range dependencies between far-away tokens. [23] has achieved the new
top performance in the msd challenge and beyond the cranial vault (btcv) segmen-
tation challenge by pretraining on large datasets. it has a u-shaped structure where the
encoder is a swin-transformer [16]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"[34],
where the challenge is in learning complex relationships and scene understanding from
a large amount of labeled training images, many medical image segmentation networks
need to be extremely focused on local boundary details while less in need of high-
level relationships. moreover, the number of training data is also limited. hence in real
clinical studies and challenges, cnns can still achieve better results than transform-
ers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"[13] are all cnn based. besides lacking
inductive bias and enough training data, one extra reason could be that transformers are
computationally much expensive and harder to tune. more improvements and empirical
evidence are needed before we say transformers are ready to replace cnns for medical
image segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"in this paper, we try to develop a new “to-go” transformer for 3d medical image
segmentation, which is expected to exhibit strong performance under different data sit-
uations and does not require extensive hyperparameter tuning. swinunetr reaches
top performances on several large benchmarks, making itself the current sota, but
without effective pretraining and excessive tuning, its performance on new datasets and
challenges is not as high-performing as expected. a straightforward direction to improve transformers is to combine the merits of
both convolutions and self-attentions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"although simple, we found it surprisingly effective for 3d medical image
segmentation. [18], msd prostate, msd
lung cancer, and msd pancreas cancer datasets [1]. compared to the original swin-
unetr which needs extensive recipe tuning on a new dataset, we utilized the same
swinunetr-v2: stronger swin backbone
419
training recipe with minimum changes across all benchmarks, showcasing the straight-
forward applicability of swinunetr-v2 to reach state-of-the-art without extensive
hyperparameter tuning or pretraining."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"3
experiments
we use extensive experiments to show its effectiveness and justify its design for 3d
medical image segmentation. to make fair comparisons with baselines, we did not use
any pre-trained weights.
datasets. [17] (large-scale whole abdominal organ dataset) contains
150 high-resolution abdominal ct volumes, each with 16 pixel-level organ annota-
tions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"a predeﬁned data split of 100 training, 30 validation, and 20 test are provided. we use this split for our experiments.
2) the miccai flare 2021 dataset [18]. it provides 361 training scans with man-
ual labels from 11 medical centers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"3) msd task05 prostate, task06 lung tumour and task07 pancreas. [1] prostate dataset contains 32 labeled prostate
mri with two modalities for the prostate peripheral zone (pz) and the transition
zone (tz). the challenges are the large inter-subject variability and limited training
data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"the challenge comes from segmenting small tumors from large full 3d ct images. the pancreas dataset contains 281 3d ct scans with annotated pancreas and tumors
1 https://github.com/masilab/3dux-net. swinunetr-v2: stronger swin backbone
421
(or cysts)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"for all three msd tasks, we perform 5-fold cross-
validation with 70%/10%/20% train, validation, and test splits. these 20% test data
will not overlap with other folds and cover all data by 5 folds. implementation details
the training pipeline is based on the publicly available swinunetr codebase
(https://github.com/project-monai/research-contributions/tree/main/swinunetr/bt
cv, our training recipe is the same as that by swinunetr)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"we changed the initial
learning rate to 4e-4, and the training epoch is adapted to each task such that the total
training iteration is about 40k. random gaussian smooth, gaussian noise, and ran-
dom gamma correction are also added as additional data augmentation. there are dif-
ferences in data preprocessing across tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"msd data are resampled to 1 × 1x1 mm
resolution and normalized to zero mean and standard deviation (ct images are ﬁrstly
clipped by .5% and 99.5% foreground intensity percentile). for word and flare
preprocessing, we use the default transforms in swinunetr codebase (https://github.
com/project-monai/research-contributions/tree/main/swinunetr/btcv, our train-
ing recipe is the same as that by swinunetr) and 3d uxnet codebase (see footnote
1). besides these, all other training hyperparameters are the same."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"results
word result. we follow the data split in [17] and report the test scores. all the
baseline scores are from [17] except nnformer and swinunetr."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"flare 2021 result. we use the 5-fold cross-validation data split and baseline scores
from [14]. following [14], the ﬁve trained models are evaluated on 20 held-out test
scans, and the average dice scores (not model ensemble) are shown in table 3."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"we can
see our swinunetr-v2 surpasses all the baseline methods by a large margin. for msd datasets, we perform 5-fold cross-validation and ran the base-
line experiments with our codebase using exactly the same hyperparameters as men-
tioned. nnunet2d/3d baseline experiments are performed using nnunet’s original code-
base2 since it has its own automatic hyperparameter selection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"we did not do any post-
processing or model ensembling, thus there can be a gap between the test values and
online msd leaderboard values. we didn’t compare with leaderboard results because
the purpose of the experiments is to make fair comparisons, while not resorting to addi-
tional training data/pretraining, postprocessing, or model ensembling.
variations of swinunetr-v2 in this section, we investigate other variations of adding
convolutions into swin transformer. we follow fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_40.pdf,"4) swin-var-down: the patch merging is replaced by convo-
lution with stride 2 like nnformer [35]. we perform the study on the word dataset,
and the mean test dice and hd95 scores are shown in table 5. we can see that adding
convolution at different places does affect the performances, and the swinunetr-v2
design is the optimal on word test set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf,"dwis with diﬀerent b-values are fused to eﬃciently uti-
lize the diﬀerence features of dwis. rather than proposing a pure data-
driven approach, we invent a multi-sequence attention module to obtain
t. zhang and l. han—equal contribution. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43990-2 8.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf,"their results showed that
the model they developed could potentially synthesize ce-mri and outperform
other cohort models. however, mri source data of too few sequences (only t1
and t2) may not provide enough valuable informative to eﬀectively synthesize
ce-mri. however, obtaining a complete
mri sequence makes the examination costly and time-consuming."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf,"on the other
hand, the information provided by multi-sequences may be redundant and may
not contain the relevant information of ce-mri. . dwi can provide information on cell density
and tissue microstructure based on the diﬀusion of tissue water. in particular,
dwi can capture the dynamic diﬀusion state of water molecules to estimate the
vascular distribution in tissues, which is closely related to the contrast-enhanced
regions in ce-mri. dwi may be a valuable alternative in breast cancer detection
in patients with contraindications to gbca [3]. inspired by this, we develop a
multi-sequence fusion network based on t1-weighted mri and multi-b-value
dwi to synthesize ce-mri."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf,"our contributions are as follows:
i from the perspective of method, we innovatively proposed a multi-sequence
fusion model, designed for combining t1-weighted imaging and multi-b-value
dwi to synthesize ce-mri for the ﬁrst time. ii we invented hierarchical fusion module, weighted diﬀerence module and
multi-sequence attention module to enhance the fusion at diﬀerent scale, to
control the contribution of diﬀerent sequence and maximising the usage of
the information within and across sequences. iii from the perspective of clinical application, our proposed model can be used
to synthesize ce-mri, which is expected to reduce the use of gbca."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf,"we retrospectively collected 765 patients with
breast cancer presenting at our cancer institute from january 2015 to novem-
ber 2020, all patients had biopsy-proven breast cancers (all cancers included
in this study were invasive breast cancers, and ductal carcinoma in situ had
been excluded). the mris were acquired with philips ingenia 3.0-t scanners,
and overall, three sequences were present in the in-house dataset, including t1-
weighted fat-suppressed mri, contrast-enhanced t1-weighted mri and dwi. [2].
82
t. zhang et al.
2.2
model
figure 1 illustrates the structure of the proposed model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf,"first, the reconstruction
module is used to automatically encode and decode each input mri sequence
information to obtain the latent representation of diﬀerent mri sequences at
multi-scale levels. then, the hierarchical fusion module is used to extract the
hierarchical representation information and fuse them at diﬀerent scales. 1. model details and ﬂowchart for this study."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf,"if the ce-mri was successfully synthesized, the enhanced region would be high-
lighted in the diﬀerence mri, otherwise it would not.
fig. 2. detailed structure of the hierarchical fusion module.
2.4
experiment settings
based on the ratio of 8:2, the training set and independent test set of the in-house
dataset have 612 and 153 cases, respectively. the trade-oﬀ parameter λ1 was set
to 100 during training, and the trade-oﬀ parameter of the reconstruction loss in
the reconstruction module is set to 5. masks for all breasts were used (weighted
by a factor of 100 during the calculation of the loss between generated and real
ce-mri) to reduce the inﬂuence of signals in the thoracic area."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf,"[5]
t1+dwis
87.58 ± 2.68
27.80 ± 1.56
0.0692 ± 0.035
proposed
t1+dwis
89.93 ± 2.91
28.92 ± 1.63
0.0585 ± 0.026
where g(x) represents a generated image, y(x) represents a ground-truth
image, μy(x) and μg(x) represent the mean of y(x) and g(x), respectively, σy(x)
and σg(x) represent the variance of y(x) and g(x), respectively, σy(x)g(x) repre-
sents the covariance of y(x) and g(x), and c1 and c2 represent positive constants
used to avoid null denominators. 3
results
first, we compare the performance of diﬀerent existing methods on synthetic
ce-mri using our source data, the quantitative indicators used include psnr,
ssim and nmse. as shown in table 1, the ssim of mmgsn-net [11] and the
method of chung et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7/paper_8.pdf,"in addition, although
the method of chung et al. [5] used full-sequence mri to synthesize ce-mri, it
would be advantageous to obtain synthetic ce-mri images using as little data
as possible, taking advantage of the most contributing sequences. they did not
take advantage of multi-b-value dwi, nor did they use the hierarchical fusion
module to fully fuse the hierarchical features of multi-sequence mri.
86
t. zhang et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"deep learning based medical image recognition systems often
require a substantial amount of training data with expert annotations,
which can be expensive and time-consuming to obtain. recently, syn-
thetic augmentation techniques have been proposed to mitigate the issue
by generating realistic images conditioned on class labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"however, the
eﬀectiveness of these methods heavily depends on the representation
capability of the trained generative model, which cannot be guaranteed
without suﬃcient labeled training data. to further reduce the depen-
dency on annotated data, we propose a synthetic augmentation method
called histodiﬀusion, which can be pre-trained on large-scale unlabeled
datasets and later applied to a small-scale labeled dataset for augmented
training. in particular, we train a latent diﬀusion model (ldm) on
diverse unlabeled datasets to learn common features and generate real-
istic images without conditional inputs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"then, we ﬁne-tune the model
with classiﬁer guidance in latent space on an unseen labeled dataset
so that the model can synthesize images of speciﬁc categories. addi-
tionally, we adopt a selective mechanism to only add synthetic sam-
ples with high conﬁdence of matching to target labels. we evaluate our
proposed method by pre-training on three histopathology datasets and
testing on a histopathology dataset of colorectal cancer (crc) excluded
from the pre-training datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"1
introduction
the recent advancements in medical image recognition systems have greatly ben-
eﬁted from deep learning techniques [15,28]. large-scale well-annotated datasets
j. ye and h. ni—these authors contributed equally to this work. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 71.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"1. comparison between diﬀerent deep generative models for synthetic augmenta-
tion. (a) cgan-based method which requires relatively large-scale annotated training
data; (b) diﬀusion model (dm) which cannot take conditional input; (c) our proposed
histodiﬀusion model that can be pretrained on large-scale unannotated data and later
applied to unseen small-scale annotated data for augmentation.
are one of the key components for training deep learning models to achieve sat-
isfactory results [3,17]. however, unlike natural images in computer vision, the
number of medical images with expert annotations is often limited by the high
labeling cost and privacy concerns."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"to overcome this challenge, a natural choice
is to employ data augmentation to increase the number of training samples. while existing works have proven eﬀective in improv-
ing the performance of downstream models to some extent, a suﬃcient amount
of labeled data is still required to adequately train models to generate decent-
quality images. more recently, diﬀusion models have become popular for natural
image generation due to their impressive results and training stability [4,13,31]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"a few studies have also demonstrated the potential of diﬀusion models for med-
ical image synthesis [19,24]. although annotated data is typically hard to acquire for medical images,
unannotated data is often more accessible. to mitigate the issue existed in cur-
rent cgan-based synthetic augmentation methods [8,36–38], in this work, we
propose to leverage the diﬀusion model with unlabeled pre-training to reduce
the dependency on the amount of labeled data (see comparisons in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"we
propose a novel synthetic augmentation method, named histodiﬀusion, which
can be pre-trained on large-scale unannotated datasets and adapted to small-
scale annotated datasets for augmented training. speciﬁcally, we ﬁrst employ a
latent diﬀusion model (ldm) and train it on a collection of unlabeled datasets
from multiple sources. this large-scale pre-training enables the model to learn
756
j. ye et al.
common yet diverse image characteristics and generate realistic medical images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"synthetic images are then generated with classiﬁer guidance [4] in
the latent space. following the prior work [36], we select generated images based
on the conﬁdence of target labels and feature similarity to real labeled images. we evaluate our proposed method on a histopathology image dataset of colorec-
tal cancer (crc)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"experiment results show that when presented with limited
annotations, the classiﬁer trained with our augmentation method outperforms
the ones trained with the prior cgan-based methods. our experimental results
show that once histodiﬀusion is well pre-trained using large datasets, it can be
applied to any future incoming small dataset with minimal ﬁne-tuning and may
substantially improve the ﬂexibility and eﬃcacy of synthetic augmentation. 2
methodology
figure 2 illustrates the overall architecture of our proposed method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"first, we
train an ldm on a large-scale set of unlabeled datasets collected from multiple
sources. we then ﬁne-tune the decoder of this pretrained ldm on a small labeled
dataset. to enable conditional image synthesis, we also train a latent classiﬁer on
the same labeled dataset to guide the diﬀusion model in ldm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"once the classiﬁer
is trained, we apply the ﬁne-tuned ldm to generate a pool of candidate images
conditioned on the target class labels. these candidate images are then passed
through the image selection module to ﬁlter out any low-quality results. finally,
we can train downstream classiﬁcation models on the expanded training data,
which includes the selected images, and then use them to perform inference on
test data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"in this section, we will ﬁrst introduce the background of diﬀusion
models and then present details about the histodiﬀusion model. 2.1
diﬀusion models
diﬀusion models (dm) [13,30,32] are probabilistic models that are designed to
learn a data distribution. given a sample from the data distribution z0 ∼ q(z0),
the dm forward process produces a markov chain z1, . . ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"the architecture of our proposed histodiﬀusion, which consists of a pre-training
process (blue solid lines), a ﬁne-tuning process (blue dashed lines), and a selective aug-
mentation process (orange lines). during pre-training, a latent autoencoder (lae) and
a diﬀusion model (dm) are trained on large-scale unlabeled datasets for unconditional
image synthesis. histodiﬀusion is then ﬁne-tuned on a small-scale dataset for condi-
tional image synthesis under the guidance of a trained latent classiﬁer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"for the dm in
ldm, both the forward and reverse sampling processes are performed in the
latent space z instead of the original image space x.
unconditional large-scale pre-training. to ensure the latent space z
can cover features of various data types, we ﬁrst pre-train our proposed his-
todiﬀusion on large-scale unlabeled datasets. speciﬁcally, we gather unlabeled
images from m diﬀerent sources to construct a large-scale set of datasets
s = {s1, s2, . ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"after training the lae, we ﬁxed the trained encoder e and then train a
dm with the loss ldm in eq. 3 to model e’s latent space z. here z0 = e(x)
in eq. in the dm
reverse sampling process to synthesize a novel latent ˜z0 ∈ rh×w×c and employ
the trained decoder d to generate a new image ˜x = d(˜z0), which should satisfy
the similar distribution as the data in s.
conditional small-scale fine-tuning. using the lae and dm pretrained
on s, we can only generate the new image ˜x following the similar distribution in
s. to generalize our histodiﬀusion to the small-scale labeled dataset s′ collected
from a diﬀerent source (i.e., s′ ̸⊂ s), we further ﬁne-tune histodiﬀusion using
the labeled data from s′. let y be the label of image x in s′. to minimize the
training cost, we ﬁx both the trained encoder e and trained dm model ϵθ to
keep latent space z unchanged."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"classiﬁer-guided conditional synthesis. to enable conditional image gen-
eration with our histodiﬀusion, we further apply the classiﬁer-guided diﬀusion
sampling proposed in [4,29,30,33] using the labeled data (x, y) from small-scale
labeled dataset s′. we ﬁrst utilize the trained encoder e to encode the data x
from s′ as latent z0. then we train a time-dependent latent classiﬁer φ with
paired (zt, y) using the following loss function:
lφ = lce(φ(zt), y) ,
(6)
where zt ∼ q(zt|z0) is the noisy version of z0 at the time step t during the
dm forward process, and lce is the cross-entropy classiﬁcation loss."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"(8)
the ﬁnal image ˜x of class y can be generated by applying the ﬁne-tuned decoder
d′, i.e., ˜x = d′( ˜z0). to further improve the eﬃcacy of synthetic aug-
mentation, we follow [36] to selectively add synthetic images to the original
labeled training data based on centroid feature distance. the augmentation ratio
is deﬁned as the ratio between the selected synthetic images and the original
training images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"more results are demonstrated later in table 1.
3
experiments
datasets. we employ three public datasets of histopathology images during
the large-scale pre-training procedure. [2], containing 312,320 patches extracted from the hematoxylin & eosin
(h&e) stained human breast cancer tissue micro-array (tma) images [18]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"the second dataset is pannuke [9],
a pan-cancer histology dataset for nuclei instance segmentation and classiﬁca-
tion. the pannuke dataset includes 7,901 patches of 19 types of h&e stained
760
j. ye et al.
fig. [14] and our proposed histodiﬀusion (zoom in for clear observation)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"qualitatively, our synthesized results contain more realistic and diagnosable patterns
than results synthesized from stylegan2.
tissues obtained from multiple data sources, and each patch has a uniﬁed size of
256×256 pixels. the third dataset is tcga-brca-a2/e2 [34], a subset derived
from the tcga-brca breast cancer histology dataset [20]. the subset consists
of 482,958 patches with a resolution of 256 × 256."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"overall, there are 803,179
patches used for pre-training. as for ﬁne-tuning and evaluation, we employ the
nct-crc-he-100k dataset that contains 100,000 patches from h&e stained
histological images of human colorectal cancer (crc) and normal tissue. the
patches have been divided into 9 classes: adipose (adi), background (back),
debris (deb), lymphocytes (lym), mucus (muc), smooth muscle (mus), nor-
synthetic augmentation with large-scale unconditional pre-training
761
mal colon mucosa (norm), cancer-associated stroma (str), colorectal adeno-
carcinoma epithelium (tum)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"this subset has been carefully selected through an even sampling without
replacement from each tissue type present in the train set. it is worth noting that
the labels for these samples have been kept, which allows the ﬁne-tuning process
to be guided by labeled data, leading to better predictions on the speciﬁc task or
domain being trained. by ensuring that the ﬁne-tuning process is representative
of the entire dataset through even sampling from each tissue type, we can elim-
inate bias towards any particular tissue type."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"we evaluate the ﬁne-tuned model
on the oﬃcial test set. the related data use declaration and acknowledgment
can be found in our supplementary materials.
evaluation metrics. [12]
to assess the image quality of the synthetic samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"a qualitative
comparison between synthetic images by histodiﬀusion and stylegan2 can be
762
j. ye et al.
table 1. quantitative comparison results of synthetic image quality and augmented
classiﬁcation. “random” refers to directly augmenting the training dataset with syn-
thesized images without any image selections while “selective” indicates applying selec-
tive module [36] to ﬁlter out low-quality images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"3, where histodiﬀusion consistently generates more realistic images
matching the given class conditions than sytlegan2, especially for classes adi
and back. when augmenting the training dataset with diﬀerent numbers of images syn-
thesized from histodiﬀusion and stylegan2, one can observe that when increas-
ing the ratio of synthesized data to 100%, the fid score of stylegan2 increases
quickly and can become even worse than the one without using image selection
strategy. in contrast, histodiﬀusion can keep synthesizing high-quality images
until the augmentation ratio reaches 300%."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_72.pdf,"regarding classiﬁcation performance
improvement of the baseline classiﬁer, the accuracy and f1 score of using his-
todiﬀusion augmentation are increased by up to 6.4% and 6.6%, respectively. even when not using the image selection module to ﬁlter out the low-quality
results (i.e., +random 50%), our histodiﬀusion can still improve the accuracy
by 1.5%. the robustness and eﬀectiveness of histodiﬀusion can be attributed
to the unconditional large-scale pre-training, our specially-designed conditional
ﬁne-tuning, and classiﬁer-guided generation, among others."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"most existing weakly-supervised segmentation methods rely
on class activation maps (cam) to generate pseudo-labels for training
segmentation models. however, cam has been criticized for highlighting
only the most discriminative parts of the object, leading to poor qual-
ity of pseudo-labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"although some recent methods have attempted to
extend cam to cover more areas, the fundamental problem still needs
to be solved. we believe this problem is due to the huge gap between
image-level labels and pixel-level predictions and that additional infor-
mation must be introduced to address this issue. thus, we propose a
text-prompting-based weakly supervised segmentation method (tpro),
which uses text to introduce additional information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"tpro employs a
vision and label encoder to generate a similarity map for each image,
which serves as our localization map. pathological knowledge is gathered
from the internet and embedded as knowledge features, which are used to
guide the image features through a knowledge attention module. addi-
tionally, we employ a deep supervision strategy to utilize the network’s
shallow information fully."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"our approach outperforms other weakly super-
vised segmentation methods on benchmark datasets luad-histoseg and
bcss-wsss datasets, setting a new state of the art. code is available
at: https://github.com/zhangst431/tpro.
keywords: histopathology tissue segmentation · weakly-supervised
semantic segmentation · vision-language
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_11.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43907-0_11
110
s. zhang et al.
1
introduction
automated segmentation of histopathological images is crucial, as it can quantify
the tumor micro-environment, provide a basis for cancer grading and prognosis,
and improve the diagnostic eﬃciency of clinical doctors [6,13,19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"1. comparison of activation maps extracted from cam and our method, from left
to right: origin image, ground truth, three activation maps of tumor epithelial (red),
necrosis (green), and tumor-associated stroma (orange) respectively. on the right side,
there are some examples of the related language knowledge descriptions used in our
method. it shows that cam only highlights a small portion of the target, while our
method, which incorporates external language knowledge, can encompass a wider and
more precise target tissue."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"(color ﬁgure online)
recent studies on weakly supervised segmentation primarily follow class acti-
vation mapping (cam) [20], which localizes the attention regions and then
generates the pseudo labels to train the segmentation network. however, the
cam generated based on the image-level labels can only highlight the most dis-
criminative region, but fail to locate the complete object, leading to defective
pseudo labels, as shown in fig. accordingly, many attempts have been made
to enhance the quality of cam and thus boost the performance of weakly super-
vised segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"han et al. [7] proposed an erasure-based method that con-
tinuously expands the scope of attention areas to obtain rich content of pseudo
labels. li et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"the primary limitation is that the symptoms and manifesta-
tions of histopathological subtypes cannot be comprehensively described by an
abstract semantic category. as a result, the image-level label supervision may
not be suﬃcient to pinpoint the complete target area. to remedy the limitations of image-level supervision, we advocate for the inte-
gration of language knowledge into weakly supervised learning to provide reliable
guidance for the accurate localization of target structures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"to this end, we pro-
pose a text-prompting-based weakly supervised segmentation method (tpro)
for accurate histopathology tissue segmentation. the text information originates
from the task’s semantic labels and external descriptions of subtype manifesta-
tions. for each semantic label, a pre-trained medical language model is utilized
to extract the corresponding text features that are matched to each feature point
in the image spatial space."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"a higher similarity represents a higher possibility of
this location belonging to the corresponding semantic category. additionally,
the text representations of subtype manifestations, including tissue morphol-
ogy, color, and relationships to other tissues, are extracted by the language
model as external knowledge. the discriminative information can be explored
from the text knowledge to help identify and locate complete tissues accurately
by jointly modeling long-range dependencies between image and text."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"we con-
duct experiments on two weakly supervised histological segmentation bench-
marks, luad-histoseg and bcss-wsss, and demonstrate the superior quality
of pseudo labels produced by our tpro model compared to other cam-based
methods. our contributions are summarized as follows: (1) to the best of our knowl-
edge, this is the ﬁrst work that leverages language knowledge to improve the
quality of pseudo labels for weakly-supervised histopathology image segmenta-
tion. (2) the proposed text prompting models the correlation between image
representations and text knowledge, eﬀectively improving the quality of pseudo
labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"(3) the eﬀectiveness of our approach has been eﬀectively validated by
two benchmarks, setting a new state of the art. lymphocyte tissue
tumor-associated stroma tissue
gap
gap
gap
stage 1
stage 2
stage 3
stage 4
sim
sim
sim
input image
knowledge input
label input
input image
bert
clip
bert: clinicalbert  
clip: medclip
sim: pixel-label correlation
knowledge attention
search from internet
1
1
0
1
1
1
0
1
1
1
0
1
reshape
reshape
fc: fc+relu+fc
fig. 2. the framework of the proposed tpro."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"112
s. zhang et al.
2
method
figure 2 displays the proposed tpro framework, a classiﬁcation network
designed to train a suitable model and extract segmentation pseudo-labels. the
framework comprises a knowledge attention module and three encoders: one
vision encoder and two text encoders (label encoder and knowledge encoder). 2.1
classiﬁcation with deep text guidance
vision encoder."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"the image features are denoted as ts ∈
rms×cs, where 2 ≤ s ≤ 4 indicates the stage number. the label encoder encodes the text labels in the dataset into
n label features, denoted as l ∈ rn×cl, where n represents the number of
classes in the dataset and cl represents the dimension of label features. since
the label features will be used to calculate the similarity with image features, it
is important to choose a language model that has been pre-trained on image-text
pairs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"here we use medclip1 as our label encoder, which is a model ﬁne-tuned
on the roco dataset [12] based on clip [14].
knowledge encoder. the knowledge encoder is responsible for embedding
the descriptions of subtype manifestations into knowledge features, denoted as
k ∈ rn×ck. the knowledge features guide the image features to focus on regions
relevant to the target tissue."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"[9].
adaptive layer. we freeze the label and knowledge encoders for training eﬃ-
ciency but add an adaptive layer after the text encoders to better tailor the text
features to our dataset. the adaptive layer is a simple fc-relu-fc block that
allows for ﬁne-tuning of the features extracted from the text encoders."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"after the input image and text labels are embedded. we employ the inner product to compute the similarity between image features
and label features, denoted as fs. specially, we ﬁrst reshape the image features
from a token format into feature maps."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"(1)
then, we perform a global average-pooling operation on the produced similarity
map to obtain the class prediction, denoted as ps ∈ r1×n. we then calculate
the binary cross-entropy loss between the class label y ∈ r1×n and the class
prediction ps to supervise the model training, which is formulated as:
1 https://github.com/kaushalya/medclip. [n])log[1 − σ(ps[n])]
(2)
deep supervision."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"the loss of the entire
network is computed as:
l = λ2l2 + λ3l3 + λ4l4. (3)
2.2
knowledge attention module
to enhance the model’s understanding of the color, morphology, and relation-
ships between diﬀerent tissues, we gather text representations of diﬀerent sub-
type manifestations from the internet and encode them into external knowledge
via the knowledge encoder. the knowledge attention module uses this exter-
nal knowledge to guide the image features toward relevant regions of the target
tissues."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"the knowledge attention module, shown in fig. 2, consists of two multi-head
self-attention modules. the image features t4 ∈ rm4×c4 and knowledge features
after adaptive layer k ∈ rn×c4 are concatenated in the token dimension to
obtain tfuse ∈ r(m4+n)×c4. this concatenated feature is then fed into the
knowledge attention module for self-attention calculation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"the output tokens
are split, and the part corresponding to the image features is taken out. noted
that the knowledge attention module is added only after the last stage of the
vision encoder to save computational resources.
2.3
pseudo label generation
in the classiﬁcation process, we calculate the similarity between image features
and label features to obtain a similarity map f, and then directly use the result
of global average pooling on the similarity map as a class prediction. that is,
the value at position (i, j, k) of f represents the probability that pixel (i, j) is
classiﬁed into the kth class."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"therefore we directly use f as our localization map. ≤ n means cth class in the dataset. then we calculate the back-
ground localization map by the following formula:
fbg(i, j) = {1 − max
c∈[0,c) f c
fg(i, j)}α,
(5)
where α ≥ 1 denotes a hyper-parameter that adjusts the background conﬁdence
scores."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"then we stitch together the localization map of foreground and background,
denoted as ˆf. (6)
finally, we perform argmax operation on fall to obtain the ﬁnal pseudo-label. [7] is a weakly-supervised histological semantic segmenta-
tion dataset for lung adenocarcinoma."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"there are four tissue classes in this
dataset: tumor epithelial (te), tumor-associated stroma (tas), necrosis (nec),
and lymphocyte (lym). the dataset comprises 17,258 patches of size 224×224. according to the oﬃcial split, the dataset is divided into a training set (16,678
patch-level annotations), a validation set (300 pixel-level annotations), and a test
set (307 pixel-level annotations)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"bcss-wsss3 is a weakly supervised tissue
semantic segmentation dataset extracted from the fully supervised segmenta-
tion dataset bcss [3], which contains 151 representative h&e-stained breast
cancer pathology slides. the dataset was randomly cut into 31826 patches of
size 224 × 224 and divided into a training set (23422 patch-level annotations),
a validation set (3418 pixel-level annotations), and a test set (4986 pixel-level
annotations) according to the oﬃcial split. there are four foreground classes in
this dataset, including tumor (tum), stroma (str), lymphocytic inﬁltrate
(lym), and necrosis (nec)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"[2] as our vision encoder, label encoder, and
table 1. comparison of the pseudo labels generated by our proposed method and
those generated by previous methods. tpro for weakly supervised histopathology tissue segmentation
115
knowledge encoder, respectively. the hyperparameters during training and eval-
uation can be found in the supplementary materials."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"we conduct all of our
experiments on 2 nvidia geforce rtx 2080 ti gpus. 3.3
compare with state-of-the-arts
comparison on pseudo-labels. table 1 compares the quality of our pseudo-
labels with those generated by previous methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"[20] outperformed grad-cam [15], with miou values of 70.44% and
56.52% on the luad-histoseg and bcss-wsss datasets, respectively. [18] consists of a classiﬁcation and a segmentation branch, and table 1
displays the pseudo-label scores generated by the classiﬁcation branch. [18] yielded inferior results
compared to cam [20]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"[18] for
single-label image segmentation, with the segmentation branch simpliﬁed to
binary segmentation to reduce the diﬃculty, while our dataset consists of multi-
label images. [20] in terms of the quality of the generated pseudo-labels, with
its proposed progressive dropout attention eﬀectively expanding the coverage
of target regions beyond what cam [20] can achieve. our proposed method
outperformed all previous methods on both luad-histoseg and bcss-wsss
datasets, with improvements of 2.64% and 5.42% over the second-best method,
respectively (table 2)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"table 2. comparison of the ﬁnal segmentation results between our method and the
methods in previous years. to further evaluate our proposed
method, we trained a segmentation model using the extracted pseudo-labels and
compared its performance with previous methods. [5] failed to produce the
desired results on our datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"[18] to perform well, and it failed to provide an overall beneﬁt to the
model. experimental results also indicate that the iou scores of its segmentation
116
s. zhang et al.
table 3. comparison the eﬀectiveness of
label text(lt), knowledge text(kt), and
deep supervision(ds). by
training the segmentation model of oeem [11] using the pseudo-labels extracted
by cam [20] in table 1, we can observe a signiﬁcant improvement in the ﬁnal
segmentation results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"our segmentation performance surpassed all previous meth-
ods. speciﬁcally, our miou scores exceeded the second-best method by 3.17% and
3.09% on luad-histoseg and bcss-wsss datasets, respectively. additionally,
it is worth noting that we did not use any strategies speciﬁcally designed for the
segmentation stage."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"we set the
baseline as the framework shown in fig. 2 with all text information and deep
supervision strategy removed. it is evident that the addition of textual informa-
tion increases our pseudo-label miou by 2.50%."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_11.pdf,"these ﬁndings demonstrate the signiﬁcant
contribution of each proposed module to the overall improvement of the results. in order to demonstrate the eﬀectiveness of fusing pseudo-labels from the last
three stages, we have presented in table 4 the iou scores for each stage’s pseudo-
labels as well as the fused pseudo-labels. it can be observed that after fusing the
pseudo-labels, not only have the iou scores for each class substantially increased,
but the miou score has also increased by 0.91% compared to the fourth stage."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"it allows a model to learn the dependency
between various characteristics of text and image. our proposed model
demonstrates superior performance compared to other medical models
using image-only data or image-text data. furthermore, we utilize our
module as a region of interest (roi) generator to classify the inﬂamma-
tion of the sacroiliac joints."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"keywords: image segmentation · multi modal learning · cross
position attention · text-guided attention · medical image
1
introduction
advances in deep learning have been witnessed in many research areas over
the past decade. in medical ﬁeld, automatic analysis of medical image data has
actively been studied. in particular, segmentation which identify region of inter-
est (roi) in an automatic way is an essential medical imaging process."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"recently, it has been demonstrated that the attention modules [4,17,20] enable
deep learning networks to better extract robust features, which can be applied in
medical image segmentation to learn subtle medical features and achieve higher
performance [14,16,18,21].
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14224, pp. lee et al.
however, as image-only training trains a model with pixels that constitute
an image, there is a limit in extracting ﬁne-grained information about a target
object even if transfer learning is applied through a pre-trained model. recently,
to overcome this limitation, multi-modality studies have been conducted, aiming
to enhance the expressive power of both text and image features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"the trend of text-image multi-modality-based research on image processing
has extended to the medical ﬁeld. [19] proposed a semantic matching loss that
learns medical knowledge to supplement the disadvantages of clip that cannot
capture uncertain medical semantic meaning. in [2], they trained to increase
the similarity between the image and text by calculating their inﬂuence on each
other as a weighted feature."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"[10] generated the
positional characteristics of lesions or target objects as text labels. furthermore,
it proposed a double u-shaped structure consisting of a u-shaped vit that
combines image and text information and a u-shaped cnn that produces a
segmentation mask. however, when combining medical images with non-ﬁne-
grained text information, noise can aﬀect the outcome."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"the image feature map generated
from an image encoder was used as a query. learning the association between
text and image enables us to learn positional information of targets in an image
more eﬀectively than existing models that learned multi-modality from medical
images. cpam t g showed an excellent segmentation performance in our com-
prehensive experiments on various medical images, such as cell, chest x-ray, and
magnetic resonance image (mri)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"in addition, by applying the proposed tech-
nique to the automatic roi setting module for the deep learning-based diagnosis
of sacroiliac arthritis, we conﬁrmed that the proposed method could be eﬀective
when it is used in a practical application of computer-aided diagnosis. our main contributions are as follows:
– we devised a text-guided cross-position attention module (cpam t g) that
eﬃciently combines text information with image feature maps. – we demonstrated the eﬀect of cpam t g on segmentation for various types
of medical images.
– for a practical computer-aided diagnosis system, we conﬁrm the eﬀectiveness
of the proposed method in a deep learning-based sacroiliac arthritis diagnosis
system.
text-guided cross-position attention for segmentation
539
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"for image encoding and decoding, we employed
u-net, widely used as a backbone in medical image segmentation. to train our
proposed model, we utilize a dataset consisting of image and text pairs. 2.1
conﬁguration of text-image encoder and decoder
as transformer has demonstrated its eﬀectiveness in handling the long-range
dependency in sequential data through self-attention [1], it performs well in
various ﬁelds requiring nlp or contextual information analysis of data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"u-net operates as an end-to-end fully connected network-based model
consisting of a convolutional encoder and decoder connected by skip connec-
tions. this architecture is particularly suitable for our purpose because it can
be successfully trained on a small amount of data. rc×h×w as fi = encoderi(i) and the decoder (decoderi) that will
generate the segmented image from the enhanced encoding vector obtained by
the cross-position attention which will be described in the following subsection."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"the weights of text and image encoders were initialized by the weights of
clip’s pre-trained transformer and vgg16 pre-trained on imagenet, respec-
tively, and ﬁne-tuned by a loss function for segmentation which will be described
in sect. [5] to
combine the semantic information of text and image. this module utilizes not
only the image feature map from the image encoder but also the global text
representation from the text encoder to learn the dependency between various
characteristics of text and image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"it eﬀectively captures spatial dependencies
among pixels by generating keys, queries, and values from feature maps. by
encoding broad contextual information into local features, and then adaptively
gathering spatial contexts, pam improves representation capability. in particu-
lar, this correlation analysis among pixels can eﬀectively analyze medical images
in which objects are relatively ambiguous compared to other types of natural
images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"by the global
text representation (vt ) to match the dimension of the text feature with that of
the image feature map as ft = r(g(vt )⊺ × l), where g(·) is a fully connected
layer that adjusts the 2c channel of the global text representation vt to the
image feature map channel c. r(·) is a reshape operator to c × h × w.
text-guided cross-position attention for segmentation
541
the text feature map ft is used as key and value, and the image feature
map fi is used as a query to perform self-attention as
q = hq(fi),
k = hk(ft ),
v = hv (ft ),
(1)
where hq, hk, and hv are convolution layers with a kernel size of 1, and q,
k, and v are queries, keys, and values for self-attention. attention = softmax(q⊺k)
(2)
cpam t g = attention⊺v + fi
(3)
finally, by upsampling the low-dimensional cpam t g obtained through cross-
attention of text and image together with skip-connection, more accurate seg-
mentation prediction can express the detailed information of an object. 3
experiments
3.1
setup
medical datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"[6] dataset, and sacroiliac joint (sij) dataset. the ﬁrst two
datasets are the same benchmark datasets used in [10]. [8] contains
30 digital microscopic tissue images of several patients and qata-cov19 are
covid-19 chest x-ray images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"the ratio of training, validation, and test sets was
the same as in [10]. sij is the dataset privately prepared for this study which con-
sists of 804 mri slices of nineteen healthy subjects and sixty patients diagnosed
with axial spondyloarthritis. among all mri slices, we selected the gadolinium-
enhanced fat-suppressed t1-weighted oblique coronal images, excluding the ﬁrst
and last several slices in which the pelvic bones did not appear, and added the
text annotations for the slices.
training and metrics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"for a better training, data augmentation was used. we
randomly rotated images by −20◦ ∼ +20◦ and conducted a horizontal ﬂip with
0.5 probability for only the monuseg and qata-cov19 datasets. the batch
size and learning rate were set to 2 and 0.001, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"by contrast, lvit and cpam t g, which utilize both text and image informa-
tion, signiﬁcantly improved image segmentation performance because of multi-
modal complementarity, even for medical images with complex and ambiguous
object boundaries. [10] on all datasets. this means that the proposed cpam t g
helps to improve segmentation performance by allowing text information to serve
as a guide for feature extraction for segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"similar
to the analysis that can be derived from table 1, fig. 3 shows that cpam t g
and lvit, which use text information together for image segmentation, create a
segmentation mask with more distinctive borders than other methods. in partic-
ular, with sij, cpam t g accurately predicted the boundaries of even thin bone
parts compared to lvit."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"from these results, we conjecture that the reasons for the
performance improvement of cpam t g are as follows. cpam t g independently
encodes the input text and image and then combines semantic information via
a cross-attention module. consequently, the two types of information (text and
image) do not act as noise from each other, and cpam t g achieves an improved
performance compared to lvit."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"3.3
ablation study
to validate the design of our proposed model, we perform an ablation study on
position attention and cpam t g. speciﬁcally, for the sij dataset, we exam-
ined the eﬀect of attention in extracting feature maps through comparison with
backbone networks (u-net) and pam. in addition, we investigated whether text
information about images serves as a guide in the position attention process for
image segmentation by comparing it with cpam t g. table 2 summarizes the
result of each case."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"this indicates that pam improves per-
formance by learning associations between pixels for ambiguous targets, as in
medical images. in addition, the best performance results of cpam t g show
that text information provided helpful information in an image segmentation
process using the proposed model.
text-guided cross-position attention for segmentation
543
table 1. 3. qualitative results of segmentation models.
3.4
application: deep-learning based disease diagnosis
in this section, we conﬁrm the eﬀectiveness of the proposed segmentation method
through a practical bio-medical application as a deep learning-based active
sacroiliitis diagnosis system."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_52.pdf,"this indicates
that the proposed method can be eﬀectively utilized in practical applications of
computer-aided diagnosis. 4
conclusion
in
this
study,
we
developed
a
new
text-guided
cross-attention
module
(cpam t g) that learns text and image information together. the proposed
model has a composite structure of position attention and cross-attention in
that the key and value are from text data, and the query is created from the
image."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"the recent surge of foundation models in computer vision
and natural language processing opens up perspectives in utilizing multi-
modal clinical data to train large models with strong generalizability. yet pathological image datasets often lack biomedical text annotation
and enrichment."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"cite injects text insights gained from
language models pre-trained with a broad range of biomedical texts,
leading to adapt foundation models towards pathological image under-
standing. through extensive experiments on the patchgastric stomach
tumor pathological image dataset, we demonstrate that cite achieves
leading performance compared with various baselines especially when
training data is scarce. cite oﬀers insights into leveraging in-domain
text knowledge to reinforce data-eﬃcient pathological image classiﬁca-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"those approaches are often
designed to address disease-speciﬁc problems with limitations in their general-
izability. in parallel, foundation models [4] have surged in computer vision [5,6]
and natural language processing [7,8] with growing model capacity and data
size, opening up perspectives in utilizing foundation models and large-scale clin-
ical data for diagnostic tasks. however, pure imaging data can be insuﬃcient to
adapt foundation models with large model capacity to the medical ﬁeld."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"an image with the visual prompt is processed through a
vision encoder and a projection layer. the text knowledge is embedded by a text
encoder, where a stop-gradient operation is applied. classiﬁcation prediction is made
by the similarity between image and text embeddings."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"pre-trained biomedical language models are increas-
ingly applied to medical context understanding [9–11]. language models prove
to be eﬀective in capturing semantic characteristics with a lower data acquisition
and annotation cost in medical areas [12]. such property is desired to address the
dilemma of medical imaging cohorts, where well-annotated, high-quality medical
imaging cohorts are expensive to collect and curate compared with text inputs
[13]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"thus, connecting
visual representations with text information from biomedical language models
becomes increasingly critical to adapting foundation models for medical image
classiﬁcation, particularly in the challenging setting of data deﬁciency. in this study, we propose cite, a data-eﬃcient adaptation framework that
connects image and text embeddings from foundation models to perform
pathological image classiﬁcation with limited training samples (see fig. 1). to
enable language comprehension, cite makes use of large language models pre-
trained on biomedical text datasets [10,11] with rich and professional biomedical
knowledge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"[6], in order to capture domain-speciﬁc knowledge with-
out modifying the backbone parameters. in this framework, we emphasize the
utility of text information to play a substitutive role as traditional classiﬁcation
heads, guiding the adaptation of the vision encoder. a favorable contribution of
our approach is to retain the completeness of both pre-trained models, enabling
a low-cost adaptation given the large capacity of foundation models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"overall,
our contributions are summarized as follows:
1. we demonstrate the usefulness of injecting biomedical text knowledge into
foundation model adaptation for improved pathological image classiﬁcation. 2. cite introduces only a small number of extra model parameters (∼0.6% of
the vision encoder), meanwhile keeping the pre-trained models frozen during
274
y. zhang et al.
adaptation, leading to strong compatibility with a variety of backbone model
architectures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"also, ﬁne-tuning
or linear-probing the pre-trained models obtained from natural images [16–18]
is reasonable. however, those methods are supported by suﬃcient high-quality
data expensive to collect and curate [19]. in addition, task-speciﬁc models do
not generalize well with diﬀerent image modalities [2]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"however, those methods establish vision-language alignment by pre-training on
large-scale image-text pairs. instead, we combine pre-trained unimodal models
on downstream tasks and build a multi-modal classiﬁer with only a few data.
model adaptation via prompt tuning. prompt tuning proves to be an
eﬃcient adaptation method for both vision and language models [22,23]. orig-
inating from natural language processing, “prompting” refers to adding (man-
ual) text instructions to model inputs, whose goal is to help the pre-trained
model better understand the current task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"however, existing prompt tuning methods lack expert knowl-
edge and understanding of downstream medical tasks. to address this challenge,
we leverage large language models pre-trained with biomedical text to inject
medical domain knowledge. biomedical language model utilization."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"those 2 steps connect text and imaging (sect. 3.1). however, the potential of biomedical text information in med-
ical imaging applications has not been explicitly addressed. in our eﬀorts, we
emphasize the importance of utilizing biomedical language models for adapting
foundational vision models into cancer pathological analysis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"importantly, we introduce two low-cost sets of trainable parameters to the
vision encoder in order to adapt the model with the guidance of text infor-
mation. they are (1) prompt tokens in the input space to model task-speciﬁc
information, and (2) a projection layer in the latent space to align image and text
embeddings. during model adaptation, we freeze the pre-trained encoders and
only tune the introduced parameters, which not only saves remarkable training
data and computational resources but also makes our approach favorable with
various foundation model architectures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"(4)
during adaptation, the extra parameters are updated by minimizing the
cross-entropy of the predictions from eq. (3) and the ground truth labels. 3.2
learning visual prompt
medical concepts exhibit a great visual distribution shift from natural images,
which becomes impractical for a ﬁxed vision encoder to capture task-speciﬁc
information in few-shot scenarios."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"the prompt parameters are
updated together with the projection layer introduced in sect. 3.1.
4
experimental settings
dataset. we adopt the patchgastric [25] dataset, which includes histopatho-
logical image patches extracted from h&e stained whole slide images (wsi)
of stomach adenocarcinoma endoscopic biopsy specimens."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"there are 262,777
patches of size 300 × 300 extracted from 991 wsis at x20 magniﬁcation. the
dataset contains 9 subtypes of gastric adenocarcinoma. we choose 3 major sub-
types including “well diﬀerentiated tubular adenocarcinoma”, “moderately diﬀer-
entiated tubular adenocarcinoma”, and “poorly diﬀerentiated adenocarcinoma”
to form a 3-class grading-like classiﬁcation task with 179,285 patches from 693
wsis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"we randomly split the wsis into train (20%) and validation (80%) sub-
sets for measuring the model performance. to extend our evaluation into the
real-world setting with insuﬃcient data, we additionally choose 1, 2, 4, 8, or 16
wsis with the largest numbers of patches from each class as the training set. the evaluation metric is patient-wise accuracy, where the prediction of a wsi
is obtained by a soft vote over the patches, and accuracy is averaged class-wise.
implementation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"prompt length p is set to 1. we resize the images to 224×224
to ﬁt the model and follow the original data pipeline in patchgastric [25]. a
class-balanced sampling strategy is adopted by choosing one image from each
class in turn."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"[5] backbone. averaged results and standard deviation (error
bars) of 3 runs are displayed. our cite consistently outperforms all baselines under
all data fractions, showing a remarkable improvement under data deﬁciency. 5
results
cite consistently outperforms all baselines under all data scales."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"(3) fine-tune: train a classiﬁcation head together with the backbone encoder. [18]: apply an attention network on image features to predict pseudo
labels and cluster the images. (5) zero-shot [5]: classify images to the nearest text
embeddings obtained by class names, without training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"note
that clip vit-b/16 vision encoder is adopted as the backbone for (2)–(7). our
cite outperforms all baselines that require training classiﬁcation heads, as well
as image feature clustering methods, demonstrating the key beneﬁt of leveraging
additional biomedical text information for pathological image classiﬁcation. cite shows a favorable improvement when data is scarce."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"when only
one training slide per class is available, cite achieves a remarkable performance,
outperforming all baselines by a signiﬁcant margin (from 51.4% to 60.2%). as
data deﬁciency is commonly seen in medical tasks, cite presents an appealing
property to handle data-limited pathological analysis. together, our ﬁndings
text-guided foundation model adaptation for medical image classiﬁcation
279
table 1. ablation study of cite with and without prompt and text."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"for each combination, cite consistently outperforms linear and ﬁne-tune baselines. visual prompt and text information are both necessary. we conduct
ablation studies to show the eﬀectiveness of visual prompt learning and text
information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"from the results in table 1, we demonstrate that visual prompt
learning outperforms ﬁne-tuning as the adaptation method, and in-domain text
information outperforms classiﬁcation heads. combining the two components
yields the best results under all data scales. importantly, text information is
particularly eﬀective when training data is extremely scarce (1 slide per class)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_27.pdf,"the results demonstrate that cite is compatible with a
variety of pre-trained models, making it immune to upstream model modiﬁca-
tions. the text information encoded in biomedical language models allows vision
280
y. zhang et al.
models pre-trained with natural imaging to bridge the domain gap without task-
speciﬁc pre-training on medical imaging. importantly, when using both the vision
and language encoders of clip vit-b/16, our approach still outperforms the
baselines by a remarkable margin (47.7% to 60.1%), demonstrating the impor-
tance of multi-modal information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"we
ﬁnd that the ability of classiﬁers to separate individuals into subgroups
varies substantially across medical imaging modalities and protected char-
acteristics; crucially, we show that this property is predictive of algorith-
mic bias. through theoretical analysis and extensive empirical evalua-
tion (code is available at https://github.com/biomedia-mira/subgroup-
separability), we ﬁnd a relationship between subgroup separability, sub-
group disparities, and performance degradation when models are trained
on data with systematic bias such as underdiagnosis. our ﬁndings shed new
light on the question of how models become biased, providing important
insights for the development of fair medical imaging ai.
1
introduction
medical image computing has seen great progress with the development of deep
image classiﬁers, which can be trained to perform diagnostic tasks to the level of
skilled professionals [19]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"an often overlooked aspect of this problem is subgroup separability: the ease
with which individuals can be identiﬁed as subgroup members. some medical
images encode sensitive information that models may leverage to classify indi-
viduals into subgroups [7]. however, this property is unlikely to hold for all
modalities and protected characteristics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"a more realistic premise is that sub-
group separability varies across characteristics and modalities. we may expect
groups with intrinsic physiological diﬀerences to be highly separable for deep
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 18. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14222, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"we show that the ability of
models to detect which group an individual belongs to varies across modalities
and groups in medical imaging and that this property has profound consequences
for the performance and fairness of deep classiﬁers. to the best of our knowledge,
ours is the ﬁrst work which analyses group-fair image classiﬁcation through the
lens of subgroup separability. our contributions are threefold:
– we demonstrate empirically that subgroup separability varies across real-
world modalities and protected characteristics."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"– we show theoretically that such diﬀerences in subgroup separability aﬀect
model bias in learned classiﬁers and that group fairness metrics may be inap-
propriate for datasets with low subgroup separability. – we corroborate our analysis with extensive testing on real-world medical
datasets, ﬁnding that performance degradation and subgroup disparities are
functions of subgroup separability when data is biased. 2
related work
group-fair image analysis seeks to mitigate performance disparities caused by
models exploiting sensitive information."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"[20] highlighted that classiﬁcation models trained through erm underdiag-
nose historically underserved population subgroups. follow-up work has addi-
tionally shown that these models may use sensitive information to bias their
predictions [7,8]. unfortunately, standard bias mitigation methods from com-
puter vision, such as adversarial training [1,14] and domain-independent training
[24], are unlikely to be suitable solutions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"[26] showed that bias mitigation methods worsen
performance for all groups compared to erm, giving a stark warning that blindly
applying methods and metrics leads to a dangerous ‘levelling down’ eﬀect [16]. one step towards overcoming these challenges and developing fair and perfor-
mant methods is understanding the circumstances under which deep classiﬁers
learn to exploit sensitive information inappropriately. today, our understanding
of this topic is limited."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"closely related to our work is oakden-rayner et al., who
consider how ‘hidden stratiﬁcation’ may aﬀect learned classiﬁers [18]; similarly,
jabbour et al. use preprocessing ﬁlters to inject spurious correlations into chest
x-ray data, ﬁnding that erm-trained models are more biased when the corre-
lations are easier to learn [12]. outside of fairness, our work may have broader
impact in the ﬁelds of distribution shift and shortcut learning [6,25], where many
examples exist of models learning to exploit inappropriate spurious correlations
[3,5,17], yet tools for detecting and mitigating the problem remain immature. → [0, 1] the
underlying mapping between images and class labels. suppose we have access to a
(biased) training dataset, where ptr is the conditional distribution between train-
ing images and training labels; we say that such a dataset is biased if ptr ̸= p.
we focus on group fairness, where each individual belongs to a subgroup a ∈ a
and aim to learn a fair model that maximises performance for all groups when
deployed on an unbiased test dataset drawn from p. we assume that the groups
are consistent across both datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"formally, group a = a∗ is said to
be underdiagnosed if it satisﬁes eq. ≤ p(y|x+, a∗) and ∀a ̸= a∗, ptr(y|x+, a) = p(y|x+, a)
(1)
we may now use the law of total probability to express the overall mapping
from image to label in terms of the subgroup-wise mappings in eq. together
with eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"(1), this implies eq. (3) – the probability of a truly positive individual
being assigned a positive label is lower in the biased training dataset than for
the unbiased test set. since this model approximates the biased
training distribution, we may expect underdiagnosis from the training data to be
reﬂected by the learned model when evaluated on the unbiased test set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"(2), notice that the prediction for any individual is a linear
combination of the mappings for each subgroup, weighted by the probability the
individual belongs to each group. when subgroup separability is high due to the
presence of sensitive information, the model will learn a diﬀerent mapping for
each subgroup, shown in eq. this model underdiagnoses group
a = a∗ whilst recovering the unbiased mapping for other groups."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"in such sit-
uations, we expect performance degradation to be uniform across groups and
thus not be detected by group fairness metrics. the severity of the degrada-
tion depends on both the proportion of corrupted labels in the underdiagnosed
subgroup and the size of the underdiagnosed subgroup in the dataset. tpr(b)
a
≈ |ptr(y|x+, a) > 0.5|
n+,a
≤ |p(y|x+, a) > 0.5|
n+,a
≈ tpr(u)
a , ∀a ∈ a
(10)
we have derived the eﬀect of underdiagnosis bias on classiﬁer performance
for the two extreme cases of high and low subgroup separability."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"in practice,
subgroup separability for real-world datasets may vary continuously between
these extremes. 4, we empirically investigate (i) how subgroup separa-
bility varies in the wild, (ii) how separability impacts performance for each group
when underdiagnosis bias is added to the datasets, (iii) how models encode sen-
sitive information in their representations. subgroup separability in medical image classiﬁcation
183
4
experiments and results
we support our analysis with experiments on ﬁve datasets adapted from a subset
of the medfair benchmark [27]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"we treat each dataset as a binary classiﬁca-
tion task (no-disease vs disease) with a binary subgroup label. for datasets with
multiple sensitive attributes available, we investigate each individually, giving
eleven dataset-attribute combinations. we record
summary statistics for the datasets used in the supplementary material (table
a1), where we also provide access links (table a2)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"subgroup separability in the real world
we begin by testing the premise of this article: subgroup separability varies
across medical imaging settings. to measure subgroup separability, we train
binary subgroup classiﬁers for each dataset-attribute combination. we use test-
set area under receiver operating characteristic curve (auc) as a proxy for
separability, reporting results over ten random seeds in table 1.
table 1. separability of protected subgroups in real-world datasets, measured by test-
set auc of classiﬁers trained to predict the groups."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"some patterns are immediately noticeable from table 1. all attributes can
be predicted from chest x-ray scans with > 0.9 auc, implying that the modal-
ity encodes substantial information about patient identity. age is consistently
well predicted across all modalities, whereas separability of biological sex varies,
184
c. jones et al.
with prediction of sex from fundus images being especially weak."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"importantly,
the wide range of auc results [0.642 → 0.986] across the dataset-attribute com-
binations conﬁrms our premise that subgroup separability varies substantially
across medical imaging applications. performance degradation under label bias
we now test our theoretical ﬁnding: models are aﬀected by underdiagnosis dif-
ferently depending on subgroup separability. we inject underdiagnosis bias into
each training dataset by randomly mislabelling 25% of positive individuals in
group 1 (see table 1) as negative."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"for each dataset-attribute combination, we
train ten disease classiﬁcation models with the biased training data and ten mod-
els with the original clean labels; we test all models on clean data. we assess
how the test-time performance of the models trained on biased data degrades
relative to models trained on clean data. we illustrate the mean percentage point
accuracy degradation for each group in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"and use the mann-whitney u test
(with the holm-bonferroni adjustment for multiple hypothesis testing) to deter-
mine if the performance degradation is statistically signiﬁcant at pcritical = 0.05. we include an ablation experiment over varying label noise intensity in fig. 1. percentage-point degradation in accuracy for disease classiﬁers trained on
biased data, compared to training on clean data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"lower values indicate worse per-
formance for the biased model when tested on a clean dataset. results are reported
over ten random seeds, and bars marked with ∗ represent statistically signiﬁcant results.
dataset-attribute combinations are sorted by ascending subgroup separability. our results in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"1 are consistent with our analysis in sect. 3. we report
no statistically signiﬁcant performance degradation for dataset-attribute combi-
nations with low subgroup separability (<0.9 auc). in these experiments, the
proportion of mislabelled images is small relative to the total population; thus,
the underdiagnosed subgroups mostly recover from label bias by sharing the
subgroup separability in medical image classiﬁcation
185
correct mapping with the uncorrupted group."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"as subgroup separabil-
ity increases, performance degrades more for the underdiagnosed group (group
1), whilst performance for the uncorrupted group (group 0) remains somewhat
unharmed. we see a statistically signiﬁcant performance drop for group 0 in the
mimic-sex experiment – we believe this is because the model learns separate
group-wise mappings, shrinking the eﬀective size of the dataset for group 0.
use of sensitive information in biased models
finally, we investigate how biased models use sensitive information. [7,8] to all
models trained for the previous experiment, involving freezing the trained back-
bone and re-training the ﬁnal layer to predict the sensitive attribute."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"2, plotting it against subgroup separability auc
from table 1 and using kendall’s τ statistic to test for a monotonic association
between the results (pcritical = 0.05). we ﬁnd that models trained on biased data
learn to encode sensitive information in their representations and see a statisti-
cally signiﬁcant association between the amount of information available and the
amount encoded in the representations. models trained on unbiased data have
no signiﬁcant association, so do not appear to exploit sensitive information.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"2. auc of the split test for sensitive information encoded in learned representa-
tions, plotted against subgroup separability. along the maximum sensitive information
line, models trained for predicting the disease encode as much sensitive information in
their representations as the images do themselves. 186
c. jones et al.
5
discussion
we investigated how subgroup separability aﬀects the performance of deep neural
networks for disease classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"we discuss four takeaways from our study:
subgroup separability varies substantially in medical imaging. in fairness liter-
ature, data is often assumed to contain suﬃcient information to identify indi-
viduals as subgroup members. but what if this information is only partially
encoded in the data?"
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"by testing eleven dataset-attribute combinations across
three medical modalities, we found that the ability of classiﬁers to predict sen-
sitive attributes varies substantially. our results are not exhaustive – there are
many modalities and sensitive attributes we did not consider – however, by
demonstrating a wide range of separability results across diﬀerent attributes and
modalities, we highlight a rarely considered property of medical image datasets. performance degradation is a function of subgroup separability."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"we showed,
theoretically and empirically, that the performance and fairness of models trained
on biased data depends on subgroup separability. when separability is high,
models learn to exploit the sensitive information and the bias is reﬂected by stark
subgroup diﬀerences. when separability is low, models cannot exploit sensitive
information, so they perform similarly for all groups."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"in our experiments, we injected underdiagnosis bias
into the training set and treated the uncorrupted test set as an unbiased ground
truth. however, this is not an endorsement of the quality of the data. this pre-existing bias will likely have
a smaller eﬀect size than our artiﬁcial bias, so it should not play a signiﬁcant
role in our results."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"future work should investigate how subgroup separability
interacts with other sources of bias. we renew the call for future datasets to be
released with patient metadata and multiple annotations to enable analysis of
diﬀerent sources and causes of bias.
reproducibility and impact. this work tackles social and technical problems in
machine learning for medical imaging and is of interest to researchers and prac-
titioners seeking to develop and deploy medical ai."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"given the sensitive nature of
this topic, and its potential impact, we have made considerable eﬀorts to ensure
full reproducibility of our results. all datasets used in this study are publicly
available, with access links in table a2. we provide a complete implementation
of our preprocessing, experimentation, and analysis of results at https://github.
com/biomedia-mira/subgroup-separability."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"in: leal-taix´e, l., roth, s.
(eds.) eccv 2018. potential sources of dataset bias complicate
investigation of underdiagnosis by machine learning algorithms. detecting and
preventing shortcut learning for fair medical ai using shortcut testing (short)
4. castro, d.c., walker, i., glocker, b.: causality matters in medical imaging."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"ebiomedicine 89
(2023). https://doi.org/10.1016/j.ebiom.2023.104467
9. groh, m., harris, c., daneshjou, r., badri, o., koochek, a.: towards transparency
in dermatology image datasets with skin tone annotations by experts, crowds,
and an algorithm. acm hum.-comput. interact."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"in: proceedings of the ieee/cvf
conference on computer vision and pattern recognition, pp. 1820–1828 (2021)
11. irvin, j., et al.: chexpert: a large chest radiograph dataset with uncertainty labels
and expert comparison. in: proceedings of the aaai conference on artiﬁcial intel-
ligence, vol."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"in: proceedings of the
machine learning for healthcare conference, pp. johnson, a.e.w., et al.: mimic-cxr, a de-identiﬁed publicly available database
of chest radiographs with free-text reports. kim, b., kim, h., kim, k., kim, s., kim, j.: learning not to learn: training deep
neural networks with biased data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"in: proceedings of the ieee/cvf conference
on computer vision and pattern recognition, pp. 9012–9020 (2019)
188
c. jones et al.
15. kovalyk, o., morales-s´anchez, j., verd´u-monedero, r., sell´es-navarro, i., palaz´on-
cabanes, a., sancho-g´omez, j.l.: papila: dataset with fundus images and clini-
cal data of both eyes of the same patient for glaucoma assessment. https://doi.org/10.1038/s41597-022-01388-1
16. mittelstadt, b., wachter, s., russell, c.: the unfairness of fair machine learning:
levelling down and strict egalitarianism by default, january 2023
17."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_18.pdf,"https://doi.org/10.1145/3368555.3384468
19. rajpurkar, p., et al.: chexnet: radiologist-level pneumonia detection on chest x-
rays with deep learning, november 2017
20. seyyed-kalantari, l., zhang, h., mcdermott, m.b., chen, i.y., ghassemi, m.:
underdiagnosis bias of artiﬁcial intelligence algorithms applied to chest radiographs
in under-served patient populations. tschandl, p., rosendahl, c., kittler, h.: the ham10000 dataset, a large collection
of multi-source dermatoscopic images of common pigmented skin lesions. vapnik, v.: an overview of statistical learning theory."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"to address this
issue, we establish a framework to adjust cnns to “think like sonog-
raphers” for gout diagnosis, which consists of three novel components:
(1) where to adjust: modeling sonographers’ gaze map to emphasize
the region that needs adjust; (2) what to adjust: classifying instances
to systematically detect predictions made based on unreasonable/biased
reasoning and adjust; (3) how to adjust: developing a training mecha-
nism to balance gout prediction accuracy and attention reasonability for
improved cnns. the experimental results on clinical mskus datasets
demonstrate the superiority of our method over several sota cnns. [7].
however, misdiagnosis of gout can occur frequently when a patient’s clinical
characteristics are atypical."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"traditional mskus diagnosis relies on the experi-
ence of the radiologist which is time-consuming and labor-intensive. although
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 16.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp. 159–168, 2023. https://doi.org/10.1007/978-3-031-43987-2_16
160
z. cao et al.
convolutional neural networks (cnns) based ultrasound classiﬁcation models
have been successfully used for diseases such as thyroid nodules and breast can-
cer, conspicuously absent from these successful applications is the use of cnns
for gout diagnosis from mskus images.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"due to these issues, sota cnn models often fail to learn the gouty
mskus features which are key factors for sonographers’ decision. in medical image analysis, recent works have attempted to inject the recorded
gaze information of clinicians into deep cnn models for helping the models to
predict correctly based on lesion area. mall et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"cai et al. [1] model,
which integrates eye-gaze data of sonographers and used generative adversarial
networks to address the lack of eye-gaze data. patra et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"[11] proposed the use
of a teacher-student knowledge transfer framework for us image analysis, which
combines doctor’s eye-gaze data with us images as input to a large teacher
model, whose outputs and intermediate feature maps are used to condition a
student model. although these methods have led to promising results, they can
be diﬃcult to implement due to the need to collect doctors’ eye movement data
for each image, along with certain restrictions on the network structure. thinking like sonographers
161
diﬀerent from the existing studies, we propose a novel framework to adjust
the general cnns to “think like sonographers” from three diﬀerent levels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"1) where to adjust: we model
the sonographers’ gaze map to emphasize the region that needs control. this part
learns the eye gaze information of the sonographers which is collected by the eye-
tracker. 2) what to adjust: we divide instances into four categories to reﬂect
whether the model prediction given to the instance is reasonable and precise."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"3)
how to adjust: a training mechanism is developed to strike the balance between
gout diagnosis and attention accuracy for improving cnn.
2.1
where to adjust
it is essential to obtain the gaze map corresponding to each mskus to emphasize
the region where gouty features are obvious. [8], we integrate transformer into cnns to capture multi-scale and long-
range contextual visual information for modeling sonographers’ gaze map. this
gaze map learns the eye gaze information, collected by the eye-tracker, of the
sonographers when they perform diagnosis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"the mskus image i0 ∈ rh×w ×3 is ﬁrst input into cnn encoder that con-
tains ﬁve convolution blocks. the output feature maps from the deeper last three
convolution blocks are denoted as f0, f1, f2 and are respectively fed into trans-
former encoders to enhance the long-range and contextual information. during
the transformer-encoder, we ﬁrst ﬂatten the feature maps produced by the cnn
encoder into a 1d sequence."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"after ﬁve cnn blocks, a 3 × 3 convolution operation and sigmoid acti-
vation is performed to output the predicted sonographers’ gaze map. we use the
eye gaze information of the sonographers which is collected by the eye-tracker
to restrain the predicted sonographers’ gaze map. [2].
2.2
what to adjust
common cnn classiﬁcation models for gout diagnosis often fail to learn the
gouty mskus features including the double contour sign, tophus, and snow-
storm which are key factors for sonographers’ decision."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"for sample in rp and
uip, α can be set 0.5 to strike the balance between accuracy and reasonability. 3
experiments
mskus dataset collection. the mskus data were collected for patients
suspected of metatarsal gout in nanjing drum tower hosptial."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"informed written
consent was obtained at the time of recruitment. dataset totally contains 1127
us images from diﬀerent patients including 509 gout images and 618 healthy
images. the resolution of the mskus images were resized to 224 × 224."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"we used 5-fold cross validation to divide the training sets and
validation sets. gaze data collection. we collected the eye movement data with the tobii 4c
eye-tracker operating at 90 hz."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"sonographers were seated in front of the screen
and free to adjust the chair’s height and the display’s inclination. binary maps
of the same size as the corresponding mskus images were generated using the
gaze data, with the pixel corresponding to the point of gaze marked with a’1’
and the other pixels marked with a’0’. a sonographer gaze map s was generated
for each binary map by convolving it with a truncated gaussian kernel g(σx,y),
where g has 299 pixels along x dimension, and 119 pixels along y dimension."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_16.pdf,"consequently, it was possible to
use predicted gaze maps for both the training and testing phases of the classiﬁca-
tion models without any notable performance decrease. this removed the need to
collect eye movement maps during the training and testing phases, signiﬁcantly
lightening the workload of data collection. therefore, our tls mechanism, which
involved predicting the gaze maps, could potentially be used in clinical environ-
ments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"dynamic contrast-enhanced ultrasound (ceus) video with
microbubble contrast agents reﬂects the microvessel distribution and
dynamic microvessel perfusion, and may provide more discriminative
information than conventional gray ultrasound (us). thus, ceus video
has vital clinical value in diﬀerentiating between malignant and benign
thyroid nodules."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"in this paper, we propose a novel framework to diagnose
thyroid nodules based on dynamic ceus video by considering microves-
sel inﬁltration and via segmented conﬁdence mapping assists diagnosis. speciﬁcally, the temporal projection attention (tpa) is proposed to
complement and interact with the semantic information of microvessel
perfusion from the time dimension of dynamic ceus. in addition, we
employ a group of conﬁdence maps with a series of ﬂexible sigmoid
alpha functions (saf) to aware and describe the inﬁltrative area of
microvessel for enhancing diagnosis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"therefore, consid-
eration of dynamic microvessel perfusion and inﬁltrative expansion is
helpful for ceus-based diagnosis and segmentation of thyroid nodules. the datasets and codes will be available. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 17.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"furthermore, by combing the us modality, chen et al. [5] proposed a
domain-knowledge-guided temporal attention module for breast cancer diagno-
sis. however, due to artifacts in ceus, sota classiﬁcation methods often fail
to learn regions where thyroid nodules are prominent (as in appendix fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"in particular, few studies have developed the
ceus video based diagnostic model inspired by the dynamic microvessel per-
fusion, or these existing methods generally ignore the inﬂuence of microvessel
inﬁltrative expansion. whether the awareness of inﬁltrative area information can
be helpful in the improvement of diagnostic accuracy is still unexplored. here, we propose an explanatory framework for the diagnosis of thyroid nod-
ules based on dynamic ceus video, which considers the dynamic perfusion
characteristics and the ampliﬁcation of the lesion region caused by microves-
sel inﬁltration."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"our contributions are twofolds. first, the temporal projection
attention (tpa) is proposed to complement and interact with the semantic
information of microvessel perfusion from the time dimension. second, we adopt
a group of conﬁdence maps instead of binary masks to perceive the inﬁltrative
expansion area from gray us to ceus of microvessels for improving diagnosis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"[fmuti, fmuti]
(1)
where ωiden, ωcls are the learnable weights. 2.1
temporal-based lesions area recognition (tlar)
the great challenge of automatic recognition of lesion area from ceus video is
that the semantic information of the lesion area is diﬀerent in the ceus video of
the diﬀerent microvessel perfusion periods. especially in the perfusion period and
the regression period, the semantic information of lesions cannot be fully depicted
in an isolated ceus frame."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"here, v ∈ rc×t × h
16 × w
16 is obtained by a
single convolution. this operation can also ﬁlter out the irrelevant background
thyroid nodule diagnosis in dynamic ceus via mia
173
and display the key information of the lesions. after the temporal projection,
a group convolution with a group size of 4 is employed on k to extract the
local temporal attention l ∈ rc× h
16 × w
16 ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"(3)
where gonv(·) is the group convolution, σ denotes the normalization, “⊕” is the
concatenation operation. the global attention g encodes not only the contextual
information within isolated query-key pairs but also the attention inside the
keys [12]. then, we use
parallel average pooling and full connection operation to reweight the channel
information of f ′′
4th to obtain the reweighted feature f ′
4th ∈ rc×t × h
16 × w
16 ."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"ˆpn} to
aware the microvessel inﬁltration for thyroid nodules diagnosis. based on saf
and ipo, ceus-based diagnosis of thyroid nodules can make full use of the
ambiguous information caused by microvessel inﬁltration. sigmoid alpha function (saf)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"then, ˆp1 is contacted with f0, and used to generate optimized proba-
bility map ˆp2 through the continuous operation based on saf and the second
optimize layer of ipo unit. the optimized probability map ˆpi−1 provides prior
information for producing the next probability map ˆpi. in this way, we can get
a group of probability map ˆp to aware the microvascular inﬁltration."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"2. we use the mean square
error lmse to constrain the generation of ˆp. assuming that the generated ˆp
is ready to supervise the classiﬁcation network, we want to ensure that the
probability maps can accurately reﬂect the classiﬁcation conﬁdence. thus, we
design a task focus loss lta to generate conﬁdence maps p, as follows:
lmse =
n

i=1
1
ω

p∈ω
∥gi(pi), ˆpi(pi)∥2
(8)
thyroid nodule diagnosis in dynamic ceus via mia
175
lta =
1
2σ2
n

i=1
∥pi − pi∥2
2 + log σ
(9)
where gi is the label of ˆpi, which is generated by the operation of saf(d(i,j), αi);
pi denotes pixel in the image domain ω, σ is a learnable parameter to eliminate
the hidden uncertainty information. for diﬀerentiating malignant and benign, we employ a hybrid loss ltotal that
consists of the cross-entropy loss lcls, the loss of lmse computing optimized
probability maps ˆp, and task focus loss lta."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"as
the weight parameter, we set λ1, λ2, λ3 are 0.5,0.2,0.3 in the experiments. 3
experiments
dataset. our dataset contained 282 consecutive patients who underwent thy-
roid nodule examination at nanjing drum tower hospital."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"on the other hand, a sonographer with more than 10 years of experience
manually annotated the nodule lesion mask to obtain the pixel-level ground-
truth of thyroid nodules segmentation. all data were approved by the institu-
tional review board of nanjing drum tower hospital, and all patients signed
the informed consent before enrollment into the study. our network was implemented using pytorch frame-
work with the single 12 gb gpu of nvidia rtx 3060."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"here, we set batch-size to 4 during the entire training process
the ceus consisted the full wash-in and wash-out phases, and the resolution
of each frame was (600 × 800). in addition, we carried out data augmentation,
including random rotation and cropping, and we resize the resolution of input
table 1. quantitative lesion recognition results are compared with sota methods
and ablation experiments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_17.pdf,"as in table 1, we compared our method with sota
method including v-net, unet3d, transunet. for the task of identifying lesions,
the index of recall is important, because information in irrelevant regions can
be discarded, but it will be disastrous to lose any lesion information. v-net
achieved the highest recall scores compared to others; thus, it was chosen as the
backbone of tlar."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"medical education is essential for providing the best patient
care in medicine, but creating educational materials using real-world
data poses many challenges. for example, the diagnosis and treatment
of a disease can be aﬀected by small but signiﬁcant diﬀerences in med-
ical images; however, collecting images to highlight such diﬀerences is
often costly."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"therefore, medical image editing, which allows users to
create their intended disease characteristics, can be useful for educa-
tion. however, existing image-editing methods typically require manu-
ally annotated labels, which are labor-intensive and often challenging to
represent ﬁne-grained anatomical elements precisely. herein, we present
a novel algorithm for editing anatomical elements using segmentation
labels acquired through self-supervised learning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"evaluation by ﬁve expert physicians demon-
strated that the edited images appeared natural as medical images and
that the disease characteristics were accurately reproduced. keywords: image editing · self-supervised segmentation · education
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0_38.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14221, pp. 1. editing of anatomical elements."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"trainee physicians require several
years of experience with a diverse range of clinical cases to develop suﬃcient
skills and expertise. however, designing educational materials solely based on
real-world data poses several challenges. for example, although small but signif-
icant disease characteristics (e.g., depth of cancer invasion) can sometimes alter
diagnosis and treatment, collecting pairs with and without these characteristics
is cumbersome."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"latent space manipulation generates images by controlling
latent feature axes [4,14], but the editable attributes are often global rather than
ﬁne-grained. conditional generation can precisely edit image content by using
class or segmentation labels. [15] or
virtual models [18], which are labor-intensive."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"here, we propose a novel framework for image editing called u3-net that
allows the generation of anatomical elements with precise conditions. the core
technique is self-supervised segmentation, which aims to achieve pixel-wise clus-
tering without manually annotated labels [6,7]. as shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"these synthetic images can help trainees intu-
itively comprehend clinically signiﬁcant ﬁndings and alleviate privacy concerns. five expert physicians evaluated the edited images from a clinical perspective
using two datasets: a pelvic mri dataset and chest ct dataset. contributions: our contributions are as follows:
– we propose a novel image-editing algorithm, u3-net, to synthesize images
for medical education via self-supervised segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"[12] on an nvidia tesla a100
gpu running cuda 10.2. [13] (see supplementary information for details). the pelvic mri dataset with rectal cancer contained 289 image series for train-
ing and 100 image series for testing."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"for each image series, the min-max nor-
malization converted the pixel values to [−1, 1]. the chest ct dataset with lung
cancer contained 500 image series for training and 100 image series for testing. the ct values in the range [−2048, 2048] were normalized to [−1, 1]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"appropri-
ate transformations were selected from six candidate functions: t1, random
horizontalflip, t2, randomaffine, t3, colorjitter, t4, randomgaussianblur,
t5, randomposterize, t6, randomgaussiannoise. because anatomical elements,
including the substructures of organs and diseases, are too detailed for human
annotators to segment, it was diﬃcult to create ground-truth labels. therefore,
the training conﬁguration was selected based on the consensus of two expert
radiologists with domain knowledge."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"by comparing diﬀerent settings on the
pelvic mri training dataset (see supplementary information), the number
of segmentation classes of 10, the combination of t1, t2, and t3 with moderate
magnitude, the weakly imposed reconstruction loss, and a certain value of the
margin parameter were considered suitable for self-supervised segmentation. a similar conﬁguration was applied
to the chest ct training dataset. the resultant segmentation maps are shown
in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"these anatomical elements may be too detailed for humans to
annotate, demonstrating the necessity of self-supervised segmentation for high-
precision medical-image editing. evaluation of the synthesized images: we measured the quality of image
reconstruction using mean square error (mse), structural similarity (ssim), and
peak signal-to-noise ratio (psnr). the mean ± standard deviations of mse,
410
k. kobayashi et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"4. results of the image segmentation and editing. the segmentation maps
were well aligned with the anatomical elements in both (a) the pelvic mri and (b)
the chest ct testing datasets. (c) a synthetic image generated by editing the testing
image with the caption, “axial t2-weighted mr image shows a tumor approximately
4 cm in size on the dorsal wall of the rectum."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_39.pdf,"first, we tested whether the evaluators could identify
real or synthesized images from 20 images, which include ten real images and ten
synthesized images. the accuracies (i.e., the ratio of images correctly identiﬁed
as real or synthetic) were 0.69 ± 0.11 and 0.65 ± 0.11, for the pelvic mri and
chest ct testing datasets, respectively. note that when the synthetic images
cannot be distinguished at all, the accuracy should be 0.5."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"speciﬁcally, two convolutional neural networks
were trained jointly to encode image features in mri and us scans to
help match the us image patch that contain the corresponding land-
marks in the mri. we developed and validated the technique using the
public resect database. with a mean landmark detection accuracy
of 5.88±4.79 mm against 18.78±4.77 mm with sift features, the pro-
posed method oﬀers promising results for mri-us landmark detection
in neurosurgical applications for the ﬁrst time."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"moreover,
inter- and intra-rater variability still exists. these factors make quality assess-
ment of brain shift correction for us-guided brain tumor resection challenging. in addition, due to the time constraints, similar evaluation of inter-modal regis-
tration quality during surgery is nearly impossible, but still highly desirable."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"to
address these needs, deep learning (dl) holds the promise to perform eﬃcient
and automatic inter-modal anatomical landmark detection. previously, many groups have proposed algorithms to label landmarks in
anatomical scans [4–9]. however, almost all earlier techniques were designed for
mono-modal applications, and inter-modal landmark detection, such as for us-
guided brain tumor resection, has rarely been attempted."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"second, cl is employed for the ﬁrst time in inter-modal anatomical landmark
detection. we developed and validated the proposed technique with the public
resect database [10] and compared its landmark detection accuracy against
the popular scale-invariant feature transformation (sift) algorithm in 3d [11]. 670
s. salari et al.
2
related work
contrastive learning has recently shown great results in a wide range of medical
image analysis tasks [12–18]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"to date, cl has not been explored in
multi-modal landmark detection, a unique problem in clinical applications. in
this paper, to bridge this knowledge gap, we proposed a novel cl-based frame-
work for mri-us anatomical landmark detection. [10] (https://archive.sigma2.no/pages/
public/dataset detail.jsf?id=10.11582/2020.00025) to train and evaluate our
proposed method."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"2: the proposed cnn for feature encoding from mri and us scans. 3.3
landmark matching with a 2.5d approach
working with 3d images is computationally expensive and can make the model
training unstable and prone to overﬁtting, especially when the size of the
database is limited. therefore, instead of a full 3d processing, we decided to
implement a 2.5d approach [25] to leverage the eﬃciency of 2d cnn in the
672
s. salari et al.
cl framework for the task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"however, as the sift algorithm pre-selects keypoint candidates based
on their feature strengths, with this in consideration, we saw no major beneﬁts
by imposing the spatial constraint. towards multi-modal anatomical landmark detection
673
4
experimental setup
4.1
data preprocessing
for cl training, both positive and negative sample pairs need to be created. all
2d patch series were extracted according to sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"here, fθ ◦ xa
i , gβ ◦ xp
i ,
and gβ ◦ xn
j
give the extracted feature vectors for mr and us patches. 4.3
implementation details and evaluation
to train our dl model, we made subject-wise division of the entire dataset into
70%:15%:15% as the training, validation, and testing sets, respectively. also,
to improve the robustness of the network, we used data augmentation for the
training data by random rotation, random horizontal ﬂip, and random vertical
ﬂip."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"furthermore, an adamw optimizer with a learning rate of 0.00001 was used,
and we trained our model for 50 epochs with a batch size of 256. in order to evaluate the performance of our technique, we used the provided
ground truth landmarks from the database and calculated the euclidean distance
between the ground truths and predictions. the utilized metric is as follows:
mean landmark identiﬁcation error = 1
n
n

i=1
∥xi − x′
i∥
(3)
674
s. salari et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"3: an overview of the framework for image feature learning. where xi and x′
i, and n are the ground truth landmark location, model predic-
tion, and the total number of landmarks per subject, respectively.
5
results
table 1 lists the mean and standard deviation of landmark identiﬁcation errors
(in mm) between the predicted position and the ground truth in intra-operative
us for each patient of the resect dataset. in the table, we also provide the
severity of brain shift for each patient."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"first, while the 2.5d approach is memory eﬃcient and quick,
3d approaches may better capture the full corresponding image features. this
is partially reﬂected by the observation that the quality of landmark localiza-
tion is associated with the level of tissue shift. however, due to limited clini-
cal data, 3d approaches caused overﬁtting in our network training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9/paper_64.pdf,"one major critique for using the sift
algorithm is that it intends to ﬁnd geometrically interesting keypoints, which
may not have good anatomical signiﬁcance. in the resect dataset, eligible
anatomical landmarks were deﬁned as deep grooves and corners of sulci, convex
points of gyri, and vanishing points of sulci. the relevant local features may be
676
s. salari et al.
hard to capture with the sift algorithm."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"existing deep learning models have achieved promising per-
formance in recognizing skin diseases from dermoscopic images. however,
these models can only recognize samples from predeﬁned categories, when
they are deployed in the clinic, data from new unknown categories are con-
stantly emerging. therefore, it is crucial to automatically discover and
identify new semantic categories from new data."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"in this paper, we propose
a new novel class discovery framework for automatically discovering new
semantic classes from dermoscopy image datasets based on the knowledge
of known classes. speciﬁcally, we ﬁrst use contrastive learning to learn a
robust and unbiased feature representation based on all data from known
and unknown categories. we then propose an uncertainty-aware multi-
view cross pseudo-supervision strategy, which is trained jointly on all cat-
egories of data using pseudo labels generated by a self-labeling strategy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"finally, we further reﬁne the pseudo label by aggregating neighborhood
information through local sample similarity to improve the clustering per-
formance of the model for unknown categories. we conducted extensive
experiments on the dermatology dataset isic 2019, and the experimen-
tal results show that our approach can eﬀectively leverage knowledge from
known categories to discover new semantic categories. we also further vali-
dated the eﬀectiveness of the diﬀerent modules through extensive ablation
experiments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"keywords: novel class discovery · skin lesion recognition · deep
learning
1
introduction
automatic identiﬁcation of lesions from dermoscopic images is of great impor-
tance for the diagnosis of skin cancer [16,22]. currently, deep learning mod-
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 3.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43987-2_3
towards novel class discovery: a study in novel skin lesions clustering
25
els, especially those based on deep convolution neural networks, have achieved
remarkable success in this task [17,18,22]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"however, this comes at the cost of a
large amount of labeled data that needs to be collected for each class. to allevi-
ate the labeling burden, semi-supervised learning has been proposed to exploit
a large amount of unlabeled data to improve performance in the case of limited
labeled data [10,15,19]. however, it still requires a small amount of labeled data
for each class, which is often impossible in real practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"[7,9,24], which aims to transfer knowledge from known classes to discover new
semantic classes. most ncd methods follow a two-stage scheme: 1) a stage of
fully supervised training on known category data and 2) a stage of clustering
on unknown categories [7,9,24]. for example, han et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"they also used ranking statistics to compute pairwise similarity for clustering.
zhong et al. [21] to further
exploit the information from known classes to improve the performance of unsu-
pervised clustering. fini et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"[23] used neighborhood infor-
mation in the embedding space to learn more discriminative representations. however, most of these methods require the construction of a pairwise similar-
ity prediction task to perform clustering based on pairwise similarity pseudo
labels between samples. in this process, the generated pseudo labels are usually
noisy, which may aﬀect the clustering process and cause error accumulation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"in this paper, we propose a new novel class discovery framework to auto-
matically discover novel disease categories. speciﬁcally, we ﬁrst use contrastive
learning to pretrain the model based on all data from known and unknown cat-
egories to learn a robust and general semantic feature representation. then, we
propose an uncertainty-aware multi-view cross-pseudo-supervision strategy to
perform clustering."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"the cross-pseudo-supervision strategy is then used to force the model to
maintain consistent prediction outputs for diﬀerent views of unlabeled images. in addition, we propose to use prediction uncertainty to adaptively adjust the
contribution of the pseudo labels to mitigate the eﬀects of noisy pseudo labels. finally, to encourage local neighborhood alignment and further reﬁne the pseudo
26
w. feng et al.
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"the overall framework of our proposed novel class discovery algorithm. labels, we propose a local information aggregation module to aggregate the infor-
mation of the neighborhood samples to boost the clustering performance. we
conducted extensive experiments on the dermoscopy dataset isic 2019, and the
experimental results show that our method outperforms other state-of-the-art
comparison algorithms by a large margin."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"in addition, we also validated the
eﬀectiveness of diﬀerent components through extensive ablation experiments. 2
methodology
given an unlabeled dataset {xu
i }n u
i=1 with n u images, where xu
i is the ith unla-
beled image. our goal is to automatically cluster the unlabeled data into cu
clusters."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,", cl
is its
corresponding label. in the novel class discovery task, the known and unknown
classes are disjoint, i.e., cl ∩ cu = ∅. however, the known and unknown classes
are similar, and we aim to use the knowledge of the known classes to help
the clustering of the unknown classes. the overall framework of our proposed
novel class discovery algorithm is shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"speciﬁcally, we ﬁrst learn gen-
eral and robust feature representations through contrastive learning. then, the
uncertainty-aware multi-view cross-pseudo-supervision strategy is used for joint
training on all category data. finally, the local information aggregation module
beneﬁts the ncd by aggregating the useful information of the neighborhood
samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"1 is the indicator
function. [12] for labeled
known category data, which can be denoted as:
lscl
i
= −
1
|n(i)|

q∈n(i)
log
exp (zi · zq/τ)

n 1[n̸=i] exp (zi · zn/τ)
(2)
where n(i) represents the sample set with the same label as xi in a mini-batch
data. |n(i)| represents the number of samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"the overall contrastive loss can be expressed as: lcl = (1 − μ) 
i∈b lucl
i
+
μ 
i∈bl lscl
i , where μ denotes the balance coeﬃcient. bl is the labeled subset
of mini-batch data. uncertainty-aware multi-view cross-pseudo-supervision."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"(3)
the prediction outputs are obtained by concatenating the outputs of the
two classiﬁcation heads and then passing a softmax layer [7]. next, we need to obtain training targets for all data. for an input image
xi, if xi is from the known category, we construct the training target as one
hot vector, where the ﬁrst cl elements are ground truth labels and the last cu
elements are 0."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"if xi is from the unknown category, we set the ﬁrst cl elements
to 0 and use pseudo labels for the remaining cu elements. 28
w. feng et al.
we follow the self-labeling method in [1,3] to generate pseudo labels. ; pu
bu
	
∈ rbu×cu denotes the ensem-
ble prediction of data of unknown categories in a mini-batch, where bu represents
the number of samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"here we only consider the output of the unknown cate-
gories head due to the samples coming from unknown categories [7]. we obtain
the pseudo label by optimizing the following objective:
max
y∈s tr

yp⊤
+ δh(y)
(4)
where y =

yu
1 ; . . . ; yu
bu
	
∈ rbu×cu will assign bu unknown category samples to
cu category prototypes uniformly, i.e., each category prototype will be selected
bu/cu times on average."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"h is the entropy function used
to control the smoothness of y. δ is the hyperparameter. after gener-
ating the pseudo-labels, we can combine them with the ground truth labels of
known categories as training targets for uniform training.
to mitigate the eﬀect of noisy pseudo labels, we propose to use prediction
uncertainty [14] to adaptively adjust the weights of pseudo labels. if the variance of the model’s predictions
for diﬀerent augmented images is large, the pseudo label may be of low quality,
and vice versa."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"yv1 and yv2 are the training targets. local information aggregation. after the cross-pseudo-supervision training
described above, we are able to assign the instances to their corresponding clus-
tering centers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"speciﬁcally, as
shown in fig. 1, we maintain a ﬁrst-in-ﬁrst-out memory bank m = {zm
k , ym
k }n m
k=1
during the training process, which contains the features of n m most recent sam-
ples and their pseudo labels. n m
k=1 dkym
k , where ρ is the balance coeﬃcient."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"by aggregating
the information of the neighborhood samples, we are able to ensure consistency
between local samples, which further improves the clustering performance. 3
experiments
dataset. the dataset contains a total of 25,331 dermoscopic images from
eight categories: melanoma (mel), melanocytic nevus (nv), basal cell carci-
noma (bcc), actinic keratosis (ak), benign keratosis (bkl), dermatoﬁbroma
(df), vascular lesion (vasc), and squamous cell carcinoma (scc)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"we use the sgd optimizer to train the model for 200
epochs with linear warm-up and cosine annealing (lrbase = 0.1, lrmin = 0.001),
and the weight decay is set to 1.5 × 10−4. for data augmentation, we use ran-
dom horizontal/vertical ﬂipping, color jitter, and gaussian blurring following
[7]. for pseudo label, we use the sinkhorn-knopp algorithm with hyperparam-
eters inherited from [7]: δ = 0.05 and the number of iterations is 3."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"the batch
size in all experiments is 512. following [9,23,24], we
report the clustering performance on the unlabeled unknown category dataset. we assume that the number of unknown categories is known and it can also be
obtained by the category number estimation method proposed in [9].
30
w. feng et al.
table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"clustering performance of diﬀerent comparison algorithms on diﬀerent tasks. following [2,9], we use the average clustering accuracy (acc), normalized
mutual information (nmi) and adjusted rand index (ari) to evaluate the clus-
tering performance of diﬀerent algorithms. after
the optimal assignment is determined, we then compute each metric."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"we imple-
ment all algorithms based on the pytorch framework and conduct experiments
on 8 rtx 3090 gpus.
comparison with state-of-the-art methods. we
also compare with the benchmark method (baseline), which ﬁrst trains a model
using known category data and then performs clustering on unknown category
data. table 1 shows the clustering performance of each comparison algorithm on
diﬀerent ncd tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"moreover, the state-of-the-art ncd methods can improve the clustering perfor-
mance, which demonstrates the eﬀectiveness of the currently popular two-stage
solution. however, our method outperforms them, mainly due to the fact that
they need to generate pairwise similarity pseudo labels through features obtained
based on self-supervised learning, while ignoring the eﬀect of noisy pseudo labels. compared with the best comparison algorithm uno, our method yields 5.23%
acc improvement, 3.56% nmi improvement, and 2.55% ari improvement on
task1, and 3.24% acc improvement, 1.34% nmi improvement, and 2.37% ari
improvement on task2, which shows that our method is able to provide more
reliable pseudo labels for ncd."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"we performed ablation exper-
iments to verify the eﬀectiveness of each component. as shown in table 2, cl
is contrastive learning, umcps is uncertainty-aware multi-view cross-pseudo-
supervision, and lia is the local information aggregation module. it can be
observed that cl brings a signiﬁcant performance gain, which indicates that
towards novel class discovery: a study in novel skin lesions clustering
31
table 2. ablation study of each key component."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"in addition, umcps also improves the clustering performance of the
model, which indicates that uniﬁed training helps to the category information
interaction. lia further improves the clustering performance, which indicates
that local information aggregation helps to provide better pseudo labels. finally,
our algorithm incorporates each component to achieve the best performance."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"we further examined the eﬀec-
tiveness of each component in contrastive learning. recall that the contrastive
learning strategy includes supervised contrastive learning for the labeled known
category data and unsupervised contrastive learning for all data. as shown in
table 3, it can be observed that both components improve the clustering perfor-
mance of the model, which indicates that scl helps the model to learn seman-
tically meaningful feature representations, while ucl makes the model learn
robust unbiased feature representations and avoid its overﬁtting to known cate-
gories."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_3.pdf,"we also
examine
the
eﬀectiveness
of
uncertainty-aware
multi-view
cross-pseudo-
supervision. we compare it with 1) w/o cps, which does not use cross-pseudo-
supervision, and 2) cps, which uses cross-pseudo-supervision but not the uncer-
tainty to control the contribution of the pseudo label. as shown in table 3, it can
be seen that cps outperforms w/o cps, which indicates that cps encourages
the model to maintain consistent predictions for diﬀerent augmented versions
32
w. feng et al.
of the input images, and enhances the generalization performance of the model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf,"it is
based on a siamese architecture, including a recurrent neural network
that leverages the ultrasound image features and the optical ﬂow to
estimate the relative position of frames. our method does not use any
additional sensor and was evaluated on ex vivo porcine data. it achieves
translation and orientation errors of 0.449 ± 0.189 mm and 1.3 ± 1.5◦
respectively for the relative pose estimation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf,"in addition, despite the
predominant non-linearity motion in our context, our method achieves
a good reconstruction with ﬁnal and average drift rates of 23.11% and
28.71% respectively. to the best of our knowledge, this is the ﬁrst work
to address volume reconstruction in the context of intravascular ultra-
sound. source code of this work is publicly available at https://github.
com/sidaty1/ivus trakerless volume reconstruction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf,"external us probes are often equipped with
an electromagnetic tracking system to track its position and orientation in real-
time. this information is then used to register the 3d ultrasound image with
the patient’s anatomy. the use of such an electromagnetic tracking system in
laparoscopic surgery is more limited due to size reduction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf,"such motion is pre-
dominant in the context highlighted above and is the source of additional non-
linearity in the pose estimation problem. to the best of our knowledge, this
is the ﬁrst work that provides a clinically sound and eﬃcient 3d us volume
reconstruction during minimally invasive procedures. the paper is organized as
follows: sect."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf,"2 details the method and its novelty, sect. 3 presents our current
results on ex vivo porcine data, and ﬁnally, we conclude in sect. 4 and discuss
future work."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf,"it is written as the mse between the estimated t(1,2k+3) = t(k+2,2k+3)×t(1,k+2)
at the corner points of the frames and the ground truth ˆt(1,2k+3). ||t(1,k+2)− ˆt(1,k+2)||2+||t(k+2,2k+3)− ˆt(k+2,2k+3)||2+||t(1,2k+3)− ˆt(1,2k+3)||2
(3)
3
results and discussion
3.1
dataset and implementation details
to validate our method, six tracked sequences were acquired from an ex vivo
swine liver. a manually manipulated ivus catheter was used (8 fr lateral ﬁring
acunavtm 4–10 mhz) connected to an ultrasound system (acuson s3000
helx touch, siemens healthineers, germany), both commercially available."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf,"first and end stages of the
sequences were removed from the six acquired sequences, as they were considered
to be largely stationary, and aiming to avoid training bias. clips were created by
sliding a window of 7 frames (corresponding to a value of k = 2) with a stride of 1
trackerless volume reconstruction from intraoperative ultrasound images
309
over each continuous sequence, yielding a data set that contains a total of 13734
clips. the tracking was provided for each frame as a 4×4 transformation matrix."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf,"the distribution of the relative transformation between the frames
in our clips is illustrated in the fig. it is clear that our data mostly contains
rotations, in particular over the axis x. heatmaps were calculated for two points
(m = 2) and with a quality level of 0.1, a minimum distance of 7 and a block
size of 7 for the optical ﬂow algorithm (see [4] for more details). the number of
heatmaps m and the frame jump k were experimentally chosen among 0, 2, 4, 6."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf,"the training
process converges in 40 epochs with a batch size of 16. the model with the best
performance on the validation data was selected and used for the testing. the distribution of the relative rotations and translations over the dataset
3.2
evaluation metrics and results
the test data was used to evaluate our method, it contains 2060 clips over
which our method achieved a translation error of ϵtranslation of 0.449 ± 0.189
mm, and an orientation error of ϵorientation 1.3 ± 1.5◦."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_29.pdf,"average drift rate (adr): the average cumulative drift of all frames
divided by the length from the frame to the starting point of the sequence. both state-of-the-art methods
use imu sensor data as additional input to estimate the relative transformation
between two relative frames. due to the diﬃculty of including an imu sensor in
our ivus catheter, the results of both methods were reported from the monet
paper where the models have been trained on arm scans, see [15] for more details."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"in our experiments, superpoint-
e obtains more and better features than any of the baseline detectors
used as supervision. we validate the eﬀectiveness of our model for 3d
reconstruction in real endoscopy data. code and model: https://github.
com/leonbp/superpointtrackingadaptation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"automatic analysis and understanding of these videos raises many
opportunities for novel assistive and automatization tasks on endoscopy proce-
dures. obtaining 3d models from the intracorporeal scenes captured in endo-
scopies is an essential step to enable these novel tasks and build applications,
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 56.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43907-0_56
584
o. l. barbed et al.
for example, for improved monitoring of existing patients or augmented reality
during training or real explorations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"these problems are accentuated when all the elements in
the scene are deformable, as it is the case in most endoscopy scenarios, and in
particular in the real use case studied in our work, the lower gastrointestinal
tract explored with colonoscopies. existing 3d reconstruction pipelines are able
to build small 3d models out of short clips from real and complete recordings [1].
one of the current bottle-necks to obtain better 3d models is the lack of more
abundant and higher quality correspondences in real data. this work introduces superpoint-e, a new model to extract interest points
from endoscopic images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"our main contribution is a novel supervision strategy to train
the model. we propose to automatically generate reliable training data from
video sequences by tracking feature points from existing detection methods,
which do not require training. we select good features with the colmap
sfm pipeline [21], generating training examples with feature points that can
be tracked across several images according to colmap result."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"[14] of high interest for the community. [16] and endomap-
per [1] datasets. earlier works like grasa et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"with this supervision, we train a model able to extract more features with good
properties for sfm algorithms, e.g., being spread and out of large specularities. 3
tracking adaptation for local feature learning
superpoint supervision is referred to as homographic adaptation and assumes
that the surfaces are locally plane, which is not the case in our data. instead,
we propose to use 3d reconstructions of points tracked along image sequences."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"we generate examples of good
features by identifying features that were successfully reconstructed with exist-
ing methods for each sequence in our training set. our training set contains
short sequences (4–7 s) from the complete colonoscopy recordings in endomap-
per dataset where colmap software was able to obtain a 3d reconstruction. this is a very challenging domain, and existing sfm pipelines fail in longer videos."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"two descriptors from diﬀerent images dai and dbj
are a positive pair if they belong to the same track (i = j), and negative pair
otherwise (i ̸= j). 4
experiments
the following experiments demonstrate the proposed feature detection eﬃcacy
to obtain 3d models on real colonoscopy videos, comparing diﬀerent variations
of our approach and relevant baseline methods.
dataset. [1], which con-
tains a hundred complete endoscopy recordings obtained during regular medical
practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"the models diﬀer in the supervision used and
the loss applied in the training, as detailed in the ﬁrst four columns of table 1.
table 1. ablation study. conﬁguration of the training (left), and average reconstruc-
tion results, i.e., quality metrics (right). best results highlighted in bold.
supervision & train conﬁg."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"points and matches
are given to colmap and the mapper module (conﬁguration in supplementary
material) attempts to generate a 3d reconstruction. the reconstruction quality
statistics used to illustrate the performance of each detector are:
– ∥3dim∥: fraction of images from the subsequence successfully introduced
in the reconstruction. the closer to 100% the better.
– ∥3dpts∥: number of points that were successfully reconstructed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"each point in each image
has been reconstructed after the corresponding colmap reconstruction process. table 2. reconstruction quality metrics for the comparison to the baselines. + total number of images in the subsequence."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"3 and 4 provide more insight on this metric.
figures 3 and 4 show a more detailed visualization of two representative
reconstructions, including a summary of the sequence frames, the point cloud
obtained by each method and a plot of the reprojection error for each point in
the reconstruction, sorted in increasing error value. note that even though sp-e
obtains many more points, it is not at the cost of quality. figure 3 shows a sce-
nario where sift fails to reconstruct a large part of the subsequence, because
it fails on the feature matching on the darker frames depicted in the middle of
the sequence."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"3. comparison of reconstructions obtained on seq 017 1 by sift, sp, and our
best model sp-e. the plot shows the reprojection error of each point reconstructed. we analyze additional aspects of our detected features to showcase the higher
quality with respect to other methods in table 3. to measure the spread of the
features over the images we deﬁned a 16 × 16 grid over each image and computed
the percentage of those cells that have at least one reconstructed point. we also
measure how many extracted points fall on top of specularities (we consider
a pixel as part of a specularity if its intensity is higher than 180)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"[3] to have ground truth available for the camera trajectory. we
took 5 sequences of 100-150 frames from this dataset, and we tested the baselines
and our model. we align the ground truth trajectories with the reconstructed
ones with horn’s method [8]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_56.pdf,"sp only reconstructed 3 out of the 5 sequences
while sift and sp-e correctly reconstructed the 5 sequences, with an average
rmse of 4.61mm and 4.71 mm respectively. simulated data lacks some of the
biggest challenges of endoscopy images (e.g. specularities, deformations), but
this experiment suggests that the camera motion estimation quality is similarly
good for all methods when they manage to converge. 4. comparison of reconstructions obtained on seq 095 1 by sift, sp, and our
best model sp-e. the plot shows the reprojection error of each point reconstructed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"we designed a
deep learning approach based on ct to assess and diﬀerentiate flls. to
achieve high accuracy, cts in diﬀerent phases are integrated to provide
more information than single-phase images. while most of the related
studies use convolutional neural networks, we exploit the transformer for
multi-phase liver lesion classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"in
addition, we introduce a pre-processing unit to resolve realistic annota-
tion issues. extensive experiments are conducted, in which we achieve
an overall accuracy of 90.9% on an in-house dataset of four ct phases
and seven liver lesion classes. the results also show distinct advantages
in comparison to state-of-art approaches in classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"focal liver lesions (flls) are the most common lesions found in liver
cancer, yet flls are challenging to diagnose because they can be either benign
lesions, such as focal nodular hyperplasia (fnh), hepatic abscess (ha), hepatic
x. wang and h. ying—equal contribution. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43895-0 31.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43895-0_31
330
x. wang et al.
hemangioma (hh), and hepatic cyst (hc) or malignant tumors, such as intra-
hepatic cholangiocarcinoma (icc), hepatic metastases (hm), and hepatocellular
carcinoma (hcc)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"accurate early diagnosis of flls is thus critical to increasing
the 5-year survival rate, a task that remains challenging as of today. dynamic
contrast-enhanced ct is a common technique for liver cancer diagnosis, where
four diﬀerent phases of imaging, namely, non-contrast (nc), arterial (art), por-
tal venous (pv), and delayed (dl) provide complementary information about
the liver. diﬀerent types of flls acquired in the four phases are shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"[5] designed a gan-based network to gener-
ate synthetic liver lesion images, improving the classiﬁcation performance based
on cnn. it is reported in many studies [9,18] that using multi-phase data,
like most professionals do in practice, can help the network get a more accu-
rate result, which also acts in liver lesion classiﬁcation [15,23,24]. yasaka et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"xu et al. [23] constructed a knowledge-guided framework to integrate
liver lesion features from three phases using self-attention and fused them with
a cross-feature interaction module and a cross-lesion correlation module. a single-phase lesion annotation means the annotation of both lesion position
and its class."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"self-attention based transformers [19] have shown strong capability in nat-
ural language processing tasks. [4] have
been shown to replace cnn with a transformer encoder in computer vision tasks
and can achieve obvious advantages on large-scale datasets. to the best of our
knowledge, we ﬁnd no study using vit backbone network in liver lesion classi-
ﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"the reason for this is twofold. [6], including ignoring local information within each patch, extracting only
single-scale features, and lacking inductive bias. second, no complete open liver
lesion classiﬁcation datasets exist."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"we use additional cross phase tokens at the last
stage to complete a multi-phase fusion, which can focus on cross-phase com-
munication and improve the fusion eﬀectiveness as compared with conventional
modes. while most multi-phase liver lesion classiﬁcation studies use datasets
with no more than three phases (without dl phase for its diﬃculty of collec-
tion) or no more than six lesion classes, we validate the whole framework on an
in-house dataset with four phases of abdominal ct and seven classes of liver
lesions. considering the disproportion of axial lesion slice number and the rela-
tively small scale of the dataset, we adopt a 2-d network in classiﬁcation part
instead of 3-d in pre-processing part and achieve a 90.9% accuracy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"332
x. wang et al.
fig. the overall architecture of the proposed transliver model.
2.1
pre-processing unit
the single-phase annotated lesion has the position and class labels in all phases
but they are not aligned, so we could have diﬃculty ﬁnding out which lesions in
diﬀerent phases are the same with 2 or more lesions in one patient. to reduce
errors caused by unregistered data and address the situation that one patient
has multiple lesions of diﬀerent types, we pre-process the multi-phase liver cts
registered and grouped by lesions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"in [1], the network needs to specify
an atlas image, otherwise, pairs of images will be registered to each other. but
in our work, we need to register the original data in a cross-phase form. we
choose an atlas phase art as suggested by clinicians and other phases of cts
are registered to the art phase of every patient."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"after registration, a lesion matcher ﬁnds the same lesions in diﬀerent phases. we generate a minimum circumscribed cuboid with padding as the lesion win-
dow for each lesion to keep the surrounding information. the windows are then
converted to 0–1 masks to calculate dice coeﬃcient."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"only lesions completely found in all phases will be used in the following
classiﬁcation network. hybrid transformer model for multi-phase liver lesion classiﬁcation
333
2.2
convolutional encoder and convolutional down-sampler
in pure vision transformer, input images are converted to tokens by patch embed-
ding and added with positional encoding to keep the positional information. patch embedding consists of a linear connected layer or a convolutional layer,
which does not enable to construct local relation [13]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"the module contains
four convolutional layers playing diﬀerent roles. the ﬁrst layer, conv1, with a
kernel size of 3, stride of 2, and output channels of 32, reduces the size to h
2 × w
2 .
next two layers, conv2 and conv3, each with a kernel size of 3, stride of 1, and
the same output channel as conv1, extract local information. conv1, conv2,
and conv3 each is followed by a gelu activation layer and a batch normaliza-
tion."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"each
convolutional down-sampler contains a residual structure with a 3×3 depthwise
convolution to increase the locality of our model. [4], but they are also prone to overﬁt on small datasets such as
private hospital datasets. [20] to largely reduce the computational overhead by reducing the size
of k and v using depthwise convolution."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"the fusion is conducted in deep
layers because the semantic concepts are learned in higher layers which beneﬁts
the cross phase connection. 3
experiments
3.1
liver lesion classiﬁcation
dataset. the employed single-phase annotated dataset is collected from sir run
run shaw hospital (srrsh), aﬃliated with the zhejiang university school of
medicine, and has received the ethics approval of irb."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"after the pre-processing unit with window
dice threshold of 0.3, we screen 761 lesions from 444 patients with four phases
hybrid transformer model for multi-phase liver lesion classiﬁcation
335
of cts, seven types of lesions (13.2% of hcc, 5.3% of hm, 11.3% of icc, 22.6%
of hh, 31.1% of hc, 8.7% of fnh, and 7.8% of ha), and totally 4820 slices. to
handle the imbalance of dataset, we randomly select 586 lesions as the training
and validation set with no more than 700 axial slices in each lesion type, and the
rest 175 lesions constitute the test set. lesions from the same patient are either
assigned to the training and validation set or the test set, but not both.
implementations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"the training and validation set is randomly divided with
a 4:1 ratio. the data is augmented by ﬂip, rotation, crop, shift, and scale. we
initialize the backbone network using pre-trained weights of cmt-s [6]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"we
measured performance by precision (pre.), sensitivity (sen.), speciﬁcity (spe.),
f1-score (f1), area under the curve (auc), and accuracy (acc.).
results. in the results of our method, hm has a relatively low performance
of 62.5%, mainly due to its low proportion in our dataset. the details can be
found in supplementary materials."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"we suppose the reason is
twofold. most of lesions in our dataset having few slices weakens the redundancy
between slices in 2-d pipeline, while the number of slices is still obviously larger
than the number of lesions, alleviating the overﬁtting issue. furthermore, vision
transformers are mostly pretrained in 2-d images, causing poor performance
when transferring to 3-d pipeline."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"we also evaluate the model performance under diﬀerent phase combinations
by cutting the branch of certain phases. it shows that information from vari-
ous phases can signiﬁcantly inﬂuence the classiﬁcation performance. a missing
phase can cause an accuracy drop of about 10% and complete four-phase model
outperforms single-phase model by nearly 20%."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2/paper_32.pdf,"the lesion features are extracted by transformer backbone with
several auxiliary convolutional modules. then, we fuse the features from diﬀer-
ent phases through cross phase tokens to enhance their information exchange. hybrid transformer model for multi-phase liver lesion classiﬁcation
337
to handle the issues in realistic cases, we design a pre-processing unit to acquire
multi-phase annotated lesions from single-phase annotated ones."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf,"nuclei appear small in size, yet, in real clinical practice, the
global spatial information and correlation of the color or brightness con-
trast between nuclei and background, have been considered a crucial com-
ponent for accurate nuclei segmentation. however, the ﬁeld of automatic
nuclei segmentation is dominated by convolutional neural networks
(cnns), meanwhile, the potential of the recently prevalent transformers
has not been fully explored, which is powerful in capturing local-global
correlations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf,"finally, a token mlp bottleneck replaces the over-parameterized trans-
former bottleneck for a further reduction in model complexity. experi-
ments on two datasets of diﬀerent modalities, including monuseg have
shown that our methods can outperform state-of-the-art counterparts
such as ca2.5-net by 2–3% dice with 30% fewer parameters. in con-
clusion, transnuseg conﬁrms the strength of transformer in the context
of nuclei segmentation, which thus can serve as an eﬃcient solution for
real clinical practice."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf,"[5,12,14,19,21] have received notable
attention due to their simplicity and generalization ability. in the literature work, the sole-decoder design in these unet variants
(fig. 1(a)) is susceptible to failures in splitting densely clustered nuclei when
precise edge information is absent. [3] with bi-decoder structure achieves improved instance segmentation
performance by adopting multi-task learning, in which one decoder learns to
segment the nuclei and the other recognizes edges as described in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf,"the overall framework of the proposed transnuseg of three output branches to
separate the nuclei, normal edges, and cluster edges, respectively. in the novel design,
a pre-deﬁned proportion of the attention heads are shared between the decoders via
the proposed sharing scheme, which considerably reduces the number of parameters
and enables more eﬃcient information communication. transnuseg
209
2
methodology
network architecture overview."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf,"our network consists of three individual output decoder
paths for nuclei segmentation, normal edges segmentation, and clustered edges
segmentation. given the high dependency between edge and clustered edge, we
are inspired to propose a novel attention sharing scheme, which can communicate
the information and share learned features across decoders while also reducing
the number of parameters. additionally, a token mlp bottleneck is incorporated
to further increase the model eﬃciency."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf,"all
decoder branches follow a uniform scheme that combines the cross-entropy loss
and the dice loss, with the balancing coeﬃcients set to 0.60 and 0.40 respec-
tively, as previous work [6]. subsequently, the overall loss l is calculated as a
weighted summation of semantic nuclei mask loss (ln), normal edge loss (le),
and clustered edge loss (lc), and the self distillation loss (lsd) i. e.
l = γn · ln + γe · le + γc · lc + γsd · lsd, where coeﬃcients γn, γe and γc are
set to 0.30, 0.35, 0.35 respectively, and γsd is initially set to 1 with a 0.3 decrease
for every 10 epochs until it reaches 0.4.
3
experiments
dataset. we evaluated the applicability of our approach across multiple modali-
ties by conducting evaluations on microscopy and histology datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf,"it consists of 524
ﬂuorescence images, each with a resolution of 512 × 512 pixels. we crop each image
in the monuseg dataset into four partially overlapping 512 × 512 images.
fig. 3. a schematic illustration of the proposed attention sharing scheme."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf,"the best performance with
respect to each metric is highlighted in boldface. the private dataset contains 300 images sized at 512 × 512 tessellated from 50
wsis scanned at 20×, and meticulously labeled by ﬁve pathologists according
to the labeling guidelines of the monuseg [10]. for both datasets, we randomly
split 80% of the samples on the patient level as the training set and the remaining
20% as the test set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf,"table 2. comparison of the model complexity in terms of the number of parameters,
flops, as well as the training cost in the form of the averaged training time per epoch. the average training time is computed using the same batch size for both datasets, with
the ﬁrst number indicating the averaged time on the fluorescence microscopy image
dataset and the second on the histology image dataset. 4. exemplary samples and their segmentation results using diﬀerent methods.
transnuseg demonstrates superior segmentation performance compared to its counter-
parts, which can successfully distinguish severely clustered nuclei from normal edges.
implementations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf,"the large margin between the swinunet and the other cnn-based or
hybrid networks also conﬁrms the superiority of the transformer in ﬁne-grained
nuclei segmentation. more importantly, our method can outperform swinunet
and the previous methods on both datasets. for example, in the histology image
dataset, transnuseg improves the dice score, f1 score, accuracy, and iou by
2.08%, 3.41%, 1.25%, and 2.70% respectively, over the second-best models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_20.pdf,"our ablation study yields that token mlp bottleneck and attention
sharing schemes can complementarily reduce the training cost while increasing
eﬃciency, as shown in table 2 (the last 4 rows). to further show the eﬀectiveness
of these schemes, as well as consistency self distillation, we conduct a comprehen-
sive ablation study on both datasets. as described in table 3, each component
proportionally contributes to the improvement to reach the overall performance
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"whole-slide histopathology image (wsi) is regarded as the
gold standard for survival prediction of breast cancer (bc) across dif-
ferent subtypes. however, in cancer prognosis applications, the cost of
acquiring patients’ survival information is high and can be extremely
diﬃcult in practice. by considering that there exists a certain common
mechanism for tumor progression among diﬀerent subtypes of breast
invasive carcinoma(brca), it becomes critical to utilize data from a
related subtype of brca to help predict the patients’ survival in the
target domain."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"thus, eﬀective and accurate prognosis of bc
as well as stratifying cancer patients into diﬀerent subgroups for personalized
cancer management has attracted more attention than ever before. among diﬀerent types of imaging biomarkers, histopathological images are
generally considered the golden standard for bc prognosis since they can confer
important cell-level information that can reﬂect the aggressiveness of bc [4].
recently, with the availability of digitalized whole-slide pathological images
(wsis), many computational models have been employed for the prognosis pre-
diction of various subtypes of bc. [5] presented a novel
approach for predicting the prognosis of er-positive bc patients by quantifying
nuclear shape and orientation from histopathological images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"[6] devel-
oped a gradient boosting algorithm to predict the disease progression for various
subtypes of bc. however, due to the high-cost of collecting survival information
from the patients, it is still a challenge to build eﬀective machine learning models
for speciﬁc bc subtypes with limited annotation data. to deal with the above challenges, several researchers began to design domain
adaption algorithms, which utilize the labeled data from a related cancer sub-
type to help predict the patients’ survival in the target domain."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"[7] presented a new representation learning-based unsuper-
vised domain adaption method to predict the clinical outcome of cancer patients
on the target domain. [8] proposed a collaborative unsupervised
domain adaptation algorithm, which conducts transferability-aware adaptation
and conquers label noise in a collaborative way. [9] developed graph neural networks for unsupervised domain adaptation
in histopathological image analysis, based on a backbone for embedding input
images into a feature space, and a graph neural layer for propagating the super-
vision signals of images with labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"[11] utilized the tils spa-
tial pattern for survival analysis in diﬀerent breast cancer subtypes including
614
y. wu et al.
er-negative, er-positive, and triple-negative. it can be expected that better
prognosis performance can be achieved if we leveraged the tils-tumor interac-
tion information to resolve the survival analysis task on the target domain. based on the above considerations, in this paper, we proposed a tils-tumor
interactions guided unsupervised domain adaptation (t2uda) algorithm to pre-
dict the patients’ survival on the target bc subtype."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"t2uda aligns tils-tumor
edge interaction weights in the ttia module and feature vectors in the fa module
to reduce the discrepancy between diﬀerent domains and achieving prognosis task on
target bc patients. t2uda
615
data pre-processing. we obtained valid patches of 512 × 512 pixels from
pathological images and segment the tils and tumor tissues using a pre-trained
u-net++ model."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"then, for
the node embeddings with diﬀerent types (tils and tumor) in both the source
and target domain, we performed a mean pooling operation to obtain their
616
y. wu et al.
aggregated features. [15].
here, we adopted mmd for feature alignment due to its ability to measure the
distance between two distributions without explicit assumptions on the data
distribution, we showed the objective function of mmd in our method as follows:
lf a =

r=1,2

k∈l,t

1
n
n

i=1
(fi,k)r − 1
m
m

i=1

f ′
i,k
r

2
h
(4)
where h is a hilbert space, f represents the features from the source, f ′ rep-
resents the feature from the target, r represents the layer number, k ∈ {l, t}
referred to tils or tumor node. in addition, n denotes the number of source
samples, while m refers to the number of target samples."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"sample i refers to
censored patient if δi = 0, otherwise δi = 1.
overall objective. to achieve domain-adaptive prognosis prediction, the ﬁnal
loss function included the cox loss, fa loss, and ttia loss as the following
formula:
lt = lcox + αlf a + βlt t ia,
(7)
where α and β represent the weights assigned to the importance of fa component
and ttia component respectively.
3
experiments and results
datasets. we conducted our experiments on the breast invasive carcinoma
(brca) dataset from the cancer genome atlas (tcga)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"among the col-
lected brca patients in tcga, the number of er positive(er+) and er
negative(er−) patients are 515 and 146, respectively. we hope to investigate if
the proposed t2uda could be used to help improve the prognosis performance
of (er+) or (er−) with the aid of the survival information on its counterpart. 3.1
implementation details and evaluation metrics
the dimension of intermediate layers in gat was 256."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"the survival curves by applying diﬀerent methods on two benchmark settings:
(1) source domain is er+ bc and target domain is er− bc in (a); (2) source domain
is er− bc and target domain is er+ bc in (b). [17]: utilize the
maximum mean discrepancy (mmd) to calculate the domain diﬀerence loss
between source and target data and optimize both classiﬁcation loss and dis-
parity loss. 2) dann [18]: an adversarial learning method that used gradi-
ent backpropagation to extract domain-independent features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"[19]: an
adversarial training method that combined metric learning and domain adapta-
tion. 4) deepjdot [20]: an unsupervised domain adaptation method based on
optimal transport that simultaneously learns features and optimizes classiﬁers
by measuring joint feature/label diﬀerences. 5) source only: it was trained on
t2uda
619
proportion of edges connected tils 
and tumor patches
source domain
target domain
patient a, 84 months
patient b, 13 months
patient d, 19 months
patient c, 73 months
 tumor patch
 tils patch
（a）
(b)
0
10
20
30
40
50
60
low-risk group
high-risk group
source
target
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"first, our
proposed method outperformed feature alignment-based methods such as ddc
and deepjdot in terms of both ci and auc values. the reason lies in that these
methods only transferred the knowledge at the feature level and neglected the
inter-relationship between tils and tumors. second, our method outperformed
adversarial-based methods such as dann and mdd, as the high heterogene-
ity between the target and source domains results in negative transfer through
adversarial training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"as shown in fig. 3, our proposed t2uda outperformed feature
alignment-based methods (such as ddc and deepjdot), adversarial-based
methods (such as dann and mdd), and t2uda-v1 in stratiﬁcation perfor-
mance, proving that considering the interaction between tils and tumors as
migration knowledge leads to better prognostic results. we also examined the consistency of important edges in each group of strat-
iﬁed patients based on the tils-tumor interaction weights calculated by the
620
y. wu et al.
gat-based framework in the source and target domains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,", the weights of the edges connecting tumor and tils regions
were higher for patients in the low survival risk group in both source and target
domains. this was consistent with our knowledge that brisk interaction between
tils and tumor regions indicates a better clinical outcome and demonstrates
the transferability of this knowledge. 4
conclusion
in this paper, we presented an unsupervised domain adaptation algorithm that
leverages tils-tumor interactions to predict patients’ survival in a target bc
subtype(t2uda)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_59.pdf,"our results demonstrated that the relationship between tils
and tumors is transferable and can be eﬀectively used to improve the accuracy
of survival prediction models. to the best of our knowledge, t2uda was the
ﬁrst method to successfully achieve interrelationship transfer between tils and
tumors across diﬀerent cancer subtypes for prognosis tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_77.pdf,"in
terms of interpretability, triangil easily beats gnn, by pulling bio-
logical insights from immune cells interplay and shedding light on the
triadic interaction of cd4+-tumor-stromal cells. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43987-2 77. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14225, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_77.pdf,"3
description of triangil methodology
3.1
notation
our approach consists of constructing heterogeneous graphs step by step and
quantifying them by extracting features from them. the graphs are deﬁned by
g = (v, e), where v is the set of vertices (nodes) v = {v1, ...vn} with τn vertex
types, and e is the collection of pairs of vertices from v, e = {e1, ...em}, which
are called edges and φn is the mapping function that maps every vertex to one
of n diﬀerential marker expressions in this dataset φn : v → τn. g is represented
by an adjacency matrix a that allows one to determine edges in constant time."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_77.pdf,"6(1), 1–15 (2022)
10. hamilton, w., ying, z., leskovec, j.: inductive representation learning on large
graphs. in: advances in neural information processing systems 30 (2017)
11. harrell, f.e., caliﬀ, r.m., pryor, d.b., lee, k.l., rosati, r.a.: evaluating the
yield of medical tests."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6/paper_77.pdf,"university of southern california los angeles, tech. simon, r.m., subramanian, j., li, m.c., menezes, s.: using cross-validation to
evaluate predictive accuracy of survival risk classiﬁers based on high-dimensional
data. tavares, m.c., et al.: a high cd8 to foxp3 ratio in the tumor stroma and
expression of pten in tumor cells are associated with improved survival in non-
metastatic triple-negative breast carcinoma."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf,"overall, uml could
produce conﬁdence estimation in features and performance for each link
(classiﬁcation and segmentation). the experiments on the public datasets
demonstrate that our uml outperforms existing methods in terms of
both accuracy and robustness. our uml has the potential to explore
the development of more reliable and explainable medical image analysis
models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf,"keywords: mutual learning · medical image classiﬁcation and
segmentation · uncertainty estimation
k. ren and k. zou—denotes equal contribution. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43901-8 4.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43901-8_4
36
k. ren et al.
1
introduction
accurate and robust classiﬁcation and segmentation of the medical image are
powerful tools to inform diagnostic schemes."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf,"in clinical practice, the image-level
classiﬁcation and pixel-wise segmentation tasks are not independent [8,27]. joint
classiﬁcation and segmentation can not only provide clinicians with results for
both tasks simultaneously, but also extract valuable information and improve
performance. however, improving the reliability and interpretability of medical
image analysis is still reaching."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf,"we introduce an uncertainty navigator
for segmentation (un) to generate preliminary segmentation results, taking into
account the uncertainty of mutual learning features. we also propose an uncer-
uncertainty-informed mutual learning
37
             uncertainty navigator (un)
             uncertainty instructor (ui)
lcls
c
c
label
cls 
feature
encoder
un
feature
mixer
seg
feature
encoder
f
c
cls 
evidence
lseg
u s
m
gt
s
(a)
(b)
mutual
feature
decoder
0
1
ui
gt
lmut
u c
b1
b0
mutual
evidence
mutual
dirichlet
cls 
dirichlet 
flow of 
feature
softplus
global average 
pooling
flow of 
uncertainty
c
concat
deep 
supervision
conv
reliable
mask
fig. the framework of uncertainty-informed mutual learning network."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf,"2. details of (a) uncertainty navigator (un) and (b) uncertainty instructor (ui). in our method, appropriate uncertainty guided decoding on the
feature list can obtain more reliable information and improve the performance
of segmentation [3,9,26]. so we introduce uncertainty navigator for segmen-
tation(un) as a feature decoder, which incorporates the pixel-wise uncertainty
in u sand lesion location information in m with the segmentation feature maps
to generate the segmentation result and reliable features."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf,"especially, the u s is also used to guide the bottom
feature with the dot product. the rs is calculated from the segmentation result
s1 and contains uncertainty navigated information not found in s1. uncertainty instructor for classiﬁcation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf,"the hyperparameter λc serves as
a crucial hyperparameter governing the kl, aligning with previous work [5]. to
obtain reliable segmentation results, we also adopt deep supervision for the ﬁnal
segmentation result s = {si, i = 1, ..., 4}, which can be denoted as:
ls =
4
i=1 ldice(υi−1(si), ys)
4
,
(9)
where υn indicates the number of up-sampling is 2n. + wclc(αc, yc) + wsls,
(10)
where wm, wc, ws denote the weights and are set 0.1, 0.5, 0.4, separately.
3
experiments
dataset and implementation. [13]. refuge contains two tasks, classiﬁ-
cation of glaucoma and segmentation of optic disc/cup in fundus images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf,"a total of 157 patients
who suﬀer the breast cancer are considered - 43 achieve pcr and 114 non-pcr. for each case, we cut out the slices in the 3d image and totally got 1,570 2d
images, which are randomly divided into the train, validation, and test datasets
with 1,230, 170, and 170 slices, respectively.
uncertainty-informed mutual learning
41
table 1. evaluation of the classiﬁcation and segmentation performance. the top-2
results are highlighted in bold and underlined (p ≤ 0.01)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf,"the result of ablation study. as shown in table 1,
we report the performance on the two datasets of the proposed uml and other
methods. by comparison, we can observe the fact that the accuracy of the model
results is low if either classiﬁcation or segmentation is done in isolation, the
acc has only just broken 0.5 in ec."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf,"excitingly, our uml not only
achieves the best classiﬁcation performance in acc (85.3%) and f1 (0.875) with
signiﬁcant increments of 1.8%, 4.9%, but also obtains the superior segmentation
performance with increments of 6.6% in didisc and 3.2% in dicup. a similar
improvement can be observed in the experimental results in ispy-1.
comparison under noisy data. to further valid the reliability of our model,
we introduce gaussian noise with various levels of standard deviations (σ) to
the input medical images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4/paper_4.pdf,"the comparison results are shown in table 2. as
can be observed that, the accuracy of classiﬁcation and segmentation signiﬁ-
cantly decreases after adding noise to the raw data. however, beneﬁting from the
uncertainty-informed guiding, our uml consistently deliver impressive results.
in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"this can be achieved
by either designing classiﬁers that are robust enough to avoid failures in
the ﬁrst place, or by detecting remaining failures using conﬁdence scoring
functions (csfs). a predominant source of failures in image classiﬁca-
tion is distribution shifts between training data and deployment data. to understand the current state of silent failure prevention in medical
imaging, we conduct the ﬁrst comprehensive analysis comparing various
csfs in four biomedical tasks and a diverse range of distribution shifts."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"keywords: failure detection · distribution shifts · benchmark
1
introduction
although machine learning-based classiﬁcation systems have achieved signiﬁcant
breakthroughs in various research and practical areas, their clinical application is
still lacking. a primary reason is the lack of reliability, i.e. failure cases produced
by the system, which predominantly occur when deployment data diﬀers from
the data it was trained on, a phenomenon known as distribution shifts. the robustness of a classiﬁer, i.e. its ability to generalize across these shifts, is
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 39."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"https://doi.org/10.1007/978-3-031-43898-1_39
silent failures in medical image classiﬁcation
401
fig. 1. a) exemplary predictions of the classiﬁer and the accompanying conﬁdence
scoring function (csf, here: conﬁdnet) on the dermoscopy dataset across several dis-
tribution shifts. note that true/false positives/negatives (t/f p/n) do not
refer to the classiﬁer decision, but to the failure detection outcome, i.e.
the assessment of the csf."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"in this context, fn, i.e. cases with incorrect predic-
tions (“failure”) and a high conﬁdence score (“failure not detected”) are referred to as
silent failures. b) sf-visuals allows to identify and analyze silent failures in a dataset
based on an interactive scatter plot in the classiﬁer’s latent space (each dot repre-
sents one image, which is displayed when selecting the dot). c) sf-visuals further
features concept cluster plots to gain an intuition of how the model perceives distinct
classes or distribution shifts."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"bernhardt et al. [3] studied failure detection on several biomedical datasets,
but only assessed the performance of csfs in isolation without considering the
classiﬁer’s ability to prevent failures. moreover, their study did not include dis-
tribution shifts thus lacking a wide range of realistic failure sources."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"[15], we present the ﬁrst comprehensive study of silent failure prevention in
the biomedical ﬁeld. we compare various csfs under a wide range of distribution
shifts on four biomedical datasets. our study provides valuable insights and the
underlying framework is made openly available to catalyze future research in the
community."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"2) since the benchmark reveals that none of the predominant csfs
can reliably prevent silent failures in biomedical tasks, we argue that a deeper
understanding of the root causes in the data itself is required. to this end, we
present sf-visuals, a visualization tool that facilitates identifying silent failures
in a dataset and investigating their causes (see fig. 1). our approach contributes
to recent research on visual analysis of failures [13], which has not focused on
silent failures and distribution shifts before."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"2
methods
benchmark for silent failure prevention under distribution shifts. we follow the spirit of recent robustness benchmarks, where existing datasets
have been enhanced by various distribution shifts to evaluate methods under a
wide range of failure sources and thus simulate real-world application [19,27].
to our knowledge, no such comprehensive benchmark currently exists in the
biomedical domain. speciﬁcally, we introduce corruptions of various intensity
levels to the images in four datasets in the form of brightness, motion blur, elas-
tic transformations and gaussian noise."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"[16], while only retaining the classes common to all
three. next, we emulate two acquisition shifts by deﬁning either the nih14 or
the chexpert data as the target domain. fc-microscopy dataset: the rxrx1
dataset [28] represents the ﬂuorescence cell microscopy domain."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"since the images
were acquired in 51 deviating acquisition steps, we deﬁne 10 of these batches as
target-domain to emulate an acquisition shift. lung nodule ct dataset:
we create a simple 2d binary nodule classiﬁcation task based on the 3d lidc-
idri data [1] by selecting the slice with the largest annotation per nodule (±two
slices resulting in 5 slices per nodule). average malignancy ratings (four raters
per nodule, scores between 1 and 5) > 2 are considered malignant and all others
as benign."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"we emulate two manifestation shifts by deﬁning nodules with high
spiculation (rating > 2), and low texture (rating < 3) as target domains. the datasets consist only of publicly available data, our benchmark provides
scripts to automatically generate the combined datasets and distribution shifts. the sf-visuals tool: visualizing silent failures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"2) concept cluster plots: see examples in fig. to abstract
away from individual points in the scatter plot, concepts of interest, such as
classes or distribution shifts can be deﬁned and visualized to identify conceptual
commonalities and diﬀerences in the data as perceived by the model. therefore,
k-means clustering is applied to the 3-dimensional embedding."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"for corruption shifts, we further allow investigating the predictions on a ﬁxed
input image over varying intensity levels. based on these visualizations, the functionality of sf-visuals is three-fold:
1) visual analysis of the dataset including distribution shifts. 2) visual analysis
of the general behavior of various csfs on a given task 3) visual analysis of
individual silent failures in the dataset for various csfs."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"[15], shows
the best results on i.i.d. testing on 3 out of 4 tasks. however, the method is not
reliable across all settings, falling short on manifestation shifts and corruptions
on the lung nodule ct dataset. silent failures in medical image classiﬁcation
405
table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"“cor” denotes the average over all corruption types and intensities levels. similarly,
“acq”/“man” denote averages over all acquisition/manifestation shifts per dataset. “iid” denotes scenarios without distribution shifts."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"for instance, devries et al. outperforms all other csfs for one clini-
cal site (mskcc) as target domain, but falls short on the other one (hcb). on the chest x-ray dataset, mcd worsens the performance for darkening cor-
ruptions across all csfs and intensity levels, whereas the opposite is observed
for brightening corruptions. further, on the lung nodule ct dataset, dg-mcd-
res performs best on bright/dark corruptions and the spiculation manifestation
shift, but worst on noise corruption and falls behind on the texture manifestation
shift."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"first, an interac-
tive scatter plot (fig. 1b, left) provides an overview of the mskcc acquisition
shift on the dermoscopy dataset and reveals a severe change of the data dis-
tribution. for instance, some malignant lesions of the target domain (purple
dots) are located deep within the “benign” cluster."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"(no shift): this analysis reveals how simple class clustering (no distribu-
tion shifts involved) can help to gain intuition on the most severe silent failures
(examples selected as the two highest-conﬁdence failures). on the lung nodule
ct data (fig. 2a), we see how the classiﬁer and csf break down when a malig-
nant sample (typically: small bright, round) exhibits characteristics typical to
benign lesions (larger, less cohesive contour, darker) and vice versa. this pat-
tern of contrary class characteristics is also observed on the dermoscopy dataset
(2c)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"corruption shift: figs. 2b and 2d show for the
lung nodule ct data and the dermoscopy data, respectively, how corruptions
can lead to silent failures in low-conﬁdent predictions. in both examples, the
brightening of the image leads to a malignant lesion taking on benign character-
istics (brighter and smoother skin on the dermoscopy data, decreased contrast
between lesion and background on the lung nodule ct data)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"acquisition
shift: additionally to the example in fig. 2e shows how the proposed
tool visualizes an acquisition shift on the chest x-ray data. while this reveals an
increased blurriness in the target domain, it is diﬃcult to derive further insights
involving speciﬁc pathologies without a clinical expert."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"while msr assigns the prediction low conﬁdence
thereby catching the failure, dg assigns high conﬁdence for the same model and
prediction, causing a silent failure. this example shows how the tool allows the
comparison of csfs and can help to identify failure modes speciﬁc to each csf.
manifestation shift: on the dermoscopy data (fig. 2g), we see how a mani-
festation shift can cause silent failures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"the benign lesions in the target domain
are similar to the malignant lesions in the source domain (rough skin, irregu-
lar shapes), and indeed the two failures in the target domain seem to fall into
this trap. on the lung nodule ct data (fig. 2f), we observe a visual distinction
408
t. j. bungert et al.
between the spiculated target domain (spiked surface) and the non-spiculated
source domain (smooth surface)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_39.pdf,"1) we hope the revealed shortcomings of current systems on biomedical
tasks in combination with the deeper understanding of csf behaviors granted
by sf-visuals will catalyze research towards a new generation of more reliable
csfs. 2) this study shows that in order to progress towards reliable ml sys-
tems, a deeper understanding of the data itself is required. sf-visuals can help
to bridge this gap and equip researchers with a better intuition of when and how
to employ ml systems for a particular task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"although deep learning has shown promise in medical
image segmentation, the scarcity of pixel-level annotations due to their
expense and time-consuming nature limits its application in covid-19
segmentation. in this paper, we propose utilizing large-scale unpaired chest
x-rays with classiﬁcation labels as a means of compensating for the lim-
ited availability of densely annotated ct scans, aiming to learn robust
representations for accurate covid-19 segmentation. to achieve this, we
design an unpaired cross-modal interaction (uci) learning framework."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"the encoder is built to capture optimal feature represen-
tations for both ct and x-ray images. to facilitate information interaction
between unpaired cross-modal data, we propose the kc that introduces
a momentum-updated prototype learning strategy to condense modality-
speciﬁc knowledge. the condensed knowledge is fed into the ki module
for interaction learning, enabling the uci to capture critical features and
relationships across modalities and enhance its representation ability for
covid-19 segmentation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"code is available at: https://github.com/gqbbbb/uci. keywords: covid-19 segmentation · unpaired data · cross-modal
q. guan and y. xie—contributed equally to this work. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43898-1 58.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"the commonly used imaging modalities for covid-19 diagnosis
are chest x-rays and chest computerized tomography (ct). the latter has been
the preferred method for detecting acute lung manifestations of the virus due to its
exceptional imaging quality and ability to produce a 3d view of the lungs. eﬀec-
tive segmentation of covid-19 infections using ct can provide valuable insights
into the disease’s development, prediction of the pathological stage, and treatment
response beyond just screening for covid-19 cases."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"automated segmen-
tation is crucial, but it is also challenging due to three factors: the infected regions
often vary in shape, size, and location, appear similar to surrounding tissues, and
can disperse within the lung cavity. however, dcnns require
large-scale annotated data to explore feature representations eﬀectively. unfor-
tunately, publicly available ct scans with pixel-wise annotations are relatively
limited due to high imaging and annotation costs and data privacy concerns."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"in comparison to ct scans, 2d chest x-rays are a more accessible and cost-
eﬀective option due to their fast imaging speed, low radiation, and low cost,
especially during the early stages of the pandemic [21]. for example, the chestx-
ray dataset [18] contains about 112,120 chest x-rays used to classify common
thoracic diseases. [1] contains 17,955 chest x-rays used for
covid-19 recognition."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"we advocate using chest x-ray datasets such as chestx-
ray and chestxr may beneﬁt covid-19 segmentation using ct scans because
of three reasons: (1) supplement limited ct data and contribute to training
a more accurate segmentation model; (2) provide large-scale chest x-rays with
labeled features, including pneumonia, thus can help the segmentation model
to recognize patterns and features speciﬁc to covid-19 infections; and (3) help
improve the generalization of the segmentation model by enabling it to learn from
diﬀerent populations and imaging facilities. inspired by this, in this study, we
propose a new learning paradigm for covid-19 segmentation using ct scans,
involving training the segmentation model using limited ct scans with pixel-
wise annotations and unpaired chest x-ray images with image-level labels. to achieve this, an intuitive solution is building independent networks to
learn features from each modality initially."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"although “chilopod”-shaped multi-modal learning [6] has been
unpaired cross-modal interaction learning for covid-19 segmentation
605
proposed to share all cnn kernels across modalities, it is still limited when
the diﬀerent modalities have a signiﬁcant dimension gap. second, the presence
of unpaired data, speciﬁcally ct and x-ray data, in the feature fusion/cross-
attention interaction can potentially cause the model to learn incorrect or irrel-
evant information due to the possible diﬀerences in their image distributions
and objectives, leading to reduced covid-19 segmentation accuracy. it’s worth
noting that the method using paired multimodal data [2] is not suitable for our
application scenario, and the latest unpaired cross-modal [3] requires pixel-level
annotations for both modalities, while our method can use x-ray images with
image-level labels for training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"the uci framework learns representations from both seg-
mentation and classiﬁcation tasks. it includes three main components: a multi-
modal encoder for image representations, a knowledge condensation and inter-
action module for unpaired cross-modal data, and task-speciﬁc networks. the
encoder contains modality-speciﬁc patch embeddings and shared transformer
layers."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"this design enables the network to capture optimal feature represen-
tations for both ct and x-ray images while maintaining the ability to learn
shared representations between the two modalities despite dimensional diﬀer-
ences. to address the challenge of information interaction between unpaired
cross-modal data, we introduce a momentum-updated prototype learning strat-
egy to condense modality-speciﬁc knowledge. this strategy groups similar repre-
sentations into the same prototype and iteratively updates the prototypes with a
momentum term to capture essential information in each modality."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"therewith,
a knowledge-guided interaction module is developed that accepts the learned
prototypes, enabling the uci to better capture critical features and relation-
ships between the two modalities. finally, the task-speciﬁc networks, including
the segmentation decoder and classiﬁcation head, are presented to learn from all
available labels. the proposed uci framework has signiﬁcantly improved per-
formance on the public covid-19 segmentation benchmark [15], thanks to the
inclusion of chest x-rays."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"2
approach
the proposed uci aims to explore eﬀective representations for covid-19 seg-
mentation by leveraging both limited dense annotated ct scans and abundant
image-level annotated x-rays. figure 1 illustrates the three primary components
of the uci framework: a multi-modal encoder used to extract features from each
modality, the knowledge condensation and interaction module used to model
unpaired cross-modal dependencies, and task-speciﬁc heads designed for seg-
mentation and classiﬁcation purposes. 2.1
multi-modal encoder
the multi-modal encoder f(·) consists of three stages of blocks, with modality-
speciﬁc patch embedding layers and shared transformer layers in each block,
capturing modality-speciﬁc and shared patterns, which can be more robust and
discriminative across modalities."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"n ct and n cxr means the length of ct and x-ray feature sequence. 2.2
knowledge condensation and interaction
knowledge condensation. it is diﬃcult to directly learn cross-modal depen-
dencies using the features obtained by the encoder because ct and x-ray data
were collected from diﬀerent patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"as shown in fig. 1(a), we design a knowledge condensation
(kc) module by introducing a momentum-updated prototype learning strategy
to condensate valuable knowledge in each modality from the learned features. i = arg min
j
m, pcxr
j
2

(2)
where ccxr
i
suggests the feature points closing to the i-th prototype."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"similarly, the prototypes pct for ct modality
can be calculated and updated with the feature set f ct. the prototypes eﬀec-
tively integrate the informative features of each modality and can be considered
modality-speciﬁc knowledge to improve the subsequent cross-modal interaction
learning. the momentum term allows prototypes to move more smoothly and
consistently towards the optimal position, even in the presence of noise or other
factors that might cause the prototypes to ﬂuctuate."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"this can result in a more
stable learning process and more accurate prototypes, thus contributing to con-
densate the knowledge of each modality better. the knowledge-guided interaction (ki)
module is proposed for unpaired cross-modality learning, which accepts the
learned prototypes from one modality and features from another modality as
inputs. as shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"similarly, for the f cxr and pct as
inputs, the ki module is also used to boost the x-ray representations. inspired
by the knowledge prototypes, ki modules boost the interaction between the two
modalities and allow for the learning of strong representations for covid-19
segmentation and x-ray classiﬁcation tasks. 2.3
task-speciﬁc networks
the outputs of the ki module are fed into two multi-task heads - one decoder
for segmentation and one prediction head for classiﬁcation respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"in each stage, the input feature map is ﬁrst up-sampled by the
3d patch embedding layer, and then reﬁned by the stacked transformer layers. besides, we also add skip connections between the encoder and decoder to keep
more low-level but high-resolution information. the decoder includes a segmen-
tation head for ﬁnal prediction."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"[1] to assist the uci training. the chestx-
ray14 dataset comprises 112,120 x-ray images showing positive cases from 30,805
patients, encompassing 14 disease image labels pertaining to thoracic and lung
ailments. an image may contain multiple or no labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"the chestxr dataset
consists of 21,390 samples, with each sample classiﬁed as healthy, pneumonia,
or covid-19. unpaired cross-modal interaction learning for covid-19 segmentation
609
3.2
implementation details
for ct data, we ﬁrst truncated the hu values of each scan using the range
of [−958, 327] to ﬁlter irrelevant regions, and then normalized truncated voxel
values by subtracting 82.92 and dividing by 136.97. we randomly cropped sub-
volumes of size 32 × 256 × 256 as the input and employed the online data aug-
mentation like [10] to diversify the ct training set."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"for chest x-ray data, we
set the size of input patches to 224 × 224. we employ the online data argumen-
tation, including random cropping and zooming, random rotation, and horizon-
tal/vertical ﬂip, to enlarge the x-ray training dataset. we follow the extension of
[20] for weight initialization and use the adamw optimizer [11] and empirically
set the initial learning rate to 0.0001, batch size to 2 and 32 for segmentation
and classiﬁcation, maximum iterations to 25w, momentum factor λ to 0.99, and
the number of prototypes k to 256.
to evaluate the covid-19 segmentation performance, we utilized six met-
rics, including the dice similarity coeﬃcient (dsc), intersection over union
(iou), sensitivity (sen), speciﬁcity (spe), hausdorﬀ distance (hd), and aver-
age surface distance (asd)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"this suggests that the
segmentation outcomes generated by our models are in good agreement with
the ground truth. notably, despite chestxr being more focused on covid-19
recognition, the uci model aided by the chestx-ray14 dataset containing 80k
images performs better than the uci model using the chestxr dataset with only
16k images. this suggests that having a larger auxiliary dataset can improve the
segmentation performance even if it is not directly related to the target task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_58.pdf,"2. eﬀectiveness of each module in uci.
3.4
discussions
ablations. we perform ablation studies over each component of uci, including
the multi-modal encoder, knowledge condensation (kc) and knowledge inter-
action (ki) models, as listed in fig. 2. we set the maximum iterations to 8w and
use chestx-ray14 as auxiliary data for all ablation experiments."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"we present a data-driven generative framework for synthe-
sizing blood vessel 3d geometry. this is a challenging task due to the
complexity of vascular systems, which are highly variating in shape, size,
and structure."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"after training, the vesselvae latent space can be sampled to generate
new vessel geometries. to the best of our knowledge, this work is the
ﬁrst to utilize this technique for synthesizing blood vessels. we achieve
similarities of synthetic and real data for radius (.97), length (.95), and
tortuosity (.96)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"despite signiﬁcant advances in
vessel segmentation [26], reconstructing thin features accurately from medical
images remains challenging [2]. manual editing of vessel geometry is a tedious
and error prone task that requires expert medical knowledge, which explains the
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_7.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43907-0_7
68
p. feldman et al.
scarcity of curated datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"as a result, several methods have been developed
to adequately synthesize blood vessel geometry [29].
within the existing literature on generating vascular 3d models, we identi-
ﬁed two primary types of algorithms: fractal-based, and space-ﬁlling algorithms. although these model-based methods
provide some degree of control and variation in the structures produced, they
often fail to capture the diversity of real anatomical data. in recent years, deep neural networks led to the development of powerful gen-
erative models [30], such as generative adversarial networks [8,12] and diﬀusion
models [11], which produced groundbreaking performance in many applications,
ranging from image and video synthesis to molecular design."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"how-
ever, this model is limited to generating single-channel blood vessels and thus
does not support the generation of more complex, tree-like vessel topologies. in this work we propose a novel data-driven framework named vesselvae
for synthesizing blood vessel geometry. [14,15],
and document layout generation [20]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"once trained, the vessel-
vae latent space is sampled to generate new vessel geometries. to the best of
our knowledge, this work is the ﬁrst to synthesize multi-branch blood vessel trees
by learning from real data. experiments show that synth and real blood vessel
geometries are highly similar measured with the cosine similarity: radius (.97),
length (.95), and tortuosity (.96)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"the encoding and decoding processes are achieved through
a depth-ﬁrst traversal of the tree, where each node is combined with its parent
node recursively. the model outputs a hierarchy of vessel branches, where each
internal node in the hierarchy is represented by a vector that encodes its own
attributes and the information of all subsequent nodes in the tree. within the rvnn decoder network there are two essential components: the
node classiﬁer (cls) and the features decoder multi-layer perceptron (fea-
tures dec-mlp)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"conversely, the gz
network, situated after the bottleneck, facilitates the decoding of latent vari-
ables, aiding the decoder network in the reconstruction of tree structures. lectively, these supplementary networks streamline the data transformation pro-
cess through the model. all activation functions used in our networks are leaky
relus."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"several algorithms have been proposed in the literature
to generate a surface 3d mesh from a tree-structured centerline [29]. for sim-
plicity and eﬃciency, we chose the approach described in [6], which produces
good quality meshes from centerlines with a low sample rate. the implemented
method iterates through the points in the curve generating a coarse quadrilateral
recursive variational autoencoders for 3d blood vessel synthesis
71
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"for
the mesh reconstruction we used 4 iterations of catmull-clark subdivision algo-
rithm. the data pre-processing pipeline and network code were implemented in
python and pytorch framework. in all stages, we set the batch size to 10 and used the adam optimizer
with β1 = 0.9, β2 = 0.999, and a learning rate of 1 × 10−4."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"however, the memory
footprint during training is very small (≤1 gb) due to the use of a lightweight
tree representation. this means that the amount of memory required to store
and manipulate our training data structures is minimal. during training, we
ensure that the reconstructed tree aligns with the original structure, rather than
relying solely on the classiﬁer’s predictions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"we deﬁned a set of metrics to evaluate our trained network’s perfor-
mance. by using these metrics, we can determine how well the generated 3d
models of blood vessels match the original dataset distribution, as well as the
diversity of the generated output. we analyzed
tortuosity per branch, the vessel centerline total length, and the average radius
of the tree."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_7.pdf,"5
conclusions
we have presented a novel approach for synthesizing blood vessel models using
a variational recursive autoencoder. our method enables eﬃcient encoding and
decoding of binary tree structures, and produces high-quality synthesized mod-
els. this could lead to more accurate and eﬃcient modeling of blood vessels
and potentially other non-tree-like structures such as capillary networks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf,"limited by expensive pixel-level labels, polyp segmentation
models are plagued by data shortage and suﬀer from impaired general-
ization. in contrast, polyp bounding box annotations are much cheaper
and more accessible."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf,"to avoid interference, we introduce the
mask-to-box (m2b) transformation. by supervising the outer box mask
of the prediction instead of the prediction itself, m2b greatly mitigates
the mismatch between the coarse label and the precise prediction. but,
m2b only provides sparse supervision, leading to non-unique predictions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf,"all above models are fully supervised and require pixel-level annota-
tions. however, pixel-by-pixel labeling is time-consuming and expensive, which
hampers practical clinical usage. besides, many polyps do not have well-
deﬁned boundaries."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf,"figure 1(a) shows the diﬀerences between our weakpolyp
and fully supervised models. compared with fully supervised ones, weakpolyp
requires only a bounding box for each polyp, thus dramatically reducing the
labeling cost. more meaningfully, weakpolyp can take existing large-scale polyp
detection datasets to assist the polyp segmentation task."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf,"by forcing feature alignment, it inhibits
the excessive diversity of predictions, thus improving the model generalization. in summary, our contributions are three-fold: (1) we build the weakpolyp
model completely based on bounding box annotations, which largely reduces the
labeling cost and achieves a comparable performance to full supervision. (2) we
propose the m2b transformation to mitigate the mismatch between the predic-
tion and the supervision, and design the sc loss to improve the robustness of the
model against the variability of the predictions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf,"we design the m2b module, which consists of two steps:
projection and back-projection, as shown in fig. in this projection, instead of using mean pooling, we use max pooling
to pick the maximum value for each row/column in p. because max pooling can
completely remove the shape information of the polyp. after projection, only
the position and scope of the polyp are stored in pw and ph."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf,"in inference,
they will be removed, thus having no eﬀect on the speed of the model. lt otal = lsum + lsc
(5)
3
experiments
datasets. two large polyp datasets are adopted to evaluate the model per-
formance, including sun-seg [9] and polyp-seg."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf,"table 2. ablation studies on the sun-seg testing set under diﬀerent backbones. polyp-seg is our private polyp segmenta-
tion dataset, which contains 15,916 training images and 4,040 testing images. note that, during training, only bounding box annotations are adopted in our
weakpolyp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf,"all input images
are uniformly resized to 352×352. for data augmentation, random ﬂip, random
rotation, and multi-scale training are adopted. the whole network is trained in
an end-to-end way with an adamw optimizer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3/paper_72.pdf,"we train the entire model for 16 epochs. 1 compares the model performance under
diﬀerent supervisions, backbones, and datasets. the overall performance order
is gt>weakpolyp>box>grabcut."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"large medical imaging datasets can be cheaply and quickly
annotated with low-conﬁdence, weak labels (e.g., radiological scores). access to high-conﬁdence labels, such as histology-based diagnoses, is
rare and costly."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"these
methods typically require large batch sizes, which poses a diﬃculty in the
case of large 3d images at full resolution, due to limited gpu memory. nevertheless, volumetric positional information about the spatial context
of each 2d slice can be very important for some medical applications. in
this work, we propose an eﬃcient weakly-supervised positional (wsp)
contrastive learning strategy where we integrate both the spatial context
of each 2d slice and a weak label via a generic kernel-based loss function."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"we illustrate our method on cirrhosis prediction using a large volume of
weakly-labeled images, namely radiological low-conﬁdence annotations,
and small strongly-labeled (i.e., high-conﬁdence) datasets. the proposed
model improves the classiﬁcation auc by 5% with respect to a baseline
model on our internal dataset, and by 26% on the public lihc dataset
from the cancer genome atlas. the code is available at: https://github.
com/guerbet-ai/wsp-contrastive."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"it is however possible to obtain lower conﬁdence assessments for a large
amount of images, either by a clinical questioning, or directly by a radiological
diagnosis. to take advantage of large volumes of unlabeled or weakly-labeled
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0 22.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14220, pp. https://doi.org/10.1007/978-3-031-43907-0_22
228
e. sarfati et al.
images, pre-training encoders with self-supervised methods showed promising
results in deep learning for medical imaging [1,4,21,27–29]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"in particular, con-
trastive learning (cl) is a self-supervised method that learns a mapping of
the input images to a representation space where similar (positive) samples are
moved closer and diﬀerent (negative) samples are pushed far apart. weak discrete
labels can be integrated into contrastive learning by, for instance, considering
as positives only the samples having the same label, as in [13], or by directly
weighting unsupervised contrastive and supervised cross entropy loss functions,
as in [19]. in this work, we focus on the scenario where radiological meta-data
(thus, low-conﬁdence labels) are available for a large amount of images, whereas
high-conﬁdence labels, obtained by histological analysis, are scarce."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"for instance, in [4], the authors proposed to integrate depth in the
sampling strategy for the batch creation. these works implicitly assume a certain
threshold on depth to deﬁne positive and negative samples, which may be diﬃ-
cult to deﬁne and may be diﬀerent among applications and datasets. diﬀerently,
inspired by [2,8], here we propose to use a degree of “positiveness” between sam-
ples by deﬁning a kernel function w on depth positions."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"mathematically, this can be deﬁned as looking for a fθ that satisﬁes
the condition:
s−
tj − s+
ti ≤ 0
∀t, j, i
(1)
where s−
tj = sim(fθ(xt), fθ(x−
j )) and s+
ti = sim(fθ(xt), fθ(x+
i )), with sim a
similarity function deﬁned here as sim(a, b) = at b
τ
with τ > 0. in the presence of discrete labels y, the deﬁnition of negative (x−
j ) and posi-
tive (x+
i ) samples may change. [13], the authors deﬁne
as positives all images with the same discrete label y. however, when working
with continuous labels d, one cannot use the same strategy since all images are
somehow positive and negative at the same time."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"however, this requires a user-deﬁned hyper-
parameter γ, which could be hard to ﬁnd in practice. [13].
in this work, we propose to leverage both continuous d and discrete y labels,
by combining (here by multiplying) the previously deﬁned kernels, wσ and wδ,
into a composite kernel loss function. in this way, samples will be considered as
similar (positive) only if they have a composite degree of “positiveness” greater
than zero, namely both kernels have a value greater (or diﬀerent) than 0 (wσ > 0
and wδ ̸= 0)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"as commonly done in
cl [3], this condition can be transformed into an optimization problem using
230
e. sarfati et al.
fig. 1. example of representation space constructed by our loss function, leveraging
both continuous depth coordinate d and discrete label y (i.e., radiological diagnosis
yradio). = {i : yi = yt} as the set of indices of images xi in the
batch with the same discrete label yi as the anchor xt, we can rewrite our ﬁnal
loss function as:
lw sp = −
n

t=1

i∈p (t)
wσ(dt, di) log
	
exp(sti)

n
j̸=i exp(stj)

(4)
where wσ(dt, di) is normalized over i ∈ p(t)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"a robustness study is available in the supplementary material. for
the experiments, we ﬁx σ = 0.1.
3
experiments
we compare the proposed method with diﬀerent contrastive and non-contrastive
methods, that either use no meta-data (simclr [5], byol [10]), or leverage
weakly-supervised positional contrastive learning
231
only discrete labels (supcon [13]), or continuous labels (depth-aware [8]). the
proposed method is the only one that takes simultaneously into account both
discrete and continuous labels."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"moreover, the
large batch sizes necessary in contrastive learning can not be handled in 3d due
to a limited gpu memory. 3.1
datasets
three datasets of abdominal ct images are used in this study. one dataset is
used for contrastive pretraining, and the other two for evaluation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"the respective numbers are 1880, 385, 415
and 119. yradio is used as the discrete label y during pre-training. it contains 106 ct-scans from diﬀerent patients in portal venous
phase, with an identiﬁed histopathological status (metavir score) obtained
by a histological analysis, designated as y1
histo."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"[9],
which presents a histological score, the ishak score, designated as y2
histo, that
diﬀers from the metavir score present in d1
histo. this score is also dis-
tributed through ﬁve labels: no fibrosis, portal fibrosis, fibrous speta, nodu-
lar formation and incomplete cirrhosis and established cirrhosis. similarly
to the metavir score in d1
histo, we also binarize the ishak score, as proposed
in [16,20], which results in two cohorts of 34 healthy and 15 pathological patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"in all datasets, we select the slices based on the liver segmentation of the
patients. [18].
for the latter pretraining dataset, it presents an average slice spacing of 3.23 mm
with a standard deviation of 1.29 mm. for the x and y axis, the dimension is
0.79 mm per voxel on average, with a standard deviation of 0.10 mm.
3.2
architecture and optimization
backbones."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"more details
and an illustration of tinynet are available in the supplementary material, as
well as a full illustration of the algorithm ﬂow. data augmentation, sampling and optimization. [5,10,11]
require strong data augmentations on input images, in order to strengthen the
association between positive samples [22]."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"in our work, we leverage three types
of augmentations: rotations, crops and ﬂips. data augmentations are computed
on the gpu, using the kornia library [17]. during inference, we remove the aug-
mentation module to only keep the original input images."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"finally, we run our experiments on a tesla v100 with 16gb of ram and
a 6 cpu cores, and we used the pytorch-lightning library to implement our
models. all models share the same data augmentation module, with a batch size
of b = 64 and a ﬁxed number of epochs nepochs = 200. for all experiments, we
ﬁx a learning rate (lr) of α = 10−4 and a weight decay of λ = 10−4."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"we ﬁrst pretrain the backbone networks on dradio using
all previously listed contrastive and non-contrastive methods. then, we train
a regularized logistic regression on the frozen representations of the datasets
d1
histo and d2
histo. we use a stratiﬁed 5-fold cross-validation."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"we also train a regularized logistic regression on represen-
tations obtained with a random initialization as a second baseline (random). finally, we report the cross-validated results for each model on the aggregated
dataset d1+2
histo = d1
histo + d2
histo. weakly-supervised positional contrastive learning
233
table 1."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"= we use the pretrained weights from
imagenet with resnet-18 and run a logistic regression on the frozen representations. backbone pretraining method weak labels depth pos. 2. projections of the resnet-18 representation vectors of 10 randomly selected
subjects of d1
histo onto the ﬁrst two modes of a pca."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"blue = healthy
subjects.
4
results and discussion
we present in table 1 the results of all our experiments. for each of them, we
report whether the pretraining method integrates the weak label meta-data, the
depth spatial encoding, or both, which is the core of our method. first, we
can notice that our method outperforms all other pretraining methods in d1
histo
and d1+2
histo, which are the two datasets with more patients."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"for d1
histo, it can be noticed that wsp (ours) provides the best auc
score whatever the backbone used. for the second dataset d2
histo, our method
is on par with byol and supcon when using a small encoder and outperforms
the other methods when using a larger backbone. to illustrate the impact of the proposed method, we report in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"2 the pro-
jections of the resnet-18 representation vectors of 10 randomly selected subjects
of d1
histo onto the ﬁrst two modes of a pca. it can be noticed that the repre-
sentation space of our method is the only one where the diagnostic label (not
available during pretraining) and the depth position are correctly integrated. indeed, there is a clear separation between slices of diﬀerent classes (healthy at
the bottom and cirrhotic cases at the top) and at the same time it seems that the
depth position has been encoded in the x-axis, from left to right."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"supcon per-
forms well on the training set of dradio (ﬁgure available in the supplementary
material), as well as d2
histo with tinynet, but it poorly generalizes to d1
histo
and d1+2
histo. the method depth-aware manages to correctly encode the depth
position but not the diagnostic class label.
to assess the clinical performance of the pretraining methods, we also com-
pute the balanced accuracy scores (bacc) of the trained classiﬁers, which is
compared in table 2 to the bacc achieved by radiologists who were asked to
visually assess the presence or absence of cirrhosis for the n=106 cases of d1
histo. table 2. comparison of the pretraining methods
with a binary radiological annotation for cirrhosis
on d1
histo."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_22.pdf,"radiologists achieved a bacc
of 82% with respect to the histo-
logical reference. the two best-
performing methods surpassed
this
score:
depth-aware
and
the proposed wsp approach,
improving respectively the radi-
ologists score by 2% and 3%,
suggesting that including 3d information (depth) at the pretraining phase was
beneﬁcial."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf,"we propose an unsupervised deep learning method to recon-
struct a 3d tomographic image from biplanar x-rays, to reduce the num-
ber of required projections, the patient dose, and the acquisition time. to
address this ill-posed problem, we introduce prior knowledge of anatomic
structures by training a generative model on 3d cts of head and neck. we optimize the latent vectors of the generative model to recover a vol-
ume that both integrates this prior knowledge and ensures consistency
between the reconstructed image and input projections."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf,"this can improve image-guided therapies and preoperative planning, espe-
cially for radiotherapy, which requires precise patient positioning with minimal
radiation exposure. however, this task is an ill-posed inverse problem: x-ray measurements are
the result of attenuation integration across the body, which makes them very
supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43999-5_66. c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. (eds.): miccai 2023, lncs 14229, pp."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf,"by contrast with feed-forward methods, our approach does not require paired
projections-reconstructions, which are very tedious to acquire, and it can be used
with diﬀerent numbers of projections and diﬀerent projection geometries with-
out retraining. compared to nerf-based methods, our method exploits prior
knowledge from many patients to require only two projections. we evaluate our
method on reconstructing cancer patients’ head-and-neck cts, which involves
intricate and complicated structures."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf,"2.1
problem formulation
given a small set of projections {ii}i, possibly as few as two, we would like
to reconstruct the 3d tomographic volume v that generates these projections. this is a hard ill-posed problem, and to solve it, we need prior knowledge about
the possible volumes. (1)
term l(v, ii) is a log-likelihood."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf,"lp is the perceptual loss [13] between
projection of v and the observed projection ii. term r(v) is a regularization
term. it is crucial as it is the term that embodies prior knowledge about the
volume to reconstruct. as discussed in the introduction, we rely on a generative
model, which we describe in the next section."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf,"for the dis-
criminator, we use the non-saturating logistic loss with r1 regularization [19]. [14]
to improve learning of the model’s manifold with limited medical imaging data. 2.3
reconstruction from biplanar projections
since our generative model provides a volume v as a function of vectors w and
n, we can reparameterize our optimization from eq."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf,"we make a diﬀerentiable using [21] to
allow end-to-end optimization for reconstruction. 3
experiments and results
3.1
dataset and preprocessing
manifold learning. [1,6,16,17,28,32] and 1203 from
private internal data, after obtention of ethical approbations."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf,"we also
used 8 fully-convolutional layers with dimension 512 and an input latent vec-
tor of dimension 512, with tanh function as output activation. [15] and style mixing [15], and added a 0.2
probability for generating images without gaussian noise to focus on embedding
the most information. we augmented the discriminator with vertical and depth-
oriented ﬂips, rotation, scaling, motion blur and gaussian noise at a probability
of 0.2."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf,"we tested our model’s ability to learn the low-dimensional
manifold. we used fid [9] to measure the distance between the distribution of
generated volumes and real volumes, and ms-ssim [29] to evaluate volumes’
diversity and quality. we obtained a 3d fid of 46 and a ms-ssim of 0.92."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10/paper_66.pdf,"additionally, no public implementation is available. [26]
uses a ﬂow-based generative model, but the results are of lower quality compared
to gans and similar to x2ct-gan [30]. to evaluate our method’s performance with biplanar pro-
jections, we focused on positioning imaging for radiotherapy."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"accurate polyp detection is essential for assisting clinical rec-
tal cancer diagnoses. colonoscopy videos contain richer information than
still images, making them a valuable resource for deep learning methods. however, unlike common ﬁxed-camera video, the camera-moving scene
in colonoscopy videos can cause rapid video jitters, leading to unstable
training for existing video detection models."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"in this paper, we propose
the yona (you only need one adjacent reference-frame) method,
an eﬃcient end-to-end training framework for video polyp detection. yona fully exploits the information of one previous adjacent frame
and conducts polyp detection on the current frame without multi-frame
collaborations. speciﬁcally, for the foreground, yona adaptively aligns
the current frame’s channel activation patterns with its adjacent refer-
ence frames according to their foreground similarity."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"keywords: video polyp detection · colonoscopy · feature alignment ·
contrastive learning
y. jiang and z. zhang—equal contribution. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43904-9_5.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43904-9_5
yona for accurate and fast video polyp detection
45
1
introduction
colonoscopy plays a crucial role in identifying and removing early polyps and
reducing mortality rates associated with rectal cancer."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"1) fast motion
speed. [9] (colonoscopy) dataset. the motion speed in
imagenetvid evenly distributes in three intervals."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"thus we conjecture that collaborating too many frames for polyp video detec-
tion will increase the misalignment between adjacent frames and leads to poor
detection performance. [26] on two
datasets with increasing reference frames. the diﬀerent trends of the two lines
conﬁrm our hypothesis."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"diﬀerent from the common
camera-ﬁxed videos, the camera-moving of colonoscopy video will introduce large
disturbances between adjacent frames (e.g., specular reﬂection, bubbles, water,
fig. 1. (a) the histogram of the motion ious distribution on two datasets. lower
motion iou denotes a faster target moving speed."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"as shown in fig. 1(e), we noticed that some polyps could be seen as
concealed objects in the colonoscopy video since such polyps have a very similar
appearance to the intestine wall. the model will be confused by such frames in
inference and result in high false-positive or false-negative predictions.
to address the above issues, we propose the yona framework, which fully
exploits the reference frame information and only needs one adjacent reference
frame for accurate video polyp detection. speciﬁcally, we propose the foreground
temporal alignment (fta) module to explicitly align the foreground channel
activation patterns between adjacent features according to their foreground simi-
larity."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"it further introduces the
cross-frame contrastive learning module to enhance the model’s discrimination
ability of polyps and intestine walls. (3) extensive experiments demonstrate that
our yona achieves new state-of-the-art performance on three large-scale public
video polyp detection datasets. 2
method
the whole pipeline is shown in fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"as a result, multi-frame
(reference>3) fusion may easily incorporate more noise features into the aggre-
gation features. on the other hand, the occluded or distorted foreground context
may also inﬂuence the quality of aggregation. thus we propose to conduct tem-
poral alignment between adjacent features by leveraging the foreground context
of only one adjacent reference frame."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"adaptive re-weighting by similarity measuring. as discussed above, due
to video jitters, adjacent frames may change rapidly at the temporal level, and
directly fusing the reference feature will introduce noisy information and mis-
guide the training. thus we designed an adaptive re-weighting method by mea-
suring the feature similarity, where the weight indicates the importance of the
reference feature to the anchor feature."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"[18], we select the foreground and background
region on both two frames guided by ground truth boxes to conduct contrastive
learning. in practice, given a batch of intermediate feature maps f a, f r ∈
rn×t ×c×h×w and corresponding binary maps m a, m r ∈ rn×t ×h×w , we
ﬁrst concatenate the anchor and reference at the batch-wise level as ˆf
∈
rnt ×c×h×w and ˆ
m ∈ rnt ×h×w to exploit the cross-frame information. then
we extract the foreground and background channel patterns of cross-frame fea-
ture ˆf using the eq. 1 base on
ˆ
m(x, y) = 1 and
ˆ
m(x, y) = 0, respectively."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"[18]:
lnce
j
= −log
exp(qj·i+/τ)
exp(qj·i+/τ)+
i−∈nj exp(qj·i−/τ)
(7)
where qj ∈ rc, j = 0, ..., nt is the query feature, i+ ∈ rc and i− ∈ rnt ×c are
positives and negatives. nj denote embedding collections of the negatives. [9] (train set: 20,942 frames, test set: 12,933
frames), and cvc-videoclinicdb [1] (train set: 7995 frames, test set: 2030
frames)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"p,
r, and f1 denote the precision, recall, and f1-score. †: results from the original paper
with the same data division. the best score is marked as red, while the second best
score is marked as blue."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"we randomly crop and resize the images to 512×512 and normalize them
using imagenet settings. random rotation and ﬂip with probability p = 0.5 are
used for data augmentation. we set the batch size n = 32."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"firstly, compared with the centernet baseline, our yona with three
novel designs signiﬁcantly improved the f1 score by 9.2%, 8.3%, and 7.4% on
three benchmarks, demonstrating the eﬀectiveness of the model design. besides,
yona achieves the best trade-oﬀ between accuracy and speed compared with
all other image-based sotas across all datasets. second, for video-based com-
petitors, previous video object detectors with multiple frame collaborations lack
the ability for accurate detection on challenging datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5/paper_5.pdf,"figure 3 visualizes the qualitative results of yona
with other competitors [19,25]. thanks to this one-adjacent-frame framework,
our yona can not only prevent the false positive caused by part occlusion (1st
and 2nd clips) but also capture useful information under severe image quality
(2nd clip). moreover, our yona shows robust performance even for challenging
scenarios like concealed polyps (3rd clip)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"firstly, it requires costly manual annotations. supplementary information the online version contains supplementary material
available at https://doi.org/10.1007/978-3-031-43907-0_58.
c
⃝ the author(s), under exclusive license to springer nature switzerland ag 2023
h. greenspan et al. https://doi.org/10.1007/978-3-031-43907-0_58
606
m. goncharov et al.
secondly, the resulting models may not generalize well to unseen data domains."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"even small changes in the task may result in a signiﬁcant drop in performance,
requiring re-training from scratch [18].
self-supervised learning (ssl) is a promising solution to these limitations. ssl pre-trains a model backbone to extract informative representations from
unlabeled data. then, a simple linear or non-linear head on top of the frozen
pre-trained backbone can be trained for various downstream tasks in a supervised
manner (linear or non-linear probing)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"alternatively, the backbone can be ﬁne-
tuned for a downstream task along with the head. pre-training the backbone
in a self-supervised manner enables scaling to larger datasets across multiple
data and task domains. in medical imaging, this is particularly useful given the
growing number of available datasets."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"our simple negative sampling
strategy and the idea of storing voxel-level representations in a feature pyramid
form result in high-dimensional, ﬁne-grained, multi-scale representations suitable
for the segmentation of diﬀerent organs and tumors in full resolution. second,
we employ vox2vec to pre-train a fpn architecture on a diverse collection of six
unannotated datasets, totaling over 6,500 ct images of the thorax and abdomen. we make the pre-trained model publicly available to simplify the reproduction
of our results and to encourage practitioners to utilize this model as a starting
vox2vec
607
point for the segmentation algorithms training."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"we show that
vox2vec performs slightly better than sota models in the ﬁne-tuning setup and
outperforms them by a huge margin in the linear and non-linear probing setups. to the best of our knowledge, this is the ﬁrst successful attempt to evaluate dense
ssl methods in the medical imaging domain in linear and non-linear probing
regimes. [8] or non-contrastive [9] joint embedding methods."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"[29] was mainly proposed for image retrieval
and uses only feature representations in the largest and smallest scales in separate
contrastive losses, while vox2vec produce voxels’ representations via concatena-
tion of feature vectors from a feature pyramid and pre-train them in a uniﬁed
manner using a single contrastive loss. finally, a number of works propose semi-
supervised contrastive learning methods [20], however, they require additional
task-speciﬁc manual labeling. 3
method
in a nutshell, vox2vec pre-trains a neural network to produce similar repre-
sentations for the same voxel placed in diﬀerent contexts (positive pairs) and
608
m. goncharov et al.
concat
projection 
head
fpn
fig."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"3.3
loss function
at each pre-training iteration, we fed 2 · n patches to the fpn and obtain the
representations for n positive pairs of voxels. 610
m. goncharov et al.
3.4
evaluation protocol
we evaluate the quality of self-supervised voxel-level representations on down-
stream segmentation tasks in three setups: 1) linear probing, 2) non-linear prob-
ing, and 3) end-to-end ﬁne-tuning.
linear or non-linear probing means training a voxel-wise linear or non-linear
classiﬁer on top of the frozen representations. if the representations are modeled
by the unet model, such classiﬁer can be implemented as one or several 1 × 1
convolutional layers with a kernel size 1 on top of the output feature map."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"[1,3,5,15,21,27], totaling
more than 6550 cts, covering abdomen and thorax domains. we do not use
the annotations for these datasets during the pre-training stage. pre-processing
includes the following steps: 1) cropping to the minimal volume containing all
the voxels with the intensity greater than −500 hu; 2) interpolation to the voxel
spacing of 1 × 1 × 2 mm3 (intensities are clipped and rescaled at the augmen-
tation step, see sect. 3.1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"further details about the
pre-training setup can be found in supplementary materials. the btcv dataset
consists of 30 ct scans along with 13 diﬀerent organ annotations. we test our
method on 6 ct msd datasets, which include 9 diﬀerent organ and tumor
segmentation tasks."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"a 5 fold cross-validation is used for btcv experiments, and
vox2vec
611
a 3 fold cross-validation for msd experiments. the segmentation performance
of each model on btcv and msd datasets is evaluated by the dice score. for our method, the pre-processing steps are the same for all datasets, as at
the pre-training stage, but in addition, intensities are clipped to (−1350, 1000)
hu window and rescaled to (0, 1)."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"in the ﬁne-tuning setup, we freeze the backbone for the ﬁrst
15000 batches and then exponentially increase the learning rate for the backbone
parameters from 0.00003 up to 0.0003 during 1200 batches. 5
results
the mean value and standard deviation of dice score across 5 folds on the btcv
dataset for all models in all evaluation setups are presented in table 1. vox2vec-
fpn performs slightly better than other models in the ﬁne-tuning setup."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"we
demonstrate an example of the excellent performance of vox2vec-fpn in both
linear and non-linear probing regimes in supplementary materials. we reproduce the key results on msd challenge ct datasets, which contain
tumor and organ segmentation tasks. table 2 shows that in the vox2vec repre-
sentation space, organ voxels can be separated from tumor voxels with a quality
comparable to the model trained from scratch."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"612
m. goncharov et al.
table 1. average cross validation dice scores
on btcv multi-organ segmentation dataset. 2.
dice score on btcv cross-
validation averaged for all organs w.r.t."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"our method expands the contrastive
learning setup to the feature pyramid architecture allowing to pre-train eﬀective
representations in full resolution. by pre-training a fpn backbone to extract
informative representations from unlabeled data, our method scales to large
datasets across multiple task domains. we pre-train a fpn architecture on
more than 6500 ct images and test it on various segmentation tasks, including
diﬀerent organs and tumors segmentation in three setups: linear probing, non-
linear probing, and ﬁne-tuning."
/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1/paper_58.pdf,"still, this work has a few limitations to consider. we plan to investigate
further how the performance of vox2vec scales with the increasing size of the
vox2vec
613
pre-training dataset and the pre-trained architecture size. another interesting
research direction is exploring the eﬀectiveness of vox2vec with regard to domain
adaptation to address the challenges of domain shift between diﬀerent medical
imaging datasets obtained from diﬀerent sources."
