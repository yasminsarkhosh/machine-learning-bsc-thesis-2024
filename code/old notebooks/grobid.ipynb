{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xml.etree import ElementTree as et\n",
    "from lxml import etree\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text, width=100):\n",
    "    \"\"\"\n",
    "    A simple function to wrap text at a given width.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return text  # Handle NaN values\n",
    "    \n",
    "    wrapped_lines = []\n",
    "    for paragraph in text.split('\\n'):  # Splitting by existing newlines to preserve paragraph breaks\n",
    "        line = ''\n",
    "        for word in paragraph.split():\n",
    "            if len(line) + len(word) + 1 > width:\n",
    "                wrapped_lines.append(line)\n",
    "                line = word\n",
    "            else:\n",
    "                line += (' ' + word if line else word)\n",
    "        wrapped_lines.append(line)\n",
    "    return '\\n'.join(wrapped_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from grobid_client.grobid_client import GrobidClient\n",
    "\n",
    "\n",
    "#client = GrobidClient(grobid_server='https://kermitt2-grobid.hf.space/')\n",
    "#client = GrobidClient(grobid_server='http://localhost:8081')\n",
    "client = GrobidClient(grobid_server='http://localhost:8070')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol1'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol01\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol2'\n",
    "#client.process('processFulltextDocument', process_file, output = \"./vol02\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol3'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol03\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol4'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol04\", force=True)\n",
    "\n",
    "process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol5'\n",
    "client.process('processFulltextDocument', process_file, output=\"./vol05\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol6'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol06\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol7'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol07\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol8'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol08\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol9'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol09\", force=True)\n",
    "\n",
    "#process_file = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/miccai_2023/miccai23vol10'\n",
    "#client.process('processFulltextDocument', process_file, output=\"./vol10\", force=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol01'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol02'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol03'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol04'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol05'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol06'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol07'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol08'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol09'\n",
    "# folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming XML files by folder path\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 'paper_63.grobid.tei.xml' to 'Unsupervised_Classification_of_Congenital_Inner_Ear_Malformations_Using_DeepDiffusion_for_Latent_Space_Representation.xml'\n",
      "Renamed 'paper_76.grobid.tei.xml' to 'Convolving_Directed_Graph_Edges_via_Hodge_Laplacian_for_Brain_Network_Analysis.xml'\n",
      "Renamed 'paper_12.grobid.tei.xml' to 'Learning_with_Synthesized_Data_for_Generalizable_Lesion_Detection_in_Real_PET_Images.xml'\n",
      "Renamed 'paper_71.grobid.tei.xml' to 'Flexible_Unfolding_of_Circular_Structures_for_Rendering_Textbook-Style_Cerebrovascular_Maps.xml'\n",
      "Renamed 'paper_64.grobid.tei.xml' to 'How_Does_Pruning_Impact_Long-Tailed_Multi-label_Medical_Image_Classifiers?.xml'\n",
      "Renamed 'paper_15.grobid.tei.xml' to 'Cluster-Induced_Mask_Transformers_for_Effective_Opportunistic_Gastric_Cancer_Screening_on_Non-contrast_CT_Scans.xml'\n",
      "Renamed 'paper_65.grobid.tei.xml' to 'Multimodal_Deep_Fusion_in_Hyperbolic_Space_for_Mild_Cognitive_Impairment_Study.xml'\n",
      "Renamed 'paper_70.grobid.tei.xml' to 'Open-Ended_Medical_Visual_Question_Answering_Through_Prefix_Tuning_of_Language_Models.xml'\n",
      "Renamed 'paper_14.grobid.tei.xml' to 'Multi-view_Vertebra_Localization_and_Identification_from_CT_Images.xml'\n",
      "Renamed 'paper_62.grobid.tei.xml' to 'MUVF-YOLOX:_A_Multi-modal_Ultrasound_Video_Fusion_Network_for_Renal_Tumor_Diagnosis.xml'\n",
      "Renamed 'paper_13.grobid.tei.xml' to 'Robust_Exclusive_Adaptive_Sparse_Feature_Selection_for_Biomarker_Discovery_and_Early_Diagnosis_of_Neuropsychiatric_Systemic_Lupus_Erythematosus.xml'\n",
      "Renamed 'paper_11.grobid.tei.xml' to 'Graph-Theoretic_Automatic_Lesion_Tracking_and_Detection_of_Patterns_of_Lesion_Changes_in_Longitudinal_CT_Studies.xml'\n",
      "Renamed 'paper_75.grobid.tei.xml' to 'M&M:_Tackling_False_Positives_in_Mammography_with_a_Multi-view_and_Multi-instance_Learning_Sparse_Detector.xml'\n",
      "Renamed 'paper_60.grobid.tei.xml' to 'Identification_of_Disease-Sensitive_Brain_Imaging_Phenotypes_and_Genetic_Factors_Using_GWAS_Summary_Statistics.xml'\n",
      "Renamed 'paper_68.grobid.tei.xml' to 'Transformer-Based_Tooth_Segmentation,_Identification_and_Pulp_Calcification_Recognition_in_CBCT.xml'\n",
      "Renamed 'paper_19.grobid.tei.xml' to 'Utilizing_Longitudinal_Chest_X-Rays_and_Reports_to_Pre-fill_Radiology_Reports.xml'\n",
      "Renamed 'paper_16.grobid.tei.xml' to 'Detecting_Domain_Shift_in_Multiple_Instance_Learning_for_Digital_Pathology_Using_Fréchet_Domain_Distance.xml'\n",
      "Renamed 'paper_67.grobid.tei.xml' to 'Improved_Flexibility_and_Interpretability_of_Large_Vessel_Stroke_Prognostication_Using_Image_Synthesis_and_Multi-task_Learning.xml'\n",
      "Renamed 'paper_72.grobid.tei.xml' to 'Dynamic_Curriculum_Learning_via_In-Domain_Uncertainty_for_Medical_Image_Classification.xml'\n",
      "Renamed 'paper_17.grobid.tei.xml' to 'Positive_Definite_Wasserstein_Graph_Kernel_for_Brain_Disease_Diagnosis.xml'\n",
      "Renamed 'paper_73.grobid.tei.xml' to 'Joint_Prediction_of_Response_to_Therapy,_Molecular_Traits,_and_Spatial_Organisation_in_Colorectal_Cancer_Biopsies.xml'\n",
      "Renamed 'paper_66.grobid.tei.xml' to 'Hierarchical_Vision_Transformers_for_Disease_Progression_Detection_in_Chest_X-Ray_Images.xml'\n",
      "Renamed 'paper_69.grobid.tei.xml' to 'Treatment_Outcome_Prediction_for_Intracerebral_Hemorrhage_via_Generative_Prognostic_Model_with_Imaging_and_Tabular_Data.xml'\n",
      "Renamed 'paper_18.grobid.tei.xml' to 'A_Reliable_and_Interpretable_Framework_of_Multi-view_Learning_for_Liver_Fibrosis_Staging.xml'\n",
      "Renamed 'paper_10.grobid.tei.xml' to 'DiffULD:_Diffusive_Universal_Lesion_Detection.xml'\n",
      "Renamed 'paper_61.grobid.tei.xml' to 'Revisiting_Feature_Propagation_and_Aggregation_in_Polyp_Segmentation.xml'\n",
      "Renamed 'paper_74.grobid.tei.xml' to 'Distributionally_Robust_Image_Classifiers_for_Stroke_Diagnosis_in_Accelerated_MRI.xml'\n",
      "Renamed 'paper_48.grobid.tei.xml' to 'Contrastive_Masked_Image-Text_Modeling_for_Medical_Visual_Representation_Learning.xml'\n",
      "Renamed 'paper_39.grobid.tei.xml' to 'Uncertainty_Inspired_Autism_Spectrum_Disorder_Screening.xml'\n",
      "Renamed 'paper_3.grobid.tei.xml' to 'SHISRCNet:_Super-Resolution_and_Classification_Network_for_Low-Resolution_Breast_Cancer_Histopathology_Image.xml'\n",
      "Renamed 'paper_31.grobid.tei.xml' to 'SwIPE:_Efficient_and_Robust_Medical_Image_Segmentation_with_Implicit_Patch_Embeddings.xml'\n",
      "Renamed 'paper_24.grobid.tei.xml' to 'Improved_Prognostic_Prediction_of_Pancreatic_Cancer_Using_Multi-phase_CT_by_Integrating_Neural_Distance_and_Texture-Aware_Transformer.xml'\n",
      "Renamed 'paper_55.grobid.tei.xml' to 'Self-supervised_Learning_for_Endoscopic_Video_Analysis.xml'\n",
      "Renamed 'paper_40.grobid.tei.xml' to 'Rad-ReStruct:_A_Novel_VQA_Benchmark_and_Method_for_Structured_Radiology_Reporting.xml'\n",
      "Renamed 'paper_4.grobid.tei.xml' to 'cOOpD:_Reformulating_COPD_Classification_on_Chest_CT_Scans_as_Anomaly_Detection_Using_Contrastive_Representations.xml'\n",
      "Renamed 'paper_23.grobid.tei.xml' to 'Discovering_Brain_Network_Dysfunction_in_Alzheimer’s_Disease_Using_Brain_Hypergraph_Neural_Network.xml'\n",
      "Renamed 'paper_36.grobid.tei.xml' to 'Style-Based_Manifold_for_Weakly-Supervised_Disease_Characteristic_Discovery.xml'\n",
      "Renamed 'paper_47.grobid.tei.xml' to 'Diversity-Preserving_Chest_Radiographs_Generation_from_Reports_in_One_Stage.xml'\n",
      "Renamed 'paper_52.grobid.tei.xml' to 'Text-Guided_Cross-Position_Attention_for_Segmentation:_Case_of_Medical_Image.xml'\n",
      "Renamed 'paper_37.grobid.tei.xml' to 'COVID-19_Pneumonia_Classification_with_Transformer_from_Incomplete_Modalities.xml'\n",
      "Renamed 'paper_22.grobid.tei.xml' to 'Punctate_White_Matter_Lesion_Segmentation_in_Preterm_Infants_Powered_by_Counterfactually_Generative_Learning.xml'\n",
      "Renamed 'paper_53.grobid.tei.xml' to 'Visual-Attribute_Prompt_Learning_for_Progressive_Mild_Cognitive_Impairment_Prediction.xml'\n",
      "Renamed 'paper_46.grobid.tei.xml' to 'Improving_Image-Based_Precision_Medicine_with_Uncertainty-Aware_Causal_Models.xml'\n",
      "Renamed 'paper_5.grobid.tei.xml' to 'YONA:_You_Only_Need_One_Adjacent_Reference-Frame_for_Accurate_and_Fast_Video_Polyp_Detection.xml'\n",
      "Renamed 'paper_25.grobid.tei.xml' to 'Contrastive_Feature_Decoupling_for_Weakly-Supervised_Disease_Detection.xml'\n",
      "Renamed 'paper_30.grobid.tei.xml' to 'What_Do_AEs_Learn?_Challenging_Common_Assumptions_in_Unsupervised_Anomaly_Detection.xml'\n",
      "Renamed 'paper_41.grobid.tei.xml' to 'Xplainer:_From_X-Ray_Observations_to_Explainable_Zero-Shot_Diagnosis.xml'\n",
      "Renamed 'paper_54.grobid.tei.xml' to 'Acute_Ischemic_Stroke_Onset_Time_Classification_with_Dynamic_Convolution_and_Perfusion_Maps_Fusion.xml'\n",
      "Renamed 'paper_49.grobid.tei.xml' to 'Adjustable_Robust_Transformer_for_High_Myopia_Screening_in_Optical_Coherence_Tomography.xml'\n",
      "Renamed 'paper_2.grobid.tei.xml' to 'You_Don’t_Have_to_Be_Perfect_to_Be_Amazing:_Unveil_the_Utility_of_Synthetic_Images.xml'\n",
      "Renamed 'paper_38.grobid.tei.xml' to 'Enhancing_Breast_Cancer_Risk_Prediction_by_Incorporating_Prior_Images.xml'\n",
      "Renamed 'paper_43.grobid.tei.xml' to 'Boosting_Breast_Ultrasound_Video_Classification_by_the_Guidance_of_Keyframe_Feature_Centers.xml'\n",
      "Renamed 'paper_56.grobid.tei.xml' to 'Fast_Non-Markovian_Diffusion_Model_for_Weakly_Supervised_Anomaly_Detection_in_Brain_MR_Images.xml'\n",
      "Renamed 'paper_8.grobid.tei.xml' to 'Liver_Tumor_Screening_and_Diagnosis_in_CT_with_Pixel-Lesion-Patient_Network.xml'\n",
      "Renamed 'paper_27.grobid.tei.xml' to 'Text-Guided_Foundation_Model_Adaptation_for_Pathological_Image_Classification.xml'\n",
      "Renamed 'paper_32.grobid.tei.xml' to 'Smooth_Attention_for_Deep_Multiple_Instance_Learning:_Application_to_CT_Intracranial_Hemorrhage_Detection.xml'\n",
      "Renamed 'paper_51.grobid.tei.xml' to 'Recruiting_the_Best_Teacher_Modality:_A_Customized_Knowledge_Distillation_Method_for_if_Based_Nephropathy_Diagnosis.xml'\n",
      "Renamed 'paper_44.grobid.tei.xml' to 'Learning_with_Domain-Knowledge_for_Generalizable_Prediction_of_Alzheimer’s_Disease_from_Multi-site_Structural_MRI.xml'\n",
      "Renamed 'paper_35.grobid.tei.xml' to 'CARL:_Cross-Aligned_Representation_Learning_for_Multi-view_Lung_Cancer_Histology_Classification.xml'\n",
      "Renamed 'paper_20.grobid.tei.xml' to 'Parse_and_Recall:_Towards_Accurate_Lung_Nodule_Malignancy_Prediction_Like_Radiologists.xml'\n",
      "Renamed 'paper_7.grobid.tei.xml' to 'Patients_and_Slides_are_Equal:_A_Multi-level_Multi-instance_Learning_Framework_for_Pathological_Image_Analysis.xml'\n",
      "Renamed 'paper_28.grobid.tei.xml' to 'Multiple_Prompt_Fusion_for_Zero-Shot_Lesion_Detection_Using_Vision-Language_Models.xml'\n",
      "Renamed 'paper_59.grobid.tei.xml' to 'Visual_Grounding_of_Whole_Radiology_Reports_for_3D_CT_Images.xml'\n",
      "Renamed 'paper_29.grobid.tei.xml' to 'Reversing_the_Abnormal:_Pseudo-Healthy_Generative_Networks_for_Anomaly_Detection.xml'\n",
      "Renamed 'paper_6.grobid.tei.xml' to 'Personalized_Patch-Based_Normality_Assessment_of_Brain_Atrophy_in_Alzheimer’s_Disease.xml'\n",
      "Renamed 'paper_58.grobid.tei.xml' to 'A_Multimodal_Disease_Progression_Model_for_Genetic_Associations_with_Disease_Dynamics.xml'\n",
      "Renamed 'paper_45.grobid.tei.xml' to 'GSDG:_Exploring_a_Global_Semantic-Guided_Dual-Stream_Graph_Model_for_Automated_Volume_Differential_Diagnosis_and_Prognosis.xml'\n",
      "Renamed 'paper_50.grobid.tei.xml' to 'Improving_Outcome_Prediction_of_Pulmonary_Embolism_by_De-biased_Multi-modality_Model.xml'\n",
      "Renamed 'paper_21.grobid.tei.xml' to 'Privacy-Preserving_Early_Detection_of_Epileptic_Seizures_in_Videos.xml'\n",
      "Renamed 'paper_34.grobid.tei.xml' to 'Beyond_the_Snapshot:_Brain_Tokenized_Graph_Transformer_for_Longitudinal_Brain_Functional_Connectome_Embedding.xml'\n",
      "Renamed 'paper_1.grobid.tei.xml' to 'Automatic_Bleeding_Risk_Rating_System_of_Gastric_Varices.xml'\n",
      "Renamed 'paper_57.grobid.tei.xml' to 'Self-supervised_Polyp_Re-identification_in_Colonoscopy.xml'\n",
      "Renamed 'paper_42.grobid.tei.xml' to 'Towards_Generalizable_Diabetic_Retinopathy_Grading_in_Unseen_Domains.xml'\n",
      "Renamed 'paper_33.grobid.tei.xml' to 'DCAug:_Domain-Aware_and_Content-Consistent_Cross-Cycle_Framework_for_Tumor_Augmentation.xml'\n",
      "Renamed 'paper_26.grobid.tei.xml' to 'Uncovering_Heterogeneity_in_Alzheimer’s_Disease_from_Graphical_Modeling_of_the_Tau_Spatiotemporal_Topography.xml'\n",
      "Renamed 'paper_9.grobid.tei.xml' to 'Self-_and_Semi-supervised_Learning_for_Gastroscopic_Lesion_Detection.xml'\n"
     ]
    }
   ],
   "source": [
    "# Rename_xml_files_in_folder(folder_path)\n",
    "def find_title(element):\n",
    "    \"\"\"Recursively search for the title element in the XML structure.\"\"\"\n",
    "    if 'title' in element.tag.lower() and element.text:\n",
    "        return element.text.strip()\n",
    "    for child in element:\n",
    "        title = find_title(child)\n",
    "        if title:\n",
    "            return title\n",
    "    return None\n",
    "\n",
    "def rename_xml_files_in_folder(folder_path):\n",
    "    \"\"\"Rename XML files based on their title tags.\"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if not filename.endswith('.xml'):  # Skip non-XML files\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            tree = et.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            paper_title = find_title(root)\n",
    "            if paper_title:\n",
    "                new_filename = paper_title.replace(\" \", \"_\") + '.xml'\n",
    "                new_file_path = os.path.join(folder_path, new_filename)\n",
    "                os.rename(file_path, new_file_path)\n",
    "                print(f\"Renamed '{filename}' to '{new_filename}'\")\n",
    "            else:\n",
    "                print(f\"Title not found in '{filename}'. Skipping.\")\n",
    "        except et.ParseError as e:\n",
    "            print(f\"Error parsing '{filename}': {e}\")\n",
    "\n",
    "# Rename the XML files in this folder\n",
    "folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol05'\n",
    "rename_xml_files_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually renaming the XML files \n",
    "***\n",
    "(some files are wrongly named/not named for some strange reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/papers_xml/vol02/COLosSAL:_A_Benchmark_for_Cold-Start_Active_Learning_for_3D_Medical_Image_Segmentation.xml'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually renaming the XML files based on their title tags\n",
    "\n",
    "# Load and parse the XML file\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol09/paper_59.grobid.tei.xml'\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/output2/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "\n",
    "''' Paper 44 XML file: title was too long to be used as a file name, removed '/CT Self-supervised Denoising' from the title '''\n",
    "#file_path ='/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/paper_44.grobid.tei.xml'\n",
    "\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/output2/paper_13.grobid.tei.xml'\n",
    "\n",
    "'''FileNotFoundError: [Errno 2] No such file or directory: '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/paper_49.grobid.tei.xml' -> \n",
    "'/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/A_Patient-Specific_Self-supervised_Model_for_Automatic_X-Ray/CT_Registration.xml'\n",
    "Solution: removed '/CT_Registration' from title\n",
    "'''\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/paper_49.grobid.tei.xml'\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/papers_xml/vol04/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "#file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/papers_xml/vol01/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "file_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/papers_xml/vol02/Medical_Image_Computing_and_Computer_Assisted_Intervention_–_MICCAI_2023.xml'\n",
    "\n",
    "tree = et.parse(file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Since XML namespaces can complicate direct tag access, we find the title tag dynamically.\n",
    "# This approach is based on the assumption that titles are relatively unique in structure.\n",
    "\n",
    "# Attempt to extract the paper title. This might need adjustments based on the actual structure.\n",
    "title = None\n",
    "for elem in root.iter():\n",
    "    if 'title' in elem.tag.lower():\n",
    "        title = elem.text\n",
    "        break\n",
    "\n",
    "title_clean = title.strip().replace(\" \", \"_\") if title else \"Untitled_Document\"\n",
    "title_clean\n",
    "\n",
    "# Attempt a more generic search for the title, considering common patterns in scholarly articles\n",
    "# We'll look for title elements that might be nested within other elements (like \"titleStmt\" or \"fileDesc\" in TEI format)\n",
    "\n",
    "def find_title(element):\n",
    "    \"\"\"\n",
    "    Recursively search for the title element in the XML structure.\n",
    "    \"\"\"\n",
    "    if 'title' in element.tag.lower() and element.text:\n",
    "        return element.text.strip()\n",
    "    for child in element:\n",
    "        title = find_title(child)\n",
    "        if title:\n",
    "            return title\n",
    "    return None\n",
    "\n",
    "# Attempt to find the title using the recursive search\n",
    "paper_title = find_title(root)\n",
    "paper_title_clean = paper_title.replace(\" \", \"_\") if paper_title else \"Untitled_Document\"\n",
    "paper_title, paper_title_clean\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the new file path with the clean title\n",
    "new_file_path = os.path.join(os.path.dirname(file_path), f\"{paper_title_clean}.xml\")\n",
    "\n",
    "# Rename the file\n",
    "os.rename(file_path, new_file_path)\n",
    "\n",
    "new_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming XML files, extracting paper titles and text\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .xpath() method with the string() function in XPath concatenates all text nodes within the current context. This method can retrieve all text within an element, including text in nested child elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 headers in 'AMAE: Adaptation of Pre-trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays'\n",
      "Found 19 headers in 'Unsupervised Domain Adaptation for Anatomical Landmark Detection'\n",
      "Found 13 headers in 'CT-Guided, Unsupervised Super-Resolution Reconstruction of Single 3D Magnetic Resonance Image'\n",
      "Found 18 headers in 'Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection'\n",
      "Found 12 headers in 'Multi-modal Variational Autoencoders for Normative Modelling Across Multiple Imaging Modalities'\n",
      "Found 18 headers in 'Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction'\n",
      "Found 22 headers in 'MedIM: Boost Medical Image Representation via Radiology Report-Guided Masking'\n",
      "Found 22 headers in 'Dual Conditioned Diffusion Models for Out-of-Distribution Detection: Application to Fetal Ultrasound Videos'\n",
      "Found 17 headers in 'Unsupervised Domain Transfer with Conditional Invertible Neural Networks'\n",
      "Found 19 headers in 'Anatomy-Driven Pathology Detection on Chest X-rays'\n",
      "Found 16 headers in 'Self-supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET'\n",
      "Found 15 headers in 'Foundation Ark: Accruing and Reusing Knowledge for Superior and Robust Performance'\n",
      "Found 12 headers in 'LOTUS: Learning to Optimize Task-Based US Representations'\n",
      "Found 18 headers in 'Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning'\n",
      "Found 18 headers in 'AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor'\n",
      "Found 13 headers in 'Correlation-Aware Mutual Learning for Semi-supervised Medical Image Segmentation'\n",
      "Found 12 headers in 'Multi-IMU with Online Self-consistency for Freehand 3D Ultrasound Reconstruction'\n",
      "Found 19 headers in 'Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering'\n",
      "Found 17 headers in 'Inter-slice Consistency for Unpaired Low-Dose CT Denoising Using Boosted Contrastive Learning'\n",
      "Found 13 headers in 'You’ve Got Two Teachers: Co-evolutionary Image and Report Distillation for Semi-supervised Anatomical Abnormality Detection in Chest X-Ray'\n",
      "Found 20 headers in 'Modularity-Constrained Dynamic Representation Learning for Interpretable Brain Disorder Analysis with Functional MRI'\n",
      "Found 12 headers in 'Weakly-Supervised Positional Contrastive Learning: Application to Cirrhosis Classification'\n",
      "Found 16 headers in 'Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models'\n",
      "Found 15 headers in 'Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features'\n",
      "Found 17 headers in 'MDA-SR: Multi-level Domain Adaptation Super-Resolution for Wireless Capsule Endoscopy Images'\n",
      "Found 19 headers in 'S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation'\n",
      "Found 18 headers in 'Graph Convolutional Network with Morphometric Similarity Networks for Schizophrenia Classification'\n",
      "Found 23 headers in 'M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization'\n",
      "Found 17 headers in 'Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy'\n",
      "Found 21 headers in 'Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training'\n",
      "Found 14 headers in 'Deblurring Masked Autoencoder Is Better Recipe for Ultrasound Image Recognition'\n",
      "Found 9 headers in 'Black-box Domain Adaptative Cell Segmentation via Multi-source Distillation'\n",
      "Found 15 headers in 'Additional Positive Enables Better Representation Learning for Medical Images'\n",
      "Found 19 headers in 'Self-Supervised Domain Adaptive Segmentation of Breast Cancer via Test-Time Fine-Tuning'\n",
      "Found 16 headers in 'Decoupled Consistency for Semi-supervised Medical Image Segmentation'\n",
      "Found 15 headers in 'PET-Diffusion: Unsupervised PET Enhancement Based on the Latent Diffusion Model'\n",
      "Found 11 headers in '3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images'\n",
      "Found 18 headers in 'Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy'\n",
      "Found 19 headers in 'Cross-Adversarial Local Distribution Regularization for Semi-supervised Medical Image Segmentation'\n",
      "Found 18 headers in 'Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography'\n",
      "Found 13 headers in 'Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training'\n",
      "Found 19 headers in 'SLPD: Slide-Level Prototypical Distillation for WSIs'\n",
      "Found 15 headers in 'Masked Frequency Consistency for Domain-Adaptive Semantic Segmentation of Laparoscopic Images'\n",
      "Found 14 headers in 'Can Point Cloud Networks Learn Statistical Shape Models of Anatomies?'\n",
      "Found 12 headers in 'PROnet: Point Refinement Using Shape-Guided Offset Map for Nuclei Instance Segmentation'\n",
      "Found 13 headers in 'Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks'\n",
      "Found 14 headers in 'Improved Multi-shot Diffusion-Weighted MRI with Zero-Shot Self-supervised Learning Reconstruction'\n",
      "Found 13 headers in 'Image2SSM: Reimagining Statistical Shape Models from Images with Radial Basis Functions'\n",
      "Found 16 headers in 'TPRO: Text-Prompting-Based Weakly Supervised Histopathology Tissue Segmentation'\n",
      "Found 16 headers in 'LSOR: Longitudinally-Consistent Self-Organized Representation Learning'\n",
      "Found 8 headers in 'VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis'\n",
      "Found 15 headers in 'A Small-Sample Method with EEG Signals Based on Abductive Learning for Motor Imagery Decoding'\n",
      "Found 13 headers in 'Weakly Supervised Lesion Localization of Nascent Geographic Atrophy in Age-Related Macular Degeneration'\n",
      "Found 24 headers in 'MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging'\n",
      "Found 18 headers in 'Multi-modal Semi-supervised Evidential Recycle Framework for Alzheimer’s Disease Classification'\n",
      "Found 16 headers in 'DAS-MIL: Distilling Across Scales for MIL Classification of Histological WSIs'\n",
      "Found 17 headers in 'An Auto-Encoder to Reconstruct Structure with Cryo-EM Images via Theoretically Guaranteed Isometric Latent Space, and Its Application for Automatically Computing the Conformational Pathway'\n",
      "Found 13 headers in 'Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation'\n",
      "Found 18 headers in 'Modeling Alzheimers’ Disease Progression from Multi-task and Self-supervised Learning Perspective with Brain Networks'\n",
      "Found 16 headers in 'vox2vec: A Framework for Self-supervised Contrastive Learning of Voxel-Level Representations in Medical Images'\n",
      "Found 21 headers in 'Multi-Target Domain Adaptation with Prompt Learning for Medical Image Segmentation'\n",
      "Found 22 headers in 'Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation'\n",
      "Found 13 headers in 'UOD: Universal One-Shot Detection of Anatomical Landmarks'\n",
      "Found 14 headers in 'Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher'\n",
      "Found 24 headers in 'MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation'\n",
      "Found 14 headers in 'PET Image Denoising with Score-Based Diffusion Probabilistic Models'\n",
      "Found 18 headers in 'CL-ADDA: Contrastive Learning with Amplitude-Driven Data Augmentation for fMRI-Based Individualized Predictions'\n",
      "Found 10 headers in 'Cross-Dataset Adaptation for Instrument Classification in Cataract Surgery Videos'\n",
      "Found 18 headers in 'Gall Bladder Cancer Detection from US Images with only Image Level Labels'\n",
      "Found 18 headers in 'Structured State Space Models for Multiple Instance Learning in Digital Pathology'\n",
      "Found 17 headers in 'Automatic Retrieval of Corresponding US Views in Longitudinal Examinations'\n",
      "Found 17 headers in 'Multi-scale Self-Supervised Learning for Longitudinal Lesion Tracking with Optional Supervision'\n",
      "Found 17 headers in 'Geometry-Invariant Abnormality Detection'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "def parse_xml_and_extract_headers(file_path):\n",
    "    tree = etree.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    # Extract the paper title by XPath in the XML's structure\n",
    "    paper_title_element = root.find('.//tei:title', ns)\n",
    "    paper_title = paper_title_element.text if paper_title_element is not None else \"No Title Found\"\n",
    "\n",
    "    headers = root.xpath('//tei:head', namespaces=ns)\n",
    "    print(f\"Found {len(headers)} headers in '{paper_title}'\")\n",
    "    \n",
    "    data = []\n",
    "    for header in headers:\n",
    "        # Use XPath string() function to get all text within the <p> tags, including nested elements\n",
    "        text_content = ''.join(header.getparent().xpath('.//tei:p//text()', namespaces=ns))\n",
    "        data.append({\n",
    "            'Paper Title': paper_title,\n",
    "            'Header Number': header.get('n'),\n",
    "            'Header Title': header.text,\n",
    "            'Text': text_content  # Updated to use text_content\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Paper Title', 'Header Number', 'Header Title', 'Text'])\n",
    "    return df\n",
    "\n",
    "def process_xml_folder(folder_path):\n",
    "    all_data_frames = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".xml\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = parse_xml_and_extract_headers(file_path)\n",
    "            all_data_frames.append(df)\n",
    "\n",
    "    if all_data_frames:\n",
    "        final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "    else:\n",
    "        final_df = pd.DataFrame()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Folder path - where XML files should be stored (gets updated for each volume path)\n",
    "folder_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/papers_xml/vol01'\n",
    "\n",
    "# Process the folder and create a DataFrame with all headers\n",
    "df_headers = process_xml_folder(folder_path)\n",
    "df_headers.to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/vol1_headers.csv\", index=False)\n",
    "#df_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if titles are missing from the volume in the current dataframe \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_headers['Paper Title'].unique())\n",
    "#df_headers['Paper Title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing dataframes for all 10 volumes within a dictionary\n",
    "***\n",
    "- Cleaning rows by removing non-relevant rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml_folder(folder_path):\n",
    "    df = pd.read_csv(folder_path)\n",
    "    return df\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Remove rows where both 'Header Title' and 'Text' are NaN or just 'Text' is NaN\n",
    "    df_cleaned = df.dropna(subset=['Header Title', 'Text'], how='all')\n",
    "    df_cleaned = df_cleaned.dropna(subset=['Text'], how='any')\n",
    "    return df_cleaned\n",
    "\n",
    "# Base path where all processed volumes are stored\n",
    "base_path = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/papers_xml/originals_dfs'\n",
    "\n",
    "# Dictionary to store cleaned DataFrames\n",
    "cleaned_dataframes = {}\n",
    "\n",
    "# Loop over the volume directories\n",
    "for i in range(1, 11):\n",
    "    vol_path = os.path.join(base_path, f'vol{str(i)}_headers.csv')\n",
    "    df_headers = process_xml_folder(vol_path)\n",
    "    df_cleaned = clean_dataframe(df_headers)\n",
    "    \n",
    "    # Store the cleaned DataFrame in the dictionary with the volume number as the key\n",
    "    cleaned_dataframes[f'vol{str(i)}'] = df_cleaned\n",
    "\n",
    "# Dictionary with all cleaned DataFrames\n",
    "# cleaned_dataframes['vol1'], cleaned_dataframes['vol2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_dataframes['vol4']['Paper Title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataframes['vol1'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol1_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol2'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol2_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol3'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol3_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol4'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol4_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol5'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol5_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol6'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol6_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol7'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol7_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol8'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol8_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol9'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol9_cleaned.csv\", index=False)\n",
    "cleaned_dataframes['vol10'].to_csv(\"/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/dfs/vol10_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking to see if papers per volume are missing after cleaning the dataframes\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_dataframes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Verify unique title counts in individual dataframes before combining\u001b[39;00m\n\u001b[1;32m      2\u001b[0m total_unique \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vol, df \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcleaned_dataframes\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      4\u001b[0m     unique_in_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPaper Title\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnique titles in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_in_df\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_dataframes' is not defined"
     ]
    }
   ],
   "source": [
    "# Verify unique title counts in individual dataframes before combining\n",
    "total_unique = 0\n",
    "for vol, df in cleaned_dataframes.items():\n",
    "    unique_in_df = len(df['Paper Title'].unique())\n",
    "    print(f\"Unique titles in {vol}: {unique_in_df}\")\n",
    "    total_unique += unique_in_df\n",
    "\n",
    "print(f\"Sum of unique titles from individual DataFrames: {total_unique}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total papers in 1: 73\n",
      "total papers in 2: 73\n",
      "total papers in 3: 72\n",
      "total papers in 4: 75\n",
      "total papers in 5: 76\n",
      "total papers in 6: 77\n",
      "total papers in 7: 75\n",
      "total papers in 8: 65\n",
      "total papers in 9: 70\n",
      "total papers in 10: 74\n"
     ]
    }
   ],
   "source": [
    "print('total papers in 1:', len(cleaned_dataframes['vol1']['Paper Title'].unique())) # 73\n",
    "print('total papers in 2:', len(cleaned_dataframes['vol2']['Paper Title'].unique())) # 73  missing one paper\n",
    "print('total papers in 3:', len(cleaned_dataframes['vol3']['Paper Title'].unique())) # 72 \n",
    "print('total papers in 4:', len(cleaned_dataframes['vol4']['Paper Title'].unique())) # 75 \n",
    "print('total papers in 5:', len(cleaned_dataframes['vol5']['Paper Title'].unique())) # 76 \n",
    "print('total papers in 6:', len(cleaned_dataframes['vol6']['Paper Title'].unique())) # 77 \n",
    "print('total papers in 7:', len(cleaned_dataframes['vol7']['Paper Title'].unique())) # 75 \n",
    "print('total papers in 8:', len(cleaned_dataframes['vol8']['Paper Title'].unique())) # 65 \n",
    "print('total papers in 9:', len(cleaned_dataframes['vol9']['Paper Title'].unique())) # 70\n",
    "print('total papers in 10:', len(cleaned_dataframes['vol10']['Paper Title'].unique())) # 74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Merge all volumes into 1 dataframe\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique titles after enhancement: 730\n"
     ]
    }
   ],
   "source": [
    "for vol, df in cleaned_dataframes.items():\n",
    "    volume_number = int(re.search(r'\\d+', vol).group())\n",
    "    # Append volume information to each title to ensure uniqueness across volumes\n",
    "    df['Paper Title'] = df['Paper Title'].astype(str) + ' (vol' + str(volume_number) + ')'\n",
    "    df['Volume'] = volume_number\n",
    "\n",
    "combined_df = pd.concat(cleaned_dataframes.values(), ignore_index=True)\n",
    "\n",
    "# Check unique titles after appending volume information\n",
    "unique_titles_count = len(combined_df['Paper Title'].unique())\n",
    "print(f\"Total unique titles after enhancement: {unique_titles_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#combined_df.to_csv('combined_df.csv')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Lowercase all text in the 'Text' column\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mText\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m      6\u001b[0m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m combined_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(wrap_text, width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Regular expression with str.replace to remove the volume information\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/GitHub/machine-learning-bsc-thesis-2024/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Text'"
     ]
    }
   ],
   "source": [
    "combined_df = combined_df.fillna(0)\n",
    "#combined_df.to_csv('combined_df.csv')\n",
    "\n",
    "# Lowercase all text in the 'Text' column\n",
    "combined_df['Text'] = combined_df['Text'].str.lower()\n",
    "combined_df['Text'] = combined_df['Text'].apply(wrap_text, width = 80)\n",
    "\n",
    "# Regular expression with str.replace to remove the volume information\n",
    "combined_df['Paper Title'] = combined_df['Paper Title'].str.replace(r'\\s*\\(vol\\d+\\)', '', regex=True)\n",
    "\n",
    "combined_df.rename(columns={'Paper Title': 'title', 'Header Number':'header_no', \n",
    "                                           'Header Title': 'header_title', 'Text':'text', 'Volume': 'volume'}, inplace=True)\n",
    "\n",
    "#combined_df.to_csv('refined_all_papers_extracted_w_text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "730\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>header_no</th>\n",
       "      <th>header_title</th>\n",
       "      <th>text</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>to reduce radiologists' reading burden and mak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Method</td>\n",
       "      <td>notation. we first formally define the problem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Stage 1-Proxy Task to Detect Synthetic Anomalies</td>\n",
       "      <td>amae starts the first training stage using onl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Stage 2-MAE Inter-Discrepancy Adaptation</td>\n",
       "      <td>the proposed mae adaptation scheme is inspired...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMAE: Adaptation of Pre-trained Masked Autoenc...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>datasets. we evaluated our method on three pub...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6963</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Using Illumination Decline as a Depth Cue</td>\n",
       "      <td>the neus formulation of sect. 2 assumes distan...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6964</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Endoscope Photometric Model</td>\n",
       "      <td>apart from illumination decline, there are sev...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6965</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>we validate our method on the c3vd dataset [4]...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6966</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Conclusion</td>\n",
       "      <td>we have presented a method for 3d dense multi-...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6967</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>0</td>\n",
       "      <td>Supplementary Information</td>\n",
       "      <td>the online version contains supplementary mate...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6968 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title header_no  \\\n",
       "0     AMAE: Adaptation of Pre-trained Masked Autoenc...       1.0   \n",
       "1     AMAE: Adaptation of Pre-trained Masked Autoenc...       2.0   \n",
       "2     AMAE: Adaptation of Pre-trained Masked Autoenc...       2.1   \n",
       "3     AMAE: Adaptation of Pre-trained Masked Autoenc...       2.2   \n",
       "4     AMAE: Adaptation of Pre-trained Masked Autoenc...       3.0   \n",
       "...                                                 ...       ...   \n",
       "6963  LightNeuS: Neural Surface Reconstruction in En...       3.1   \n",
       "6964  LightNeuS: Neural Surface Reconstruction in En...       3.2   \n",
       "6965  LightNeuS: Neural Surface Reconstruction in En...       4.0   \n",
       "6966  LightNeuS: Neural Surface Reconstruction in En...       5.0   \n",
       "6967  LightNeuS: Neural Surface Reconstruction in En...         0   \n",
       "\n",
       "                                          header_title  \\\n",
       "0                                         Introduction   \n",
       "1                                               Method   \n",
       "2     Stage 1-Proxy Task to Detect Synthetic Anomalies   \n",
       "3             Stage 2-MAE Inter-Discrepancy Adaptation   \n",
       "4                                          Experiments   \n",
       "...                                                ...   \n",
       "6963         Using Illumination Decline as a Depth Cue   \n",
       "6964                       Endoscope Photometric Model   \n",
       "6965                                       Experiments   \n",
       "6966                                        Conclusion   \n",
       "6967                         Supplementary Information   \n",
       "\n",
       "                                                   text  volume  \n",
       "0     to reduce radiologists' reading burden and mak...       1  \n",
       "1     notation. we first formally define the problem...       1  \n",
       "2     amae starts the first training stage using onl...       1  \n",
       "3     the proposed mae adaptation scheme is inspired...       1  \n",
       "4     datasets. we evaluated our method on three pub...       1  \n",
       "...                                                 ...     ...  \n",
       "6963  the neus formulation of sect. 2 assumes distan...      10  \n",
       "6964  apart from illumination decline, there are sev...      10  \n",
       "6965  we validate our method on the c3vd dataset [4]...      10  \n",
       "6966  we have presented a method for 3d dense multi-...      10  \n",
       "6967  the online version contains supplementary mate...      10  \n",
       "\n",
       "[6968 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '/Users/yasminsarkhosh/Documents/GitHub/machine-learning-bsc-thesis-2024/code/outputs/databases/refined_all_papers_extracted_w_text.csv'\n",
    "combined_df = pd.read_csv(filename, index_col=0)\n",
    "print(len(combined_df['title'].unique()))\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select papers related to cancer\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>header_no</th>\n",
       "      <th>header_title</th>\n",
       "      <th>text</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>chest radiographs (chest x-rays) represent the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Related Work</td>\n",
       "      <td>weakly supervised pathology detection. due to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Model</td>\n",
       "      <td>figure 1 provides an overview of our method. g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Inference</td>\n",
       "      <td>during inference, the trained model predicts a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anatomy-Driven Pathology Detection on Chest X-...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>Training</td>\n",
       "      <td>the anatomical region detector is trained usin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Using Illumination Decline as a Depth Cue</td>\n",
       "      <td>the neus formulation of sect. 2 assumes distan...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Endoscope Photometric Model</td>\n",
       "      <td>apart from illumination decline, there are sev...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Experiments</td>\n",
       "      <td>we validate our method on the c3vd dataset [4]...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Conclusion</td>\n",
       "      <td>we have presented a method for 3d dense multi-...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>LightNeuS: Neural Surface Reconstruction in En...</td>\n",
       "      <td>0</td>\n",
       "      <td>Supplementary Information</td>\n",
       "      <td>the online version contains supplementary mate...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title header_no  \\\n",
       "0     Anatomy-Driven Pathology Detection on Chest X-...       1.0   \n",
       "1     Anatomy-Driven Pathology Detection on Chest X-...       2.0   \n",
       "2     Anatomy-Driven Pathology Detection on Chest X-...       3.1   \n",
       "3     Anatomy-Driven Pathology Detection on Chest X-...       3.2   \n",
       "4     Anatomy-Driven Pathology Detection on Chest X-...       3.3   \n",
       "...                                                 ...       ...   \n",
       "2512  LightNeuS: Neural Surface Reconstruction in En...       3.1   \n",
       "2513  LightNeuS: Neural Surface Reconstruction in En...       3.2   \n",
       "2514  LightNeuS: Neural Surface Reconstruction in En...       4.0   \n",
       "2515  LightNeuS: Neural Surface Reconstruction in En...       5.0   \n",
       "2516  LightNeuS: Neural Surface Reconstruction in En...         0   \n",
       "\n",
       "                                   header_title  \\\n",
       "0                                  Introduction   \n",
       "1                                  Related Work   \n",
       "2                                         Model   \n",
       "3                                     Inference   \n",
       "4                                      Training   \n",
       "...                                         ...   \n",
       "2512  Using Illumination Decline as a Depth Cue   \n",
       "2513                Endoscope Photometric Model   \n",
       "2514                                Experiments   \n",
       "2515                                 Conclusion   \n",
       "2516                  Supplementary Information   \n",
       "\n",
       "                                                   text  volume  \n",
       "0     chest radiographs (chest x-rays) represent the...       1  \n",
       "1     weakly supervised pathology detection. due to ...       1  \n",
       "2     figure 1 provides an overview of our method. g...       1  \n",
       "3     during inference, the trained model predicts a...       1  \n",
       "4     the anatomical region detector is trained usin...       1  \n",
       "...                                                 ...     ...  \n",
       "2512  the neus formulation of sect. 2 assumes distan...      10  \n",
       "2513  apart from illumination decline, there are sev...      10  \n",
       "2514  we validate our method on the c3vd dataset [4]...      10  \n",
       "2515  we have presented a method for 3d dense multi-...      10  \n",
       "2516  the online version contains supplementary mate...      10  \n",
       "\n",
       "[2517 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for 'cancer' in the Text column, case insensitive\n",
    "cancer_papers_mask = combined_df['text'].str.contains('cancer|tumor|tumour', case=False, na=False)\n",
    "papers_with_cancer = combined_df[cancer_papers_mask]\n",
    "\n",
    "# Get the unique titles of papers that mention 'cancer'\n",
    "unique_titles_with_cancer = papers_with_cancer['title'].unique()\n",
    "\n",
    "# Extract all headers and their related text for papers that mention 'cancer'\n",
    "extracted_info = pd.DataFrame()\n",
    "for title in unique_titles_with_cancer:\n",
    "    paper_info = combined_df[combined_df['title'] == title]\n",
    "    extracted_info = pd.concat([extracted_info, paper_info])\n",
    "\n",
    "# Reset index of the resulting DataFrame\n",
    "extracted_info.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "unique_paper_titles_with_cancer = extracted_info['title'].unique()\n",
    "print(len(unique_paper_titles_with_cancer))\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "extracted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_info.to_csv(\"cancer_related_papers_w_text.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
