chest radiographs (chest x-rays) represent the most widely utilized type of
medical imaging examination globally and hold immense significance in the
detection of prevalent thoracic diseases, including pneumonia and lung cancer,
making them a crucial tool in clinical care [10,15]. pathology detection and
localization -for brevity we will use the term pathology detection throughout
this workenables the automatic interpretation of medical scans such as chest
x-rays by predicting bounding boxes for detected pathologies. unlike
classification, which only predicts the presence of pathologies, it provides a
high level of explainability supporting radiologists in making informed
decisions. however, while image classification labels can be automatically
extracted from electronic health records or radiology reports [7,20], this is
typically not possible for bounding boxes, thus limiting the availability of
large datasets for pathology detection. additionally, manually annotating
pathology bounding boxes is a time-consuming task, further exacerbating the
issue. the resulting scarcity of large, publicly available datasets with
pathology bounding boxes limits the use of supervised methods for pathology
detection, such that current approaches typically follow weakly supervised
object detection approaches, where only classification labels are required for
training. however, as these methods are not guided by any form of bounding
boxes, their performance is limited.we, therefore, propose a novel approach
towards pathology detection that uses anatomical region bounding boxes, solely
defined on anatomical structures, as proxies for pathology bounding boxes. these
region boxes are easier to annotate -the physiological shape of a healthy
subject's thorax can be learned relatively easily by medical students -and
generalize better than those of pathologies, such that huge labeled datasets are
available [21]. in summary:-we propose anatomy-driven pathology detection
(adpd), a pathology detection approach for chest x-rays, trained with pathology
classification labels together with anatomical region bounding boxes as proxies
for pathologies. -we study two training approaches: using localized
(anatomy-level) pathology labels for our model loc-adpd and using image-level
labels with multiple instance learning (mil) for our model mil-adpd. -we train
our models on the chest imagenome [21] dataset and evaluate on nih chestx-ray 8
[20], where we found that our loc-adpd model outperforms both, weakly supervised
methods and fully supervised detection with a small training set, while our
mil-adpd model is competitive with supervised detection and slightly outperforms
weakly supervised approaches.
weakly supervised pathology detection. due to the scarcity of bounding box
annotations, pathology detection on chest x-rays is often tackled using weakly
supervised object detection with class activation mapping (cam) [25], which only
requires image-level classification labels. after training a classification
model with global average pooling (gap), an activation heatmap is computed by
classifying each individual patch (extracted before pooling) with the trained
classifier, before thresholding this heatmap for predicting bounding boxes.
inspired by this approach, several methods have been developed for chest x-rays
[6,14,20,23]. while chexnet [14] follows the original approach, the method
provided with the nih chestx-ray 8 dataset [20] and the stl method [6] use
logsumexp (lse) pooling [13], while the multimap model [23] uses max-min pooling
as first proposed for the weldon [3] method. unlike our method, none of these
methods utilize anatomical regions as proxies for predicting pathology bounding
boxes, therefore leading to inferior performance. localized pathology
classification. anatomy-level pathology labels have been utilized before to
train localized pathology classifiers [1,21] or to improve weakly supervised
pathology detection [24]. along with the chest imagenome dataset [21] several
localized pathology classification models have been proposed which use a faster
r-cnn [16] to extract anatomical region features before predicting observed
pathologies for each region using either a linear model or a gcn model based on
pathology co-occurrences. this approach has been further extended to use gcns on
anatomical region relationships [1]. while utilizing the same form of
supervision as our method, these methods do not tackle pathology detection.in
agxnet [24], anatomy-level pathology classification labels are used to train a
weakly-supervised pathology detection model. unlike our and the other described
methods, it does however not use anatomical region bounding boxes.
figure 1 provides an overview of our method. given a chest x-ray, we apply a
densenet121 [5] backbone and extract patch-wise features by using the feature
map after the last convolutional layer (before gap). we then apply a lightweight
object detection model consisting of a single detr [2] decoder layer to detect
anatomical regions. following [2], we use learned query tokens attending to
patch features in the decoder layer, where each token corresponds to one
predicted bounding box. as no anatomical region can occur more than once in each
chest x-ray, each query token is assigned to exactly one pre-defined anatomical
region, such that the number of tokens equals the number of anatomical regions.
this one-to-one assignment of tokens and regions allows us to remove the
hungarian matching used in [2]. as described next, the resulting per-region
features from the output of the decoder layer will be used for predictions on
each region.for predicting whether the associated region is present, we use a
binary classifier with a single linear layer, for bounding box prediction we use
a three-layer mlp followed by sigmoid. we consider the prediction of observed
pathologies as a multi-label binary classification task and use a single linear
layer (followed by sigmoid) to predict the probabilities of all pathologies.
each of these predictors is applied independently to each region with their
weights shared across regions.we experimented with more complex pathology
predictors like an mlp or a transformer layer but did not observe any benefits.
we also did not observe improvements when using several decoder layers and
observed degrading performance when using roi pooling to compute region
features.
during inference, the trained model predicts anatomical region bounding boxes
and per-region pathology probabilities, which are then used to predict pathology
bounding boxes in two steps, as shown in fig. 2. in step (i), pathology
probabilities are first thresholded and for each positive pathology (with
probability larger than the threshold) the bounding box of the corresponding
anatomical region is predicted as its pathology box, using the pathology
probability as box score. this means, if a region contains several predicted
pathologies, then all of its predicted pathologies share the same bounding box
during step (i). in step (ii), weighted box fusion (wbf) [19] merges bounding
boxes of the same pathology with iou-overlaps above 0.03 and computes weighted
averages (using box scores as weights) of their box coordinates. as many
anatomical regions are at least partially overlapping, and we use a small
iou-overlap threshold, this allows the model to either pull the predicted boxes
to relevant subparts of an anatomical region or to predict that pathologies
stretch over several regions.
the anatomical region detector is trained using the detr loss [2] with fixed
oneto-one matching (i.e. without hungarian matching). for training the pathology
classifier, we experiment with two different levels of supervision (fig. 3). for
our loc-adpd model, we utilize anatomy-level pathology classification labels.
here, the target set of observed pathologies is available for each anatomical
region individually such that the pathology observation prediction can directly
be trained for each anatomical region. we apply the asl [17] loss function
independently on each region-pathology pair and average the results over all
regions and pathologies. the decoder feature dimension is set to 512.for our
mil-adpd model, we experiment with a weaker form of supervision, where pathology
classification labels are only available on the per-image level. we utilize
multiple instance learning (mil), where an image is considered a bag of
individual instances (i.e. the anatomical regions), and only a single label (per
pathology) is provided for the whole bag, which is positive if any of its
instances is positive. to train using mil, we first aggregate the predicted
pathology probabilities of each region over all detected regions in the image
using lse pooling [13], acting as a smooth approximation of max pooling. the
resulting per-image probability for each pathology is then trained using the asl
[17] loss. in this model, the decoder feature dimension is set to 256.in both
models, the asl loss is weighted by a factor of 0.01 before adding it to the
detr loss. we train using adamw [12] with a learning rate of 3e-5 (loc-adpd) or
1e-4 (mil-adpd) and weight decay 1e-5 (loc-adpd) or 1e-4 (mil-adpd) in batches
of 128 samples with early stopping (with 20 000 steps patience) for roughly 7 h
on a single nvidia rtx a6000.
training dataset. we train on the chest imagenome dataset [4,21,22]1 ,
consisting of roughly 240 000 frontal chest x-ray images with corresponding
scene graphs automatically constructed from free-text radiology reports. it is
derived from the mimic-cxr dataset [9,10], which is based on imaging studies
from 65 079 patients performed at beth israel deaconess medical center in
boston, us. amongst other information, each scene graph contains bounding boxes
for 29 unique anatomical regions with annotated attributes, where we consider
positive anatomical finding and disease attributes as positive labels for
pathologies, leading to binary anatomy-level annotations for 55 unique
pathologies. we consider the image-level label for a pathology to be positive if
any region is positively labeled with that pathology.we use the provided
jpg-images [11] 2 and follow the official mimic-cxr training split but only keep
samples containing a scene graph with at least five valid region bounding boxes,
resulting in a total of 234 307 training samples.during training, we use random
resized cropping with size 224 × 224, apply contrast and brightness jittering,
random affine augmentations, and gaussian blurring.evaluation dataset and class
mapping. we evaluate our method on the subset of 882 chest x-ray images with
pathology bounding boxes, annotated by radiologists, from the nih chestxray-8
(cxr8) dataset [20] 3 from the national institutes of health clinical center in
the us. we use 50% for validation and keep the other 50% as a held-out test set.
note that for evaluation only pathology bounding boxes are required (to compute
the metrics), while during training only anatomical region bounding boxes
(without considering pathologies) are required. all images are center-cropped
and resized to 224 × 224.the dataset contains bounding boxes for 8 unique
pathologies. while partly overlapping with the training classes, a one-to-one
correspondence is not possible for all classes. for some evaluation classes, we
therefore use a many-to-one mapping where the class probability is computed as
the mean over several training classes. we refer to the supp. material for a
detailed study on class mappings.
we compare our method against several weakly supervised object detection methods
(chexnet [14], stl [6], gradcam [18], cxr [20], weldon [3], mul-timap model
[23], lse model [13]), trained on the cxr8 training set using only image-level
pathology labels. note that some of these methods focus on (imagelevel)
classification and do not report quantitative localization results.
nevertheless, we compare their localization approaches quantitatively with our
method. we also use agxnet [24] for comparison, a weakly supervised method
trained using anatomy-level pathology labels but without any bounding box
supervision. it was trained on mimic-cxr (sharing the images with our method)
with labels from radgraph [8] and finetuned on the cxr8 training set with
imagelevel labels. additionally, we also compare with a faster-rcnn [16] trained
on a small subset of roughly 500 samples from the cxr8 training set that have
been 2 https://physionet.org/content/mimic-cxr-jpg/2.0.0/ (physionet
credentialed health data license 1.5.0). annotated with pathology bounding boxes
by two medical experts, including one board-certified radiologist.table 1.
results on the nih chestx-ray 8 dataset [20]. our models loc-adpd and mil-adpd,
trained using anatomy (an) bounding boxes, both outperform all weakly supervised
methods trained with image-level pathology (pa) and anatomy-level pathology
(an-pa) labels by a large margin. mil-adpd is competitive with the supervised
baseline trained with pathology (pa) bounding boxes, while loc-adpd outperforms
it by a large margin.
supervision iou@10-70 iou@10 iou@30 iou@50 box class map ap loc-acc ap loc-acc
ap loc-acc mil-adpd (ours) an pa for all models, we only consider the predicted
boxes with the highest box score per pathology, as the cxr8 dataset never
contains more than one box per pathology. we report the standard object
detection metrics average precision (ap) at different iou-thresholds and the
mean ap (map) over thresholds (0.1, 0.2, . . . , 0.7), commonly used thresholds
on this dataset [20]. additionally, we report the localization accuracy
(loc-acc) [20], a common localization metric on this dataset, where we use a box
score threshold of 0.7 for our method.
comparison with baselines. table 1 shows the results of our mil-adpd and
loc-adpd models and all baselines on the cxr8 test set. compared to the best
weakly supervised method with image-level supervision (chexnet) our methods
improve by large margins (mil-adpd by δ+35.2%, loc-adpd by δ+87.8% in map).
improvements are especially high when considering larger iou-thresholds and huge
improvements are also achieved in loc-acc at all thresholds. both models also
outperform agxnet (which uses anatomy-level supervision) by large margins
(mil-adpd by δ + 47.9% and loc-adpd by δ + 105.5% map), while improvements on
larger thresholds are smaller here. even when compared to faster r-cnn trained
on a small set of fully supervised samples, mil-adpd is competitive (δ + 6.5%),
while loc-adpd improves by δ + 48.0%. however, on larger thresholds (iou@50) the
supervised baseline slightly outperforms mil-adpd, while loc-adpd is still
superior. this shows that using anatomical regions as proxies is an effective
approach to tackle pathology detection. while using image-level annotations
(mil-adpd) already gives promising results, the full potential is only achieved
using anatomy-level supervision (loc-adpd). unlike loc-adpd and mil-adpd, all
baselines were either trained or finetuned on the cxr8 dataset, showing that our
method generalizes well to unseen datasets and that our class mapping is
effective.for detailed results per pathology we refer to the supp. material. we
found that the improvements of mil-adpd are mainly due to improved performance
on cardiomegaly and mass detection, while loc-adpd consistently outperforms all
baselines on all classes except nodule, often by a large margin.ablation study.
in table 1 we also show the results of different ablation studies. without wbf,
results degrade for both of our models, highlighting the importance of merging
region boxes. combining the training strategies of loc-adpd and mil-adpd does
not lead to an improved performance. different class mappings between training
and evaluation set are studied in the supp. material.qualitative results. as
shown in fig. 4 loc-adpd detects cardiomegaly almost perfectly, as it is always
exactly localized at one anatomical region. other pathologies are detected but
often with too large or too small boxes as they only cover parts of anatomical
regions or stretch over several of them, which cannot be completely corrected
using wbf. detection also works well for predicting several overlapping
pathologies. for qualitative comparisons between loc-adpd and mil-adpd, we refer
to the supp. material.
limitations. while our proposed adpd method outperforms all competing models, it
is still subject to limitations. first, due to the dependence on region proxies,
for pathologies covering only a small part of a region, our models predict the
whole region, as highlighted by their incapability to detect nodules. we however
note that in clinical practice, chest x-rays are not used for the final
diagnosis of such pathologies and even rough localization can be beneficial.
additionally, while not requiring pathology bounding boxes, our models still
require supervision in the form of anatomical region bounding boxes, and
loc-adpd requires anatomy-level labels. however, anatomical bounding boxes are
easier to annotate and predict than pathology bounding boxes, and the used
anatomylevel labels were extracted automatically from radiology reports [21].
while our work is currently limited to chest x-rays, we see huge potential for
modalities where abnormalities can be assigned to meaningful regions.
we proposed a novel approach tackling pathology detection on chest x-rays using
anatomical region bounding boxes. we studied two training approaches, using
anatomy-level pathology labels and using image-level labels with mil. our
experiments demonstrate that using anatomical regions as proxies improves
results compared weakly supervised methods and supervised training on little
data, thus providing a promising direction for future research.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_6.
positron emission tomography (pet) is a 3d imaging modality using
radiopharmaceuticals, such as f-18-fluorodeoxyglucose (fdg), as tracers. newly
introduced long axial field-of-view pet scanners have enabled dynamic pet (dpet)
with frame duration < 1 min [18], allowing the observation of dynamic metabolic
processes throughout the body. for a given voxel in space, the radioactivity
concentration over time can be described by a characteristic curve known as time
activity curve (tac), measured in [bq/ml]. tacs can be described by mathematical
functions, called physiologically-based pharmacokinetic (pbpk) models or kinetic
models (km) [14]. the parameters of the km represent physiologically relevant
quantities and are often called micro-parameters, whereas their combinations are
called macro-parameters [14,20]. while the former can be retrieved only by
methods that directly use the km function, the latter can be computed by
simplified linearized methods (such as the logan and the patlak-gjedde plots).
the approaches to extract km parameters can be split in two categories: volume
of interest (voi) methods, in which the average tac in a voi is used, or
voxel-based methods. despite the former displaying less noise and, therefore,
lower variance in the kinetic parameters (kps), voi-based methods only provide
organ-wise information. on the other hand, voxel-based methods allow the
generation of parametric images (kpis), in which the kps are visualized at a
voxel level, but suffer from motion and breathing artifacts, and require more
computational power or simplified linearized methods.parametric images are
reported to be superior in lesion detection and delineation when compared to
standard-of-care activity-and weight-normalized static pet volumes, known as
standard uptake value (suv) volumes [6,7]. changes in the kps during oncological
therapy are associated with pathological response to treatment, whereas this is
not true for changes in suv [15]. despite the advantages of kpis in diagnosis,
the generation of accurate micro-parametric images is not yet possible in
clinical practice.to address the problem of the generation of micro-parametric
images, we propose a custom 3d unet [3] to estimate kinetic micro-parameters in
an unsupervised setting drawing inspiration from physics-informed neural
networks (pinn). the main contributions of this work are:-a self-supervised
formulation of the problem of kinetic micro-parameters estimation -a
spatio-temporal deep neural network for parametric images estimation -a
quantitative and qualitative comparison with conventional methods for pbpk
modelingthe code is available at:
https://github.com/francescadb/self_supervised_pbpk_modelling.
finding the parameters of a km is a classical optimization problem [2,19,21]
solved by fitting the km equation to a measured tac in a least squares sense
[1,14,17]. the non-linearity of the km functions makes this approach prone to
overfitting and local minima, and sensitive to noise [14]. therefore, non-linear
parametric imaging is still too noisy for clinical application [20].to limit the
drawbacks of the non-linear parameter fitting, the identification of kps is
commonly performed using simplified linearized versions of the km [6,20], such
as the logan and the patlak-gjedde plots [5,16], which are often included in
clinical software for km such as pmod 1 .preliminary works towards km parameter
estimation in dpet imaging have recently begun to be explored. moradi et al.
used an auto-encoder along with a gaussian process regression block to select
the best km to describe simulated kinetic data [13]. a similar approach was
presented for the quantification of myocardial blood flow from simulated pet
sinograms [11]. huang et al. used a supervised 3d u-net to predict a
macro-parametric image using an suv image as input, and a patlak image derived
from dpet acquisition as ground truth [9]. cui et al. proposed a conditional
deep image prior framework to predict a macro-parametric image using a dnn in an
unsupervised setting [4]. finally, a supervised dnn was used to predict patlak
kpis from dynamic pet sinograms [12]. until now, methods used simulated data
[11,13] or static pet [9], were supervised [9,[11][12][13] or predicted
macro-parameters [4,9,12].
we propose to compute the kinetic micro-parameters in a self-supervised setting
by directly including the km function in the loss function and comparing the
predicted tac to the measured tac. for this reason, an understanding of the km
is fundamental to describing our pipeline.
the concentration of the tracer c(t) [bq/ml] in each tissue can be described as
a set of ordinary differential equations [20]. it represents the interaction of
two compartments, f (t) (free) and b(t) (bound), and takes as input the
radioactivity concentration in blood or plasma a(t) [14]:where [6,20]. equation
1 describes a general two-tissue compartment (2tc) kinetic model. however, an
fdg-tac is conventionally described by an irreversible 2tc, in which k 4 is set
to 0 [20]. therefore, in the following, we will use k 4 = 0. moreover, including
the blood fraction volume v b [•] allows to correctly model the contribution to
the radioactivity in a voxel coming from vessels that are too small to be
resolved by the pet scanner [14]. together, the tac of each voxel in an fdg dpet
acquisition can be modeled as, and solved using the laplace transform [20]:(2)
our network takes as input a sequence of 2d axial slices and returns a 4-channel
output representing the spatial distribution of the km parameters of a 2tc for
fdg metabolisation [14]. the network has a depth of four, with long [3] and
short skip connections [10]. the kernel size of the max-pooling is [2, 2, 2].
after the last decoder block, two 3d convolutional layers (with kernel size
[3,3,3] and [64, 1, 1]) estimate the kps per voxel given the output feature of
the network. inside the network the activation function is elu and critically
batch normalization is omitted. the network was trained with an initial learning
rate of 10 -4 , which was divided by half every 25 epochs, for a maximum of 500
epochs. following the approach taken by küstner et al. for motion correction of
4d spatio-temporal cine mri [10], we replaced a conventional 3d convolutional
layer with (2+1)d spatial and temporal convolutional layers. the spatial
convolutional layer is a 3d convolutional layer with kernel size [1,3,3] in [t,
x, y]. similarly, the temporal convolutional layer has a kernel size of
[3,1,1].we imposed that the kps predicted by the network satisfy eq. 2 by
including it in the computation of the loss. at a pixel level, we computed the
mean squared error between the tac estimated using the corresponding predicted
parameters ( tac i ) and the measured one (tac i ), as seen in fig. 1.we
introduced a final activation function to limit the output of the network to the
valid parameter domain of the km function. using the multi-clamp function, each
channel of the logits is restricted to the following parameter spaces:the limits
of the ranges were defined based on the meaning of the parameter (as in v b ),
mathematical requirements (as in the minimum values of k 2 and k 3 , whose sum
can not be zero) [6] or previous knowledge on the dataset derived by the work of
sari et al. [16] (as in the maximum values of k 1 , k 2 and k 3 ).we evaluated
the performance of the network using the mean absolute error (mae) and the
cosine similarity (cs) between tac i and tac i .
for comparison, parameter optimization via non-linear fitting was implemented in
python using the scipy.optimize.curve_fit function (version 1.10), with step
equal to 0.001. the bounds were the same as in the dnn.
the dataset is composed of 23 oncological patients with different tumor types.
dpet data was acquired on a biograph vision quadra for 65 min, over 62 frames.
the exposure duration of the frames were 2 × 10 s, 30 × 2 s, 4 × 10 s, 8 × 30 s,
4 × 60 s, 5 × 120 s and 9 × 300 s. the pet volumes were reconstructed with an
isotropic voxel size of 1.6 mm. the dataset included the label maps of 7 organs
(bones, lungs, heart, liver, kidneys, spleen, aorta) and one image-derived input
function a(t) [bq/ml] from the descending aorta per patient. further details on
the dataset are presented elsewhere [16].the pet frames and the label map were
resampled to an isotropic voxel size of 2.5 mm. then, the dataset was split
patient-wise into training, validation, and test set, with 10, 4, and 9 patients
respectively. details on the dataset split are available in the supplementary
material (table 1). the training set consisted of 750 slices and the validation
consisted of 300. in both cases, 75 axial slices per patient were extracted in a
pre-defined patient-specific range from the lungs to the bladder (included) and
were cropped to size 112 × 112 pixels.
table 1 shows the results of the 8 ablation studies we performed to find the
best model. we evaluated the impact of the design of the convolutional and
maxpooling kernels, as well as the choice of the final activation function. the
design of the max pooling kernel (i.e., kernel size equal to [2,2,2] or [1,2,2])
had no measurable effects in terms of cs in most of the experiments, with the
exception of exp. 3.2, where max-pooling only in space resulted in a drop of
0.06. when evaluating the mae, the use of 3d max-pooling was generally better.
[16] in green. the exact values are reported in the supplementary material
(table 3 and4) and in [16].
the most important design choice is the selection of the final activation
function. indeed, the multi-clamp final activation function was proven to be the
best both in terms of cs (exp 4.1: cs = 0.78 ± 0.05) and mae (exp 4.2: mae =
3.27 ± 2.01). compared to the other final activation functions, when the
multi-clamp is used the impact of the max-pooling design is negligible also in
terms of mae. for the rest of the experiments, the selected configuration is the
one from exp. 4.1 (see table 1).figure 2 shows the kps for four selected organs
as computed with the proposed dnn (kp dnn ), as computed with curve fit using
only the 9 patients of the test set (kp cf ) and using all 23 patients (kp ref
cf ) [16]. the voxel-wise kps predicted by the dnn were averaged over the
available organ masks.in terms of run-time, the dnn needed ≈ 1 min to predict
the kps of the a whole-body scan (≈ 400 slices), whereas curve fit took 8.7 min
for a single slice: the time reduction of the dnn is expected to be ≈ 3.500
times.
even though the choice of the final activation function has a greater impact,
the selection of the kernel design is important. using spatial and temporal
convo- lution results in an increase in the performance (+0.01 in cs) and
reduces the number of trainable parameters (from 2.1 m to 8.6 k), as pointed out
by [10]. therefore, the convergence is reached faster. moreover, the use of two
separate kernels in time and space is especially meaningful. pixel counts for a
given exposure are affected by the neighboring count measurements due to the
limited resolution of the pet scanner [20]. the temporally previous or following
counts are independent. in general, there is good agreement between kp dnn , kp
cf and kp ref cf . the dnn prediction of k 1 and k 2 in the spleen and k 3 in
the lungs is outside the confidence interval of the results published by sari et
al. [16].an analysis per slice of the metrics shows that the cs between tac i
and tac i changes substantially depending on the region: cs max = 0.87 within
the liver boundaries and cs min = 0.71 in the region corresponding to the heart
and lungs (see fig. 3a). this can be explained by the fact that v b is
underestimated for the heart and aorta. the proposed network predicts v heart b
= 0.376 ± 0.133 and v aorta b = 0.622 ± 0.238 while values of nearly 1 are to be
expected. this is likely due to breathing and heartbeat motion artifacts, which
cannot be modeled properly with a 2tc km that assumes no motion between
frames.figure 3b-e shows the central coronal slice of the four kpis in an
exemplary patient. as expected, k 1 is high in the heart, liver, and kidney.
similarly, the blood fraction volume v b is higher in the heart, blood vessels,
and lungs.the kp dnn are more homogeneous than kp cf , as can be seen in the
exemplary k 1 axial slice shown in fig. 4. a quantitative evaluation of the
smoothness of the images is reported in the supplementary material (fig. 1).
moreover, the distribution in the liver is more realistic in kp dnn , where the
gallbladder can be seen as an ellipsoid between the right and left liver lobes.
high k 1 regions are mainly within the liver, spleen, and kidney for kp dnn ,
while they also appear in unexpected areas in the kp cf (e.g., next to the spine
or in the region of the stomach).the major limitation of this work is the lack
of ground truth and a canonical method to evaluate quantitatively its
performance. this limitation is inherent to pbpk modeling and results in the
need for qualitative analyses based on expected physiological processes. a
possible way to leverage this would be to work on simulated data, yet the
validity of such evaluations strongly depends on how realistic the underlying
simulation models are. as seen in fig. 3a, motion (gross, respiratory, or
cardiac) has a major impact on the estimation quality. registering different
dpet frames has been shown to improve conventional pbpk models [8] and would
possibly have a positive impact on our approach.
in this work, inspired by pinns, we combine a self-supervised spatio-temporal
dnn with a new loss formulation considering physiology to perform kinetic
modeling of fdg dpet. we compare the best dnn model with the most commonly used
conventional pbpk method, curve fit. while no ground truth is available, the
proposed method provides similar results to curve fit but qualitatively more
plausible images in physiology and with a radically shorter run-time.further,
our approach can be applied to other kms without significantly increasing the
complexity and the need for computational power. in general, eq. 2 should be
modified to represent the desired km [20], and the number of channels of the
output of the network should be the same as the kp to be predicted.overall, this
work offers scalability and a new research direction for analysing
pharmacokinetics.
fig. 2. comparison between the kinetic parameters obtained with different
methods: kpdnn in blue, kpcf in orange and, as plausibility check, kp ref cf
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_28.
deep learning techniques have greatly improved medical image segmentation by
automatically extracting specific tissue or substance location information,
which facilitates accurate disease diagnosis and assessment. however, most deep
learning approaches for segmentation require fully or partially labeled training
datasets, which can be time-consuming and expensive to annotate. to address this
issue, recent research has focused on developing segmentation frameworks that
require little or no segmentation labels.to meet this need, many researchers
have devoted their efforts to weakly-supervised semantic segmentation (wsss)
[21], which utilizes weak supervision, such as image-level classification
labels. recent wsss methods can be broadly categorized into two types [4]:
class-activation-mapping-based (cam-based) [9,13,16,19,20,22], and
multiple-instance-learning-based (mil-based) [15] methods.the literature has not
adequately addressed the issue of low-resolution class-activation maps (cams),
especially for medical images. some existing methods, such as dilated residual
networks [24] and u-net segmentation architecture [3,7,17], have attempted to
tackle this issue, but still require many upsampling operations, which the
results become blurry. meanwhile, layercam [9] has proposed a hierarchical
solution that extracts activation maps from multiple convolution layers using
grad-cam [16] and aggregates them with equal weights. although this approach
successfully enhances the resolution of the segmentation mask, it lacks
flexibility and may not be optimal.in this paper, we propose an attentive
multiple-exit cam (ame-cam) for brain tumor segmentation in magnetic resonance
imaging (mri). different from recent cam methods, ame-cam uses a classification
model with multipleexit training strategy applied to optimize the internal
outputs. activation maps from the outputs of internal classifiers, which have
different resolutions, are then aggregated using an attention model. the model
learns the pixel-wise weighted sum of the activation maps by a novel contrastive
learning method.our proposed method has the following contributions:-to tackle
the issues in existing cams, we propose to use multiple-exit classification
networks to accurately capture all the internal activation maps of different
resolutions. -we propose an attentive feature aggregation to learn the
pixel-wise weighted sum of the internal activation maps. -we demonstrate the
superiority of ame-cam over state-of-the-art cam methods in extracting
segmentation results from classification networks on the 2021 brain tumor
segmentation challenge (brats 2021) [1,2,14]. -for reproducibility, we have
released our code at https://github.com/windstormer/ame-cam overall, our
proposed method can help overcome the challenges of expensive and time-consuming
segmentation labeling in medical imaging, and has the potential to improve the
accuracy of disease diagnosis and assessment.
the proposed ame-cam method consists of two training phases: activation
extraction and activation aggregation, as shown in fig. 1. in the activation
extraction phase, we use a binary classification network, e.g., resnet-18, to
obtain the class probability y = f (i) of the input image i. to enable
multipleexit training, we add one internal classifier after each residual block,
which generates the activation map m i of different resolutions. we use a
cross-entropy loss to train the multiple-exit classifier, which is defined as
where gap (•) is the global-average-pooling operation, ce(•) is the
cross-entropy loss, and l is the image-wise ground-truth label.in the activation
aggregation phase, we create an efficient hierarchical aggregation method to
generate the aggregated activation map m f by calculating the pixel-wise
weighted sum of the activation maps m i . we use an attention network a(•) to
estimate the importance of each pixel from each activation map. the attention
network takes in the input image i masked by the activation map and outputs the
pixel-wised importance score s xyi of each activation map. we formulate the
operation as follows:where [•] is the concatenate operation, n(•) is the min-max
normalization to map the range to [0,1], and ⊗ is the pixel-wise multiplication,
which is known as image masking. the aggregated activation map m f is then
obtained by the pixel-wise weighted sum of m i , which iswe train the attention
network with unsupervised contrastive learning, which forces the network to
disentangle the foreground and the background of the aggregated activation map m
f . we mask the input image by the aggregated activation map m f and its
opposite (1 -m f ) to obtain the foreground feature and the background feature,
respectively. the loss function is defined as follows:where v f i and v b i
denote the foreground and the background feature of the i-th sample,
respectively. simm in and simm ax are the losses that minimize and maximize the
similarity between two features (see c 2 am [22] for details).finally, we
average the activation maps m 1 to m 4 and the aggregated map m f to obtain the
final cam results for each image. we apply the dense conditional random field
(densecrf) [12] algorithm to generate the final segmentation mask. it is worth
noting that the proposed method is flexible and can be applied to any
classification network architecture.
we evaluate our method on the brain tumor segmentation challenge (brats) dataset
[1,2,14], which contains 2,000 cases, each of which includes four 3d volumes
from four different mri modalities: t1, post-contrast enhanced t1 (t1-ce), t2,
and t2 fluid attenuated inversion recovery (t2-flair), as well as a
corresponding segmentation ground-truth mask. the official data split divides
these cases by the ratio of 8:1:1 for training, validation, and testing (5,802
positive and 1,073 negative images). in order to evaluate the performance, we
use the validation set as our test set and report statistics on it. we
preprocess the data by slicing each volume along the z-axis to form a total of
193,905 2d images, following the approach of kang et al. [10] and dey and hong
[6]. we use the ground-truth segmentation masks only in the final evaluation,
not in the training process.
we implement our method in pytorch using resnet-18 as the backbone classifier.
we pretrain the classifier using supcon [11] and then fine-tune it in our
experiments. we use the entire training set for both pretraining and
fine-tuning. we set the initial learning rate to 1e-4 for both phases, and use
the cosine annealing scheduler to decrease it until the minimum learning rate is
5e-6. we set the weight decay in both phases to 1e-5 for model regularization.
we use adam optimizer in the multiple-exit phase and sgd optimizer in the
aggregation phase. we train all classifiers until they converge with a test
accuracy of over 0.9 for all image modalities. note that only class labels are
available in the training set.we use the dice score and intersection over union
(iou) to evaluate the quality of the semantic segmentation, following the
approach of xu et al. [23], tang et al. [18], and qian et al. [15]. in addition,
we report the 95% hausdorff distance (hd95) to evaluate the boundary of the
prediction mask.interested readers can refer to the supplementary material for
results on other network architectures.
in this section, we compare the segmentation performance of the proposed ame-cam
with five state-of-the-art weakly-supervised segmentation methods, namely
grad-cam [16], scorecam [19], lfi-cam [13], layercam [9], and swin-mil [15]. we
also compare with an unsupervised approach c&f [5], the supervised version of
c&f, and the supervised optimized u-net [8] to show the comparison with
non-cam-based methods. we acknowledge that the results from fully supervised and
unsupervised methods are not directly comparable to the weakly supervised cam
methods. nonetheless, these methods serve as interesting references for the
potential performance ceiling and floor of all the cam methods. quantitatively,
grad-cam and scorecam result in low dice scores, demonstrating that they have
difficulty extracting the activation of medical images. lfi-cam and layercam
improve the dice score in all modalities, except lfi-cam in t1-ce and t2-flair.
finally, the proposed ame-cam achieves optimal performance in all modalities of
the brats dataset.compared to the unsupervised baseline (ul), c&f is unable to
separate the tumor and the surrounding tissue due to low contrast, resulting in
low dice scores in all experiments. with pixel-wise labels, the dice of
supervised c&f improves significantly. without any pixel-wise label, the
proposed ame-cam outperforms supervised c&f in all modalities.the fully
supervised (fsl) optimized u-net achieves the highest dice score and iou score
in all experiments. however, even under different levels of supervision, there
is still a performance gap between the weakly supervised cam methods and the
fully supervised state-of-the-art. this indicates that there is still potential
room for wsss methods to improve in the future.qualitatively, fig. 2 shows the
visualization of the cam and segmentation results from all six cam-based
approaches under four different modalities from the brats dataset. grad-cam
(fig. 2(c)) results in large false activation region, where the segmentation
mask is totally meaningless. scorecam eliminates false activation corresponding
to air. lfi-cam focus on the exact tumor area only in the t1 and t2 mri (row 1
and 3). swin-mil can hardly capture the tumor region of the mri image, where the
activation is noisy. among all, only layercam and the proposed ame-cam
successfully focus on the exact tumor area, but ame-cam reduces the
under-estimation of the tumor area. this is attributed to the benefit provided
by aggregating activation maps from different resolutions.
in table 2, we conducted an ablation study to investigate the impact of using
different aggregation approaches after extracting activations from the
multiple-exit network. we aim to demonstrate the superiority of the proposed
attention-based aggregation approach for segmenting tumor regions in t1 mri of
the brats dataset. note that we only report the results for t1 mri in the brats
dataset. please refer to the supplementary material for the full set of
experiments.as a baseline, we first conducted the average of four activation
maps generated by the multiple-level activation extraction (avg. me). we then
applied c 2 am [22], a state-of-the-art cam-based refinement approach, to refine
the result of the baseline, which we call "avg. me+c 2 am". however, we observed
that c 2 am tended to segment the brain region instead of the tumor region due
to the larger contrast between the brain tissue and the air than that between
the tumor region and its surrounding tissue. any incorrect activation of c 2 am
also led to inferior results, resulting in a degradation of the average dice
score from 0.617 to 0.484. in contrast, the proposed attention-based approach
provided a significant weighting solution that led to optimal performance in all
cases.table 3. ablation study for using single-exit from m1, m2, m3 or m4 of
fig. 1 and the multiple-exit using results from m2 and m3 and using all exits
(ame-cam). the experiments are done on the t1-ce mri of brats dataset. the dice
score, iou, and the hd95 are reported in the form of mean ± std.
dice effect of single-exit and multiple-exit: table 3 summarizes the performance
of using single-exit from m 1 , m 2 , m 3 , or m 4 of fig. 1 and the
multipleexit using results from m 2 and m 3 , and using all exits (ame-cam) on
t1-ce mri in the brats dataset.the comparisons show that the activation map
obtained from the shallow layer m 1 and the deepest layer m 4 result in low dice
scores, around 0.15. this is because the network is not deep enough to learn the
tumor region in the shallow layer, and the resolution of the activation map
obtained from the deepest layer is too low to contain sufficient information to
make a clear boundary for the tumor. results of the internal classifiers from
the middle of the network (m 2 and m 3 ) achieve the highest dice score and iou,
both of which are around 0.5.to evaluate whether using results from all internal
classifiers leads to the highest performance, we further apply the proposed
method to the two internal classifiers with the highest dice scores, i.e., m 2
and m 3 , called m 2 + m 3 . compared with using all internal classifiers (m 1
to m 4 ), m 2 + m 3 results in 18.6% and 22.1% lower dice and iou, respectively.
in conclusion, our ame-cam still achieves the optimal performance among all the
experiments of single-exit and multiple-exit.other ablation studies are
presented in the supplementary material due to space limitations.
in this work, we propose a brain tumor segmentation method for mri images using
only class labels, based on an attentive multiple-exit class activation mapping
(ame-cam). our approach extracts activation maps from different exits of the
network to capture information from multiple resolutions. we then use an
attention model to hierarchically aggregate these activation maps, learning
pixel-wise weighted sums.experimental results on the four modalities of the 2021
brats dataset demonstrate the superiority of our approach compared with other
cam-based weakly-supervised segmentation methods. specifically, ame-cam achieves
the highest dice score for all patients in all datasets and modalities. these
results indicate the effectiveness of our proposed approach in accurately
segmenting brain tumors from mri images using only class labels.
lfi-cam (2021) 0.121 ± 0.120 0.069 ± 0.076 136.246 ± 38.619 layercam (2021)
0.510 ± 0.209 0.367 ± 0.180 29.850 ± 45.877 swin-mil (2022) 0.460 ± 0.169 0.314
± 0.140 46.996 ± 22.821 ame-cam (ours) 0.695 ± 0.095 0.540 ± 0.108 18.129 ±
12.335
opt. u-net (2021) 0.914 ± 0.058 0.847 ± 0.093 8.093 ± 11.879 4 results
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0 17.
in the medical domain, obtaining a large amount of high-confidence labels, such
as histopathological diagnoses, is arduous due to the cost and required
technicality. it is however possible to obtain lower confidence assessments for
a large amount of images, either by a clinical questioning, or directly by a
radiological diagnosis. to take advantage of large volumes of unlabeled or
weakly-labeled images, pre-training encoders with self-supervised methods showed
promising results in deep learning for medical imaging [1,4,21,[27][28][29]. in
particular, contrastive learning (cl) is a self-supervised method that learns a
mapping of the input images to a representation space where similar (positive)
samples are moved closer and different (negative) samples are pushed far apart.
weak discrete labels can be integrated into contrastive learning by, for
instance, considering as positives only the samples having the same label, as in
[13], or by directly weighting unsupervised contrastive and supervised cross
entropy loss functions, as in [19]. in this work, we focus on the scenario where
radiological meta-data (thus, low-confidence labels) are available for a large
amount of images, whereas high-confidence labels, obtained by histological
analysis, are scarce.naive extensions of contrastive learning methods, such as
[5,10,11], from 2d to 3d images may be difficult due to limited gpu memory and
therefore small batch size. a usual solution consists in using patch-based
methods [8,23]. however, these methods pose two difficulties: they reduce the
spatial context (limited by the size of the patch), and they require similar
spatial resolution across images. this is rarely the case for abdominal ct/mri
acquisitions, which are typically strongly anisotropic and with variable
resolutions. alternatively, depth position of each 2d slice, within its
corresponding volume, can be integrated in the analysis. for instance, in [4],
the authors proposed to integrate depth in the sampling strategy for the batch
creation. likewise, in [26], the authors proposed to define as similar only 2d
slices that have a small depth difference, using a normalized depth coordinate d
∈ [0, 1]. these works implicitly assume a certain threshold on depth to define
positive and negative samples, which may be difficult to define and may be
different among applications and datasets. differently, inspired by [2,8], here
we propose to use a degree of "positiveness" between samples by defining a
kernel function w on depth positions. this allows us to consider volumetric
depth information during pre-training and to use large batch sizes. furthermore,
we also propose to simultaneously leverage weak discrete attributes during
pre-training by using a novel and efficient contrastive learning composite
kernel loss function, denoting our global method weakly-supervised positional
(wsp).we apply our method to the classification of histology-proven liver
cirrhosis, with a large volume of (weakly) radiologically-annotated ct-scans and
a small amount of histopathologically-confirmed cirrhosis diagnosis. we compare
the proposed approach to existing self-supervised methods.
let x t be an input 2d image, usually called anchor, extracted from a 3d volume,
y t a corresponding discrete weak variable and d t a related continuous
variable. in this paper, y t refers to a weak radiological annotation and d t
corresponds to the normalized depth position of the 2d image within its
corresponding 3d volume: if v max corresponds to the maximal depth-coordinate of
a volume v , we compute d t = pt vmax with p t ∈ [0, v max ] being the original
depth coordinate.let x - j and x + i be two semantically different (negative)
and similar (positive) images with respect to x t , respectively.the definition
of similarity is crucial in cl and is the main difference between existing
methods. for instance, in unsupervised cl, methods such as simclr [5,6] choose
as positive samples random augmentations of the anchor x + i = t(x t ), where t
∼ t is a random transformation chosen among a user-selected family t . negative
images x - j are all other (transformed) images present in the batch. once x - j
and x + i are defined, the goal of cl is to compute a mapping function f θ : x →
s d , where x is the set of images and s d the representation space, so that
similar samples are mapped closer in the representation space than dissimilar
samples. mathematically, this can be defined as looking for a f θ that satisfies
the condition:where, with sim a similarity function defined here as sim(a, b) =
a t b τ with τ > 0. in the presence of discrete labels y, the definition of
negative (x - j ) and positive (x + i ) samples may change. for instance, in
supcon [13], the authors define as positives all images with the same discrete
label y. however, when working with continuous labels d, one cannot use the same
strategy since all images are somehow positive and negative at the same time. a
possible solution [26] would be to define a threshold γ on the distance between
labels (e.g., d a , d b ) so that, if the distance is smaller than γ (i.e., ||d
ad b || 2 < γ), the samples (e.g., x a and x b ) are considered as positives.
however, this requires a user-defined hyperparameter γ, which could be hard to
find in practice. a more efficient solution, as proposed in [8], is to define a
degree of "positiveness" between samples using a normalized kernel function w σ
(d,, where k σ is, for instance, a gaussian kernel, with user defined
hyper-parameter σ and 0 ≤ w σ ≤ 1. it is interesting to notice that, for
discrete labels, one could also define a kernel as: w δ (y, y i ) = δ(yy i ), δ
being the dirac function, retrieving exactly supcon [13].in this work, we
propose to leverage both continuous d and discrete y labels, by combining (here
by multiplying) the previously defined kernels, w σ and w δ , into a composite
kernel loss function. in this way, samples will be considered as similar
(positive) only if they have a composite degree of "positiveness" greater than
zero, namely both kernels have a value greater (or different) than 0 (w σ > 0
and w δ = 0). an example of resulting representation space is shown in fig. 1.
this constraint can be defined by slightly modifying the condition introduced in
eq. 1, as:where the indices t, i, j traverse all n images in the batch since
there are no "hard" positive or negative samples, as in simclr or supcon, but
all images are considered as positive and negative at the same time. as commonly
done in cl [3], this condition can be transformed into an optimization problem
using the max operator and its smooth approximation logsumexp:arg min(3) by
defining p (t) = {i : y i = y t } as the set of indices of images x i in the
batch with the same discrete label y i as the anchor x t , we can rewrite our
final loss function as:wherein practice, it is rather easy to find a good value
of σ, as the proposed kernel method is quite robust to its variation. a
robustness study is available in the supplementary material. for the
experiments, we fix σ = 0.1.
we compare the proposed method with different contrastive and non-contrastive
methods, that either use no meta-data (simclr [5], byol [10]), or leverage only
discrete labels (supcon [13]), or continuous labels (depth-aware [8]). the
proposed method is the only one that takes simultaneously into account both
discrete and continuous labels. in all experiments, we work with 2d slices
rather than 3d volumes due to the anisotropy of abdominal ct-scans in the depth
direction and the limited spatial context or resolution obtained with 3d
patchbased or downsampling methods, respectively, which strongly impacts the
cirrhosis diagnosis that is notably based on the contours irregularity.
moreover, the large batch sizes necessary in contrastive learning can not be
handled in 3d due to a limited gpu memory.
three histo . it corresponds to absent fibrosis (f0), mild fibrosis (f1),
significant fibrosis (f2), severe fibrosis (f3) and cirrhosis (f4). this score
is then binarized to indicate the absence or presence of advanced fibrosis [14]:
f0/f1/f2 (n = 28) vs. f3/f4 (n = 78). d 2 histo . this is the public lihc
dataset from the cancer genome atlas [9], which presents a histological score,
the ishak score, designated as y 2 histo , that differs from the metavir score
present in d 1 histo . this score is also distributed through five labels: no
fibrosis, portal fibrosis, fibrous speta, nodular formation and incomplete
cirrhosis and established cirrhosis. similarly to the metavir score in d 1 histo
, we also binarize the ishak score, as proposed in [16,20], which results in two
cohorts of 34 healthy and 15 pathological patients.in all datasets, we select
the slices based on the liver segmentation of the patients. to gain in
precision, we keep the top 70% most central slices with respect to liver
segmentation maps obtained manually in d radio , and automatically for d 1 histo
and d 2 histo using a u-net architecture pretrained on d radio [18]. for the
latter pretraining dataset, it presents an average slice spacing of 3.23 mm with
a standard deviation of 1.29 mm. for the x and y axis, the dimension is 0.79 mm
per voxel on average, with a standard deviation of 0.10 mm.
backbones. we propose to work with two different backbones in this paper:
tinynet and resnet-18 [12]. tinynet is a small encoder with 1.1m parameters,
inspired by [24], with five convolutional layers, a representation space (for
downstream tasks) of size 256 and a latent space (after a projection head of two
dense layers) of size 64. in comparison, resnet-18 has 11.2m parameters, a
representation space of dimension 512 and a latent space of dimension 128. more
details and an illustration of tinynet are available in the supplementary
material, as well as a full illustration of the algorithm flow.data
augmentation, sampling and optimization. cl methods [5,10,11] require strong
data augmentations on input images, in order to strengthen the association
between positive samples [22]. in our work, we leverage three types of
augmentations: rotations, crops and flips. data augmentations are computed on
the gpu, using the kornia library [17]. during inference, we remove the
augmentation module to only keep the original input images.for sampling,
inspired by [4], we propose a strategy well-adapted for contrastive learning in
2d medical imaging. we first sample n patients, where n is the batch size, in a
balanced way with respect to the radiological/histological classes; namely, we
roughly have the same number of subjects per class. then, we randomly select
only one slice per subject. in this way, we maximize the slice heterogeneity
within each batch. we use the same sampling strategy also for classification
baselines. for d 2 histo , which has fewer patients than the batch size, we use
a balanced sampling strategy with respect to the radiological/histological
classes with no obligation of one slice per patient in the batch. as we work
with 2d slices rather than 3d volumes, we compute the average probability per
patient of having the pathology. the evaluation results presented later are
based on the patient-level aggregated prediction.finally, we run our experiments
on a tesla v100 with 16gb of ram and a 6 cpu cores, and we used the
pytorch-lightning library to implement our models. all models share the same
data augmentation module, with a batch size of b = 64 and a fixed number of
epochs n epochs = 200. for all experiments, we fix a learning rate (lr) of α =
10 -4 and a weight decay of λ = 10 -4 . we add a cosine decay learning rate
scheduler [15] to prevent over-fitting. for byol, we initialize the moving
average decay at 0.996. evaluation protocol. we first pretrain the backbone
networks on d radio using all previously listed contrastive and non-contrastive
methods. then, we train a regularized logistic regression on the frozen
representations of the datasets d 1 histo and d 2 histo . we use a stratified
5-fold cross-validation. as a baseline, we train a classification algorithm from
scratch (supervised) for each dataset, d 1 histo and d 2 histo , using both
backbone encoders and the same 5-fold crossvalidation strategy. we also train a
regularized logistic regression on representations obtained with a random
initialization as a second baseline (random). finally, we report the
cross-validated results for each model on the aggregated dataset
we present in table 1 the results of all our experiments. for each of them, we
report whether the pretraining method integrates the weak label meta-data, the
depth spatial encoding, or both, which is the core of our method. first, we can
notice that our method outperforms all other pretraining methods in d 1 histo
and d 1+2 histo , which are the two datasets with more patients. for the latter,
the proposed method surpasses the second best pretraining method, depth-aware,
by 4%. for d 1 histo , it can be noticed that wsp (ours) provides the best auc
score whatever the backbone used. for the second dataset d 2 histo , our method
is on par with byol and supcon when using a small encoder and outperforms the
other methods when using a larger backbone.to illustrate the impact of the
proposed method, we report in fig. 2 the projections of the resnet-18
representation vectors of 10 randomly selected subjects of d 1 histo onto the
first two modes of a pca. it can be noticed that the representation space of our
method is the only one where the diagnostic label (not available during
pretraining) and the depth position are correctly integrated. indeed, there is a
clear separation between slices of different classes (healthy at the bottom and
cirrhotic cases at the top) and at the same time it seems that the depth
position has been encoded in the x-axis, from left to right. supcon performs
well on the training set of d radio (figure available in the supplementary
material), as well as d 2 histo with tinynet, but it poorly generalizes to d 1
histo and d 1+2 histo . the method depth-aware manages to correctly encode the
depth position but not the diagnostic class label.to assess the clinical
performance of the pretraining methods, we also compute the balanced accuracy
scores (bacc) of the trained classifiers, which is compared in table 2 to the
bacc achieved by radiologists who were asked to visually assess the presence or
absence of cirrhosis for the n=106 cases of d 1 histo . the reported bacc values
correspond to the best scores among those obtained with tiny and resnet
encoders.radiologists achieved a bacc of 82% with respect to the histological
reference. the two bestperforming methods surpassed this score: depth-aware and
the proposed wsp approach, improving respectively the radiologists score by 2%
and 3%, suggesting that including 3d information (depth) at the pretraining
phase was beneficial.
in this work, we proposed a novel kernel-based contrastive learning method that
leverages both continuous and discrete meta-data for pretraining. we tested it
on a challenging clinical application, cirrhosis prediction, using three
different datasets, including the lihc public dataset. to the best of our
knowledge, this is the first time that a pretraining strategy combining
different kinds of meta-data has been proposed for such application. our results
were compared to other stateof-the-art cl methods well-adapted for cirrhosis
prediction. the pretraining methods were also compared visually, using a 2d
projection of the representation vectors onto the first two pca modes. results
showed that our method has an organization in the representation space that is
in line with the proposed theory, which may explain its higher performances in
the experiments. as future work, it would be interesting to adapt our kernel
method to non-contrastive methods, such as simsiam [7], byol [10] or barlow
twins [25], that need smaller batch sizes and have shown greater performances in
computer vision tasks. in terms of application, our method could be easily
translated to other medical problems, such as pancreas cancer prediction using
the presence of intrapancreatic fat, diabetes mellitus or obesity as discrete
meta-labels.
deep neural networks (dnns) have been successfully applied to various supervised
3d biomedical image analysis tasks, such as classification [11], segmentation
[7], and registration [35]. acquiring volumetric annotations manually to
supervise deep learning models is costly and labor intensive. for example, the
supervised training of 3d dnns for segmentation requires the manual labeling of
every voxel of the structures of interest for the entire training set.
additionally, the diversity of existing biomedical 3d volumetric image types
(e.g. mri, ct, electron tomography) and different tasks associated with them
precludes image annotations for all existing problems in practice. furthermore,
experts may focus on annotating objects they are already aware of, thereby
restricting the possibility of new structural discoveries in large datasets
using deep learning. we hypothesize that the nested hierarchical structure
intrinsic to many 3d biomedical images [13] might be useful for unsupervised
segmentation. as a step in this direction, our goal in this work is to develop a
computational approach for unsupervised structure discovery.recently,
unsupervised part discovery in 2d natural images has gained significant
attention [6,8,15]. these methods are based on the finding that intermediate
activations of deep imagenet-pre-trained classification models capture
semantically meaningful conceptual regions [8]. these regions are robust to pose
and viewpoint variations and help high-level image understanding by providing
local object representations, leading to more explainable recognition [15].
however, a naive application of part discovery methods to 3d volumetric
segmentation is not feasible, due to the lack of good feature extractors for 3d
biomedical images [5] and imagenet-pretrained networks operate only on 2d
images.we hypothesize that deep generative models are good feature extractors
for unsupervised structure discovery for the following reasons. first, these
models do not require expert labels as they are trained in a self-supervised
way. second, the ability to generate high-quality images suggests that these
models capture semantically meaningful information. third, generative
representation learning has been successfully applied to global and dense
prediction tasks in 2d images [9] and has shown improvements in label efficiency
and generalization [19].besides creating stunning image generation results,
diffusion-based generative models [12] are applied to other downstream tasks.
several works use pretrained diffusion models for 2d label-efficient semantic
segmentation of natural images [1,4]. in 2d medical imaging, diffusion models
are used for self-supervised vessel segmentation [18], anomaly detection
[27,29,31,34], denoising [14], and improving supervised segmentation models
[32,33]. in 3d medical imaging, diffusion models are used for ct and mr image
synthesis [10,33]. inspired by the success of unsupervised part discovery
methods in 2d images and the effective abilities of diffusion models for many
downstream tasks we hypothesize that feature representations of generative
diffusion models discover intrinsic hierarchical structures in 3d biomedical
images. our work explores this hypothesis. our contributions are:1) we pretrain
3d diffusion models, use them as feature extractors (fig. 1), and design losses
(fig. 2) for unsupervised 3d structure discovery. 2) we show that features from
different stages of ladder-like u-net-based diffusion models capture different
hierarchy levels in 3d biomedical volumes. 3) our approach outperforms previous
3d unsupervised discovery methods on challenging synthetic datasets and on a
real-world brain tumor segmentation (brats'19) dataset.
diffusion models [12] consist of two parts: a forward pass and a reverse pass.
the forward pass is a t -step process of adding a small gaussian noise,
gradually destroying image information and transforming a clean image x 0 into
pure gaussian noise x t . each step t ∈ 1, t is:where), can be written as:the
reverse pass is a corresponding t -step denoising process using a neural network
(usually, u-net [28]) with parameters θ. for small noises, the reverse pass is
also gaussian:(practically, instead of μ θ (x t , t) and σ θ (x t , t), models
are designed to predict either the noise t at timestep t, or a less noisier
version of image x t-1 directly.
we formulate the 3d structure discovery task in biomedical images as an
unsupervised segmentation into k parts. given a one-channel 3d imagewe use three
losses for unsupervised training (see fig. 2):for an arbitrary representation
h(x 0 ) of an image x 0 with voxels u, the consistency of this representation
c(h(x 0 )) across k predicted parts in the form of segmentation m is defined
as:where n is the number of voxels. this is a form of volume-normalized k-means
loss with z k describing the mean feature value of partition k.feature
consistency. we pretrain generative 3d diffusion models and use them as feature
extractors [4]. noise is added to a clean image x 0 based on eq. ( 2) and the
noisy image x t ∈ r 1×h×w ×d is passed to the 3d diffusion model. intermediate
activations (either from different stages of ladder-like u-nets or their
concatenation, see fig. 1) upsampled to the original image size serve as a
p-dimensional feature extractor φ(x 0 ) ∈ r p×h×w ×d . the feature consistency
loss encourages voxels corresponding to the same parts to have similar
features:visual consistency. the extracted features are upsampled from low
spatial resolutions and therefore do not accurately align with image boundaries.
to alleviate this problem, we use a voxel visual consistency loss:where i(x 0 )
is the identity feature extractor, i.e. i(x 0 ) = x 0 .photometric invariance.
as biomedical images often show acquisition differences (e.g., based on mr or ct
scanner), they can be heterogeneous in their voxel intensities [25]. therefore,
robustness of models to voxel-level photometric perturbations might be helpful
for unsupervised discovery. we use the dice loss [22] to encourage invariance to
such a photometric transformation t :we assume our images are min-max normalized
(x 0 ∈ [0, 1]). we then use gamma-correction of the form t (x 0 ) = x γ 0 as a
photometric transformation. we draw γ from the uniform distribution:
to compare with state-of-the-art unsupervised 3d segmentation methods we follow
[13] and evaluate our method on challenging biologically inspired 3d synthetic
datasets and a real-world brain tumor segmentation (brats'19) dataset.the
synthetic dataset of [13], consists of 120 volumes (80-20-20 split) of size 50 ×
50 × 50. inspired by cryo-electron tomography images, it contains a three-level
structure, representing a biological cell, vesicles and mitochondria, as well as
protein aggregates. the intensities and locations of the objects are randomized
without destroying the hierarchy. the regular variant of the dataset contains
cubical and spherical objects, while the irregular variant contains more complex
shapes. pink noise of magnitude m = 0.25 which is commonly seen in biological
data [30] is applied to the volume. figure 3 shows sample slices of both
variants.the brats'19 dataset [2,3,21] is an established benchmark for 3d tumor
segmentation of brain mris. volumes are co-registered to the same template,
interpolated to (1 mm) 3 resolution and brain-extracted. following [13], images
are cropped to volumes of size 200 × 200 × 155. as in [13], flair images and
corresponding whole tumor (wt) annotations are used for unsupervised
segmentation evaluation with the same split of 259 high grade glioma training
examples into 180 train, 39 validation, and 40 test samples. the official
brats'19 validation and test sets are not used as their segmentation masks are
not available.
all diffusion models use the same architecture shown in fig. 1. we pretrain them
for 50k epochs with batch size 4, using an l1 loss between the denoised and the
original images. we use the adam optimizer, a cosine noise schedule, learning
rate 10 -4 and t = 250 steps. the first layer has 64 channels and this number is
doubled for the proceeding downsampling layers. due to memory constraints for
brats'19, we trained diffusion models at 128 × 128 × 128 resolution. however,
the extracted features are upsampled to the original 200 × 200 × 155
resolution.our segmentation networks (f in fig. 2) use a 3d u-net architecture
[7,28]. we trained them for 100 epochs using the adam optimizer, a learning rate
of 3 * 10 -4 and the losses in eq. ( 4). we selected the epoch that gave the
best average probability of the segmentation mask for all inputs [26] as our
final model. noisy images at timestep t = 25 are used as input to the diffusion
models. due to the memory limits, for brats'19, we used stage 2 features, as
they have the least number of channels. we set λ f = λ v = λ inv = 1 and γ ∼ u
[0.9, 1.1] for all cases.for all experiments we used pytorch and 4 nvidia a6000
gpus (48 gb).
we compare our method with state-of-the-art unsupervised 3d structure discovery
approaches including clustering using 3d feature learning [23], a 3d
convolutional autoencoder [24], and self-supervised hyperbolic representations
[13].for the synthetic datasets, we used k = 2 (background and cell) for level
1, k = 4 (background, cell, vesicle, mitochondria) for level 2, and k = 8
(background, cell, vesicle, mitochondria, and 4 small protein aggregates) for
level 3 predictions. the evaluation metric is the average dice score on the
annotated test labels. as the label order may differ we use the hungarian
algorithm to match the predicted masks with the ground truth segmentations.
table 1 shows the results for the regular and irregular variants of the
cryo-et-inspired synthetic dataset. our models outperform all previous
unsupervised work at all hierarchy levels. for some levels, our models even
outperform semi-supervised methods (c ¸içek et al. [7] used 2% of annotated
data, zhao et al. [36] used one annotated volume). we found that simple
unsupervised denoising (bm4d [20]) followed by k-means clustering provides a
good baseline, although vanilla kmeans clustering on voxel intensities does not
perform well due to noise. results in fig. 3 demonstrate that our proposed
unsupervised method indeed discovers the hierarchical structure of different
levels. we also show in table 2 that features from early decoder stages of the
u-net-based diffusion models better discover larger objects in the hierarchy,
features at intermediate stages better capture intermediate objects, and
features at later stages better find smaller objects.for the brain tumor
segmentation (brats'19) dataset, we use the whole tumor (wt) segmentation mask
for evaluation, which is detectable based on the flair images alone. we train
segmentation models with k = 3 parts (background, brain, tumor). the evaluation
metric, as in the brats'19 challenge [21], is dice score and the 95th percentile
of the symmetric hausdorff distance, which quantifies the surface distance of
the predicted segmentation from the manual tumor segmentation in millimeters.
table 3 shows that our model outperforms all prior unsupervised methods for both
evaluation metrics. as an approximate upper bound we show for reference the
reported results of the 1st place solution [17] on brats'19 which is based on
supervised training on the full train set and evaluated on the brats'19 test
set. the qualitative results in fig. 4 show that our model can detect tumors of
different sizes. our predictions look smoother and do not capture fine details
of tumor segmentations.we perform ablation studies on the brats'19 dataset
(table 3: below the line). measuring the impact of each loss, we see that the
smallest performance drop is due to a deactivated invariance loss (λ inv = 0)
while deactivating the visual consistency (λ v = 0) and feature consistency (λ f
= 0) losses results in larger, but similar performance drops. however, to
achieve best performance all three components are necessary. we also perform
k-means clustering on intensities and features. we observe that using our deep
network model dramatically improves performance, although our losses are similar
to k-means clustering.this might be due to the fact that predictive modeling
involves learning from a distribution of images and a model may therefore
extract useful knowledge from a collection of images. to evaluate the
significance of the diffusion features, we replaced our diffusion feature
extractor with a 3d resnet from med3d [5] trained on 23 medical datasets. we use
the "layer1 2 conv2" features as they showed the best performance. although
performance does not drop significantly when med3d features are used with our
losses, med3d features do not produce good results when directly used for
k-means clustering.
in this work, we showed that features from 3d generative diffusion models using
a ladder-like u-net-based architecture can discover intrinsic 3d structures in
biomedical images. we trained predictive unsupervised segmentation models using
losses that encourage the decomposition of biomedical volumes into nested
subvolumes aligned with their hierarchical structures. our method outperforms
existing unsupervised segmentation approaches and discovers meaningful
hierarchical concepts on challenging biologically-inspired synthetic datasets
and on the brats brain tumor dataset. while we tested our approach for
unsupervised image segmentation it is conceivable that it could also be useful
in semisupervised settings and that could be applied to data types other than
images.
colorectal cancer is a leading cause of cancer-related deaths worldwide [1].
early detection and efficient diagnosis of polyps, which are precursors to
colorectal cancer, is crucial for effective treatment. recently, deep learning
has emerged as a powerful tool in medical image analysis, prompting extensive
research into its potential for polyp segmentation. the effectiveness of deep
learning models in medical applications is usually based on large,
well-annotated datasets, which in turn necessitates a time-consuming and
expertise-driven annotation process. this has prompted the emergence of
approaches for annotation-efficient weakly-supervised learning in the medical
domain with limited annotations like points [8], bounding boxes [12], and
scribbles [15]. compared with other sparse labeling methods, scribbles allow the
annotator to annotate arbitrary shapes, making them more flexible than points or
boxes [13]. besides, scribbles provide a more robust supervision signal, which
can be prone to noise and outliers [5]. hence, this work investigates the
feasibility of conducting polyp segmentation using scribble annotation as
supervision. the effectiveness of medical applications during in-site deployment
depends on their ability to generalize to unseen data and remain robust against
data corruption. improving these factors is crucial to enhance the accuracy and
reliability of medical diagnoses in real-world scenarios [22,27,28]. therefore,
we comprehensively evaluate our approach on multiple datasets from various
medical sites to showcase its viability and effectiveness across different
contexts.dual-branch learning has been widely adopted in annotation-efficient
learning to encourage mutual consistency through co-teaching. while existing
approaches are typically designed for learning in the spatial domain
[21,25,29,30], a novel spatial-spectral dual-branch structure is introduced to
efficiently leverage domain-specific complementary knowledge with synergistic
mutual teaching. furthermore, the outputs from the spatial-spectral branches are
aggregated to produce mixed pseudo labels as supplementary supervision.
different from previous methods, which generally adopt the handcrafted fusion
strategies [15], we design to aggregate the outputs from spatial-spectral dual
branches with an entropy-guided adaptive mixing ratio for each pixel.
consequently, our incorporated tactic of pseudo-label fusion aptly assesses the
pixel-level ambiguity emerging from both spatial and frequency domains based on
their entropy maps, thereby allocating substantially assured categorical labels
to individual pixels and facilitating effective pseudo label ensemble learning.
contributions. overall, the contributions of this work are threefold: first, we
devise a spatial-spectral dual-branch structure to leverage cross-space
knowledge and foster collaborative mutual teaching. to our best knowledge, this
is the first attempt to explore the complementary relations of the
spatial-spectral dual branch in boosting weakly-supervised medical image
analysis. second, we introduce the pixel-level entropy-guided fusion strategy to
generate mixed pseudo labels with reduced noise and increased confidence, thus
enhancing ensemble learning. lastly, our proposed hybrid loss optimization,
comprising scribblessupervised loss, mutual training loss with domain-specific
pseudo labels, and ensemble learning loss with fused-domain pseudo labels,
facilitates obtaining a generalizable and robust model for polyp image
segmentation. an extensive assessment of our approach through the examination of
four publicly accessible datasets establishes its superiority and clinical
significance.
spectral-domain learning [26] has gained increasing popularity in medical image
analysis [23] for its ability to identify subtle frequency patterns that may not
be well detected by the pure spatial-domain network like unet [20]. for
instance, a recent dual-encoder network, ynet [6], incorporates a spectral
encoder with fast fourier convolution (ffc) [4] to disentangle global patterns
across varying frequency components and derives hybrid feature representation.
in addition, spectrum learning also exhibits advantageous robustness and
generalization against adversarial attacks, data corruption, and distribution
shifts [19]. in label-efficient learning, some preliminary works have been
proposed to encourage mutual consistency between outputs from two networks [3],
two decoders [25], and teacher-student models [14], yet only in the spatial
domain. as far as we know, spatial-spectral cross-domain consistency has never
been investigated to promote learning with sparse annotations of medical data.
this has motivated us to develop the cross-domain cooperative mutual teaching
scheme to leverage the favorable properties when learning in the spectral
space.besides consistency constraints, utilizing pseudo labels as supplementary
supervision is another principle in label-efficient learning [11,24]. to prevent
the model from being influenced by noise and inaccuracies within the pseudo
labels, numerous studies have endeavored to enhance their quality, including
averaging the model predictions from several iterations [11], filtering out
unreliable pixels [24], and mixing dual-branch outputs [15] followingwhere α is
the random mixing ratio. p 1 , p 2 , and p mix denote the probability maps from
the two spatial decoders and their mixture. these approaches only operate in the
spatial domain, regardless of single or dual branches, while we consider both
spatial and spectral domains and propose to adaptively merge dual-branch outputs
with respective pixel-wise entropy guidance.
spatial-spectral cross-domain mutual teaching. in contrast to prior
weakly-supervised learning methods that have merely emphasized spatial
considerations, our approach designs a dual-branch structure consisting of a
spatial branch f spa (x, θ spa ) and a spectral branch f spe (x, θ spe ), with x
and θ being the input image and randomly initialized model parameters. as
illustrated in fig. 1, the spatial and spectral branches take the same training
image as the input and extract domain-specific patterns. the raw model outputs,
i.e., the logits l spa and l spe , will be converted to probability maps p spa
and p spe with softmax normalization, and further to respective pseudo labels
ŷspa and ŷspe by ŷ = arg max p. the spatial and spectral pseudo labels supervise
the other branch collaboratively during mutual teaching and can be expressed
aswhere "→" denotes supervision1 . through cross-domain engagement, these two
branches complement each other, with each providing valuable domain-specific
insights and feedback to the other. consequently, such a scheme can lead to
better feature extraction, more meaningful data representation, and
domain-specific knowledge transmission, thus boosting model generalization and
robustness.entropy-guided pseudo label ensemble learning. in addition to mutual
teaching, we consider aggregating the pseudo labels from the spatial and
spectral branches in ensemble learning, aiming to take advantage of the
distinctive yet complementary properties of the cross-domain features. as we
know, a pixel characterized by a higher entropy value indicates elevated
uncertainty in terms of its corresponding prediction. we can observe from the
entropy maps h spa and h spe in fig. 1 that the pixels of the polyp boundary
exhibit greater difficulties in accurate segmentation, presenting with higher
entropy values (the white contours). considering such property, we propose a
novel adaptive strategy to automatically adjust the mixing ratio for each pixel
based on the entropy of its categorical probability distribution. hence, the
mixed pseudo labels are more reliable and beneficial for ensemble learning.
concretely, with the spatial and spectral probability maps p spa and p spe , the
corresponding entropy maps h spa and h spe can be computed withwhere c is the
number of classes that equals 2 in our task. unlike previous image-level
fixed-ratio mixing or random mixing as eq. ( 1), we can update the mixing ratio
between the two probability maps p spa and p spe with the weighted entropy
guidance at each pixel location bywhere "⊗" denotes pixel-wise multiplication. p
s 2 is the merged probability map and can be further converted to the pseudo
label by ŷs 2 = arg max p s 2 to supervise the spatial and spectral branch in
the context of ensemble learning following ŷs 2 → f spa and ŷs 2 → f spe .(by
absorbing strengths from the spatial and spectral branches, ensemble learning
from the mixed pseudo labels facilitates model optimization with reduced
overfitting, increased stability, and improved generalization and
robustness.hybrid loss supervision from scribbles and pseudo labels. besides the
scribble annotations for partial pixels, the aforementioned three types of
pseudo labels ŷspa , ŷspe , and ŷs 2 can offer complementary supervision for
every pixel, with different learning regimes. overall, our hybrid loss
supervision is based on cross entropy loss ce and dice loss dice . specifically,
we employ the partial cross entropy loss [13] pce , which only calculates the
loss on the labeled pixels, for learning from scribbles followingwhere y denotes
the scribble annotations. furthermore, the mutual teaching loss with supervision
from domain-specific pseudo labels is . (8) holistically, our hybrid loss
supervision can be stated aswhere λ mt and λ el serve as weighting coefficients
that regulate the relative significance of various modes of supervision. the
hybrid loss considers all possible supervision signals in the spatial-spectral
dual-branch network and exceeds partial combinations of its constituent
elements, as evidenced in the ablation study.
datasets. we employ the sun-seg [10] dataset with scribble annotations for
training and assessing the in-distribution performance. this dataset is based on
the sun database [16], which contains 100 different polyp video cases. to reduce
data redundancy and memory consumption, we choose the first of every five
consecutive frames in each case. we then randomly split the data into 70, 10,
and 20 cases for training, validation, and testing, leaving 6677, 1240, and 1993
frames in the respective split. for out-of-distribution evaluation, we utilize
three public datasets, namely kvasir-seg [9], cvc-clinicdb [2], and polypgen [1]
with 1000, 612, and 1537 polyp frames, respectively. these datasets are
collected from diversified patients in multiple medical centers with various
data acquisition systems. varying data shifts and corruption like motion blur
and specular reflections2 pose significant challenges to model generalization
and robustness.implementation details. we implement our method with pytorch [18]
and run the experiments on a single nvidia rtx3090 gpu. the sgd optimizer is
utilized for training 30k iterations with a momentum of 0.9, a weight decay of
0.0001, and a batch size of 16. the execution time for each experiment is
approximately 4 h. the initial learning rate is 0.03 and updated with the
polyscheduling policy [15]. the loss weighting coefficients λ mt and λ el are
empirically set the same and exponentially ramped up [3] from 0 to 5 in 25k
iterations. all the images are randomly cropped at the border with maximally 7
pixels and resized to 224×224 in width and height. besides, random horizontal
and vertical flipping are applied with a probability of 0.5, respectively. we
utilize unet [20] and ynet [6] as the respective segmentation model in the
spatial and spectral branches. the performance of the scribble-supervised model
with partial cross entropy [13] loss (scrib-pce) and the fully-supervised model
with cross entropy loss (fully-ce) are treated as the lower and upper bound,
respectively. five classical and relevant methods, including entmin [7], gcrf
[17], ustm [14], cps [3], and dmpls [15] are employed as the comparative
baselines and implemented with unet [20] as the segmentation backbone referring
to the wsl4mis3 repository. for a fair comparison, the output from the spatial
branch is taken as the final prediction and utilized in evaluation without
post-processing. in addition, statistical evaluations are conducted with
multiple seeds, and the mean and standard deviations of the results are
reported. [17] 0.656±0.019 0.541±0.022 0.690±0.017 4.983±0.089 ustm [14]
0.654±0.008 0.533±0.009 0.663±0.011 5.207±0.138 cps [3] 0.658±0.004 0.539±0.005
0.676±0.005 5.092±0.063 dmpls [15] 0.656±0.006 0.539±0.005 0.659±0.011
5.208±0.061 s 2 me (ours) 0.674±0.003 0.565±0.001 0.719±0.003 4.583±0.014
fully-ce 0.713±0.021 0.617±0.023 0.746±0.027 4.405±0.119the performance of
weakly-supervised methods is assessed with four metrics,i.e., dice similarity
coefficient (dsc), intersection over union (iou), precision (prec), and a
distance-based measure of hausdorff distance (hd). as shown in table 1 and fig.
2, our s 2 me achieves superior in-distribution performance quantitatively and
qualitatively compared with other baselines on the sun-seg [10] dataset.
regarding generalization and robustness, as indicated in table 2, our method
outperforms other weakly-supervised methods by a significant margin on three
unseen datasets, and even exceeds the fully-supervised upper bound on two of
them 4 . these results suggest the efficacy and reliability of the proposed
solution s 2 me in fulfilling polyp segmentation tasks with only scribble
annotations. notably, the encouraging performance on unseen datasets exhibits
promising clinical implications in deploying our method to real-world scenarios.
network structures. we first conduct the ablation analysis on the network
components. as shown in table 3, the spatial-spectral configuration of our s 2
me yields superior performance compared to single-domain counterparts with me,
confirming the significance of utilizing cross-domain features. pseudo label
fusion strategies. to ensure the reliability of the mixed pseudo labels for
ensemble learning, we present the pixel-level adaptive fusion strategy according
to entropy maps of dual predictions to balance the strengths and weaknesses of
spatial and spectral branches. as demonstrated in table 4, our method achieves
improved performance compared to two image-level fusion strategies, i.e., random
[15] and equal mixing.hybrid loss supervision. we decompose the proposed hybrid
loss l hybrid in eq. ( 9) to demonstrate the effectiveness of holistic
supervision from scribbles,
to our best knowledge, we propose the first spatial-spectral dual-branch network
structure for weakly-supervised medical image segmentation that efficiently
leverages cross-domain patterns with collaborative mutual teaching and ensemble
learning. our pixel-level entropy-guided fusion strategy advances the
reliability of the aggregated pseudo labels, which provides valuable
supplementary supervision signals. moreover, we optimize the segmentation model
with the hybrid mode of loss supervision from scribbles and pseudo labels in a
holistic manner and witness improved outcomes. with extensive in-domain and
out-ofdomain evaluation on four public datasets, our method shows superior
accuracy, generalization, and robustness, indicating its clinical significance
in alleviating data-related issues such as data shift and corruption which are
commonly encountered in the medical field. future efforts can be paid to apply
our approach to other annotation-efficient learning contexts like
semi-supervised learning, other sparse annotations like points, and more medical
applications.
lmt = { ce (lspa, ŷspe)+ dice(pspa, ŷspe)} ŷspe→fspa + { ce (lspe, ŷspa)+
dice(pspe, ŷspa)} ŷspa→fspe .(7)
. this work was supported by hong kong research grants council (rgc)
collaborative research fund (crf c4063-18g), the shun hing institute of advanced
engineering (shiae project bme-p1-21) at the chinese university of hong kong
(cuhk), general research fund (grf 14203323), shenzhen-hong kong-macau
technology research programme (type c) stic grant sgdx20210823103535014
(202108233000303), and (grs) #3110167.
endoscopy is an important medical procedure with many applications, from routine
screening to detection of early signs of cancer and minimally invasive
treatment. automatic analysis and understanding of these videos raises many
opportunities for novel assistive and automatization tasks on endoscopy
procedures. obtaining 3d models from the intracorporeal scenes captured in
endoscopies is an essential step to enable these novel tasks and build
applications, for example, for improved monitoring of existing patients or
augmented reality during training or real explorations.3d reconstruction
strategies have been studied for long, and one crucial step in these strategies
is feature detection and matching which serves as input for structure from
motion (sfm) pipelines. endoscopic images are a challenging case for feature
detection and matching, due to several well known challenges for these tasks,
such as lack of texture, or the presence of frequent artifacts, like specular
reflections. these problems are accentuated when all the elements in the scene
are deformable, as it is the case in most endoscopy scenarios, and in particular
in the real use case studied in our work, the lower gastrointestinal tract
explored with colonoscopies. existing 3d reconstruction pipelines are able to
build small 3d models out of short clips from real and complete recordings [1].
one of the current bottle-necks to obtain better 3d models is the lack of more
abundant and higher quality correspondences in real data.this work introduces
superpoint-e, a new model to extract interest points from endoscopic images. we
build on the well known superpoint architecture [5], a seminal work that
delivers state-of-the-art results when coupled with downstream tasks1 . our main
contribution is a novel supervision strategy to train the model. we propose to
automatically generate reliable training data from video sequences by tracking
feature points from existing detection methods, which do not require training.
we select good features with the colmap sfm pipeline [21], generating training
examples with feature points that can be tracked across several images according
to colmap result. when used to train superpoint, our approach yields a
self-supervised method outperforming current ones.
3d reconstruction is an open problem for laparoscopic and endoscopic settings
[14] of high interest for the community. this idea is supported for example by
recent efforts on collecting new public dataset, to further advance in this
field, such as endoscopic recordings from endoslam [16] and endomapper [1]
datasets. earlier works like grasa et al. [7] have evaluated the performance of
modern slam approaches on endoscopic sequences. mahmoud et al. [13] improved the
performance of such methods in laparoscopic sequences. more recent approaches
attempt to tackle specific endoscopy challenges, such as the deformation [18] or
the artifacts due to specular reflections in the feature extraction step
[2].well known sfm and slam pipelines rely on accurate and robust feature
extraction methods. colmap [21,22], a public sfm tool, uses sift [11] features
while orb-slam [15] extracts orb [19] features because of their efficiency. both
these feature extraction methods count with classical, hand-crafted descriptors
that allowed to build such complex applications. however, transferring that
performance to endoscopy settings remains a difficult task due to several
challenges. artifacts or the lack of texture result in low amount of
correspondences along real endoscopy videos, what motivates the need for
improved strategies.deep learning methods for feature extraction and matching is
a very active research field. the survey ma et al. [12] shows the introduction
of deep learning methods to feature detection and matching. notable mentions are
superpoint [5] for its self-supervised approach, r2d2 [17] for using reliability
metrics as output of the network instead of the features themselves and d2-net
[6] that built a describe-and-detect strategy that aims to improve sfm
applicability. exporting this progress to the matching stage, disk [24] proposes
a formulation of the problem to optimize in an end-to-end manner. other recent
works have extended the networks to take advantage of the advances in attention
for the matching task, as in superglue [20] and loftr [23].in this work we
improve the performance of superpoint [5] on endoscopy images. we chose
superpoint because it is a seminal work that has inspired many follow up works,
and it is still among the top performers on current feature matching challenges
[10]. similar to detone et al. [4], we explore improvements on feature
extraction that provide good properties for downstream tasks. they design an
end-to-end method to optimize the visual odometry computed with their features.
differently, we propose to supervise our training with points that have been
successfully used for 3d reconstruction using existing sfm pipelines. with this
supervision, we train a model able to extract more features with good properties
for sfm algorithms, e.g., being spread and out of large specularities.
superpoint supervision is referred to as homographic adaptation and assumes that
the surfaces are locally plane, which is not the case in our data. instead, we
propose to use 3d reconstructions of points tracked along image sequences. this
makes no assumptions about the local surface shapes and we will show in sect. 4
that this yields a better trained network. we will refer to this as tracking
adaptation and we will here describe how we obtain the tracks.
we generate examples of good features by identifying features that were
successfully reconstructed with existing methods for each sequence in our
training set. our training set contains short sequences (4-7 s) from the
complete colonoscopy recordings in endomapper dataset where colmap software was
able to obtain a 3d reconstruction. this is a very challenging domain, and
existing sfm pipelines fail in longer videos.
we generate 3d reconstructions for all our training sequences with
out-of-the-box colmap. in particular, we use the following blocks: feature
extractor, exhaustive matcher and mapper. configuration parameters are detailed
in the supplementary materials. we turn on the "guided matching" option for the
exhaustive matcher module to find the best matches possible. we additionally
compute the 3d reconstruction for the same sequences with a modified colmap
pipeline that uses the official super-point and superglue2 implementation with
the indoor set of weights. all the parameters are left as default except for the
keypoint threshold= 0.015 and the nms radius= 1. after providing the superglue
resulting matches to colmap, we execute only the mapper module with the same
configuration as before. re-project good features to the training set frames. a
successful 3d reconstruction includes the computed positions of the cameras that
took the images and a point cloud with 3d coordinates of the triangulated
points. we use the camera poses, the points' coordinates and the camera
calibration parameters to reproject the 3d point cloud points into every image.
not all points were originally detected and triangulated at all frames, so we
establish two types of reprojected points. if they were "originally" detected
and matched in a particular image, we set them to green. otherwise, we set them
to blue (see fig. 1).for supervision, we only use reprojected points that fall
within a reliable track. a reliable track is an interval bounded by green
points. so, the reprojected points selected for training are either green or
have preceding and subsequent green points along its track.the different
appearances of the same 3d point in different frames of the track are our
correspondences for training our models. figure 1 contains examples of
reprojected points and an example of a reliable track.deep feature extraction
for endoscopy. superpoint uses a fully-convolutional network as backbone and
learns to extract good features using homographic adaptation: extracting
features that are robust to homographic deformations. it achieves this by using
as supervision y the average detections over several random homographic
deformations of the same image. the feature extraction network then is run on an
image i and a warped version i of it with a new homography. the network
optimizes the loss functionwhere x and d are the detection and description
heads' outputs, respectively. y is the supervision for the detection. s is the
correspondence between i and i computed from the homography. l p is the
detection loss that measures the discrepancies between the supervision y and the
detection head's output x . λ = 1 is a weighting parameter. l d is the
description loss that measures the discrepancies between both description head's
outputs d and d using s.using our new supervision from sfm in the form of tracks
of points, we propose a new loss to train superpoint that is more aligned with
our goal, called tracking adaptation. instead of an image i and a warped version
i , we use different images i a and i b from the same sequence. the supervision
y for the detection in this case is the set of points that have been reprojected
on i a and i b from the 3d reconstruction. the detection loss l p is calculated
as in the original superpoint. we replace the description loss l d for a new
tracking losswhere d a and d b are the description head's outputs for i a and i
b , respectively. t is the set of all the tracks that appear in both images. l t
is a common triplet loss that measures the distance between positive pairs
(weighting parameter λ t = 1 and positive margin m p = 1) and the distance
between negative pairs (negative margin m n = 0.2). two descriptors from
different images d ai and d bj are a positive pair if they belong to the same
track (i = j), and negative pair otherwise (i = j).
the following experiments demonstrate the proposed feature detection efficacy to
obtain 3d models on real colonoscopy videos, comparing different variations of
our approach and relevant baseline methods.dataset. we seek techniques that are
applicable to real medical data, so we train and evaluate with subsequences from
the endomapper dataset [1], which contains a hundred complete endoscopy
recordings obtained during regular medical practice. we use colmap 3d
reconstructions obtained from subsequences from this dataset (11260 frames from
65 reconstructions obtained along 14 different videos for training, and 838
frames from 7 reconstructions from 6 different videos for testing). the exact
details are in the supplementary material.baselines and our variations. we use
colmap as our first baseline. it uses sift features and a standard guided
matching algorithm to produce very accurate camera pose estimates. we also
include as baseline the results of superpoint (sp) with superglue matches and
the colmap reconstruction module. the configuration for both baselines is the
same as detailed in sect. 3. we evaluate different variations of the original
superpoint. all models were trained with a modification of a pytorch
implementation of superpoint [9]. training parameters in supplementary material.
the models differ in the supervision used and the loss applied in the training,
as detailed in the first four columns of table 1. ablation study. table 1 (last
five columns) summarizes the performance of our approach variations. we run all
the models on the test set subsequences to extract points. matches between the
points in two images are obtained with bi-directional nearest neighbor algorithm
with l2 distance. points and matches are given to colmap and the mapper module
(configuration in supplementary material) attempts to generate a 3d
reconstruction. the reconstruction quality statistics used to illustrate the
performance of each detector are:-3dim : fraction of images from the subsequence
successfully introduced in the reconstruction. the closer to 100% the better.
-3dp ts : number of points that were successfully reconstructed. the more points
the better, since it means a denser coverage of the scene. -err: mean
reprojection error of the 3d points after being reprojected onto the images of
the subsequence. -err-10k: mean reprojection error of the best 10000 points of
the reconstruction. since all reconstructions have outliers that skew the
average, this metric is more representative of the performance of the
models.-len(tr): mean track length represents the average number of images where
a point is being consecutively matched, tracked.sp-e v2 (sp-e moving forward) is
our best variation, with the highest amount of reconstructed points and the
lowest reprojection error for top 10000.
comparison. this experiment compares the performance of the considered baselines
against the best configuration of our feature extraction model. table 2 contains
a summary of the results. in most metrics we observe a significant improvement
using sp-e compared to the others. for example, the number of points at the
final reconstruction is more than three times higher (see the example in fig.
2). the mean reprojection error of all the points is the lowest for sift,
possibly due to it being more restrictive in all other aspects (number of images
reconstructed, number of points, track length). however, the mean error for the
top 10000 points is always lower for sp-e. the reprojection error plots in figs.
3 and4 provide more insight on this metric. figures 3 and4 show a more detailed
visualization of two representative reconstructions, including a summary of the
sequence frames, the point cloud obtained by each method and a plot of the
reprojection error for each point in the reconstruction, sorted in increasing
error value. note that even though sp-e obtains many more points, it is not at
the cost of quality. figure 3 shows a scenario where sift fails to reconstruct a
large part of the subsequence, because it fails on the feature matching on the
darker frames depicted in the middle of the sequence. note how the
reconstruction from sp-e is notably denser than the others. figure 4 shows a
scenario where all approaches perform well and sift achieves the lowest
reprojection error. we analyze additional aspects of our detected features to
showcase the higher quality with respect to other methods in table 3. to measure
the spread of the features over the images we defined a 16 × 16 grid over each
image and computed the percentage of those cells that have at least one
reconstructed point. we also measure how many extracted points fall on top of
specularities (we consider a pixel as part of a specularity if its intensity is
higher than 180). for both metrics, our detector achieves significantly better
results, showcasing the better properties of our detector for 3d
reconstruction.to provide quantitative evaluation of the camera motion
estimation, we use a simulated dataset [3] to have ground truth available for
the camera trajectory. we took 5 sequences of 100-150 frames from this dataset,
and we tested the baselines and our model. we align the ground truth
trajectories with the reconstructed ones with horn's method [8]. sp only
reconstructed 3 out of the 5 sequences while sift and sp-e correctly
reconstructed the 5 sequences, with an average rmse of 4.61mm and 4.71 mm
respectively. simulated data lacks some of the biggest challenges of endoscopy
images (e.g. specularities, deformations), but this experiment suggests that the
camera motion estimation quality is similarly good for all methods when they
manage to converge.
this work presents a novel training strategy for superpoint to improve its
performance in sfm from endoscopy images. this strategy has two main benefits:
we show how to use 3d reconstructions of endoscopy sequences as supervision to
train feature extraction models; and we design a new tracking loss to perform
tracking adaptation using this supervision. the benefits of our method are
explored with an ablation study and against established baselines on sfm and
feature extraction. our proposed model is able to obtain more suitable features
for 3d reconstruction, and to reconstruct larger sets of images with much denser
point clouds.
point (base point detector): sp-o: original superpoint detector; sf*/sp*:
sift/sp points that were successfully reconstructed after the colmap
optimization, reprojected in each video frame. match (matches supervision): h:
homography based, i.e., homographic adaptation from original superpoint work;
tr: the proposed tracking adaptation. loss (loss used for training): sp:
original superpoint training loss; tr-2 or tr-n: track -based loss. tr-2 means
that the loss is computed for every pair of images in the track. tr-n means we
optimize simultaneously n views of the track (n=4 in our experiments).
+ total number of images in the subsequence. * if 10k points are not available,
average is computed over all available reconstructed points.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0 56.
semantic segmentation plays a vital role in pathological image analysis. it can
help people conduct cell counting, cell morphology analysis, and tissue
analysis, which reduces human labor [19]. however, data acquisition for medical
images poses unique challenges due to privacy concerns and the high cost of
manual annotation. moreover, pathological images from different tissues or
cancer types often show significant domain shifts, which hamper the
generalization of models trained on one dataset to others. due to the
abovementioned challenges, some researchers have proposed various white-box
domain adaptation methods to address these issues.recently, [8,16] propose to
use generative adversarial networks to align the distributions of source and
target domains and generate source-domain lookalike outputs for target images.
source-free domain adaptation methods have been also widely investigated due to
the privacy protection. [3,5,14] explore how to implicitly align target domain
data with the model trained on the source domain without accessing the source
domain data. there are also many studies on multi-source white-box domain
adaptation. ahmed et al. [1] propose a novel algorithm which automatically
identifies the optimal blend of source models to generate the target model by
optimizing a specifically designed unsupervised loss. li et al. [13] extend the
above work to semantic segmentation and proposed a method named model-invariant
feature learning, which takes full advantage of the diverse characteristics of
the source-domain models.nonetheless, several recent investigations have
demonstrated that the domain adaptation methods for source-free white-box models
still present a privacy risk due to the potential leakage of model parameters
[4]. such privacy breaches may detrimental to the privacy protection policies of
hospitals. moreover, the target domain uses the same neural network as the
source domain, which is not desirable for low-resource target users like
hospitals [15]. we thus present a more challenging task of relying solely on
black-box models from vendors to avoid parameter leakage. in clinical
applications, various vendors can offer output interfaces for different
pathological images. while black-box models are proficient in specific domains,
their performances greatly degrade when the target domain is updated with new
pathology slices. therefore, how to leverage the existing knowledge of black-box
models to effectively train new models for the target domain without accessing
the source domain data remains a critical challenge.in this paper, we present a
novel source-free domain adaptation framework for cross-tissue cell segmentation
without accessing both source domain data and model parameters, which can
seamlessly integrate heterogeneous models from different source domains into any
cell segmentation network with high generality. to the best of our knowledge,
this is the first study on the exploration of multi-source black-box domain
adaptation for cross-tissue cell segmentation. in this setting, conventional
multi-source ensemble methods are not applicable due to the unavailability of
model parameters, and simply aggregating the black-box outputs would introduce a
considerable amount of noise, which can be detrimental to the training of the
target domain model. therefore, we develop two strategies within this new
framework to address this issue. firstly, we propose a pixel-level multi-source
domain weighting method, which reduces source domain noise by knowledge
weighting. this method effectively addresses two significant challenges
encountered in the analysis of cellular images, namely, the uncertainty in
source domain output and the ambiguity in cell boundary semantics. secondly, we
also take into account the structured information from cells to images, which
may be overlooked during distillation, and design an adaptive knowledge voting
strategy. this strategy enables us to ignore low-confidence regions, similar to
cutout [6], but with selective masking of pixels, which effectively balances the
trade-off between exploiting similarities and preserving differences of
different domains. as a result, we refer to the labels generated through the
voting strategy as pseudo-cutout labels.
overview: figure 1 shows a binary cell segmentation task with three source
models trained on different tissues and a target model, i.e., the student model
in fig. 1. we only use the source models' predictions on the target data for
knowledge transfer without accessing the source data and parameters. the η and η
indicate that different perturbations are added to the target images.
subsequently, we feed the perturbed images into the source domain predictor to
generate the corresponding raw segmentation outputs. these outputs are then
processed by two main components of our framework: a pixel-level weighting
method that takes into account the prediction uncertainty and cell boundary
ambiguity, and an adaptive knowledge voter that utilizes confidence gates and a
dynamic ensemble strategy. these components we designed are to extract reliable
knowledge from the predictions of source domain models and reduce noise during
distillation. finally, we obtain a weighted logit for knowledge distillation
from pixel level and a high-confidence pseudo-cutout label for further
structured distillation from cell to global pathological image.
we denote d n s = {x s , y s } n as a collection of n source domains and d t =
{x i t , y j t } as single target domain, where the number of labeled instances
y j t x i t . we are only provided with black-box models {f n s } n n=1 trained
on multiple source domains {x i s , y i s } n n=1 for knowledge transfer. the
parameters {θ n s } n n=1 of these source domain predictors are not allowed to
participate in gradient backpropagation as a result of the privacy policy. thus,
our ultimate objective is to derive a novel student model f t : x t → y t that
is relevant to the source domain task. accordingly, direct knowledge transfer
using the output of the source domain predictor may lead to feature bias in the
student model due to the unavoidable covariance [20] between the target and
source domains. inspired by [21], we incorporate prediction uncertainty and cell
boundary impurity to establish pixel-level weights for multi-source outputs. we
assume that k-square-neighbors of a pixel as a cell region, i.e., for a logits
map with height h and width w , we define the region as follow:where (i, j)
denotes centre of region, and k denotes the size of k-square-neighbors. firstly,
we develop a pixel-level predictive uncertainty algorithm to aid in assessing
the correlation between multiple source domains and the target domain. for a
given target image x t ∈ x i t , we initially feed it into the source predictors
{f n s } n n=1 to obtain their respective prediction {p n s } n n=1 . to
leverage the rich semantic information from the source domain predictor
predictions, we utilize predictive entropy of the softmax outputs to measure the
prediction uncertainty scores. in the semantic segmentation scenario of
c-classes classification, we define the pixel-level uncertainty score u (i,j) n
as follow:where o n s denotes softmax output,i.e.,o n s = softmax(p n s ) from
nth source predictor.due to the unique characteristics of cell morphology,
merely relying on uncertainty information is insufficient to produce
high-quality ensemble logits map that accurately capture the relevance between
the source and target domains. the target pseudo-label for the nth predictor f n
s can be obtained by applying the softmax function to the output and selecting
the category with the highest probability score, i.e., y t = arg max c∈{1,...,c}
(softmax(p n s )). then according to c-classes classification tasks, we divide
the cell region into c subsets,after that, we determine the degree of impurity
in an area of interest by analyzing the statistics of the boundary region, which
represents the level of semantic information ambiguity. specifically, the number
of different objects within the area is considered a proxy for its impurity
level, with higher counts indicating higher impurity.the boundary impurity p
(i,j) can be calculated as:where | • | denotes the number of pixels in the area.
by assigning lower weights to the pixels with high uncertainty and boundary
ambiguity, we can obtain pixel-level weight scores w n for each p n s ,
i.e.,where denotes element-wise matrix multiplication. according to the
pixellevel weight, we will obtain an ensemble logits map m = n n=1 w n • p n s .
and the object of the knowledge distillation is a classical regularization term
[9]:where d kl denotes the kullback-leibler (kl) divergence loss.adaptive
pseudo-cutout label: as previously mentioned, the outputs from the source domain
black-box predictors have been adjusted by the pixel-level weight. however, they
are still noisy and only pixel-level information is considered while ignoring
structured information in the knowledge distillation process. thus, we utilize
the output of the black-box predictor on the target domain to produce an
adaptive pseudo-cutout label, which will be employed to further regularize the
knowledge distillation process. we have revised the method in [7] to generate
high-quality pseudo labels that resemble the cutout augmentation technique. for
softmax outputs {o n s } n n=1 from n source predictors, we first set a
threshold α to filter low-confidence pixels. to handle pixels with normalized
probability values below the threshold, we employ a cutout-like operation and
discard these pixels. subsequently, we apply an adaptive voting strategy to the
n source domain outputs. initially, during the training of the target model, if
at least one source domain output exceeds the threshold, we consider the pixel
as a positive or negative sample, which facilitates rapid knowledge acquisition
by the model. as the training progresses, we gradually tighten the voting
strategy and only retain regional pixels that have received adequate votes. the
strategy can be summarised as follow:where α is empirically set as 0.9.then we
will aggregate the voting scores, i.e., v (i,j) = n n=1 v (i,j) n and determine
whether to retain each pixel using an adaptive vote gate g ∈ {1, 2, 3, etc.}. by
filtering with a threshold and integrating the voting strategy, we generate
high-confidence pseudo-labels that remain effective even when the source and
target domains exhibit covariance. finally, we define the ensemble result as a
pseudo-cutout label ps and employ consistency regularization as below:where l ce
denotes cross-entropy loss function.loss functions: finally, we incorporate
global structural information about the predicted outcome of the target domain
into both distillation and semisupervised learning. to mitigate the noise effect
of the source domain predictors, we introduce maximize mutual information
targets to facilitate discrete representation learning by the network. we define
e(p) =i p i log p i as conditional entropy. the object can be described as
follow:where the increasing h(y t ) and the decreasing h(y t |x t ) help to
balances class separation and classifier complexity [15]. we adopt the classical
and effective mean-teacher framework as a baseline for semi-supervised learning
and update the teacher model parameters by exponential moving average. also, we
apply two different perturbations (η, η ) to the target domain data and feed
them into the student model and the mean-teacher model respectively. the
consistency loss of unsupervised learning can be defined as below:finally, we
get the overall objective:where l sup denotes the ordinary cross-entropy loss
for supervised learning and we set the weight of each loss function to 1 in the
training.
dataset and setting: we collect four pathology image datasets to validate our
proposed approach. firstly, we acquire 50 images from a cohort of patients with
triple negative breast cancer (tnbc), which is released by naylor et al [18].
hou et al. [10] publish a dataset of nucleus segmentation containing 5,060
segmented slides from 10 tcga cancer types. in this work, we use 98 images from
invasive carcinoma of the breast (brca). we have also included 463 images of
kidney renal clear cell carcinoma (kirc) in our dataset, which are made publicly
available by irshad et al [11]. awan et al. [2] publicly release a dataset
containing tissue slide images and associated clinical data on colorectal cancer
(crc), from which we randomly select 200 patches for our study. in our
experiments, we transfer knowledge from three black-box models trained on
different source domains to a new target domain model (e.g.,from crc, tnbc, kirc
to brca). the backbone network for the student model and source domain black-box
predictors employ the widely adopted residual u-net [12], which is commonly used
for medical image segmentation. for each source domain network, we conduct
full-supervision training on the corresponding source domain data and directly
evaluate its performance on target domain data. the upper performance metrics
(source-only upper) are shown in the table 1. to ensure the reliability of the
results, we use the same data for training, validation, and testing, which
account for 80%, 10%, and 10% of the original data respectively. for the target
domain network, we use unsupervised and semi-supervised as our task settings
respectively. in semi-supervised domain adaptation, we only use 10% of the
target domain data as labeled data.experimental results: to validate our method,
we compare it with the following approaches: (1) cellsegssda [8], an adversarial
learning based semisupervised domain adaptation approach. (2) us-msma [13], a
multi-source model domain aggregation network. (3) sfda-dpl [5], a source-free
unsupervised domain adaptation approach. (4) bbuda [17], an unsupervised
black-box model domain adaptation framework. a point worth noting is that most
of the methods we compared with are white-box methods, which means they can
obtain more information from the source domain than us. for single-source domain
adaptation approach, cellsegssda and sfda-dpl, we employ two strategies to
ensure the fairness of the experiments: (1) single-source, i.e. performing
adaptation on each single source, where we select the best results to display in
the table 1; (2) source-combined, i.e. all source domains are combined into a
traditional single source. the table 1 and fig. 2 demonstrate that our proposed
method exhibits superior performance, even when compared to these white-box
methods, surpassing them in various evaluation metrics and visualization
results.in addition, the experimental results also show that simply combining
multiple source data into a traditional single source will result in performance
degradation in some cases, which also proves the importance of studying
multi-source domain adaptation methods.ablation study: to evaluate the impact of
our proposed methods of weighted logits(wl), pseudo-cutout label(pcl) and
maximize mutual information(mmi) on the model performance, we conduct an
ablation study. we compare the baseline model with the models that added these
three methods separately. we chose crc, kirc and brca as our source domains, and
tnbc as our target domain. the results of these experiments, presented in the
table 2, show that our proposed modules are indeed useful.
our proposed multi-source black-box domain adaptation method achieves
competitive performance by solely relying on the source domain outputs, without
the need for access to the source domain data or models, thus avoiding
information leakage from the source domain. additionally, the method does not
assume the same architecture across domains, allowing us to learn lightweight
target models from large source models, improving learning efficiency. we
demonstrate the effectiveness of our method on multiple public datasets and
believe it can be readily applied to other domains and adaptation scenarios.
moving forward, we plan to integrate our approach with active learning methods
to enhance annotation efficiency in the semi-supervised setting. by leveraging
multi-source domain knowledge, we aim to improve the reliability of the target
model and enable more efficient annotation for better model performance.
in recent years, deep learning (dl) methods have demonstrated remarkable
performance in detecting and localizing tumors on ultrasound images [2,27].
compared with conventional image processing methods, dl methods provide an
accurate feature extraction capability on ultrasound images, despite their low
resolution and noise disturbance, leading to superior segmentation accuracy
[2,5,14]. however, there are some limitations in developing a dl model in a
source domain and deploying it in an unseen target domain. the primary
limitation is that dl models require a large number of training samples to
achieve accurate predictions [8,24]. yet, acquiring large training datasets and
their corresponding labels, especially from a cohort of patients, can be costly
or even infeasible, which poses a significant challenge in developing a dl model
with high performance [7]. second, even when large-scale datasets are available
through collaborative research from multiple sites, dl models trained on such
datasets may yield sub-optimal solutions due to domain gaps caused by
differences in images acquired from different sites [20]. third, due to the
small number of datasets from each domain, the images for each individual domain
may not capture representative features, limiting the ability of dl models to
generalize across domains [3].domain adaptation (da) has been extensively
studied to alleviate the aforementioned limitations, the goal of which is to
reduce the domain gap caused by the diversity of datasets from different domains
[12,20,26,29,33]. example solutions include transfer learning-and style
transfer-based methods. nonetheless, unlike natural images, generating labels
can be a challenging task, making it difficult to apply general da methods; thus
bridging domain gaps by da methods remains limited [26,33]. this is due to
sensitive privacy issues in patients' data, particularly in collaborative
research, which restricts access to labels from different domains. as a result,
conventional da methods cannot be easily applied [10]. more recently,
unsupervised domain adaptation (uda) has been introduced to address this issue
[16,33], aiming to generate semi-predictions (pseudo-labels) in target domains
first, followed by producing accurate predictions using the pseudo-labels. one
critical limitation of pseudo-label-based uda is the possibility of error
accumulation due to mispredicted pseudo-labels. this can lead to significant
degradation of the performance of dl models, as errors can compound and become
more pronounced over time [17,25].to alleviate the problem of pseudo-label-based
uda, in this work, we propose an advanced uda framework based on self-supervised
da with a test-time finetuning network. test-time adaptation methods have been
developed [4,11,13,23] to improve the learning of knowledge in target domains.
the distinctive feature of our test-time self-supervised da is that it enables
the dl network (i) to learn knowledge about the features of target domains by
fine-tuning the network itself during the test-time phase, rather than
generating pseudo-labels and then (ii) to provide precise predictions on images
in target domains, by using the fine-tuned network. specifically, we adopt
self-supervised learning and verify the model via thorough mathematical
analysis. our framework was tested on the task of breast cancer segmentation in
ultrasound images, but it could also be applied to other lesion segmentation
tasks.to summarize, our contributions are three-fold:• we design a
self-supervised da framework that includes a parameter search method and provide
a mathematical justification for it. with our framework, we are able to identify
the best-performing parameters that result in improved performance in da tasks.
• our framework is effective at preserving privacy, since it carries out da
using only pre-trained network parameters, without transferring any patient
data. • we applied our framework to the task of segmenting breast cancer from
ultrasound imaging data, demonstrating its superior performance over competing
uda methods.our results indicate that our framework is effective in improving
the accuracy of breast cancer segmentation from ultrasound images, which could
have potential implications for improving the diagnosis and treatment of breast
cancer. sample batches of (t, ?) ∼ t
return ŷ 15: end output: predictions ( ŷ) on t fig. 1. architecture of our ttft
network (left) and its pipeline (right).
network architecture. our proposed ttft network is based on selfsupervised da
[31], which is a part of uda and can be seen as multi-task learning, involving
both the main and pretext tasks, as shown in fig. 1. in the main task, an
encoder (e), a decoder for segmentation (d seg ), and a segmentation header (h)
are included. the main task is the segmentation task,in predicting segmentation
labels in the target domain (t ), d ft is also involved in the main task, and
the final prediction after the fine-tuning is where l bce and l gan represent
the loss functions for binary cross-entropy and generative adversarial network
[6], respectively. θ m s includes e s , d s seg , and h s , while θ p s includes
e s , d s gen , and c s . additionally, d s seg = d s gen .fine-tuning in target
domain. since the pre-trained model is likely to produce imprecise predictions
in t , the model should learn domain knowledge about t . to this end, in the
pretext task, for self-supervised learning, the model is fine-tuned in t to
generate synthetic images identical to the input images as below:where only d
gen is fine-tuned to achieve memory efficiency and to decrease the fine-tuning
time, and d s gen is fine-tuned as d s→t gen . then, d s→t gen is transferred to
d ft , and knowledge distillation via self-supervised learning is realized.
hence, the precise predictions in t could be provided bybenefits of our
dual-pipeline. due to the symmetric property of mutual information in
information entropy (h), we haveas a result, the predictions made by the
fine-tuned network in the target domain (t ) lead to reduced entropy, as shown
below:(since d s seg is fully optimized for s in a supervised manner, it
guarantees a baseline segmentation performance. furthermore, since d t ft is
fine-tuned in t
since the loss function and its values can vary based on the distribution of
inputs, and different domains can have different distributions, the local
minimum identified in the source domain (s) cannot be considered as the same
local minimum in t , as illustrated in fig. 2. the y-axis of fig. 2
x l(m (x; θ), x), and the local minimum is different in s and t as θ s in fig.
2a andθ t in fig. 2c, respectively. a longer fine-tuning time is required to
re-position θ s to θ t as in fig. 2c than to re-position θ t to θ t . therefore,
efficient fine-tuning is necessary to re-position the local minimum in fig. 2b
and this process is known as parameter fluctuation. note that the parameter
fluctuation is followed by the fine-tuning step. suppose c i be the i th
convolution operator in d seg with weight w i , thenseg provides the baseline
segmentation performance, d t f t should provide similar feature maps to achieve
the baseline performance. to this end, the mid-feature maps generated should be
similar, i.e.,where c i represents the convolution in d t f t , f i represents i
th feature map, andwhich can be expressed as:here, we denote w i -w i = f i as
the fluctuation vector in the vector space, and the condition f i = 0 indicates
that the sum of the fluctuation vectors should be zero under the condition of |f
i | < r 1. hence, we achieve the condition for the parameter fluctuation that
the centers of parameters of θ s and θ t should be the same in the vector space,
and the length of the fluctuation vector should be less than a certain small
threshold (0 < r 1). therefore, the parameter fluctuation aims to add random
vectors of which length is less than 0 < r 1 on the parameters of θ s , and the
sum of vectors should be zero. to summarize, the parameter fluctuation aims to
add randomness on θ s as follows:(5)3 experiments
to evaluate the segmentation performance of our ttft framework, we used three
different ultrasound databases: bus [32], busi [1], and buv [18], which are
considered to be different domains. all three databases contain ultrasound
imaging data and segmentation masks for breast cancer, with the masks labeled as
0 (background) and 1 (lesion) using a one-hot encoding. the bus database
consists of 163 images along with corresponding labels. the busi database
contains 780 images, with 133 images belonging to the normal class and having
labels containing only 0 values. the buv database originally consists of
ultrasound videos, providing a total of 21,702 frames. while the database also
provides labels for the detection task, we processed these labels as
segmentation masks using a region growing method [15]. we employed different
deep-learning models for evaluation. specifically, u-net [22] and fusionnet [21]
were employed as our baseline models, since u-net is a widely used basic model
for segmentation, and fusionnet contains advanced residual modules, compared
with u-net. ours i and ours ii were based on u-net and fusionnet as the baseline
network, respectively. additionally, mib-net [28], which is a state-of-the-art
model for breast cancer segmentation using ultrasound images, was employed for
comparison. furthermore, cbst [33] and ct-net [16] were employed as the
comparison models for uda methods. as the evaluation metrics, dice coefficient
(d. coef), prauc, which is an area under a precision-recall curve, and cohen
kappa (κ) were employed [30]. our experimental set-ups included: (i) individual
databases were used to assess the baseline segmentation performance (appendix);
(ii) the domain adaptive segmentation performance was assessed using the three
databases, where two databases were regarded as the source domain, and the
remaining database was regarded as the target domain; and (iii) the ablation
study was carried out to evaluate the proposed network architecture along with
the randomized re-initialization method.
since all compared dl models show similar d. coef, only uda performance is
comparable as a control in our experiments. in this experiment, two databases
were used for training, and the remaining database was used for testing. for
instance, bus in fig. 3 illustrates the bu s database was used for testing, and
the other two databases of busi and buv were used for training. figs. 3 and4
show quantitative results, and fig. 5 shows the sample segmentation results.
unlike the experiment using the individual database, u-net, fusionnet, and
mib-net showed significantly inferior scores due to domain gaps. in contrast,
uda methods of cbst and ct-net showed superior scores, compared with others, and
the scores were not strongly reduced, compared with the experiment with the
single database. note that, our ttft framework achieved the best performance
compared with other dl models. additionally, ours ii, based on fusionnet, showed
the best scores, potentially due to the advanced residual connection module.
furthermore, as illustrated in fig 4, our framework provides superior precision
scores in a long range of (0, 0.7), indicating that our frameworks estimated
unnecessary mispredictions but precise predictions on cancer.
in this work, we proposed a dl-based segmentation framework for multi-domain
breast cancer segmentation on ultrasound images. due to the low resolution of
ultrasound images, manual segmentation of breast cancer is challenging even for
expert clinicians, resulting in a sparse number of labeled data. to address this
issue, we introduced a novel self-supervised da network for breast cancer
segmentation in ultrasound images. in particular, we proposed a test-time
finetuning network to learn domain-specific knowledge via knowledge distillation
by self-supervised learning. since uda is susceptible to error accumulation due
to imprecise pseudo-labels, which can lead to degraded performance, we employed
a self-supervised learning-based pretext task. specifically, we utilized an
autoencoder-based network architecture to generate synthetic images that matched
the input images. moreover, we introduced a randomized re-initialization module
that injects randomness into network parameters to reposition the network from
the local minimum in the source domain to a local minimum that is better suited
for the target domain. this approach enabled our framework to efficiently
fine-tune the network in the target domain and achieve better segmentation
performance. experimental results, carried out with three ultrasound databases
from different domains, demonstrated the superior segmentation performance of
our framework over other competing methods. additionally, our framework is
well-suited to a scenario in which access to source domain data is limited, due
to data privacy protocols. it is worth noting that we used vanilla u-net [22]
and fusionnet [21] as baseline models to evaluate the basic performance of our
ttft framework. however, the use of more advanced baseline models could lead to
even better segmentation performance, which is a subject for our future work.
moreover, our proposed framework is not limited to breast cancer segmentation on
ultrasound images acquired from different domains. it can also be applied to
other disease groups or imaging modalities such as mri or ct.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_52.
in order to assess the effectiveness of each of the proposed modules, including
the parameter fluctuation and fine-tuning methods, the ablation study was
carried out. since our framework contains three types of decoders, including d s
seg , d fl seg , and d s→t seg for the fine-tuning, we mainly targeted those
decoders in our ablation study. table 1 illustrates the quantitative results by
different types of decoders. the higher d. coef value (+3.4%) of pre-train + pf
than that of pre-train + random init and pre-train + offset confirms the
effectiveness of the parameter fluctuation in the uda performance. additionally,
the higher score (+11%) of fine-tuning than pre-train shows an outstanding uda
performance of the fine-tuning pipeline. furthermore, the simultaneous
utilization of the dual pipeline with d s seg and d s→t seg is justified by the
scores of pre-train + fine-tuning. using dual-pipeline and parameter fluctuation
yielded the best performance. however, the utilization of ensemble pipelines of
multiple fine-tuning modules was inefficient, since negligible performance
improvements (+0.002) were observed, despite the heavy memory
utilization.furthermore, fig. 6 shows the effectiveness of the parameter
fluctuation and fine-tuning methods. we first compared the similarity of
feature-maps by decoders, including d s seg , d fl seg , and d s→t seg , with d
s seg and d t seg , which was fully optimized decoder in t . here, a style loss
[9] was employed to measure the similarity of feature maps. our in s and t are
plotted with t-sne, where the short distance represents the similar features
[19]. the generated images became similar to t in order of d s seg , d fl seg ,
and d s→t seg , which confirmed the effectiveness of the fine-tuning method in
terms of knowledge distillation. additionally, the parameters were successfully
re-positioned from the local minimum in s by parameter fluctuation, which was
confirmed by the distances from s to d s gen and d fl gen .
positron emission tomography (pet) is a sensitive nuclear imaging technique, and
plays an essential role in early disease diagnosis, such as cancers and
alzheimer's disease [8]. however, acquiring high-quality pet images requires
injecting a sufficient dose (standard dose) of radionuclides into the human
body, which poses unacceptable radiation hazards for pregnant women and infants
even following the as low as reasonably achievable (alara) principle [19]. to
reduce the radiation hazards, besides upgrading imaging hardware, designing
advanced pet enhancement algorithms for improving the quality of low-dose pet
(lpet) images to standard-dose pet (spet) images is a promising alternative.in
recent years, many enhancement algorithms have been proposed to improve pet
image quality. among the earliest are filtering-based methods such as non-local
mean (nlm) filter [1], block-matching 3d filter [4], bilateral filter [7], and
guided filter [22], which are quite robust but tend to over-smooth images and
suppress the high-frequency details. subsequently, with the development of deep
learning, the end-to-end pet enhancement networks [9,14,21] were proposed and
achieved significant performance improvement. but these supervised methods
relied heavily on the paired lpet and spet data that are rare in actual clinic
due to radiation exposure and involuntary motions (e.g., respiratory and muscle
relaxation). consequently, unsupervised pet enhancement methods such as deep
image prior [3], noise2noise [12,20], and their variants [17] were developed to
overcome this limitation. however, these methods still require lpet to train
models, which contradicts with the fact that only spet scans are conducted in
clinic.fortunately, the recent glowing diffusion model [6] provides us with the
idea for proposing a clinically-applicable pet enhancement approach, whose
training only relies on spet data. generally, the diffusion model consists of
two reversible processes, where the forward diffusion adds noise to a clean
image until it becomes pure noise, while the reverse process removes noise from
pure noise until the clean image is recovered. by combining the mechanics of
diffusion model with the observation that the main differences between lpet and
spet are manifested as levels of noises in the image [11], we can view lpet and
spet as results at different stages in an integrated diffusion process.
therefore, when a diffusion model (trained only on spet) can recover noisy
samples to spet, this model can also recover lpet to spet. however, extending
the diffusion model developed for 2d photographic images to pet enhancement
still faces two problems: a) three-dimensionsal (3d) pet images will
dramatically increase the computational cost of diffusion model; b) pet is the
detail-sensitive images and may be introduced/lost some details during the
procedure of adding/removing noise, which will affect the downstream
diagnosis.taking all into consideration, we propose the spet-only unsupervised
pet enhancement (upete) framework based on the latent diffusion model.
specifically, upete has an encoder-<diffusion model>-decoder structure that
first uses the encoder to compress input the lpet/spet images into latent
representations, then uses the latent diffusion model to learn/estimate the
distribution of spet latent representations, and finally uses the decoder to
recover spet images from the estimated spet latent representations. the keys of
our upete include 1) compressing the 3d pet images into a lower dimensional
space for reducing the computational cost of diffusion model, 2) adopting the
poisson noise, which is the dominant noise in pet imaging [20], to replace the
gaussian noise in the diffusion process for avoiding the introduction of details
that are not existing in pet images, and 3) designing ct-guided cross-attention
to incorporate additional ct images into the inverse process for helping the
recovery of structural details in pet.our work had three main
features/contributions: i) proposing a clinicallyapplicable unsupervised pet
enhancement framework, ii) designing three targeted strategies for improving the
diffusion model, including pet image compression, poisson diffusion, and
ct-guided cross-attention, and iii) achieving better performance than
state-of-the-art methods on the collected pet datasets.
the framework of upete is illustrated in fig. 1. when given an input pet image x
(i.e., spet for training and lpet for testing), x is first compressed into the
latent representation z 0 by the encoder e. subsequently, z 0 is fed into a
latent diffusion model followed by the decoder d to output the expected spet
image x. in addition, a specialized encoder e ct is used to compress the ct
image corresponding to the input pet image into the latent representation z ct ,
which is fed into each denoising network for ct-guided cross-attention. in the
following, we introduce the details of image compression, latent diffusion
model, and implementation.
the conventional diffusion model is computationally-demanding due to its
numerous inverse denoising steps, which severely restricts its application to 3d
pet enhancement. to overcome this limitation, we adopt two strategies including
1) compressing the input image and 2) reducing the diffusion steps (as described
in sect. 2.3).similar to [10,18], we adopt an autoencoder (e and d) to compress
the 3d pet images into a lower dimensional but more compact space. the crucial
aspects of this process is to ensure that the latent representation contains the
necessary and representative information for the input image. to achieve this,
we train the autoencoder by a combination of perceptual loss [24] and
patch-based adversarial loss [5], instead of simple voxel-level loss such as l 2
or l 1 loss. among them, the perceptual loss, designed on a pre-trained 3d
resnet [2], constrains higher-level information such as texture and semantic
content, and the patchbased adversarial loss ensures globally coherent while
remaining locally realistic. let x ∈ r h,w,z denote the input image and z 0 ∈ r
h,w,z,c denote the latent representation. the compression process can be
formulated as x = d(z 0 ) = d(e(x)). in this way, we compress the input image by
a factor of f = h/h = w/w = z/z. the results of spet estimation under different
compression rates f are provided in the supplement.
after compressing the input pet image, its latent representation is fed into the
latent diffusion model, which is the key to achieving the spet-only unsupervised
pet enhancement. as described above, the lpet can be viewed as noisy spet (even
in the compressed space), so the diffusion process from spet to pure noise
actually covers the situations of lpet. that is, the diffusion model trained
with spet is capable of estimating spet from the noisy sample (diffused from
lpet). but the diffusion model is developed from photographic images, which have
significant difference with the detail-sensitive pet images. to improve its
applicability for pet images, we design several targeted strategies for the
diffusion process and inverse process, namely poisson diffusion and ct-guided
cross-attention, respectively.poisson diffusion. in conventional diffusion
models, the forward process typically employs gaussian noise to gradually
perturb input samples. however, in pet images, the dominant source of noise is
poisson noise, rather than gaussian noise. considering this, in our upete we
choose to adopt poisson diffusion to perturb the input samples, which
facilitates the diffusion model for achieving better performance on the pet
enhancement task.let z t be the perturbation sample in poisson diffusion, where
t = 0, 1, ..., t . then the poisson diffusion can be formulate as follows:at
each diffusion step, we apply the perturb function to the previous perturbed
sample z t-1 by imposing a poisson noise with an expectation of λ t , which is
linearly interpolated from [0, 1] and incremented with t. in our implementation,
we apply the same poisson noise imposition operation as in [20], i.e., applying
poisson deviates on the projected sinograms, to generate a sequence of perturbed
samples with increasing poisson noise intensity as the step number t
increases.ct-guided cross-attention. the attenuation correction of pet typically
relies on the corresponding anatomical image (ct or mr), resulting in a pet scan
usually accompanied by a ct or mr scan. to fully utilize the extramodality
images (i.e., ct in our work) as well as improve the applicability of diffusion
models, we design a ct-guided cross-attention to incorporate the ct images into
the reverse process for assisting the recovery of structural details.as shown in
fig. 1, to achieve a particular spet estimation, the corresponding ct image is
first compressed into the latent representation z ct by encoder e ct . then z ct
is fed into a denoising attention u-net [16] at each step for calculation of
cross-attention, where the query q and key k are calculated from z ct while the
value v is still calculated from the output of the previous layer because our
final goal is spet estimation. denoting the output of previous layer as z p et ,
the ct-guided cross-attention can be formulated as follows:where d is the number
of channels, b is the position bias, and conv(•) denotes the 1 × 1 × 1
convolution with stride of 1.
typically, the trained diffusion model generates target images from random
noise, requiring a large number of steps t to make the final perturbed sample (z
t ) close to pure noise. however, in our task, the target spet image is
generated from a given lpet image during testing, and making z t as close to
pure noise as possible is not necessary since the remaining pet-related
information can also benefit the image recovery. therefore, we can considerably
reduce the number of diffusion steps t to accelerate the model training, and t
is set to 400 in our implementation. we evaluate the quantitative results using
two metrics, including peak signal to noise ratio (psnr) and structural
similarity index measure (ssim). 3 experiments
our dataset consists of 100 spet images for training and 30 paired lpet and spet
images for testing. among them, 50 chest-abdomen spet images are collected from
(total-body) uexplorer pet/ct scanner [25], and 20 paired chest-abdomen images
are collected by list mode of the scanner with 256 mbq of [ 18 f]-fdg injection.
specifically, the spet images are reconstructed by using the 1200 s data between
60-80 min after tracer injection, while the corresponding lpet images are
simultaneously reconstructed by 120 s data uniformly sampled from 1200 s data.
as a basic data preprocessing, all images are resampled to voxel spacing of 2 ×
2 × 2 mm 3 and resolution of 256 × 256 × 160, while their intensity range is
normalized to [0, 1] by min-max normalization. for increasing the training
samples and reducing the dependence on gpu memory, we extract the overlapped
patches of size 96 × 96 × 96 from every whole pet image.
to verify the effectiveness of our proposed strategies, i.e. poisson diffusion
process and ct-guided cross-attention, we design another four variant latent
diffusion models (ldms) with the same compression model, including: 1) ldm:
standard ldm; 2) ldm-p: ldm with poisson diffusion process; 3) ldm-ct: ldm with
ct-guided cross-attention; 4) ldm-p-ct: ldm with poisson diffusion process and
ct-guided cross-attention. all methods use the same experimental settings, and
their quantitative results are given in table 1.from table 1, we can have the
following observations. (1) ldm-p achieves better performance than ldm. this
proves that the poisson diffusion is more appropriate than the gaussian
diffusion for pet enhancement. (2) ldm-ct with the corresponding ct image for
assisting denoising achieves better results than ldm. this can be reasonable as
the ct image can provide anatomical information, thus benefiting the recovery of
structural details (e.g., organ boundaries) in spet images. (3) ldm-p-ct
achieves better results than all other variants on both psnr and ssim, which
shows both of our proposed strategies contribute to the final performance. these
three comparisons conjointly verify the effective design of our proposed upete,
where the poisson diffusion process and ct-guided cross-attention both benefit
the pet enhancement.
we further compare our upete with several state-of-the-art pet enhancement
methods, which can be divided into two classes: 1) fully-supervised methods,
including la-gan [21], transformer-gan (trans-gan) [13], dual-frequency gan
(df-gan) [9], and ar-gan [14]; 2) unsupervised methods, including deep image
prior (dip) [3], noisier2noise [23], magnetic resonance guided deep decoder
(mr-gdd) [17], and noise2void [20]. the quantitative and qualitative results are
provided in table 2 and fig. 3, respectively.quantitative comparison: table 2
shows that our upete outperforms all competing methods. compared to the
fully-supervised method ar-gan which achieves sub-optimal performance, our upete
does not require paired lpet and spet, yet still achieves improvement.
additionally, upete also achieves noticeable performance improvement to
noise2void (which is a supervised method). specifically, the average improvement
in psnr and ssim on spet estimation are 1.554 db and 0.005, respectively. this
suggests that our upete can generate promising results without relying on paired
data, demonstrating its potential for clinical applications.qualitative
comparison: in fig. 3, we provide a visual comparison of spet estimation for two
typical cases. first, compared to unsupervised methods such as dip and
noise2void, the spet images estimated by our upete have less noise but clearer
boundaries. second, our upete performs better on the structural details compared
to the fully-supervised methods, i.e., missing unclear tissue (trans-gan) or
introducing non-existing artifacts in pet image (df-gan). overall, these pieces
of evidence demonstrate the superiority of our upete over state-of-the-art
methods.
we further evaluate the generalizability of our upete to tracer dose changes by
simulating poisson noise on spet to produce different doses for lpet, which is a
common way to generate noisy pet data [20]. notably, we do not need to retrain
the models since they have been trained in sect. 3.3. the quantitative results
of our upete and five state-of-the-art methods are provided in fig. 2. as shown
in fig. 2, our upete outperforms the other five methods at all doses and
exhibits a lower psnr descent slope as dose decreases (i.e., λ increases),
demonstrating its superior generalizability to dose changes. this is because
upete is based on diffusion model, which simplifies the complex distribution
prediction task into a series of simple denoising tasks and thus has strong
generalizability. moreover, we also find that the unsupervised methods (i.e.,
upete, noise2void, and dip) have stronger generalizability than fully-supervised
methods (i.e., ar-gan, df-gan, and trans-gan) as they have a smoother descent
slope. the main reason is that the unsupervised learning has the ability to
extract patterns and features from the data based on the inherent structure and
distribution of the data itself [15].
in this paper, we have developed a clinically-applicable unsupervised pet
enhancement framework based on the latent diffusion model, which uses only the
clinically-available spet data for training. meanwhile, we adopt three
strategies to improve the applicability of diffusion models developed from
photographic images to pet enhancement, including 1) compressing the size of the
input image, 2) using poisson diffusion, instead of gaussian diffusion, and 3)
designing ct-guided cross-attention to enable additional anatomical images
(e.g., ct) to aid the recovery of structural details in pet. validated by
extensive experiments, our upete achieved better performance than both
state-of-the-art unsupervised and fully-supervised pet enhancement methods, and
showed stronger generalizability to the tracer dose changes.despite the advance
of upete, our current work still suffers from a few limitations such as (1)
lacking theoretical support for our poisson diffusion, which is just an
engineering attempt, and 2) only validating the generalizability of upete on a
simulated dataset. in our future work, we will complete the design of poisson
diffusion from theoretical perspective, and collect more real pet datasets
(e.g., head datasets) to comprehensively validate the generalizability of our
upete.
automated segmentation of blood vessels in 3d medical images is a crucial step
for the diagnosis and treatment of many diseases, where the segmentation can aid
in visualization, help with surgery planning, be used to compute biomarkers, and
further downstream tasks. automatic vessel segmentation has been extensively
studied, both using classical computer vision algorithms [16] such as vesselness
filters [8], or more recently with deep learning [3,5,6,11,19,21], where
state-ofthe-art performance has been achieved for various vessel structures.
supervised deep learning typically requires large, well-curated training sets,
which are often laborious to obtain. this is especially the case for 3d vessel
segmentation.manually delineating 3d vessels typically involves visualizing and
annotating a 3d volume through a sequence of 2d cross-sectional slices, which is
not a good medium for visualizing 3d vessels. this is because often only the
cross-section of a vessel is visible in a 2d slice. in order to segment a
vessel, the annotator has to track the cross-section of that vessel through
several adjacent slices, which is especially tedious for curved or branching
vessel trees. projecting 3d vessels to a 2d plane allows for the entire vessel
tree to be visible within a single 2d image, providing a more robust
representation and potentially alleviating the burden of manual annotation.
kozinski et al. [13] propose to annotate up to three maximum intensity
projections (mip) for the task of centerline segmentation [13], obtaining
results comparable to full 3d supervision. compared to centerline segmentation,
where the vessel diameter is disregarded, training a 3d vessel segmentation
model from 2d annotations poses additional segmentationspecific challenges, as
2d projections only capture the outline of the vessels, providing no information
about their interior. furthermore, the axes of projection are crucial for the
model's success, given the sparsity of information in 2d annotations.to achieve
3d vessel segmentation with only 2d supervision from projections, we first
investigate which viewpoints to annotate in order to maximize segmentation
performance. we show that it is feasible to segment the full extent of vessels
in 3d images with high accuracy by annotating only a single randomlyselected 2d
projection per training image. this approach substantially reduces the
annotation effort, even compared to works training only on 2d projections.
secondly, by mapping the 2d annotations to the 3d space using the depth of the
mips, we obtain a partially segmented 3d volume that can be used as an
additional supervision signal. we demonstrate the utility of our method on the
challenging task of peripancreatic arterial segmentation on contrast-enhanced
arterial-phase computed tomography (ct) images, which feature large variance in
vessel diameter. our contribution to 3d vessel segmentation is three-fold:• our
work shows that highly accurate automatic segmentation of 3d vessels can be
learned by annotating single mips. • based on extensive experimental results, we
determine that the best annotation strategy is to label randomly selected
viewpoints, while also substantially reducing the annotation cost.• by
incorporating additional depth information obtained from 2d annotations at no
extra cost to the annotator, we almost close the gap between 3d supervision and
2d supervision.
learning from weak annotations. weak annotations have been used in deep learning
segmentation to reduce the annotation effort through cheaper, less accurate, or
sparser labeling [20]. bai et al. [1] learn to perform aortic image segmentation
by sparsely annotating only a subset of the input slices. multiple instance
learning approaches bin pixels together by only providing labels at the bin
level. jia et al. [12] use this approach to segment cancer on histopathology
images successfully. annotating 2d projections for 3d data is another approach
to using weak segmentation labels, which has garnered popularity recently in the
medical domain. bayat et al. [2] propose to learn the spine posture from 2d
radiographs, while zhou et al. [22] use multi-planar mips for multi-organ
segmentation of the abdomen. kozinski et al. [13] propose to segment vessel
centerlines using as few as 2-3 annotated mips. chen et al. [4] train a vessel
segmentation model from unsupervised 2d labels transferred from a publicly
available dataset, however, there is still a gap to be closed between
unsupervised and supervised model performance. our work uses weak annotations in
the form of annotations of 2d mips for the task of peripancreatic vessel
segmentation, where we attempt to reduce the annotation cost to a minimum by
only annotating a single projection per training input without sacrificing
performance.incorporating depth information. depth is one of the properties of
the 3d world. loss of depth information occurs whenever 3d data is projected
onto a lower dimensional space. in natural images, depth loss is inherent
through image acquisition, therefore attempts to recover or model depth have
been employed for 3d natural data. for instance, fu et al. [9] use neural
implicit fields to semantically segment images by transferring labels from 3d
primitives to 2d images. lawin et al. [14] propose to segment 3d point clouds by
projecting them onto 2d and training a 2d segmentation network. at inference
time, the predicted 2d segmentation labels are remapped back to the original 3d
space using the depth information. in the medical domain, depth information has
been used in volume rendering techniques [7] to aid with visualization, but it
has so far not been employed when working with 2d projections of 3d volumes to
recover information loss. we propose to do the conceptually opposite approach
from lawin et al. [14], by projecting 3d volumes onto 2d to facilitate and
reduce annotation. we use depth information to map the 2d annotations to the
original 3d space at annotation time and generate partial 3d segmentation
volumes, which we incorporate in training as an additional loss term.
overview. the maximum intensity projection (mip) of a 3d volume i ∈ r nx×ny×nz
is defined as the highest intensity along a given axis:for simplicity, we only
describe mips along the z-axis, but they can be performed on any image axis.
exploiting the fact that arteries are hyperintense in arterial phase cts, we
propose to annotate mips of the input volume for binary segmentation. the
hyperintensities of the arteries ensures their visibility in the mip, while
additional processing removes most occluding nearby tissue (sect. 4).given a
binary 2d annotation of a mip a ∈ {0, 1} nx×ny , we map the foreground pixels in
a to the original 3d image space. this is achieved by using the first and last z
coordinates where the maximum intensity is observed along any projection ray.
owing to the fact that the vessels in the abdominal cavity are relatively sparse
in 2d projections and most of the occluding tissue is removed in postprocessing,
this step results in a fairly complete surface of the vessel tree. furthermore,
we can partially fill this surface volume, resulting in a 3d depth map d, which
is a partial segmentation of the vessel tree. we use the 2d annotations as well
as the depth map to train a 3d segmentation network in a weakly supervised
manner.an overview of our method is presented in fig. 1. in the following, we
describe these components and how they are combined to train a 3d segmentation
network in more detail.depth information. we can view mip as capturing the
intensity of the brightest pixel along each ray r xy ∈ r nz , where r xy (z) =
i(x, y, z). along each projection ray, we denote the first and last z
coordinates which have the same intensity as the mip to be the forward depth z
fw = arg max z i(x, y, z) and backward depth z bw = arg min z i(x, y, z). this
information can be utilized for the following: (1) enhancing the mip
visualization, or (2) providing a way to map pixels from the 2d mip back to the
3d space (depth map). the reason why the maximum intensity is achieved multiple
times along a ray is because our images are clipped, which removes a lot of the
intensity fluctuations. where α ∈ [0, 1]. our final loss is a convex combination
between: (a) the crossentropy(ce) of the network output projected to 2d and the
2d annotation, as well as (b) the cross-entropy between the network output and
the depth map, but only applied to positive pixels in the depth map. notably,
the 2d loss constrains the shape of the vessels, while the depth loss promotes
the segmentation of the vessel interior.
dataset. we use an in-house dataset of contrast-enhanced abdominal computed
tomography images (cts) in the arterial phase to segment the peripancreatic
arteries [6]. the cohort consists of 141 patients with pancreatic ductal
adenocarcinoma, of an equal ratio of male to female patients. given a 3d
arterial ct of the abdominal area, we automatically extract the vertebrae
[15,18] and semi-automatically extract the ribs, which have similar intensities
as arteries in arterial cts and would otherwise occlude the vessels. in order to
remove as much of the cluttering surrounding tissue and increase the visibility
of the vessels in the projections, the input is windowed so that the vessels
appear hyperintense. details of the exact preprocessing steps can be found in
table 2 of the supplementary material. the dataset contains binary 3d
annotations of the peripancreatic arteries carried out by two radiologists, each
having annotated half of the dataset. the 2d annotations we use in our
experiments are projections of these 3d annotations. for more information about
the dataset, see [6].image augmentation and transformation. as the annotations
lie on a 2d plane, 3d spatial augmentation cannot be used due to the information
sparsity in the ground truth. instead, we apply an invertible transformation t
to the input volume and apply the inverse transformation t -1 to the network
output before applying the loss, such that the ground truth need not be altered.
a detailed description of the augmentations and transformations used can be
found in table 1 in the supplementary material.training and evaluation. we use a
3d u-net [17] with four layers as our backbone, together with xavier
initialization [10]. a diagram of the network architecture can be found in fig.
2 in the supplementary material. the loss weight α is tuned at 0.5, as this
empirically yields the best performance. our experiments are averaged over
5-fold cross-validation with 80 train samples, 20 validation samples, and a
fixed test set of 41 samples. the network initialization is different for each
fold but kept consistent across different experiments run on the same fold. this
way, both data variance and initialization variance are accounted for through
cross-validation. to measure the performance of our models, we use the dice
score, precision, recall, and mean surface distance (msd). we also compute the
skeleton recall as the percentage of the ground truth skeleton pixels which are
present in the prediction.
the effectiveness of 2d projections and depth supervision. we compare training
using single random viewpoints with and without depth information against
baselines that use more supervision. models trained on full 3d ground truth
represent the upper bound baseline, which is very expensive to annotate. we
implement [13] as a baseline on our dataset, training on up to 3 fixed
orthogonal projections. we distinguish between models selected according to the
2d performance on the validation set (2d) which is a fair baseline, and models
selected according to the 3d performance on the validation set (3d), which is an
unfair baseline as it requires 3d annotations on the validation set.with the
exception of the single fixed viewpoint baselines where the models have the
tendency to diverge towards over-or segmentation, we perform binary holefilling
on the output of all of our other models, as producing hollow objects is a
common under-segmentation issue.in table 1 we compare our method against the 3d
baseline, as well as baselines trained on multiple viewpoints. we see that by
using depth information paired with training using a single random viewpoint per
sample performs almost at the level of models trained on 3d labels, at a very
small fraction of the annotation cost. the depth information also reduces model
variance compared to the same setup without depth information. even without
depth information, training the model on single randomly chosen viewpoints
offers a robust training signal that the dice score is on par with training on 2
fixed viewpoints under ideal model selection at only half the annotation cost.
randomly selecting viewpoints for training acts as powerful data augmentation,
which is why we are able to obtain performance comparable to using more fixed
viewpoints. under ideal 3d-based model selection, three views would come even
closer to full 3d performance; however, with realistic 2d-based model selection,
fixed viewpoints are more prone to diverge. this occurs because sometimes
2d-based model selection favors divergent models which only segment hollow
objects, which cannot be fixed in postprocessing. single fixed viewpoints
contain so little information on their own that models trained on such input
fail to learn how to segment the vessels and generally converge to
over-segmenting in the blind spots in the projections. we conclude that using
random viewpoints is not only helpful in reducing annotation cost but also
decreases model variance.in terms of other metrics, randomly chosen projection
viewpoints with and without depth improve both recall and skeleton recall even
compared to fully 3d annotations, while generally reducing precision. we
theorize that this is because the dataset itself contains noisy annotations and
fully supervised models better overfit to the type of data annotation, whereas
our models converge to following the contrast and segmenting more vessels, which
are sometimes wrongfully labeled as background in the ground truth. msd are not
very telling in our dataset due to the noisy annotations and the nature of
vessels, as an under-or over-segmented vessel branch can quickly translate into
a large surface distance.the effect of dataset size. we vary the size of the
training set from |d tr | = 80 to as little as |d tr | = 10 samples, while
keeping the size of the validation and test sets constant, and train models on
single random viewpoints.in table 2, we compare single random projections
trained with and without depth information at varying dataset sizes to ilustrate
the usefulness of the depth information with different amounts of training data.
our depth loss offers consistent improvement across multiple dataset sizes and
reduces the overall performance variance. the performance boost is noticeable
across the board, the only exception being precision. the smaller the dataset
size is, the greater the performance boost from the depth. we perform a wilcoxon
rank-sum statistical test comparing the individual sample predictions of the
models trained at various dataset sizes with single random orthogonal viewpoints
with or without depth information, obtaining a statistically significant
(p-value of < 0.0001). we conclude that the depth information complements the
segmentation effectively.
in this work, we present an approach for 3d segmentation of peripancreatic
arteries using very sparse 2d annotations. using a labeled dataset consisting of
single, randomly selected, orthogonal 2d annotations for each training sample
and additional depth information obtained at no extra cost, we obtain accuracy
almost on par with fully supervised models trained on 3d data at a mere fraction
of the annotation cost. limitations of our work are that the depth information
relies on the assumption that the vessels exhibit minimal intensity fluctuations
within local neighborhoods, which might not hold on other datasets, where more
sophisticated ray-tracing methods would be more effective in locating the front
and back of projected objects. furthermore, careful preprocessing is performed
to eliminate occluders, which would limit its transferability to datasets with
many occluding objects of similar intensities. further investigation is needed
to quantify how manual 2d annotations compare to our 3d-derived annotations,
where we expect occluders to affect the annotation process.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_14.
statistical shape modeling (ssm) is a powerful tool in medical image analysis
and computational anatomy to quantify and study the variability of anatomical
structures within populations. ssm has shown great promise in medical research,
particularly in diagnosis [12,23], pathology detection [19,25], and treatment
planning [27]. ssm has enabled researchers to better understand the underlying
biological processes, leading to the development of more accurate and
personalized diagnostic and treatment plans [3,9,14,17].over the years, several
ssm approaches have been developed that implicitly represent the shapes
(deformation fields [8], level set methods [22]) or explicitly represent them as
a ordered set of landmarks or correspondence points (aka point distribution
models, pdms). here, we focus on the automated construction of pdms because,
compared to deformation fields, point correspondences are easier to interpret by
clinicians, are computationally efficient for large datasets, and less sensitive
to noise and outliers than deformation fields [5].ssm performance depends on the
underlying process used to generate shape correspondences and the quality of the
input data. various correspondence generation methods exist, including
non-optimized landmark estimation and parametric and non-parametric
correspondence optimization. non-optimized methods manually label a reference
shape and warp the annotated landmarks using registration techniques [10,16,18].
parametric methods use fixed geometrical bases to establish correspondences
[26], while group-wise non-parametric approaches find correspondences by
considering the variability of the entire cohort during the optimization
process. examples of non-parametric methods include particlebased optimization
[4] and minimum description length (mdl) [7].traditional ssm methods assume that
population variability follows a gaussian distribution, which implies that a
linear combination of training shapes can express unseen shapes. however,
anatomical variability can be far more complex than this linear approximation,
in which case nonlinear variations normally exist (e.g., bending fingers, soft
tissue deformations, and vertebrae with different types). furthermore,
conventional ssm pipelines are computationally intensive, where inferring pdms
on new samples entail an optimization process. deep learning-based approaches
for ssm have emerged as a promising avenue to overcoming these limitations. deep
learning models can learn complex nonlinear representations of the shapes, which
can be used to generate shape models. moreover, they can efficiently perform
inference on new samples without computation overhead or re-optimization. recent
works such as flowssm [15], shapeflow [11], deepssm [2], and vib-deepssm [1]
have incorporated deep learning to generate shape models. flowssm [15] and
shapeflow [11] operate on surface meshes and use neural networks to parameterize
the deformations field between two shapes in a low dimensional latent space and
rely on an encoderfree setup. encoder-free methods randomly initialize the
latent representations for each sample that are then optimized to produce the
optimal deformations. one major caveat of an encoder-free setup is that
inference on new meshes is no longer straightforward; the latent representation
has to be re-optimized for every new sample. on the other hand, deepssm [2],
tl-deepssm [2], and vib-deepssm [1] learn the pdm directly from unsegmented
ct/mri images, and hence alleviate the need for pdm optimization given new
samples and can bypass anatomy segmentation by operating directly on unsegmented
images. however, these methods rely on supervised losses and require volumetric
images, segmented images, and established/optimized pdms for training. this
reliance on supervised losses introduces linearity assumptions in generating
ground truth pdms. tl-deepssm [2], a variant of deepssm [2], differs from the
others by not utilizing pca scores as shape descriptors. instead, it adopts an
established correspondence model hence, similar to the vanilla deepssm [2]
learns a linear model.in this paper, we introduce mesh2ssm1 , a deep learning
method that addresses the limitations of traditional and deep learning-based ssm
approaches. mesh2ssm leverages unsupervised, permutation-invariant
representation learning to learn the low dimensional nonlinear shape descriptor
directly from mesh data and uses the learned features to generate a
correspondence model of the population. mesh2ssm also includes an analysis
network that operates on the learned correspondences to obtain a data-driven
template point cloud (i.e., template point cloud), which can replace the initial
template, and hence reducing the bias that could arise from template selection.
furthermore, the learned representation of meshes can be used for predicting
related quantities that rely on shape. our main contributions are:1. we
introduce mesh2ssm, a fully unsupervised correspondence generation deep learning
framework that operates directly on meshes. mesh2ssm uses an autoencoder to
extract the shape descriptor of the mesh and uses this descriptor to transform a
template point cloud using im-net [6]. 2. the proposed method uses an
autoencoder that combines geodesic distance features and edgeconv [28] (dynamic
graph convolution neural network) to extract meaningful feature representation
of each mesh that is permutationinvariant. 3. mesh2ssm also includes a
variational autoencoder (vae) [13,21] operating on the learned correspondence
points and trained end-to-end with correspondence generation network. this vae
branch serves two purposes: (a) serves as a shape analysis module for the
non-linear shape variations and (b) learns a data-specific template from the
latent space of the correspondences that is fed back to the correspondence
generation network.to motivate the need for the mesh feature encoder and study
the effect of the template selection, we considered the box-bump dataset, a
synthetic dataset of 3d shapes of boxes with a moving bump. in fig. 1, we
compare mesh2ssm (sans the vae analysis branch) with flowsmm [15] since this
approach is the closest to mesh2ssm. we performed experiments with three
templates: medoid, sphere, and box without the bump. although both methods show
some sensitivity to the choice of template, flowssm is more sensitive toward the
choice of the template than mesh2ssm. moreover, flowssm fails to identify the
correct mode of variation, the horizontal movement of the bump as the primary
variation, which can also be inferred by comparing the compactness curves in
fig. 1.c. mesh2ssm performs best when the template is a medoid shape, which
makes the case for learning a data-specific template. since mesh2ssm model uses
an autoencoder, inference on unseen meshes only requires a single forward pass
(1 s per sample); flowssm requires re-optimization, increasing the inference
time drastically and require a convergence criteria to determine the best number
of iterations per sample (0.15 s for one iterations per sample). [15] with three
templates: sphere, box without a bump, and medoid shape. flowssm fails to
capture the horizontal movement as the primary mode of variation. (c) the
compactness curves for both models with different templates. the overview of the
proposed pipeline is provided in fig. 2. this section provides a brief
description of each module.
given a set of n aligned surface meshes x = {x 1 , x 2 , ...x n }, each mesh x i
= (v i , e i ), where v i and e i represent the vertices and edge connectivity,
respectively. the goal of the model is to predict a set c i of m 3d
correspondence points that fully describe each surface x i and are anatomically
consistent across all meshes. this goal is achieved by learning a low
dimensional representation of the surface mesh z m ∈ r l using the mesh
autoencoder and then z m is used to transform the template point cloud via the
implicit field decoder (im-net) [6]. the network optimization is driven
primarily by point-set to point-set two-way chamfer distance between the learned
correspondence point sets c i and the vertex locations v i of the original
meshes. to ensure that the encoder learns useful features for the task, we
regularize the optimization using the vertex reconstruction loss of the
autoencoder between the input v i and the predicted vi . the correspondence loss
function is given by:where α, γ are the hyperparameters. we consider a
combination of l 1 and l 2 two-way chamfer distance for numerical stability as
the magnitude of l 2 loss can be low over epochs and l 1 can compensate for it.
the correspondence generation uses two networks:
we use edgeconv [28] blocks, which are dynamic graph convolution neural network
(dgcnn) blocks in the encoder and decoder to capture local geometric features of
the mesh. the model takes vertices as input, computes an edge feature set of
size k (using nearest neighbors) for each vertex at an edgeconv layer, and
aggregates features within each set to compute edgeconv responses. the output
features of the last edgeconv layer are then globally aggregated to form a 1d
global descriptor z m i of the mesh. the first edgeconv block uses geodesic
distance on the surface of the mesh to calculate the k features. the dynamic
feature creation property of edgeconv and the global pooling make this
autoencoder permutation invariant.
the im-net [6] architecture consists of fully connected layers with
non-linearity and skip-layer connections. this network enforces the notion of
correspondence across the samples. the network takes in two inputs, the latent
representation of the mesh z m and a template point cloud (a set of unordered
points). im-net estimates the deformation of each point in the template required
to deform the template to each sample, conditioned on z m . based on the learned
deformation, im-net directly produces the resultant displaced template point
without the computational complexity of the deformation fields. correspondence
is established since the same template is deformed to all the samples.
the mesh2ssm model also consists of an analysis branch that acts as a shape
analysis module to capture non-linear shape variations identified by the learned
correspondences {c i } n i=1 and also learns a data-informed template from the
latent space of correspondences to be fed back into the correspondence
generation network during training. this branch uses one network module:
the vae [13,21] is a latent variable model parameterized by an encoder φ,
decoder θ, and the prior p(z p ) ∼ n (0, i). the encoder maps the shape
represented by the learned correspondence points c to the latent space and the
decoder reconstructs the correspondences from the latent representation z p . by
capturing the underlying structure of the pdm through a low-dimensional
representation, sp-vae allows for the estimation of the mean shape of the
learned correspondences. the sp-vae is trained using the loss function given
by:the main difference between m-ae and a sp-vae lies in the input and output
representations they handle. sp-vae operates directly on sets of landmarks or
correspondences, aiding in the analysis of shape models. it takes a set of
correspondences describing a shape as input and aims to learn a compressed
latent representation of the shape. importantly, the sp-vae maintains the same
ordering of correspondences at the input and output, so it does not use
permutation-invariant layers or operations like pooling.
we begin with a burn-in stage, where only the correspondence generation module
is trained while the analysis module is frozen. after the burn-in stage,
alternate optimization of the correspondence and analysis module begins. during
the alternate optimization phase, we generate the data-informed template from
the latent space of sp-vae at regular intervals. the learned data-informed
template is used in the correspondence generation module in the subsequent
epochs. for the learned template, we sample 500 samples from the prior p(z p ) ∼
n (0, i) and pass it through the decoder of sp-vae to get the reconstructed
correspondence point set. the mean template is defined by taking the average of
these generated samples. inference with unseen meshes is straight forward; the
meshes are passed through the mesh encoder and im-net of the correspondence
generation module to get the predicted correspondences. all hyperparameters and
network architecture details are mentioned in the supplementary material.
dataset: we use the publicly available decath-pancreas dataset of 273
segmentations from patients who underwent pancreatic mass resection [24]. the
shapes of the pancreas are highly variable and have thin structures, making it a
good candidate for non-linear ssm analysis. the segmentations were isotropically
resampled, smoothed, centered, and converted to meshes with roughly 2000
vertices. although the dgcnn mesh autoencoder used in mesh2ssm does not require
the same number of vertices, uniformity across the dataset makes it
computationally efficient; hence, we pad the smallest mesh by randomly repeating
the vertices (akin to padding image for convolutions). the samples were randomly
divided, with 218 used for training, 26 for validation, and 27 for testing.
flowssm [15] with two templates: sphere, medoid. the color map and arrows show
the signed distance and direction from the mean shape.
we perform experiments with two templates: sphere and medoid. we compare the
performance of flowssm [15] with mesh2ssm with the template feedback loop. for
mesh2ssm template, we use 256 points uniformly spread across the surface of the
sample. mesh2ssm and flowssm do not have a equivalent latent space for
comparison of the shape models, hence, we consider the deformed mesh vertices of
flowssm as correspondences and perform pca analysis. figure 3 shows the top
three pca modes of variations identified by mesh2ssm and flowssm. similar to the
observations made box-bump dataset, flowssm is affected by the choice of the
template, and the modes of variation differ as the template changes. on the
other hand, pdm predicted by mesh2ssm identifies the same primary modes
consistently. pancreatic cancer mainly presents itself on the head of the
structure [20] and for the decath dataset, we can see the first mode identifies
the change in the shape of the head. we evaluate the models based on
compactness, generalization, and specificity. compactness measures the ability
of the model to reconstruct new shape instances with fewer parameters using pca
explained variance. generalization measures the average surface distance between
all test shapes and their reconstructions, and specificity measures the distance
between randomly generated pca samples. figure 4.a shows the metrics for the
pancreas dataset. mesh2ssm outperforms flowssm in all three metrics, despite
using only 256 correspondence points compared to flowssm's ∼2000 vertices.
mesh2ssm correspondence generation module efficiently parameterizes the surface
of the pancreas with a minimum number of parameters. mesh2ssm template, shown in
fig. 4.b, becomes more detailed as optimization continues, regardless of the
starting template. the model can learn correct deformations in the
correspondence generation module and identify the correct mean shape in the
latent space of sp-vae in the analysis module. using the analysis module of
mesh2ssm, we visualized the top three modes of variation identified by sorting
the latent dimensions of sp-vae based on the standard deviations of the latent
embeddings of the training dataset. variations are generated by perturbing the
latent representation of a sample in three directions, resulting in non-linear
modes such as changes in the size and shape of the pancreas head and narrowing
of the neck and body. this is shown in fig. 4.c for meshssm model with medoid
starting template. the distance metrics for the reconstructions of the testing
samples were also computed. the results of the metrics are summarized in table
1. the calculation involved the l 1 chamfer loss between the predicted points
(correspondences in the case of mesh2ssm and the deformed mesh vertices in the
case of flowssm) and the original mesh vertices. additionally, the surface to
surface distance of the mesh reconstructions (using the correspondences in
mesh2ssm and deformed meshes in flowssm) was included. for the pancreas dataset
with the medoid as the initial template, mesh2ssm with the template feedback
produced more precise models.
as ssm is included a part of diagnostic clinical support systems, it is crucial
to address the drawbacks of the models. like most deep learning models,
performance of mesh2ssm could be affected by small dataset size, and it can
produce overconfident estimates. an augmentation scheme and a layer uncertainty
calibration are could improve its usability in medical scenarios. additionally,
enforcing disentanglement in the latent space of sp-vae can make the analysis
module interpretable and allow for effective non-linear shape analysis by
clinicians.
the paper presents a new systematic approach of generating non-linear
statistical shape models using deep learning directly from meshes, which
overcomes the limitations of traditional ssm and current deep learning
approaches. the use of an autoencoder for meaningful feature extraction of
meshes to learn the pdm provides a versatile and scalable framework for ssm.
incorporating template feedback loop via vae [13,21] analysis module helps in
mitigating bias and capturing non-linear characteristics of the data. the method
is demonstrated to have superior performance in identifying shape variations
using fewer parameters on synthetic and clinical datasets. to conclude, our
method of generating highly accurate and detailed models of complex anatomical
structures with reduced computational complexity has the potential to establish
statistical shape modeling from non-invasive imaging as a powerful diagnostic
tool.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0 59.
medical image segmentation is a critical task in computer-aided diagnosis and
treatment planning. it involves the delineation of anatomical structures or
pathological regions in medical images, such as magnetic resonance imaging (mri)
or computed tomography (ct) scans. accurate and efficient segmentation is
essential for various medical applications, including tumor detection, surgical
planning, and monitoring disease progression. however, manual medical imaging
annotation is time-consuming and expensive because it requires the domain
knowledge from medical experts. therefore, there is a growing interest in
developing semi-supervised learning that leverages both labeled and unlabeled
data to improve the performance of image segmentation models [16,27].existing
semi-supervised segmentation methods exploit smoothness assumption, e.g., the
data samples that are closer to each other are more likely to to have the same
label. in other words, the smoothness assumption encourages the model to
generate invariant outputs under small perturbations. we have seen such
perturbations being be added to natural input images at data-level
[4,9,14,19,21], feature-level [6,17,23,25], and model-level [8,11,12,24,28].
among them, virtual adversarial training (vat) [14] is a well-known one which
promotes the smoothness of the local output distribution using adversarial
examples. the adversarial examples are near decision boundaries generated by
adding adversarial perturbations to natural inputs. however, vat can only create
one adversarial sample in a run, which is often insufficient to completely
explore the space of possible perturbations (see sect. 2.1). in addition, the
adversarial examples of vat can also lie together and lose diversity that
significantly reduces the quality of adversarial examples [15,20]. mixup
regularization [29] is a data augmentation method used in deep learning to
improve model generalization. the idea behind mixup is to create new training
examples by linearly interpolating between pairs of existing examples and their
corresponding labels, which has been adopted in [2,3,19] to semi-supervised
learning. the work [5] suggests that mixup improves the smoothness of the neural
function by bounding the lipschitz constant of the gradient function of the
neural networks. however, we show that mixing between more informative samples
(e.g., adversarial examples near decision boundaries) can lead to a better
performance enhancement compared to mixing natural samples (see sect. 3.3).in
this paper, we propose a novel cross-adversarial local distribution
regularization for semi-supervised medical image segmentation for smoothness
assumption enhancement1 . our contributions are summarized as follows: 1) to
overcome the vat's drawback, we formulate an adversarial local distribution
(ald) with dice loss function that covers all possible adversarial examples
within a ball constraint. 2) to enhance smoothness assumption, we propose a
novel cross-adversarial local distribution regularization (cross-ald) to
encourage the smoothness assumption, which is a random mixing between two alds.
we also propose a sufficiently approximation for the cross-ald by a multiple
particle-based search using semantic feature stein variational gradient decent
(svgdf), an enhancement of the vanilla svgd [10]. 4) we conduct comprehensive
experiments on adcd [1] and la [26] datasets, showing that our cross-ald
regularization achieves state-of-the-art performance against existing solutions
[8,11,12,14,21,22,28].
in this section, we begin by reviewing the minimax optimization problem of
virtual adversarial training (vat) [14]. given an input, we then formulate a
novel adversarial local distribution (ald) with dice loss, which benefits the
medical semi-supervised image segmentation problem specifically. next, a
crossadversarial local distribution (cross-ald) is constructed by randomly
combining two alds. we approximate the ald by a particle-based method named
semantic feature stein variational gradient descent (svgdf). considering the
resolution of medical images are usually high, we enhance the vanilla svgd [10]
from data-level to feature-level, which is named svgdf. we finally provide our
regularization loss for semi-supervised medical image segmentation.
let d l and d ul be the labeled and unlabeled dataset, respectively, with p d l
and p d ul being the corresponding data distribution. denote x ∈ r d as our
ddimensional input in a space x. the labeled image x l and segmentation
groundtruth y are sampled from the labeled dataset d l (x l , y ∼ p d l ), and
the unlabeled image sampled from d ul is x ∼ p d ul .given an input x ∼ p d ul
(i.e., the unlabeled data distribution), let us denote the ball constraint
around the image x aswhere is a ball constraint radius with respect to a norm ||
• || p , and x is an adversarial example2 . given that f θ is our model
parameterized by θ, vat [14] trains the model with the loss of vat that a
minimax optimization problem:where d kl is the kullback-leibler divergence. the
inner maximization problem is to find an adversarial example near decision
boundaries, while the minimization problem enforces the local smoothness of the
model. however, vat is insufficient to explore the set of of all adversarial
examples within the constraint c because it only find one adversarial example x
given a natural input x. moreover, the works [15,20] show that even solving the
maximization problem with random initialization, its solutions can also lie
together and lose diversity, which significantly reduces the quality of
adversarial examples.
in order to overcome the drawback of vat, we introduce our proposed adversarial
local distribution (ald) with dice loss function instead of d kl in [14,15]. ald
forms a set of all adversarial examples x within the ball constraint given an
input x. therefore, the distribution can helps to sufficiently explore all
possible adversarial examples. the adversarial local distribution p θ (x |x) is
defined with a ball constraint c as follow:where p θ (•|x) is the conditional
local distribution, and z(x; θ) is a normalization function. the dice is the
dice loss function as shown in eq. 3where c is the number of classes. p θ ( ŷc
|x) and p θ ( ỹc |x ) are the predictions of input image x and adversarial image
x , respectively.
given two random samples x i , x j ∼ p d (i = j), we define the
cross-adversarial distribution (cross-ald) denoted pθ as shown in eq. 4where γ ∼
beta(α, α) for α ∈ (0, ∞), inspired by [29]. the pθ is the cross-ald
distribution, a mixture between the two adversarial local distributions. given
eq. 4, we propose the cross-ald regularization at two random input images x i ,
x j ∼ p d (i = j) aswhere h indicates the entropy of a given distribution. when
minimizing r(θ, x i , x j ) or equivalently -h(p θ (•|x i , x j )) w.r.t. θ, we
encourage p θ (•|x i , x j ) to be closer to a uniform distribution. this
implies that the outputs of f ( x ) = f ( x ) = a constant c, where x , x ∼ pθ
(•|x i , x j ). in other words, we encourages the invariant model outputs under
small perturbations. therefore, minimizing the cross-ald regularization loss
leads to an enhancement in the model smoothness. while vat only enforces local
smoothness using one adversarial example, cross-ald further encourages
smoothness of both local and mixed adversarial distributions to improve the
model generalization.
in eq. 2, the normalization z(x; θ) in denominator term is intractable to find.
therefore, we propose a multiple particle-based search method named svgdf to
sample x (1) , x (2) , . . . , x (n ) ∼ p θ (•|x)). n is the number of samples
(or adversarial particles). svgdf is used to solve the optimization problem of
finding a target distribution p θ (•|x)). svgdf is a particle-based bayesian
inference algorithm that seeks a set of points (or particles) to approximate the
target distribution without explicit parametric assumptions using iterative
gradient-based updates. specifically, a set of adversarial particles (x (n) ) is
initialized by adding uniform noises, then projected onto the ball c . these
adversarial particles are then iteratively updated using a closed-form solution
(eq. 6) until reaching termination conditions (, number of iterations).[k(φ(x
(j),(l) ), φ(x ))∇ x (j),(l) log p (x (j),(l) |x)where x (n),(l) is a n th
adversarial particle at l th iteration (n ∈ {1, 2, ..., n }, and l ∈ {1, 2, ...,
l} with the maximum number of iteration l). c is projection operator to the c
constraint. τ is the step size updating. k is the radial basis function. φ is a
fixed feature extractor (e.g., encoder of u-net/v-net). while vanilla svgd [10]
is difficult to capture semantic meaning of high-resolution data because of
calculating rbf kernel (k) directly on the data-level, we use the feature
extractor φ as a semantic transformation to further enhance the svgd algorithm
performance for medical imaging. moreover, the two terms of φ in eq. 6 have
different roles: (i) the first one encourages the adversarial particles to move
towards the high density areas of p θ (•|x) and (ii) the second one prevents all
the particles from collapsing into the local modes of p θ (•|x) to enhance
diversity (e.g.,pushing the particles away from each other). please refer to the
cross-ald github repository for more details.svgdf approximates p θ (•|x i ) and
p θ (•|x j ) in eq. 4, where x i , x j ∼ p d ul (i = j). we form sets of
adversarial particles as}. the problem (5) can then be relaxed towhere γ ∼
beta(α, α) for α ∈ (0, ∞).
in this paper, the overall loss function total consists of three loss terms. the
first term is the dice loss, where labeled image x l and segmentation
ground-truth y are sampled from labeled dataset d l . the second term is a
contrastive learning loss for inter-class separation cs proposed by [21]. the
third term is our cross-ald regularization, which is an enhancement of vat to
significantly improve the model performance.where λ cs and λ cross-ald are the
corresponding weights to balance the losses. note that our implementation is
replacing vat loss with the proposed cross-ad regularization in ss-net code
repository3 [21] to reach the state-of-the-art performance.
in this section, we conduct several comprehensive experiments using the acdc4
dataset [1] and the la5 dataset [26] for 2d and 3d image segmentation tasks,
respectively. for fair comparisons, all experiments are conducted using the
identical setting, following [21]. we evaluate our model in challenging
semi-supervised scenarios, where only 5% and 10% of the data are labeled and the
remaining data in the training set is treated as unlabeled. the cross-ald uses
the u-net [18] and v-net [13] architectures for the acdc and la dataset,
respectively. we compare the diversity between the adversarial particles
generated by our method against vanilla svgd and vat with random initialization
in sect. 3.1 . we then illustrate the cross-ad outperforms other recent methods
on acdc and la datasets in sect. 3.2. we show ablation studies in sect. 3.3. the
effect of the number particles to the model performance is studied in the
cross-ald github repository.
settings. we fixed all the decoder models (u-net for acdc and v-net for la). we
run vat with random initialization and svgd multiple times to produce
adversarial examples, which we compared to the adversarial particles generated
using svgdf. svgdf is the proposed algorithm, which leverages feature
transformation to capture the semantic meaning of inputs. φ is the decoder of
u-net in acdc dataset, while φ is the decoder of v-net in la dataset. we set the
same radius ball constraint, updating step, and etc. we randomly pick three
images from the datasets to generate adversarial particles. to evaluate their
diversity, we report the sum squared error (sse) between these particles. higher
sse indicates more diversity, and for each number of particles, we calculate the
average of the mean of sses. results. note that the advantage of svgd over vat
is that the former generates diversified adversarial examples because of the
second term in eq. 6 while vat only creates one example. moreover, vanilla svgd
is difficult to capture semantic meaning of high-resolution medical imaging
because it calculates kernel k on image-level. in fig. 1, our svgdf produces the
most diverse particles compared to svgd and vat with random initialization.
settings. we use the metrics of dice, jaccard, 95% hausdorff distance (95hd),
and average surface distance (asd) to evaluate the results. we compare our
cross-ald to six recent methods including ua-mt [28] (miccai '19), sassnet [8]
(miccai '20), dtc [11] (aaai'21) , urpc [12] (miccai'21) , mc-net [22] (miccai
'21), and ss-net [21] (miccai '22). the loss weights λ cross-ald and λ cs are
set as an iteration dependent warming-up function [7], and number of particles n
= 2. all experiments are conducted using the identical settings in the github
repository 6 [21] for fair comparisons.results. recall that our cross-ald
generates diversified adversarial particles using svgdf compared to vanilla svgd
and vat, and further enhances smoothness of cross-adversarial local
distributions. in table 1 and 2, the cross-ald can significantly outperform
other recent methods with only 5%/10% labeled data training based on the four
metrics. especially, our method impressively gains 14.7% and 2.3% dice score
higher than state-of-the-art ss-net using 5% labeled data of acdc and la,
respectively. moreover, the visualized results of fig. 2 shows cross-ald can
segment the most organ details compared to other methods.
settings. we use the same network architectures and parameter settings in sect.
3.2, and train the models with 5% labeled training data of acdc and la. we
illustrate that crossing adversarial particles is more beneficial than random
mixup between natural inputs (ranmixup [29]) because these particles are near
decision boundaries. recall that our svgdf is better than vat and svgd by
producing more diversified adversarial particles. applying svgdf's particles and
cs (svgdf + cs ) to gain the model performance in the semi-supervised
segmentation task, while cross-ald efficiently enhances smoothness to
significantly improve the generalization.result. table 3 shows that mixing
adversarial examples from vat outperform those from ranmixup. while svgdf + cs
is better than svgd and vat, the proposed cross-ald achieves the most
outstanding performance among comparisons methods. in addition, our method
produces more accurate segmentation masks compared to the ground-truth, as shown
in fig. 2.
in this paper, we have introduced a novel cross-adversarial local distribution
(cross-ald) regularization that extends and overcomes drawbacks of vat and mixup
techniques. in our method, svgdf is proposed to approximate cross-ald, which
produces more diverse adversarial particles than vanilla svgd and vat with
random initialization. we adapt cross-ald to semi-supervised medical image
segmentation to achieve start-of-the-art performance on the acdc and la datasets
compared to many recent methods such as vat [14], ua-mt [28], sassnet [8], dtc
[11], urpc [12] , mc-net [22], and ss-net [21].
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0 18.
ultrasound elastography (use) provides information related to the stiffness of
the tissue. ultrasound (us) data before and after the tissue deformation (which
can be caused by an external or internal force) are collected and compared to
calculate the displacement map, indicating each individual sample's relative
motion. the strain is computed by taking the derivative of the displacement
fields. in free-hand palpation, the force is external and applied by the
operator by the probe [10].convolutional neural networks (cnn) have been
successfully employed for use displacement estimation [11,12,15]. unsupervised
and semi-supervised training methods have been proposed, which enable the
networks to use real us images for training [1,14,17]. the proposed networks
have achieved high-quality axial strains. in contrast to axial strain, lateral
strain, which is highly required in poisson's ratio imaging and elasticity
reconstruction, has a poor quality due to the low sampling frequency, limited
motion and lack of carrier signal in the lateral direction.recently, physically
inspired constraint in unsupervised regularized elastography (picture) has been
proposed [5]. this method aims to improve lateral displacement by exploiting the
high-quality axial displacement estimation and the relation between the lateral
and axial strains defined by the physics of motion. despite the substantial
improvement, the regularization is only applied during the training phase. in
addition, only a considerably large feasible range for poisson's ratio was
enforced, thereby providing further opportunities for the network to contravene
the laws of physics.known operators, introduced by maier et al. [7], have been
widely utilized in deep neural networks. the core idea is that some known
operations (for example inversion of a matrix) are embedded inside the networks
to simplify the training and improving the generalization ability of the
network. the known operator can be viewed as the prior knowledge related to the
physics of the problem. maier et al. investigated known operators in different
applications such as computed tomography, magnetic resonance imaging, and vessel
segmentation, and showed a substantial reduction in the maximum error bounds
[7].in this paper, we aim to embed two lateral displacement refinement
algorithms in the cnns to improve the lateral strains. the first algorithm
limits the range of effective poisson's ratio (epr) inside the feasible range
during the test time. it is important to note that in contrast to [5], the epr
range is enforced using the regularization during the training phase and the
known operators framework during the test phase; therefore, it is enforced
during both training and test phases. the second algorithm employs the
refinement method proposed be gou et al. [2] which exploits incompressibility
constraint to refine the lateral displacement. the network weight and a demo
code are publicly available online at http://code.sonography.ai.
in this section, we first provide a brief overview of picture and underlie some
differences to this work. we then introduce our method for incorporating known
operators into our deep model and outline our unsupervised training technique.
we then present the training and test datasets and finish the section by
demonstrating the network architecture.
let ε x denote axial (x = 1), lateral (x = 2), and out-of-plane (x = 3) strains.
assuming linear elastic, isotropic, and homogeneous material that can move
freely in the lateral direction, the lateral strain can be obtained from the
axial strain and the poisson's ratio by ε 2 = -v × ε. real tissues are
inhomogeneous, and boundary conditions exist; therefore, the lateral strain
cannot be directly obtained by the axial strain and the poisson's ratio alone.
in such conditions, epr, which is defined as v e = -ε22 ε11 can be employed [6].
epr is spatially variant, and it is not equal to poisson's ratio, particularly
in the vicinity of inclusion boundaries or within inhomogeneous tissue. its
value tends to converge towards the poisson's ratio in homogeneous regions, and
it has a similar range of poisson's ratio, i.e., between 0.2 and 0.5 [9]. in
picture, a regularization was defined to exploit this range and the out-of-range
eprs were penalized [5]. picture loss can be obtained from the following
procedure: 1-detect out-of-range eprs by:where v e is the epr obtained from the
estimated displacements. v emin and v emax are two hyperparameters that specify
the minimum and maximum accepted epr values, which are assumed to be 0.1 and
0.6, respectively.2-penalize the out-of-range lateral strains using:where < v e
> is the average of epr values within the feasible range. the operator s denotes
stop gradient operation, which is employed to avoid the axial strain being
affected by this regularization. it should be noted in contrast to [5] in which
only out-of-range samples were contributing to the loss, in this work, all
samples contribute to l vd to reduce the estimation bias.3-smoothness of epr is
considered by:4-picture loss is defined as l v = l vd + λ vs × l vs , where λ vs
is the weight of the smoothness loss. picture loss is added to the data and
smoothness losses of unsupervised training.
the known operators are added to the network in the inference mode only due to
the high computational complexity of unsupervised training (outlined in the next
section). we employ two known operators to impose physically known constraints
on the lateral displacement.the first known operator (we refer to it as
poisson's ratio clipper) limits the epr to the feasible range of v emin -v emax
. although picture tries to move all epr values to the feasible range, in [5],
it was shown that some samples in test time were still outside of the feasible
range. poisson's ratio clipper is an iterative algorithm since the lateral
strains are altered by clipping the epr values and affecting the neighbor
samples' strain values.the second algorithm employs the incompressibility of the
tissue which can be formulated by:in free-hand palpation, the force is
approximately uniaxial (ε 3 ε 2 ); therefore eq. 4 can be written as:guo et al.
enforced incompressibility in an iterative algorithm [2]. we made a few changes
to increase the method's robustness by adding gaussian filtering and using a
hyper-parameter weight in each iteration. it should be noted that the algorithm
can be employed for compressible tissues as well, and the incompressibility
constraint is employed for the refinement of the obtained displacement. the
proposed algorithms are outlined in algorithm 1 and 2. the network architecture
with the known operators is illustrated in fig. 1. it is worth highlighting that
known operators offer a compelling alternative to regularization. while the
latter involves adjusting trained weights based on the training data and keeping
them fixed during testing, the former relies on iterative refinement that is
adaptable to the test data and does not require any learnable weights.
we followed a similar unsupervised training approach presented in [5] for both
picture and kpicture methods. the loss function can be written as: where l d
denotes photometric loss which is obtained by comparing the precompressed and
warped compressed rf data, l s is smoothness loss in both axial and lateral
directions. λ s and λ v specify the weights of the smoothness loss and picture
loss, respectively.
we use publicly available data collected from a breast phantom (model 059, cirs:
tissue simulation & phantom technology, norfolk, va) using an alpinion e-cube
r12 research us machine (bothell, wa, usa). the center frequency was 8 mhz and
the sampling frequency was 40 mhz. the young's modulus of the experimental
phantom was 20 kpa and contains several inclusions with young's modulus of
higher than 40 kpa. this data is available online at http://code.sonography.ai
in [16].in vivo data was collected at johns hopkins hospital from patients with
liver cancer during open-surgical rf thermal ablation by a research antares
siemens system using a vf 10-5 linear array with the sampling frequency of 40
mhz and the center frequency of 6.67 mhz. the institutional review board
approved the study with the consent of the patients. we selected 600 rf frame
pairs of this dataset for the training of the networks.two well-known metrics of
contrast to noise ratio (cnr) and strain ratio (sr) are utilized to evaluate the
compared methods. two regions of interest (roi) are selected to compute these
metrics and they can be defined as [10]:where the subscript t and b denote the
target and background rois. the sr is only sensitive to the mean (s x ), while
cnr depends on both the mean and the standard deviation (σ x ) of rois. for
stiff inclusions as the target, higher cnr correlates with better target
visibility, and lower sr translates to a higher difference between the target
and background strains.
we employed mpwc-net++ [4] which has been adapted from pwc-net-irr [3] for use.
the network architecture with the added known operators is shown in fig. 1. the
training schedule is similar to [5], known operators are not present in the
training and only employed during the test phase. the known operators are added
in different pyramid levels. this has the advantage of correcting lateral
displacements in different pyramid levels. the known operators are added to the
last 3 pyramid levels (there are 5 pyramid levels in this network) since the
estimate in the first 2 pyramid levels are not accurate enough and adding the
known operators would propagate the error. the hyper-parameters' values of
unsupervised training and the known operators are given in supplementary
materials.3 results and discussions
kpicture is compared to the following methods: -overwind, an optimization-based
use method [8].-the post-processing method of guo et al. [2], which employs the
output of overwind as the initial displacement (overwind+ guo et al.). -picture,
which penalize epr values outside of feasible range [5]. we decided to compare
with picture instead of spicture [13] (picture with self-supervision) since
self-supervision is not related to the physics of motion. to focus on the
effectiveness of the known operators, we, therefore, provide a comparison to its
corresponding method picture. the proposed known operators can be applied to the
network trained with spicture method as well. we made the network's weight
trained using both picture and spic-ture methods publicly available online at
http://code.sonography.ai. we also employed a similar hyper-parameters and
training schedule for experimental phantom and in vivo data.
the lateral strains of ultrasound rf data collected from three different
locations of the tissue-mimicking breast phantom are depicted in fig. 2, and the
quantitative results are given in table 1. visual inspection of fig. 2 denotes
that the method proposed by gou et al. [2] improves the displacement obtained by
overwind. for example, the inclusion borders in sample 2 are much more clearly
visible. the strain images obtained by kpicture have a much higher quality than
those of picture. furthermore, kpicture has the highest quality strain images
among the compared methods. for example, the inclusion on the bottom in sample 1
(highlighted by the arrows) is clearly visible in kpic-ture, a substantial
improvement over all other methods that do not even show the inclusion.table 1.
quantitative results of lateral strains for experimental phantoms. mean and
standard deviation (±) of cnr (higher is better) and sr (lower is better) of
lateral strains are reported. the pair marked by † is not statistically
significant (p-value > 0.05, using friedman test). the differences between all
other numbers are statistically significant (p-value < 0.05).sample (1) sample (
2 the histograms of epr values of overwind+gou et al., picture and kpicture are
illustrated for the experimental phantom sample (1). to improve visualization,
overwind results are not included because the histogram was similar to that of
overwind+gou et al.. although picture limits the range of epr using a
regularization (eq. 2), some epr values are outside the feasible range. kpicture
further limits the epr values; only a small number of samples are outside of the
physically plausible range.the lateral strain results of in vivo data are
depicted in fig. 3 (b), and axial strains are given in the supplementary
materials (the quality of axial strains is high in all methods). while picture
may produce an adequate strain image, it still contains noisy regions. on the
other hand, kpicture delivers exceptionally refined strain images and surpasses
the other compared methods. the quantitative results given in table 1 also
confirm the visual inspection. the applied known operators and picture assume
that the material is isotropic. their performance on anisotropic materials can
be investigated by experiments on anisotropic tissues such as muscles.
furthermore, 3d imaging data can be collected from 2d arrays to have information
in out-of-plane direction to be able to formulate known operators and picture
loss for anisotropic tissues.it should be noted that after incorporating the
known operators, the inference time of the network increased from an average of
195 ms to 240 ms (having 10 iterations for algorithm 1 and 100 iterations for
algorithm 2).
in this paper, we proposed to incorporate two known operators inside a use
network. the network is trained by physically inspired constraints specifically
designed to tackle the long-standing illusive problem of lateral strain imaging.
the proposed operators provide a refinement in each pyramid level of the
architecture and substantially improve the lateral strain image quality. tissue
mimicking phantom and in vivo results show that the method substantially
outperforms previous displacement estimation method in the lateral direction.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_45.
in computational histopathology, visual representation extraction is a
fundamental problem [14], serving as a cornerstone of the (downstream)
task-specific learning on whole slide pathological images (wsis). our community
has witnessed the progress of the de facto representation learning paradigm from
the supervised imagenet pre-training to self-supervised learning (ssl) [15,36].
numerous pathological applications benefit from ssl, including classification of
glioma [7], breast carcinoma [1], and non-small-cell lung carcinoma [25],
mutation prediction [32], microsatellite instability prediction [31], and
survival prediction from wsis [2,16]. among them, pioneering works [12,22,27]
directly apply the ssl algorithms developed for natural images (e.g., simclr
[10], cpc [30] and moco [11]) to wsi analysis tasks, and the improved
performance proves the effectiveness of ssl. however, wsi is quite different
from natural images in that it exhibits a hierarchical structure with giga-pixel
resolution. following works turn to designing pathological-specific tasks to
explore the inherent characteristics of wsis for representation learning, e.g.,
resolution-aware tasks [18,34,37] and color-aware tasks [2,38]. since the
pretext tasks encourage to mine the pathologically relevant patterns, the
learned representations are expected to be more suitable for wsi analysis.
nevertheless, these works only consider learning the representations at the
patch level, i.e, the cellular organization, but neglecting macro-scale
morphological features, e.g., tissue phenotypes and intra-tumoral heterogeneity.
as a result, there is still a gap between the pre-trained representations and
downstream tasks, as the latter is mainly at the slide level, e.g., subtyping,
grading and staging.more recently, some works propose to close the gap via
directly learning slidelevel representations in pre-training. for instance, hipt
[8], a milestone work, introduces hierarchical pre-training (dino [6]) for the
patch-level (256 × 256) and region-level (4096 × 4096) in a two-stage manner,
achieving superior performance on slide-level tasks. ss-camil [13] uses
efficientnet-b0 for image compression in the first stage and then derives
multi-task learning on the compressed wsis, which assumes the primary site
information, e.g., the organ type, is always available and can be used as pseudo
labels. ss-mil [35] also proposes a two-stage pre-training framework for wsis
using contrastive learning (simclr [10]), where the differently subsampled bags1
from the same wsi are positive pairs in the second stage. a similar idea can be
found in giga-ssl [20] with delicate patch-and wsi-level augmentations. the
aforementioned methods share the same two-stage pre-training paradigm, i.e.,
patch-to-region/slide. thus broader context information is preserved to close
the gap between pretext and downstream tasks. however, they are essentially
instance discrimination where only the self-invariance of region/slide is
considered, leaving the intraand inter-slide semantic structures unexplored.in
this paper, we propose to encode the intra-and inter-slide semantic structures
by modeling the mutual-region/slide relations, which is called slpd: slide-level
prototypical distillation for wsis. specifically, we perform the slide-level
clustering for the 4096 × 4096 regions within each wsi to yield the prototypes,
which characterize the medically representative patterns of the tumor (e.g.,
morphological phenotypes). in order to learn this intra-slide semantic
structure, we encourage the region representations to be closer to the assigned
prototypes. by representing each slide with its prototypes, we further select
semantically simi- lar slides by the set-to-set distance of prototypes. then, we
learn the inter-slide semantic structure by building correspondences between
region representations and cross-slide prototypes. we conduct experiments on two
benchmarks, nsclc subtyping and brca subtyping. slpd achieves state-of-the-art
results on multiple slide-level tasks, demonstrating that representation
learning of semantic structures of slides can make a suitable proxy task for wsi
analysis. we also perform extensive ablation studies to verify the effectiveness
of crucial model components.
as shown in fig. 1(a), a wsi exhibits hierarchical structure at varying
resolutions under 20× magnification: 1) the 4096×4096 regions describing
macro-scale organizations of cells, 2) the 256 × 256 patches capturing local
clusters of cells, 3) and the 16 × 16 images characterizing the fine-grained
features at the celllevel. given n unlabeled wsis, where l n denotes the number
of regions of wsi w n , we aim to learn a powerful encoder that maps each x l n
to an embedding z l n ∈ r d . slpd is built upon the two-stage pre-training
paradigm proposed by hipt, which will be described in sect. 2.2. fig 1(c-d)
illustrates the pipeline of slpd. we characterize the semantic structure of
slides in sect. 2.2, which is leveraged to establish the relationship within and
across slides, leading to the proposed intraand inter-slide distillation in
sect. 2.4 and sect. 2.5.
we revisit hierarchical image pyramid transformer (hipt) [8], a cutting-edge
method for learning representations of wsis via self-supervised vision
transformers. as shown in fig. 1 hipt leverages dino [6] to pre-train vit 256
-16 and vit 4096 -256, respectively. the learning objective of dino is
self-distillation. taking stage two as an example, dino distills the knowledge
from teacher to student by minimizing the cross-entropy between the probability
distributions of two views at region-level:where h(a, b) = -a log b, and p d is
the data distribution that all regions are drawn from. the teacher and the
student share the same architecture consisting of an encoder (e.g., vit) and a
projection head g t /g s . ẑ and z are the embeddings of two views at
region-level yielded by the encoder. the parameters of the student are
exponentially moving averaged to the parameters of the teacher.
many histopathologic features have been established based on the morphologic
phenotypes of the tumor, such as tumor invasion, anaplasia, necrosis and
mitoses, which are then used for cancer diagnosis, prognosis and the estimation
of response-to-treatment in patients [3,9]. to obtain meaningful representations
of slides, we aim to explore and maintain such histopathologic features in the
latent space. clustering can reveal the representative patterns in the data and
has achieved success in the area of unsupervised representation learning
[4,5,24,26].to characterize the histopathologic features underlying the slides,
a straightforward practice is the global clustering, i.e., clustering the region
embeddings from all the wsis, as shown in the left of fig. 1(d). however, the
obtained clustering centers, i.e., the prototypes, are inclined to represent the
visual bias related to staining or scanning procedure rather than medically
relevant features [33]. meanwhile, this clustering strategy ignores the
hierarchical structure "region→wsi→whole dataset" underlying the data, where the
id of the wsi can be served as an extra learning signal. therefore, we first
consider the slidelevel clustering that clusters the embeddings within each wsi,
which is shown in the right of fig. 1(d). specifically, we conduct k-means
algorithm before the start of each epoch over l n region embeddings {z l n } ln
l=1 of w n to obtain m prototypes {c m n ∈ r d } m m=1 . similar operations are
applied across other slides, and then we acquire n groups of prototypes {{c m n
} m m=1 } n n=1 . each group of prototypes is expected to encode the semantic
structure (e.g., the combination of histopathologic features) of the wsi.
the self-distillation utilized by hipt in stage two encourages the
correspondence between two views of a region at the macro-scale because the
organizations of cells share mutual information spatially. however, the
self-distillation, which solely mines the spatial correspondences inside the
4096 × 4096 region, cannot comprehensively understand the histopathologic
consistency at the slide-level. in order to achieve better representations, the
histopathologic connections between the wsi and its regions should be modeled
and learned, which is called intraslide correspondences. with the proposed
slide-level clustering in sect. 2.3, a slide can be abstracted by a group of
prototypes, which capture the semantic structure of the wsi. as shown in fig.
1(e), we assume that the representation z and its assigned prototype c also
share mutual information and encourage z to be closer to c with the intra-slide
distillation:we omit super-/sub-scripts of z for brevity. through eq. 2, we can
leverage more intra-slide correspondences to guide the learning process. for
further understanding, a prototype can be viewed as an augmented representation
aggregating the slide-level information. thus this distillation objective is
encoding such information into the corresponding region embedding, which makes
the learning process semantic structure-aware at the slide-level.
tumors of different patients can exhibit morphological similarities in some
respects [17,21], so the correspondences across slides should be characterized
during learning. previous self-supervised learning methods applied to
histopathologic images only capture such correspondences with positive pairs at
the patchlevel [22,23], which overlooks the semantic structure of the wsi. we
rethink this problem from the perspective how to measure the similarity between
two slides accurately. due to the heterogeneity of the slides, comparing them
with the local crops or the averaged global features are both susceptible to
being one-sided. to address this, we bridge the slides with their semantic
structures and define the semantic similarity between two slides w i and w j
through an optimal bipartite matching between two sets of prototypes:where
cos(•, •) measures the cosine similarity between two vectors, and s m enumerates
the permutations of m elements. the optimal permutation σ * can be computed
efficiently with the hungarian algorithm [19]. with the proposed setto-set
distance, we can model the inter-slide correspondences conveniently and
accurately. specifically, for a region embedding z belonging to the slide w and
assigned to the prototype c, we first search the top-k nearest neighbors of w in
the dataset based on the semantic similarity, denoted as { ŵk } k k=1 . second,
we also obtain the matched prototype pairs {(c, ĉk )} k k=1 determined by the
optimal permutation, where ĉk is the prototype of ŵk . finally, we encourage z
to be closer to ĉk with the inter-slide distillation:the inter-slide
distillation can encode the sldie-level information complementary to that of
intra-slide distillation into the region embeddings.the overall learning
objective of the proposed slpd is defined as:where the loss scale is simply set
to α 1 = α 2 = 1. we believe the performance can be further improved by tuning
this.
datasets. we conduct experiments on two public wsi datasets for downstream
tasks. with the pre-extracted embeddings, we fine-tune three aggregators (i.e.,
mil [28], ds-mil [22] and vit wsi -4096 [8]) for 20 epochs and follow other
settings in the official code of hipt.evaluation metrics. we adopt the 10-fold
cross validated accuracy (acc.) and area under the curve (auc) to evaluate the
weakly-supervised classification performance. the data splitting scheme is kept
consistent with hipt.
we conduct experiments on two slide-level classification tasks, nsclc subtyping
and brca subtyping, and report the results in table 1. the region-level
embeddings generated by slpd outperform the patch-level embeddings across two
aggregators3 and two tasks (#1∼ 5). this illustrates that learning
representations with broader image contexts is more suitable for wsi
analysis.compared with the strong baseline, i.e., the two-stage pre-training
method proposed by hipt (#6), slpd achieves performance increases of 1.3% and
3.2% auc on nsclc and brca (#9). nontrivial performance improvements are also
observed under knn evaluation (#10 vs.#13): +2.3% and +3.1% auc on nsclc and
brca. the superior performance of slpd demonstrates that learning
representations with slide-level semantic structure appropriately can
significantly narrow the gap between pre-training and downstream slide-level
tasks. moreover, intra-slide and inter-slide distillation show consistent
performance over the baseline, corroborating the effectiveness of these critical
components of slpd.
different clustering methods. as discussed in sect. 2.3, we can alternatively
use the global clustering to obtain prototypes and then optimize the network
with a similar distillation objective as eq. 2. for a fair comparison, the total
number of prototypes of the two clustering methods is approximately the same.
different inter-slide distillations. the proposed inter-slide distillation is
semantic structure-aware at the slide-level, since we build the correspondence
between the region embedding and the matched prototype (#4 in table 2). to
verify the necessity of this distillation method, we turn to another design
where the inter-slide correspondence is explored through two nearest region
embeddings across slides (#3 in table 2). as can be seen, the region-level
correspondences lead to inferior performances, even worse than the baseline (#5
in table 1), because the learning process is not guided by the slide-level
information.
as shown in table 2(#5∼7), the performance of slpd is relatively robust to the
number of prototypes on nsclc, but is somewhat affected by it on brca. one
possible reason is that the heterogeneity of invasive breast carcinoma is low
[29], and thus the excessive number of prototypes cannot obtain medically
meaningful clustering results. empirically, we set m = 4 on nsclc and m = 2 on
brca as the default configuration. we suggest the optimal number of prototypes
should refer to clinical practice, by considering tissue types, cell morphology,
gene expression and other factors.number of slide neighbors. as demonstrated in
table 2(#5∼7), the performance of slpd is robust to the number of slide
neighbors. considering that more slide neighbors require more computation
resources, we set k = 1 as the default configuration. for more results, please
refer to the supplementary.
this paper reflects on slide-level representation learning from a novel
perspective by considering the intra-and inter-slide semantic structures. this
leads to the proposed slide-level prototypical distillation (slpd), a new
self-supervised learning approach achieving the more comprehensive understanding
of wsis. slpd leverages the slide-level clustering to characterize semantic
structures of slides. by representing slides as prototypes, the
mutual-region/slide relations are further established and learned with the
proposed intra-and inter-slide distillation. extensive experiments have been
conducted on multiple wsi benchmarks and slpd achieves state-of-the-art results.
though slpd is distillation-based, we plan to apply our idea to other
pre-training methods in the future, e.g., contrastive learning [10,11].
256 -16 is freezed and leveraged to tokenize the patches within 4096 × 4096
regions. then a region-level vision transformer vit 4096 -256 aggregates these
tokens to obtain region-level representations. with this hierarchical
aggregation strategy, a wsi can be represented as a bag of region-level
representations, which are then aggregated with another vision transformer, vit
wsi -4096, to perform slide-level prediction tasks.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_25.
nuclei segmentation in histopathology images is an important task for cancer
diagnosis and immune response prediction [1,13,18]. while several fully
supervised deep learning approaches to segment nuclei exist [2,6,8,9,19,25],
labeling s. nam and j. jeong-equal contribution.thousands of instances are
tedious and the ambiguous nature of nuclei boundaries requires high-level expert
annotators. to address this, weakly-supervised nuclei segmentation methods
[5,10,15,20,23,28] have emerged as an attractive alternative using cheap and
inexact labels e.g., center point annotations. as point labels alone do not
provide sufficient foreground information, it is common to use euclidean
distance-based voronoi diagrams and k-means clustering [7] to generate pseudo
segmentation labels for training. however, since euclidean distance-based
schemes only use distance information while ignoring color, they often fail to
capture nuclei shape information; resulting in inadequate boundary delineation
between adjacent nuclei. moreover, prior methods [17,21,22] typically assume
that point labels are located precisely at the center of the nuclei. in
real-world scenarios, point annotation locations may shift from nuclei centers
as a result of the expert labeling process, leading to a lower performance after
model training.to overcome these challenges, we propose a novel weakly
supervised instance segmentation method that effectively distinguishes adjacent
nuclei and is robust to point shifts. the proposed model consists of three
modules responsible for binary segmentation, boundary delineation, and instance
separation. to train the binary segmentation module, we generate pseudo binary
segmentation masks using geodesic distance-based voronoi labels and cluster
labels from point annotations. geodesic distance provides more precise nuclei
shape information than previous euclidean distance-based schemes. to train the
offset map module, we generate pseudo offset maps by computing the offset
distance between binary segmentation pixel predictions and the point label. the
offset information facilitates precise delineation of the boundaries between
adjacent nuclei. to make the model robust to center point shifts, we introduce
an expectation maximization (em) [4] algorithm-based process to refine point
labels. note that previous approaches [17,21,22] optimize model parameters only
using a fixed set of point labels, while we instead alternatively update model
parameters and the center point locations. this refinement process ensures that
the model maintains high performance even when the point annotation is not
exactly located at the center of the nuclei.the contributions of this paper are
as follows: (1) we propose an end-toend weakly supervised segmentation model
that simultaneously predicts binary mask, offset map, and center map to
accurately identify and segment nuclei.(2) by utilizing geodesic distance, we
produce more detailed voronoi and cluster labels that precisely delineate the
boundary between adjacent nuclei. (3) we introduce an em algorithm-based
refinement process to encourage model robustness on center-shifted point labels.
(4) ablation and evaluation studies on two public datasets demonstrate our
model's ability to outperform state-of-the-art techniques not only with ideal
labels but also with shifted labels.
we propose an end-to-end nuclei segmentation method that only uses point
annotations p to predict nuclei instance segmentation masks ŝ. the proposed fig.
1. overview of the proposed method. it consists of an encoder and three modules
for binary segmentation, offset map and center map prediction. to train offset
map and center map modules(blue lines), pseudo labels are generated using point
label and predicted binary segmentation mask(green lines). during inference, the
instance map, obtained by predicted offset map and center map, is multiplied
with predicted binary mask to produce instance segmentation prediction(orange
lines). (color figure online) model consists of three modules: 1) binary
segmentation module, 2) offset map module, and 3) center map module (fig. 1).
for a given input image, we extract feature maps with an imagenet-pretrained
vgg16 backbone encoder. the feature maps are further processed through a series
of residual units (rus) and attention units (aus) to predict a binary
segmentation mask b, an offset map ô, and a center map ĉ. the rus are employed
to maintain feature information so that subsequent modules can reuse the
features from early-stage modules. in contrast, the aus are used to refine the
features of initial modules by using the predictions of later modules. in
particular, the aus use the point predictions to refine the features in the
offset module, and the offset predictions to refine the features in the binary
module.in the training stage, we first generate a voronoi label v and a cluster
label k along the green lines in fig. 1 to train the segmentation module. then,
we generate the pseudo offset map o by using b and p . next, following [29], we
generate the center map c by expanding the point label p with gaussian kernel
within a radius r. herein, our model is trained wih a segmentation loss l b
(v,k, b), an offset map loss l o (o, ô), and a center map loss l c (c, ĉ). note
that p can not sufficiently enable model robustness to imprecise point
annotations. thus, we employ an em algorithm to search the optimal model
parameters θ to obtain more reliable points p .in the inference stage, b, ô, ĉ
are predicted following the orange lines in fig. 1. then, we generate an
instance map i, which shares the same values among the same instances as
follows:where (x, y) represents a coordinate and (x ĉi , y ĉi ) means the
location of i th point obtained from ĉ. finally, the instance segmentation
output ŝ is obtained by b × i. 2.1 loss functions using pseudo labels
segmentation loss. we generate v and k to train the binary segmentation module.
in [21], v was generated based on euclidean distance between points without
considering color information. as a result, the voronoi boundaries are often
created across nuclei instances, and the offset map's quality was limited. to
mitigate this, we instead generate v using geodesic distance [3,24] by computing
distances d i between all center points p i ∈ p and pixels. the boundaries of
the diagram in v are defined as 0, while center points and the other regions are
defined as 1 and 2, respectively. for k-means clustering, we concatenate the rgb
values and the geodesic distance value d i truncated by d * to generate the
feature vectorswe cluster f into three clusters (0 for background, 1 for
foreground, and 2 for ignore) to generate k (fig. 2d). to train the binary
segmentation module using v and k, we employ a voronoi loss l v and a cluster
loss l k based on the cross-entropy:where ω v and ω k are the set of foreground
and background pixels in v and k, n ωv and n ωk denote the cardinality of ω v
and ω k . following [17], we define the final segmentation loss ascenter map
loss. to achieve instance-level predictions, we introduce a center map module.
the module predicts a keypoint heatmap ĉ ∈ [0, 1] w ×h where ĉ = 1 identifies
nuclei centers and ĉ = 0 for other pixels. w and h are the width and height of
the input image. to train the module we employ a focal loss, commonly used in
point detection problems. this loss can focus on a set of sparse hard examples
while preventing easy negatives from dominating the model [16]:where n p denotes
the number of point labels. we set the focal loss hyperparameters α = 2 and β =
4 following [14,29]. by placing the center map module at the end of the model,
the model is able to retain center point information along the rus, so that each
module can inherently reflect the information into their predictions. offset map
loss. we employ an offset map module that considers the shape of each nucleus to
improve boundary detection. inspired by [2], we define an offset vector o(x, y)
that indicates the displacement of a point (x, y) to the center of its
corresponding nucleus. to train the offset module, we first compute o(x, y) of
each nucleus segmented by b. then, l o is defined as an l1 loss:it is worth
noting that in the early stages of training, the pseudo offset map o generated
by b and p is unreliable. thus, we empirically use l o for backpropagation after
20 epochs. we optimize the entire model using the loss, where λ b , λ o and λ c
denote loss weights.
training with nuclei (center) shifted point labels can lead to blurry center map
predictions (see fig. 3c). this in turn limits model optimization and it's
ability to distinguish objects, resulting in poor adjacent nuclei segmentation.
to address this, we propose an em based center point refinement process. instead
of the standard fixed-point label based model optimization, we alternatively
optimize both model parameters and point labels.in the e-step, we update the
center of each nucleus according to ô. we use ô to generate refined point labels
p , since ô is reliable regardless of the point location i.e., center of the
nuclei or shifted.where v i is i th voronoi region and p i is the refined center
point. we repeat this for all voronoi regions to obtain p , and replace p with p
if the distance between them is < δ. in the m -step of iteration n, we generate
c by adapting the gaussian mask to p , and then use it to train offset and
center map modules. as maximizing a probability distribution is the same as
minimizing the loss, the model parameter θ minimizing l is optimized as:since
reliable ô is necessary to refine nuclei centers, refinement starts after 30
epochs. e and m steps are alternately repeated to correct imprecise annotations
bringing them closer to the real nuclei center points.
dataset. to validate the effectiveness of our model, we use two public nuclei
segmentation datasets i.e., cpm17 [26] & monuseg [12]. cpm17 contains 64 h&e
stained images with 7,570 annotated nuclei boundaries sized from 500×500 to
600×600. the set is split into 32/32 images for training and testing. images
were normalized and cropped to 300×300. monuseg is a multi-organ nuclei
segmentation dataset consisting of 30 h&e stained images (1000×1000) extracted
from seven different organs. we used 16 images (4 images from the breast, liver,
kidney, and prostate) as training and 14 images (2 images from each breast,
liver, kidney, prostate, bladder, brain, and stomach) as testing. for a fair
comparison, images were pre-processed before training/testing i.e., normalized
and cropped to 250×250 patches following the setting used in [17].to make point
labels, we use the center point of full mask annotations. for a realistic
scenario, we generate shifted point label. the shift is performed in pixels and
is randomly selected between the minimum and maximum values.implementation
details. for training, all evaluated models were run for 150 epochs with the
adam optimizer [11] using a learning rate of 1e-4, weight decay of 3e-2, and
batch size of 4. the geodistk [27] library was used to compute geodesic
distances. for clustering, we set the maximum distance d * as 90 and 70 on cpm17
and monuseg, respectively. the gaussian kernel r was set as r = 6 and δ was set
as 8 for refinement on cpm17. for monuseg, r = 8 and δ = 8, respectively. a
threshold of 0.2 was applied to eliminate the noise and find important points in
ĉ. finally, a variety of augmentations were employed i.e., random resizing,
cropping, and rotations etc., following [17], with loss weights λ b , λ o and λ
c empirically set to 1. we used a nvidia rtx a5000 gpu and pytorch version
1.7.1. [21] 75.0 55.5 75.3 56.9 74.4 53.7 72.2 49.9 70.1 44.9 69.9 45.0 66.3
39.9 61.0 31.5 mixed anno [22] 75.3 53.2 75.9 55.5 73.3 52.3 73.1 49.9 73.3 51.6
72.0 49.4 66.0 40.5 66.9 41.8 spn+ien [17] 74. main results. table 1 shows the
performance of our method against stateof-the-art weakly supervised nuclei
segmentation methods [17,21,22] based on dice and aggregated jaccard index (aji)
metrics. as opposed to the dice score, aji is key when evaluating adjacent
nuclei separation in instance segmentation tasks. on cpm17, our method
outperformed the prior approach by a large margin of +3.4% in dice and +7.2% in
aji when the point label is located at the nuclei center. more importantly, our
approach surpassed prior approaches by a substantial margin when the shift
exists. we obtain statistically significant (p-value <0.05) for the aji of all
comparison methods on two datasets in all scenarios. regarding refinement, we
observed that our strategy is more beneficial when points exhibit significant
shifts i.e., on both cpm and monuseg. figure 3 showcases the effectiveness of
the refinement process wherein the model generates precise instance and center
maps. with the geodesic distance and the refinement process, our proposed method
achieved state-of-the-art performance. this demonstrates that our method
separates adjacent nuclei accurately, and maintains its robustness, achieving
consistent performance even when the point annotations are not located at the
center of the nuclei. additionally, in fig. 4, we qualitatively show the results
to highlight how our method precisely separates adjacent nuclei. ablation
studies. we conducted ablation studies to assess the impact of the offset
regression module, geodesic distance, and point refinement process (table 2).
when the binary segmentation module is combined only with the center map module
without the offset module, the model could separate nuclei only trained by the
ideal label. on the other hand, since there was no refinement process due to the
absence of the offset map, inaccurate points extracted from the center map are
obtained in the real-world scenario. we also demonstrate that labels with
geodesic distance help improve overall performance. this is because it creates
confident labels and more decent divides the boundaries between nuclei. finally,
using the full set of modules along with a complete instance map, the model was
able to separate adjacent nuclei with precise boundaries, ultimately reporting
higher scores. these findings validate the utility of the center map and offset
map modules i.e., they synergistically facilitate precise instance delineation
and nuclei boundary prediction. the geodesic distance and refinement process
also improved the accuracy by contributing to more accurate pseudo labels.
especially, most variants show a significant drop in performance when the
annotations shift was over 4 pixels. compared to other variants, our proposed
model is more robust to the point shift in both datasets.
in this work, we proposed a novel and robust framework for weakly supervised
nuclei segmentation. we demonstrated the effectiveness of geodesic distancebased
voronoi diagrams and k-means clustering to generate accurate pseudo binary
segmentation labels. this allowed us to generate reliable pseudo offset maps,
and then we iteratively improve the pseudo offset maps that facilitate the
precise separation of adjacent nuclei as well as progressively refine the
location of the center point labels. according to our experimental results, we
established a new state-of-art on two publicly available datasets across
different levels of point annotation imperfections. we believe being able to use
low-precision point annotations while retaining good segmentation performance is
an essential step for automatic nuclei segmentation models to become a
widespread tool in realworld clinical practice.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_51.
in recent years, the workload of radiologists has grown drastically, quadrupling
from 2006 to 2020 in western europe [4]. this huge increase in pressure has led
to long patient-waiting times and fatigued radiologists who make more mistakes
[3]. the most common of these errors is underreading and missing anomalies
(42%); followed by missing additional anomalies when concluding their search
after an initial finding (22%) [10]. interestingly, despite the challenging work
environment, only 9% of errors reviewed in [10] were due to mistakes in the
clinicians' reasoning. therefore, there is a need for automated second-reader
capabilities, which brings any kind of anomalies to the attention of
radiologists. for such a tool to be useful, its ability to detect rare or
unusual cases is particularly important. traditional supervised models would not
be appropriate, as acquiring sufficient training data to identify such a broad
range of pathologies is not feasible. unsupervised or self-supervised methods to
model an expected feature distribution, e.g., of healthy tissue, is therefore a
more natural path, as they are geared towards identifying any deviation from the
normal distribution of samples, rather than a particular type of pathology.there
has been rising interest in using end-to-end self-supervised methods for anomaly
detection. their success is most evident at the miccai medical
outof-distribution analysis (mood) challenge [31], where all winning methods
have followed this paradigm so far (2020-2022). these methods use the variation
within normal samples to generate diverse anomalies through sample mixing
[7,[23][24][25]. however all these methods lack a key component: structured
validation. this creates uncertainty around the choice of hyperparameters for
training. for example, selecting the right training duration is crucial to avoid
overfitting to proxy tasks. yet, in practice, training time is often chosen
arbitrarily, reducing reproducibility and potentially sacrificing generalisation
to real anomalies.
we propose a cross-validation framework, using separate selfsupervision tasks to
minimise overfitting on the synthetic anomalies that are used for training. to
make this work effectively we introduce a number of non-trivial and
seamlessly-integrated synthetic tasks, each with a distinct feature set so that
during validation they can be used to approximate generalisation to unseen,
real-world anomalies. to the best of our knowledge, this is the first work to
train models to directly identify anomalies on tasks that are deformation-based,
tasks that use poisson blending with patches extracted from external datasets,
and tasks that perform efficient poisson image blending in 3d volumes, which is
in itself a new contribution of our work. we also introduce a synthetic anomaly
labelling function which takes into account the natural noise and variation in
medical images. together our method achieves an average precision score of 76.2
for localising glioma and 78.4 for identifying pathological chest x-rays, thus
setting the state-of-the-art in self-supervised anomaly detection.
the most prevalent methods for self-supervised anomaly detection are based on
generative auto-encoders that analyse the residual error from reconstructing a
test sample. this is built on the assumption that a reconstruction model will
only be able to correctly reproduce data that is similar to the instances it has
been trained on, e.g. only healthy samples. theoretically, at test time, the
residual reconstruction error should be low for healthy tissues but high for
anomalous features. this is an active area of research with several recent
improvements upon the initial idea [22], e.g., [21] applied a diffusion model to
a vq-vae [27] to resample the unlikely latent codes and [30] gradually
transition from a u-net architecture to an autoencoder over the training process
in order to improve the reconstruction of finer details. several other methods
aim to ensure that the model will not reproduce anomalous regions by training it
to restore samples altered by augmentations such as masking out regions [32],
interpolating heavily augmented textures [29] or adding coarse noise [9]. [5]
sought to identify more meaningful errors in image reconstructions by comparing
the reconstructions of models trained on only healthy data against those trained
on all available data.however, the general assumption that reconstruction error
is a good basis for an anomaly scoring function has recently been challenged.
auto-encoders are unable to identify anomalies with extreme textures [16], are
reliant on empirical post-processing to reduce false-positives in healthy
regions [2] and can be outperformed by trivial approaches like thresholding of
flair mri [15].self-supervised methods take a more direct approach, training a
model to directly predict an anomaly score using synthetic anomalies. foreign
patch interpolation (fpi) [24] was the first to do this at a pixel-level, by
linearly interpolating patches extracted from other samples and predicting the
interpolation factor as the anomaly score. similar to cutpaste [11], [7] fully
replaces 3d patches with data extracted from elsewhere in the same sample, but
then trains the model to segment the patches. poisson image interpolation (pii)
[25] seamlessly integrates sample patches into training images, preventing the
models from learning to identify the anomalies by their discontinuous
boundaries. natural synthetic anomalies (nsa) [23] relaxes patch extraction to
random locations in other samples and introduces an anomaly labelling function
based on the changes introduced by the anomaly. some approaches combine
self-supervised and reconstruction-based methods by training a discriminator to
compute more exact segmentations from reconstruction model errors [6,29]. other
approaches have also explored contrasting self-supervised learning for anomaly
detection [12,26].
the core idea of our method is to use synthetic tasks for both training and
validation. this allows us to monitor performance and prevent overfitting, all
without the need for real anomalous data. each self-supervised task involves
introducing a synthetic anomaly into otherwise normal data whilst also producing
the corresponding label. since the relevant pathologies are unknown a priori, we
avoid simulating any specific pathological features. instead, we use a wide
range of subtle and well-integrated anomalies to help the model detect many
different kinds of deviations, ideally including real unforeseen anomalies. in
our experiments, we use five tasks, but more could be used as long as each one
is sufficiently unique. distinct tasks are vital because we want to use these
validation tasks to estimate the model's generalisation to unseen classes of
anomalies. if the training and validation tasks are too similar, the performance
on the validation set may be an overly optimistic estimate of how the model
would perform on unseen real-world anomalies.when performing cross-validation
over all synthetic tasks and data partitions independently, the number of
possible train/validation splits increases significantly, requiring us to train
f • (t n ct ) independent models, where t n is the total number of tasks, t is
the number of tasks used to train each model and f is the number of data folds,
which is computationally expensive. instead, as in our case t n = f = 5, we opt
to associate each task with a single fold of the training data (fig. 1). we then
apply 5ct -fold cross-validation over each combination. in each iteration, the
corresponding data folds are collected and used for training or validation,
depending on which partition forms the majority. synthetic tasks: figure 2 shows
examples of our self-supervised tasks viewed in both one and two dimensions.
although each task produces visually distinct anomalies, they fall into three
overall categories, based on blending, deformation, or intensity variation.
also, all tasks share a common recipe: the target anomaly mask m h is always a
randomly sized and rotated ellipse or rectangle (ellipsoids/cuboids in 3d); all
anomalies are positioned such that at least 50% of the mask intersects with the
foreground of the image; and after one augmentation is applied, the process is
randomly repeated (based on a fair coin toss, p = 0.5), for up to a maximum of 4
anomalies per image.the intra-dataset blending task. poisson image blending is
the current state-of-the-art for synthetic anomaly tasks [23,25], but it does
not scale naturally to more than two dimensions or non-convex interpolation
regions [17]. therefore, we extend [20] and propose a d-dimensional variant of
poisson image editing following earlier ideas by [17].poisson image editing [20]
uses the image gradient to seamlessly blend a patch into an image. it does this
by combining the target gradient with dirichlet boundary conditions to define a
minimisation problem min fin h |∇f in -v| also, by defining h as the
axis-aligned bounding box of m h , we can ensure the boundaries coincide with
coordinate lines. this enables us to use the fourier transform method to solve
this partial differential equation [17], which yields a direct relationship
between fourier coefficients of δf in and v after padding to a symmetric image.
to simplify for our use case, an image with shape n 0 × • • •×n d-1 , we replace
the fourier transformation with a discrete sine transform (dst. this follows as
a dst is equivalent to a discrete fourier transform of a real sequence that is
odd around the zeroth and middle points, scaled by 0.5, which can be established
for our images. with this, the poisson equation becomes congruent to a
relationship of the coefficients,vd where v=(v 0 , ..., v d-1 ) and v is the dst
of each component. the solution for fu can then be computed in dst space by
dividing the right side through the terms on the left side and the destination
image can be obtained through x i = dst -1 ( fu ). because this approach uses a
frequency transform-based solution, it may slightly alter areas outside of m h
(where image gradients are explicitly edited) in order to ensure the changes are
seamlessly integrated. we refer to this blending process as x = p oissonblend(x
i , x j , m h ) in the following. the intra-dataset blending task therefore
results from xintra = p oissonblend(x, x , m h ) with x, x ∈ d with samples from
a common dataset d and is therefore similar to the self-supervision task used in
[23] for 2d images.the inter-dataset blending task follows the same process as
intra-dataset blending but uses patches extracted from an external dataset d ,
allowing for a greater variety of structures. therefore, samples from this task
can be defined as xinter = p oissonblend(x, x , m h ) with x ∈ d, x ∈ d .the
sink/source tasks shift all points in relation to a randomly selected
deformation centre c. for a given point p, we resample intensities from a new
location p. to create a smooth displacement centred on c, we consider the
distance p-c 2 in relation to the radius of the mask (along this direction), d.
the extent of this displacement is controlled by the exponential factor f > 1.
for example, the sink task (eqn. 1) with a factor of f = 2 would take the
intensity at 0.75d and place it at 0.5d, effectively pulling these intensities
closer to the centre. note that unlike the sink equation in [24] this
formulation cannot sample outside of the boundaries of p m h meaning it
seamlessly blends into the surrounding area. the source task (eqn. 2) performs
the reverse, appearing to push the pixels away from the centre by sampling
intensities towards it.the smooth intensity change task aims to either add or
subtract an intensity over the entire anomaly mask. to avoid sharp
discontinuities at the boundaries, this intensity change is gradually dampened
for pixels within a certain margin of the boundary. this smoothing starts at a
random distance from the boundary, d s , and the change is modulated by d p /d s
.anomaly labelling: in order to train and validate with multiple tasks
simultaneously we use the same anomaly labelling function across all of our
tasks. the scaled logistic function, used in nsa [23], helps to translate raw
intensity changes into more semantic labels. but, it also rounds imperceptible
differences up to a minimum score of about 0.1. this sudden and arbitrary jump
creates noisy labels and can lead to unstable training. we correct this semantic
discontinuity by computing labels as [23]. this flipped gaussian shape is c1
continuous and smoothly approaches zero, providing consistent labels even for
smaller changes.
data: we evaluate our method on t2-weighted brain mr and chest x-ray datasets to
provide direct comparisons to state-of-the-art methods over a wide range of real
anomalies. for brain mri we train on the human connectome project (hcp) dataset
[28] which consists of 1113 mri scans of healthy, young adults acquired as part
of a scientific study. to evaluate, we use the brain tumor segmentation
challenge 2017 (brats) dataset [1], containing 285 cases with either high or low
grade glioma, and the ischemic stroke lesion segmentation challenge 2015 (isles)
dataset [13], containing 28 cases with ischemic stroke lesions. the data from
both test sets was acquired as part of clinical routine. the hcp dataset was
resampled to have 1mm isotropic spacing to match the test datasets. we apply
z-score normalisation to each sample and then align the bounding box of each
brain before padding it to a size of 160 × 224 × 160. lastly, samples are
downsampled by a factor of two.for chest x-rays we use the vindr-cxr dataset
[18] including 22 different local labels. to be able to compare with the
benchmarks reported in [6] we use the same healthy subset of 4000 images for
training along with their test set (ddad ts ) of 1000 healthy and 1000 unhealthy
samples, with some minor changes outlined as follows. first note that [6]
derives vindr-cxr labels using the majority vote of the 3 annotators.
unfortunately, this means there are 52 training samples, where 1/3 of
radiologists identified an anomaly, but the majority label is counted as
healthy. the same applies to 10 samples within the healthy testing subset. to
avoid this ambiguity, we replace these samples with leftover training data that
all radiologists have labelled as healthy. we also evaluate using the true test
set (vindr ts ), where two senior radiologists have reviewed and consolidated
all labels. for preprocessing, we clip pixel intensities according to the window
centre and width attributes in each dicom file, and apply histogram
equalisation, before scaling intensities to the range [-1, 1]. finally, images
are resized to 256 × 256. • indicates that the metrics are evaluated over the
same region and at the same resolution as cradl [12]. upper right part: metrics
on vindr-cxr, presented as ap/auroc on the vindr and ddad test splits. random is
the baseline performance of a random classifier. lower part: a sensitivity
analysis of the average ap of each individual fold (mean±s.d.) alongside that of
the model ensemble, varying how many tasks we use for training versus
validation. best results are highlighted in bold. comparison to state-of-the-art
methods: validating on synthetic tasks is one of our main motivations; as such,
we use a 1/4 (train/val.) task split to compare with benchmark methods. for
brain mri, we evaluate results at the slice and voxel level, computing average
precision (ap) and area under the receiver operating characteristic curve
(auroc), as implemented in scikit learn [19]. note that the distribution shift
between training and test data (research vs. clinical scans) adds further
difficulty to this task. in spite of this, we substantially improve upon the
current state-of-the-art (table 1 upper left). in particular, we achieve a
pixel-wise ap of 76.2 and 45.9 for brats and isles datasets respectively. to
make our comparison as faithful as possible, we also re-evaluate after
post-processing our predictions to match the region and resolution used by
cradl, where we see similar improvement. qualitative examples are shown in fig.
3. note that all baseline methods use a validation set consisting of real
anomalous samples from brats and isles to select which anomaly scoring function
to use. we, however, only use synthetic validation data. this further verifies
that our method of using synthetic data to estimate generalisation works
well.for both vindr-cxr test sets we evaluate at a sample and pixel level,
although previous publications have only reported their results at a sample
level. we again show performance above the current state-of-the-art (table 1
upper right). our results are also substantially higher than previously proposed
selfsupervised methods, improving on the current state-of-the-art nsa [23] by
12.6 to achieve 78.4 image-level ap. this shows that our use of synthetic
validation data succeeds where their fixed training schedule fails.
we also investigate how performance changes as we vary the number of tasks used
for training and validation (table 1 lower). for vindr-cxr, in an individual
fold, the average performance increases as training becomes more diverse (i.e.
more tasks); however, the performance of the ensemble plateaus. having more
training tasks can help the model to be sensitive to a wider range of anomalous
features. but as the number of training tasks increases, so does the overlap
between different models in the ensemble, diminishing the benefit of pooling
predictions. this could also explain why the standard deviation (across folds)
decreases as the number of training tasks increases, since the models are
becoming more similar. our best configuration is close to being competitive with
the state-of-the-art semi -supervised method ddad-asr [6]. even though their
method uses twice as much training data, as well as some real anomalous data,
our purely synthetic method begins to close the gap (ap of [6] 84.3 vs. ours
80.7 on ddad ts ). for the brain datasets, all metrics generally decrease as the
number of training tasks increases. this could be due to the distribution shift
between training and test data. although more training tasks may increase
sensitivity to diverse irregularities, this can actually become a liability if
there are differences between (healthy) training and test data (e.g. acquisition
parameters). more sensitive models may then lead to more "false" positives.
we demonstrate the effectiveness of our method in multiple settings and across
different modalities. a unique aspect of the brain data is the domain shift. the
hcp training data was acquired at a much higher isotropic resolution than the
brats and isles test data, which are both anisotropic. here we achieve the best
performance using more tasks for validation, which successfully reduces
overfitting and hypersensitivity. incorporating greater data augmentations, such
as simulating anisotropic spacing, could further improve results by training the
model to ignore these transformations. we also achieve strong results for the
x-ray data, although precise localisation remains a challenging task. the gap
between current performance and clinicially useful localisation should therefore
be high priority for future research.
in this work we use multiple synthetic tasks to both train and validate
selfsupervised anomaly detection models. this enables more robust training
without the need for real anomalous training or validation data. to achieve this
we propose multiple diverse tasks, exposing models to a wide range of anomalous
features. these include patch blending, image deformations and intensity
modulations. as part of this, we extend poisson image editing to images of
arbitrary dimensions, enabling the current state-of-the-art tasks to be applied
beyond just 2d images. in order to use all of these tasks in a common framework
we also design a unified labelling function, with improved continuity for small
intensity changes. we evaluate our method on both brain mri and chest x-rays and
achieve state-of-the-art performance and above. we also report pixel-wise
results, even for the challenging case of chest x-rays. we hope this encourages
others to do the same, as accurate localisation is essential for anomaly
detection to have a future in clinical workflows.
automated segmentation of histopathological images is crucial, as it can
quantify the tumor micro-environment, provide a basis for cancer grading and
prognosis, and improve the diagnostic efficiency of clinical doctors [6,13,19].
however, pixellevel annotation of images is time-consuming and labor-intensive,
especially for histopathology images that require specialized knowledge.
therefore, there is an urgent need to pursue weakly supervised solutions for
pixel-wise segmentation. nonetheless, weakly supervised histopathological image
segmentation presents a challenge due to the low contrast between different
tissues, intra-class variations, and inter-class similarities [4,11].
additionally, the tissue structures in histopathology images can be randomly
arranged and dispersed, which makes it difficult to identify complete tissues or
regions of interest [7].ours cam under the microscope, tumor epithelial ɵssue
may appear as solid nests, acinar structures, or papillary formaɵons. the cells
may have enlarged and irregular nuclei.
necrosis ɵssue tumor-associated stromanecrosis may appear as areas of pink,
amorphous material under the microscope, and may be surrounded by viable tumor
cells and stroma.tumor-associated stroma ɵssue is the connecɵve ɵssue that
surrounds and supports the tumor epithelial ɵssue.fig. 1. comparison of
activation maps extracted from cam and our method, from left to right: origin
image, ground truth, three activation maps of tumor epithelial (red), necrosis
(green), and tumor-associated stroma (orange) respectively. on the right side,
there are some examples of the related language knowledge descriptions used in
our method. it shows that cam only highlights a small portion of the target,
while our method, which incorporates external language knowledge, can encompass
a wider and more precise target tissue. (color figure online)recent studies on
weakly supervised segmentation primarily follow class activation mapping (cam)
[20], which localizes the attention regions and then generates the pseudo labels
to train the segmentation network. however, the cam generated based on the
image-level labels can only highlight the most discriminative region, but fail
to locate the complete object, leading to defective pseudo labels, as shown in
fig. 1. accordingly, many attempts have been made to enhance the quality of cam
and thus boost the performance of weakly supervised segmentation. han et al. [7]
proposed an erasure-based method that continuously expands the scope of
attention areas to obtain rich content of pseudo labels. li et al. [11] utilized
the confidence method to remove any noise that may exist in the pseudo labels
and only included the confident pixel labels for the segmentation training.
zhang et al. [18] leveraged the transformer to model the long-distance
dependencies on the whole histopathological images to improve the cam's ability
to find more complete regions. lee et al. [10] utilized the ability of an
advanced saliency detection model to assist cam in locating more precise
targets. however, these improved variants still face difficulties in capturing
the complete tissues. the primary limitation is that the symptoms and
manifestations of histopathological subtypes cannot be comprehensively described
by an abstract semantic category. as a result, the image-level label supervision
may not be sufficient to pinpoint the complete target area.to remedy the
limitations of image-level supervision, we advocate for the integration of
language knowledge into weakly supervised learning to provide reliable guidance
for the accurate localization of target structures. to this end, we propose a
text-prompting-based weakly supervised segmentation method (tpro) for accurate
histopathology tissue segmentation. the text information originates from the
task's semantic labels and external descriptions of subtype manifestations. for
each semantic label, a pre-trained medical language model is utilized to extract
the corresponding text features that are matched to each feature point in the
image spatial space. a higher similarity represents a higher possibility of this
location belonging to the corresponding semantic category. additionally, the
text representations of subtype manifestations, including tissue morphology,
color, and relationships to other tissues, are extracted by the language model
as external knowledge. the discriminative information can be explored from the
text knowledge to help identify and locate complete tissues accurately by
jointly modeling long-range dependencies between image and text. we conduct
experiments on two weakly supervised histological segmentation benchmarks,
luad-histoseg and bcss-wsss, and demonstrate the superior quality of pseudo
labels produced by our tpro model compared to other cam-based methods.our
contributions are summarized as follows: (1) to the best of our knowledge, this
is the first work that leverages language knowledge to improve the quality of
pseudo labels for weakly-supervised histopathology image segmentation. (2) the
proposed text prompting models the correlation between image representations and
text knowledge, effectively improving the quality of pseudo labels. (3) the
effectiveness of our approach has been effectively validated by two benchmarks,
setting a new state of the art.
figure 2 displays the proposed tpro framework, a classification network designed
to train a suitable model and extract segmentation pseudo-labels. the framework
comprises a knowledge attention module and three encoders: one vision encoder
and two text encoders (label encoder and knowledge encoder).
vision encoder. the vision encoder is composed of four stages that encode the
input image into image features. the image features are denoted as t s ∈ r ms×cs
, where 2 ≤ s ≤ 4 indicates the stage number.label encoder. the label encoder
encodes the text labels in the dataset into n label features, denoted as l ∈ r n
×c l , where n represents the number of classes in the dataset and c l
represents the dimension of label features. since the label features will be
used to calculate the similarity with image features, it is important to choose
a language model that has been pre-trained on image-text pairs. here we use
medclip1 as our label encoder, which is a model fine-tuned on the roco dataset
[12] based on clip [14].knowledge encoder. the knowledge encoder is responsible
for embedding the descriptions of subtype manifestations into knowledge
features, denoted as k ∈ r n ×c k . the knowledge features guide the image
features to focus on regions relevant to the target tissue. to encode the
subtype manifestations description into more general semantic features, we
employ clinicalbert [2] as our knowledge encoder. clinicalbert is a language
model that has been fine-tuned on the mimic-iii [8] dataset based on biobert
[9].adaptive layer. we freeze the label and knowledge encoders for training
efficiency but add an adaptive layer after the text encoders to better tailor
the text features to our dataset. the adaptive layer is a simple fc-relu-fc
block that allows for fine-tuning of the features extracted from the text
encoders.label-pixel correlation. after the input image and text labels are
embedded. we employ the inner product to compute the similarity between image
features and label features, denoted as f s . specially, we first reshape the
image features from a token format into feature maps. we denote the feature map
as i s ∈ r hs×ws×cs , where h s and w s mean the height and width of the feature
map. f s is computed with the below formula(1)then, we perform a global
average-pooling operation on the produced similarity map to obtain the class
prediction, denoted as p s ∈ r 1×n . we then calculate the binary cross-entropy
loss between the class label y ∈ r 1×n and the class prediction p s to supervise
the model training, which is formulated as:deep supervision. to leverage the
shallow features in the network, we employ a deep supervision strategy by
calculating the similarity between the image features from different stages and
the label features from different adaptive layers. class predictions are derived
from these similarity maps. the loss of the entire network is computed as:(3)
to enhance the model's understanding of the color, morphology, and relationships
between different tissues, we gather text representations of different subtype
manifestations from the internet and encode them into external knowledge via the
knowledge encoder. the knowledge attention module uses this external knowledge
to guide the image features toward relevant regions of the target tissues.the
knowledge attention module, shown in fig. 2, consists of two multi-head
self-attention modules. the image features t 4 ∈ r m4×c4 and knowledge features
after adaptive layer k ∈ r n ×c4 are concatenated in the token dimension to
obtain t fuse ∈ r (m4+n )×c4 . this concatenated feature is then fed into the
knowledge attention module for self-attention calculation. the output tokens are
split, and the part corresponding to the image features is taken out. noted that
the knowledge attention module is added only after the last stage of the vision
encoder to save computational resources.
in the classification process, we calculate the similarity between image
features and label features to obtain a similarity map f , and then directly use
the result of global average pooling on the similarity map as a class
prediction. that is, the value at position (i, j, k) of f represents the
probability that pixel (i, j) is classified into the k th class. therefore we
directly use f as our localization map. we first perform min-max normalization
on it, the formula is as followswhere 1 ≤ c ≤ n means c th class in the dataset.
then we calculate the background localization map by the following formula:where
α ≥ 1 denotes a hyper-parameter that adjusts the background confidence scores.
referring to [1] and combined with our own experiments, we set α to 10. then we
stitch together the localization map of foreground and background, denoted as f
. in order to make full use of the shallow information of the network, we
perform weighted fusion on the localization maps from different stages by the
following formula:finally, we perform argmax operation on f all to obtain the
final pseudo-label.3 experiments
for the classification part, we adopt mixtransformer [17] pretrained on
ima-genet, medclip, and clinicalbert [2] as our vision encoder, label encoder,
and knowledge encoder, respectively. the hyperparameters during training and
evaluation can be found in the supplementary materials. we conduct all of our
experiments on 2 nvidia geforce rtx 2080 ti gpus.
comparison on pseudo-labels. table 1 compares the quality of our pseudolabels
with those generated by previous methods. cam [20] and grad-cam [15] were
evaluated using the same resnet38 [16] classifier, and the results showed that
cam [20] outperformed grad-cam [15], with miou values of 70.44% and 56.52% on
the luad-histoseg and bcss-wsss datasets, respectively. tran-sws [18] consists
of a classification and a segmentation branch, and table 1 displays the
pseudo-label scores generated by the classification branch. despite using cam
[20] for pseudo-label extraction, transws [18] yielded inferior results compared
to cam [20]. this could be due to the design of transws [18] for single-label
image segmentation, with the segmentation branch simplified to binary
segmentation to reduce the difficulty, while our dataset consists of multilabel
images. among the compared methods, mlps [7] was the only one to surpass cam
[20] in terms of the quality of the generated pseudo-labels, with its proposed
progressive dropout attention effectively expanding the coverage of target
regions beyond what cam [20] can achieve. our proposed method outperformed all
previous methods on both luad-histoseg and bcss-wsss datasets, with improvements
of 2.64% and 5.42% over the second-best method, respectively (table 2).
comparison on segmentation results. to further evaluate our proposed method, we
trained a segmentation model using the extracted pseudo-labels and compared its
performance with previous methods. due to its heavy reliance on dataset-specific
post-processing steps, histosegnet [5] failed to produce the desired results on
our datasets. as we have previously analyzed since the datasets we used are all
multi-label images, it was challenging for the segmentation branch of transws
[18] to perform well, and it failed to provide an overall benefit to the model.
experimental results also indicate that the iou scores of its segmentation
branch were even lower than the pseudo-labels of the classification branch. by
training the segmentation model of oeem [11] using the pseudo-labels extracted
by cam [20] in table 1, we can observe a significant improvement in the final
segmentation results. the final segmentation results of mlps [7] showed some
improvement compared to its pseudo-labels, indicating the effectiveness of the
multi-layer pseudo supervision and classification gate mechanism strategy
proposed by mlps [7]. our segmentation performance surpassed all previous
methods. specifically, our miou scores exceeded the second-best method by 3.17%
and 3.09% on luad-histoseg and bcss-wsss datasets, respectively. additionally,
it is worth noting that we did not use any strategies specifically designed for
the segmentation stage.
the results of our ablation experiments are presented in table 3. we set the
baseline as the framework shown in fig. 2 with all text information and deep
supervision strategy removed. it is evident that the addition of textual
information increases our pseudo-label miou by 2.50%. furthermore, including the
deep supervision strategy and knowledge attention module improves our
pseudo-label by 0.98% and 2.74%, respectively. these findings demonstrate the
significant contribution of each proposed module to the overall improvement of
the results. in order to demonstrate the effectiveness of fusing pseudo-labels
from the last three stages, we have presented in table 4 the iou scores for each
stage's pseudolabels as well as the fused pseudo-labels. it can be observed that
after fusing the pseudo-labels, not only have the iou scores for each class
substantially increased, but the miou score has also increased by 0.91% compared
to the fourth stage.
in this paper, we propose the tpro to address the limitation of weakly
supervised semantic segmentation on histopathology images by incorporating text
supervision and external knowledge. we argue that image-level labels alone
cannot provide sufficient information and that text supervision and knowledge
attention can provide additional guidance to the model. the proposed method
achieves the best results on two public datasets, luad-histoseg and bcss-wsss,
demonstrating the superiority of our method.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_11.
accurate 3d models of blood vessels are increasingly required for several
purposes in medicine and science [25]. these meshes are typically generated
using either image segmentation or synthetic methods. despite significant
advances in vessel segmentation [26], reconstructing thin features accurately
from medical images remains challenging [2]. manual editing of vessel geometry
is a tedious and error prone task that requires expert medical knowledge, which
explains the scarcity of curated datasets. as a result, several methods have
been developed to adequately synthesize blood vessel geometry [29].within the
existing literature on generating vascular 3d models, we identified two primary
types of algorithms: fractal-based, and space-filling algorithms. fractal-based
algorithms use a set of fixed rules that include different branching parameters,
such as the ratio of asymmetry in arterial bifurcations and the relationship
between the diameter of the vessel and the flow [7,33]. on the other hand,
space-filling algorithms allow the blood vessels to grow into a specific
perfusion volume while aligning with hemodynamic laws and constraints on the
formation of blood vessels [9,17,21,22,25]. although these model-based methods
provide some degree of control and variation in the structures produced, they
often fail to capture the diversity of real anatomical data.in recent years,
deep neural networks led to the development of powerful generative models [30],
such as generative adversarial networks [8,12] and diffusion models [11], which
produced groundbreaking performance in many applications, ranging from image and
video synthesis to molecular design. these advances have inspired the creation
of novel network architectures to model 3d shapes using voxel representations
[28], point clouds [31], signed distance functions [19], and polygonal meshes
[18]. in particular, and close to our aim, wolterink et al. [27] propose a gan
model capable of generating coronary artery anatomies. however, this model is
limited to generating single-channel blood vessels and thus does not support the
generation of more complex, tree-like vessel topologies.in this work we propose
a novel data-driven framework named vesselvae for synthesizing blood vessel
geometry. our generative framework is based on a recursive variational neural
network (rvnn), that has been applied in various contexts, including natural
language [23,24], shape semantics modeling [14,15], and document layout
generation [20]. in contrast to previous data-driven methods, our recursive
network fully exploits the hierarchical organization of the vessel and learns a
low-dimensional manifold encoding branch connectivity along with geometry
features describing the target surface. once trained, the vessel-vae latent
space is sampled to generate new vessel geometries. to the best of our
knowledge, this work is the first to synthesize multi-branch blood vessel trees
by learning from real data. experiments show that synth and real blood vessel
geometries are highly similar measured with the cosine similarity: radius (.97),
length (.95), and tortuosity (.96).
input. the network input is a binary tree representation of the blood vessel 3d
geometry. formally, each tree is defined as a tuple (t, e), where t is the set
of nodes, and e is the set of directed edges connecting a pair of nodes (n, m),
with n, m ∈ t . in order to encode a 3d model into this representation, vessel
segments v are parameterized by a central axis consisting of ordered points in
euclidean space: v = v 1 , v 2 , . . . , v n and a radius r, assuming a
piece-wise tubular vessel for simplicity. we then construct the binary tree as a
set of nodes t = n 1 , n 2 , . . . , n n , where each node n i represents a
vessel segment v and contains an attribute vector 4 with the coordinates of the
corresponding point and its radius r i . see sect. 3 for details.network
architecture. the proposed generative model is a recursive variational neural
network (rvnn) consisting of two main components: the encoder (enc) and the
decoder (dec) networks. the encoder transforms a tree structure into a
hierarchical encoding on the learned manifold. the decoder network is capable of
sampling from this encoded space to decode tree structures, as depicted in fig.
1. the encoding and decoding processes are achieved through a depth-first
traversal of the tree, where each node is combined with its parent node
recursively. the model outputs a hierarchy of vessel branches, where each
internal node in the hierarchy is represented by a vector that encodes its own
attributes and the information of all subsequent nodes in the tree.within the
rvnn decoder network there are two essential components: the node classifier
(cls) and the features decoder multi-layer perceptron (features dec-mlp). the
node classifier discerns the type of node to be decoded, whether it is a leaf
node or an internal node with one or two bifurcations. this is implemented as a
multi-layer perceptron trained to predict a three-category bifurcation
probability based on the encoded vector as input. complementing the node
classifier, the features dec-mlp is responsible for reconstructing the
attributes of each node, specifically its coordinates and radius. furthermore,
two additional components, the right and left dec-mlp, are in charge of
recursively decoding the next encoded node in the tree hierarchy. these
decoder's branches execute based on the classifier prediction for that encoded
node. if the node classifier predicts a single child for a node, a right child
is assumed by default.in addition to the core architecture, our model is further
augmented with three auxiliary, shallow, fully-connected neural networks: f μ ,
f σ , and g z . positioned before the rvnn bottleneck, the f μ and f σ networks
shape the distribution of the latent space where encoded tree structures lie.
conversely, the g z network, situated after the bottleneck, facilitates the
decoding of latent variables, aiding the decoder network in the reconstruction
of tree structures. collectively, these supplementary networks streamline the
data transformation process through the model. all activation functions used in
our networks are leaky relus. see the appendix for implementation
details.objective. our generative model is trained to learn a probability
distribution over the latent space that can be used to generate new blood vessel
segments. after encoding, the decoder takes samples from a multivariate gaussian
distribution:, where enc is the recursive encoder and f μ , f σ are two
fully-connected neural networks. in order to recover the feature vectors x for
each node along with the tree topology, we simultaneously train the regression
network (features dec-mlp in fig. 1) on a reconstruction objective l recon , and
the node classifier using l topo . additionally, in line with the general
framework proposed by β-vae [10], we incorporated a kullback-leibler (kl)
divergence term encouraging the distribution p(z s (x)) over all training
samples x to move closer to the prior of the standard normal distribution p(z).
we therefore minimize the following equation:where the reconstruction loss is
defined asand the topology objective is a three-class cross entropy loss l topo
= σ 3 c=1 x c log(cls(dec(x)) c ). notice that x c is a binary indicator (0 or
1) for the true class of the sample x. specifically, x c = 1 if the sample
belongs to class c and 0 otherwise. cls(dec(x)) c is the predicted probability
of the sample x belonging to class c (zero, one, or two bifurcations), as output
by the classifier. here, dec(x) denotes the encoded-decoded node representation
of the input sample x. 3d mesh synthesis. several algorithms have been proposed
in the literature to generate a surface 3d mesh from a tree-structured
centerline [29]. for simplicity and efficiency, we chose the approach described
in [6], which produces good quality meshes from centerlines with a low sample
rate. the implemented method iterates through the points in the curve generating
a coarse quadrilateral fig. 2. dataset and pre-processing overview: the raw
meshes from the intraa 3d collection undergo pre-processing using the vmtk
toolkit. this step is crucial for extracting centerlines and cross-sections from
the meshes, which are then used to construct their binary tree representations.
mesh along the segments and joints. the centerline sampling step is crucial for
a successful reconstruction outcome. thus, our re-sampling is not equispaced but
rather changes with curvature and radius along the centerline, increasing the
frequency of sampling near high-curvature regions. this results in a better
quality and more accurate mesh. finally, catmull-clark subdivision algorithm [5]
is used to increase mesh resolution and smooth out the surface.
materials. we trained our networks using a subset of the open-access intra
dataset1 published by yang et al. in 2020 [32]. this subset consisted of 1694
healthy vessel segments reconstructed from 2d mra images of patients. we
converted 3d meshes into a binary tree representation and used the network
extraction script from the vmtk toolkit2 to extract the centerline coordinates
of each vessel model. the centerline points were determined based on the ratio
between the sphere step and the local maximum radius, which was computed using
the advancement ratio specified by the user. the radius of the blood vessel
conduit at each centerline sample was determined using the computed
crosssections assuming a maximal circular shape (see fig. 2). to improve
computational efficiency during recursive tree traversal, we implemented an
algorithm that balances each tree by identifying a new root. we additionally
trimmed trees to a depth of ten in our experiments. this decision reflects a
balance between the computational demands of depth-first tree traversal in each
training step and the complexity of the training meshes. we excluded from our
study trees that exhibited greater depth, nodes with more than two children, or
with loops. however, non-binary trees can be converted into binary trees and it
is possible to train with deeper trees at the expense of higher computational
costs. ultimately, we were able to obtain 700 binary trees from the original
meshes using this approach.implementation details. for the centerline
extraction, we set the advancement ratio in the vmtk script to 1.05. the script
can sometimes produce multiple cross-sections at centerline bifurcations. in
those cases, we selected the sample with the lowest radius, which ensures proper
alignment with the centerline principal direction. all attributes were
normalized to a range of [0, 1]. for the mesh reconstruction we used 4
iterations of catmull-clark subdivision algorithm. the data pre-processing
pipeline and network code were implemented in python and pytorch
framework.training. in all stages, we set the batch size to 10 and used the adam
optimizer with β 1 = 0.9, β 2 = 0.999, and a learning rate of 1 × 10 -4 . we set
α = .3 and γ = .001 for eq. 1 in our experiments. to enhance computation speed,
we implemented dynamic batching [16], which groups together operations involving
input trees of dissimilar shapes and different nodes within a single input
graph. it takes approximately 12 h to train our models on a workstation equipped
with an nvidia a100 gpu, 80 gb vram, and 256 gb ram. however, the memory
footprint during training is very small (≤1 gb) due to the use of a lightweight
tree representation. this means that the amount of memory required to store and
manipulate our training data structures is minimal. during training, we ensure
that the reconstructed tree aligns with the original structure, rather than
relying solely on the classifier's predictions. we train the classifier using a
crossentropy loss that compares its predictions to the actual values from the
original tree. since the number of nodes in each class is unbalanced, we scale
the weight given to each class in the cross-entropy loss using the inverse of
each class count. during preliminary experiments, we observed that accurately
classifying nodes closer to the tree root is critical. this is because a
miss-classification of top nodes has a cascading effect on all subsequent nodes
in the tree (i.e. skip reconstructing a branch). to account for this, we
introduce a weighting scheme that for each node, assigns a weight to the
cross-entropy loss based on the number of total child nodes. the weight is
normalized by the total number of nodes in the tree.metrics. we defined a set of
metrics to evaluate our trained network's performance. by using these metrics,
we can determine how well the generated 3d models of blood vessels match the
original dataset distribution, as well as the diversity of the generated output.
the chosen metrics have been widely used in the field of blood vessel 3d
modeling, and have shown to provide reliable and accurate quantification of
blood vessels main characteristics [3,13]. we analyzed tortuosity per branch,
the vessel centerline total length, and the average radius of the tree.
tortuosity distance metric [4] is a widely used metric in the field of blood
vessel analysis, mainly because of its clinical importance. it measures the
amount of twistiness in each branch of the vessel. vessel's total length and
average radius were used in previous work to distinguish healthy vasculature
from cancerous malformations. finally, in order to measure the distance across
distributions for each metric, we compute the cosine similarity.
we conducted both quantitative and qualitative analyses to evaluate the model's
performance. for the quantitative analyses, we implemented a set of metrics
commonly used for characterizing blood vessels. we computed histograms of the
radius, total length, and tortuosity for the real blood vessel set and the
generated set (700 samples) in fig. 3 (a). the distributions are aligned and
consistent. we measured the closeness of histograms with the cosine similarity
by projecting the distribution into a vector of n-dimensional space (n is the
number of bins in the histogram). since our points are positive, the results
range from 0 to 1. we obtain a radius cosine similarity of .97, a total length
cosine similarity of .95, and a tortuosity cosine similarity of .96. results
show high similarities between histograms demonstrating that generated blood
vessels are realistic. given the differences with the baselines generated
topologies, for a fair comparison, we limited our evaluation to a visual
inspection of the meshes.the qualitative analyses consisted of a visual
evaluation of the reconstructed outputs provided by the decoder network. we
visually compared them to stateof-the-art methods in fig. 3 (b). the method
described by wolterink and colleagues [27] is able to generate realistic blood
vessels but without branches, and the method described by hamarneh et al. [9] is
capable of generating branches with straight shapes, missing on realistic
modeling. in contrast, our method is capable of generating realistic blood
vessels containing branches, with smooth varying radius, lengths, and
tortuosity.
we have presented a novel approach for synthesizing blood vessel models using a
variational recursive autoencoder. our method enables efficient encoding and
decoding of binary tree structures, and produces high-quality synthesized
models. in the future, we aim to explore combinations of our approach with
representing surfaces by the zero level set in a differentiable implicit neural
representation (inr) [1]. this could lead to more accurate and efficient
modeling of blood vessels and potentially other non-tree-like structures such as
capillary networks. since the presented framework would require significant
adaptations to accommodate such complex topologies, exploring this problem would
certainly be an interesting direction for future research. additionally, the
generated geometries might show self-intersections. in the future, we would like
to incorporate restrictions into the generative model to avoid such artifacts.
overall, we believe that our proposed approach holds great promise for advancing
3d blood vessel geometry synthesis and contributing to the development of new
clinical tools for healthcare professionals.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_7.
transfer learning has become a standard practice in medical image analysis as
collecting and annotating data in clinical scenarios can be costly. the
pre-trained parameters endow better generalization to dnns than the models
trained from scratch [8,23]. a popular approach to enhancing model
transferability is by pretraining on domains similar to the targets
[9,21,[27][28][29]. however, utilizing specialized pre-training for all medical
applications becomes impractical due to the fig. 1. the motivation of metalr.
previous works fix transferable layers in pre-trained models to prevent them
from catastrophic forgetting. it is inflexible and labor-expensive for this
method to find the optimal scheme. metalr uses meta-learning to automatically
optimize layer-wise lr for fine-tuning.diversity between domains and tasks and
privacy concerns related to pre-training data. consequently, recent work
[2,6,14,22] has focused on improving the generalization capabilities of existing
pre-trained dnn backbones through fine-tuning techniques.previous studies have
shown that the transferability of lower layers is often higher than higher
layers that are near the model output [26]. layer-wise finetuning [23], was thus
introduced to preserve the transferable low-level knowledge by fixing lower
layers. but recent studies [7] revealed that the lower layers may also be
sensitive to small domains like medical images. given the two issues,
transferability for medical tasks becomes more complicated [24,25]. it can even
be irregular among layers for medical domains far from pre-training data [7].
given the diverse medical domains and model architectures, there is currently no
universal guideline to follow to determine whether a particular layer should be
retrained for a given target domain.to search for optimal layer combinations for
fine-tuning, manually selecting transferable layers [2,23] can be a solution,
but it requires a significant amount of human labor and computational cost. in
order to address this issue and improve the flexibility of fine-tuning
strategies, we propose controlling the fine-tuning process with layer-wise
learning rates (lrs), rather than simply manually fixing or updating the layers
(see fig. 1). our proposed algorithm, meta learning rate (metalr), is based on
meta-learning [13] and adaptively adjusts lrs for each layer according to
transfer feedback. it treats the layer-wise lrs as metaknowledge and optimizes
them to improve the model generalization. larger lrs indicate less
transferability of corresponding layers and require more updating, while smaller
lrs preserve transferable knowledge in the layers. inspired by [20], we use an
online adaptation strategy of lrs with a time complexity of o(n), instead of the
computationally-expensive bi-level o(n 2 ) meta-learning. we also enhance the
algorithm's performance and stability with a proportional hyper-lr (lr for lr)
and a validation scheme on training data batches.in summary, this work makes the
following three contributions. 1) we introduce metalr, a meta-learning-based lr
tuner that can adaptively adjust layerwise lrs based on transfer learning
feedback from various medical domains.2) we enhance metalr with a proportional
hyper-lr and a validation scheme using batched training data to improve the
algorithm's stability and efficacy. 3) extensive experiments on both lesion
detection and tumor segmentation tasks were conducted to demonstrate the
superior efficiency and performance of met-alr compared to current sota medical
fine-tuning techniques.
this section provides a detailed description of the proposed metalr. it is a
meta-learning-based [13,18] approach that determines the appropriate lr for each
layer based on its transfer feedback. it is important to note that fixing
transferable layers is a special case of this method, where fixed layers always
have zero lrs. first, we present the theoretical formulation of metalr. next, we
discuss online adaptation for efficiently determining optimal lrs. finally, we
demonstrate the use of a proportional hyper-lr and a validation scheme with
batched training data to enhance performance.
let (x, y) denote a sample-label pair, and {(x i , y i ) | i = 1, ..., n } be
the training data. the validation dataset {(x v i , y v i ) | i = 1, ..., m } is
assumed to be independent and identically distributed as the training dataset.
let ŷ = φ(x, θ) be the prediction for sample x from deep model φ with parameters
θ. in standard training of dnns, the aim is to minimize the expected risk for
the training set: based on the generalization, one can tune the hyper-parameters
of the training process to improve the model. the key idea of metalr is
considering the layer-wise lrs as self-adaptive hyper-parameters during the
training and automatically adjusting them to achieve better model
generalization. we denote the lr and model parameters for the layer j at the
iteration t as α t j and θ t j . the lr scheduling scheme α = {α t j | j = 1,
..., d; t = 1, ..., t } is what metalr wants to learn, affecting which local
optimal θ * (α) the model parameters θ t = {θ t j | j = 1, ..., d} will converge
to. the optimal parameters θ * (α) are given by optimization on the training
data. at the same time, the best lr tuning scheme α * can be optimized based on
the feedback for θ * (α) from
training data d, validation data d v , initial model parameter {θ 0 1 , ..., θ 0
d }, lrs {α 0 1 , ..., α 0 d }, batch size n, max iteration t; output:final
model parameterstep forward for one step to get { θt 1 (α 4); 7: end for the
validation loss. this problem can be formulated as the following bi-level
optimization problem:metalr aims to use the validation set to optimize α through
an automatic process rather than a manual one. the optimal scheme α * can be
found by a nested optimization [13], but it is too computationally expensive in
practice. a faster and more lightweight method is needed to make it practical.
inspired by the online approximation [20], we propose efficiently adapting the
lrs and model parameters online. the motivation of the online lr adaptation is
updating the model parameters θ t and lrs {α t j | j = 1, 2, ..., d} within the
same loop. we first inspect the descent direction of parameters θ t j on the
training loss landscape and adjust the α t j based on the transfer feedback.
positive feedback (lower validation loss) means the lrs are encouraged to
increase.we adopt stochastic gradient descent (sgd) as the optimizer to conduct
the meta-learning. the whole training process is summarized in algorithm 1. at
the iteration t of training, a training data batch {(x i , y i ) | i = 1, ...,
n} and a validation data batch {(x v i , y v i ) | i = 1, ..., n} are sampled,
where n is the size of the batches. first, the parameters of each layer are
updated once with the current lr according to the descent direction on training
batch.this step of updating aims to get feedback for lr of each layer. after
taking derivative of the validation loss w.r.t. α t j , we can utilize the
gradient to know how the lr for each layer should be adjusted. so the second
step of metalr is to move the lrs along the meta-objective gradient on the
validation data:where η is the hyper-lr. finally, the updated lrs can be
employed to optimize the model parameters through gradient descent truly.for
practical use, we constrain the lr for each layer to be α t j ∈ [10 -6 , 10 -2
]. online metalr optimizes the layer-wise lrs as well as the training objective
on a single task, which differentiates it from traditional meta-learning
algorithms [12,19] that train models on multiple small tasks.
in practice, lrs are often tuned in an exponential style (e.g., 1e-3, 3e-3,
1e-2) and are always positive values. however, if a constant hyper-lr is used,
it will linearly update its corresponding lr regardless of numerical
constraints. this can lead to fluctuations in the lr or even the risk of the lr
becoming smaller than 0 and being truncated. to address this issue, we propose
using a proportional hyper-lr η = β × α t j , where β is a pre-defined
hyper-parameter. this allows us to rewrite eq. (3) as:the exponential update of
α t j guarantees its numerical stability.
one limitation of metalr is that the lrs are updated using separate validation
data, which reduces the amount of data available for the training process. this
can be particularly problematic for medical transfer learning, where the amount
of downstream data has already been limited. in eq. 2 and eq. 3, the update of
model parameter θ t j and lr α t j is performed using different datasets to
ensure that the updated θ t j can be evaluated for generalization without being
influenced by the seen data. as an alternative, but weaker, approach, we explore
using another batch of training data for eq. 3 to evaluate generalization. since
this batch was not used in the update of eq. 2, it may still perform well for
validation in meta-learning. the effect of this approach is verified in sect.
3.2, and the differences between the two methods are analyzed in sect. 3.4.
we extensively evaluate metalr on four transfer learning tasks (as shown in
table 1). to ensure the reproducibility of the results, all pre-trained models
(uscl [9], imagenet [11], c2l [28], models genesis [29]) and target datasets
(pocus [5], busi [1], chest x-ray [17], lits [4]) are publicly available. in our
work, we consider models pre-trained on both natural and medical image datasets,
with three target modalities and three target organs, which makes our
experimental results more credible. for the lesion detection tasks, we used
resnet-18 [15] with the adam optimizer. the initial learning rate (lr) and
hyper-lr coefficient β are set to 10 -3 and 0.1, respectively. in addition, we
use 25% of the training set as the validation set for meta-learning. for the
segmentation task, we use 3d u-net [10] with the sgd optimizer. the initial lr
and hyper-lr coefficient β are set to 10 -2 and 3 × 10 -3 , respectively. the
validation set for the lits segmentation dataset comprises 23 samples from the
training set of size 111. all experiments are implemented using pytorch 1.10 on
an nvidia rtx a6000 gpu. we report the mean values and standard deviations for
each experiment with five different random seeds. for more detailed information
on the models and hyper-parameters, please refer to our supplementary material.
imagenet [11] supervised busi [1] breast us tumor detection 780 images mimic-cxr
[16] c2l [28] chest x-ray [17] lung x-ray pneumonia detection 5856 images
lidc-idri [3] models genesis [29] lits [4] liver ct liver segmentation 131
volumes
in order to evaluate the effectiveness of our proposed method, we conduct an
ablation study w.r.t. the basic metalr algorithm, the proportional hyper-lr, and
batched-training-data validation (as shown in table 2). when applying only the
basic metalr, we observe only marginal performance improvements for the four
downstream tasks. we conjecture that this is due to two reasons: firstly, the
constant hyper-lr makes the training procedures less stable than direct
training, which is evident from the larger standard deviation of performance.
secondly, part of the training data are split for validation, which can be
detrimental to the performance. after applying the proportional hyper-lr,
significant improvements are in both the performance and its stability.
moreover, although the generalization validation on the training data batch may
introduce bias, providing sufficient training data ultimately benefits the
performance.
in our study, we compare metalr with several other fine-tuning schemes,
including tuning only the last layer / all layers with constant lrs, layer-wise
finetuning [23], bi-directional fine-tuning [7], and autolr [22]. the u-net
finetuning scheme proposed by amiri et al. [2] was also evaluated.
metalr consistently shows the best performance on all downstream tasks (table
3). it shows 1%-2.3% accuracy improvements compared to direct training (i.e.,
tuning all layers) because it takes into account the different transferabilities
of different layers. while manual picking methods, such as layer-wise and
bi-directional fine-tuning, also achieve higher performance, they require much
more training time (5×-50×) for searching the best tuning scheme. on the other
hand, autolr is efficient, but its strong hypothesis harms its performance
sometimes. in contrast, metalr makes no hypothesis about transferability and
learns appropriate layer-wise lrs on different domains. moreover, its
performance improvements are gained with only 1.5×-2.5× training time compared
with direct training.results on segmentation task. metalr achieves the best dice
performance on the lits segmentation task (table 4). unlike resnet for lesion
detection, the u-net family has a more complex network topology. with skip
connections, there are two interpretations [2] of depths for layers: 1) the
left-most layers are the shallowest, and 2) the top layers of the "u" are the
shallowest. this makes the handpicking methods even more computationally
expensive. however, metalr updates the lr for each layer according to their
validation gradients, and its training efficiency is not affected by the complex
model architecture.
the lrs learned with metalr. for resnet-18 (fig. 2 (a)), the layer-wise lrs
fluctuate drastically during the first 100 iterations. however, after iteration
100, all layers except the first layer "conv1" become stable at different
levels.the first layer has a decreasing lr (from 2.8 × 10 -3 to 3 × 10 -4 )
throughout the process, reflecting its higher transferability. for 3d u-net
(fig. 2 (b)), the middle layers of the encoder "down-128" and "down-256" are the
most transferable and have the lowest lrs, which is difficult for previous
fine-tuning schemes to discover. as expected, the randomly initialized "fc" and
"out" layers have the largest lrs since they are not transferable.
validation. we illustrate the lr curves with a constant hyper-lr instead of a
proportional one. the lr curves of "block 3-1" and "block 4-2" become much more
fluctuated (fig. 2 (c)). this instability may be the key reason for the
instability of performance when using a constant hyper-lr. furthermore, we
surprisingly find that the learned lrs are similar to the curves learned when
validated on the training set when using a separate validation set fig. 2
in this work, we proposed a new fine-tuning scheme, metalr, for medical transfer
learning. it achieves significantly superior performance to the previous sota
fine-tuning algorithms. metalr alternatively optimizes model parameters and
layer-wise lrs in an online meta-learning fashion with a proportional hyper-lr.
it learns to assign lower lrs for the layers with higher transferability and
higher lrs for the less transferable layers. the proposed algorithm is easy to
implement and shows the potential to replace manual layer-wise fine-tuning
schemes. future works include adapting metalr to a wider variety of clinical
tasks.
modern microscopes allow the digitalization of conventional glass slides into
gigapixel whole-slide images (wsis) [18], facilitating their preservation and
fig. 1. overview of our proposed framework, das-mil. the features extracted at
different scales are connected (8-connectivity) by means of different graphs.
the nodes of both graphs are later fused into a third one, respecting the rule
"part of". the contextualized features are then passed to distinct
attention-based mil modules that extract bag labels. furthermore, a knowledge
distillation mechanism encourages the agreement between the predictions
delivered by different scales.retrieval, but also introducing multiple
challenges. on the one hand, annotating wsis requires strong medical expertise,
is expensive, time-consuming, and labels are usually provided at the slide or
patient level. on the other hand, feeding modern neural networks with the entire
gigapixel image is not a feasible approach, forcing to crop data into small
patches and use them for training. this process is usually performed considering
a single resolution/scale among those provided by the wsi image.recently,
multi-instance learning (mil) emerged to cope with these limitations. mil
approaches consider the image slide as a bag composed of many patches, called
instances; afterwards, to provide a classification score for the entire bag,
they weigh the instances through attention mechanisms and aggregate them into a
single representation. it is noted that these approaches are intrinsically flat
and disregard the pyramidal information provided by the wsi [15], which have
been proven to be more effective than single-resolution [4,13,15,19]. however,
to the best of our knowledge, none of the existing proposals leverage the full
potential of the wsi pyramidal structure. indeed, the flat concatenation of
features [19] extracted at different resolutions does not consider the
substantial difference in the informative content they provide. a proficient
learning approach should instead consider the heterogeneity between global
structures and local cellular regions, thus allowing the information to flow
effectively across the image scales.to profit from the multi-resolution
structure of wsi, we propose a pyramidal graph neural network (gnn) framework
combined with (self) knowledge distillation (kd), called das-mil (distilling
across scales). a visual representation of the proposed approach is depicted in
fig. 1. distinct gnns provide contextualized features, which are fed to distinct
attention-based mil modules that compute bag-level predictions. through
knowledge distillation, we encour-age agreement across the predictions delivered
at different resolutions, while individual scale features are learned in
isolation to preserve the diversity in terms of information content. by
transferring knowledge across scales, we observe that the classifier
self-improves as information flows during training. our proposal has proven its
effectiveness on two well-known histological datasets, camelyon16 and tcga lung
cancer, obtaining state-of-the-art results on wsi classification.
mil approaches for wsi classification. we herein summarize the most recent
approaches; we refer the reader to [11,26] for a comprehensive overview.
a classical approach is represented by ab-mil [16], which employs a side-branch
network to calculate the attention scores. in [28], a similar attention
mechanism is employed to support a double-tier feature distillation approach,
which distills features from pseudo-bags to the original slide. differently,
ds-mil [19] applies non-local attention aggregation by considering the distance
with the most relevant patch. the authors of [20] and [25] propose variations of
ab-mil, which introduce clustering losses and transformers, respectively. in
addition, setmil [31] makes use of spatial-encoding transformer layers to update
the representation. the authors of [7] leverage dino [5] as feature extractor,
highlighting its effectiveness for medical image analysis. beyond classical
attention mechanisms, there are also algorithms based on recurrent neural
networks (rnn) [4], and graphs neural networks (gnn) [32].multi-scale. recently,
different authors focused on multi-resolution approaches. dsmil-lc [19] merges
representations from different resolutions, i.e., low instance representations
are concatenated with the ones obtained at a higher resolution. ms-rnnmil [4],
instead, fed an rnn with instances extracted at different scales. in [6], a
self-supervised hierarchical transformer is applied at each scale. in ms-da-mil
[13], multi-scale features are included in the same attention algorithm. [10]
and [15] exploit multi-resolution through gnn architectures.knowledge
distillation. distilling knowledge from a more extensive network (teacher ) to a
smaller one (student) has been widely investigated in recent years [21,24] and
applied to different fields, ranging from model compression [3] to wsi analysis
[17]. typically, a tailored learning objective encourages the student to mimic
the behaviour of its teacher. recently, self-supervised representation learning
approaches have also employed such a schema: as an example, [5,9] exploit kd to
obtain an agreement between networks fed with different views of the same image.
in [28], kd is used to transfer the knowledge between mil tiers applied on
different subsamples bags. taking inspiration from [23] and [30], our model
applies (self) knowledge distillation between wsi scale resolutions.
our approach aims to promote the information flow through the different employed
resolutions. while existing works [19,20,25] take into account interscales
interactions by mostly leveraging trivial operations (such as concatenation of
related feature representations), we instead provide a novel technique that
builds upon: i) a gnn module based on message passing, which propagates patches'
representation according to the natural structure of multi-resolutions wsi; ii)
a regulation term based on (self) knowledge distillation, which pins the most
effective resolution to further guide the training of the other one(s). in the
following, we are delving into the details of our architecture.feature
extraction. our work exploits dino, the self-supervised learning approach
proposed in [5], to provide a relevant representation of each patch. differently
from other proposals [19,20,28], it focuses solely on aligning positive pairs
during optimization (and hence avoids negative pairs), which has shown to
require a lower memory footprint during training. we hence devise an initial
stage with multiple self-supervised feature extractors f (•; θ 1 ), . . . , f m
(•; θ m ), one dedicated to each resolution: this way, we expect to promote
feature diversity across scales. after training, we freeze the weights of these
networks and use them as patch-level feature extractors. although we focus only
on two resolutions at time (i.e., m = 2) the approach can be extended to more
scales.architecture. the representations yield by dino provide a detailed
description of the local patterns in each patch; however, they retain poor
knowledge of the surrounding context. to grasp a global guess about the entire
slide, we allow patches to exchange local information. we achieve it through a
pyramidal graph neural network (pgnn) in which each node represents an
individual wsi patch seen at different scales. each node is connected to its
neighbors (8-connectivity) in the euclidean space and between scales following
the relation "part of"1 . to perform message passing, we adopt graph attention
layers (gat) [27].in general terms, such a module takes as input multi-scale
patch-level representations x = [x 1 x 2 ], where x 1 ∈ r n1×f and x 2 ∈ r n2×f
are respectively the representations of the lower and higher scale. the input
undergoes two graph layers: while the former treats the two scales as
independent subgraphs a 1 ∈ r n1×n1 and a 2 ∈ r n2×n2 , the latter process them
jointly by considering the entire graph a (see fig. 1, left). in formal
terms:where h ≡ [h 1 h 2 ] stands for the output of the pgnn obtained by
concatenating the two scales. these new contextualized patch representations are
then fed to the attention-based mil module proposed in [19], which produces
bag-level scores y bag 1 , y bag 2 ∈ r 1×c where c equals the number of classes.
notably, such a module provides additional importance scores z 1 ∈ r n1 and z 2
∈ r n2 , which quantifies the importance of each original patch to the overall
prediction.aligning scales with (self ) knowledge distillation. we have hence
obtained two distinct sets of predictions for the two resolutions: namely, a
bag-level score (e.g., a tumor is either present or not) and a patch-level one
(e.g., which instances contribute the most to the target class). however, as
these learned metrics are inferred from different wsi zooms, a disagreement may
emerge: indeed, we have observed (see table 4) that the higher resolutions
generally yield better classification performance. in this work, we exploit such
a disparity to introduce two additional optimization objectives, which pin the
predictions out of the higher scale as teaching signal for the lower one.
further than improving the results of the lowest scale only, we expect its
benefits to propagate also to the shared message-passing module, and so to the
higher resolution.formally, the first term seeks to align bag predictions from
the two scales through (self) knowledge distillation [14,29]:where kl stands for
the kullback-leibler divergence and τ is a temperature that lets secondary
information emerge from the teaching signal.the second aligning term regards the
instance scores. it encourages the two resolutions to assign criticality scores
in a consistent manner: intuitively, if a lowresolution patch has been
considered critical, then the average score attributed to its children patches
should be likewise high. we encourage such a constraint by minimizing the
euclidean distance between the low-resolution criticality grid map z 1 and its
subsampled counterpart computed by the high-resolution branch:(2)in the equation
above, graphpooling identifies a pooling layer applied over the higher scale: to
do so, it considers the relation "part of" between scales and then averages the
child nodes, hence allowing the comparison at the instance level.overall
objective. to sum up, the overall optimization problem is formulated as a
mixture of two objectives: the one requiring higher conditional likelihood
w.r.t. ground truth labels y and carried out through the cross-entropy loss l ce
(•; y); the other one based on knowledge distillation:where λ is a
hyperparameter weighting the tradeoff between the teaching signals provided by
labels and the higher resolution, while β balances the contributions of the
consistency regularization introduced in eq. ( 2).
wsis pre-processing. we remove background patches through an approach similar to
the one presented in the clam framework [20]: after an initial segmentation
process based on otsu [22] and connected component analysis [2], non-overlapped
patches within the foreground regions are considered.optimization. we use adam
as optimizer, with a learning rate of 2 × 10 -4 and a cosine annealing scheduler
(10 -5 decay w/o warm restart). we set τ = 1.5, β = 1, and λ = 1. the dino
feature extractor has been trained with two rtx5000 gpus: differently, all
subsequent experiments have been performed with a single rtx2080 gpu using
pytorch-geometric [12]. to asses the performance of our approach, we adhere to
the protocol of [19,28] and use the accuracy and auc metrics. moreover, the
classifier on the higher scale has been used to make the final overall
prediction. regarding the kd loss, we apply the temperature term to both student
and teacher outputs for numerical stability. [4] 0.806 0.806 0.862 0.911 abmil
[16] 0.845 0.865 0.900 0.949 clam-sb [20] 0.865 0.885 0.875 0.944 clam-mb [20]
0.850 0.894 0.878 0.949 trans-mil † [25] 0.883 0.942 0.881 0.948 dtfd (afs) [28]
0.908 0.946 0.891 0.951 dtfd (maxmins) [28] 0.899 0.941 0.894 0.961 dsmil † [19]
0.915 0.952 0.888 0.951 multi scale ms-da-mil [13] 0.876 0.887 0.900 0.955
ms-milrnn [4] 0.814 0.837 0.891 0.921 hipt † [6] 0.890 0.951 0.890 0.950
dsmil-lc † [19] 0.909 0.955 0.913 0.964 h 2 -mil † [15] 0.859 0.912 0.823 0.917
das-mil (ours) 0.945 0.973 0.925 0.965camelyon16. [1] we adhere to the official
training/test sets. to produce the fairest comparison with the single-scale
state-of-the-art solution, the 270 remaining wsis are split into training and
validation in the proportion 9:1.
it is available on the gdc data transfer portal and comprises two subsets of
cancer: lung adenocarcinoma (luad) and lung squamous cell carcinoma (lusc),
counting 541 and 513 wsis, respectively. the aim is to classify luad vs lusc; we
follow the split proposed by dsmil [19].
table 1 compares our das-mil approach with the state-of-the-art, including both
single-and multi-scale architectures. as can be observed: i) the joint
exploitation of multiple resolutions is generally more efficient; ii) our
das-mil yields robust and compelling results, especially on camelyon16, where it
provides 0.945 of accuracy and 0.973 auc (i.e., an improvement of +3.3% accuracy
and +1.9% auc with respect to the sota). finally, we remark that most of the
methods in the literature resort to different feature extractors; however, the
next subsections prove the consistency of das-mil benefits across various
backbones.
on the impact of knowledge distillation. to assess its merits, we conducted
several experiments varying the values of the corresponding balancing
coefficients (see table 2). as can be observed, lowering their values (even
reaching λ = 0, i.e., no distillation is performed) negatively affects the
performance. such a statement holds not only for the lower resolution (as one
could expect), but also for the higher one, thus corroborating the claims we
made in sect. 3 on the bidirectional benefits of knowledge distillation in our
multi-scale architecture. we have also performed an assessment on the
temperature τ , which controls the smoothing factor applied to teacher's
predictions (table 3). we found that the lowest the temperature, the better the
results, suggesting that the teacher scale is naturally not overconfident about
its predictions, but rather well-calibrated.single-scale vs multi-scale. the
impact of the feature extractors and gnns. table 5 proposes an investigation of
these aspects, which considers both simcrl [8] and dino, as well as the recently
proposed graph mechanism h 2 -mil [15]. in doing so, we fix the input
resolutions to 5× and 20×. we draw the following conclusions: i) when our
das-mil feature propagation layer is used, the selection of the optimal feature
extractor (i.e., simclr vs dino) has less impact on performance, as the
message-passing can compensate for possible lacks in the initial representation;
ii) das-mil appears a better features propagator w.r.t. h 2 -mil.h 2 -mil
exploits a global pooling layer (ihpool) that fulfils only the spatial structure
of patches: as a consequence, if non-tumor patches surround a tumor patch, its
contribution to the final prediction is likely to be outweighed by the ihpool
module of h 2 -mil. differently, our approach is not restricted in such a way,
as it can dynamically route the information across the hierarchical structure
(also based on the connections with the critical instance).
we proposed a novel way to exploit multiple resolutions in the domain of
histological wsi. we conceived a novel graph-based architecture that learns
spatial correlation at different wsi resolutions. specifically, a gnn cascade
architecture is used to extract context-aware and instance-level features
considering the spatial relationship between scales. during the training
process, this connection is further amplified by a distillation loss, asking for
an agreement between the lower and higher scales. extensive experiments show the
effectiveness of the proposed distillation approach.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0 24.
the development of deep neural networks has greatly promoted medical
imagingbased computer-aided diagnosis. due to the large amount of learnable
parameters in neural networks, sufficient annotated training samples are
required for training. however, the labeling process of medical images is
tedious and timeconsuming. to address this problem, the common paradigm of
transfer learning, which first pre-trains a model on upstream image datasets and
then fine-tunes it on various target tasks, has been widely investigated in
recent years [10,21,30]. compared with the distributed training across multiple
centers, there are no specific ethical issues or computational design of
distributed/federated learning frameworks with the "pre-train-then-fine-tune"
workflow.previous works mainly focused on the fine-tuning strategy to
effectively adapt the knowledge from the pre-trained models to target tasks
[4,12,19,26]. with the increasing number of pre-trained networks provided by the
community, model repositories like hugging face [25] and pytorch hub [18] enable
researchers to experiment across a large number of downstream datasets and
tasks. these pre-trained models require less training time and have better
performance and robustness compared with the learning-from-scratch models.
however, it has been observed by recent works [23] that the pre-trained models
cannot always benefit the downstream tasks. when the knowledge is transferred
from a less relevant source, it may not improve the performance or even
negatively affect the intended outcome [24]. a brute-force method is to
fine-tune a set of pretrained models with target datasets to find the optimal
one. this process is timeconsuming and laborious. existing methods also measured
the task-relatedness between source and target datasets [6,7,22,28]. however,
most of these works require source information available while medical images
have more privacy and ethical issues and fewer datasets are publicly available
than natural images.considering the issues mentioned above, this work focused on
source-free pre-trained model selection for segmentation tasks in the medical
image. as shown in fig. 1, models pre-trained by upstream data constitute the
model bank. the main idea is to directly measure the transferability of the
pre-trained models without fully training based on the downstream/target
dataset. among the recent works, leep [15] and its variant [1,13] were developed
to utilize the loglikelihood between the target labels and the predictions from
the source model. logme [27] computed evidence based on the linear parameters
assumption and efficiently leverages the compatibility between features and
labels. gbc [17] applied the gaussian distribution to each class, and estimate
the separability between classes as the basis for transferability estimation.
transrate [9] evaluated the transferability of models with the compactness and
the completeness of embedding space. cui et.al [5] contended that
discriminability and transferability are crucial properties of representations
and introduce the information bottleneck theory for transferability estimation.
these methods have achieved promising performance on classification and
regression tasks without fully considering the properties of medical image
segmentation. first, unlike classification and regression problems that can use
a single n-dimensional feature vector to represent each image, segmentation
problems lack a global semantic representation, which poses difficulties for
direct transferability estimation. in addition, most label-comparison-based
methods [9,15,17,27] focus on the relationship between the embeddings and
downstream labels without exploring the effectiveness of the features
themselves. third, medical images face severe class imbalance problems, with
excessive differences between foreground and background. however, existing
algorithms rarely give additional attention to the class imbalance problem.
besides, for semantic segmentation tasks, the feature pyramid is critical for
the segmentation output of multi-scale objects while existing works neglect
it.in our work, we propose a new method using class consistency and feature
variety(cc-fv) with an efficient framework to estimate the transferability in
medical image segmentation tasks. class consistency employs the distribution of
features extracted from foreground voxels of the same category in each sample to
model and calculate their distance, the smaller the distance the better the
result; feature diversity utilizes features sampled in the whole global feature
map, and the uniformity of the feature distribution obtained by sampling is used
to measure the effectiveness of the features themselves. extensive experiments
have proved the superiority of our method compared with baseline methods.2
methodology
in our work, a model bank m consisting of pre-trained models {m i } k i=1 are
available to be fine-tuned and evaluated with a target dataset, where x j is the
image and y j is the ground truth of segmentation. after fine-tuning, the
performance of m i can be measured with the segmentation metric (e.g. dice
score), which is denoted by p i s→t in this paper. our work is to directly
estimate the transferability score t i s→t without fine-tuning the model on
target datasets. a perfect transferability score should preserve the ordering,
i.e. t i s→t > t j s→t if and only if p i s→t > p j s→t .
the transferability of models from a weakly related source domain to a target
domain can be compromised if the domains are not sufficiently comparable [24].
this intrigues us about the question of "what kind of models are transferable".
the proposed method is intuitive and straightforward: features extracted by the
pre-trained model should be consistent within the class of the target dataset
while representative and various globally. therefore, class consistency and
feature variety are considered to estimate the transferability between models
and downstream data.class consistency. the pre-trained models are trained with
specific pretext tasks based on the upstream dataset. therefore, features
extracted by the pretrained models cannot perfectly distinguish the foreground
and background of target data. if the features are generalizable, foreground
region features will likely follow a similar distribution even without
fine-tuning. given a pair of target data x j and x j , the distribution of the
features is modeled with the n-dimensional gaussian distribution. since the size
of the foreground class varies across the cases, we therefore randomly sample
the pixels/voxels of x j and x j for each class and establish the feature
distribution f k j , f k j based on the voxels of the k th class to approximate
the case-wise distribution of different classes. the class consistency between
the data pair is measured by the wasserstein distance [16] as follows:whereare
covariance matrices of f k j and f k j . compared to some commonly used metrics
like kl-divergence or bhattacharyya distance [17], wasserstein distance is more
stable during the computation of high-dimensional matrices because it is
unnecessary to compute the determinant or inverse of a high-dimensional matrix,
which can easily lead to an overflow in numerical computation. we calculate the
wasserstein distance of the distribution with voxels of the same class in a
sample pair comprised of every two samples in the dataset, and obtained the
following definition of class consistency c consgiven that 3d medical images are
computationally intensive, and prone to causing out-of-memory problems, in the
sliding window inference process for each case, we do not concatenate the output
of each patch into the final prediction result, but directly sample from the
patched output and concatenate them into the final sampled feature matrix. in
the calculation of class consistency, we only sample the foreground voxels with
a pre-defined sampling number which is proportional to the voxel number of each
class in the image because of the severe class imbalance problem.feature
variety. class consistency is not the only criterion for transferability
estimation. as a result of learning some trivial solutions, some overfitted
models have limited generalization capacity and are difficult to apply to new
tasks. we believe that the essential reason for this phenomenon is that class
consistency is only concerned with local homogeneity of information while
neglecting the integral feature quality assessment. hence we propose the feature
variety constraint, which measures the expressiveness of the features themselves
and the uniformity of their probability distribution. highly complex features
are not easily overfitted in the downstream tasks and do not collapse to cause a
trivial solution.to calculate the variety of features we need to analytically
measure the properties of the feature distribution over the full feature space.
besides, to prevent overfitting and trivial features, we expect the distribution
of features in the feature space to be as uniform and dispersed as possible.
therefore we employ the following hyperspherical potential energy e s ashere v
is sampled feature of each image with point-wise embedding v i and l is the
length of the feature, which is also the number of sampled voxels. we randomly
sample from the whole case so that the features can better express the overall
representational power of the model. the feature vectors will be more widely
dispersed in the unit sphere if the hyperspherical energy (hse) is lower [3].
for the dataset with n cases, we choose s = 1 and the feature variety f v is
formulated asoverall estimation. as for semantic segmentation problems, the
feature pyramid structure is critical for segmentation results [14,29]. hence in
our framework, different decoders' outputs are upsampled to the size of the
output and can be used in the sliding window sampling process. besides, we
decrease the sampling ratio in the decoder layer close to the bottleneck to
avoid feature redundancy.the final transferability of pre-trained model m to
dataset t t m→t iswhere d is the number of decoder layers used in the
estimation.
the medical segmentation decathlon (msd) [2] dataset is composed of ten
different datasets with various challenging characteristics, which are widely
used in the medical image analysis field. to evaluate the effectiveness of
cc-fv, we conduct extensive experiments on 5 of the msd dataset, including
task03 liver(liver and tumor segmentation), task06 lung(lung nodule
segmentation), task07 pancreas(pancreas and pancreas tumor segmentation), task09
spleen(spleen segmentation), and task10 colon(colon cancer segmentation). all of
the datasets are 3d ct images. the public part of the msd dataset is chosen for
our experiments, and each dataset is divided into a training set and a test set
at a scale of 80% and 20%. for each dataset, we use the other four datasets to
pre-train the model and fine-tune the model on this dataset to evaluate the
performance as well as the transferability using the correlation between two
ranking sequences of upstream pre-trained models. we load all the pre-trained
models' parameters except for the last convolutional layer and no parameters are
frozen during the fine-tuning process. on top of that, we follow the nnunet [11]
with the selfconfiguring method to choose the pre-processing, training, and
post-processing strategy. for fair comparisons, the baseline methods including
transrate [9], logme [27], gbc [17] and leep [15] are also implemented. for
these currently available methods, we employ the output of the layer before the
final convolution as the feature map and sample it through the same sliding
window as cc-fv to obtain different classes of features, which can be used for
the calculation. figure 2 visualizes the average dice score and the estimation
value on task 03 liver. the te results are obtained from the training set only.
u-net [20] and unetr [8] are applied in the experiment and each model is
pre-trained for 250k iterations and fine-tuned for 100k iterations with batch
size 2 on a single nvidia a100 gpu. besides, we use the model at the end of
training for inference and calculate the final dsc performance on the test set.
and we use weighted kendall's τ [27] and pearson correlation coefficient for the
correlation between the te results and fine-tuning performance. the kendall's τ
ranges from [-1, 1], and τ=1 means the rank of te results and performance are
perfectly correlated(t i s→t > t j s→t if and only if p i s→t > p j s→t ). since
model selection generally picks the top models and ignores the poor performers,
we assign a higher weight to the good models in the calculation, known as
weighted kendall's τ. the pearson coefficient also ranges from [-1, 1], and
measures how well the data can be described by a linear equation. the higher the
pearson coefficient, the higher the correlation between the variables. it is
clear that the te results of our method have a more positive correlation with
respect to dsc performance.table 1 demonstrates that our method surpasses all
the other methods. most of the existing methods are inferior to ours because
they are not designed for segmentation tasks with a serious class imbalance
problem. besides, these methods rely only on single-layer features and do not
make good use of the hierarchical structure of the model. 2. correlation between
the fine-tuning performance and transferability metrics using task03 as an
example. the vertical axis represents the average dice of the model, while the
horizontal axis represents the transferability metric results. we have
standardized the various metrics uniformly, aiming to observe a positive
relationship between higher performance and higher transferability estimations.
in table 2 we analyze the different parts of our method and compare some other
methods. first, we analyze the impact of class consistency c cons and feature
variety f v . though f v can not contribute to the final kendall's τ directly, c
cons with the constraint of f v promotes the total estimation result. then we
compare the performance of our method at single and multiple scales to prove the
effectiveness of our multi-scale strategy. finally, we change the distance
metrics in class consistency estimation. kl-divergence and bha-distance are
unstable in high dimension matrics calculation and the performance is also
inferior to the wasserstein distance. figure 3 visualize the distribution of
different classes using t-sne methods. we can easily find that with models with
a pre-training process have a more compact intra-class distance and a higher
fine-tuning performance.
in our work, we raise the problem of model selection for upstream and downstream
transfer processes in the medical image segmentation task and analyze the
practical implications of this problem. in addition, due to the ethical and
privacy issues inherent in medical care and the computational load of 3d image
segmentation tasks, we design a generic framework for the task and propose a
transferability estimation method based on class consistency with feature
variety constraint, which outperforms existing model transferability estimation
methods as demonstrated by extensive experiments.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_64.
medical image segmentation often relies on supervised model training [14], but
this approach has limitations. firstly, it requires costly manual
annotations.secondly, the resulting models may not generalize well to unseen
data domains. even small changes in the task may result in a significant drop in
performance, requiring re-training from scratch [18].self-supervised learning
(ssl) is a promising solution to these limitations. ssl pre-trains a model
backbone to extract informative representations from unlabeled data. then, a
simple linear or non-linear head on top of the frozen pre-trained backbone can
be trained for various downstream tasks in a supervised manner (linear or
non-linear probing). alternatively, the backbone can be finetuned for a
downstream task along with the head. pre-training the backbone in a
self-supervised manner enables scaling to larger datasets across multiple data
and task domains. in medical imaging, this is particularly useful given the
growing number of available datasets.in this work, we focus on contrastive
learning [8,12], one of the most effective approaches to ssl in computer vision.
in contrastive learning, the model is trained to produce similar vector
representations for augmented views of the same image and dissimilar
representations for different images. contrastive methods can also be used to
learn dense, i.e., patch-level or even pixel-or voxellevel representations:
pixels of augmented image views from the same region of the original image
should have similar representations, while different pixels should have
dissimilar ones [23].several works have implemented contrastive learning of
dense representations in medical imaging [2,7,25,26,29]. representations in
[7,25] do not resolve nearby voxels due to the negative sampling strategy and
the architectural reasons. this makes them unsuitable for full-resolution
segmentation, especially in linear and non-linear probing regimes. in the
current sota dense ssl methods [2,26], authors employ restorative learning in
addition to patch-level contrastive learning, in order to pre-train voxel-level
representations in full-resolution. in [29], separate global and voxel-wise
representations are learned in a contrastive manner to implement efficient dense
image retrieval.the common weakness of all the above works is that they do not
evaluate their ssl models in linear or non-linear probing setups, even though
these setups are de-facto standards for evaluation of ssl methods in natural
images [8,13,23]. moreover, fine-tuned models can deviate drastically from their
pre-trained states due to catastrophical forgetting [11], while models trained
in linear or non-linear probing regimes are more robust as they have several
orders of magnitude fewer trainable parameters.our contributions are threefold.
first, we propose vox2vec, a framework for contrastive learning of voxel-level
representations. our simple negative sampling strategy and the idea of storing
voxel-level representations in a feature pyramid form result in
high-dimensional, fine-grained, multi-scale representations suitable for the
segmentation of different organs and tumors in full resolution. second, we
employ vox2vec to pre-train a fpn architecture on a diverse collection of six
unannotated datasets, totaling over 6,500 ct images of the thorax and abdomen.
we make the pre-trained model publicly available to simplify the reproduction of
our results and to encourage practitioners to utilize this model as a starting
point for the segmentation algorithms training. finally, we compare the
pretrained model with the baselines on 22 segmentation tasks on seven ct
datasets in three setups: linear probing, non-linear probing, and fine-tuning.
we show that vox2vec performs slightly better than sota models in the
fine-tuning setup and outperforms them by a huge margin in the linear and
non-linear probing setups. to the best of our knowledge, this is the first
successful attempt to evaluate dense ssl methods in the medical imaging domain
in linear and non-linear probing regimes.
in recent years, self-supervised learning in computer vision has evolved from
simple pretext tasks like jigsaw puzzles [22], rotation prediction [17], and
patch position prediction [10] to the current sota methods such as restorative
autoencoders [13] and contrastive [8] or non-contrastive [9] joint embedding
methods.several methods produce dense or pixel-wise vector representations
[6,23,28] to pre-train models for downstream tasks like segmentation or object
detection. in [23], pixel-wise representations are learned by forcing local
features to remain constant over different viewing conditions. this means that
matching regions describing the same location of the scene on different views
should be positive pairs, while non-matching regions should be negative pairs.
in [28], authors define positive and negative pairs as spatially close and
distant pixels, respectively. while in [6], authors minimize the mean square
distance between matched pixel embeddings, simultaneously preserving the
embedding variance along the batch and decorrelating different embedding vector
components.the methods initially proposed for natural images are often used to
pretrain models on medical images. in [25], authors propose the 3d adaptation of
jigsaw puzzle, rotation prediction, patch position prediction, and image-level
contrastive learning. another common way for pre-training on medical images is
to combine different approaches such as rotation prediction [26], restorative
autoencoders [2,26], and image-level contrastive learning [2,26].several methods
allows to obtain voxel-wise features. the model [29] maximizes the consistency
of local features in the intersection between two differently augmented images.
the algorithm [29] was mainly proposed for image retrieval and uses only feature
representations in the largest and smallest scales in separate contrastive
losses, while vox2vec produce voxels' representations via concatenation of
feature vectors from a feature pyramid and pre-train them in a unified manner
using a single contrastive loss. finally, a number of works propose
semisupervised contrastive learning methods [20], however, they require
additional task-specific manual labeling.
in a nutshell, vox2vec pre-trains a neural network to produce similar
representations for the same voxel placed in different contexts (positive pairs)
and predict distinctive representations for different voxels (negative pairs).
in the following sects. 3.1, 3.2, 3.3, we describe in detail the main components
of our method: 1) definition and sampling of positive and negative pairs of
voxels; 2) modeling voxel-level representations via a neural network; 3)
computation of the contrastive loss. the whole pre-training pipeline is
schematically illustrated in fig. 1. we also describe the methodology of the
evaluation of the pre-trained representations on downstream segmentation tasks
in sect. 3.4.
we define a positive pair as any pair of voxels that correspond to the same
location in a given volume. conversely, we call a negative pair any pair of
voxels that correspond to different locations in the same volume as well as
voxels belonging to different volumes. figure 1 (left) illustrates our strategy
for positive and negative pairs sampling. for a given volume, we sample two
overlapping 3d patches of size (h, w, d). we apply color augmentations to them,
including random gaussian blur, random gaussian sharpening, adding random
gaussian noise, clipping the intensities to the random hounsfield window, and
rescaling them to the (0, 1) interval. next, we sample m different positions
from the patches' overlapping region. each position yields a pair of voxels -one
from each patch, which results in a total of m positive pairs of voxels. at each
pre-training iteration, we repeat this procedure for n different volumes,
resulting in 2•n patches containing n = n • m positive pairs. thus, each sampled
voxel has one positive counterpart and forms negative pairs with all the
remaining 2n -2 voxels.in our experiments we set (h, w, d) = (128, 128, 32), n =
10 and m = 1000.we exclude the background voxels from the sampling and do not
penalize their representations. we obtain the background voxels by using a
simple twostep algorithm: 1) thresholding voxels with an intensity less than
-500 hu; 2) keep voxels from the same connected component as the corner voxel of
the ct volume, using a flood fill algorithm.
a standard architecture for voxel-wise prediction is 3d unet [24]. unet's
backbone returns a feature map of the same resolution as the input patch.
however, our experiments show that this feature map alone is insufficient for
modeling self-supervised voxel-level representations. the reason is that
producing a feature map with more than 100 channels in full resolution is
infeasible due to memory constraints. meanwhile, to be suitable for many
downstream tasks, representations should have a dimensionality of about 1000, as
in [8].to address this issue, we utilize a 3d fpn architecture instead of a
standard 3d unet. fpn returns voxel-level representations in the form of a
feature pyramid. the pyramid's base is a feature map with 16 channels of the
same resolution as the input patch. each next pyramid level has twice as many
channels and two times lower resolution than the previous one. each voxel's
representation is a concatenation of the corresponding feature vectors from all
the pyramid levels. we use fpn with six pyramid levels, which results in
1008-dimensional representations. see fig. 1 (right) for an illustration.
at each pre-training iteration, we fed 2 • n patches to the fpn and obtain the
representations for n positive pairs of voxels. we denote the representations in
i-th positive pair as h (1) i and h (2) i , i = 1, . . . , n. following [8],
instead of penalizing the representations directly, we project them on
128-dimensional unit sphere via a trainable 3-layer perceptron g(•) followed by
l2-normalization: zi ) , i = 1, . . . , n. similar to [8] we use the infonce
loss as a contrastive objective:where
we evaluate the quality of self-supervised voxel-level representations on
downstream segmentation tasks in three setups: 1) linear probing, 2) non-linear
probing, and 3) end-to-end fine-tuning. linear or non-linear probing means
training a voxel-wise linear or non-linear classifier on top of the frozen
representations. if the representations are modeled by the unet model, such
classifier can be implemented as one or several 1 × 1 convolutional layers with
a kernel size 1 on top of the output feature map. a linear voxel-wise head
(linear fpn head) can be implemented as follows. each pyramid level is
separately fed to its own convolutional layer with kernel size 1. then, as the
number of channels on all pyramid levels has decreased, they can be upsampled to
the full resolution and summed up. this operation is equivalent to applying a
linear classifier to fpn voxel-wise representations described in sect. 3.2.
linear fpn head has four orders of magnitude fewer parameters than fpn. the
architecture of the non-linear voxel-wise head replicates the unet's decoder but
sets the kernel size of all convolutions to 1. it has 50 times fewer parameters
than the entire fpn architecture.in the end-to-end fine-tuning setup, we attach
the voxel-wise non-linear head, but in contrast to the non-linear probing
regime, we also train the backbone.
we use vox2vec to pre-train both fpn and unet models (further vox2vec-fpn and
vox2vec-unet) in order to ablate the effect of using a feature pyramid instead
of single full-resolution feature map for modeling voxel-wise representations.
for pre-training, we use 6 public ct datasets [1,3,5,15,21,27], totaling more
than 6550 cts, covering abdomen and thorax domains. we do not use the
annotations for these datasets during the pre-training stage. pre-processing
includes the following steps: 1) cropping to the minimal volume containing all
the voxels with the intensity greater than -500 hu; 2) interpolation to the
voxel spacing of 1 × 1 × 2 mm 3 (intensities are clipped and rescaled at the
augmentation step, see sect. 3.1). we pre-train both models for 100k batches
using the adam optimizer [16] with a learning rate of 0.0003. both models are
trained on a single a100-40gb gpu for an average of 3 days. further details
about the pre-training setup can be found in supplementary materials.
we evaluate our method on the beyond the cranial vault abdomen (btcv) [19] and
medical segmentation decathlon (msd) [4] datasets. the btcv dataset consists of
30 ct scans along with 13 different organ annotations. we test our method on 6
ct msd datasets, which include 9 different organ and tumor segmentation tasks. a
5 fold cross-validation is used for btcv experiments, and a 3 fold
cross-validation for msd experiments. the segmentation performance of each model
on btcv and msd datasets is evaluated by the dice score.for our method, the
pre-processing steps are the same for all datasets, as at the pre-training
stage, but in addition, intensities are clipped to (-1350, 1000) hu window and
rescaled to (0, 1).we compare our results with the current state-of-the-art
self-supervised methods [2,26] in medical imaging. the pre-trained weights for
the swinunetr encoder and transvw unet are taken from the official repositories
of corresponding papers. in these experiments, we keep the crucial pipeline
hyperparameters (e.g., spacing, clipping window, patch size) the same as in the
original works. to evaluate the pre-trained swinunetr and transvw in linear and
nonlinear probing setups, we use similar linear and non-linear head
architectures as for vox2vec-fpn (see sect. 3.4). swinunetr and transvw cost 391
gflops and 1.2 tflops, correspondingly, compared to 115 gflops of vox2vec-fpn.we
train all models for 45000 batches of size 7 (batch size for swinunetr is set to
3 due to memory constraints), using the adam optimizer with a learning rate of
0.0003. in the fine-tuning setup, we freeze the backbone for the first 15000
batches and then exponentially increase the learning rate for the backbone
parameters from 0.00003 up to 0.0003 during 1200 batches.
the mean value and standard deviation of dice score across 5 folds on the btcv
dataset for all models in all evaluation setups are presented in table 1.
vox2vec-fpn performs slightly better than other models in the fine-tuning setup.
however, considering the standard deviation, all the fine-tuned models perform
on par with their counterparts trained from scratch.nevertheless, vox2vec-fpn
significantly outperforms other models in linear and non-linear regimes. on top
of that, we observe that in non-linear probing regime, it performs (within the
standard deviation) as well as the fpn trained from scratch while having x50
times fewer trainable parameters (see fig. 2). we demonstrate an example of the
excellent performance of vox2vec-fpn in both linear and non-linear probing
regimes in supplementary materials.we reproduce the key results on msd challenge
ct datasets, which contain tumor and organ segmentation tasks. table 2 shows
that in the vox2vec representation space, organ voxels can be separated from
tumor voxels with a quality comparable to the model trained from scratch. a
t-sne embedding of vox2vec representations on msd is available in the
supplementary materials.
in this work, we present vox2vec -a self-supervised framework for voxel-wise
representation learning in medical imaging. our method expands the contrastive
learning setup to the feature pyramid architecture allowing to pre-train
effective representations in full resolution. by pre-training a fpn backbone to
extract informative representations from unlabeled data, our method scales to
large datasets across multiple task domains. we pre-train a fpn architecture on
more than 6500 ct images and test it on various segmentation tasks, including
different organs and tumors segmentation in three setups: linear probing,
nonlinear probing, and fine-tuning. our model outperformed existing methods in
all regimes. moreover, vox2vec establishes a new state-of-the-art result on the
linear and non-linear probing scenarios. still, this work has a few limitations
to consider. we plan to investigate further how the performance of vox2vec
scales with the increasing size of the pre-training dataset and the pre-trained
architecture size. another interesting research direction is exploring the
effectiveness of vox2vec with regard to domain adaptation to address the
challenges of domain shift between different medical imaging datasets obtained
from different sources. a particular interest is a lowshot scenario when only a
few examples from the target domain are available.
fig. 2. dice score on btcv crossvalidation averaged for all organs w.r.t. the
number of trainable paramaters of different models in different evaluation
setups.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_58.
deep learning has brought medical image segmentation into the era of datadriven
approaches, and has made significant progress in this field [1,2], i.e., the
segmentation accuracy has improved considerably. in spite of the huge success,
the deployment of trained segmentation models is often severely impacted by a
distribution shift between the training (or labeled) and test (or unlabeled)
data since the segmentation performance will deteriorate greatly in such
situations. domain shift is typically caused by various factors, including
differences in acquisition protocols (e.g., parameters, imaging methods,
modalities) and characteristics of data (e.g., age, gender, the severity of the
disease and so on).domain adaptation (da) has been proposed and investigated to
combat distribution shift in medical image segmentation. many researchers
proposed using adversarial learning to tackle distribution shift problems
[3][4][5][6][7]. these methods mainly use the game between the domain classifier
and the feature extractor to learn domain-invariant features. however, they
easily suffer from the balance between feature alignment and discrimination
ability of the model. some recent researchers begin to explore self-training
based da algorithms, which generate pseudo labels for the 'other' domain samples
to fulfill self-training [8][9][10][11]. while it is very difficult to ensure
the quality of pseudo labels in the 'other' domain and is also hard to build
capable models with noise labels. however, most of these methods cannot well
handle the situation when the domains are very diverse, since it is very
challenging to learn domain-invariant features when each domain contains
domain-specific knowledge. also, the domain information itself is well utilized
in the da algorithms.to tackle the aforementioned issues, we propose utilizing
prompt learning to take full advantage of domain information. prompt learning
[12,13] is a recently emergent strategy to extend the same natural language
processing (nlp) model to different tasks without re-training. prompt learning
models can autonomously tune themselves for different tasks by transferring
domain knowledge introduced through prompts, and they can usually demonstrate
better generalization ability across many downstream tasks. very few works have
attempted to apply prompt learning to the computer vision field, and have
achieved promising results. [14] introduced prompt tuning as an efficient and
effective alternative to full finetuning for large-scale transformer models.
[15] exploited prompt learning to fulfill domain generalization in image
classification tasks. the prompts in these models are generated and used in the
very early stage of the models, which prevents the smooth combination with other
domain adaptation methods.in this paper, we introduce a domain prompt learning
method (prompt-da) to tackle distribution shift in multi-target domains.
different from the recent prompt learning methods, we generate domain-specific
prompts in the encoding feature space instead of the image space. as a
consequence, it can improve the quality of the domain prompts, more importantly,
we can easily consolidate the prompt learning with the other da methods, for
instance, adversarial learning based da. in addition, we propose a specially
designed fusion module to reinforce the respective characteristics of the
encoder features and domain-specific prompts, and thus generate domain-aware
features. as a way to prove the prompt-da is compatible with other das, a very
simple adversarial learning module is jointly adopted in our method to further
enhance the model's generalization ability (we denote this model as comb-da). we
evaluate our proposed method on two multi-domain datasets: 1). the infant brain
mri dataset for cross-age segmentation; 2). the brats2018 dataset for
cross-grade tumor segmentation. experiments show our proposed method outperforms
state-of-the-art methods. moreover, ablation study demonstrates the
effectiveness of the proposed domain prompt learning and the feature fusion
module. our claim about the successful combination of prompt learning with
adversarial learning is also well-supported by experiments.
our proposed prompt-da network consists of three main components as depicted in
fig. 1(a): a typical encoder-decoder network (e.g., unet) serving the
segmentation baseline, a prompt learning network to learn domain-specific
prompts, and a fusion module aggregating the image features and domain-specific
prompts to build domain-aware feature representation, where the fused features
are fed into the decoder. it is worth noting that our proposed method is
compatible with other da algorithms, and thus we can add an optional extra da
module to further optimize the domain generalization ability, in this paper, we
choose an adversarial learning based da as an example since it is the mostly
used da methods in medical image segmentation (as introduced in sect. 1).there
are various encoder-decoder segmentation networks, many of which are well known.
as a result, we donot introduce the details of the encoder-decoder and just
choose two typical networks to work as the segmentation backbone, that is,
3d-unet [16] (convolution based and 3d) and transunet [17] (transformer based
and 2d). in the following subsections, our focus will be on domain-specific
prompt generation and domain-aware feature learning with feature fusion.
in our designed prompt learning based da method, it is essential to learn
domain-specific prompts. moreover, the quality of generated prompts directly
determines the domain-aware features. therefore, we specially designed a prompt
generation module to learn domain-specific prompts which mainly consists of two
components, i.e., a classifier and a prompt generator.our approach incorporates
domain-specific information into the prompts to guide the model in adapting to
the target domain. to achieve this, we introduce a classifier h(x) that
distinguishes the domain (denoted as d) of the input image, shown in eq. 1.where
x is the image or abstracted features from the encoder. to optimize the
parameters, we adopt cross-entropy loss to train the classifier, as shown in eq.
2.where d is the predicted domain information, and d is the ground truth domain
information.prompt generation: instead of directly using d as the category
information, we fed the second-to-last layer's features (i.e., z) of the
classifier to a prompt generation, namely, g(z). in particular, the g(z) is a
multi-layer-perception, as defined in eq. 3.where φ can be a conv+bn+relu
sequence. note this module does not change the size of the feature map, instead,
it transforms the extracted category features into domain-specific prompts.
the learned prompt captures clearly about a certain domain and the features from
the encoder describe the semantics as well as spatial information for the
images. we can combine them to adapt the image features to domain-aware
representations.basically, suppose we have an image denoted as i, and the prompt
encodings for the domain knowledge is g(e(i)) (where e(i) is the features from a
shallow layer), e(i) is the encoder features for this image. then the
domain-aware features (i.e., f ) are extracted by a fusion module as eq. 4.as
the learned prompt and encoder feature capture quite different aspects of the
input data, we cannot achieve good effect by simply using addition,
multiplication or concatenation to serve as the fusion function ψ. specifically,
while the encoder feature emphasizes spatial information for image segmentation,
the prompt feature highlights inter-channel information for domain-related
characteristics. to account for these differences, we propose a simple
attention-based fusion (denoted as afusion) module to smoothly aggregate the
information. this module computes channel-wise and spatial-wise weights
separately to enhance both the channel and spatial characteristics of the input.
figure 1(c) illustrates the structure of our proposed module.our module utilizes
both channel and spatial branches to obtain weights for two input sources. the
spatial branch compresses the encoder feature in the channel dimension using an
fc layer to obtain spatial weights. meanwhile, the channel branch uses global
average pooling and two fc layers to compress the prompt and obtain channel
weights. we utilize fc layers for compression and rescaling, denoted as f cp and
f re respectively. the spatial and channel weights are computed according to eq.
5.afterward, we combine the weights from the spatial and channel dimensions to
obtain a token that can learn both high-level and low-level features from both
the encoder feature and the prompt, which guides the fusion of the two features.
the process is illustrated as follows:this module introduces only a few
parameters, yet it can effectively improve the quality of the prompted
domain-aware features after feature fusion. in the experimental section (i.e.,
sect. 3.3), we conducted relevant experiments to verify that this module can
indeed improve the performance of our prompt-da method.
as aforementioned, our proposed prompt-da is fully compatible with other da
algorithms. we thus use adversarial learning, which is widely adopted in medical
image da, to work as an optional component in our network to continuously
enhance the domain adaptation ability. specially, inspired by the adversarial da
in [18], we adopt the classic gan loss to train the discriminator and prompt
generator (note the adversarial loss, l adv , for the generator will only be
propagated to the prompt generator).
to optimize the segmentation backbone network, we use a combined loss function,
l seg , that incorporates both dice loss [19] and cross-entropy loss with a
balance factor.by summing the above-introduced losses, the total loss to train
the segmentation network can be defined by eq. 7.where λ is the scaling factor
to balance the losses.
we use basic 3d-unet [16] or transunet [17] as our segmentation network. we use
a fully convolutional neural network consisting of four convolutional layers
with 3 × 3 kernels and stride of 1 as the domain classifier, with each
convolution layer followed by a relu parameterized by 0.2. we used three
convolutional layers with relu activation function as the prompt generator and
constructed a discriminator with a similar structure to the classifier. we adopt
adam as the optimizer and set the learning rate to 0.0002 and 0.002 for the
segmentation and domain classifier, respectively. the learning rate will be
decayed by 0.1 every quarter of the training process.
our proposed method was evaluated using two medical image segmentation da
datasets. the first dataset, i.e., cross-age infant segmentation [20], was used
for cross-age infant brain image segmentation, while the second dataset, i.e.,
brats2018 [21], was used for hgg to lgg domain adaptation.the first dataset is
for infant brain segmentation (white matter, gray matter and cerebrospinal
fluid). to build the cross-age dataset, we take advantage 10 brain mris of
6-month-old from iseg2019 [20], and also build 3-month-old and 12-month-old
in-house datasets. in this dataset, we collect 11 brain mri for both the
3-month-old and 12-month-old infants. we take the 6-month-old data as the source
domain, the 3-month-old and 12-month-old as the target domains.the 2nd dataset
is for brain tumor segmentation (enhancing tumor, peritumoral edema and necrotic
and non-enhancing tumor core), which has 285 mri samples (210 hgg and 75 lgg).
we take hgg as the source domain and lgg as the target domain.
we compared our method with four sota methods: adda [18], cycada [22], sifa [23]
and adr [24]. we directly use the code from the corresponding papers.for fair
comparison, we have replaced the backbone of these models with the same we used
in our approach. the quantitative comparison results of cross-age infant brain
segmentation is presented in table 1, and due to space limitations, we put the
experimental results of the brain tumor segmentation task in table 1 of
supplementary material, sec.3. as observed, our method demonstrates very good da
ability on the crossage infant segmentation task, which improves about 5.46 dice
and 4.75 dice on 12-month-old and 3-month-old datasets, respectively. when
compared to the four selected sota da methods, we also show superior transfer
performance in all the target domains. specially, we outperform other sota
methods by at least 2.83 dice and 1.04 dice on the 12-month-old and 3-month-old
tasks.when transferring to a single target domain in the brain tumor
segmentation task, our proposed da solution improves about 3.09 dice in the
target lgg domain. also, the proposed method shows considerable improvements
over adda and cycada, but very subtle improvements to the sifa and adr methods
(although adr shows a small advantage on the whole category).we also visualize
the segmentation results on a typical test sample of the infant brain dataset in
fig. 2, which once again demonstrates the advantage of our method in some
detailed regions.
prompt-da vs. adv-da: since the performance reported in table 1 is achieved with
the method combining prompt-da and adv-da, we carry out more studies to
investigate: 1). does prompt-da itself shows the transfer ability? 2). is
prompt-da compatible with adv-da?the corresponding experiments are conducted on
the infant brain dataset and experimental results are shown in table 2. to make
the table more readable, we denote: no-da means only training the segmentation
network without any da strategies; adv-da presents only using adversarial
learning based da; prompt-da is the proposed prompt learning based da and
comb-da is our final da algorithm which combines both adv-da and prompt-da.as
observed in table 2, both adv-da and prompt-da can improve the transfer
performance on all the target domains. when looking into details, the proposed
prompt-da can improve more (1.44 dice and 0.65 dice respectively) compared to
the adv-da on both 12-month-old and 3-month-old with 3d-unet segmentation
backbone. when combined together (i.e., comb-da), the performance can be further
improved by a considerable margin, 2.87 dice and 1.75 dice on 12-month-old and
3-month-old respectively, compared to prompt-da. with transunet segmentation
backbone, we can find the similar phenomenon. to this end, we can draw
conclusions that 1). prompt-da itself is beneficial to improve the transfer
ability; 2). prompt-da is quite compatible with adv-da.fusion strategy for
learning domain-aware features: one of the key components of the prompt-da is to
learn domain-aware features through fusion. we have evaluated the effectiveness
of our proposed feature fusion strategy in both 3d and 2d models. for
comparison, we considered several other fusion strategies, including 'add/mul',
which adds or multiplies the encoder feature and prompt directly, 'conv', which
employs a single convolutional layer to process the concatenated features, and
'rafusion', which utilizes a reverse version of the afusion module, sending the
prompt to the spatial branch and the encoder feature to the channel branch. the
results of these experiments are presented in table 3. our experimental results
demonstrate that the proposed afusion module improves the model's performance
significantly, and it is effective for both 3d and 2d models.
in this paper, we propose a new da paradigm, namely, prompt learning based da.
the proposed prompt-da uses a classifier and a prompt generator to produce
domain-specific information and then employs a fusion module (for encoder
features and prompts) to learn domain-aware representation. we show the
effectiveness of our proposed prompt-da in transfer ability, and also we prove
that the prompt-da is smoothly compatible with the other da algorithms.
experiments on two da datasets with two different segmentation backbones
demonstrate that our proposed method works well on da problems.
a common challenge for deploying deep learning to clinical problems is the
discrepancy between data distributions across different clinical sites
[6,15,20,28,29]. this discrepancy, which results from vendor or protocol
differences, can cause a significant performance drop when models are deployed
to a new site [2,21,23]. to solve this problem, many unsupervised domain
adaptation (uda) methods [6] have been developed for adapting a model to a new
site with only unlabeled data (target domain) by transferring the knowledge
learned from the original dataset (source domain). however, most uda methods
require sufficient target samples, which are scarce in medical imaging due to
the limited accessibility to patient data. this motivates a new problem of
few-shot unsupervised domain adaptation (fsuda), where only a few unlabeled
target samples are available for training.few approaches [11,22] have been
proposed to tackle the problem of fsuda. luo et. al [11] introduced adversarial
style mining (asm), which uses a pretrained style-transfer module to generate
augmented images via an adversarial process. however, this module requires extra
style images [9] for pre-training. such images are scarce in clinical settings,
and style differences across sites are subtle. this hampers the applicability of
asm to medical image analysis. sm-ppm [22] trains a style-mixing model for
semantic segmentation by augmenting source domain features to a fictitious
domain through random interpolation with target domain features. however, sm-ppm
is specifically designed for segmentation tasks and cannot be easily adapted to
other tasks. also, with limited target domain samples in fsuda, the random
feature interpolation is ineffective in improving the model's
generalizability.in a different direction, numerous uda methods have shown high
performance in various tasks [4,[16][17][18]. however, their direct application
to fsuda can result in severe overfitting due to the limited target domain
samples [22]. previous studies [7,10,24,25] have demonstrated that transferring
the amplitude spectrum of target domain images to a source domain can
effectively convey image style information and diversify training dataset. to
tackle the overfitting issue of existing uda methods, we propose a novel
approach called sensitivityguided spectral adversarial mixup (samix) to augment
training samples. this approach uses an adversarial mixing scheme and a spectral
sensitivity map that reveals model generalizability weaknesses to generate
hard-to-learn images with limited target samples efficiently. samix focuses on
two key aspects. 1) model generalizability weaknesses: spectral sensitivity
analysis methods have been applied in different works [26] to quantify the
model's spectral weaknesses to image amplitude corruptions. zhang et al. [27]
demonstrated that using a spectral sensitivity map to weigh the amplitude
perturbation is an effective data augmentation. however, existing sensitivity
maps only use single-domain labeled data and cannot leverage target domain
information. to this end, we introduce a domain-distance-modulated spectral
sensitivity (dodiss) map to analyze the model's weaknesses in the target domain
and guide our spectral augmentation. 2) sample hardness: existing studies
[11,19] have shown that mining hard-to-learn samples in model training can
enhance the efficiency of data augmentation and improve model generalization
performances. therefore, to maximize the use of the limited target domain data,
we incorporate an adversarial approach into the spectral mixing process to
generate the most challenging data augmentations. this paper has three major
contributions. 1) we propose samix, a novel approach for augmenting target-style
samples by using an adversarial spectral mixing scheme. samix enables
high-performance uda methods to adapt easily to fsuda problems. 2) we introduce
dodiss to characterize a model's generalizability weaknesses in the target
domain. 3) we conduct thorough empirical analyses to demonstrate the
effectiveness and efficiency of samix as a plug-in module for various uda
methods across different tasks.
we denote the labeled source domain as x s = {(x s n , y s n )} n n=1 and the
unlabeled k-shot target domain as 1 depicts the framework of our method as a
plug-in module for boosting a uda method in the fsuda scenario. it contains two
components. first, a domain-distancemodulated spectral sensitivity (dodiss) map
is calculated to characterize a source model's weaknesses in generalizing to the
target domain. then, this sensitivity map is used for sensitivity-guided
spectral adversarial mixup (samix) to generate target-style images for uda
models. the details of the components are presented in the following sections.
the prior research [27] found that a spectral sensitivity map obtained using
fourier-based measurement of model sensitivity can effectively portray the
generalizability of that model. however, the spectral sensitivity map is limited
to single-domain scenarios and cannot integrate target domain information to
assess model weaknesses under specific domain shifts. thus, we introduce dodiss,
extending the previous method by incorporating domain distance to tackle domain
adaptation problems. fig. 1 (a) depicts the dodiss pipeline. it begins by
computing a domain distance map for identifying the amplitude distribution
difference between the source and target domains in each frequency.
subsequently, this difference map is used for weighting amplitude perturbations
when calculating the dodiss map.
to overcome the limitations of lacking target domain images, we first augment
the few-shot images from the target domain with random combinations of various
geometric transformations, including random cropping, rotation, flipping, and
jigsaw [13]. these transformations keep the image intensities unchanged,
preserving the target domain style information. the fast fourier transform (fft)
is then applied to all the source images and the augmented target domain images
to obtain their amplitude spectrum, denoted as a s and ât , respectively. we
calculate the probabilistic distributions p s i,j and p t i,j of a s and ât at
the (i, j) th frequency entry, respectively. the domain distance map at (i, j)
is defined as, where w 1 is the 1-wasserstein distance.
with the measured domain difference, we can now compute the dodiss map of a
model. as shown in fig. 1 (a), a fourier basis is defined as a hermitian matrix
h i,j ∈ r h×w with only two non-zero elements at (i, j) and (-i, -j). a fourier
basis image u i,j can be obtained by 2 -normalized inverse fast fourier
transform (ifft) of a i,j , i.e., u i,j =to analyze the model's generalization
weakness with respect to the frequency (i, j), we generate perturbed source
domain images by adding the fourier basis noise n i,j = r • d w (i, j) • u i,j
to the original source domain image x s as x s + n i,j . d w (i, j) controls the
2 -norm of n i,j and r is randomly sampled to be either -1 or 1. the n i,j only
introduces perturbations at the frequency components (i, j) to the original
images. the d w (i, j) guarantees that images are perturbed across all frequency
components following the real domain shift. for rgb images, we add n i,j to each
channel independently following [27]. the sensitivity at frequency (i, j) of a
model f trained on the source domain is defined as the prediction error rate
over the whole dataset x s as in (1), where acc denotes the prediction accuracy
using the dodiss map m s and an adversarially learned parameter λ * as a
weighting factor, samix mixes the amplitude spectrum of each source image with
the spectrum of a target image. dodiss indicates the spectral regions where the
model is sensitive to the domain difference. the parameter λ * mines the
heard-tolearn samples to efficiently enrich the target domain samples by
maximizing the task loss. further, by retaining the phase of the source image,
samix preserves the semantic meaning of the original source image in the
generated target-style sample. specifically, as shown in fig. 1 (b), given a
source image x s and a target image x t , we compute their amplitude and phase
spectrum, denoted as (a s , φ s ) and (a t , φ t ), respectively. samix mixes
the amplitude spectrum bythe target-style image is reconstructed by x st λ * =
ifft (a st λ * , φ s ). the adversarially learned parameter λ * is optimized by
maximizing the task loss l t using the projected gradient descent with t
iterations and step size of δ:in the training phase, as shown in fig. 1 (b), the
samix module generates a batch of augmented images, which are combined with
few-shot target domain images to train the uda model. the overall training
objective is to minimizewhere l t is the supervised task loss in the source
domain; js is the jensen-shannon divergence [27], which regularizes the model
predictions consistency between the source images x s and their augmented
versions x st λ * ; l uda is the training loss in the original uda method, and μ
is a weighting parameter.
we evaluated samix on two medical image datasets. fundus [5,14] is an optic disc
and cup segmentation task. following [21], we consider images collected from
different scanners as distinct domains. the source domain contains 400 images of
the refuge [14] training set. we took 400 images from the refuge validation set
and 159 images of rim-one [5] to form the target domain 1 & 2. we center crop
and resize the disc region to 256 × 256 as network input. camelyon [1] is a
tumor tissue binary classification task across 5 hospitals. we use the training
set of camelyon as the source domain (302, 436 images from hospitals 1 -3) and
consider the validation set (34, 904 images from hospital 4) and test set (85,
054 images from the hospital 5) as the target domains 1 and 2, respectively. all
the images are resized into 256 × 256 as network input. for all experiments, the
source domain images are split into training and validation in the ratio of 4 :
1. we randomly selected k-shot target domain images for training, while the
remaining target domain images were reserved for testing.
samix is evaluated as a plug-in module for four uda models: adaptseg [17] and
advent [18] for fundus, and srdc [16] and daln [4] for camelyon. for a fair
comparison, we adopted the same network architecture for all the methods on each
task. for fundus, we use a deeplabv2-res101 [3] as the backbone with sgd
optimizer for 80 epochs. the task loss l t is the dice loss. the initial
learning rate is 0.001, which decays by 0.1 for every 20 epochs. the batch size
is 16. for camelyon, we use a resnet-50 [8] with sgd optimizer for 20 epochs. l
t is the binary cross-entropy loss. the initial learning rate is 0.0001, which
decays by 0.1 every 5 epochs. the batch size is 128. we use the fixed weighting
factor μ = 0.01, iterations t = 10, and step size δ = 0.1 in all the
experiments.
we demonstrate the effectiveness of samix by comparing it with two sets of
baselines. first, we compare the performance of uda models with and without
samix. second, we compare samix against other fsuda methods [9,11].fundus. table
1 shows the 10-run average dice coefficient (dsc) and average surface distance
(asd) of all the methods trained with the source domain and 1-shot target domain
image. the results are evaluated in the two target domains. compared to the
model trained solely on the source domain (source only), the performance gain
achieved by uda methods (adaptseg and advent) is limited. however, incorporating
samix as a plug-in for uda methods (adapt-seg+samix and advent+samix) enhances
the original uda performance significantly (p < 0.05). moreover, samix+advent
surpasses the two fsuda methods (asm and sm-ppm) significantly. this improvement
is primarily due to spectrally augmented target-style samples by samix.to assess
the functionality of the target-aware spectral sensitivity map in measuring the
model's generalization performance on the target domain, we computed the dodiss
maps of the four models (adaptseg, asm, sm-ppm, and adaptseg+samix). the results
are presented in fig. 2(a). the dodiss map of adaptseg+samix demonstrates a
clear suppression of sensitivity, leading to improved model performance. to
better visualize the results, the model generalizability (average dsc) versus
the averaged 1 -norm of the dodiss map is presented in fig. 2 (b). the figure
shows a clear trend of improved model performance as the averaged dodiss
decreases. to assess the effectiveness of samix-augmented target-style images in
bridging the gap of domain shift, the feature distributions of fundus images
before and after adaptation are visualized in fig. 2 (c) by t-sne [12]. figure
2(c1) shows the domain shift between the source and target domain features. the
augmented samples from samix build the connection between the two domains with
only a single example image from the target domain. please note that, except the
1-shot sample, all the other target domain samples are used here for
visualization only but never seen during training/validation. incorporating
these augmented samples in adaptseg merges the source and target distributions
as in fig. 2 (c2).
as the availability of target domain images is limited, data efficiency plays a
crucial role in determining the data augmentation performance. therefore, we
evaluated the model's performance with varying numbers of target domain images
in the training process.
to assess the efficacy of the components in samix, we conducted an ablation
study with adaptseg+samix and daln+samix (full model) on fundus and camelyon
datasets. this was done by 1) replacing our proposed dodiss map with the
original one in [27] (original map); 2) replacing the samix module with the
random spectral swapping (fda, β = 0.01, 0.09) in [25]; 3) removing the three
major components (no l uda , no samix, no js) in a leave-one-out manner. figure
5 suggests that, compared with the full model, the model performance degrades
when the proposed components are either removed or replaced by previous methods,
which indicates the efficacy of the samix components.
this paper introduces a novel approach, sensitivity-guided spectral adversarial
mixup (samix), which utilizes an adversarial mixing scheme and a spectral
sensitivity map to generate target-style samples effectively. the proposed
method facilitates the adaptation of existing uda methods in the few-shot
scenario. thorough empirical analyses demonstrate the effectiveness and
efficiency of samix as a plug-in module for various uda methods across multiple
tasks.
*
camelyon.
gbc is a deadly disease that is difficult to detect at an early stage [12,15].
early diagnosis can significantly improve the survival rate [14]. non-ionizing
radiation, low cost, and accessibility make us a popular non-invasive diagnostic
modality for patients with suspected gall bladder (gb) afflictions. however,
identifying signs of gbc from routine us imaging is challenging for radiologists
[11]. in recent years, automated gbc detection from us images has drawn
increased interest [3,5] due to its potential for improving diagnosis and
treatment outcomes. many of these works formulate the problem as an object
detection, since training a image classification model for gbc detection seems
challenging due to the reasons outlined in the abstract (also see fig. 1).
recently, gbcnet [3], a cnn-based model, achieved sota performance on
classifying malignant gb from us images. gbcnet uses a two-stage pipeline
consisting of object detection followed by classification, and requires bounding
box annotations for gb as well as malignant regions for training. such bounding
box annotations surrounding the pathological regions are time-consuming and
require an expert radiologist for annotation. this makes it expensive and
non-viable for curating large datasets for training large dnn models. in another
recent work, [5] has exploited additional unlabeled video data for learning good
representations for downstream gbc classification and obtained performance
similar to [3] using a resnet50 [13] classifier. the reliance of both sota
techniques on additional annotations or data, limits their applicability. on the
other hand, the image-level malignancy label is usually available at a low cost,
as it can be obtained readily from the diagnostic report of a patient without
additional effort from clinicians.instead of training a classification pipeline,
we propose to solve an object detection problem, which involves predicting a
bounding box for the malignancy. the motivation is that, running a classifier on
a focused attention/ proposal region in an object detection pipeline would help
tackle the low inter-class and high intra-class variations. however, since we
only have image-level labels available, we formulate the problem as a weakly
supervised object detection (wsod) problem. as transformers are increasingly
outshining cnns due to their ability to aggregate focused cues from a large area
[6,9], we choose to use transformers in our model. however, in our initial
experiments sota wsod methods for transformers failed miserably. these methods
primarily rely on training a classification pipeline and later generating
activation heatmaps using attention and drawing a bounding box circumscribing
the heatmaps [2,10] to show localization. however, for gbc detection, this line
of work is not helpful as we discussed earlier.inspired by the success of the
multiple instance learning (mil) paradigm for weakly supervised training on
medical imaging tasks [20,22], we train a detection transformer, detr, using the
mil paradigm for weakly supervised malignant region detection. in this, one
generates region proposals for images, and then considers the images as bags and
region proposals as instances to solve the instance classification (object
detection) under the mil constraints [8]. at inference, we use the predicted
instance labels to predict the bag labels. our experiments validate the utility
of this approach in circumventing the challenges in us images and detecting gbc
accurately from us images using only image-level labels.
the key contributions of this work are:-we design a novel detr variant based on
mil with self-supervised instance learning towards the weakly supervised disease
detection and localization task in medical images. although mil and
self-supervised instance learning has been used for cnns [24], such a pipeline
has not been used for transformerbased detection models. -we formulate the gbc
classification problem as a weakly supervised object detection problem to
mitigate the effect of low inter-class and large intra-class variances, and
solve the difficult gbc detection problem on us images without using the costly
and difficult to obtain additional annotation (bounding box) or video data. -our
method provides a strong baseline for weakly supervised gbc detection and
localization in us images, which has not been tackled earlier. further, to
assess the generality of our method, we apply our method to polyp detection from
colonoscopy images.
gallbladder cancer detection in ultrasound images: we use the public gbc us
dataset [3] consisting of 1255 image samples from 218 patients. the dataset
contains 990 non-malignant (171 patients) and 265 malignant (47 patients) gb
images (see fig. 2 for some sample images). the dataset contains image labels as
well as bounding box annotations showing the malignant regions.note that, we use
only the image labels for training. we report results on 5-fold
cross-validation. we did the cross-validation splits at the patient level, and
all images of any patient appeared either in the train or validation split.
polyp detection in colonoscopy images: we use the publicly available kvasir-seg
[17] dataset consisting of 1000 white light colonoscopy images showing polyps
(see fig. 2). since kvasir-seg does not contain any control images, we add 600
non-polyp images randomly sampled from the polypgen [1] dataset.since the
patient information is not available with the data, we use random stratified
splitting for 5-fold cross-validation.
revisiting detr: the detr [6] architectures utilize a resnet [13] backbone to
extract 2d convolutional features, which are then flattened and added with a
positional encoding, and fed to the self-attention-based transformer encoder.
the decoder uses cross-attention between learned object queries containing
positional embedding, and encoder output to produce output embedding containing
the class and localization information. the number of object queries, and the
decoder output embeddings is set to 100 in detr. subsequently, a feed-forward
network generates predictions for object bounding boxes with their corresponding
labels and confidence scores.proposed architecture: fig. 3 gives an overview of
our method. we use a coco pre-trained class-agnostic detr as proposal generator.
the learned object queries contain the embedded positional information of the
proposal. classagnostic indicates that all object categories are considered as a
single object class, as we are only interested in the object proposals. we then
finetune a regular, class-aware detr for the wsod task. this class-aware detr is
initialized with the checkpoint of the class-agnostic detr. the learned object
queries from the class-agnostic detr is frozen and shared with the wsod detr
during finetuning to ensure that the class-aware detr attends similar locations
of the object proposals. the class-agnostic detr branch is frozen during the
finetuning phase. we finally use the mil-based instance classification with the
self-supervised instance learning over the finetuning branch. for gbc
classification, if the model generates bounding boxes for the input image, then
we predict the image to be malignant, since the only object present in the data
is the cancer.mil setup: the decoder of the fine-tuning detr generates r
d-dimensional output embeddings. each embedding corresponds to a proposal
generated by the class-agnostic detr. we pass these embeddings as input to two
branches with fc layers to obtain the matrices x c ∈ r r×nc and x r ∈ r r×nc ,
where r is the number of object queries (same as proposals) and n c is the
number of object (disease) categories. let σ(•) denote the softmax operation. we
then generate the class-wise and detection-wise softmax matrices c ∈ r r×nc and
d ∈ r r×nc , where c ij = σ((x c ) t j )i and d ij = σ(x r i )j, and x i denotes
the i-th row of x. c provides classification probabilities of each proposal, and
d provides the relative score of the proposals corresponding to each class. the
two matrices are element-wise multiplied and summed over the proposal dimension
to generate the image-level classification predictions, φ ∈ r nc :notice, φ j ∈
(0, 1) since c ij and d ij are normalized. finally, the negative loglikelihood
loss between the predicted labels, and image labels y ∈ r nc is computed as the
mil loss:the mil classifier further suffers from overfitting to the distinctive
classification features due to the mismatch of classification and detection
probabilities [24].to tackle this, we further use a self-supervised module to
improve the instances.
inspired by [24], we design a instance learning module with n r blocks in a
self-supervised framework to refine the instance scores with instance-level
supervision. each block consists of an fc layer. a class-wise softmax is used to
generate instance scores x n ∈ r r×(nc+1) at n-th block. n c + 1 includes the
background/ no-finding class. instance supervision of each layer (n) is obtained
from the scores of the previous layer (x (n-1) ). the instance supervision for
the first layer is obtained from the mil head. suppose ŷn ∈ r r×(nc+1) is the
pseudo-labels of the instances. an instance (p j ) is labelled 1 if it overlaps
with the highest-scoring instance by a chosen threshold. otherwise, the instance
is labeled 0 as defined in eq. 3:the loss over the instances is given by eq.
4:here x n ij denotes the score of i-th instance for j-th class at layer n.
following [24], the loss weightis applied to stabilize the loss. assuming λ to
be a scaling value, the overall loss function is given in eq. 5:
experimental setup: we use a machine with intel xeon gold 5218@2.30ghz processor
and 8 nvidia tesla v100 gpus for our experiments. the model is trained using sgd
with lr 0.001 (for mil head), weight decay 10 -6 , and momentum 0.9 for 100
epochs with batch size 32. the lr at backbone and transformer are 0.003, and
0.0003, respectively. we use a cosine annealing of the lr. comparison with sota:
table 1 shows the bounding box localization results of the wsod task. our method
surpasses all latest sota wsod techniques by 9 points, and establishes itself as
a strong wsod baseline for gbc localization in us images. our method also
achieves 7-point higher ap score for polyp detection. we present visualizations
of the predicted bounding boxes in fig. 4 which shows that the localization by
our method is more precise and clinically relevant as compared to the baselines.
we assess the generality of our method by applying it to polyp detection on
colonoscopy images. the applicability of our method on two different tasks -(1)
gbc detection from us and (2) polyp detection from colonoscopy, indicates the
generality of the method across modalities.
we show the detection sensitivity to the self-supervised instance learning
module in table 2 for two variants, (1) vanilla mil head on detr, and (2) mil
with self-supervised instance learning on detr. table 2 shows the average
precision and detection sensitivity for both diseases. the results establish the
benefit of using the self-supervised instance learning. other ablations related
to the hyper-parameter sensitivity is given in supplementary fig.
s1.classification performance: we compare our model with the standard cnnbased
and transformer-based classifiers, sota wsod-based classifiers, and sota
classifiers using additional data or annotations (table 3). our method beats the
sota weakly supervised techniques and achieves 1.2% higher sensitivity for gbc
detection. the current sota gbc detection models require additional bound- [25]
0.829 ± 0.030 0.900 ± 0.040 0.875 ± 0.063 pvtv2 [26] 0.824 ± 0.033 0.887 ± 0.057
0.894 ± 0.076 radformer [4] 0.921 ± 0.062 0.961 ± 0.049 0.923 ± 0.062 additional
data/ annotation uscl [7] 0.889 ± 0.047 0.895 ± 0.054 0.869 ± 0.097 us-ucl [5]
0.920 ± 0.034 0.926 ± 0.043 0.900 ± 0.046 gbcnet [3] 0.921 ± 0.029 0.967 ± 0.023
0.919 ± 0.063 point-beyond-class [18] ts-cam [10] 0.704 ± 0.017 0.394 ± 0.042
0.891 ± 0.054 scm [2] 0.751 ± 0.026 0.523 ± 0.014 0.523 ± 0.016 od-wscl [21]
0.805 ± 0.056 0.609 ± 0.076 0.923 ± 0.034 ws-detr [19] 0.857 ± 0.071 0.812 ±
0.088 0.882 ± 0.034 point-beyond-class [18] 0.953 ± 0.007 0.993 ± 0.004 0.924 ±
0.011 ours 0.878 ± 0.067 0.785 ± 0.102 0.932 ± 0.022 ing box annotation [3] or,
us videos [5,7]. however, even without these additional annotations/ data, our
method reaches 86.1% detection sensitivity. the results for polyp classification
are reported in table 4. although our method has a slightly lower specificity,
the sensitivity surpasses the baselines reported in literature [16], and the
sota wsod based baselines.
gbc is a difficult-to-detect disease that benefits greatly from early diagnosis.
while automated gbc detection from us images has gained increasing interest from
researchers, training a standard image classification model for this task is
challenging due to the low inter-class variance and high intra-class variability
of malignant regions. current sota models for gbc detection require costly
bounding box annotation of the pathological regions, or additional us video
data, which limit their applicability. we proposed to formulate gbc detection as
a weakly supervised object detection/ localization problem using a detr with
selfsupervised instance learning in a mil framework. our experiments show that
the approach achieves competitive performance without requiring additional
annotation or data. we hope that our technique will simplify the model training
at the hospitals with easily available data locally, enhancing the applicability
and impact of automated gbc detection.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0 20.
precision medicine efforts are shifting cancer care standards by providing novel
personalised treatment plans with promising outcomes. patient selection for such
treatment regimes is based principally on the assessment of tissue biopsies and
the characterisation of the tumor microenvironment. this is typically performed
by experienced pathologists, who closely inspect chemically stained
histopathological whole slide images (wsis). increasingly, clinical centers are
investing in the digitisation of such tissue slides to enable both automatic
processing as well as research studies to elucidate the underlying biological
processes of cancer. the resulting images are of gigapixel size, rendering their
computational analysis challenging. to deal with this issue, multiple instance
learning (mil) schemes based on weakly supervised training are used for wsi
classification tasks. in such schemes, the wsi is typically divided into a grid
of patches, with general purpose features derived from pretrained imagenet [18]
networks extracted for each patch. these representations are subsequently pooled
together using different aggregation functions and attention-based operators for
a final slide-level prediction.state space models are designed to efficiently
model long sequences, such as the sequences of patches that arise in wsi mil. in
this paper, we present the first use of state space models for wsi mil.
extensive experiments on three publicly available datasets show the potential of
such models for the processing of gigapixel-sized images, under both weakly and
multi-task schemes. moreover, comparisons with other commonly used mil schemes
highlight their robust performance, while we demonstrate empirically the
superiority of state space models in processing the longest of wsi sequences
with respect to commonly used mil methods.
using pretrained networks for patch-wise feature extraction is a well
established strategy for histopathology analysis [4,20]. an extension of this
approach is with mil, where the patch-wise features of an entire slide are
digested simultaneously by an aggregator model, such as attention-based models
clam [17] and transmil [19], the latter being a variant of self-attention
transformers [21]. [3] proposes another transformer-based method in the form of
a hierarchical vit. similar to our multitask experiments, [6] explores combining
slide-level and tilelevel annotations with a minimal point-based annotation
strategy. one of the key components of mil methods is the aggregation module
that pools together the set of patch representations. mean or max pooling
operations are among the simplest and most effective for aggregating predictions
over a whole slide [2]. in contrast, recurrent neural networks (rnn) with long
short-term memory (lstm) [14] model the patches more explicitly as a set of
tokens in sequence. in particular, lstm networks have been shown to work well in
different mil settings including both visual cognition [22] and computational
pathology [1].the state space model is a linear differential equation,that is
widely studied in control theory, and describes a continuous time process for
input and output signals u(t) ∈ r p and y(t) ∈ r q , and state signal x(t) ∈ r n
, and where the process is governed by matricesin hippo [9] (high-order
polynomial projection operator), continuous time memorisation is posed as a
problem of function approximation in a hilbert space defined by a probability
measure μ. for a scaled legendre probability measure, one obtains the hippo
matrix a, which enforces uniform weight in the memorisation of all previously
observed inputs, in contrast to the exponentially decaying weighting of the
constant error carousel of lstms [14]. the hippo mode of memorisation is shown
empirically to be better suited to modeling long-range dependencies (lrd) than
other neural memory layers, for which it serves as a drop-in replacement.whereas
in hippo, the state matrix a is a fixed constant, the linear state space layer
(lssl) [12] incorporates a as a learnable parameter. however, this increased
expressiveness introduces intractable powers of a. in [10], the lssl is instead
reparameterised as the sum of diagonal and low-rank matrices, allowing for the
efficient computation of the layer kernel in fourier space. this updated
formulation is known as the structured state space sequence layer (s4). note
that as a linear operator, the inverse discrete fourier transform is amenable to
backpropagation in the context of a neural network. note also that under this
formulation, the hidden state x(t) is only computed implicitly. finally, [11]
presents a simplification of the s4 layer, known as diagonal s4 (s4d), in which
a is approximated by a diagonal matrix.
given that the patch extraction of whole slide images at high magnifications
results in long sequences of patches, we propose to incorporate a state space
layer in a mil aggregation network to better represent each patch sequence.
in practice, neural state space models (ssm) simulate eq. 1 in discrete time,
invoking a recurrence relation on the discretised hidden state,where the
sequences u t , x t , and y t are the discretised u(t), x(t), and y(t), and the
modified model parameters arise from a bilinear discretisation [12]. as such,
ssms bear an inherent resemblance to rnns, where the hidden representation x t
can be interpreted as a memory cell for the observed sequence over the interval
[0, t], and with du t acting as a skip connection between the input and output
at point t. due to their lack of non-linearities, state space models can also be
viewed as a convolution between two discrete sequences. playing out the
recurrence in eq. 2, one obtains,where u ∈ r l and y ∈ r l are the full input
and output sequences, and the sequence k ∈ r l is defined as,which is computed
efficiently by the s4d algorithm [11]. note that although ssm layers are linear,
they may be combined with other, non-linear layers in a neural network. note
also that although eq. 3 is posed as modeling a onedimensional signal, in
practice multi-dimensional inputs are modelled simply by stacking ssm layers
together, followed by an affine "mixing" layer.
in our pipeline (fig. 1) wsis are first divided into a sequence of l patches {u
1 , u 2 , . . . , u l }, where l will vary by slide. a pretrained resnet50 is
then used to extract a 1024-dimensional feature vector from each patch {u 1 , u
2 , . . . , u l }, which constitute the model inputs. we define a ssm-based
neural network f to predict a wsi-level class probability given this input
sequence,the architecture of f is composed of an initial linear projection
layer, used to lower the dimensionality of each vector in the input sequence. a
ssm layer is then applied feature-wise by applying the s4d algorithm. that is,
eq. 3, including the skip connection, transforms the sequence {u 1,d , u 2,d , .
. . , u l,d } for all features d, and the resulting sequences are concatenated.
a linear "mixing" layer is applied token-wise, doubling the dimensionality of
each token, followed by a gated linear unit [5] acting as an output gate, which
restores the input dimensionality. for the ssm layer, we used the official
implementation of s4d1 . a max pooling layer merges the ssm layer outputs into a
single vector, which is projected by a final linear layer and softmax to give
the class probabilities ŷ. the model is trained according to,where ŷcm denotes
the probability corresponding to c m , the slide-level label of the sequence
corresponding to the m th of m whole slide images.
one advantage of processing an entire slide as a sequence is the ease with which
additional supervision may be incorporated, when available. a patch-level ground
truth creates the opportunity for multitask learning, which can enhance the
representations learned for slide-level classification. as an extension of our
base model in eq. 6, we train a multitask model to jointly predict a slide-level
and patch-level labels. prior to the max pooling layer of the base model, an
additional linear layer is applied to each sequence token, yielding l additional
model outputs. this multitask model is trained according to a sum of log
losses,where c m,l indexes the class of the l th patch in the m th training
slide and λ is a tunable hyperparameter used to modulate the relative importance
of each task.
we extracted patches of size 256 × 256 from the tissue regions of wsis at 20x
magnification. following clam [17], the third residual block of a pretrained
resnet50 [13] was used as a feature extractor, followed by a mean pooling
operation, resulting in a 1024-dimensional representation for each patch. these
features were used as inputs to all models. all model training was performed
under a 10-fold cross-validation, and all reported results are averaged over the
validation sets of the folds, aside from camelyon16, for which the predefined
test set was utilized. thus, for camelyon16, we report test set performances
averaged over the validation. baseline models were chosen to be prior art clam
[17] and transmil [19]. the official code of these two models was used to
perform the comparison. in addition, we included a vanilla transformer, a lstm
rnn, and models based on mean and max pooling. our vanilla transformer is
composed of two stacked self-attention blocks, with four attention heads, a
model dimension of 256, and a hidden dimension of 256. for the lstm, we used an
embedding size of 256 and a width of 256. the pooling models applied pooling
feature-wise across each sequence, then used a random forest with 200 trees for
classification. for the s4 models, the dimension of the state matrix a was tuned
to 32 for camelyon16 and tcga-rcc, and 128 for tcga-luad. our models were
trained using the adam [15] optimizer with the lookahead method [23], with a
learning rate of 2 • 10 -4 , and weight decay of 10 -4 for tcga-luad and
tcga-rcc and 10 -3 for camelyon16. early stopping with a patience of 10 was used
for all our training. our implementation is publicly available2 .
camelyon16 [16] is a dataset that consists of resections of lymph nodes, where
each wsi is annotated with a binary label indicating the presence of tumour
tissue in the slide, and all slides containing tumors have a pixel-level
annotation indicating the metastatic region. in multitask experiments, we use
this annotation to give each patch a label indicating local tumour presence.
there are 270 wsis in the training/validation set, and 130 wsis in the
predefined test set. in our experiments, the average patch sequence length
arising from camelyon16 is 6129 (ranging from 127 to 27444).tcga-luad is a tcga
lung adenocarcinoma dataset that contains 541 wsis along with genetic
information about each patient. we obtained genetic information for this cohort
using xena browser [7]. as a mil task, we chose the task of predicting the
patient mutation status of tp53, a tumor suppressor gene that is highly relevant
in oncology studies. the average sequence length is 10557 (ranging from 85 to
34560).tcga-rcc is a tcga dataset for three kidney cancer subtypes (denoted
kich, kirc, and kirp). it consists of 936 wsis (121 kich, 518 kirc, and 297
kirp). the average sequence length is 12234 (ranging from 319 to 62235).
multiple instance learning results. we evaluate our method on each dataset by
accuracy and area under receiver operating characteristic curve (auroc). for
multiclass classification, these were computed in a one-versus-rest manner.table
1 summarises the comparison between our proposed model and baselines. for the
camelyon16 dataset, our method performs on par with trans-mil and the clam
models, while it clearly outperforms the other methods. similarly, in the
tcga-luad dataset the proposed model achieves comparable performance with both
clam models, while outperforming transmil and the other methods. we note that
tcga-luad proves to be a more challenging dataset for all models. moreover, our
method outperforms clam models on the tcga-rcc dataset, while reporting very
similar performance with respect to transmil. overall, looking at the average
metrics per model across all three datasets, our proposed method achieves the
highest accuracy and the second highest auroc, only behind clam-mb. a pairwise
t-test between the proposed method, clam, and transmil shows that there is no
statistical significance performance difference (see supplementary material).we
further compare our method with respect to model and time complexity. in table 2
we report the number of trainable parameters, as well as the inference time for
all models. the number of parameters is computed with all models configured to
be binary classifiers, and the inference time is computed as the average time
over 100 samples for processing a random sequence of 1024-dimensional vectors of
length 30000. for our proposed method, we report both models with the different
state dimensions (ours (ssm 32 )) and (ours (ssm 128 )). compared with transmil,
our method runs four times faster and has less than half the parameters. the
clam models are more efficient in terms of number of trainable parameters, yet
clam mb is slower. table 3 shows the effect of modifying parts of the
architecture on the results for tcga-rcc. most modifications had very little
impact on auroc, but a more significant impact can be seen on the accuracy of
the model. models a and b show that stacking multiple ssm layers results in
lower accuracy, which was observed over all three datasets, while models c and d
show that modifying the state dimension of the ssm module can have an impact on
the accuracy. the optimal state space dimension varies depending on the dataset.
we explored the ability of our model to combine slide-and patch-level
information on the cameylon16 dataset. we compared our model with the best
performing model on camelyon16, transmil. both models were trained according to
eq. 7 with λ = 5 tuned by hand. in table 4 we give slide-level accuracy and
auroc for the two models. we observe that all accuracies and auroc increase
compared with those reported in table 1. this indicates that the use of
patch-level annotations complements the learning of the slide-level label. we
furthermore observe that our model outperforms transmil when combining slide-and
patch-level annotations. we map the sequence of output probabilities to their
slide coordinates giving a heatmap localising metastasis (see supplementary
material). performance on longest sequences. in order to highlight the inherent
ability of ssm models to effectively model long sequences, we performed an
experiment on only the largest wsis of the tcga-rcc dataset. indeed, this
dataset contains particularly long sequences (up to 62235 patches at 20x). we
evaluated the trained models for each fold on a subset of the validation set,
only containing sequences with a length in the 85 th percentile. table 5 shows
the obtained average accuracy (weighted by the number of long sequences in each
validation set) and auroc on both clam models, transmil, and our proposed
method. both in terms of auroc and accuracy, our method outperforms the other
methods on long sequences, while the performances are comparable to table 1,
albeit slightly lower, illustrating the challenge of processing large wsis.
in this work we have explored the ability of state space models to act as
multiple instance learners on sequences of patches extracted from histopathology
images. these models have been developed for their ability to memorise long
sequences, and they have proven competitive with state of the art mil models
across a range of pathology problems. additionally, we demonstrated the ability
of these models to perform multiclass classification, which furthermore allowed
us to visualise the localisation of metastasic regions. finally, we demonstrated
that on the longest sequences in our datasets, state space models offer better
performance than competing models, confirming their power in modeling long-range
dependencies.
* indicates results from[19].
longitudinal lesion or tumor tracking is a fundamental task in treatment
monitoring workflows, and for planning of re-treatments in radiation therapy.
based on longitudinal imaging for a given patient it requires establishing which
lesions are corresponding (i.e., same lesion, observed at different timepoints),
which lesions have disappeared and which are new compared to prior scanning.
this information can be leveraged to assess treatment response, e.g., by
analyzing the evolution of size and morphology for a given tumor [1], but also
for adaptation of (re-)treatment radiotherapy plans that take into account new
tumors.in practice, the development of automatic and reliable lesion tracking
solutions is hindered by the complexity of the data (over different modalities),
the absence of large, annotated datasets, and the difficulties associated with
lesion identification (i.e., varying sizes, poses, shapes, and sparsely
distributed locations). in this work, we present a multi-scale self-supervised
learning solution for lesion tracking in longitudinal studies using the
capabilities of contrastive learning [9]. inspired by the pixel-wise contrastive
learning strategy introduced in [5], we choose to learn pixel-wise feature
representations that embed consistent anatomical information from unlabeled
(i.e., without lesion-related annotations) and unpaired (i.e., without the use
of longitudinal scans) data, overcoming barriers to data collection. to increase
the system robustness and emulate the clinician's reading strategies, we propose
to use multi-scale embeddings to enable the system to progressively refine the
fine-grained location. in addition, as imaging offers contextual information
about the human body that is naturally consistent, we design the model to
benefit from biologically-meaningful points (i.e., anatomical landmarks). the
reasoning behind this strategy is that simple data augmentation methods cannot
faithfully model inter-subject variability or possible organ deformations.
hence, we ensure the spatial coherence of the tracked lesion location using
well-defined anatomical landmarks.our proposed method brings two elements of
novelty from a technical point of view: (1) the multi-scale approach for the
anatomical embedding learning and (2) a positive sampling approach that
incorporates anatomically significant landmarks across different subjects. with
these two strategies, the goal is to ensure a high degree of robustness in the
computation of the lesion matching across different lesion sizes and varying
anatomies. furthermore, a significant focus and contribution of our research is
the experimental study at a very large scale: we (1) train a pixel-wise
self-supervised system using a very large and diverse dataset of 52,487 ct
volumes and (2) evaluate on two publicly available datasets. notably, one of the
datasets, nlst, presents challenging cases with 68% of lesions being very small
(i.e., radius < 5 mm).
the problem of lesion tracking in longitudinal data is typically divided into
two steps: (1) detection of lesions and (2) tracking the same lesion over
multiple time points. classical methods to solve this problem rely on image
registration, where tracking is performed via image alignment and rule-based
correspondence matching [15,16,21]. these approaches are difficult to optimize,
especially when scaling across different body regions and fields of view.
appearance-based trackers [19,20] adopt a different strategy by projecting
lesions detected beforehand with dedicated detectors [17,18] onto a
representation space and employing nearest neighbor analysis. one recent
approach, deep lesion tracker (dlt) [8], integrates both strategies to perform
appearance-based recognition under anatomical constraints. as a more direct
matching approach, yan et al. [5] uses a self-supervised anatomical embedding
model (sam) to create semantic embeddings for each image pixel, avoiding the
detection step. training exclusively on augmented paired data prevents sam from
accurately representing anatomical changes and deformations that occur over
time. this can influence the contextual information of a pixel, which in turn
impacts the pixel-wise embeddings on which the similarity-based tracker depends.
to overcome this, we propose to train a pixel-wise multi-scale embedding model
that accounts for anatomical similarity among different subjects, making the
embeddings more effective.
let i 1 (i.e., template or baseline image) and i 2 (i.e., query or follow-up
image) be two 3d-ct scans acquired at time t 1 and t 2 , respectively,
additionally, let p 1 and p 2 denote the point of interest (i.e., the lesion
center) in both images. the problem of lesion tracking can be formulated as
finding the optimal transformation that maps p 1 to its corresponding location,
p 2 , in i 2 .
let d = {x 1 , x 2 , ..., x n } be a set of n unpaired and unlabeled 3d-ct
volumes.as shown in fig. 1, given an image x ∈ r d×h×w from the training dataset
d, we randomly select two overlapping 3d patches (anchor and query), namely x a
and x q . to create synthetic paired data that mimics appearance changes across
different images, we apply random data augmentation (i.e., random spatial and
intensity-related transformations) to the content of x a and x q . we implement
a similar augmentation strategy to that described in [5]. given x a and x q , we
use an embedding extraction model to construct a hierarchy of multi-scale
semantic embeddings for each image pixel, labeled f a and f q respectively. the
embedding at ith scale, 1 ≤ i ≤ s, is denoted as f i a and f i q and is
represented as a 4d feature map, with an embedding vector of length l associated
with each pixel.given the nature of contrastive learning, the sampling strategy
(extracting negative and positive pixel pairs from augmented 3d paired patches)
is essential to achieving discriminative pixel-wise embeddings. we arbitrary
sample n pos positive pixel pairs from the overlapping area of x a and x q ,
denoted by a + = {a + 1 , ..., a + j ..., a + npos }, q + = {q + 1 , ..., q + j
, ..., q + npos }, 1 ≤ j ≤ n pos . to further enhance embeddings, 10% of the
positive pixel pairs are derived from biologically-meaningful points across
different volumes in the batch. we use data-driven models [14] to extract 37
anatomical landmarks, such as the top right lung, suprasternal notch, tracheal
bifurcation, etc. similar to [5], for each positive pixel pair (a + j , q + j ),
we select n neg hard and diverse negative pixels, denoted bynext, at each scale,
we extract the embedding vectors for positive and negative pixel pairs from f i
a , f i q , guided by the corresponding locations, a + , q + , h -, which are
downsampled to match the scale. we denote the positive embeddings at ith scale
at pixel location a + j , q + j as f a i j , f q i j ∈ r l . similarly, we
denote the negative embeddings at pixel location h - k associated to a positive
positive pixel pair (a + j , q + j ) as f i jk ∈ r l . we use l2-norm to
normalize the embedding vectors before the loss computation. we use pixel-wise
infonce loss [5,10] to enhance the similarity among similar pixels (i.e.,
positive pairs of pixels) and decrease the similarity among dissimilar pixels
(i.e., negative pairs of pixels). correspondingly, we set the contrastive loss
at the ith scale:where τ = 0.5 is a temperature parameter. the final loss is
then calculated as the average of all these individual losses.
let x a be a 3d-ct volume template with an input point of interest p a ∈ x a ,
and x q a corresponding query 3d-ct volume. the first step is to project the
image x a into a multi-scale feature space, creating a hierarchy of multi-scale
semantic embeddings f a for each pixel in the image (i.e., a 4d feature
map).next, we follow a similar process for the query image x a and acquire the
pixellevel embeddings f q .to measure the similarity between the embeddings of
the input x a at the point of interest p a and the query embeddings f q , we
compute cosine similarity maps at each scale:(2)finally, we combine the
multi-scale similarity maps through summation and select the voxel with the
highest similarity as the matching point in the query volume.
datasets: we train the universal and fine-grained anatomical point matching
model using an in-house ct dataset (variousct). the training dataset contains
52,487 unlabeled 3d ct volumes capturing various anatomies, including chest,
head, abdomen, pelvis, and more.the evaluation is based on two datasets, the
publicly released deep longitudinal study (dls) dataset [8] and the national
lung screening trial (nlst) dataset [12]. the dls dataset is a subset of the
deeplesion [11] medical imaging dataset, containing 3891 pairs of lesions with
information on their location and size. the dataset covers various types of
lesions across different organs. we follow the official data split for dls
dataset and perform evaluation on the testing dataset which comprises 480 lesion
pairs. for nlst, we randomly selected a subset of 1045 test images coming from
420 patients with up to 3 studies. a certified radiologist annotated the testing
data by identifying the location and size of the pulmonary nodules, resulting in
a total of 825 paired annotations. we evaluate lesion tracking in both
directions, from baseline to follow-up and from follow-up to baseline [8]. this
results in a total of 960 and 1650 testing lesion pairs in dls and nlst test
sets, respectively. the isotropic resolution of all ct volumes is adjusted to
2mm through bilinear interpolation.system training: our learning model is
implemented in pytorch and uses the torchio library [13] for medical data
manipulation and augmentation.we employ a u-net-based encoder-decoder
architecture [2] that utilizes an inflated 3d resnet-18 [3,4] as its encoder,
which extends all 2d convolutions table 1. comparison between the proposed
solution and several state-of-the-art approaches (reference results are from
[8]). the exact same test set was used to compute the performance of each
approach listed in the table; however, we retrained only sam. in the standard
resnet to 3d convolutions and allows the use of pre-trained imagenet weights.
the multi-scale embedding model employs s = 5 scales, and the embedding length
is fixed at l = 128 for each scale. convolution with a stride of (2, 2, 2) is
used to reduce the feature map size at the first and fifth levels, while a
stride of (1, 2, 2) is employed for intermediary levels 2 to 4. the u-net
decoder uses a convolution layer with a 3 × 3 × 3 kernel after every up-sampling
layer to generate the final cascade of feature embeddings. the model is trained
with adamw optimizer [6] for 64 epochs using an early stopping strategy with a
patience of 5 epochs, a batch size of 8 augmented 3d paired patches of 32 × 96 ×
96, and a learning rate of 0.0001.
for data augmentation, we apply random cropping, scaling, rotation, and gaussian
noise injections. a windowing approach that covers the intensity ranges of lungs
and soft tissues is used to scale ct intensity values to [-1, 1]. the sampling
hyperparameters consist of 100 positive pixel pairs (n pos = 100), 100 hard
negative pixel pairs, and 200 diverse negative pixel pairs (n neg = 300).
we use mean euclidean distance (med) to measure the distance between predicted
lesion center and ground truth, and the center point matching accuracy (i.e.,
percentage of accurately matched lesions given the annotated lesion radius),
denoted with cpm@radius. for lesions of large sizes, we set a maximum distance
limit of 10 mm as acceptance criteria [8], denoted with cpm@10 mm. the nlst
testing dataset has a distinctive feature wherein nodules are relatively small,
68% of annotated lesions have a radius of less than 5 mm (compared to 6% in dls
dataset). to ensure that such small nodules are not missed during evaluation, we
relax the minimum distance requirement and consider a distance of 6 mm as a
permissible matching error.
for the lesion tracking task on dls dataset, we quantitatively compare our
system against existing trackers in table 1. these include the deep lesion
tracker (dlt) and its variants [8], as well as registration-based trackers
[15,16,21] and appearance-based trackers via detector learning [17][18][19][20].
given the clear superiority of approach [5] compared to all reference solutions,
we focus on achieving a direct comparison against sam [5]. hence, for
performance comparison against self-supervised anatomical embedding tracker, we
retrain sam [5] with images from variousct dataset. our method achieves a
matching accuracy of 91.87%, that is 1.84% higher than sam and 5.74% higher than
dlt. to confirm the significance of the improvement achieved by our method
compared to sam [5], we conduct a paired t-test for statistical analysis and
show that the improvement is statistically significant (p-value < 10 -6 ).
compared to the self-supervised version of dlt, the difference in performance is
significantly greater, the proposed systems outperforms dlt-ssl by more than
10%. when imposing a maximum distance limit of 10 mm between the ground truth
and prediction, our method increases performance by 1.46%, showing the
importance of the multi-scale approach in lesion on the nlst dataset, our
proposed method obtains a center point matching accuracy of 92.12% (table 2). in
the case of longitudinal lung nodule tracking (fig. 3), it is more frequent to
observe significant changes in size and density. as our system relies on the
concept of anatomical embedding matching, the most substantial errors in lesion
matching for our system occur when there are significant pathological
distortions that deviate greatly from one timepoint to another. examples of such
cases are depicted in fig. 4, based on expert radiologist feedback.
in conclusion, this paper presents an effective method for longitudinal lesion
tracking based on multi-scale self-supervised learning. the method is generic,
it does not require expert annotations or longitudinal data for training and can
generalize to different types of tumors/organs/modalities. the multi-scale
approach ensures a high degree of robustness and accuracy for small lesions.
through large-scale experiments and validation on two longitudinal datasets, we
highlight the superiority of the proposed method in comparison to
state-of-theart. we found that adopting a multi-scale approach (instead of the
global/local approach as proposed in [5]) can lead to embeddings that better
capture the anatomical location and are able to handle lesions that vary in size
or appearance at different scales. moreover, the changes proposed in this work
help to alleviate the confusion caused by left-right body symmetries (e.g., the
apices of the lungs). this effect challenged the tracking of small nodules in
the lungs using [5]. our future work aims to enhance the matching accuracy by
examining the implications of correlation magnitude, conducting robustness
studies on slight variations in tracking initialization, and implementing a more
advanced fusion strategy for the multi-scale similarity maps. in addition, we
aim to expand to more applications, e.g., treatment monitoring for brain cancer
using mri.disclaimer: the concepts and information presented in this
paper/presentation are based on research results that are not commercially
available. future commercial availability cannot be guaranteed.
the use of machine learning for anomaly detection in medical imaging analysis
has gained a great deal of traction over previous years. most recent approaches
have focused on improvements in performance rather than flexibility, thus
limiting approaches to specific input types -little research has been carried
out to generate models unhindered by variations in data geometries. often,
research assumes certain similarities in data acquisition parameters, from image
dimensions to voxel dimensions and fields-of-view (fov). these restrictions are
then carried forward during inference [5,25]. this strong assumption can often
be complex to maintain in the real-world and although image pre-processing steps
can mitigate some of this complexity, test error often largely increases as new
data variations arise. this can include variances in scanner quality and
resolution, in addition to the fov selected during patient scans. usually
training data, especially when acquired from differing sources, undergoes
significant preprocessing such that data showcases the same fov and has the same
input dimensions, e.g. by registering data to a population atlas. whilst making
the model design simpler, these pre-processing approaches can result in poor
generalisation in addition to adding significant pre-processing times
[11,13,26]. given this, the task of generating an anomaly detection model that
works on inputs with a varying resolution, dimension and fov is a topic of
importance and the main focus of this research.unsupervised methods have become
an increasingly prominent field for automatic anomaly detection by eliminating
the necessity of acquiring accurately labelled data [4,7] therefore relaxing the
stringent data requirements of medical imaging. this approach consists of
training generative models on healthy data, and defining anomalies as deviations
from the defined model of normality during inference. until recently, the
variational autoencoder (vae) and its variants held the state-of-the-art for the
unsupervised approach. however, novel unsupervised anomaly detectors based on
autoregressive transformers coupled with vector-quantized variational
autoencoders (vq-vae) have overcome issues associated with autoencoder-only
methods [21,22]. in [22], the authors explore the advantage of tractably
maximizing the likelihood of the normal data to model the long-range
dependencies of the training data. the work in [21] takes this method a step
further through multiple samplings from the transformer to generate a
non-parametric kernel density estimation (kde) anomaly map.even though these
methods are state-of-the-art, they have stringent data requirements, such as
having a consistent geometry of the input data, e.g., in a whole-body imaging
scenario, it is not possible to crop a region of interest and feed it to the
algorithm, as this cropped region will be wrongly detected as an anomaly. this
would happen even in the case that a scan's original fov was restricted [17].as
such, we propose a geometric-invariant approach to anomaly detection, and apply
it to cancer detection in whole-body pet via an unsupervised anomaly detection
method with minimal spatial labelling. through adapting the vq-vae transformer
approach in [21], we showcase that we can train our model on data with varying
fields of view, orientations and resolutions by adding spatial conditioning in
both the vq-vae and transformer. furthermore, we show that the performance of
our model with spatial conditioning is at least equivalent to, and sometimes
better, than a model trained on whole-body data in all testing scenarios, with
the added flexibility of a "one model fits all data" approach. we greatly reduce
the pre-processing requirements for generating a model (as visualised in fig.
1), demonstrating the potential use cases of our model in more flexible
environments with no compromises on performance.
the main building blocks behind the proposed method are introduced below.
specifically, a vq-vae plus a transformer are jointly used to learn the
probability density function of 3d pet images as explored in prior research
[21,22,24].
the vq-vae model provides a data-efficient encoding mechanism-enabling 3d inputs
at their original resolution-while generating a discrete latent representation
that can trivially be learned by a transformer network [20]. the vq-vae is
composed of an encoder that maps an image x ∈ r h×w ×d onto a compressed latent
representation z ∈ r h×w×d×nz where n z is the latent embedding vector
dimension. z is then passed through a quantization block where each feature
column vector is mapped to its nearest codebook vector. each spatial code z ijl
∈ r nz is then replaced by its nearest codebook element e k ∈ r nz , k ∈ 1, ...,
k where k denotes the codebook vocabulary size, thus obtaining z q . given z q ,
the vq-vae decoder then reconstructs the observations x ∈ r h×w ×d . the
architecture used for the vq-vae model used an encoder consisting of three
downsampling layers that contain a convolution with stride 2 and kernel size 4
followed by a relu activation and 3 residual blocks. each residual block
consists of a kernel of size 3, followed by a relu activation, a convolution of
kernel size 1 and another relu activation. similar to the encoder, the decoder
has 3 layers of 3 residual blocks, each followed by a transposed convolutional
layer with stride 2 and kernel size 4. finally, before the last transposed
convolutional layer, a dropout layer with a probability of 0.05 is added. the
vq-vae codebook used had 256 atomic elements (vocabulary size), each of length
128. the ct vq-vae was identical in hyperparameters except each codebook vector
has length 64. see appendix a for implementation details.
after training a vq-vae model, the next stage is to learn the probability
density function of the discrete latent representations. using the vq-vae, we
can obtain a discrete representation of the latent space by replacing the
codebook elements in z q with their respective indices in the codebook yielding
z iq . to model the imaging data, we require the discretized latent space z iq
to take the form of a 1d sequence s, which we achieve via a raster scan of the
latent. the transformer is then trained to maximize the log-likelihoods of the
latent tokens sequence in an autoregressive manner. by doing this, the
transformer can learn the codebook distribution for position i within s with
respect to previous codes p(s i ) = p(s i |s <i ). as with [21], we additionally
use ct data to condition the transformer via cross-attention using a separate
vq-vae to encode the ct. this transforms the problem to learning the codebook
distribution at position i as p(s i ) = p(s i |s <i , c) where c is the entire
ct latent sequence. the performer used in this work corresponds to a decoder
transformer architecture with 14 layers, each with 8 heads, and an embedding
dimension of 256. similarly the embedding dimension for the ct data and the
spatial conditioning data had an embedding dimension of 256. see appendix b for
implementation details.
building on [21], given a sample for inference, a tokenized representation z iq
is extracted from the vq-vae. then, the representation is flattened into s where
the trained transformer model obtains the likelihoods for each token. these
inferred likelihoods represent the probability of each token appearing at a
certain position in the sequence -p(s i ) = p(s i |s <i , c). this can then be
used to single out tokens with low probability, i.e. anomalous tokens. we then
resample anomalous tokens p(s i ) < t where t is the resampling threshold chosen
empirically using the validation set performance. anomalous tokens are then
replaced with higher likelihood (normal) tokens by resampling from the
transformer. we can then reshape the "healed" sequence back into its 3d
quantized representation to feed into the vq-vae to generate a healed
reconstruction x r without anomalies.in this work, abnormalities are defined as
deviations between the distribution of "healed" reconstructions and the observed
data, measured using a kernel density estimation (kde) approach. we generate
multiple healed latent sequences by sampling multiple times for each position i
with a likelihood p(s i ) < t. in each resampling, the transformer outputs the
likelihood for every possible token at position i. based on these probabilities,
we can create a multinomial distribution showcasing the probability of each
token. we can then randomly sample multiple tokens. each of these healed latent
spaces is then decoded via the vq-vae multiple times with dropout. this
generates multiple healed representations of the original image. a voxel-wise
kde anomaly map is generated by fitting a kde independently at each voxel
position to estimate the probability density function f across reconstructions.
this is then scored at the original intensity of that voxel in the scan. our kde
implementation used 60 samples for each anomalous token in s, followed by five
decodings with dropout, yielding 300 "healed" reconstructions that are then used
to calculate the kde.
to date, there has been little research on generating autoencoder models capable
of using images of varying sizes and resolutions (i.e. the input tensor shape to
a autoencoder is assumed to be fixed). although fully convolutional models can
ingest images of varying dimensions, we have found that using training data with
varying resolutions resulted in poor auto-encoder reconstructions. in this work,
we take inspiration from coordconv [19] as a mechanism to account for some level
of spatial awareness, an approach which has been applied to various tasks in
medical imaging scenarios with ranging levels of success [1,18].a coordconv
layer is a concatenation of channels to the input image referencing a predefined
coordinate system. after concatenation, the input is simply fed through a
standard convolutional layer. for a 3d scan, we would have 3 coordinates, ijk,
where the i coordinate channel is an h × w × d rank-1 matrix with its first row
filled with 0's, its second row with 1's, and so on. this would be the same for
the j coordinate channel, except the columns would be filled with constant
values, not the rows, and likewise for the k coordinate channel in a depth-wise
fashion. these channels are then normalised between [0, 1].the advantage of the
coordconv implementation is the constant scale of 0-1 across the channels
regardless of image resolution. for example, two wholebody images with large
differences in voxel-size will have coordconv channels from 0-1 along each axis,
thus conveying the notion of spatial resolution to the network. we found when
training the vq-vae model on data with varying resolutions and dimensions that
reconstructions showcased unwanted and significant artifacts, while by adding
the coordconv channels this issue was not present (see appendix c for examples).
furthermore, when dealing with images of a ranging fov, we adapted the [0, 1]
channel values to convey the image's fov. for example, suppose a whole body
image (neck to upper leg) represented our range [0, 1] where 0 is the upper leg,
and 1 is the neck. in that case, we can contract this range to represent the
area displayed in the image (fig. 2). in doing so, we convey information about
the fov to the vq-vae through coordconv layers. note that while the proposed
model assumes only translation and scale changes between samples, it can be
trivially extended to a full affine mapping of the coordinate system (including
rotations/shearing between samples).we used random crops during training to
simulate varying fovs of wholebody data. the random crop parameters are then
used to define the coordinate system. for the implementation of the coordconv
layer, these channels are added once to the original input image and at the
beginning of the vq-vae decoder, concatenated to the latent space, using the
same value ranges but at a lower resolution given the reduced spatial dimension
of the latent space.
numerous approaches have used transformers in the visual domain [7,8]. given
that transformers work natively on 1d sequences, the spatial information in
images is often lost. while various works have aimed to convey the spatial
information of the original image when projected onto a 1d sequence [14,28], we
require our spatial positioning to encode both where in the image ordering a
token belongs, and where the token belongs in the context of the whole body. as
the images have different fovs and the image resolution, this results in to do
this, we use the same coordconv principle applied to the input fed to the
vq-vae. in order to map image coordinates to the token latent representation, we
apply average pooling to each coordconv channel separately, with kernel size and
stride equal to the downsampling used in the vq-vae (8 used in this research).
this gives us three channels i, j, k in the range of [0, 1], the same dimension
as our latent space, but at lower spatial resolution to the original input. we
then bin each value in each channel and combine the three values using base
notation. for example, we use 20 bins (equal bins of 0.05), to which the final
quantized spatial value for a given token is given aswhere sp is the quantized
spatial value allocated to a given token at position ijk in the latent space,
and b represents the binned value along a given channel for that token, and b is
a pre-defined bin size. the choice of b = 20 bins was empirically chosen to
closely resemble the average latent dimension of images.during training,
whole-body images and random crops are used. the spatial conditioning tokens are
then generated and fed through an embedding layer of equal dimension to the ct
embedding. the two embedded sequences (ct and spatial) are then added together
and fed to the transformer via cross-attention. for reference, this mechanism
can be visualised in fig. 3.
for this work we leveraged whole-body pet/ct data from different sources to
explore the efficacy of our approach for varying image geometries. 211 scans
from nsclc radiogenomics [2,3,10,16] combined with 83 scans from a proprietary
dataset constitute our lower resolution dataset with voxel dimensions of 3.6 ×
3.6×3 mm. from this, we split the data to give 210 training samples, 34
validation and 50 testing. our higher resolution dataset uses autopet [10,15]
(1014 scans) with voxel dimensions of 2.036 × 2.036 × 3 mm. from this, 850 scans
are used for training, 64 for validation and 100 for testing.all baseline models
work in a single space with constant dimensions, obtained by registering the
autopet images to the space of the nsclc dataset.for evaluation, we use four
testing sets: a lower resolution set derived from both the nsclc and the private
dataset; a higher resolution set from autopet; a testing set with random crops
of the same nsclc/private testing dataset and finally a testing set that has
been rotated through 90 • using the high resolution testing data. as the cropped
and rotated dataset cannot be fed into the baseline models, we pad the images to
the common image sizing before inference.
the proposed model was trained on the data described in sect. 3.3, with random
crops applied while training. model and anomaly detection hyperparameter tuning
was done on our validation samples using the best dice scores. we then test our
model and baselines on 4 hold-out test sets: a low-resolution whole-body set, a
low-resolution cropped set, a high-resolution rotated set and a high-resolution
test set of pet images with varying cancers. the visual results shown in fig. 4
show outputs rotated back to the original orientation. we measure our models'
performance using the dice score, obtained by thresholding the residual/density
score maps. in addition, we calculate the area under the precision-recall curve
(auprc) as a suitable measure for segmentation performance under class
imbalance. we additionally showcase the performance of the classic vq-vae +
transformer approach trained on whole-body data only (without the proposed
spatial conditioning), as well as the proposed coordconv model trained with
varying image geometries but without the transformer spatial conditioning to
explicitly showcase the added contribution of both spatial conditionings. the
full results are presented in table 1 with visual examples shown in fig. 4. we
can observe that the addition of spatial conditioning improves performance even
against the same model without conditioning trained on whole-body data (mann
whitney u test, p < 0.01 on high resolution and p < 0.001 on cropped data for
dice and auprc). for cropped data, models trained on whole-body data fail around
cropping borders, as showcased in fig. 4. this is not the case for the models
trained on varying geometries. note that the vq-vae + transformer trained on
varying geometries still shows adequate performance, highlighting the resilience
of the transformer network to varying sequence lengths without any form of
spatial conditioning. however, by adding the transformer spatial conditioning,
we see improvements across all test sets (most significantly on cropped data and
the rotated data p < 0.001) for both evaluation metrics. for the rotated data,
we see little performance degradation in the conditioned model thanks to the
spatial conditioning. the same model without conditioning showed much lower
performance with higher false positives likely due to the model's inability to
comprehend the anatomical structures present due to the rotated orientation.
detection and segmentation of anomalous regions, particularly for cancer
patients, is essential for staging, treatment and intervention planning.
generally, the variation scanners and acquisition protocols can cause failures
in models trained on data from single sources. in this study, we proposed a
system for anomaly detection that is robust to variances in geometry. not only
does the proposed model showcase strong and statistically-significant
performance improvements on varying image resolutions and fov, but also on
whole-body data. through this, we demonstrate that one can improve the
adaptability and flexibility to varying data geometries while also improving
performance. such flexibility also increases the pool of potential training
data, as they dont require the same fov. we hope this work serves as a
foundation for further exploration into geometry-invariant deep-learning methods
for medical-imaging.
vq-vae coordconv0.57 ± 0.09 0.65 ± 0.08 0.63 ± 0.12 0.32 ± 0.17 0.55 ± 0.09 0.64
± 0.09 0.61 ± 0.13 0.30 ± 0.15 full coordconv 0.58 ± 0.08 0.68 ± 0.10 0.67 ±
0.10 0.65 ± 0.12 0.56 ± 0.09 0.66 ± 0.11 0.64 ± 0.11 0.62 ± 0.12
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43907-0_29.
deep learning-based systems show remarkable predictive performance in many
computer vision tasks, including medical image analysis, and are often
comparable to human performance. however, the complexity of this technique makes
it challenging to extract model knowledge and understand model decisions. this
limitation is being addressed by the field of explainable ai, in which
significant progress has been made in recent years. an important line of
research is the use of inherently explainable models, which circumvent the need
for indirect, errorprone on-top explanations [14]. a common misconception is
that the additional explanation comes with a decrease in performance. however,
rudin et al. [14] and others have already pointed out that this can be avoided
by designing algorithms that build explainability into the core concept, rather
than just adding it on top. our work proves this once again by providing a
powerful and explainable solution for medical image classification.a promising
approach for interpretability is the use of privileged information, i.e.
information that is only available during training [19,20]. besides using the
additional knowledge to improve performance, it can also help to increase
explainability, as has already been shown using the lidc-idri dataset [3]. in
addition to the malignancy of the lung nodules, which is the main goal of the
prediction task, the radiologists also marked certain nodule characteristics
such as sphericity, margin or spiculation. shen et al. [16] used the attributes
with a hierarchical 3d cnn approach, demonstrating the potential of using this
privileged information. lalonde et al. [11] extended this idea using capsule
networks, a technique for learning individual, encapsulated representations
rather than general convolutional layers [1,15]. this method was used to jointly
learn the predefined attributes in the capsules and their associations with the
classification target, i.e. malignancy. explainability is enabled by providing
additional attribute values that are essential to the model output. however, the
predicted, possibly incorrect scores for the attributes must be trusted, which
raises the question of whether there is a way to validate the
predictions.prototype networks are another line of research implementing the
idea that the representations of images cluster around a prototypical
representation for each class [17]. the goal is to find embedded prototypes
(i.e. examples) that best separate the images by their classes [5]. this idea
has been applied to various methods, such as unsupervised learning [13], few-and
zero-shot learning [17,18,22], as well as for capsule networks [21], however
without the use of privileged information. a successful approach is prototypical
models with case-based reasoning, which justify their prediction by showing
prototypical training examples similar to the input instance [4,12]. this idea
can be used for region-wise prototypical samples [6]. however, these networks
can only tell which prototypical samples resemble the query image, not why.
similar to attention models, regional explanations are learned and provided
[23,24]. it is up to the user to guess which features of the image regions are
relevant to the network and are exemplified by the prototypes.our method
addresses the limitations of privileged information-based and prototype-based
explanation by combining case-based visual reasoning through exemplary
representation of high-level attributes to achieve explainability and
high-performance. the proposed method is an image classifier that satisfies
explainable-by-design with two elements: first, decisive intermediate results of
a high-performance cnn are trained on human-defined attributes which are being
predicted during application. second, the model provides prototypical natural
images to validate the attribute prediction. in addition to the enhanced
explainability offered by the proposed approach, to our knowledge the proposed
method outperforms existing studies on the lidc-idri dataset.the main
contributions of our work are:-a novel method that, for the first time to our
knowledge, combines privileged information and prototype learning to provide
increased explanatory power for medical classification tasks. -a prototype
network architecture based on a capsule network that leverages the benefits of
both techniques. -an explainable solution outperforming state-of-the-art
explainable and nonexplainable methods on the lidc-idri dataset.we provide the
code with the model architecture and training algorithm of proto-caps on github.
the idea behind our approach is to combine the potential of attribute and
prototype learning for a powerful and interpretable learning system. for this,
we use a capsule network of attribute capsules from which the target class is
predicted. as the attribute prediction can also be susceptible to error, we use
prototypes to explain the predictions made for each attribute. based on [11],
our approach, called proto-caps, consists of a backbone capsule network. the
network is trained using multiple heads. an attribute head is used to ensure
that each capsule represents a single attribute, a reconstruction head learns
the original segmentation, and the main target prediction head learns the final
classification. the model is extended by a prototype layer that provides
explanations for each attribute decision. the overall architecture of proto-caps
is shown in fig. 1.the backbone of our approach is a capsule network consisting
of three layers: features of the input image of size 1×32×32 are extracted by a
2d convolutional layer containing 256 kernels of size 9×9. we decided not to use
3d convolutional layers, as preliminary experiments showed only marginal
differences (within std. dev. of results), but required significantly more
computing time. the primary capsule layer then segregates low-level features
into 8 different capsules, with each capsule applying 256 kernels of size 9 × 9.
the final dense capsule layer consists of one capsule for each attribute and
extracts high-level features, overall producing eight 16-dimensional vectors.
these vectors form the starting point for the different prediction branches.the
target head, a fully connected layer, combines the capsule encodings. the loss
function for the malignancy prediction was chosen according to lalonde et al.
[11], where the distribution of radiologist malignancy annotations is optimized
with the kullback-leibler divergence l mal to reflect the inter-observer
agreement and thus uncertainty. the reconstruction branch to predict the
segmentation mask of the nodule consists of a simple decoder with three fully
connected layers with the output filters 512, 1024, and the size of the
resulting image 1 × 32 × 32. the reconstruction loss l recon implements the mean
square error between the output and the binary segmentation mask. it has been
shown that incorporating reconstruction learning is beneficial to performance
[11]. for the attribute head, we propose to use fully connected layers, instead
of determining the attribute manifestation by the length of the capsule
encoding, as was done previously [11]. each capsule vector is processed by a
separate linear layer to fit the respective attribute score. we formulate the
attribute loss aswhere y a is the ground truth mean attribute score by the
radiologists, o a is the network score prediction for the a-th attribute, and b
is a random binary mask allowing semi-supervised attribute learning. two
prototypes are learned per possible attribute class, resulting in 8-12
prototypes per attribute (i.e. capsule). during the training, a combined loss
function encourages a training sample to be close to a prototype of the correct
attribute class and away from prototypes dedicated to others, similar to
existing approaches [6]. randomly initialized, the prototypes are a
representative subset of the training dataset for each attribute after the
training. for this, a cluster cost reduces the euclidean distance of a sample's
capsule vector o a to the nearest prototype vector p j of group p as which is
dedicated to its correct attribute score.in order to clearly distinguish between
different attribute specifications, a separation loss is applied to increase the
distance to the capsule prototypes that do not have the correct specification,
limited by a maximum distance:prototype optimization begins after 100 epochs. in
addition to fitting the prototypes with the loss function, each prototype is
replaced every 10 epochs by the most similar latent vector of a training sample.
the original image of the training sample is stored and used for prototype
visualization. during inference, the predicted attribute value is set to the
ground truth attribute value of the closest prototype, ignoring the learned
dense layers in the attribute head at this stage.the overall loss function is
the following weighted sum, where λ recon = 0.512 was chosen according to [11],
and the prototype weights were chosen empirically:
data. the proposed approach is evaluated using the publicly available lidc-idri
dataset consisting of 1018 clinical thoracic ct scans from patients with
non-small cell lung cancer (nsclc) [2,3]. each lung nodule with a minimum size
of 3 mm was segmented and annotated with a malignancy score ranging from
1-highly unlikely to 5-highly suspicious by one to four expert raters. nodules
were also scored according to their characteristics with respect to predefined
attributes, namely subtlety (difficulty of detection, 1-extremely subtle,
5-obvious), internal structure (1-soft tissue, 4-air ), pattern of calcification
(1popcorn, 6-absent), sphericity (1-linear, 5-round ), margin (1-poorly defined,
5sharp), lobulation (1-no lobulation, 5-marked lobulation), spiculation (1-no
spiculation, 5-marked spiculation), and texture (1-non-solid, 5-solid ). the
pylidc framework [7] is used to access and process the data. the mean attribute
annotation and the mean and standard deviation of the malignancy annotations are
calculated. the latter was used to fit a gaussian distribution, which serves as
the ground truth label for optimization. samples with a mean expert malignancy
score of 3-indeterminate or annotations from fewer than three experts were
excluded in consistency with the literature [8,9,11].experiment designs. to
ensure comparability with previous work [8,9,11], the main metric used is
within-1-accuracy, where a prediction within one score is considered correct.
five-fold stratified cross-validation was performed using 10 % of the training
data for validation and the best run of three is reported.the algorithm was
implemented using the pytorch framework version 1.13 and cuda version 11.6. a
learning rate of 0.5 was chosen for the prototype vectors and 0.02 for the other
learnable parameters. the batch size was set to 128 and the optimizer was adam
[10]. with a maximum of 1000 epochs, but stopping early if there was no
improvement in target accuracy within 100 epochs, the experiments lasted an
average of three hours on a geforce rtx 3090 graphics card. the code is publicly
available at https://github.com/xrad-ulm/proto-caps.besides pure performance,
the effect of reduced availability of attribute annotations was investigated.
this was done by using attribute information only for a randomly selected
fraction of the nodules during the training.to investigate the effect of
prototypes on the network performance, an ablation study was performed. three
networks were compared: proto-caps (proposed) including learning and applying
prototypes during inference, proto-caps w/o use where prototypes are only
learned but ignored for inference, and proto-caps w/o learn using the proposed
architecture without any prototypes.
qualitative. figure 2 shows examples of model output. the predicted malignancy
score is justified by the closest prototypical sample of a certain attribute.
the respective original image for each attribute prototype is being saved during
the training process and used for visualization during inference. in case b,
there are large differences between the margin and lobulation prototype and the
sample. similarly, in case c, the spiculation prediction is very different from
the sample.during application, these discrepancies between the prototypes and
the sample nodule raise suspicion, and help to assess the malignancy prediction.
a quantitative evaluation of the relationship between correctness in attribute
and in target prediction using logistic regression analysis shows a strong
relationship between both with an accuracy of 0.93/0.1.quantitative. table 1
shows the results of our experiments compared to other state-of-the-art
approaches, with results taken from original reports. the accuracy of the
proposed method exceeds previous work in both the malignancy and almost all
attribute predictions, while modelling all given attributes.table 2 lists the
results obtained when only fractions of the training samples come with attribute
information. the experiments indicate that the performance of the given approach
is maintained up to a fraction of 10 %. using no attribute annotations at all,
i.e. no privileged information, achieves a similar performance, but results in a
loss of explainability, as the high-level features extracted in the capsules are
not understandable to humans. this result suggests that privileged information
here leads to an increase in interpretability for humans by providing attribute
predictions and prototypes without interfering with the model performance. the
ablation study shows no significant differences between the three models
evaluated. for the malignancy accuracy, proto-caps w/o use and proto-caps w/o
learn achieved μ = 93.9 % (σ = 0.8) and μ = 93.7 % (σ = 1.1), respectively. the
average difference in attribute accuracy compared to the proposed methods is 1.7
% and 1.5 % better, respectively, and is more robust across experiments. the
best result was obtained when the prototypes were learned but not used, possibly
indicating that the prototypes may have a regularising effect during training,
but further experiments are needed to confirm this due to the close results. to
give an indication of the decoder performance, proto-caps w/o use achieved a
dice score of 79.7 %.
we propose a new method, named proto-caps, which combines the advantages of
privileged information, and prototype learning for an explainable network,
achieving more than 6 % better accuracy than the state-of-the-art explainable
method. as shown by qualitative results (fig. 2), the obtained prototypes can be
used to detect potential false classifications. our method is based on capsule
networks, which allow prediction based on attribute-specific prototypes.
compared to class-specific prototypes, our approach is more specific and allows
better interpretation of the predictions made. in summary, proto-caps outputs
prediction results for the main classification task and for predefined
attributes, and provides visual validation through the prototypical samples of
the attributes.the experiments demonstrate that it outperforms state-of-the-art
methods that provide less explainability. our data reduction studies show that
the proposed solution is robust to the number of annotated examples, and good
results are obtained even with a 90% reduction in privileged information. this
opens the door for application to other datasets by reducing the additional
annotation overhead. while we did see a reduction in performance with too few
labels, our results suggest that this is mainly due to inhomogeneous coverage of
individual attribute values. in this respect, it would be interesting to find
out how a specific selection of the annotated samples, e.g. with extremes,
affects the accuracies, especially since our results show that the overall
performance is robust even when the attributes are not explicitly trained, i.e.
without additional privileged information. another area of research would be to
explore other types of privileged information that require less extra annotation
effort, such as medical reports, to train the attribute capsules. it would also
be worth investigating more sophisticated 3d-based capsule networks.in
conclusion, we believe that the approach of leveraging privileged information
with comprehensible architectures and prototype learning is promising for
various high-risk application domains and offers many opportunities for further
research.
accurate segmentation of a variety of anatomical structures is a crucial
prerequisite for subsequent diagnosis or treatment [28]. while recent advances
in datadriven deep learning (dl) have achieved superior segmentation performance
[29], the segmentation task is often constrained by the availability of costly
pixel-wise labeled training datasets. in addition, even if static dl models are
trained with extraordinarily large amounts of training datasets in a supervised
learning manner [29], there exists a need for a segmentor to update a trained
model with new data alongside incremental anatomical structures [24].in
real-world scenarios, clinical databases are often sequentially constructed from
various clinical sites with varying imaging protocols [19][20][21]23]. as well,
labeled anatomical structures are incrementally increased with additional
lesions or new structures of interest, depending on study goals or clinical
needs [18,27]. furthermore, access to previously used data for training can be
restricted, due to data privacy protocols [17,18]. therefore, efficiently
utilizing heterogeneous structure-incremental (hsi) learning is highly desired
for clinical practice to develop a dl model that can be generalized well for
different types of input data and varying structures involved. straightforwardly
fine-tuning dl models with either new structures [30] or heterogeneous data [17]
in the absence of the data used for the initial model training, unfortunately,
can easily overwrite previously learned knowledge, i.e., catastrophic forgetting
[14,17,30].at present, satisfactory methods applied in the realistic hsi setting
are largely unavailable. f irst, recent structure-incremental works cannot deal
with domain shift. early attempts [27] simply used exemplar data in the previous
stage. [5,18,30,33] combined a trained model prediction and a new class mask as
a pseudo-label. however, predictions from the old model under a domain shift are
likely to be unreliable [38]. the widely used pooled feature statistics
consistency [5,30] is also not applicable for heterogeneous data, since the
statistics are domain-specific [2]. in addition, a few works [13,25,34] proposed
to increase the capacity of networks to avoid directly overwriting parameters
that are entangled with old and new knowledge. however, the solutions cannot be
domain adaptive. second, from the perspective of continuous domain adaptation
with the consistent class label, old exemplars have been used for the
application of prostate mri segmentation [32]. while li et al. [17] further
proposed to recover the missing old stage data with an additional generative
model, hallucinating realistic data, given only the trained model itself, is a
highly challenging task [31] and may lead to sensitive information leakage [35].
t hird, while, for natural image classification, kundu et al. [16] updated the
model for class-incremental unsupervised domain adaption, its class prototype is
not applicable for segmentation.in this work, we propose a unified hsi segmentor
evolving framework with a divergence-aware decoupled dual-flow (d 3 f) module,
which is adaptively optimized via hsi pseudo-label distillation using a momentum
mixup decay (mmd) scheme. to explicitly avoid the overwriting of previously
learned parameters, our d 3 f follows a "divide-and-conquer" strategy to balance
the old and new tasks with a fixed rigidity branch and a compensated learnable
plasticity branch, which is guided by our novel divergence-aware continuous
batch renormalization (cbrn). the complementary knowledge can be flexibly
integrated with the model re-parameterization [4]. our additional parameters are
constant in training, and 0 in testing. then, the flexible d 3 f module is
trained following the knowledge distillation with novel hsi pseudo-labels.
specifically, inspired by the self-knowledge distillation [15] and self-training
[38] that utilize the previous prediction for better generalization, we
adaptively construct the hsi pseudo-label with an mmd scheme to smoothly adjust
the contribution of potential noisy old model predictions on heterogeneous data
and progressively learned new model predictions along with the training. in
addition, unsupervised self-entropy minimization is added to further enhance
performance.our main contributions can be summarized as follow:• to our
knowledge, this is the first attempt at realistic hsi segmentation with both
incremental structures of interest and diverse domains. • we propose a
divergence-aware decoupled dual-flow module guided by our novel continuous batch
renormalization (cbrn) for alleviating the catastrophic forgetting under domain
shift scenarios. • the adaptively constructed hsi pseudo-label with
self-training is developed for efficient hsi knowledge distillation.we evaluated
our framework on anatomical structure segmentation tasks from different types of
mri data collected from multiple sites. our hsi scheme demonstrated superior
performance in segmenting all structures with diverse data distributions,
surpassing conventional class-incremental methods without considering data
shift, by a large margin.
for the segmentation model under incremental structures of interest and domain
shift scenarios, we are given an off-the-shelf segmentor f θ 0 : x 0 → y 0
parameterized with θ 0 , which has been trained with the data {x 0 n , y 0 n } n
0 n=1 in an initial source domain d 0 = {x 0 , y 0 }, where x 0 n ∈ r h×w and y
0 n ∈ r h×w are the paired image slice and its segmentation mask with the height
of h and width of w , respectively. there are t consecutive evolving stages with
heterogeneous target domains d t = {x t , s t } t t=1 , each with the paired
slice set {x t n } n t n=1 ∈ x t and the current stage label set {s t n } n t
n=1 ∈ s t , where x t n , s t n ∈ r h×w . due to heterogeneous domain shifts, x
t from different sites or modalities follows diverse distributions across all t
stages. due to incremental anatomical structures, the overall label space,
across the previous t stages, y t is expanded from y t-1 with the additional
annotated structuresfor delineating all of the structures y t seen in t stages.
to alleviate the forgetting through parameter overwriting, caused by both new
structures and data shift, we propose a d 3 f module for flexible decoupling and
integration of old and new knowledge. specifically, we duplicate the convolution
in each layer initialized with the previous model f θ t-1 to form two branches
as in [13,25,34]. the first rigidity branch f r θ t is fixed at the stage t to
keep the old knowledge we have learned. in contrast, the extended plasticity
branch f p θ t is expected to be adaptively updated 2 } with the model
re-parameterization [4]. in fact, the dual-flow model can be regarded as an
implicit ensemble scheme [9] to integrate multiple sub-modules with a different
focus. in addition, as demonstrated in [6], the fixed modules will regularize
the learnable modules to act as the fixed one. thus, the plasticity modules can
also be implicitly encouraged to keep the previous knowledge along with its hsi
learning.however, under the domain shift, it can be sub-optimal to directly
average the parameters, since f r θ t may not perform well to predict y t-1 on x
t . it has been demonstrated that batch statistics adaptation plays an important
role in domain generalizable model training [22]. therefore, we propose a
continual batch renormalization (cbrn) to mitigate the feature statistics
divergence between each training batch at a specific stage and the life-long
global data distribution.of note, as a default block in the modern convolutional
neural networks (cnn) [8,37], batch normalization (bn) [11] normalizes the input
feature of each cnn channel z ∈ r hc×wc with its batch-wise statistics, e.g.,
mean μ b and standard deviation σ b , and learnable scaling and shifting factors
{γ, β} as zi = zi-μb σb • γ + β, where i indexes the spatial position in r hc×wc
. bn assumes that the same mini-batch training and testing distribution [10],
which does not hold in hsi. simply enforcing the same statistics across domains
as [5,30,33] can weaken the model expressiveness [36].the recent brn [10]
proposes to rectify the data shift between each batch and the dataset by using
the moving average μ and σ along with the training:where η ∈ [0, 1] is applied
to balance the global statistics and the current batch.in addition, γ = σb σ and
β = μb -μ σ are used in both training and testing. therefore, brn renormalizes
zi = zi-μ σ to highlight the dependency on the global statistics {μ, σ} in
training for a more generalizable model, while limited to the static learning.in
this work, we further explore the potential of brn in the continuously evolving
hsi task to be general for all of domains involved. specifically, we extend brn
to cbrn across multiple consecutive stages by updating {μ c , σ c } along with
all stages of training, which is transferred as shown in fig. 1. the
conventional bn also inherits {μ, σ} for testing, while not being used in
training [11]. at the stage t, μ c and σ c are succeeded from t -1 stage, and
are updated with the current batch-wise {μ r b , σ r b } and {μ p b , σ p b } in
rigidity and plasticity branches:for testing, the two branches in final model f
θ t can be merged for the lightweight implementation:therefore, f t θ does not
introduce additional parameters for deployment (fig. 2).
the training of our developed f θ t with d 3 f is supervised with the previous
model f θ t-1 and current stage data {x t n , s t n } n t n=1 . in conventional
class incremental learning, the knowledge distillation [31] is widely used to
construct the combined label y t n ∈ r h×w by adding s t n and the prediction of
f θ t-1 (x t n ). then, f θ t can be optimized by the training pairs of {x t n ,
y t n } n t n=1 . however, with heterogeneous data in different stages, f θ t-1
(x t n ) can be highly unreliable. simply using it as ground truth cannot guide
the correct knowledge transfer.in this work, we construct a complementary
pseudo-label ŷt n ∈ r h×w with a mixup decay scheme to adaptively exploit the
knowledge in the old segmentor for the progressively learned new segmentor. in
the initial training epochs, f θ t-1 could be a more reliable supervision
signal, while we would expect f θ t can learn to perform better on predicting y
t-1 . of note, even with the rigidity branch, the integrated network can be
largely distracted by the plasticity branch in the initial epochs. therefore, we
propose to dynamically adjust their importance in constructing pseudo-label
along with the training progress. specifically, we mixup the predictions of f θ
t-1 and f θ t w.r.t. y t-1 , i.e., f θ t (•)[: t -1], and control their
pixel-wise proportion for the pseudo-label ŷt n with mmd:where i indexes each
pixel, and λ is the adaptation momentum factor with the exponential decay of
iteration i. λ 0 is the initial weight of f θ t-1 (x t n:i ), which is
empirically set to 1 to constrain λ ∈ (0, 1]. therefore, the weight of old model
prediction can be smoothly decreased along with the training, and f θ t (x t n:i
) gradually represents the target data for the old classes in [: t -1]. of note,
we have ground-truth of new structure s t n:i under hsi scenarios [5,18,30,33].
we calculate the cross-entropy loss l ce with the pseudo-label ŷt n:i as
self-training [15,38]. in addition to the old knowledge inherited in f θ t-1 ,
we propose to explore unsupervised learning protocols to stabilize the initial
training. we adopt the widely used self-entropy (se) minimization [7] as a
simple add-on training objective. specifically, we have the slice-level
segmentation se, which is the averaged entropy of the pixel-wise softmax
prediction asin training, the overall optimization loss is formulated as
follows:where α is used to balance our hsi distillation and se minimization
terms, and i max is the scheduled iteration. of note, strictly minimizing the se
can result in a trivial solution of always predicting a one-hot distribution
[7], and a linear decreasing of α is usually applied, where λ 0 and α 0 are
reset in each stage.
we carried out two evaluation settings using the brats2018 database [1],
including cross-subset (relatively small domain shift) and cross-modality
(relatively large domain shift) tasks. the brats2018 database is a continually
evolving database [1] with a total of 285 glioblastoma or low-grade gliomas
subjects, comprising three consecutive subsets, i.e., 30 subjects from brats2013
[26], 167 subjects from tcia [3], and 88 subjects from cbica [1]. notably, these
three subsets were collected from different clinical sites, vendors, or
populations [1]. each subject has t1, t1ce, t2, and flair mri volumes with
voxel-wise labels for the tumor core (coret), the enhancing tumor (enht), and
the edema (ed). we incrementally learned coret, enht, and ed structures
throughout three consecutive stages, each following different data
distributions. we used subjectindependent 7/1/2 split for training, validation,
and testing. for a fair comparison, we adopted the resnet-based 2d nnu-net
backbone with bn as in [12] for all of the methods and all stages used in this
work.
in our cross-subset setting, three structures were sequentially learned across
three stages: (coret with brats2013) → (enht with tcia) → (ed with cbica). of
note, we used a coret segmentator trained with brats2013 as our off-the-shelf
segmentor in t = 0. testing involved all subsets and anatomical structures. we
compared our framework with the three typical structureincremental (si-only)
segmentation methods, e.g., plop [5], margexcil [18], and ucd [30], which cannot
address the heterogeneous data across stages. as tabulated in table 1, plop [5]
with additional feature statistic constraints has lower performance than
margexcil [18], since the feature statistic consistency was not held in hsi
scenarios. of note, the domain-incremental methods [17,32] cannot handle the
changing output space. our proposed hsi framework outperformed si-only methods
[5,18,30] with respect to both dsc and hd, by a large margin. for the anatomical
structure coret learned in t = 0, the difference between our hsi and these
si-only methods was larger than 10% dsc, which indicates the data shift related
forgetting lead to a more severe performance drop in the early stages. we set η
= 0.01 and α 0 = 10 according to the sensitivity study in the supplementary
material.for the ablation study, we denote hsi-d 3 f as our hsi without the d 3
f module, simply fine-tuning the model parameters. hsi-cbrn used dual-flow to
avoid direct overwriting, while the model was not guided by cbrn for more
generalized prediction on heterogeneous data. as shown in table 1, both the
dual-flow and cbrn improve the performance. notably, the dual-flow model with
flexible re-parameterization was able to alleviate the overwriting, while our
cbrn was developed to deal with heterogeneous data. in addition, hsi-mmd
indicates our hsi without the momentum mixup decay in pseudo-label construction,
i.e., simply regarding the prediction of f θ t-1 (x t ) is ground truth for y
t-1 . however, f θ t-1 (x t ) can be quite noisy, due to the low quantification
performance of early stage structures, which can be aggravated in the case of
the long-term evolving scenario. of note, the pseudo-label construction is
necessary as in [5,18,30]. we also provide the qualitative comparison with
si-only methods and ablation studies in fig. 3.
in our cross-modality setting, three structures were sequentially learned across
three stages: (coret with t1) → (enht with t2) → (ed with t2 flair). of note, we
used the coret segmentator trained with t1 modality as our off-the-shelf
segmentor in t = 0. testing involved all mri modalities and all structures. with
the hyperparameter validation, we empirically set η = 0.01 and α 0 = 10.in table
2, we provide quantitative evaluation results. we can see that our hsi framework
outperformed si-only methods [5,18,30] consistently. the improvement can be even
larger, compared with the cross-subset task, since we have much more diverse
input data in the cross-modality setting. catastrophic forgetting can be severe,
when we use si-only method for predicting early stage structures, e.g., coret.
we also provide the ablation study with respect to d 3 f, cbrn, and mmd in table
2. the inferior performance of hsi-d 3 f/cbrn/mmd demonstrates the effectiveness
of these modules for mitigating domain shifts.
this work proposed an hsi framework under a clinically meaningful scenario, in
which clinical databases are sequentially constructed from different
sites/imaging protocols with new labels. to alleviate the catastrophic
forgetting alongside continuously varying structures and data shifts, our hsi
resorted to a d 3 f module for learning and integrating old and new knowledge
nimbly. in doing so, we were able to achieve divergence awareness with our
cbrn-guided model adaptation for all the data involved. our framework was
optimized with a self-entropy regularized hsi pseudo-label distillation scheme
with mmd to efficiently utilize the previous model in different types of mri
data. our framework demonstrated superior segmentation performance in learning
new anatomical structures from cross-subset/modality mri data. it was
experimentally shown that a large improvement in learning anatomic structures
was observed.
colonoscopy is a critical tool for identifying adenomatous polyps and reducing
rectal cancer mortality. deep learning methods have shown powerful abilities in
automatic colonoscopy analysis, including polyp segmentation [5,22,26,27,29] and
polyp detection [19,24]. however, the scarcity of annotated data due to high
manual annotation costs results in poorly trained and low generalizable models.
previous methods have relied on generative adversarial networks (gans) [9,25] or
data augmentation methods [3,13,28] to enhance learning features, but these
methods yielded limited improvements in downstream tasks. recently, diffusion
models [6,15] have emerged as promising solutions to this problem, demonstrating
remarkable progress in generating multiple modalities of medical data
[4,10,12,21].
fig. 1. overview of the pipeline of our proposed approach, where details of
arsdm are described in sect. 2.despite recent progress in these methods for
medical image analysis, existing models face two major challenges when applied
to colonoscopy image analysis. firstly, the foreground (polyp) of colonoscopy
images contains rich pathological information yet is often tiny compared with
the background (intestine wall) and can be easily overwhelmed during training.
thus, naive generative models may generate realistic colonoscopy images but
those images seldom contain polyp regions. in addition, in order to generate
high-quality annotated samples, it is crucial to maintain the consistency
between the polyp morphologies in synthesized images and the original masks,
which current generative models struggle to achieve.to tackle these issues and
inspired by the remarkable success achieved by diffusion models in generating
high-quality ct or mri data [8,11,23], we creatively propose an effective
adaptive refinement semantic diffusion model (arsdm) to generate polyp-contained
colonoscopy images while preserving the original annotations. the pipeline of
the data generation and downstream task training is shown in fig. 1.
specifically, we use the original segmentation masks as conditions to train a
conditional diffusion model, which makes the generated samples share the same
masks with the input images. moreover, during diffusion model training, we
employ an adaptive loss re-weighting method to assign loss weights for each
input according to the size ratio of polyps and background, which addresses the
overfitting problem for the large background. in addition, we fine-tune the
diffusion model by minimizing the distance between the original ground truth
masks and the prediction masks from synthesis images via a pretrained
segmentation network. thus the refined model could generate samples better
aligned with the original masks.in summary, our contributions are three-fold:
(1) adaptive refinement sdm: based on the standard semantic diffusion model
[21], we propose a novel arsdm with the adaptive loss re-weighting and the
prediction-guided sample refinement mechanisms, which is capable of generating
realistic polyp-contained colonoscopy images while preserving the original
annotations. to the best of our knowledge, this is the first work for adapting
diffusion models to colonoscopy image synthesis. (2) large-scale colonoscopy
generation: the proposed approach can be used to generate large-scale datasets
with no/arbitrary annotations, which significantly benefits the medical image
society, laying the foundation for large-scale pre-training models in automatic
colonoscopy analysis. (3) qualitative and quantitative evaluation: we conduct
extensive experiments to evaluate our method on five public benchmarks for polyp
segmentation and detection. the results demonstrate that our approach could help
deep learning methods achieve better performances. the source code is available
at https:// github.com/duyooho/arsdm.
background. denoising diffusion probabilistic models (ddpms) [6] are classes of
deep generative models, which have forward and reverse processes. the forward
process is a markov chain that gradually adds gaussian noise to the original
data. this process can be formulated as the joint distribution q (x 1:t | x 0
):where q (x 0 ) is the original data distribution with x 0 ∼ q (x 0 ), x 1:t
are latents with the same dimension of x 0 and β t is a variance schedule.the
reverse process is aiming to learn a model to reverse the forward process that
reconstructs the original input data, which is defined as:(2) where p (x t ) is
the noised gaussian transition from the forward process at timestep t . in this
case, we only need to use deep-learning models to represent μ θ with θ as the
model parameters. according to the original paper [6], the loss function can be
simplified as:
diffusion loss ℒ thus, instead of training the model μ θ to predict μt , we can
train the model θ to predict ˜ , which is easier for parameterization and
learning.
in this paper, we propose an adaptive refinement semantic diffusion model, a
variant of ddpm, which has three key parts, i.e., mask conditioning, adaptive
loss re-weighting, and prediction-guided sample refinement. the overall
illustration of our framework is shown in fig. 2.
unlike the previous generative methods, our work aims to generate a synthetic
image with an identical segmentation mask to the original annotation. to
accomplish this, we adapt the widely used conditional u-net architecture [21] in
the reverse process, where the mask is fed as a condition. specifically, for an
input image x 0 ∈ r h×w ×c , x t can be sampled at any timestep t with the
closed form:where ∼ n (0, i) , α t := 1β t and ᾱt := t s=1 α s . it will be fed
into the encoder e of the u-net, and its corresponding mask annotation c 0 ∈ r
h×w will be injected into the decoder d. the model output can be formulated
as:thus, the u-net model θ in eq. 3 becomes θ (x t , t, c 0 ), and the loss
function in eq. 3 is changed to:algorithm 1: one training iteration of arsdm6
take gradient descent step on ∇ θ l total
the polyp regions in the colonoscopy images differ from the background regions,
which contain more pathological information and should be adequately treated to
learn a better model. however, training the diffusion models using the original
loss function ignores the difference between different regions, where each pixel
shares the same weights when calculating the loss. this would lead to the model
generating more background-like polyps since the large background region will
easily overwhelm the small foreground polyp regions during training. a simple
way to alleviate this problem is to apply a weighted loss function that assigns
the polyp and background regions with different weights. however, most polyps
vary a lot in size and shape. thus assigning constant weights for all polyps
exacerbated the imbalance problem. in this case, to tackle this problem, we
propose an adaptive loss function that vests different weights according to the
size ratio of the polyp over the background. specifically, we define a
pixel-wise weights matrix w λ ∈ r h×w with each entry w λ i,j to be:where p = 1
means the pixel p at (h, w) belongs to the polyp region and p = 0 means it
belongs to the background region. thus, the loss function becomes:
the downstream tasks of polyp segmentation and detection require rich semantic
information on polyp regions to train a good model. through extensive
experiments, we found inaccurate sample images with coarse polyp boundary that
is not aligned properly with the original masks may introduce large biases and
noises to the datasets. the model can be confused by several conflicting
training images with the same annotation. to this end, we design a refinement
strategy that uses the prediction of a pre-trained segmentation model on the
sampled images to guide the training process and restore the proper polyp
boundary information. specifically, at each iteration of training, the output ˜
= θ (x t , t, c 0 ) will go into the sampler to generate sample image x0 . then,
we take the sample image as the input of the segmentation model to predict the
pseudo masks c0 . we propose the following refinement loss based on iou loss and
binary cross entropy (bce) loss between c0 and c 0 . the refinement loss
is:where l = l iou + l bce is the sum of the iou loss and bce loss, c0 is the
collection of the three side-outputs ( c3 , c4 , c5 ) and the global map cg as
described in [5]. p(•) represents the pranet model and s(•) is the ddim [16]
sampler. the detailed procedure of one training iteration is shown in algorithm
1 and the overall loss function is defined as:3 experiments
we conducted our experiments on five public polyp segmentation datasets:
endoscene [20], cvc-clincdb/cvc-612 [1], cvc-colondb [18], etis [14] and kvasir
[7]. following the standard of pranet, 1,450 image-mask pairs from kvasir and
cvc-clinicdb are taken as the training set. the evaluations are conducted on the
five datasets separately to verify the learning and generalization capability.
the training image-mask pairs are padded to have the same height and width and
then resized to the size of 384 × 384. experiments with predictionguided sample
refinement are trained with around one-half nvidia a100 days, while others are
trained with approximately one day for convergence. we use the ddim sampler with
a maximum timestep of 200 for sampling images.
we conduct the evaluation of our methods and the state-of-the-art counterparts
on polyp segmentation and detection tasks. we generated the same number of
samples as the diffusion training set using the original masks, and then
combined them to create a new downstream training set. we employed pranet [5],
sanet [22], and polyp-pvt [2] as baseline segmentation models with default
settings, and evaluated them using mean intersection over union (iou) and mean
dice metrics. for detection, we selected centernet [30], sparse-rcnn [17], and
deformable-detr [31] as baseline models with the same settings as the original
papers, and evaluated them using average precision (ap) and f1-scores.
the experimental results presented in table 1 and 2 demonstrate the
effectiveness of our proposed method in training better downstream models to
achieve superior performance. specifically, data generated by our approach
assists the significant improvements for each model in mdice and miou, with
increases of 6.0% and 5.7% over pranet, 2.1% and 2.7% over sanet, and 0.7% and
0.7% over polyp-pvt. we also observe superior ap and f1-scores compared to
centernet, sparse-rcnn, and deformable-detr trained with original data, with
gains of 9.1% and 5.3%, 2.7% and 5.8%, and 3.4% and 6.1%, respectively.
moreover, we conducted a comprehensive comparison with sota models, noting that
these models were not specifically designed for colonoscopy images and may
generate data that hinder the training process or lack the ability for effective
improvement. nevertheless, our experimental results confirm the superiority of
our proposed method.ablation study. we conducted an ablation study to assess the
importance of each proposed component. table 3 and 4 report the overall
accuracies on the test set. the results demonstrate both components contribute
to the accuracy improvement of baseline models, indicating their essential roles
in achieving the best final performance.
to further investigate the generative performance of our approach, we present
visualization results in fig. 3, which displays the generated samples and their
corresponding masks, alongside the original images for reference. the generated
samples demonstrate differences from the original images in both the polyp
regions and the backgrounds while maintaining alignment with the masks.
additionally, we sought evaluations from medical professionals to assess the
authenticity of the generated samples, and non-medical professionals to locate
polyps in the images, which yielded positive feedback on the quality of the
generated samples.
automatic generation of annotated data is essential for colonoscopy image
analysis, where the scale of existing datasets is limited by the expertise and
time required for manual annotation. in this paper, we propose an adaptive
refinement semantic diffusion model (arsdm) for generating colonoscopy images
while preserving annotations by introducing innovative adaptive loss
re-weighting and prediction-guided sample refinement mechanisms. to evaluate our
approach comprehensively, we conduct polyp segmentation and detection
experiments on five widely used datasets, where experimental results demonstrate
the effectiveness of our approach, in which model performances are greatly
enhanced with little synthesized data.
samplesfig.3. illustration of generated samples with the corresponding masks and
original images for comparison reference.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43895-0_32.
the recent advancements in medical image recognition systems have greatly
benefited from deep learning techniques [15,28]. large-scale well-annotated
datasets are one of the key components for training deep learning models to
achieve satisfactory results [3,17]. however, unlike natural images in computer
vision, the number of medical images with expert annotations is often limited by
the high labeling cost and privacy concerns. to overcome this challenge, a
natural choice is to employ data augmentation to increase the number of training
samples. although conventional augmentation techniques [23] such as flipping and
cropping can be directly applied to medical images, they merely improve the
diversity of datasets, thus leading to marginal performance gains [1]. another
group of studies employ conditional generative adversarial networks (cgans) [10]
to synthesize visually appealing medical images that closely resemble those in
the original datasets [36,37]. while existing works have proven effective in
improving the performance of downstream models to some extent, a sufficient
amount of labeled data is still required to adequately train models to generate
decentquality images. more recently, diffusion models have become popular for
natural image generation due to their impressive results and training stability
[4,13,31]. a few studies have also demonstrated the potential of diffusion
models for medical image synthesis [19,24].although annotated data is typically
hard to acquire for medical images, unannotated data is often more accessible.
to mitigate the issue existed in current cgan-based synthetic augmentation
methods [8,[36][37][38], in this work, we propose to leverage the diffusion
model with unlabeled pre-training to reduce the dependency on the amount of
labeled data (see comparisons in fig. 1). we propose a novel synthetic
augmentation method, named histodiffusion, which can be pre-trained on
large-scale unannotated datasets and adapted to smallscale annotated datasets
for augmented training. specifically, we first employ a latent diffusion model
(ldm) and train it on a collection of unlabeled datasets from multiple sources.
this large-scale pre-training enables the model to learn common yet diverse
image characteristics and generate realistic medical images. second, given a
small labeled dataset that does not exist in the pre-training datasets, the
decoder of the ldm is fine-tuned using annotations to adapt to the domain shift.
synthetic images are then generated with classifier guidance [4] in the latent
space. following the prior work [36], we select generated images based on the
confidence of target labels and feature similarity to real labeled images. we
evaluate our proposed method on a histopathology image dataset of colorectal
cancer (crc). experiment results show that when presented with limited
annotations, the classifier trained with our augmentation method outperforms the
ones trained with the prior cgan-based methods. our experimental results show
that once histodiffusion is well pre-trained using large datasets, it can be
applied to any future incoming small dataset with minimal fine-tuning and may
substantially improve the flexibility and efficacy of synthetic augmentation.
figure 2 illustrates the overall architecture of our proposed method. first, we
train an ldm on a large-scale set of unlabeled datasets collected from multiple
sources. we then fine-tune the decoder of this pretrained ldm on a small labeled
dataset. to enable conditional image synthesis, we also train a latent
classifier on the same labeled dataset to guide the diffusion model in ldm. once
the classifier is trained, we apply the fine-tuned ldm to generate a pool of
candidate images conditioned on the target class labels. these candidate images
are then passed through the image selection module to filter out any low-quality
results. finally, we can train downstream classification models on the expanded
training data, which includes the selected images, and then use them to perform
inference on test data. in this section, we will first introduce the background
of diffusion models and then present details about the histodiffusion model.
diffusion models (dm) [13,30,32] are probabilistic models that are designed to
learn a data distribution. given a sample from the data distribution z 0 ∼ q(z 0
), the dm forward process produces a markov chain z 1 , . . . , z t by gradually
adding gaussian noise to z 0 based on a variance schedule β 1 , . . . , β t ,
that is:where variances β t are constants. if β t are small, the posterior q(z
t-1 |z t ) can be well approximated by diagonal gaussian [21,30]. furthermore,
when the t of the chain is large enough, z t can be well approximated by
standard gaussian distribution n (0, i). these suggest that the true posterior
q(z t-1 |z t ) can be estimated by p θ (z t-1 |z t ) defined as [22]:(2) the dm
reverse process (also known as sampling) then generates samples z 0 ∼ p θ (z 0 )
by initiating a markov chain with gaussian noise z t ∼ n(0, i) and progressively
decreasing noise in the chain of z t -1 , z t -2 , . . . , z 0 using the learnt
p θ (z t-1 |z t ). to learn p θ (z t-1 |z t ), gaussian noise is added to z 0 to
generate samples z t ∼ q(z t |z 0 ), then a model θ is trained to predict using
the following mean-squared error loss:where time step t is uniformly sampled
from {1, . . . , t }. then μ θ (z t ) and σ θ (z t ) in eq. 2 can be derived
from θ (z t , t) to model p θ (z t-1 |z t ) [13,22]. the denoising model θ is
typically implemented using a time-conditioned u-net [27] with residual blocks
[11] and self-attention layers [35]. sinusoidal position embedding [35] is also
usually used to specify the time step t to θ .
model architecture. our proposed histodiffusion is built on latent diffusion
models (ldm) [26], which requires fewer computational resources without
degradation in performance, compared to prior works [4,15,28]. ldm first trains
a latent autoencoder (lae) [16] to encode images as lower-dimensional latent
representations and then learns a diffusion model (dm) for image synthesis by
modeling the latent space of the trained lae. particularly, the encoder e of the
lae encodes the input image x ∈ r h×w ×3 into a latent representation z = e(x) ∈
r h×w×c in a lower-dimensional latent space z. here h and w are the height and
width of image x, and h, w, and c are the height, width, and channel of latent
z, respectively. the latent z is then passed into the decoder d to reconstruct
the image x = d(z). through this process, the compositional features from the
image space x can be extracted to form the latent space z, and we then model the
distribution of z by learning a dm. for the dm in ldm, both the forward and
reverse sampling processes are performed in the latent space z instead of the
original image space x .unconditional large-scale pre-training. to ensure the
latent space z can cover features of various data types, we first pre-train our
proposed his-todiffusion on large-scale unlabeled datasets. specifically, we
gather unlabeled images from m different sources to construct a large-scale set
of datasets s = {s 1 , s 2 , . . . , s m }. we then train an lae using the data
from s with the following self-reconstruction loss to learn a powerful latent
space z that can describe diverse features: (4) where l rec is the loss
measuring the difference between the output reconstructed image x and the input
ground truth image x. here we implement l rec with a combination of a pixel-wise
l 1 loss, a perceptual loss [39], and a patch-base adversarial loss [6,7]. to
avoid arbitrarily high-variance latent spaces, we also add a kl regularization
term d kl [16,26] to constrain the variance of the latent space z with a slight
kl-penalty.after training the lae, we fixed the trained encoder e and then train
a dm with the loss l dm in eq. 3 to model e's latent space z. here z 0 = e(x) in
eq. 3. once the dm is trained, we can use denoising model θ in the dm reverse
sampling process to synthesize a novel latent z0 ∈ r h×w×c and employ the
trained decoder d to generate a new image x = d(z 0 ), which should satisfy the
similar distribution as the data in s.conditional small-scale fine-tuning. using
the lae and dm pretrained on s, we can only generate the new image x following
the similar distribution in s. to generalize our histodiffusion to the
small-scale labeled dataset s collected from a different source (i.e., s ⊂ s),
we further fine-tune histodiffusion using the labeled data from s . let y be the
label of image x in s . to minimize the training cost, we fix both the trained
encoder e and trained dm model θ to keep latent space z unchanged. then we only
fine-tune the decoder d using labeled data (x, y) from s with the following loss
function: ld = lrec(x, x) + λcelce(ϕ(x), y) , (5) where l rec (x, x) is the
self-reconstruction loss between the output reconstructed image x = d(e(x)) and
the input ground truth image x. to enhance the correlation between the decoder
output x and label y, we also add an auxiliary image classifier ϕ trained with
(x, y) on the top of d and impose the cross-entropy classification loss l ce
when fine-tuning d. λ ce is the balancing parameter. we annotate this fine-tuned
decoder as d for differentiation.
to enable conditional image generation with our histodiffusion, we further apply
the classifier-guided diffusion sampling proposed in [4,29,30,33] using the
labeled data (x, y) from small-scale labeled dataset s . we first utilize the
trained encoder e to encode the data x from s as latent z 0 . then we train a
time-dependent latent classifier φ with paired (z t , y) using the following
loss function: (6) where z t ∼ q(z t |z 0 ) is the noisy version of z 0 at the
time step t during the dm forward process, and l ce is the cross-entropy
classification loss. based on the trained unconditional diffusion model θ , and
a classifier φ trained on noisy input z t , we enable conditional diffusion
sampling by perturbing the reverseprocess mean with the gradient of the log
probability p φ (y|z t ) of a target class y predicted by the classifier φ as
follows:where g is the guidance scale. then the dm reverse process in
histodiffusion can finally generate a novel latent z0 satisfying the class
condition y through a markov chain starting with a standard gaussian noise z t ∼
n(0, i) using p θ,φ (z t-1 |z t , y) defined as follows: p θ,φ (zt-1|zt, y) = n
(zt-1; μθ (zt|y), σ θ (zt)) . (8) the final image x of class y can be generated
by applying the fine-tuned decoder d , i.e., x = d ( z0 ).selective
augmentation. to further improve the efficacy of synthetic augmentation, we
follow [36] to selectively add synthetic images to the original labeled training
data based on centroid feature distance. the augmentation ratio is defined as
the ratio between the selected synthetic images and the original training
images. more results are demonstrated later in table 1.
datasets. we employ three public datasets of histopathology images during the
large-scale pre-training procedure. the first one is the h&e breast cancer
dataset [2], containing 312,320 patches extracted from the hematoxylin & eosin
(h&e) stained human breast cancer tissue micro-array (tma) images [18]. each
patch has a resolution of 224 × 224. the second dataset is pannuke [9], a
pan-cancer histology dataset for nuclei instance segmentation and
classification. the pannuke dataset includes 7,901 patches of 19 types of h&e
stained tissues obtained from multiple data sources, and each patch has a
unified size of 256×256 pixels. the third dataset is tcga-brca-a2/e2 [34], a
subset derived from the tcga-brca breast cancer histology dataset [20]. the
subset consists of 482,958 patches with a resolution of 256 × 256. overall,
there are 803,179 patches used for pre-training. as for fine-tuning and
evaluation, we employ the nct-crc-he-100k dataset that contains 100,000 patches
from h&e stained histological images of human colorectal cancer (crc) and normal
tissue. the patches have been divided into 9 classes: adipose (adi), background
(back), debris (deb), lymphocytes (lym), mucus (muc), smooth muscle (mus),
nor-mal colon mucosa (norm), cancer-associated stroma (str), colorectal
adenocarcinoma epithelium (tum). the resolution of each patch is 224 × 224.to
replicate a scenario where only a small annotated dataset is available for
training, we have opted to utilize a subset of 5,000 (5%) samples for
finetuning. this subset has been carefully selected through an even sampling
without replacement from each tissue type present in the train set. it is worth
noting that the labels for these samples have been kept, which allows the
fine-tuning process to be guided by labeled data, leading to better predictions
on the specific task or domain being trained. by ensuring that the fine-tuning
process is representative of the entire dataset through even sampling from each
tissue type, we can eliminate bias towards any particular tissue type. we
evaluate the fine-tuned model on the official test set. the related data use
declaration and acknowledgment can be found in our supplementary
materials.evaluation metrics. we employ fréchet inception distance (fid) score
[12] to assess the image quality of the synthetic samples. we further compute
the accuracy, f1-score, sensitivity, and specificity of the downstream
classifiers to evaluate the performance gain from different augmentation
methods.model implementation. all the patches are resized to 256 × 256 × 3
before being passed into the models. our implementation of histodiffusion
basically follows the ldm-4 [26] architecture, where the input is downsampled by
a factor of 4, resulting in a latent representation with dimensions of 64 × 64 ×
3. we use 1000 timesteps (t = 1000) for the training of diffusion model and
sample with classifier-free guidance scale g = 1.0 and 200 ddim steps. the
latent classifier φ is constructed using the encoder architecture of the lae and
an additional attention pooling layer [25] added before the output layer.we use
the same architecture for the auxiliary image classifier ϕ. for downstream
evaluation, we implement the classifier using the vit-b/16 architecture [5] in
all experiments to ensure fair comparisons. the default hyper-parameter settings
provided in their officially released codebases are followed.comparison to
state-of-the-art. we compare our proposed histodiffusion with the current
state-of-the-art cgan-based method [36]. we employ style-gan2 [14] as the
backbone generative model for cgan-based synthesis. to ensure a fair comparison,
all images synthesized by stylegan2 and histodiffusion model are further
selected based on feature centroid distances [36]. more implementation details
of our proposed histodiffusion, stylegan2, and baseline classifier can also be
found in our supplementary materials. 3, where histodiffusion consistently
generates more realistic images matching the given class conditions than
sytlegan2, especially for classes adi and back. when augmenting the training
dataset with different numbers of images synthesized from histodiffusion and
stylegan2, one can observe that when increasing the ratio of synthesized data to
100%, the fid score of stylegan2 increases quickly and can become even worse
than the one without using image selection strategy. in contrast, histodiffusion
can keep synthesizing high-quality images until the augmentation ratio reaches
300%. regarding classification performance improvement of the baseline
classifier, the accuracy and f1 score of using his-todiffusion augmentation are
increased by up to 6.4% and 6.6%, respectively. even when not using the image
selection module to filter out the low-quality results (i.e., +random 50%), our
histodiffusion can still improve the accuracy by 1.5%. the robustness and
effectiveness of histodiffusion can be attributed to the unconditional
large-scale pre-training, our specially-designed conditional fine-tuning, and
classifier-guided generation, among others.
in this study, we have introduced a novel synthetic augmentation technique,
termed histodiffusion, to enhance the performance of medical image recognition
systems. histodiffusion leverages multiple unlabeled datasets for large-scale,
unconditional pre-training, while employing a labeled dataset for small-scale
conditional fine-tuning. experiment results on a histopathology image dataset
excluded from the pre-training demonstrate that given limited labels,
histodiffusion with image selection remarkably enhances the classification
performance of the baseline model, and can potentially handle any future
incoming small dataset for augmented training using the same pre-trained model.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43895-0 71.
a must-have ingredient for training a deep neural network (dnn) is a large
number of labelled data that is not always available in real-world applications.
this challenge of data annotation becomes even worse for medical image
segmentation tasks that require pixel-level annotation by experts. data
augmentation (da) is a recognized approach to tackle this challenge. common da
strategies create new samples by using predefined transformations such as
rotation, translation, and colour jitter to existing data, where the performance
gains heavily relies on the choice of augmentation operations and parameters
[1].to mitigate this reliance, recent efforts have focused on learning optimal
augmentation operations for a given task and dataset [3,8,11,15]. however,
transformations learned from these methods are typically still limited to a
predefined set of simple operations such as rotation, translation, and scaling.
in the meantime, another direction of research has emerged that provides an
alternative way of learning more expressive augmentations based on
deformation-based transformations commonly used in image registration [6,12,16].
instead of pre-specifying a list of operations such as rotation and scaling [3],
these deformation-based transformations can describe more general spatial
transformations. moreover, they are perfectly suited for modelling an object's
shape changes [16] that are crucial for image segmentation tasks. it thus
provides an excellent candidate for learning shape variations of an object from
the data, and via which to enable shape-based augmentations for medical image
segmentation tasks. [12,16].however, to date, all existing approaches to
learning deformable registrationbased da assume a perfect alignment of image
pairs to learn the transformations. in other words, the deformation-based
transformations are learned globally for the image. this assumption is
restrictive and associated with several challenges. first, the learning of a
global image-level transformation requires image alignment that may be
non-trivial in many scenarios, such as the alignment of tumours that can appear
at different locations of an image, or alignment of images from different
modalities. the learning of transformations itself is also complicated by the
presence of other objects in the image and is best suited when the object of
interest is always in the same (and often centre) location in all the images,
i.e., images are globally aligned a priori [16]. second, the application of the
learned global transformations for da is also restricted to images similar (and
aligned) to those in training. it thus will be challenging to transfer the
learned shape variations to even the same objects across different locations,
orientations, or sizes in the image, let alone transferring across dataset
(e.g., to transfer the learned shape variations of an organ from one image
modality to another).intuitively, object-centric transformations and
augmentations have the potential to overcome the challenges associated with
global image-level transformations. recently, an object-centric augmentation
method termed as tumorcp [13] showed that a simple object-level augmentation,
via copy-pasting a tumour from one location to another, can yield impressive
performance gains. however, the diversity of samples generated by tumorcp is
limited to pasting tumours on different backgrounds with random distortions
without further learned shapebased augmentation.similarly, other existing works
on object-level augmentation of lesions have mostly focused on position,
orientation, and random transformations of the lesion on different backgrounds
[14,17]. to date, no existing works have considered shape-based object-centric
augmentations. enriching object-centric da with learned shape variations -a
factor critical to object segmentation -can result in more diverse samples and
thereby improve dnn training for medical image segmentation.in this paper, we
present a novel approach for learning and transferring object-centric
deformations for da in medical image segmentation tasks. as illustrated in fig.
1, this is achieved with two key elements:-a generative model of object-centric
deformations -constrained to c1 diffeomorphism for better dnn training -to
describe shape variability learned from paired patches of objects of interest.
this allows the learning to focus on the shape variations of an object
regardless of its positions and sizes in the image, thus bypassing the
requirement for image alignment. -an online augmentation strategy to sample
transformations from the generative model and to augment the objects of interest
in place without distorting the surrounding content in the image. this allows us
to add shape diversity to the objects of interest in an image regardless of
their positions or sizes, eventually facilitating transferring the learned
variations across datasets. we demonstrated the effectiveness of the presented
object-centric diffeomorphic augmentation in kidney tumour segmentation,
including using shape variations of kidney tumours learned from the same dataset
(kits [7]), as well as transferring those learned from a larger liver tumour
dataset (lits [2]). experimental results showed that it can enrich the
augmentation diversity of other techniques such as tumorcp [13], and improve
kidney tumour segmentation [7] using shape variations learned either within or
outside the same training data.
we focus on da for tumour segmentation because tumours can occur at different
locations of an organ with substantially different orientations and sizes. it
thus presents a challenging scenario where global image-level deformable
transformations cannot apply. mentation approach comprises as outlined in below
we describe the two key methodological elements.
the goal of this element is to learn to generate diffeomorphic transformation
parameters θ that describe shape variations -in the form of deformable
transformations t θ -that are present within training instances of tumour x. to
realize this, we train a generative model g(.) for θ such that, when given two
instances of tumours (x src , x tgt ), it is asked to generate θ from the
encoded latent representations z in order to deform x src through t θ (x src )
to x tgt .transformations: in order to model shape deformations between x src
and x tgt , we need highly expressive transformations to capture rich shape
variations in tumour pairs. we assume a spatial transformation t θ in the form
of pixel-wise displacement field u as t θ (x) = x + u. inspired from [4,6], we
turn to c 1 diffeomorphisms to model our transformations. c 1 diffeomorphisms
are smooth and invertible transformations that preserve differentiability up to
the first derivative, making them a suitable choice to be embedded in a neural
network for gradient-based optimization [4]. however, the set of all
diffeomorphisms is an infinitely large lie group. to overcome this issue, we
focus on a specific finitedimensional subset of the lie group that is large
enough to capture the relevant variations in the tumours. for this, we make use
of continuous piecewise-affinebased (cpab) transformation based on the
integration of cpa velocity field v θ proposed in [5]. let ω ⊂ r 2 denote the
tumour domain and let p be triangular tesselation of ω [6]. a velocity field
that maps points from ω to r 2 is said to be piecewise affine (pa) if it is
affine when restricted to each triangle of p . the set v of v θ which are zero
on the boundary of ω can be shown to be finitedimensional linear space [5]. the
dimensionality d of v is a result of how fine p is tessellated. it can be shown
that v is parameterized by θ, i.e., any instance of v is a linear combination of
d orthonormal cpa fields with weights θ [5]. a spatial transformation t θ can be
derived by integrating a velocity field v θ [5] as:where the integration can be
done via a specialized solver [5]. the solver chosen produces faster and more
accurate results than a generic ode solver. specifically, the cost for this
solver is o(c1)+o(c2 x number of integration steps), where c1 is matrix
exponential for the number of cells an image is divided into and c2 is the
dimensionality of an image. the transformations t θ thus can be described by a
generative model of θ. we also experimented with squaring and scaling layers for
integration but that resulted in texture loss when learning
transformations.generative modeling: the data generation process can be
described as:where z is the latent variable assumed to follow an isotropic
gaussian prior, p φ (θ|z) is modeled by a neural network parameterized by φ, and
p(x tgt |θ, x src ) follows the deformable transformation as described in
equation ( 1).we define variational approximations of the posterior density as q
ψ (z|x src , x tgt ), modeled by a convolutional neural network that expects two
inputs x src and x tgt . passing a tuple of x src and x tgt as the input helps
the latent representations to learn the spatial difference between two tumour
samples. alternatively, the generative model as described can be considered as a
conditional model where both the generative and inference model is conditioned
on the source tumour sample x src .variational inference: the parameters ψ and φ
are optimized by the modified evidence lower bound (elbo) of the log-likelihood
log p(x tgt |x src ):where the first term in the elbo takes the form of
similarity loss: l 2 norm on the difference between x tgt and xsrc = t θ (x src
) synthesized using the θ from g(z).the second kl term constrains our
approximated posterior q ψ (z|x src , x tgt ) to be closer to the isotropic
gaussian prior p(z), and its contribution to the overall loss is scaled by the
hyperparameter β. to further ensure that xsrc looks realistic, we discourage
g(z) from generating overly-expressive transformations by adding a
regularization term over the l 2 norm of the displacement field u with a tunable
hyperparameter λ reg . the final objective function becomes:object-centric
learning: to learn object-centric spatial transformations, x src and x tgt are
in the forms of image patches that solely contain tumours. given an image and
its corresponding tumour segmentation mask (x, y ), we first extract a bounding
box around the tumour by applying skimage.measure.regionprops from the
scikit-image package to y . we then use this bounding box to carve out the
tumour x from the image x, masking out all the regions within the bounding box
that do not belong to the tumour. all the tumour patches are then resized to the
same scale, such that tumours of different sizes can be described by the same
tesselation resolution. when pairing tumour patches, we pair each tumour with
its k nearest neighbour tumours based on their euclidean distance -this again
avoids learning overly expressive transformation when attempting to deform
between significantly different tumour shapes.
the goal of this element is to sample random object-centric transformations of t
θ from g(z), to generate diverse augmentations of different instances of tumours
in place. however, if we only transform the tumour and keep the rest of the
image identical, the transformed tumour may appear unrealistic and out of
place.to ensure that the entire transformed image appears smooth, we use a
hybrid strategy to construct a deformation field for the entire image x that
combines tumour-specific deformations with an identity transform for the rest of
the image. specifically, we fill a small region around the tumour with
displacements of diminishing magnitudes, achieved by propagating the
deformations from the boundaries of the deformation fields from g(z) to their
neighbours with reduced magnitudes. repeating this process ensures that the
change at the boundaries is smooth and that the transformed region appears
naturally as part of the image.
we used two publicly available datasets, lits [2] and kits [7], for our
experiments. lits [7] 1, ranging from using 11000 pairs from lits to only 3000
pairs when using only 25% samples from kits.
the encoder of g(z) consisted of five convolutional layers and three fully
connected layers, with a latent dimension of 12 for z. the decoder consisted of
five fully connected layers to output the parameters θ for t θ . we trained the
g(z) for a total of 400 epochs and a batch size of 16. we also implemented early
stopping if the validation loss does not improve for 20 epochs. we used adam
optimizer [10] with a learning rate of 1e-4. we trained separate g(z)'s from
kits and lits, respectively. we set β = 0.001 for both models but needed a high
λ reg of 0.009 for the lits model compared to 0.004 for kits model. the tumours
in the lits have higher intensity differences, which may explain why a higher
value of λ reg was needed to ensure that transformed tumours did not become
unrealistic.results: we evaluated g(z) with two criteria. first, the model needs
to be able to reconstruct x tgt by generating θ to transform x src . second, the
model needs to be able to generate diverse transformed tumour samples for a
given tumour sample. figure 2 presents visual examples of the reconstruction and
generation results achieved by g(z). it can be observed that the reconstruction
is successful in most cases, except when x src and x tgt were too different.
this was necessary to ensure that t θ (x src ) did not produce unrealistic
examples. the averaged l2-loss of transformed xsrc was 1.23 on the validation
pairs. we also visually inspected validation samples after training to make sure
that the deformed tumours were similar to the original tumours in appearance.
the generated examples of tumours from a single source, as shown in fig. 2(b),
demonstrated that the generations were diverse yet realistic.
data: we then used g(z) to generate deformation-based augmentations to increase
the size and diversity of training samples for kidney tumour segmentation on
kits. to assess the effect of augmentations on different sizes of labelled data,
we considered training using 25%, 50%, 75%, and 100% of the kits training set.
we considered two da scenarios: augment with transformations learned from kits
(within-data augmentation) versus from lits (cross-data augmentation).models:
for the base segmentation network, we adopted nnu-net [9] as it contains state
of the art (sota) pipeline for medical image segmentation on most datasets. to
make the segmentation pipeline compatible with g(z), we used the 2d segmentation
module of nnu-net. for baselines, we considered 1) default augmentations such as
rotation, scaling, and random crop in nnu-net as well as 2) tumorcp, all
modified for 2d segmentation. note that our goal is not to achieve sota results
on kits, but to test the relative efficacy of the presented da strategies in
comparison with existing object-centric da methods.results: we use sørensen-dice
coefficient (dice) to measure segmentation network performance. dice measures
the overlap between prediction and ground truth. as summarized in table 1, when
combined with tumorcp, the presented augmentations were able to generate
statistically significant (paired t-test, p ≤ 0.05) improvements in all cases
compared to tumorcp alone. this demonstrated the benefit of enriching simple
copy-and-paste da with shape variations. interestingly, cross-data transferring
of the learned augmentations (from lits) outperformed the within-data
augmentation in the majority of the cases. which we believe is because of two
factors. firstly, learning of the within-data augmentations is limited to the
percentage of the training set used for segmentation. the number of objects to
learn transformations from is thus greater in crossdata augmentation settings.
secondly, the transformations present in cross-data are completely unseen in the
segmentation training network which helps in generating more diverse samples.
note that, as the transformations are learned as variations in object shapes,
they can be transferred easily across datasets surprisingly, the improvements
achieved by the presented augmentation strategy were the most prominent when the
segmentation was trained on 50% and 75% of the kits training set. this is
contrary to the expectation that da would be most beneficial when the labelled
training set is small. this may be because smaller sample sizes do not provide
sufficient initial tumor samples for shape transformations. this may also
explain why the combination of tumorcp boosted the performance of our
augmentation strategy, as the oversampling nature of tumorcp provided more
tumour samples for the presented strategy to transform to further enrich the
training set. it is also worth noting that in contrast to prior literature,
random wrapping of objects does not come close to the learned augmentations. we
speculate that while unrealistic transformations work for whole images, they may
be problematic when only augmenting specific local objects in an image.
in this work, we presented a novel diffeomorphism-based object-centric
augmentation that can be learned and used to augment the objects of interest
regardless of their position and size in an image. as demonstrated by the
experimental results, this allows us to not only introduce new variations to
unfixed objects like tumours in an image but also transfer the knowledge of
shape variations across datasets. an immediate next step will be to extend the
presented approach to learn and transfer 3d transformations for 3d segmentation
tasks, and to enrich the shape-based transformation with appearance-based
transformations. in the long term, it would be interesting to explore ways to
transfer knowledge about more general forms of variations across datasets.
the absence of highly accurate and noninvasive diagnostics for risk-stratifying
benign vs malignant solitary pulmonary nodules (spns) leads to increased
anxiety, costs, complications, and mortality [22,26]. the use of noninvasive
methods to discriminate malignant from benign spns is a high-priority public
health initiative [8,9]. deep learning approaches have shown promise in
classifying spns from longitudinal chest computed tomography (ct) [1,5,12,21],
but approaches that only consider imaging are fundamentally limited. multimodal
models generally outperform single modality models in disease diagnosis and
prediction [24], and this is especially true in lung cancer which is heavily
contextualized through non-imaging risk factors [6,23,30]. taken together, these
findings suggest that learning across both time and multiple modalities is
important in biomedical predictive modeling, especially spn diagnosis. however,
such an approach that scales across longitudinal multimodal data from
comprehensive representations of the clinical routine has yet to be demonstrated
[24]. related work. directly learning from routinely collected electronic health
records (ehrs) is challenging because observations within and between modalities
can be sparse and irregularly sampled. previous studies overcome these
challenges by aggregating over visits and binning time series within a
bidirectional encoder representations from transformers (bert) architecture
[2,14,20,25], limiting their scope to data collected on similar time scales,
such as icu measurements, [11,29], or leveraging graph guided transformers to
handle asynchrony [33]. self-attention [31] has become the dominant technique
for learning powerful representations of ehrs with trade-offs in
interpretability and quadratic scaling with the number of visits or bins, which
can be inefficient with data spanning multiple years. in contrast, others
address the episodic nature of ehrs by converting non-imaging variables to
continuous longitudinal curves that provide the instantaneous value of
categorical variables as intensity functions [17] or continuous variables as
latent functions [16]. operating with the hypothesis that distinct disease
mechanisms manifest independently of one another in a probabilistic manner, one
can learn a transformation that disentangles latent sources, or clinical
signatures, from these longitudinal curves. clinical signatures learned in this
way are expert-interpretable and have been well-validated to reflect known
pathophysiology across many diseases [15,18]. given that several clinical risk
factors have been shown to independently contribute to lung cancer risk, these
signatures are well poised for this predictive task. despite the wealth of
studies seeking to learn comprehensive representations of routine ehrs, these
techniques have not been combined with longitudinal imaging.
in this work, we jointly learn from longitudinal medical imaging, demographics,
billing codes, medications, and lab values to classify spns. we converted 9195
non-imaging event streams from the ehr to longitudinal curves to impute
cross-sections and synchronize across modalities. we use independent component
analyses (ica) to disentangle latent clinical signatures from these curves, with
the hypothesis that the disease mechanisms known to be important for spn
classification can also be captured with probabilistic independence. we leverage
a transformer-based encoder to fuse features from both longitudinal imaging and
clinical signature expressions sampled at intervals ranging from weeks to up to
five years. due to the importance of time dynamics in spn classification, we use
the time interval between samples to scale self-attention with the intuition
that recent observations are more important to attend to than older
observations. compared with imaging-only and a baseline that aggregates
longitudinal data into bins, our approach allowed us to incorporate additional
modalities from routinely collected ehrs, which led to improved spn
classification.
latent clinical signatures via probabilistic independence. we obtained event
streams for billing codes, medications, and laboratory tests across the full
record of each subject in our ehr cohorts (up to 22 years). after removing
variables with less than 1000 events and mapping billing codes to the snomed-ct
ontology [7], we arrived at 9195 unique variables. we transformed each variable
to a longitudinal curve at daily resolution, estimating the variable's
instantaneous value for each day [18]. we used smooth interpolation for
continuous variables [4] or a continuous estimate of event density per time for
event data. previous work used gaussian process inference to compute both types
of curves [16,17]. for this work we traded approximation for computational
efficiency. to encode a limited memory into the curve values, each curve was
smoothed using a rolling uniform mean of the past 365 d (fig. 1,left).we use an
ica model to estimate a linear decomposition of the observed curves from the
ehr-pulmonary cohort to independent latent sources, or clinical signatures.
formally, we have dataset d ehr-pulmonary = {l k | k = 1, . . . , n} with
longitudinal curves denoted as l k = {l i |i = 1, . . . , 9195}. we randomly
sample l i ∀i ∈ [1,9195] at a three-year resolution and concatenate samples
across all subjects as x i ∈ r m . for d ehr-pulmonary , m was empirically found
to be 630037. we make a simplifying assumption that x i is a linear mixture of c
latent sources, s, with longitudinal expression levels e ∈ r m : the linear
mixture is then x = se with x i forming the rows of x, s ∈ r 9195×c denoting the
independent latent sources and e ∈ r c×m denoting the expression matrix. we set
c = 2000 and estimated s in an unsupervised manner using fastica [13]. given
longitudinal curves for another cohort, for instance d image-ehr = {x k | k = 1,
. . . , n}, we obtain expressions of clinical signatures for subject k via e k =
s -1 x k (fig. 1, left).longitudinal multimodal transformer (tdsig). we
represent our multimodal datasets d image-ehr and }, where t is the maximum
sequence length. we set t = 3 and added a fixed padding embedding to represent
missing items in the sequence. embeddings that incorporate positional and
segment information are computed for each item in the sequence (fig. 1, right).
token embeddings for images are a convolutional embeddings of five concatenated
3d patches proposed by a pretrained spn detection model [21]. we use a 16-layer
resnet [10] to compute this embedding. likewise, token embeddings for clinical
signature expressions are linear transformations to the same dimension as
imaging token embeddings. the sequence of embeddings are then passed through a
multi-headed transformer. all embeddings except the nodule detection model are
co-optimized with the transformer. we will refer to this approach as
tdsig.time-distance self-attention. following [5,19,32], we intuit that if
medical data is sampled as a cross-sectional manifestation of a continuously
progressing phenotype, we can use a temporal emphasis model (tem) emphasize the
importance of recent observations over older ones. additionally, self-attention
is masked for padded embeddings, allowing our approach to scale with varying
sequence lengths across subjects. formally, if subject k has a sequence of t
images at relative acquisition days t 1 . . . t t , we construct a matrix r of
relative times with entries r i,j = |t t -t i | where t i is the acquisition day
of tokens êk,i and ĝk,i , or 0 if they are padded embeddings. we map the
relative times in r to a [0,1] value in r using a tem of the form:this is a
flipped sigmoid function that monotonically decreases with the relative time
from the most recent observation. its slope of decline and decline offset are
governed by learnable non-negative parameters b and c respectively. a separate
tem is instantiated for each attention head, with the rationale that separate
attention heads can learn to condition on time differently. the transformer
encoder computes query, key, and value matrices as linear transformations of
input embedding h = { ê ĝ} at attention head ptem-scaled self-attention is
computed via element-wise multiplication of the query-key product and r:where m
is the padding mask [31] and d is the dimension of the query and key matrices.
relu gating of the query-key product allows the tem to adjust the attention
weights in an unsigned direction.baselines. we compared against a popular
multimodal strategy that aggregates event streams into a sequence of bins as
opposed to our method of extracting instantaneous cross-sectional
representations. for each scan, we computed a tf-idf [27] weighted vector from
all billing codes occurring up to one year before the scan acquisition date. we
passed this through a published word2vec-based medical concept embedding [3] to
compute a contextual representation ∈ r 100 . this, along with the subject's
scans, formed a sequence that was used as input to a model we call tdcode2vec.
our search for contextual embeddings for medications and laboratory values did
not yield any robust published models that were compatible with our ehr's
nomenclature, so these were not included in tdcode2vec. we also performed
experiments using only image sequences as input, which we call tdimage. finally,
we implemented single cross-sectional versions of tdimage, tdcode2vec, and
tdsig, csimage, cscode2vec, and cssig respectively, using the scan date closest
to the lung malignancy diagnosis for cases or spn date for controls. all
baselines except csimage, which employed a multi-layer perceptron directly after
the convolutional embedding, used the same architecture and time-distance
self-attention as tdsig. the transformer encoders in this study were
standardized to 4 heads, 4 blocks, input token size of 320, multi-layer
perception size of 124, self-attention weights of size 64. this work was
supported by pytorch 1.13.1, cuda 11.7.
datasets. this study used an imaging-only cohort from the nlst [28] and three
multimodal cohorts from our home institution with irb approval (table 1). for
the nlst cohort (https://cdas.cancer.gov/nlst/), we identified cases who had a
biopsy-confirmed diagnosis of lung malignancy and controls who had a positive
screening result for an spn but no lung malignancy. we randomly sampled from the
control group to obtain a 4:6 case control ratio. next, ehr-pulmonary was the
unlabeled dataset used to learn clinical signatures in an unsupervised manner.
we searched all records in our ehr archives for patients who had billing codes
from a broad set of pulmonary conditions, intending to capture pulmonary
conditions beyond just malignancy. additionally, image-ehr was a labeled dataset
with paired imaging and ehrs. we searched our institution's imaging archive for
patients with three chest cts within five years. in the ehr-image cohort,
malignant cases were labeled as those with a billing code for lung malignancy
and no cancer of any type prior. importantly, this case criteria includes
metastasis from cancer in non-lung locations. benign controls were those who did
not meet this criterion. finally, image-ehr-spn was a subset of image-ehr with
the inclusion criteria that subjects had a billing code for an spn and no cancer
of any type prior to the spn. we labeled malignant cases as those with a lung
malignancy billing code occurring within three years after any scan and only
used data collected before the lung malignancy code. all data within the
five-year period were used for controls. we removed all billing codes relating
to lung malignancy. a description of the billing codes used to define spn and
lung cancer events are provided in supplementary 1.2. training and validation.
all models were pretrained with the nlst cohort after which we froze the
convolutional embedding layer. while this was the only pretraining step for
image-only models (csimage and tdimage), the multimodal models underwent another
stage of pretraining using the image-ehr cohort with subjects from image-ehr-spn
subtracted. in this stage, we randomly selected one scan and the corresponding
clinical signature expressions for each subject and each training epoch. models
were trained until the running mean over 100 global steps of the validation loss
increased by more than 0.2. for evaluation, we performed five-fold
cross-validation with image-ehr-spn, using up to three of the most recent scans
in the longitudinal models. we report the mean auc and 95% confidence interval
from 1000 bootstrapped samples, sampling with replacement from the pooled
predictions across all test folds. a two-sided wilcoxon signed-rank test was
used to test if differences in mean auc between models were
significant.reclassification analysis. we performed a reclassification analysis
of low, medium, and high-risk tiers separated by thresholds of 0.05 and 0.65,
which are the cutoffs used to guide clinical management. given a baseline
comparison, our approach reclassifies a subject correctly if it predicts a
higher risk tier than the baseline in cases, or a lower risk tier than the
baseline in controls (fig. 2).
the significant improvement with tdsig over cssig demonstrates the advantage of
longitudinally in the context of combining images and clinical signatures (table
2). there were large performance gaps between tdsig and tdcode2vec, as well as
between cssig and cscode2vec, demonstrating the advantage of clinical signatures
over a binned embedding strategy. cross-sectional embedded billing codes did not
significantly improve performance over images alone (cscode2vec vs csimage, p =
0.56), but adding clinical signatures did (cssig vs csimage, p < 0.01; tdsig vs
tdimage, p < 0.01) and the greatest improvement in longitudinal data over single
cross sections occurred when clinical signatures were included. for control
subjects, tdsig correctly/incorrectly reclassified 40/18 from tdcode2vec, 54/8
from tdimage, 12/18 from cssig, 104/7 from cscode2vec, and 125/5 from csimage.
for case subjects, tdsig correctly/incorrectly reclassified 13/10 from
tdcode2vec, 17/8 from tdimage, 12/2 from cssig, 23/16 from cscode2vec, and 29/16
from csimage (fig. 2). full reclassification matrices are reported in
supplementary 6.1. on qualitative inspection of a control subject, clinical
signatures likely added clarity to benign imaging findings that were difficult
for baseline approaches to classify (fig. 3).
this work presents a novel transformer-based strategy for integrating
longitudinal imaging with interpretable clinical signatures learned from
comprehensive multimodal ehrs. we demonstrated large performance gains in spn
classification compared with baselines, although calibration of our models is
needed to assess clinical utility. we evaluated on clinically-billed spns,
meaning that clinicians likely found these lesions difficult enough to conduct a
clinical workup. in this setting, we found that adding clinical context
increased the performance gap between longitudinal data and single
cross-sections. our clinical signatures incorporated longitudinality and
additional modalities to build a better representation of clinical context than
binned embeddings. we release our implementation at
https://github.com/masilab/lmsignatures.the lack of longitudinal multimodal
datasets has long been a limiting factor [24] in conducting studies such as
ours. one of our contributions is demonstrating training strategies in a
small-dataset, incomplete-data regime. we were able to overcome our small cohort
size (image-ehr-spn) by leveraging unsupervised learning on datasets without
imaging (ehr-pulmonary), pretraining on public datasets without ehrs (nlst), and
pretraining on paired multimodal data with noisy labels (image-ehr) within a
flexible transformer architecture.our approach of sampling cross-sections where
clinical decisions are likely to be made scales well with long, multi-year
observation windows, which may not be true for bert-based embeddings [20,25]. we
did not compare against these contextual embeddings because none have been
publically released, but integrating these with longitudinal imaging is an area
of future investigation.
automatic multi-organ segmentation (mos) plays a vital role in computeraided
diagnosis and treatment planning. recently, deep learning based methods have
made remarkable progress in solving mos tasks. however, they typically require a
large amount of expert-level accurate, densely-annotated data for training,
which is laborious and time consuming to collect. therefore, existing fully
labeled datasets (termed as flds) are very few and often low in sample size [1].
while there exist many publicly available partially labeled datasets (plds)
[2,3], each with one or a few out of the many organs annotated. this has
motivated the development of various partially-supervised multi-organ
segmentation (psmos) methods that aim to learn a unified model from a union of
such datasets. for example, dmitriev and kaufman proposed the conditional u-net
to enable psmos using a single unified network [4]. co-training between two
models with consistency constraints on soft pseudo labels [6], and multi-scale
features learned in a pyramid-input and pyramid-output network [7] were both
explored for psmos. other researchers resorted to prior knowledge to guide the
training process. in pann [8], the average organ size distributions on the plds
were constrained to resemble the prior statistics obtained from the fld. another
method was introduced in [9] where the non-overlapping characteristics between
different organs were exploited to design the exclusion loss.although witnessed
great progress in psmos, existing methods are faced with the following
challenges: 1) shortage in sufficiently labeled samples for supervised learning,
since voxel-level labels are only available for a subset of organs in plds; 2)
significant cross-site appearance variations caused by different imaging
protocols or subject cohorts. different from existing methods, we propose a
novel framework to explicitly tackle the above-mentioned challenges.to handle
the label-scarcity problem in plds, we propose a novel affinityaware consistency
learning (acl) scheme to incorporate voxel-to-organ affinity in the embedding
space into consistency learning. although consistency learning is frequently
used for leveraging unlabeled data in label-efficient learning [10][11][12], it
is mostly deployed in the label space [13][14][15], while little attention has
been paid to exploring consistency in the latent feature space. zheng et al.
[16] proposed to adopt auxiliary student-teacher networks to utilize the
features for consistency learning, which introduced more parameters, thus were
computationally expensive. by incorporating voxel-to-organ affinity in the
embedding space into consistency learning, our acl scheme is plug-and-play and
can capture rich context information in the embedding space.to tackle the data
discrepancy problem [17], based on the assumption that a well trained joint
model should generate consistent feature distributions across different sites,
we propose a novel cross-site feature alignment (csfa) module, where two terms
are introduced to attend to both the organ-specific and interorgan statistics in
the latent feature space. concretely, for each pld, we restrain the
organ-specific prototypes calculated in each mini-batch to be close to the
corresponding prototypes generated on the small-sized fld. to further reduce the
data discrepancy problem, we constrain the affinity relationships across
different organ-specific prototypes to be consistent among different sites. by
doing this, we transfer not only the single-class centroid, but also the
inter-organ affinity learned from the small-sized fld to plds, allowing for
knowledge propagation at multiple granularity levels. our contributions can be
summarized as follows:-we propose a novel affinity-aware consistency learning
scheme to incorporate voxel-to-organ affinity in the embedding space into a
consistency learning framework, which can capture semantic context in the latent
feature space. -we design a novel cross site feature alignment module to
calibrate feature distributions of plds with distribution priors learned from a
small-sized fld, alleviating the cross-site data discrepancy. -we demonstrate on
five datasets collected from different sites that our method can effectively
learn a unified mos model from multi-source datasets, achieving superior
performance over the state-of-the-art (sota) methods. a schematic illustration
of our framework. "aug" refers to perturbations with data augmentations. in the
csfa module, hollow shapes refer to the features belonging to unlabeled organs
in the plds, while solid ones refer to labeled organs. the affinity matrix is
calculated according to eq. 10 and eq. 11. lseg is the segmentation loss.
to learn a unified model from a small-sized fld and a number of plds, we propose
a novel framework to address the issues of label-scarcity and cross-site data
discrepancy. the overall workflow of our method is presented in fig. 1.during
training, in each batch, we sample 3d patches from both the fld and one of the
plds, where the teacher-student scheme [14] is adopted to impose consistency
constraints on the unlabeled voxels of the pld. in our method, apart from the
label space consistency, we introduce the acl scheme to explore consistency in
the embedding space. we further leverage the csfa module to perform feature
alignment between the fld and the pld. please note that consistency constraints
are only imposed on the unlabeled voxels of plds. the label space consistency
loss is omitted in fig. 1 for brevity.
denote y full as the full label set, i.e., y full = {0, 1, 2, • • • , c}, where
0 refers to the background class, and {1, • • • , c} are one-to-one mappings to
the target organs, c is the number of target organs. given a small-sized fld d f
and a number of pldswhere n is the number of plds. each dataset can then be
formally defined as either, where i f j,i is the i-th pixel of the j-th image in
the fld d f , and y f j,i is its corresponding label. similarly, (i n j,i , y n
j,i ) is the i-th pixel-label pair of the j-th image in the n-th pld d n p .
please note that each d n p contains only a subset of the full label set, i.e.,
y n p = unique({y n j,i }) y full , where unique(•) returns the unique values in
the label set. the task of psmos aims to learn the mapping function ϕ = f • g to
project the 3d image patch i j ∈ r h×w×z to its corresponding semantic labels,
where f is the feature extractor, g is the segmentation head, and • means
sequentially executing f and g, (h, w, z) are the 3d patch size. since
foreground organ in one pld may be labeled as background in another dataset,
such a background ambiguity brings challenges to joint training on multiple
plds. to address this issue, we follow [7,9] to calculate the marginal cross
entropy and marginal dice loss as the baseline segmentation loss l seg .
in our proposed framework, the calculation of both the pixel-to-prototype
predictions (in acl) and the feature alignment loss (in csfa) are based on
organspecific prototypes. in each mini-batch, denote the organ-specific
prototypes for the fld as {q c }, c ∈ {0, • • • , c} and prototypes for the n-th
pld as {q n c }, c ∈ {0, • • • , c}, then they are generated as follows. on the
fld, we generate the prototypes in an exponential moving average scheme.
specifically, the feature prototype of the t-th iteration is calculated as (for
brevity, we omit the iteration superscript t),where q update c is the average
feature of the c-th class in current mini-batch of the fld and α is the
weighting coefficient. given the feature maps f = {f i } and their related
labels {y i }, where f i represents the i-th pixel in the feature maps of
current mini-batch, the feature center of the c-th class is then calculated
as,where z c is the number of pixels belonging to the c-th class in current
mini-batch. on the n-th pld, we directly adopt the feature centers calculated in
each mini-batch as the organ-specific prototypes. in specific, for the labeled
organs, the prototypes {q n c }, c ∈ y n p are calculated according to eq. 2,
with feature maps generated on 3d patches from the n-th pld. while on the
unlabeled organs, only reliable features are used for calculating the pseudo
feature centers as,where p n i is the normalized prediction score generated from
the teacher model, y i denotes the corresponding pseudo label, τ is the
confidence threshold, z c is the number of reliable predictions in class c, and
1[•] returns 1 if the inside condition is true, otherwise, returns 0.
in this paper, we propose to incorporate the voxel-to-organ affinity into
consistency learning. specifically, instance-to-prototype matching is calculated
to capture the voxel-to-organ affinity. the affinities are then transformed into
normalized scores for calculating the consistency constraint on two perturbed
inputs. we adopt the teacher-student scheme [14] for consistency learning on the
unlabeled data. formally, denote i t , i s as the perturbed versions of the same
sampled 3d patch for the teacher branch and the student branch respectively. in
the teacher branch, denote φ i = f tea (i t,i ) ∈ r d as the extracted feature
for the i-th pixel of 3d image patch i t . given the prototypes generated on the
fldwhere < •, • > calculates the cosine similarity between the two
terms.similarly, in the student branch, denote ψ i as the i-th featuresince p
t,i , p s,i model the voxel-to-organ affinities in the embedding space,
constraining consistency on them introduces rich context information for
training on the unlabeled data, which is formulated as,where 1is the
normalization factor to get the mean kl-divergence in the feature embedding
space. denote ϕ tea = f tea • g tea , ϕ stu = f stu • g stu as the teacher and
student segmentation model respectively, the logits from the student and the
teacher branch can be calculated as l s,i = ϕ stu (i s,i ), l t,i = ϕ tea (i t,i
). then the consistency loss in the label space is calculated as,where 1is the
normalization factor. the overall affinity-aware consistency loss is finally
formulated as,
the csfa module is proposed to calibrate feature distributions across different
sites. specifically, given the learned prototypes from current mini-batch of the
n-th pld ({q n c }, c ∈ {0, • • • , c}), they can be regarded as the
organ-specific cluster centers in the embedding space. then, compactness loss is
introduced to calibrate d n p with the cluster centers learned from the fld
as,where |y n p | returns the number of labeled organs in d n p . to further
take into consideration the inter-organ affinity relationships during feature
distribution alignment, we first model inter-organ affinity relationships on the
fld by calculating the affinity matrix a = {a ij } ∈ r (c+1)×(c+1) as shown in
fig. 1,similarly, we can obtain the affinity matrixthen the affinity
relationship aware feature alignment loss is calculated as,where a c , a n p,c
refer to the c-th row of a and a n p respectively. the overall cross-site
alignment loss is then calculated as the sum of the compactness loss and the
affinity relationship aware calibration loss,the overall training objective is
finally formulated as,where λ a is the tradeoff parameter.
datasets and implementation details. we use five abdominal ct datasets (malbcvwc
[1], decathlon spleen [3], kits [2], decathlon liver [3] and decathlon pancreas
[3] datasets respectively) to evaluate the effectiveness of our method
[1][2][3]. the spatial resolution of all these datasets are resampled to (1 × 1
× 3)mm 3 . we randomly split each dataset into training (60%), validation (20%)
and testing (20%). we adopt 3d u-net [18] as our backbone model. the patch size
(h, w, z) is set to (160, 160, 96). the hyper-parameters α and τ are empirically
set to 0.9, and 0.8, respectively. λ a is initialized as 0.01 and linearly
decreased to 1e-3 at 20000 iterations. we use sgd optimizer to train the model
and the initial learning rate is set to 0.01. we adopt dice similarity
coefficient (dsc) as metric to evaluate the performance of different methods. 1,
the "baseline+acl" setting reports the results with our proposed affinity-aware
consistency learning scheme. comparing to the baseline, it brings a 1.1%
performance gain in terms of dsc. by introducing the csfa module, the
"baseline+acl+csfa" setting can further boost the performance by 0.5% in terms
of dsc. we further study the effectiveness of the csfa module in alleviating
crosssite data discrepancy. concretely, we measure the feature distribution
discrepancy between the fld and each pld by calculating the maximum mean
discrepancy (mmd) using gaussian kernel [19], which was designed to quantify
domain discrepancy. we conduct "full vs partial" mmd analysis on the following
two settings: "ours w/csfa" and "ours wo/csfa", where "ours w/csfa" is the
proposed framework, while "ours wo/csfa" setting refers to removing the csfa
module from our framework. in the mmd calculation, for each dataset, we first
generate features from the penultimate layer. then we randomly select 2000
features in each class for mmd calculation. please note, for each pld, we adopt
the pseudo labels for feature selection. detailed comparison results are
illustrated in table 2. as shown, by introducing the csfa module, the feature
distribution discrepancy in terms of mmd can be effectively alleviated across
all the "full vs partial" dataset pairs.comparison with the state-of-the-art
(sota) methods. we compare with four sota methods, including pann [8], pipo [7],
marginal loss [9], and dodnet [5]. for fair comparison, all the sota methods
were trained/tested on our own dataset splits. we also implemented our method
taking the nnunet as the backbone to compare with marginal loss [9] and pann
[8]. we reported the dsc values for each organ across test sets from all the
datasets. for a straightforward comparison with the sota, we also recorded the
average dsc over all the organs. detailed results are illustrated in table 3. as
shown, our method achieves the best performance. specifically, our method
outperforms the secondbest method pann [8] with a 1.2% dsc gain using the same
nnunet backbone. and our method when taking 3d-unet as the backbone also
outperforms the listed sota methods. we further conduct paired t-test to compare
the difference between ours and other sota methods, the p-values are 2e-8
(pipo), 2e-5 (dodnet), 2e-4 (marginal loss), 0.037 (pann), respectively. as all
p-values are smaller than 0.05, the differences between ours and other sota
methods are statistically significant. in practice, some organs are much harder
to be well-segmented than others due to their relatively small organ sizes.
therefore, we pay more attention to the performance on those hard organs (in our
datasets, pancreas and kidneys are deemed to be more difficult due to their
relatively small sizes). from the last column of table 3, we can see that the
segmentation performance gains of our method are more pronounced on hard organs
(on average a 1.8% dsc gain). figure 2 demonstrates the qualitative
visualization results on some hard samples. as shown in this figure, our method
can generate better segmentation results than other sota methods. besides, the
reasonable performance on segmenting kidney with tumors (row 2 in fig. 2) makes
our method promising in clinical practice.
in this paper, we designed a novel affinity-aware consistency learning scheme
(acl) to model voxel-to-organ affinity context in the feature embedding space
into consistency learning. meanwhile, the csfa module was designed to perform
feature distribution alignment across different sites, where both organ-specific
cluster centers and the inter-organ affinity relationships were propagated from
the small-sized fld to plds for cross-site feature alignment. extensive ablation
studies validated effectiveness of each component in our method. quantitative
and qualitative comparison results with other sota methods demonstrated superior
performance of our method.
fig. 2. 3d visualized results of some hard samples.
globally, cancer is a leading cause of death and the burden of cancer incidence
and mortality is rapidly growing [1]. in cancer diagnosis, treatment, and
management, pathologydriven information plays a pivotal role. cancer grade is,
in particular, one of the major factors that determine the treatment options and
life expectancy. however, the current pathology workflow is sub-optimal and
low-throughput since it is, by and large, manually conducted, and the large
volume of workloads can result in dysfunction or errors in cancer grading, which
have an adversarial effect on patient care and safety [2]. therefore, there is a
high demand to automate and expedite the current pathology workflow and to
improve the overall accuracy and robustness of cancer grading.recently, many
computational tools have shown to be effective in analyzing pathology images
[3]. these are mainly built based upon deep convolutional neural networks
(dcnns). for instance, [4] used dccns for prostate cancer detection and grading,
[5] classified gliomas into three different cancer grades, and [6] utilized an
ensemble of dcnns for breast cancer classification. to further improve the
efficiency and effectiveness of dcnns in pathology image analysis, advanced
methods that are tailored to pathology images have been proposed. for example,
[7] proposed to incorporate both local and global contexts through the
aggregation learning of multiple context blocks for colorectal cancer
classification; [8] extracted and utilized multi-scale patterns for cancer
grading in prostate and colorectal tissues; [9] proposed to re-formulate cancer
classification in pathology images as both categorical and ordinal
classification problems. built based upon a shared feature extractor, a
categorical classification branch, and an ordinal classification branch, it
simultaneously conducts both categorical and ordinal learning for colorectal and
prostate cancer grading; a hybrid method that combines dccns with hand-crafted
features was developed for mitosis detection in breast cancer [10]. moreover,
attention mechanisms have been utilized for an improved pathology image
analysis. for instance, [11] proposed a two-step framework for glioma sub-type
classification in the brain, which consists of a contrastive learning framework
for robust feature extractor training and a sparse-attention block for
meaningful multiple instance feature aggregation. such attention mechanisms have
been usually utilized in a multiple instance learning framework or as
self-attention for feature representations. to the best of our knowledge,
attention mechanisms have not been used for feature representations of class
centroids.in this study, we propose a centroid-aware feature recalibration
network (cafenet) for accurate and robust cancer grading in pathology images.
cafenet is built based upon three major components: 1) a feature extractor, 2) a
centroid update (cup) module, and 3) a centroid-aware feature recalibration
(cafe) module. the feature extractor is utilized to obtain the feature
representation of pathology images. cup module obtains and updates the centroids
of class labels, i.e., cancer grades. cafe module adjusts the input embedding
vectors with respect to the class centroids (i.e., training data distribution).
assuming that the classes are well separated in the feature space, the centroid
embedding vectors can serve as reference points to represent the data
distribution of the training data. this indicates that the centroid embedding
vectors can be used to recalibrate the input embedding vectors of pathology
images. during inference, we fix the centroid embedding vectors so that the
recalibrated embedding vectors do not vary much compared to the input embedding
vectors even though the data distribution substantially changes, leading to
improved stability and robustness of the feature representation. in this manner,
the feature representations of the input pathology images are re-calibrated and
stabilized for a reliable cancer classification. the experimental results
demonstrate that cafenet achieves the state-of-the-art cancer grading
performance in colorectal cancer grading datasets. the source code of cafenet is
available at https://github.com/col in19950703/cafenet.
the overview of the proposed cafenet is illustrated in fig. 1. cafenet employs a
deep convolutional neural network as a feature extractor and an attention
mechanism to produce robust feature representations of pathology images and
conducts cancer grading with high accuracy. algorithm 1 depicts the detailed
algorithm of cafenet.
let {x i , y i } n i=1 be a set of pairs of pathology images and ground truth
labels where n is the number of pathology image-ground truth label pairs, x i ∈
r h×w×c is the i th pathology image, y i ∈ {c 1 , . . . , c m } represents the
corresponding ground truth label. h, w, and c denote the height, width, and the
number of channels, respectively. m is the cardinality of the class labels.
given x i , a deep neural network f maps x i into an embedding space, producing
an embedding vector e i ∈ r d . the embedding vector e i is fed into 1) a
centroid update (cup) module and 2) a centroid-aware feature recalibration
(cafe) module. cup module obtains and updates the centroid of the class label in
the embedding space e c ∈ r m ×d . cafe module adjusts the embedding vector e i
in regard to the embedding vectors of the class centroids and produces a
recalibrated embedding vector e r i . e i and e r i are concatenated together
and is fed into a classification layer to conduct cancer grading. from the
centroid embedding vectors e c by using a linear layer. then, attention scores
are computed via a dot product between q e and k c followed by a softmax
operation. multiplying the attention scores by v c , we obtain the recalibrated
feature representation e r for the input embedding vectors e. the process can be
formulated as follows:finally, cafe concatenates e and e r and produces them as
the output.
we employ efficientnet-b0 [12] as a backbone network. efficientnet is designed
to achieve the state-of-the-art accuracy on computer vision tasks while
minimizing computational costs through a compound scaling method.
efficientnet-b0 is composed of one convolution layer and 16 stages of mobile
inverted bottleneck blocks, of which each with a different number of layers and
channels. each mobile inverted bottleneck block comprises one pointwise
convolution (1 × 1 convolution for the channel expansion), one depth-wise
separable convolution with a kernel size of 3 or 5, and one project pointwise
convolution (1 × 1 convolution for the channel reduction). 3 experiments and
results
two publicly available colorectal cancer datasets [9] were employed to evaluate
the effectiveness of the proposed cafenet.
we conducted a series of comparative experiments to evaluate the effectiveness
of cafenet for cancer grading, in comparison to several existing methods: 1)
three dcnnbased models: resnet [13], densenet [14], efficientnet [12], 2) two
metric learningbased models: triplet loss (triplet) [15] and supervised
contrastive loss (sc) [16], 3) two transformer-based models: vision transformer
(vit) [17] and swin transformer (swin) [18], and 4) one (pathology)
domain-specific model (m mae-ce o ) [9], which demonstrates the state-of-the-art
performance on the two colorectal cancer datasets under consideration. for
triplet and sc, efficientnet was used as a backbone network. we trained cafenet
and other competing networks on c train and selected the best model usingc
validation . then, the chosen model of each network was separately applied to c
testi andc testii . the results of m mae-ce o were obtained from the original
literature.
we initialized all models using the pre-trained weights on the imagenet dataset,
and then trained them using the adam optimizer with default parameter values (β
1 = 0.9, β 2 = 0.999, ε = 1.0e-8) for 50 epochs. we employed cosine anneal warm
restart schedule with initial learning rates of 1.0 e -3 , η min = 1.0 e -3 ,
and t 0 = 20. after data augmentation, all patches, except for those used in vit
[17] and swin [18] models, were resized to 512 × 512 pixels. for vit and swin,
the patches were resized to 384 × 384 pixels. we implemented all models using
the pytorch platform and trained on a workstation equipped with two rtx 3090
gpus. to increase the variability of the dataset during the training phase, we
applied several data augmentation techniques, including affine transformation,
random horizontal and vertical flip, image blurring, random gaussian noise,
dropout, random color saturation and contrast conversion, and random contrast
transformations. all these techniques were implemented using the aleju library
(https:// github.com/aleju/imgaug).
we evaluated the performance of colorectal cancer grading by the proposed
cafenet and other competing models using five evaluation metrics, including
accuracy (acc), precision, recall, f1-score (f1), and quadratic weighted kappa
(κ w ). table 2 demonstrates the quantitative experimental results on c testi .
the results show that cafenet was one of the best performing models along with
resnet, swin, and m mae-ce o .were the best performing models. among dcnn-based
models, resnet was superior to other dcnn-based models. metric learning was able
to improve the classification performance. effcientnet was the worst model among
them, but with the help of triplet loss (triplet) or supervised contrastive loss
(sc), the overall performance increased by ≥2.8% acc, ≥0.023 precision, ≥0.001
recall, ≥0.010 f1, and ≥0.047 κ w . among the transformer-based models, swin was
one of the best performing models, but vit showed much lower performance in all
evaluation metrics. moreover, we applied the same models to c testii to test the
generalizability of the models. we note that c testi originated from the same
set with c train and c validation and c testii was obtained from different time
periods and using a different slide scanner. table 3 depicts the quantitative
classification results on c testii . cafenet outperformed other competing models
in all evaluation metrics except triplet for recall. in a headto-head comparison
of the classification results between c testi and c testii , there was a
consistent performance drop in the proposed cafenet and other competing models.
this is ascribable to the difference between the test datasets (c testi and c
testii ) and the training and validation datasets (c train and c validation ).
in regard to such differences, it is striking that the proposed cafenet achieved
the best performance on c testii . cafenet, resnet, swin, and m mae-ce o were
the four best performing models on c testi . however, resnet, swin, and m mae-ce
o showed a higher performance drop in all evaluation metrics. cafenet had a
minimal performance drop except efficientnet. efficientnet, however, obtained
poorer performance on both c testi and c testii . these results suggest that
cafenet has the better generalizability so as to well adapt to unseen
histopathology image data.we conducted ablation experiments to investigate the
effect of the cafe module on cancer classification. the results are presented in
table 4. the exclusion of the cafe module, i.e., efficientnet, resulted in much
worse performance than cafenet. using only the recalibrated embedding vectors e
r , a substantial drop in performance was observed. these two results indicate
that the recalibrated embedding vectors complement to the input embedding
vectors e. moreover, we examined the effect of the method that merges the two
embedding vectors. using addition, instead of concatenation, there was a
consistent performance drop, indicating that concatenation is the superior
approach for combining the two embedding vectors together. in addition, we
compared the model complexity of the proposed cafenet and other competing
models. table 5 demonstrates the number of parameters, floating point operations
per second (flops), and training and inference time (in milliseconds). the
proposed cafenet was one of the models that require a relatively small number of
parameters and flops and a short amount of time during training and inference.
densenet, efficientnet, triplet, sc, and m mae-ce o contain the smaller number
of parameters than that of cafenet, but these models show either the higher
number of flops or longer time during training and/or inference. similar
observations were made for resnet, vit, and swin. these models require much
larger number of parameters and flops and longer training time. these results
confirm that the proposed cafenet is computational efficient and it does not
achieve its superior learning capability and generalizability at the expense of
the model complexity.
herein, we propose an attention mechanism-based deep neural network, called
cafenet, for cancer classification in pathology images. the proposed approach
proposes to improve the feature representation of deep neural networks by
re-calibrating input embedding vectors via an attention mechanism in regard to
the centroids of cancer grades. in the experiments on colorectal cancer datasets
against several competing models, the proposed network demonstrated that it has
a better learning capability as well as a generalizability in classifying
pathology images into different cancer grades. however, the experiments were
only conducted on two public colorectal cancer datasets from a single institute.
additional experiments need to be conducted to further verify the findings of
our study. therefore, future work will focus on validating the effectiveness of
the proposed network for other types of cancers and tissues in pathology images.
shows the details of the datasets. both datasets provide colorectal pathology
images with ground truth labels for cancer grading. the ground labels are benign
(bn), well-differentiated (wd) cancer, moderatelydifferentiated (md) cancer, and
poorly-differentiated (pd) cancer.
artificial intelligence (ai) research has evolved rapidly, and unprecedented
breakthroughs have been made in many fields. applications of ai products can be
witnessed in our daily life, such as autonomous driving, computer-aided
diagnosis, automatic voice customer service, etc. the development of ai is
undoubtedly a revolution in the course of human history.the most effective and
commonly used ai model is the one based on deep neural networks [1]. however,
with continuous research being held in methodologies, dl models are becoming
more and more complicated. researchers found that the deeper and more complex dl
models are, the better the performance they could achieve for the tasks that
traditional ai algorithms could not work well. the complexity of dl models
reduces interpretability and transparency substantially; therefore, the current
dl models are "black-box" [2]. it is difficult to find how the model works in a
way that humans can understand. because of that, what researchers can do is only
to prepare enough data and spend time training a model to obtain a high
performance. therefore, researchers or users can hardly find the reason why a dl
model made a wrong decision.xai is an old area in ai research, but was named
relatively recently [3,4], focusing now on the explainability of dl models. the
final goal of xai is to develop methods for revealing a basis for the decision
made by a dl model and how the decision was made by the model to let users
understand and trust the decision and model. many xai methods have been proposed
to explain a trained dl model (i.e., post-hoc methods). representative xai
methods include class activation mapping (cam) [5], grad-cam, layer-wise
relevance propagation (lrp) [6], dl important features (deeplift) [7], local
interpretable model-agnostic explanations (lime) [8], and shapley additive
explanations (shap) [9]. these xai methods offer post-hoc explanations that
indicate which areas in a given input image the trained model focuses on and
identify which areas in the image have a positive or negative impact on the
model decision. in other words, those xai methods are "instance-based" and
limited to the visual explanation of model's attentions in a given input image
(i.e., an instance). however, they do not offer explanations of the learned
functions of the network.in this study, we developed and presented an original
xai approach that can reveal the learned functions of groups of neurons in a
neural network, which we call "functional explanations" and define as
explanations of the model behavior by a combination of functions, as opposed to
the visualization of a pattern to which a neuron responds. to our knowledge,
there is no xai method that offers functional explanations. thus, our method is
a post-hoc method that offers both instance-based and model-based functional
explanations. we applied our xai method to an mtann model to emphasize the
explainability and trustability of the mtann, so that users can trust the mtann.
in the field of image processing, supervised nonlinear filters and edge
enhancers based on an artificial neural network (ann) [10] have been
investigated for the reduction of the quantum noise in angiograms and supervised
semantic segmentation of the left ventricles in angiography [11], which are
called neural filters and neural edge enhancers, respectively. by extending the
neural filter and edge enhancer, massive-training artificial neural networks
(mtanns) have been developed to reduce false positives in the computerized
detection of lung nodules in computed tomography (ct) [12]. the mtanns have also
shown promising performance in pattern recognition and classification tasks
[13,14].an mtann is a deep learning model consisting of linear-output artificial
neural network regression model that directly operates on pixels in an input
image, as shown in fig. 1. a large number of patches are extracted from input
images; and corresponding pixels at the same positions in desired output images,
named as teaching images, are extracted for the mtann to learn. this patch-based
training leads to the fact that the mtann can be trained with only a small
number of input and teaching images.
the numbers of hidden layers and their units in an mtann model are adjustable
hyperparameters. a relatively large structure is used to ensure that the model
performs well on a specific task. a trained large model, however, may contain
redundant units, and functions of neurons for the task would be "distributed and
diluted" in many neurons in the model. this makes the analysis of the functions
of neurons very difficult [15].to address this issue, we applied our
sensitivity-based structure optimization algorithm [16] to a trained large mtann
model to "consolidate" the diluted functions of neurons in the mtann model. with
this algorithm, redundant hidden units of the model are gradually removed; and a
compact model with equivalent performance is obtained. the algorithm is
described as the following steps: train on until the loss value converges the
loss value of on other necessary evaluation metrics of on (like psnr, dice
coefficient, etc., which depend on the task) (initialize the maximum loss value
after removing a hidden unit from , and the loss value is supposed to be between
0 and 1) (initialize the index of the hidden layer where the hidden unit
belongs)(initialize the index of the hidden layer until in the -th hidden
layer)for in do (go through each hidden layer) if do (this layer has only one
unit which cannot be removed) skip to the next iteration for in do (go through
each hidden unit in the -th hidden layer) remove the -th hidden unit in the -th
hidden layer from temporarily the loss value of on if doput the -th hidden unit
in the -th hidden layer back to ← (copy current model's weights and structure)
remove the -th hidden unit in the -th hidden layer from permanently return with
the proposed optimization algorithm, the hidden units of mtann could be
gradually removed until the performance drops greatly when any of the rest unit
is deleted.
after applying the structure optimization algorithm, every hidden unit in the
compact model is expected to have an essential function for the target task. to
understand the functions of the hidden units, function maps were obtained by
performing the mtann convolution of a hidden unit over a given input image. for
better discrimination between enhancement and suppression, the function maps
were normalized and then multiplied by the sign of the weight between the hidden
units and the output unit. weighted function maps were finally generated by
shifting the range of the function map by 0.5. namely, for a given hidden unit,
in the weighted function map, a pixel value >0.5 means enhancement of patterns
in the input image, whereas a pixel value <0.5 means suppression.
to group similar functions of the hidden units of the mtann, we applied an
unsupervised hierarchical clustering algorithm [17] to the weighted functional
visualization maps. with this algorithm, the hidden units were automatically
divided into several groups based on the following distance function between the
weighted function maps of the hidden units:where ssim is the structural
similarity index, and nmrse is the normalized root mean square error. with the
unsupervised hierarchical clustering algorithm, we visualize the function maps
of the hidden units group by group to explain the behavior of each group of the
hidden units.
our xai technique was applied to explain the mtann model's decision in a liver
tumor segmentation task [20]. dynamic contrast-enhanced liver ct scans
consisting of 42 patients with 194 liver tumors in the portal venous phase from
the lits database [21] were used in this study. each slice of the ct volumes in
the dataset has a matrix size of 512 × 512 pixels, with in-plane pixel sizes of
0.60-1.00 mm and thicknesses of 0.20-0.70 mm. the dataset consists of the
original hepatic ct image with the liver mask and the "gold-standard" liver
tumor region manually segmented by a radiologist, as illustrated in fig. 2.
firstly, to have the same physical scale on spatial coordinates, bicubic
interpolation was applied on the original hepatic ct images together with the
corresponding liver mask and "gold-standard" tumor segmentation to obtain
isotropic images with a voxel size of 0.60 × 0.60 × 0.60 mm 3 . then, to unify
the image size into the same size, the isotropic image was cropped to obtain the
liver region volume of interest (voi) with an in-plane matrix size of 512 × 512.
an anisotropic diffusion filter was applied to reduce the quantum noise, which
could substantially reduce the noise while major structures such as tumors and
vessels maintained [22]. finally, a z-score normalization was applied to unify
complex histograms of tumors in different cases. the final pre-processed ct
images were used as the input images.in addition, since most liver tumors' shape
is ellipsoidal, the liver tumors can also be enhanced by the hessian-based
method and utilized in the model to improve the performance [23,24]. hence, the
model consisted of these two input channels: segmented liver ct image and its
hessian-enhanced image. also, the patches were extracted from input images from
both channels: a 5 × 5 × 5 sized patch in the same spatial position was
extracted to form a training patch with a size of 2 × 5 × 5 × 5 pixels.seven
cases and 24 cases in the dynamic contrast-enhanced ct scans dataset were used
for training and testing, respectively. 10,000 patches were randomly selected
from the liver mask region in each case, summing up to a total of 70,000
training samples for training. the number of input units in the mtann model with
one hidden layer was 250. the structure optimization process started with 80
hidden units in the hidden layer. the binary cross-entropy (bce) loss function
was used to train the model. the mtann model classified the input patches into
tumor or non-tumor classes, and the output pixels represented the probability of
being a tumor class. during the structure optimization process, the f1 score on
the training patches and the dice coefficient on the training images were also
calculated as the reference to select a suitable compact model that performed
equivalently to the original large model.as observed in the four evaluation
metric curves in fig. 3, as the number of hidden units was reduced from 80 to 9,
the performance of the model fluctuated up and down, and after it was reduced
below 9, the performance of the model dropped dramatically. therefore, we chose
a number of hidden units of 9 as the optimized structure.then, we applied the
unsupervised hierarchical clustering algorithm to the weighted function maps
from the optimized compact model with 9 hidden units. figure 4 shows that the 9
hidden units are clearly divided into 3 different groups. we denote hidden units
3, 4, and 7 as group a, hidden units 2, 6, 1, and 8 as group b, and hidden units
0 and 5 as group c. the hidden units in the same group should have a similar
function, and the function maps from each group should show the function of the
group. as illustrated in fig. 5, the low-intensity areas in the function maps of
hidden units 0 and 5 in group c match the high-intensity areas in the
hessian-enhanced input image, which means they suppress the high-intensity
areas. likewise, group a enhances the liver area, and group b suppresses the
non-tumor area. we also understood that groups a and b worked together to
enhance the tumor area, and group c suppressed the liver's boundary as well as
reduced the false enhancements inside the liver. thus, our xai method was able
to reveal the learned functions of groups of neurons in the neural network,
which we call "functional explanations" and define as the explanations of the
model behavior by a combination of functions. our method is a post-hoc method
that offers both instance-based and model-based functional explanations.
in this study, we proposed a novel xai approach to explain the functions and
behavior of an mtann model for semantic segmentation of liver tumors in ct. our
structure optimization algorithm refined the structure and made every hidden
unit in the model have a clear, meaningful function by removing redundant hidden
units and "condensing" the functions into fewer hidden units, which solved the
issue of unstable xai results with conventional xai methods. the unsupervised
hierarchical clustering algorithm in our xai approach grouped the hidden units
with a similar function into one group so as to explain their functions by
group. through the experiments, we successfully proved that the mtann model was
explainable by functions.
vision transformers (vits) are self-attention based neural networks that have
achieved state-of-the-art performance on various medical imaging tasks
[8,24,30]. since vits are capable of encoding long range dependencies between
input f. almalik and n. alkhunaizi-equal contribution sequences [16], they are
more robust against distribution shifts and are wellsuited for handling
heterogeneous distributions [5]. however, training vit models typically requires
significantly more data than traditional convolutional neural network (cnn)
models [16], which limits their application in domains such as healthcare, where
data scarcity is a challenge. one way to overcome this challenge is to train
such models in a collaborative and distributed manner, where large amounts of
data can be leveraged from different sites without the need for sharing private
data [9,11]. federated learning and split learning are two well-known approaches
for collaborative model training.federated learning (fl) enables clients to
collaboratively learn a global model by aggregating locally trained models [14].
since this can be accomplished without sharing raw data, fl mitigates risks
related to private data leakage. several aggregation rules such as fedavg [20]
and fedprox [19] have been proposed for fl. however, it has been demonstrated
that most fl algorithms are vulnerable to gradient inversion attacks [13], which
dilute their privacy guarantees. in contrast, split learning (sl) divides a deep
neural network into components with independently accessible parameters [10].
since no participant in sl can access the complete model parameters, it has been
claimed that sl offers better data confidentiality compared to fl. in
particular, the u-shaped sl configuration, where each client has its own feature
extraction head and a task-specific tail [27] can further improve client
privacy, as it circumvents the need to share the data or labels. recently, sl
frameworks have been proposed for various medical applications such as tumor
classification [3] and chext x-ray classification [23].recent studies [21,22]
have demonstrated that both fl and sl can be combined to effectively train vits.
in [22], a framework called festa was proposed for medical image classification.
the festa framework involves a hybrid vit architecture with u-shaped sl
configuration -each client has its own cnn head and a multilayer perceptron
(mlp) tail, while the shared vit body resides on a central server. this
architecture can be trained using both sl and fl in a potentially task-agnostic
fashion, leading to better performance compared to other distributed learning
methods. the work in [21] focuses on privacy and incorporates differential
privacy with mixed masked patches sent from the vit on the server to the clients
to prevent any potential data leakage.in this work, we build upon the festa
framework [22] for collaborative learning of vit. despite its success, festa
requires pretraining the vit body on a large dataset prior to its utilization in
the sl and fl training process. in the absence of pretraining, limited training
data availability (a common problem in medical imaging) leads to severe
overfitting and poor generalization. furthermore, the festa framework exploits
only the final cls token produced by the vit body and ignores all the other
intermediate features of the vit. it is well-known that intermediate features
(referred to as patch tokens) also contain discriminative information that could
be useful for the classification task [4].to overcome the above limitations, we
propose a framework called federated split learning of vision transformer with
block sampling (fesvibs). our primary novelty is the introduction of a block
sampling module, which randomly selects an intermediate transformer block for
each client in each training round, extracts intermediate features, and distills
these features into a pseudo cls token using a shared projection network. the
proposed approach has two key benefits: (i) it effectively leverages
intermediate vit features, which are completely ignored in festa, and (ii)
sampling these intermediate features from different blocks, rather than relying
solely on an individual block's features or the final cls token, serves as a
feature augmentation strategy for the network, enhancing its generalization. the
contributions of this work can be summarized as follows: i. we propose the
fesvibs framework, a novel federated and split learning framework that leverages
the features learned by intermediate vit blocks to enhance the performance of
the collaborative system. ii. we introduce block sampling at the server level,
which acts as a feature augmentation strategy for better generalization.
we first describe the working of a typical split vision transformer before
proceeding to describe fesvibs., where n c is the number of training samples
available at client c, x represents the input data, and y is the class label.
following [22], we assume u-shaped split learning setting, with each client
having two local networks called head (h θc ) and tail (t ψc ), where θ c and ψ
c are client-specific head and tail parameters, respectively. the server
consists of a vit body (b φ ), which includes a stack of l transformer blocks
denoted ashere, φ l represents the parameters of the l th transformer block
anddenotes the complete set of parameters of the transformer body.during
training, the client performs a forward pass of the input data through the head
to produce an embedding h c = h θc (x c ) ∈ r 768×m of its local data, which is
typically organized as m patch tokens representing different patches of the
input image. these embeddings (smashed representations) are then sent to the
server. the vit appends an additional token called the class token (cls ∈ r
768×1 ) and utilizes the self-attention mechanism to obtain a representation b c
= b φ (h c ) ∈ r 768×1 , which is typically the cls token resulting from the
last transformer block. this cls token is returned to the client for further
processing. the tail at each client projects the received class token
representation b c into a class probability distribution to get the final
prediction ŷc = t ψc (b c ). this marks the end of the forward pass.
subsequently, the backpropagation starts with computing loss c (y c , ŷc ),
where c (.) represents the client's loss function between the true labels y c
and predicted labels ŷc . the gradient of this loss is propagated back in the
reverse order from the client's tail, server's body, to the client's head. we
refer to this setting as split learning of vision transformer (slvit), where
each client optimizes the following objective in each round:in festa [22], an
additional federation step was introduced. after every few sl rounds, the local
(client-specific) heads and tails are aggregated in a unifying round using
fedavg [20] to produce global parameters θ and ψ. note that the above framework
completely ignores all the intermediate features (patch tokens) extracted from
various vit blocks. in [4], it was demonstrated that these patch tokens are also
discriminative and valuable for classification tasks. hence, we aim to exploit
these intermediate features to further enhance the performance.
the proposed fesvibs method is illustrated in fig. 1 and detailed in algorithm
1. the working of the fesvibs framework is very similar to festa, except for one
key difference. during the forward pass of slvit and festa, the server always
returns the cls token from the last vit block. in contrast, a fesvibs server
samples an intermediate block l ∈ {1, 2. . . . , l} for each client c in each
round and extracts the intermediate features z c,l from the chosen l th block as
follows:where z c,l ∈ r 768×m . the server then projects the extracted
intermediate features into a lower dimension using a projection network r
(shared across all blocks) to obtain the final representation b c,l = r π (z c,l
), where b c,l ∈ r 768×1 . this final representation b c,l can be considered as
a pseudo class token and the role of the projection network is to distill the
discriminative information contained in the intermediate features into this
pseudo class token. the primary motivation for block sampling is to effectively
leverage intermediate vit features that are better at capturing local texture
information (but are lost when only the final cls token is used). stochasticity
in the block selection serves as a feature augmentation strategy, thereby aiding
the generalization performance.the architecture of the projection network is
shown in fig. 1 and it resembles a simple resnet [12] block with skip
connection. the pseudo class token is then sent to the client's tail to obtain
the final prediction ŷc = t ψc (b c,l ) and complete the forward pass. each
client uses ŷc along with the true labels y c to compute the loss c (y c , ŷc ).
the gradients of the client's tail are then calculated and sent back to the
server, which then carries out the back-propagation through the projection
network and relevant blocks of the vit body (only those blocks involved in the
corresponding forward pass). next, the server sends the gradients back to the
client to propagate them through the head and end the back-propagation step.
hence, the client's optimization problem is:in the fesvibs framework, the heads
and tails of all the clients are assumed to have the same network architecture.
within each collaboration round, all the clients perform the forward and
backward passes. while the parameters of the relevant head and tail as well as
the shared projection network are updated after every backward pass, the
parameters of the vit body are updated only at the end of a collaboration round
after aggregating updates from all the clients. the above protocol until this
step is referred to as svibs, because there is still no federation of the heads
and tails. similar to festa, we also perform aggregation of the local heads and
tails periodically in unifying rounds, resulting in the final fesvibs framework.
while in svibs, the clients can initialize their heads and tails independently,
fesvibs requires a common initialization by the server and sharing of aggregated
head and tail parameters after a unifying round.
datasets. we conduct our experiments on three medical imaging datasets. the
first dataset is ham10000 [26], a multi-class dataset comprising of 10, 015
dermoscopic images from diverse populations. ham10000 includes (end if 21: end
for categories of pigmented lesions; we randomly perform 80%/20% split for
training and testing, respectively. the second dataset [2] termed "bloodmnist"
is a multi-class dataset consisting of 17, 092 blood cell images for 8 different
imbalanced cell types. we followed [29] and split the dataset into 70% training,
10% validation, and 20% testing. finally, the fed-isic2019 dataset consists of
23, 247 dermoscopy images for 8 different melanoma classes. this dataset was
prepared by flamby [25] from the original isic2019 dataset [6,7,26] and the data
was collected from 6 centers, with significant differences in population
characteristics and acquisition systems, representing real-world domain shifts.
we use 80%/20% split for training and testing, respectively. the training
samples in all datasets are divided among 6 clients, whereas the testing set is
shared among them all. the distribution of each dataset is depicted in fig. 2.
note that fed-isic2019 and bloodmnist are non-iid, whereas ham10000 is
iid.server's network. for the server's body, we chose the vit-b/16 model from
timm library [28] which includes l = 12 transformer blocks, embedding dimension
d = 768, 12 attention heads, and divides the input image into patches each of
size 16 × 16 with m = 196 patches. we limit the block sampling to the first 6
vit blocks. additionally, the projection network has two convolution layers with
a skip connection, which takes an input of dimension 768 × 196 and projects it
into a lower dimension of 768. clients' networks. each client has two main
networks: head and tail. we followed timm library's implementation of hybrid
vits (h-vit) to design each client's head, which is a resnet-50 [12] with a
convolution layer added to project the features extracted by resnet-50 to a
dimension of 768 × 196. the tail is a linear classifier. also, we unify the
clients' networks (head and tail) every 2 rounds using fedavg. we conduct our
experiments for 200 rounds with adam optimizer [17], a learning rate of 1 × 10
-4 , and 32 batch size with a cross-entropy loss calculated at the tail. the
code was implemented using pytorch 1.10 and the models were trained using nvidia
a100 gpu with 40 gb memory.
following [25], we used balanced accuracy in all experiments to evaluate the
performance of the classification task across all datasets. this metric defines
as the average recall on each class. in table 1, we compare the performance of
fes-vibs and svibs frameworks with other sota methods. fesvibs consistently
outperforms other methods on the three datasets with both iid and non-iid
settings. more specifically, for ham10000 (iid), fesvibs outperforms all other
methods with a 4.4% gain in performance over festa and approximately 11% over
fedavg and fedprox (μ = 0.006). in the non-iid settings with both blood-mnist
and fed-isic2019, fesvibs maintains a high performance compared to other
methods. under extreme non-iid settings (fed-isic2019), our approach
demonstrated a performance improvement of 10.4% compared to festa and 5.8% over
fedavg and fedprox, demonstrating the robustness of fesvibs. we investigate the
impact of sampling intermediate blocks in svibs, by analysing the individual
performance of intermediate features from specific blocks during training. the
results in fig. 3 demonstrate that the majority of individual blocks outperform
the vanilla split learning setting (slvit), which is dependent on the cls token.
on the other hand, svibs shows dominant performance across datasets, where the
sampling of vit blocks provides augmented representations of the input images at
different rounds and improves the generalizability. from table 1, we also
observe that the variance of the accuracy achieved by fesvibs due to stochastic
block sampling during inference is very low.
set of vit blocks. to study the impact of vit blocks from which the intermediate
features are sampled on the overall performance of fesvibs, we carry out
experiments choosing different sets of blocks. the results depicted in fig. 4
(left) show consistent performance for different sets of blocks across different
datasets. this indicates that implementing fesvibs with the first 6 vit blocks
would reduce the computational cost without compromising performance.fesvibs
with differential privacy. differential privacy (dp) [1] is a widelyused
approach that aims to improve the privacy of local client's data by adding
noise. we conduct experiments where we add gaussian noise to the client's head
output (h c ). in such a scenario, dp makes it more challenging for a
malicious/curious server to infer the client's input from the smashed
representations. with different values, the results in fig. 4 (right) show that
fesvibs maintains its performance even under a small value ( = 0.1), while also
outperforming festa under the same constraints.number of unifying rounds. we
investigated the impact of reducing communication rounds (unifying rounds) on
fesvibs performance. however, our results showed that performance was maintained
even with decreasing the number of communication rounds.computational and
communication overhead. except for moon and scaffold, all methods in table 1
share the same h-vit architecture, resulting in similar computational costs.
svibs and fesvibs require training an additional projection network but avoid
needing a complete vit forward/backward pass. centralized and local training
methods have no communication cost. for other methods, the communication cost
per client per collaboration round: (i) fedavg/fedprox: ∼ 97m, (ii) slvit/svibs:
∼ 197m values for ham10000 dataset, and (iii) festa/fesvibs: ∼ 197m values +12m
parameters per client per unifying round. thus, the proposed method has a
marginally higher communication overload than sl and twice the communication
burden as fl.
we proposed a novel federated split learning of vision transformer with block
sampling (fesvibs), which utilizes fl, sl and sampling of vit blocks to enhance
the performance of the collaborative system. we evaluate fesvibs framework under
iid and non-iid settings on three real-world medical imaging datasets and
demonstrate consistent performance. in the future, we aim to (i) extend our work
and evaluate the privacy of fesvibs under the presence of malicious
clients/server, (ii) evaluate fesvibs in the context of natural images and (iii)
extend the current framework to multi-task settings.
liver cancer is one of the most deadly cancers and has the second highest
fatality rate [17]. focal liver lesions (flls) are the most common lesions found
in liver cancer, yet flls are challenging to diagnose because they can be either
benign lesions, such as focal nodular hyperplasia (fnh), hepatic abscess (ha),
hepatic hemangioma (hh), and hepatic cyst (hc) or malignant tumors, such as
intrahepatic cholangiocarcinoma (icc), hepatic metastases (hm), and
hepatocellular carcinoma (hcc). accurate early diagnosis of flls is thus
critical to increasing the 5-year survival rate, a task that remains challenging
as of today. dynamic contrast-enhanced ct is a common technique for liver cancer
diagnosis, where four different phases of imaging, namely, non-contrast (nc),
arterial (art), portal venous (pv), and delayed (dl) provide complementary
information about the liver. different types of flls acquired in the four phases
are shown in fig. 1. with the development of deep learning, computer-aided liver
lesion diagnosis has attracted much attention [5,8,16] in recent years. romero
et al. [16] presented an end-to-end framework based on inception-v3 and
inceptionresnet-v2 to discriminate liver lesions between cysts and malignant
tumors. heker et al. [8] combined liver segmentation and classification using
transfer learning and joint learning to increase the performance of cnn. as a
manner to elevate the accuracy of cnns, frid-adar et al. [5] designed a
gan-based network to generate synthetic liver lesion images, improving the
classification performance based on cnn. it is reported in many studies [9,18]
that using multi-phase data, like most professionals do in practice, can help
the network get a more accurate result, which also acts in liver lesion
classification [15,23,24]. yasaka et al. [24] proposed multi-channel cnn to
extract features from multi-phase liver ct by concatenation. roboh et al. [15]
proposed an algorithm based on cnns to handle 3d context in liver cts and
utilized clinical context to assist the classification. xu et al. [23]
constructed a knowledge-guided framework to integrate liver lesion features from
three phases using self-attention and fused them with a cross-feature
interaction module and a cross-lesion correlation module.a single-phase lesion
annotation means the annotation of both lesion position and its class. in
hospitals, collected multi-phase cts are normally grouped by patients rather
than lesions, which makes single-phase lesion annotation insufficient for
feature fusion learning. however, the number of lesions inside a single patient
can vary from one to dozens and they can be of different types in realistic
cases. multi-phase cts are also not co-registered in most cases, therefore, it
is necessary to make sure the lesions extracted from different phases are
somehow aligned for feature fusion, which is called as multi-phase lesion
annotation. moreover, while most works have attached much importance to liver
lesion segmentation [2], its outcome is usually organized at a single-phase
level. additional effort will be needed when consolidating segmentation and
multi-phase classification.self-attention based transformers [19] have shown
strong capability in natural language processing tasks. meanwhile, vision
transformers (vit) [4] have been shown to replace cnn with a transformer encoder
in computer vision tasks and can achieve obvious advantages on large-scale
datasets. to the best of our knowledge, we find no study using vit backbone
network in liver lesion classification. the reason for this is twofold. first,
pure vit has several limitations itself [6], including ignoring local
information within each patch, extracting only single-scale features, and
lacking inductive bias. second, no complete open liver lesion classification
datasets exist. most relevant studies are based on private datasets, which tend
to be small in size and cause overfitting in learning models.in this paper, we
construct a hybrid framework with vit backbone for liver lesion classification,
transliver. we design a pre-processing unit to reduce the annotation cost, where
we obtain lesion area on multi-phase cts from annotations marked on a single
phase. to alleviate the limitations of pure transformers, we propose a
multi-stage pyramid structure and add convolutional layers to the original
transformer encoder. we use additional cross phase tokens at the last stage to
complete a multi-phase fusion, which can focus on cross-phase communication and
improve the fusion effectiveness as compared with conventional modes. while most
multi-phase liver lesion classification studies use datasets with no more than
three phases (without dl phase for its difficulty of collection) or no more than
six lesion classes, we validate the whole framework on an in-house dataset with
four phases of abdominal ct and seven classes of liver lesions. considering the
disproportion of axial lesion slice number and the relatively small scale of the
dataset, we adopt a 2-d network in classification part instead of 3-d in
pre-processing part and achieve a 90.9% accuracy.
figure 2 illustrates the overall architecture of transliver, where activation
layers and batch normalization layers are omitted. multi-phase liver lesion cts
are converted from single-phase annotation to multi-phase annotation by a
preprocessing unit including a registration network and a lesion matcher.for
each phase, a convolutional encoder extracts preliminary lesion features on
axial slices. as the backbone of the whole framework, transformer encoder
employs a 4-stage pyramid structure extracting multi-scale features, with each
stage connected by a convolutional down-sampler. there are two types of
transformer blocks, single-phase liver transformer block (spltb) and multi-phase
liver transformer block (mpltb). the former is phase-specific, while the latter
is in charge of multi-phase fusion. extracted features from different phases are
averaged and classified by two successive fully connected networks. a voting
strategy about slices is applied to decide the classification results.
the single-phase annotated lesion has the position and class labels in all
phases but they are not aligned, so we could have difficulty finding out which
lesions in different phases are the same with 2 or more lesions in one patient.
to reduce errors caused by unregistered data and address the situation that one
patient has multiple lesions of different types, we pre-process the multi-phase
liver cts registered and grouped by lesions.the registration network is based on
voxelmorph [1], with a u-net learning registration field and moving data
transformed by the field. we also use auxiliary dice loss function between fixed
image lesion masks and moved image lesion masks to help the registration field
learning. in [1], the network needs to specify an atlas image, otherwise, pairs
of images will be registered to each other. but in our work, we need to register
the original data in a cross-phase form. we choose an atlas phase art as
suggested by clinicians and other phases of cts are registered to the art phase
of every patient.after registration, a lesion matcher finds the same lesions in
different phases. we generate a minimum circumscribed cuboid with padding as the
lesion window for each lesion to keep the surrounding information. the windows
are then converted to 0-1 masks to calculate dice coefficient. lesions with the
maximal window dice coefficient that is no less than a set threshold are
considered the same. only lesions completely found in all phases will be used in
the following classification network.
in pure vision transformer, input images are converted to tokens by patch
embedding and added with positional encoding to keep the positional information.
patch embedding consists of a linear connected layer or a convolutional layer,
which does not enable to construct local relation [13]. absolute positional
encoding destroys the translation variance [10] that keeps the rotation and
shift operations from altering the final results [6].so, we construct a
convolutional encoder without absolute positional encoding to replace the
original embedding layer. for an input image x ∈ r b×h×w ×1 , b is the batch
size, and h × w is the size of the input. the module contains four convolutional
layers playing different roles. the first layer, conv 1 , with a kernel size of
3, stride of 2, and output channels of 32, reduces the size to h 2 × w 2 . next
two layers, conv 2 and conv 3 , each with a kernel size of 3, stride of 1, and
the same output channel as conv 1 , extract local information. conv 1 , conv 2 ,
and conv 3 each is followed by a gelu activation layer and a batch
normalization. considering the design of pvtv2 [21], an overlapped convolutional
layer, conv 4 , with a kernel size of 7, stride of 2, and output channels of 64,
is used to strengthen the connection among patches. it is followed by layer
normalization.to finish the tokenization of transformer.we add convolutional
down-samplers between stages of transformer encoder so that they can produce
hierarchical representation like cnn structure. each convolutional down-sampler
contains a residual structure with a 3×3 depthwise convolution to increase the
locality of our model. we also utilize a convolutional layer with a kernel size
of 2 and stride of 2, which halves the image resolution and doubles the number
of channels.
vision transformers can get excellent performance on large-scale datasets such
as imagenet [4], but they are also prone to overfit on small datasets such as
private hospital datasets. we adopt the spatial reduction structure proposed in
pvt [20] to largely reduce the computational overhead by reducing the size of k
and v using depthwise convolution. following [6,12], a learnable relative
positional encoding is added here to replace the absolute positional encoding.
the self-attention module can be written as:where q, k, v are the same with
original vit, d h is the head dimension, and p is the relative positional
encoding. spatial reduction sr consists of a k × k depthwise convolution with a
stride of k and a batch normalization, where k is the spatial reduction ratio
set in each stage. feed forward network (ffn) is designed for a better capacity
of representation. we use the module designed in [6] irffn (inverted residual
ffn) with three convolutions instead of two linear layers in the initial vision
transformer. the first and third convolutions are pointwise for dimension
translation, which has a similar effect to the original linear layers. the
second convolution with a shortcut connection extracts local information in a
higher dimension and improves the gradient propagation ability across layers
[6]. the structure also has two gelu activation layers between convolutional
layers and three batch normalizations after the gelus and the last convolutional
layer for better performance.
single-phase liver transformer block (spltb) is phase-specific, which means the
model parameters of each phase are independent. it can fully extract features in
different phases before fusion. inspired by [14], in stage 4, we design a
multi-phase liver transformer block (mpltb) for communication between phases.
mpltb introduced some new parameters that are not in the original transformer.
these parameters are randomly initialized, concatenated with the corresponding
phase tokens respectively, and updated in phase-specific spltb. then, they are
separated and averaged for the next layer. the whole module is defined as:where
x l i is phase tokens of the ith phase and the lth layer and t l is cross phase
tokens of the lth layer. because of the phase-specific spltb, t l i represents
the corresponding cross phase tokens output of the ith phase in the lth layer.
cross phase tokens need negligible extra cost and can force the information to
concentrate inside the tokens [14]. compared to the direct fusion of input
images or output features like average and concatenation, cross phase tokens can
also reduce fusion granularity to sufficiently explore the relationship among
phases. it is worth noting that these tokens will be removed provisionally when
reshaping the phase tokens in spltb for right execution. the fusion is conducted
in deep layers because the semantic concepts are learned in higher layers which
benefits the cross phase connection.
dataset. the employed single-phase annotated dataset is collected from sir run
run shaw hospital (srrsh), affiliated with the zhejiang university school of
medicine, and has received the ethics approval of irb. the collection process
can be found in supplementary materials. the size of each ct slice is decreased
to 224×224 using cubic interpolation. after the pre-processing unit with window
dice threshold of 0.3, we screen 761 lesions from 444 patients with four phases
of cts, seven types of lesions (13.2% of hcc, 5.3% of hm, 11.3% of icc, 22.6% of
hh, 31.1% of hc, 8.7% of fnh, and 7.8% of ha), and totally 4820 slices. to
handle the imbalance of dataset, we randomly select 586 lesions as the training
and validation set with no more than 700 axial slices in each lesion type, and
the rest 175 lesions constitute the test set. lesions from the same patient are
either assigned to the training and validation set or the test set, but not
both.implementations. the training and validation set is randomly divided with a
4:1 ratio. the data is augmented by flip, rotation, crop, shift, and scale. we
initialize the backbone network using pre-trained weights of cmt-s [6]. our
models are implemented by pytorch1.12.1 and timm0.6.13 [22]. then, they are
trained on four nvidia tesla a100 gpus for 200 epochs using cross-entropy loss
function with label smoothing and sgd optimizer with learning rate warmup and
cosine annealing. the batch size is 32 and the learning rate is 1e-3. we
measured performance by precision (pre.), sensitivity (sen.), specificity
(spe.), f1-score (f1), area under the curve (auc), and accuracy (acc.).results.
we first compare the class-wise accuracy of our model against other advanced
methods applying different architectures in multi-phase liver lesion
classification with more than four lesion types [3,11,15,23,24]. transliver gets
the highest overall accuracy of 90.9% classifying the most lesion types of seven
(hcc 90.9%, hm 62.5%, icc 73.7%, hh 91.7%, hc 100.0%, fnh 100.0%, and ha 93.3%).
in the results of our method, hm has a relatively low performance of 62.5%,
mainly due to its low proportion in our dataset. the details can be found in
supplementary materials.because the sources of data are different among the
methods compared above and to the best of our knowledge, no relevant study based
on transformers was found, we further train some sota normal classification
models on our dataset. considering the fairness, all the models below are
initialized with pre-trained weights and adopt 2-d structures using the same
slice-level classification strategy. for completeness, we concatenate the
multi-phase features to execute the fusion. as illustrated in table 1, our
proposed transliver model gets better performance than other models in all
metrics. behind our model, cmt-s achieves the best performance, indicating the
effect of convolutional structures in transformer.
to verify the improvement of our modules, we conduct three baseline experiments
for comparison. here convolutional encoder, convolutional down-sampler, and
spltb as a whole is called c-spltb. baseline 0 does not use c-spltb or cross
phase tokens in mpltb but replaces them with pure vision transformer and output
feature concatenation respectively. baseline 1 adds the c-spltb and baseline 2
adds the cross phase tokens. a 3-d version of baseline 2 utilizing 3-d patch
embedding is also studied in baseline 3 to validate the advantage of our 2-d
model. the result shown in fig. 3 demonstrates that our design choice is
appropriate. it is worth mentioning that the 2-d structure is prone to
redundancy between axial slices and ignores the relation between slices compared
with the 3-d structure but gets observably higher accuracy. we suppose the
reason is twofold. most of lesions in our dataset having few slices weakens the
redundancy between slices in 2-d pipeline, while the number of slices is still
obviously larger than the number of lesions, alleviating the overfitting issue.
furthermore, vision transformers are mostly pretrained in 2-d images, causing
poor performance when transferring to 3-d pipeline.we also evaluate the model
performance under different phase combinations by cutting the branch of certain
phases. it shows that information from various phases can significantly
influence the classification performance. a missing phase can cause an accuracy
drop of about 10% and complete four-phase model outperforms single-phase model
by nearly 20%. figure 4 contains average results of phase number and details
with all phase combinations can be found in supplementary materials.
we have presented a hybrid architecture for multi-phase liver lesion
classification in this paper. the lesion features are extracted by transformer
backbone with several auxiliary convolutional modules. then, we fuse the
features from different phases through cross phase tokens to enhance their
information exchange.to handle the issues in realistic cases, we design a
pre-processing unit to acquire multi-phase annotated lesions from single-phase
annotated ones. we report performance of an overall 90.9% classification
accuracy on a four-phase seven-class dataset through quantitative experiments
and show obvious improvement compared with sota classification methods. in
future work, we will extend classification to instance segmentation and provide
an end-to-end effective model for liver lesion diagnosis.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43895-0 31.
semantic segmentation on histological whole slide images (wsis) allows precise
detection of tumor boundaries, thereby facilitating the assessment of metastases
[3] and other related analytical procedures [17]. however, pixel-level
annotations of gigapixel-sized wsis (e.g. 100, 000 × 100, 000 pixels) for
training a segmentation model are difficult to acquire. for instance, in the
camelyon16 breast cancer metastases dataset [10], 49.5% of wsis contain
metastases that are smaller than 1% of the tissue, requiring a high level of
expertise and long inspection time to ensure exhaustive tumor localization;
whereas other wsis have large tumor lesions and require a substantial amount of
annotation time for boundary delineation [18]. identifying potentially
informative image regions (i.e., providing useful information for model
training) allows requesting the minimum amount of annotations for model
optimization, and a decrease in annotated area reduces both localization and
delineation workloads. the challenge is to effectively select annotation regions
in order to achieve full annotation performance with the least annotated area,
resulting in high sampling efficiency.we use region-based active learning (al)
[13] to progressively identify annotation regions, based on iteratively updated
segmentation models. each region selection process consists of two steps. first,
the prediction of the most recently trained segmentation model is converted to a
priority map that reflects informativeness of each pixel. existing studies on
wsis made extensive use of informativeness measures that quantify model
uncertainty (e.g., least confidence [8], maximum entropy [5] and highest
disagreement between a set of models [19]). the enhancement of priority maps,
such as highlighting easy-to-label pixels [13], edge pixels [6] or pixels with a
low estimated segmentation quality [2], is also a popular area of research.
second, on the priority map, regions are selected according to a region
selection method. prior works have rarely looked into region selection methods;
the majority followed the standard approach [13] where a sliding window divides
the priority map into fixed-sized square regions, the selection priority of each
region is calculated as the cumulative informativeness of its constituent
pixels, and a number of regions with the highest priorities are then selected.
in some other works, only non-overlapping or sparsely overlapped regions were
considered to be candidates [8,19]. following that, some works used additional
criteria to filter the selected regions, such as finding a representative subset
[5,19]. all of these works selected square regions of a manually predefined
size, disregarding the actual shape and size of informative areas.this work
focuses on region selection methods, a topic that has been largely neglected in
literature until now, but which we show to have a great impact on al sampling
efficiency (i.e., the annotated area required to reach the full annotation
performance). we discover that the sampling efficiency of the aforementioned
standard method decreases as the al step size (i.e., the annotated area at each
al cycle, determined by the multiplication of the region size and the number of
selected regions per wsi) increases. to avoid extensive al step size tuning, we
propose an adaptive region selection method with reduced reliance on this al
hyperparameter. specifically, our method dynamically determines an annotation
region by first identifying an informative area with connected component
detection and then detecting its bounding box. we test our method using a breast
cancer metastases segmentation task on the public camelyon16 dataset and
demonstrate that determining the selected regions individually provides greater
flexibility and efficiency than selecting regions with a uniform predefined
shape and size, given the variability in histological tissue structures. results
show that our method consistently outperforms the standard method by providing a
higher sampling efficiency, while also being more robust to al step size
choices. additionally, our method is especially beneficial for settings where a
large al step size is desirable due to annotator availability or computational
restrictions.
we are given an unlabeled pool u = {x 1 . . . x n }, where x i ∈ r wi×hi denotes
the i th wsi with width w i and height h i . initially, x i has no annotation;
regions are iteratively selected from it and annotated across al cycles. we
denote the j th annotated rectangular region in, where (c ij x , c ij y ) are
the center coordinates of the region and w ij , h ij are the width and height of
that region, respectively. in the standard region selection method, where
fixedsize square regions are selected, w ij = h ij = l, ∀i, j, where l is
predefined.figure 1 illustrates the workflow of region-based al for wsi
annotation. the goal is to iteratively select and annotate potentially
informative regions from wsis in u to enrich the labeled set l in order to
effectively update the model g. to begin, k regions (each containing at least
10% of tissue) per wsi are randomly selected and annotated to generate the
initial labeled set l. the model g is then trained on l and predicts on u to
select k new regions from each wsi for the new round of annotation. the newly
annotated regions are added to l for retraining g in the next al cycle. the
train-select-annotate process is repeated until a certain performance of g or
annotation budget is reached.the selection of k new regions from x i is
performed in two steps based on the model prediction p i = g(x i ). first, p i
is converted to a priority map m i using a per-pixel informativeness measure.
second, k regions are selected based on m i using a region selection method. the
informativeness measure is not the focus of this study, we therefore adopt the
most commonly used one that quantifies model uncertainty (details in sect. 3.2).
next we describe the four region selection methods evaluated in this work.
random. this is the baseline method where k regions of size l × l are randomly
selected. each region contains at least 10% of tissue and does not overlap with
other regions. standard [13] m i is divided into overlapping regions of a fixed
size l × l using a sliding window with a stride of 1 pixel. the selection
priority of each region is calculated as the summed priority of the constituent
pixels, and k regions with the highest priorities are then selected. non-maximum
suppression is used to avoid selecting overlapping regions. standard
(non-square) we implement a generalized version of the standard method that
allows non-square region selections by including multiple region candidates
centered at each pixel with various aspect ratios. to save computation and
prevent extreme shapes, such as those with a width or height of only a few
pixels, we specify a set of candidates as depicted in fig. 2. specifically, we
define a variable region width w as spanning from 1 2 l to l with a stride of
256 pixels and determine the corresponding region height as h = l 2 w . adaptive
(proposed) our method allows for selecting regions with variable aspect ratios
and sizes to accommodate histological tissue variability. the k regions are
selected sequentially; when selecting the j th region r ij in x i , we first set
the priorities of all pixels in previously selected regions (if any) to zero. we
then find the highest priority pixel (c ij x , c ij y ) on m i ; a median filter
with a kernel size of 3 is applied beforehand to remove outliers. afterwards, we
create a mask on m i with an intensity threshold of τ th percentile of
intensities in m i , detect the connected component containing (c ij x , c ij y
), and select its bounding box. as depicted in fig. 3, τ is determined by
performing a bisection search over [98, 100] th percentiles, such that the
bounding box size is in range. this size range is chosen to be comparable to the
other three methods, which select regions of size l 2 . note that standard
(non-square) can be understood as an ablation study of the proposed method
adaptive to examine the effect of variable region shape by maintaining constant
region size.
this section describes the breast cancer metastases segmentation task we use for
evaluating the al region selection methods. the task is performed with
patch-wise classification, where the wsi is partitioned into patches, each patch
is classified as to whether it contains metastases, and the results are
assembled.training. the patch classification model h(x, w) : r d×d -→ [0, 1]
takes as input a patch x and outputs the probability p(y = 1|x, w) of containing
metastases, where w denotes model parameters. patches are extracted from the
annotated regions at 40× magnification (0.25 µm px ) with d = 256 pixels.
following [11], a patch is labeled as positive if the center 128 × 128 pixels
area contains at least one metastasis pixel and negative otherwise. in each
training epoch, 20 patches per wsi are extracted at random positions within the
annotated area; for wsis containing annotated metastases, positive and negative
patches are extracted with equal probability. a patch with less than 1% tissue
content is discarded. data augmentation includes random flip, random rotation,
and stain augmentation [12]. inference. x i is divided into a grid of uniformly
spaced patches (40× magnification, d = 256 pixels) with a stride s. the patches
are predicted using the trained patch classification model and the results are
stitched to a probability map p i ∈ [0, 1] w i ×h i , where each pixel
represents a patch prediction. the patch extraction stride s determines the size
of p i (w i = wi s , h i = hi s ).
we used the publicly available camelyon16 challenge dataset [10]
training schedules. we use mobilenet v2 [15] initialized with imagenet [14]
weights as the backbone of the patch classification model. it is extended with
two fully-connected layers with sizes of 512 and 2, followed by a softmax
activation layer. the model is trained for up to 500 epochs using cross-entropy
loss and the adam optimizer [7], and is stopped early if the validation loss
stagnates for 100 consecutive epochs. model selection is guided by the lowest
validation loss. the learning rate is scheduled by the one cycle policy [16]
with a maximum of 0.0005. the batch size is 32. we used fastai v1 [4] for model
training and testing. the running time of one al cycle (select-train-test) on a
single nvidia geforce rtx3080 gpu (10gb) is around 7 h.active learning setups.
since the camelyon16 dataset is fully annotated, we perform al by assuming all
wsis are unannotated and revealing the annotation of a region only after it is
selected during the al procedure. we divide the wsis in u randomly into five
stratified subsets of equal size and use them sequentially. in particular,
regions are selected from wsis in the first subset at the first al cycle, from
wsis in the second subset at the second al cycle, and so on. this is done
because wsi inference is computationally expensive due to the large patch
amount, reducing the number of predicted wsis to one fifth helps to speed up al
cycles. we use an informativeness measure that prioritizes pixels with a
predicted probability close to 0.5 (i.e., m i = 1-2|p i -0.5|), following [9].
we annotate validation wsis in the same way as the training wsis via
al.evaluations. we use the camelyon16 challenge metric free response operating
characteristic (froc) score [1] to validate the segmentation framework.to
evaluate the wsi segmentation performance directly, we use mean intersection
over union (miou). for comparison, we follow [3] to use a threshold of 0.5 to
generate the binary segmentation map and report miou (tumor), which is the
average miou over the 48 test wsis with metastases. we evaluate the model
trained at each al cycle to track performance change across the al procedure.
full annotation performance. to validate our segmentation framework, we first
train on the fully-annotated data (average performance of five repetitions
reported). with a patch extraction stride s = 256 pixels, our framework yields
an froc score of 0.760 that is equivalent to the challenge top 2, and an miou
(tumor) of 0.749, which is higher than the most comparable method in [3] that
achieved 0.741 with s = 128 pixels. with our framework, reducing s to 128 pixels
improves both metastases identification and segmentation (froc score: 0.779,
miou (tumor): 0.758). however, halving s results in a 4-fold increase in
inference time. this makes an al experiment, which involves multiple rounds of
wsi inference, extremely costly. therefore, we use s = 256 pixels for all
following al experiments to compromise between performance and computation
costs. because wsis without metastases do not require pixel-level annotation, we
exclude the 159 training and validation wsis without metastases from all
following al experiments. this reduction leads to a slight decrease of full
annotation performance (miou (tumor) from 0.749 to 0.722). all experiments
(except for random) use uncertainty sampling. when using region selection method
standard, the sampling efficiency advantage of uncertainty sampling over random
sampling decreases as al step size increases. a small al step size minimizes the
annotated tissue area for a certain high level of model performance, such as an
miou (tumor) of 0.7, yet requires a large number of al cycles to achieve full
annotation performance (fig. 4 (a-d)), table 1. annotated tissue area (%)
required to achieve full annotation performance. the symbol "/" indicates that
the full annotation performance is not achieved in the corresponding
experimental setting in fig. 4. resulting in high computation costs. a large al
step size allows for full annotation performance to be achieved in a small
number of al cycles, but at the expense of rapidly expanding the annotated
tissue area (fig. 4(e), (f), (h) and (i)). enabling selected regions to have
variable aspect ratios does not substantially improve the sampling efficiency,
with standard (non-square) outperforming standard only when the al step size is
excessively large (fig. 4(i)). however, allowing regions to be of variable size
consistently improves sampling efficiency. table 1 shows that adaptive achieves
full annotation performance with fewer al cycles than standard for small al step
sizes and less annotated tissue area for large al step sizes. as a result, when
region selection method adaptive is used, uncertainty sampling consistently
outperforms random sampling. furthermore, fig. 4(e-i)) shows that adaptive
effectively prevents the rapid expansion of annotated tissue area as al step
size increases, demonstrating greater robustness to al step size choices than
standard. this is advantageous because extensive al step size tuning to balance
the annotation and computation costs can be avoided. this behavior can also be
desirable in cases where frequent interaction with annotators is not possible or
to reduce computation costs, because the proposed method is more tolerant to a
large al step size. we note in fig. 4(h) that the full annotation performance is
not achieved with adaptive within 15 al cycles; in fig. s1 in the supplementary
materials we show that allowing for oversampling of previously selected regions
can be a solution to this problem. additionally, we visualize examples of
selected regions in fig. 5 and show that adaptive avoids two region selection
issues of standard : small, isolated informative areas are missed, and
irrelevant pixels are selected due to the region shape and size restrictions.
we presented a new al region selection method to select annotation regions on
wsis. in contrast to the standard method that selects regions with predetermined
shape and size, our method takes into account the intrinsic variability of
histological tissue and dynamically determines the shape and size for each
selected region. experiments showed that it outperforms the standard method in
terms of both sampling efficiency and the robustness to al hyperparameters.
although the uncertainty map was used to demonstrate the efficacy of our
approach, it can be seamlessly applied to any priority maps. a limitation of
this study is that the annotation cost is estimated only based on the annotated
area, while annotation effort may vary when annotating regions of equal size.
future work will involve the development of a wsi dataset with comprehensive
documentation of annotation time to evaluate the proposed method and an
investigation of potential combination with self-supervised learning.
segmentation is among the most common medical image analysis tasks and is
critical to a wide variety of clinical applications. to date, data-driven deep
learning (dl) methods have shown prominent segmentation performance when trained
on fully-annotated datasets [8]. however, data annotation is a significant
bottleneck for dataset creation. first, annotation process is tedious, laborious
and time-consuming, especially for 3d medical images where dense annotation with
voxel-level accuracy is required. second, medical images typically need to be
annotated by medical experts whose time is limited and expensive, making the
annotations even more difficult and costly to obtain. active learning (al) is a
promising solution to improve annotation efficiency by iteratively selecting the
most important data to annotate with the goal of reducing the total number of
annotated samples required. however, most deep al methods require an initial set
of labeled samples to start the active selection. when the entire data pool is
unlabeled, which samples should one select as the initial set? this problem is
known as cold-start active learning , a low-budget paradigm of al that permits
only one chance to request annotations from experts without access to any
previously annotated data.cold-start al is highly relevant to many practical
scenarios. first, cold-start al aims to study the general question of
constructing a training set for an organ that has not been labeled in public
datasets. this is a very common scenario (whenever a dataset is collected for a
new application), especially when iterative al is not an option. second, even if
iterative al is possible, a better initial set has been found to lead to
noticeable improvement for the subsequent al cycles [4,25]. third, in low-budget
scenarios, cold-start al can achieve one-shot selection of the most informative
data without several cycles of annotation. this can lead to an appealing 'less
is more' outcome by optimizing the available budget and also alleviating the
issue of having human experts on standby for traditional iterative al.despite
its importance, very little effort has been made to address the coldstart
problem, especially in medical imaging settings. the existing cold-start al
techniques are mainly based on the two principles of the traditional al
strategies: (1) uncertainty sampling [5,11,15,18], where the most uncertain
samples are selected to maximize the added value of the new annotations. (2)
diversity sampling [7,10,19,22], where samples from diverse regions of the data
distribution are selected to avoid redundancy. in the medical domain,
diversity-based cold-start strategies have been recently explored on 2d
classification/segmentation tasks [4,24,25]. the effectiveness of these
approaches on 3d medical image segmentation remains unknown, especially since 3d
models are often patch-based while 2d models can use the entire image. a recent
study on 3d medical segmentation shows the feasibility to use the uncertainty
estimated from a proxy task to rank the importance of the unlabeled data in the
cold-start scenario [14]. however, it fails to compare against the
diversity-based approaches, and the proposed proxy task is only limited to ct
images, making the effectiveness of this strategy unclear on other 3d imaging
modalities. consequently, no comprehensive cold-start al baselines currently
exist for 3d medical image segmentation, creating additional challenges for this
promising research direction.in this paper, we introduce the colossal benchmark,
the first cold-start active learning benchmark for 3d medical image segmentation
by evaluating on six popular cold-start al strategies. specifically, we aim to
answer three important open questions: (1) compared to random selection, how
effective are the uncertainty-based and diversity-based cold-start strategies
for 3d segmentation tasks? (2) what is the impact of allowing a larger budget on
the compared strategies? (3) can these strategies work better if the local roi
of the target organ is known as prior? we train and validate our models on five
3d medical image segmentation tasks from the publicly available medical
segmentation decathlon (msd) dataset [1], which covers two of the most common 3d
image modalities and the segmentation tasks for both healthy tissue and
tumor/pathology.our contributions are summarized as follows:• we offer the first
cold-start al benchmark for 3d medical image segmentation. we make our code
repository, data partitions, and baseline results publicly available to
facilitate future cold-start al research. • we explore the impact of the budget
and the extent of the 3d roi on the cold-start al strategies.
formally, given an unlabeled data pool of size n , cold-start al aims to select
the optimal m samples (m ≪ n ) without access to any prior segmentation labels.
specifically, the optimal samples are defined as the subset of 3d volumes that
can lead to the best validation performance when training a standard 3d
segmentation network. in this study, we use m = 5 for low-budget scenarios.
we use the medical segmentation decathlon (msd) collection [1] to define our
benchmark, due to its public accessibility and the standardized datasets
spanning across two common 3d image modalities, i.e., ct and mri. we select five
tasks from the collection appropriate for the 3d segmentation tasks, namely
tasks 2-heart, 3-liver, 4-hippocampus, 7-pancreas, and 9-spleen. liver and
pancreas tasks include both organ and tumor segmentation, while the other tasks
focus on organs only.
in this study, we investigate the cold-start al strategies for 3d segmentation
tasks in three scenarios, as illustrated in fig. 1.1. with a low budget of 5
volumes (except for heart, where 3 volumes are used because of the smaller
dataset and easier segmentation task), we assess the performance of the
uncertainty-based and diversity-based approaches against the random selection.
2. next, we explore the impact of budgets for different cold-start al schemes by
allowing a higher budget, as previous work shows inconsistent effectiveness of
al schemes in different budget regimes [7]. 3. finally, we explore whether the
cold-start al strategies can benefit from using the uncertainty/diversity from
only the local roi of the target organ, rather than the entire volume. this
strategy may be helpful for 3d tasks especially for small organs, whose
uncertainty/diversity can be outweighted by the irrelevant structures in the
entire volume, but needs to be validated.evaluation metrics. to evaluate the
segmentation performance, we use the dice similarity coefficient and 95%
hausdorff distance (hd95), which measures the overlap between the segmentation
result and ground truth, and the quality of segmentation boundaries by computing
the 95 th percentile of the distances between the segmentation and the ground
truth boundary points, respectively.
we provide the implementation for the baseline approaches: random selection, two
variants of an uncertainty-based approach named proxyrank [14], and three
diversity-based methods, namely alps [22], calr [10], and typiclust [7].random
selection. as suggested by prior works [3,4,7,12,20,26], random selection is a
strong competitor in the cold-start setting, since it is independent and
identically distributed (i.i.d.) to the entire data pool. we shuffle the entire
training list with a random seed and select the first m samples. in our
experiments, random selection is conducted 15 times and the mean dice score is
reported.uncertainty-based selection. many traditional al methods use
uncertainty sampling, where the most uncertain samples are selected using the
uncertainty of the network trained on an initial labeled set. without such an
initial labeled set, it is not straightforward to capture uncertainty in the
cold-start setting.recently, nath et al. [14] proposed a proxy task and then
utilized uncertainty generated from the proxy task to rank the unlabeled data.
by selecting the most uncertain samples, this strategy has shown superior
performance to random selection. specifically, pseudo labels were generated by
thresholding the ct images with an organ-dependent hounsfield unit (hu)
intensity window. these pseudo labels carry coarse information for the target
organ, though they also include other unrelated structures. the uncertainty
generated by this proxy task is assumed to represent the uncertainty of the
actual segmentation task.however, this approach [14] was limited to ct images.
here, we extend this strategy to mr images. for each mr image, we apply a
sequence of transformations to convert it to a noisy binary mask: (1) z-score
normalization, (2) intensity clipping to the [1 st , 99 th ] percentile of the
intensity values, (3) intensity normalization to [0, 1] and (4) otsu
thresholding [16]. we visually verify that the binary pseudo label includes the
coarse boundary of the target organ.as in [14], we compute the model uncertainty
for each unlabeled data using monte carlo dropout [6]: with dropout enabled
during inference, multiple predictions are generated with stochastic dropout
configurations. entropy [13] and variance [21] are used as uncertainty measures
to create two variants of this proxy ranking method, denoted as proxyrank-ent
and proxyrank-var. the overall uncertainty score of an unlabeled image is
computed as the mean across all voxels. finally, we rank all unlabeled data with
the overall uncertainty scores and select the most uncertain m
samples.diversity-based selection. unlike uncertainty-based methods which
require a warm start, diversity-based methods can be used in the cold-start
setting. generally, diversity-based approaches consist of two stages. first, a
feature extraction network is trained using unsupervised/self-supervised tasks
to represent each unlabeled data as a latent feature. second, clustering
algorithms are used to select the most diverse samples in latent space to reduce
data redundancy. the major challenge of benchmarking the diversity-based methods
for 3d tasks is to have a feature extraction network for 3d volumes. to address
this issue, we train a 3d auto-encoder on the unlabeled training data using a
self-supervised task, i.e., image reconstruction. specifically, we represent
each unlabeled 3d volume as a latent feature by extracting the bottleneck
feature maps, followed by an adaptive average pooling for dimension reduction
[24]. afterwards, we adapt the diversity-based approaches to our 3d tasks by
using the same clustering strategies as proposed in the original works, but
replacing the feature extraction network with our 3d version. in our benchmark,
we evaluate the clustering strategies from three state-of-the-art
diversity-based methods. 1. alps [22]: k -means is used to cluster the latent
features with the number of clusters equal to the query number m. for each
cluster, the sample that is the closest to the cluster center is selected. 2.
calr [10]: this approach is based on the maximum density sampling, where the
sample with the most information is considered the one that can optimally
represent the distribution of a cluster. a bottom-up hierarchical clustering
algorithm termed birch [23] is used and the number of clusters is set as the
query number m. for each cluster, the information density for each sample within
the cluster is computed and the sample with the highest information density is
selected. the information density is expressed as, where x c = {x 1 , x 2 , ...x
j } is the feature set in a cluster and cosine similarity is used as sim(⋅). 3.
typiclust [7]: this approach also uses the points density in each cluster to
select a diverse set of typical examples. k -means clustering is used, followed
by selecting the most typical data from each cluster, which is similar to the
alps strategy but less sensitive to outliers. the typicality is calculated as
the inverse of the average euclidean distance of x to its k nearest neighbors
knn(x), expressed as:. k is set as 20 in the original paper but that is too high
for our application. instead, we use all the samples from the same cluster to
calculate typicality.
in our benchmark, we use the 3d u-net as the network architecture. for
uncertainty estimation, 20 monte carlo simulations are used with a dropout rate
of 0.2. as in [14], a dropout layer is added at the end of every level of the
u-net for both encoder and decoder. the performance of different al strategies
is evaluated by training a 3d patch-based segmentation network using the
selected data, which is an important distinction from the earlier 2d variants in
the literature. the only difference between different experiments is the
selected data. for ct pre-processing, image intensity is clipped to [-1024,
1024] hu and rescaled to [0, 1]. for mri pre-processing, we sequentially apply
z-score normalization, intensity clipping to [1 st , 99 th ] percentile and
rescaling to [0, 1]. during training, we randomly crop a 3d patch with a patch
size of 128 × 128 × 128 (except for hippocampus, where we use 32 × 32 × 32) with
the center voxel of the patch being foreground and background at a ratio of 2 ∶
1. stochastic gradient descent algorithm with a nesterov momentum (µ = 0.99) is
used as the optimizer and l dicece is used as the segmentation loss. an initial
learning rate is set as 0.01 and decayed with a polynomial policy as in [9]. for
each experiment, we train our model using 30k iterations and validate the
performance every 200 iterations.a variety of augmentation techniques as in [9]
are applied to achieve optimal performance for all compared methods. all the
networks are implemented in pytorch [17] and monai [2]. our experiments are
conducted with the deterministic training mode in monai with a fixed random
seed=0. we use a 24g nvidia geforce rtx 3090 gpu.for the global vs. local
experiments, the local rois are created by extracting the 3d bounding box from
the ground truth mask and expanding it by five voxels along each direction. we
note that although no ground truth masks are accessible in the cold-start al
setting, this analysis is still valuable to determine the usefulness of local
rois. it is only worth exploring automatic generation of these local rois if the
gold-standard rois show promising results.
impact of selection strategies. in fig. 2, with a fixed budget of 5 samples
(except for heart, where 3 samples are used), we compare the uncertainty-based
and diversity-based strategies against the random selection on five different
segmentation tasks. note that the selections made by each of our evaluated al
strategies are deterministic. for random selection, we visualize the individual
dice scores (red dots) of all 15 runs as well as their mean (dashed line). hd95
results (supp. tab. 1) follow the same trends.our results explain why random
selection remains a strong competitor for 3d segmentation tasks in cold-start
scenarios, as no strategy evaluated in our benchmark consistently outperforms
the random selection average performance.however, we observe that typiclust
(shown as orange) achieves comparable or superior performance compared to random
selection across all tasks in our benchmark, whereas other approaches can
significantly under-perform on certain tasks, especially challenging ones like
the liver dataset. hence, typi-clust stands out as a more robust cold-start
selection strategy, which can achieve at least a comparable (sometimes better)
performance against the mean of random selection. we further note that typiclust
largely mitigates the risk of 'unlucky' random selection as it consistently
performs better than the low-performing random samples (red dots below the
dashed line).impact of different budgets. in fig. 3(a), we compare al strategies
under the budgets of m = 5 vs. m = 10 (3 vs. 5 for hearts). we visualize the
performance under each budget using a heatmap, where each element in the matrix
is the difference of dice scores between the evaluated strategy and the mean of
random selection under that budget. a positive value (warm color) means that the
al strategy is more effective than random selection. we observe an increasing
amount of warm elements in the higher-budget regime, indicating that most
cold-start al strategies become more effective when more budget is allowed. this
is especially true for the diversity-based strategies (three bottom rows),
suggesting that when a slightly higher budget is available, the diversity of the
selected samples is important. hd95 results (supp. tab. 1) are similar.impact of
different rois. in fig. 3(b), with a fixed budget of m = 5 volumes, we compare
the al strategies when uncertainty/diversity is extracted from the entire volume
(global) vs. a local roi (local). each element in this heatmap is the dice
difference of the al strategy between global and local; warm color means global
is better than local. the hippocampus images in msd are already cropped to the
roi, and thus are excluded from this comparison. we observe different trends
across different methods and tasks. overall, we can observe more warm elements
in the heatmap, indicating that using only the local uncertainty or diversity
for cold-start al cannot consistently outperform the global counterparts, even
with ideal roi generated from ground truth. hd95 results (supp. tab. 2) follow
the same trends.limitations. for the segmentation tasks that include tumors (
4th and 5th columns on fig. 3(a)), we find that almost no al strategy is very
effective, especially the uncertainty-based approaches. the uncertainty-based
methods heavily rely on the uncertainty estimated by the network trained on the
proxy tasks, which likely makes the uncertainty of tumors difficult to capture.
it may be nec-essary to allocate more budget or design better proxy tasks to
make cold-start al methods effective for such challenging tasks. lastly,
empirical exploration of cold-start al on iterative al is beyond the scope of
this study and merits its own dedicated study in future.
in this paper, we presented the colossal benchmark for cold-start al strategies
on 3d medical image segmentation using the public msd dataset. comprehensive
experiments were performed to answer three important open questions for
cold-start al. while cold-start al remains an unsolved problem for 3d
segmentation, important trends emerge from our results; for example,
diversitybased strategies tend to benefit more from a larger budget. among the
compared methods, typiclust [7] stands out as the most robust option for
cold-start al in medical image segmentation tasks. we believe our findings and
the open-source benchmark will facilitate future cold-start al studies, such as
the exploration of different uncertainty estimation/feature extraction methods
and evaluation on multi-modality datasets.
when using a machine learning (ml) model during intraoperative tissue
characterisation, it is vital that the surgeon is able to assess how reliable a
model's prediction is [8]. for the surgeon to trust the output predictions of
the model, the model must be able to explain itself reliably in a clinical
scenario [2]. to assess an explainability method we consider five metrics of
performance: speed, usability, generalisability, trustworthiness and ability to
localise semantic features. the explanation of a model's predictions is
trustworthy if small perturbations in the input or model parameters, results in
a similar output explanation. one form of explainability in the image
classification domain is pixel attribution (pa) mapping. pa maps aim to
highlight the "most important" pixels to the classification. pa maps can be used
to visually highlight whether a model is poorly extracting semantic features
[32] and/or that the model is misinformed due to spurious correlations within
the data that it was trained on [16]. to efficiently process image data, these
methods mainly rely on convolutional neural networks (cnns) and achieve
state-of-the-art (sota) performance. one of the first pa methods proposed for
cnns was class activation maps (cam) [33]. cam uses one forward pass of the
model to find the channels in the last convolutional layer that contributed most
to the prediction. one of cam's limitations is its reliance on global average
pooling (gap) [21] after the last convolutional layer as it dramatically reduces
the number of architectures that can use cam. to improve on this, grad-cam [30]
generalises to all cnn architectures which are differentiable from the output
logit layer to the chosen convolutional layer. however, grad-cam often lacks
sharpness in object localisation, as noted and improved on in grad-cam++ [6] and
smoothgrad-cam++ [24]. these extensions of grad-cam have good semantic feature
localisation but they are unable to be deployed for use in surgery [5]. both
score-cam [31] and recipro-cam [5] also generalise to all cnn architectures but
are deployable. score-cam improves on object localisation within the visual pa
map without losing the class specific capabilities of grad-cam by masking out
regions of the image and measuring the change in the output score. this is
similar to perturbation methods like rise [26], lime [28] and other perturbation
techniques [3,32]. on the other hand, recipro-cam focuses on the speed of pa map
computation whilst maintaining comparable sota performance. by utilising the
cnn's receptive field, recipro-cam generates a number of spatial masks and then
measures the effect on the output score much like score-cam.despite being
speedy, easy to deploy and able to localise semantic features, the above methods
lack trustworthiness due to the training strategy of their underlying model.
deep learning (dl) models trained with empirical risk minimisation (erm) are
overconfident in prediction [12] and vulnerable to adversarial attacks [13].
bayesian neural networks (bnns) [23] bring improved regularisation and output
uncertainty estimates. unfortunately, the non-linearity and number of variables
within nns make bayesian inference a computationally intensive task. for this
reason, variational methods [15,18] are used to approximate bayesian inference.
more recently, the variational method bayes by backprop [4] used dropout [19] to
approximate bayesian inference. dropout is a regularisation technique which has
also been noted to improve salient feature extraction. although bayes by
backprop is trustworthy, it often fails to scale to the complex architectures of
sota models. to improve on this lack of generalisability, another variational
method called monte carlo (mc) dropout [12] proposes that a model trained with
dropout is equivalent to a probabilistic deep gaussian process [7,11]. with this
assumption, an estimated output distribution is computed after a number of
forward passes with dropout have been applied. this output distribution is used
in practice to indicate risk in the model's predictions. surgeons in practice
can use this risk during diagnosis to trust the model for decision making [14].
using dropout to perturb a model is a computationally cheap method of model
averaging [19]. it is worth noting though that this method's validity as a
bayesian inference approximation was later questioned [10]. however, this does
not affect the use of this method for risk estimation. so far, model
explainability and risk estimation have mostly been used separately to assess
models' suitability for surgical applications. distdeepshap [20] computed the
uncertainty of shapley values to show uncertainty in explainability maps.
however, distdeepshap is a model-agnostic interpretability method that shows the
global effect of perturbing inputs, instead of providing an insight to the
model's learned representations. the aim of this paper is to show that the
fusion of mc dropout and pa methods leads to improved explainability.in this
paper, we propose the first approach which incorporates risk estimation into a
pa method. a classification model is trained with dropout and a pa method is
used to generate a pa map. at test time, the classification model is employed
with the dropout enabled. in this work, we propose to repeat this process for a
number of iterations creating a volume of pa maps. this volume is used to
generate a pixel-wise distribution of pa values from which we can infer risk.
more specifically, we introduce a method to generate an enhanced pa map by
estimating the expectation values of the pixel-wise distributions. in addition,
the coefficient of variation (cv) is used to estimate pixel-wise risk of this
enhanced pa map. this provides an improved explanation of the model's prediction
by clearly presenting to the surgeon which salient areas to trust in the model's
enhanced pa map. in this work, we focus on the explainability of the
classification of brain tumours using probe-based confocal laser endomicroscopy
(pcle) data. performance evaluation on pcle data shows that our improved
explainability method outperforms the sota.
the aim of the proposed method is to produce an improved pa map of a
classification model, while providing risk estimation of the model's
explainability to enhance trustworthiness in decision making during
intraoperative tissue characterisation.in our method, any cnn classification
model trained with dropout can be used. let ŷ be the output logits of the cnn
model, where dropout is enabled at test time, with input image x ∈ r
height×width×channels . any pa method can be used to generate a pa map using the
output logits s = f s ( ŷ ) ∈ r height×width where f s (.) is the pa method. we
propose to repeat the above process for t iterations to create a volume of pa
maps s = {s 1 , ..., s t } ∈ r height×width×t . a visual representation of how
the volume is generated is show in fig. 2. the aim is to use this volume to
generate a pixel-wise distribution of pa values from which we can infer risk. to
achieve this, we compute the expectation and variance values of the volume along
the third dimension as:where, i, j represent the pixel's row and column
coordinates, respectively. the expectation e(s i,j ) of each pixel (i, j) is
used to generate an enhanced pa map of size height × width. the intuition is
that the above distribution of pa values can produce less noisy and risky
estimations of a pixel's contribution to the final explainability map compared
to a single estimate.as well as advancing sota pa methods, our method also
estimates the trustworthiness of the enhanced pa map generated above. for risk
estimation, it is important to consider that different pixels in the pa map
correspond to different semantic features which contribute differently to the
output logits. this makes the pixel-wise distributions have different scales.
for this reason, the coefficient of variation (cv) is used to estimate
pixel-wise risk, as it allows us to compare pixel-wise variances despite their
different scales. this is mathematically defined as:our proposed method improves
trustworthiness of explainability as it allows visualisation of both the
explainability of the classification model (provided by the enhanced pa map)
together with the pixel-wise risk of this map (provided by the cv map). for
instance, salient areas on the pa map should not be trusted unless the cv values
are low. an example of the enhanced pa and risk maps generated with the proposed
method are shown in fig. 3. this shows that the proposed method not only
improves explainability but also provides associated risk information which
improves trustworthiness.
dataset. the developed explainability framework has been validated on an in vivo
and ex vivo pcle dataset of meningioma, glioblastoma and metastases of an
invasive ductal carcinoma (idc). all studies on human subjects were performed
according to the requirements of the local ethic committee and in agreement with
the declaration of helsinki (no. cle-001 nr: 2014480). the cellvizio c by mauna
kea technologies, paris, france has been used in combination with the mini laser
probe cystoflex c uhd-r. the distinguishing characteristic of the meningioma is
the psammoma body with concentric circles that show various degrees of
calcification. regarding glioblastomas, the pcle images allow for the
visualization of the characteristic hypercellularity, evidence of irregular
nuclei with mitotic activities or multinuclear appearance with irregular cell
shape. when examining metastases of an idc, the tumor presents as egg-shaped
cells with uniform evenly spaced nuclei. our dataset includes 38 meningioma
videos, 24 glioblastoma and 6 idc. each pcle video represents one tumour type
and corresponds to a different patient. the data has been curated to remove
noisy images and similar frames. this resulted in a training dataset of 2500
frames per class (7500 frames in total) and a testing dataset of the same size.
the dataset is split into a training and testing subset, with the division done
on the patient level.implementation. to implement the dl models we use the
open-source framework pytorch [25] and a nvidia geforce rtx 3090 graphics card
for parallel computation. to show our method generalises we trained two
lightweight models: resnet-18 [17] with a learning rate of 0.01 and mobilenetv2
[29] with a learning rate of 0.001. both were trained using the adam-w [22]
optimiser with a weight decay of 0.01 and dropout probability 0.1. we report the
model's top-1 accuracy for resnet18 as 94.0% and for mobilenet as 86.6%. at test
time, we set t = 100 to create a fair distribution of pa maps. pa methods were
implemented with the help of torchcam [9] and reciprocam was implemented using
the authors' source code.evaluation metrics. evaluating a pa method is not a
trivial task because a pa map may not need to be inline with what a human deems
"reasonable" [1]. segmentation scores like intersection over union (iou) may be
used with caution to compare thresholded pa maps to ground truth maps with
annotated salient regions. by doing so, we can measure how informed the model is
about a particular class. to quantify how misinformed a model is, we can
estimate at its average drop [6]:where, x = x f s ( ŷ (x). the above equation
measures the effect on the output score of the classification model if we only
include the pixels which the pa method scored highly. a minimum average drop is
desired.as average drop was found to not be sufficient on its own, the unified
method adcc [27] was introduced which is the harmonic mean of average drop,
coherency and complexity, defined as:coherency is the pearson correlation
coefficient which ensures that the remaining pixels after dropping are still
important, defined as:where, cov(., .) is the covariance and σ is the standard
deviation. a higher coherency is better. complexity is the l1 norm of the output
pa map.complexity is used to measure how cluttered a pa map is. for a good pa
map, complexity should be a minimum. as it has been shown in the literature, the
metrics in eqs. ( 3), ( 5) and ( 6), can not be used individually to evaluate a
pa method [27]. adcc combined with computation time gives us a reliable overall
metric of how a pa method is performing. performance evaluation. the proposed
method has been compared to combinations of resnet18 and mobilenetv2 with sota
pa methods. at test time, dropout is not enabled for these standard methods, it
is only enabled for our method. in table 1, we show that our method outperforms
all the compared cnn-pa method combinations on adcc. the dropout version of
scorecam is too computationally expensive and therefore is not included in our
comparison. we believe that the better performance of our method is because of
the random dropping of features taking place during dropout at test time which
helps to suppress noise in the estimated enhanced pa map. the combination of
recipro-cam with our proposed method improves performance (increases adcc) at
the expense of increasing the computational complexity. we believe that this
could be reduced using a batched implementation of recipro-cam. we attribute
slow down in smoothgradcam++ when dropout is applied during test time to the
perturbations it adds on top of the pa method. our validation study shows that
grad-cam, grad-cam++ and recipro-cam are often leading in terms of speed as
expected from the literature. in fig. 1, we can see our proposed method reduces
noise in the pa map around the salient region. the distinguishing characteristic
of the meningioma is the psammoma body which is highlighted by all the pa
methods. risk estimations from eq. ( 2) are also displayed and provide an added
visualisation for a surgeon to trust the model. as it can be seen, areas of low
cv match the areas of high pa values which verifies the trustworthiness of our
method. we believe that the proposed explainability method could be used to
support the surgeon intraoperatively in diagnosis and decision making during
tumour resection. the enhanced pa map extracted with our method highlights the
areas which were the most important to the model's prediction. when these areas
correlate with clinically relevant areas, it shows that the model has learned to
robustly classify the different tissue classes. hence, it can be trusted by the
surgeon for diagnosis.
in this work we have introduced the first combination of risk in an
explainability method. using our proposed framework we not only improve on all
the tested sota pa method's adcc performances but also produce an estimation of
risk on the output pa values. the proposed method can clearly present to the
surgeon areas of the explainability map that are more trustworthy. from this
work we hope to encourage trust between the surgeon and dl models. for future
work, we plan to reducing the computation time of our method and deploy the
proposed framework for use in surgery.
humans inherently learn in an incremental manner, acquiring new concepts over
time without forgetting previous ones. in contrast, deep learning models suffer
from catastrophic forgetting [10], where learning from new data can override
previously acquired knowledge. in this context, the class-incremental continual
learning problem was formalized by rebuffi et al. [23], where new classes are
observed in different stages, restricting the model from accessing previous
data.the medical domain faces a similar problem: the ability to dynamically
extend a model to new classes is critical for multiple organ and tumor
segmentation, wherein the key obstacle lies in mitigating 'forgetting.' a
typical strategy involves retaining some previous data. for instance, liu et al.
[13] introduced a memory module to store the prototypical representation of
different organ categories. however, such methods, reliant on an account of data
and annotations, may face practical constraints as privacy regulations could
make accessing prior data and annotations difficult [9]. an alternative strategy
is to use pseudo labels generated by previously trained models on new data.
ozdemir et al. [18,19] extended the distillation loss to medical image
segmentation. a concurrent study of ours [7] mainly focused on architectural
extension, addressing the forgetting problem by freezing the encoder and decoder
and adding additional decoders when learning new classes. while these strategies
have been alleviating the forgetting problem, they led to tremendous memory
costs for model parameters.therefore, we identify two main open questions that
must be addressed when designing a multi-organ and tumor segmentation framework.
q1: can we relieve the forgetting problem without needing previous data and
annotations? q2: can we design a new model architecture that allows us to share
more parameters among different continual learning steps?to tackle the above
questions, in this paper, we propose a novel continual multi-organ and tumor
segmentation method that overcomes the forgetting problem with little memory and
computation overhead. first, inspired by knowledge distillation methods in
continual learning [11,14,15,17], we propose to generate soft pseudo annotations
for the old classes on newly-arrived data. this enables us to recall old
knowledge without saving the old data. we observe that with this simple
strategy, we are able to maintain a reasonable performance for the old classes.
second, we propose image-aware segmentation heads for each class on top of the
shared encoder and decoder. these heads allow the use of a single backbone and
easy extension to new classes while bringing little computational cost. inspired
by liu et al. [12], we adopt the text embedding generated by contrastive
language-image pre-training (clip) [22]. clip is a large-scale image-text
co-training model that is able to encode high-level visual semantics into text
embeddings. this information will be an advantage for training new classes with
the class names known in advance.we focus on organ/tumor segmentation because it
is one of the most critical tasks in medical imaging [6,21,27,28], and continual
learning in semantic segmentation is under-explored in the medical domain. we
evaluate our continual learning method using three datasets: btcv [8], lits [1]
and jhh [25] (a private dataset at johns hopkins hospital) 1 . on the public
datasets, the learning trajectory is to first segment 13 organs in the btcv
dataset, then learn to fig. 1. an overview of the proposed method. an encoder
(enc) processes the input image to extract its features, which are then reduced
to a feature vector (fimage) by a global average pooling layer. this feature
vector is subsequently concatenated with a clip embedding (ω class ), calculated
using the pre-trained clip model. through a series of multi-layer perceptron
(mlp) layers, we derive class-specific parameters of convolution kernels (θ
class ). these kernels, when applied to the decoder (dec) feature, yield the
mask for the respective class.segment liver tumors in the lits dataset. on the
private dataset, the learning trajectory is to first segment 13 organs, followed
by continual segmentation of three gastrointestinal tracts and four
cardiovascular system structures. in our study, we review and compare three
popular continual learning baselines that apply knowledge distillation to
predictions [11], features [17], and multi-scale pooled features [3],
respectively. the extensive results demonstrate that the proposed method
outperforms existing methods, achieving superior performance in both keeping the
knowledge of old classes and learning the new ones while maintaining high memory
efficiency.
we formulate the continual organ segmentation as follows: given a sequence of
partially annotated datasets {d 1 , d 2 , . . . , d n } each with organ classes
{c 1 , c 2 , . . . , c n }, we learn a single multi-organ segmentation model
sequentially using one dataset at a time. when training on the i-th dataset d t
, the previous datasets {d 1 , . . . , d t-1 } are not available. the model is
required to predict the accumulated organ labels for all seen datasets {d 1 , .
. . , d t }:(1)where j is a voxel index, x is an image from d t , p is the
probability function that the model learns and ŷ is the output segmentation
mask.
in the context of continual organ segmentation, the model's inability to access
the previous dataset presents a challenge as it often results in the model
forgetting the previously learned classes. in a preliminary experiment, we
observed that a segmentation model pre-trained on some organ classes will
totally forget the old classes when fine-tuned on new ones. we found the use of
pseudo-labeling can largely mitigate this issue and preserve the existing
knowledge. specifically, we leverage the output prediction from the previous
learning step t -1, denoted as ŷt-1 , which includes the old classes c t-1 , as
the pseudo label for the current step's old classes. for new classes, we still
use the ground truth label. formally, the label lc t for class c in current
learning step t can be expressed as:where l c t represents the ground truth
label for class c in step t obtained from dataset d t . by utilizing this
approach, we aim to maintain the original knowledge and prevent the model from
forgetting the previously learned information while learning the new classes.
the following proposed model is trained only with pseudo labeling of old classes
without any other distillation or regularization.
in the following, we introduce the proposed multi-organ segmentation model for
continual learning. figure 1 illustrates the overall framework of the proposed
model architecture. it has an encoder-decoder backbone, a set of image-aware
organ-specific output heads, and text-driven head parameter generation.backbone
model: for continual learning, ideally, the model should be able to learn a
sufficiently general representation that would easily adapt to new classes. we
use swin unetr [4] as our backbone since it exhibits strong performance in
self-supervised pre-training and the ability to transfer to various medical
image segmentation tasks. swin unetr has swin transformer [16] as the encoder
and several deconvolution layers as the decoder.
the vanilla swin unetr has a softmax layer as the output layer that predicts the
probabilities of each class. we propose to replace the output layer with
multiple image-aware organ-specific heads. we first use a global average pooling
(gap) layer on the last encoder features to obtain a global feature f of the
current image x. then for each organ class k, a multilayer perceptron (mlp)
module is learned to map the global image feature to a set of parameters θ k
:where e(x) denotes the encoder feature of image x. an output head for organ
class k is a sequence of convolution layers that use parameters θ k as
convolution kernel parameters. these convolution layers are applied to the
decoder features, which output the segmentation prediction for organ class
k:where e is the encoder, d is the decoder, σ is the sigmoid non-linear layer
and p (y k j = 1) denotes the predicted probability that pixel j belongs to the
organ class k. the predictions for each class are optimized by binary cross
entropy loss. the separate heads allow independent probability prediction for
newly introduced and previously learned classes, therefore minimizing the impact
of new classes on old ones during continual learning. moreover, this design
allows multi-label prediction for cases where a pixel belongs to more than one
class (e.g., a tumor on an organ).
we further equip the segmentation heads with semantic information about each
organ class. with the widespread success of large-scale vision-language models,
there have been many efforts that apply these models to the medical domain
[2,5,26]. it is suggested that vision-language models could be used for
zero-shot learning in the medical domain and recognize novel classes with
well-designed prompts [20]. we propose to use clip [22] to generate text
embeddings for the target organ names. specifically, we produce the organ name
embedding by the pre-trained clip text encoder and a medical prompt (e.g., "a
computerized tomography of a [cls]", where [cls] is an organ class name). then
we use the text embeddings ω together with the global image feature f to
generate parameters for the organ segmentation heads:where ω k is the text
embedding for organ class k. clip embeddings carry highlevel semantic meanings
and have the ability to connect correlated concepts. therefore, it guides the
mlp module to generate better convolution parameters for each organ class. more
importantly, the fixed-length clip embedding allows us to adapt the pre-trained
model to open-vocabulary segmentation and extend to novel classes. [12]: for the
purpose of continual learning, we improve the original design of universal model
in the mlp module.
unlike liu et al. [12], who utilized a single mlp to manage multiple classes, we
allocate an individual and independent mlp to each class. this design
significantly mitigates interference among different classes.
another key contribution of our work is the reduction of computational
complexity in continual segmentation. we compare our proposed model's flops
(floating-point operations per second) with the baseline model, swin unetr [4].
our model's flops are just slightly higher than swin unetr's, with 661.6 gflops
and 659.4 gflops, respectively. this is because we used lightweight output
convolution heads with a small number of channels. ji et al. [7] proposed a
state-of-the-art architecture for medical continual semantic segmentation, which
uses a pre-trained and then frozen encoder coupled with incrementally added
decoders in each learning step. however, subsequent continual learning steps
using this architecture introduce massive computational complexity. for example,
swin unetr's decoder alone has 466.08 gflops, meaning that every new learning
step adds an additional 466.08 gflops. in contrast, our model only needs to add
a few image-aware organ-specific heads for new classes of the new task, with
each head consuming only 0.12 gflops. as a result, the computational complexity
of our model nearly remains constant in continual learning for segmentation,
while that of the architecture of ji et al. [7] increases linearly to the number
of steps.
datasets: we empirically evaluate the proposed model under two data settings: in
one setting, both training and continual learning are conducted on the inhouse
jhh dataset. it has multiple classes annotated, which can be categorized into
three groups: the abdominal organs (in which seven classes are learned in step
1: spleen, right kidney, left kidney, gall bladder, liver, postcava, pancreas),
the gastrointestinal tract (in which three classes are learned in step 2:
stomach, colon, intestine), and other organs (in which four classes are learned
in step 3: aorta, portal vein and splenic vein, celiac truck, superior
mesenteric artery). the categorization is in accordance with totalsegmentator
[24]. in the other setting, we first train on the btcv dataset and then do
continual learning on the lits dataset. the btcv dataset contains 47 abdominal
ct images delineating 13 organs. the lits dataset contains 130 contrast-enhanced
abdominal ct scans for liver and liver tumor segmentation. we use 13 classes
(spleen, right kidney, left kidney, gall bladder, esophagus, liver, stomach,
aorta, inferior vena cava, portal vein and splenic vein, pancreas, right adrenal
gland, left adrenal gland) from btcv in step 1 learning and the live tumor from
lits in step 2 learning.
for a fair comparison, all the compared methods use the same swin unetr [4] as
the backbone, which is the state-of-the-art model in a bunch of medical image
segmentation tasks. we compare with three popular continual learning baseline
methods that apply knowledge distillation, including lwf [11], ilt [17] and plop
[3]. we compare the proposed method with different baseline models using the
commonly used dice score (dsc) metric (the sørensen-dice coefficient). in each
learning step, we report the average dsc for the classes that are used at the
current step as well as the previous steps (e.g., in step 2 of the jhh dataset,
we report the average dice of the gastrointestinal tracts and the abdominal
organs). the dice score at old classes reveals a model's ability to retain its
previous knowledge, and the score for the current step classes indicates the
model's ability to acquire new knowledge under the regularization of old ones.
the proposed model architecture is trained on new classes with pseudo labeling
of old classes. no other distillation techniques are used. we use a lightweight
design for the image-aware organ-specific heads. each head consists of three
convolution layers. the number of kernels in the first two layers is 8, and in
the last layer is 1. all the compared models are trained using the adamw
optimizer for 100 epochs with a cosine learning rate scheduler. we use a batch
size of 2 and a patch size of 96 × 96 × 96 for the training. the initial
the continual segmentation results using the jhh dataset and public datasets are
shown in tables 1 and2, respectively. notably, by simply using the pseudo
labeling technique (lwf), we are able to achieve reasonably good performance in
remembering the old classes (dice of 0.777 in step 2 and 0.767 in step 3 for
abdominal organs in the jhh dataset; dice of 0.770 in step 2 for btcv organs).
class-wise dsc scores are in appendix tables 4567. all the compared methods use
prediction-level or feature-level distillation as regularization. among them,
the proposed method achieves the highest performance in most learning steps.
specifically, the proposed method exhibits the least forgetting in old classes
and a far better ability to adapt to new data and new classes.to evaluate the
proposed model designs, we also conduct the ablation study on the jhh dataset,
shown in table 3. specifically, we ablate the performance improvement introduced
by the organ-specific segmentation heads as well as the clip text embeddings.
the first line in table 3 shows the performance of the baseline swin unetr model
learned with pseudo labeling (lwf). the second row introduces the organ-specific
segmentation heads, but uses one-hot embeddings rather than the clip text
embeddings for each organ. the third row gives the performance of the full
method. the results show that by adapting the model to use organ-specific heads
as segmentation outputs, we are able to achieve improvement of a large margin
(e.g., 0.144 in step 2 and 0.179 in step 3 for gastrointestinal tracts). with
the application of clip text embeddings, we are able to further improves the
performance (e.g., by a margin of 0.019 in step 2 and 0.027 in step 3 for
gastrointestinal tracts). this study validates the effectiveness of the proposed
organ-specific segmentation heads and the clip text embeddings in the continual
organ segmentation task. finally, we show the qualitative segmentation results
of the proposed method together with the best baseline method ilt on the jhh
dataset. we show the results of learning steps 2 and 3 in fig. 2, one case per
column and two cases for each step. the visualization demonstrates that the
proposed method successfully segments the correct organs while the best baseline
method fails throughout the continual learning process.
in this paper, we propose a method for continual multiple organ and tumor
segmentation in 3d abdominal ct images. we first empirically verified the
effectiveness of high-quality pseudo labels in retaining previous knowledge.
then, we propose a new model design that uses organ-specific heads for
segmentation, which allows easy extension to new classes and brings little
computational cost in the meantime. the segmentation heads are further
strengthened by utilizing the clip text embeddings that encode the semantics of
organ or tumor classes. numerical results on an in-house dataset and two public
datasets demonstrate that the proposed method outperforms the continual learning
baseline methods in the challenging multiple organ and tumor segmentation tasks.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43895-0 4.
in recent years, the use of deep learning for automatic medical image
segmentation has led to many successful results based on large amounts of
annotated training data. however, the trend towards segmenting medical images
into finergrained classes (denoted as subclasses) using deep neural networks has
resulted in an increased demand for finely annotated training data [4,11,21].
this process requires a higher level of domain expertise, making it both
time-consuming and demanding. as annotating coarse-grained (denoted as
superclasses) classes is generally easier than subclasses, one way to reduce the
annotation cost is to collect a large number of superclasses annotations and
then labeling only a small number of samples in subclasses. moreover, in some
cases, a dataset may have already been annotated with superclass labels, but the
research focus has shifted towards finer-grained categories [9,24]. in such
cases, re-annotating an entire dataset may not be as cost-effective as
annotating only a small amount of data with subclass labels.here, the primary
challenge is to effectively leverage superclass annotations to facilitate the
learning of fine-grained subclasses. to solve this problem, several works have
proposed approaches for recognizing new subclasses with limited subclass
annotations while utilizing the abundant superclass annotations in
classification tasks [6,8,18,25]. in general, they assume the subclasses are not
known during the training stage and typically involve pre-training a base model
on superclasses to automatically group samples of the same superclass into
several clusters while adapting them to finer subclasses during test
time.however, to the best of our knowledge, there has been no work specifically
exploring learning subclasses with limited subclass and full superclass
annotations in semantic segmentation task. previous label-efficient learning
methods, such as semi-supervised learning [7,17,26], few-shot learning
[10,15,19] and weakly supervised learning [13,27], focus on either utilize
unlabeled data or enhance the model's generalization ability or use weaker
annotations for training. however, they do not take into account the existence
of superclasses annotations, making them less competitive in our setting.in this
study, we focus on the problem of efficient subclass segmentation in medical
images, whose goal is to segment subclasses under the supervision of limited
subclass and sufficient superclass annotations. unlike previous works such as
[6,8,18,25], we assume that the target subclasses and their corresponding
limited annotations are available during the training process, which is more in
line with practical medical scenarios.our main approach is to utilize the
hierarchical structure of categories to design network architectures and data
generation methods that make it easier for the network to distinguish between
subclass categories. specifically, we propose 1) a prior concatenation module
that concatenates predicted logits from the superclass classifier to the input
feature map before subclass segmentation, serving as prior knowledge to enable
the network to focus on recognizing subclass categories within the current
predicted superclass; 2) a separate normalization module that aims to stretch
the intra-class distance within the same superclass, facilitating subclass
segmentation; 3) a hierarchicalmix module inspired by guidedmix [23], which for
the first time suggests fusing similar labeled and unlabeled image pairs to
generate high-quality pseudo labels for the unlabeled samples. however,
guidedmix selects image pairs based on their similarity and fuses entire images.
in contrast, our approach is more targeted. we mix a certain superclass region
from an image with subclass annotation to the corresponding superclass region in
an unlabeled image without subclass annotation, avoiding confusion between
different superclass regions. this allows the model to focus on distinguishing
subclasses within the same superclass. our experiments on the brats 2021 [3] and
acdc [5] datasets demonstrate that our model, with sufficient superclass and
very limited subclass annotations, achieves comparable accuracy to a model
trained with full subclass annotations.
problem definition. we start by considering a set of r coarse classes, denoted
by y c = {y 1 , ..., y r }, such as background and brain tumor, and a set of n
training images, annotated with y c , denoted by d c = {(x l , y l )|y l i ∈ y c
} n l=1 . each pixel i in image x l is assigned a superclass label y l i . to
learn a finer segmentation model, we introduce a set of fine subclasskr }, such
as background, enhancing tumor, tumor core, and whole tumor. we assume that only
a small subset of n training images have pixel-wise subclass labels z ∈ y f
denoted byour goal is to train a segmentation network f (x l ) that can
accurately predict the subclass labels for each pixel in the image x l , even
when n n . without specification, we consider r = 2 (background and foreground)
and extend the foreground class to multi subclass in this work.prior
concatenation. one direct way to leverage the superclass and subclass
annotations simultaneously is using two 1×1×1 convolution layers as superclass
and subclass classification heads for the features extracted from the network.
the superclassification and subclassification heads are individually trained by
superclass p c (x l ) labels and subclass labels p f (x l ). with enough
superclass labels, the feature maps corresponding to different superclasses
should be well separated. however, this coerces the subclassification head to
discriminate among k subclasses under the mere guidance from few subclass
annotations, making it prone to overfitting.another common method to incorporate
the information from superclass annotations into the subclassification head is
negative learning [14]. this technique penalizes the prediction of pixels being
in the wrong superclass label, effectively using the superclass labels as a
guiding principle for the subclassification head. however, in our experiments,
we found that this method may lead to lower overall performance, possibly due to
unstable training gradients resulting from the uncertainty of the subclass
labels.to make use of superclass labels without affecting the training of the
subclass classification head, we propose a simple yet effective method called
prior concatenation (pc): as shown in fig. 1 (a), we concatenate predicted
superclass logit scores s c (x l ) onto the feature maps f (x l ) and then
perform subclass segmentation. the intuition behind this operation is that by
concatenating the predicted superclass probabilities with feature maps, the
network is able to leverage the prior knowledge of the superclass distribution
and focus more on learning the fine-grained features for better discrimination
among subclasses.separate normalization. intuitively, given sufficient
superclass labels in supervised learning, the superclassification head tends to
reduce feature distance among samples within the same superclass, which
conflicts with the goal of increasing the distance between subclasses within the
same superclass. to alleviate this issue, we aim to enhance the internal
diversity of the distribution within the same superclass while preserving the
discriminative features among superclasses.to achieve this, we propose separate
normalization(sn) to separately process feature maps belonging to hierarchical
foreground and background divided by superclass labels. as a superclass and the
subclasses within share the same background, the original conflict between
classifiers is transferred to finding the optimal transformations that separate
foreground from background, enabling the network to extract class-specific
features while keeping the features inside different superclasses
well-separated.our framework is shown in fig. 1 (b). first, we use batch norm
layers [12] to perform separate affine transformations on the original feature
map. the transformed feature maps, each representing a semantic foreground and
background, are then passed through a convolution block for feature extraction
before further classification. the classification process is coherent with the
semantic meaning of each branch. namely, the foreground branch includes a
superclassifier and a subclassifier that classifies the superclass and subclass
foreground, while the background branch is dedicated solely to classify
background pixels. finally, two separate network branches are jointly supervised
by segmentation loss on superand subclass labels. the aforementioned prior
concatenation continues to take effect by concatenating predicted superclass
logits on the inputs of subclassifier.hierarchicalmix. given the scarcity of
subclass labels, we intend to maximally exploit the existent subclass
supervision to guide the segmentation of coarsely labeled samples. inspired by
guidedmix [23], which provides consistent knowledge transfer between similar
labeled and unlabeled images with pseudo labeling, we propose
hierarchicalmix(hm) to generate robust pseudo supervision. nevertheless,
guidedmix relies on image distance to select similar images and performs a
whole-image mixup, which loses focus on the semantic meaning of each region
within an image. we address this limitation by exploiting the additional
superclass information for a more targeted mixup. this information allows us to
fuse only the semantic foreground regions, realizing a more precise transfer of
foreground knowledge. a detailed pipeline of hierarchicalmix is described
below.as shown in fig. 2, for each sample (x, y) in the dataset that does not
have subclass labels, we pair it with a randomly chosen fine-labeled sample (x ,
y , z ). first, we perform an random rotation and flipping t on (x, y) and feed
both the original sample and the transformed sample tx into the segmentation
network f . an indirect segmentation of x is obtained by performing the inverse
transformation t -1 on the segmentation result of tx. a transform-invariant
pseudo subclass label map z pse is generated according to the following scheme:
pixel (i, j) in z pse is assigned a valid subclass label index (z pse ) i,j = f
(x) i,j only when f (x) i,j agrees with [t -1 f (tx)] i,j with a high confidence
τ as well as f (x) i,j and x i,j both belong to the same superclass label.next,
we adopt image mixup by cropping the bounding box of foreground pixels in x ,
resizing it to match the size of foreground in x, and linearly overlaying them
by a factor of α on x. this semantically mixed image x mix has subclass labels z
= resize(α • z ) from the fine-labeled image x . then, we pass it through the
network to obtain a segmentation result f (x mix ). this segmentation result is
supervised by the superposition of the pseudo label map z pse and subclass
labels z, with weighting factor α:the intuition behind this framework is to
simultaneously leverage the information from both unlabeled and labeled data by
incorporating a more robust supervision from transform-invariant pseudo labels.
while mixing up only the semantic foreground provides a way of exchanging
knowledge between similar foreground objects while lifting the confirmation bias
in pseudo labeling [1].
dataset and preprocessing. we conduct all experiments on two public datasets.
the first one is the acdc1 dataset [5], which contains 200 mri images with
segmentation labels for left ventricle cavity (lv), right ventricle cavity (rv),
and myocardium (myo). due to the large inter-slice spacing, we use 2d
segmentation as in [2]. we adopt the processed data and the same data division
in [16], which uses 140 scans for training, 20 scans for validation and 40 scans
for evaluation. during inference, predictions are made on each individual slice
and then assembled into a 3d volume. the second is the brats20212 dataset [3],
which consists of 1251 mpmri scans with an isotropic 1 mm 3 resolution. each
scan includes four modalities (flair, t1, t1ce, and t2), and is annotated for
necrotic tumor core (tc), peritumoral edematous/invaded tissue (pe), and the
gd-enhancing tumor (et). we randomly split the dataset into 876, 125, and 250
cases for training, validation, and testing, respectively. for both datasets,
image intensities are normalized to values in [0, 1] and the foreground
superclass is defined as the union of all foreground subclasses for both
datasets.implementation details and evaluation metrics. to augment the data
during training, we randomly cropped the images with a patch size of 256 × 256
for the acdc dataset and 96 × 96 × 96 for the brats2021 dataset. the model loss
l is set by adding the losses from cross entropy loss and dice loss. the
weighing factor α in hierarchicalmix section is chosen to be 0.5, while τ
linearly decreases from 1 to 0.4 during the training process.we trained the
model for 40,000 iterations using sgd optimizer with a 0.9 momentum and a
linearly decreasing learning rate that starts at 0.01 and ends with 0. we used a
batch size of 24 for the acdc dataset and 4 for the brats2021 dataset, where
half of the samples are labeled with subclasses and the other half only labeled
with superclasses. more details can be found in the supplementary materials. to
evaluate the segmentation performance, we used two widely-used metrics: the dice
coefficient (dsc) and 95% hausdorff distance (hd 95 ). the confidence factor τ
mentioned in hierarchicalmix starts at 1 and linearly decays to 0.4 throughout
the training process, along with a weighting factor α sampled according to the
uniform distribution on [0.5, 1].performance comparison with other methods. to
evaluate the effectiveness of our proposed method, we firstly trained two u-net
models [20] to serve as upper and lower bounds of performance. the first u-net
was trained on the complete subclass dataset {(x l , y l , z l )} n l=1 , while
the second was trained on its subset {(x l , y l , z l )} n l=1 . then, we
compared our method with the following four methods, all of which were trained
using n subclass labels and n superclass labels: modified u-net (mod): this
method adds an additional superclass classifier alongside the subclass
classifier in the u-net. negative learning (nl): this method incorporates
superclass information into the loss module by introducing a separate negative
learning loss in the original u-net. this additional loss penalizes pixels that
are not segmented as the correct superclass. cross pseudo supervision (cps) [7]:
this method simulates pseudo supervision by utilizing the segmentation results
from two models with different parameter initializations, and adapts their
original network to the modified u-net architecture. uncertainty aware mean
teacher (uamt) [26]: this method modifies the classical mean teacher
architecture [22] by adapting the teacher model to learn from only reliable
targets while ignoring the rest, and also adapts the original network to the
modified u-net architecture.table 1. mean dice score (%, left) and hd95 (mm,
right) of different methods on acdc and brats2021 datasets. sup. and sub.
separately represents the number of data with superclass and subclass
annotations in the experiments. '_' means the result of our proposal is
significantly better than the closet competitive result (p-value < 0.05). the
standard deviations of each metric are recorded in the supplementary materials.
the quantitative results presented in table 1 reveal that all methods that
utilize additional superclass annotations outperformed the baseline method,
which involved training a u-net using only limited subclass labels. however, the
methods that were specifically designed to utilize superclass information or
explore the intrinsic structure of the subclass data, such as nl, cps, and uamt,
did not consistently outperform the simple modified u-net. in fact, these
methods sometimes performed worse than the simple modified u-net, indicating the
difficulty of utilizing superclass information effectively. in contrast, our
proposed method achieved the best performance among all compared methods on both
the acdc and brats2021 datasets. specifically, our method attained an average
dice score of 87.3% for acdc and 75.4% for brats2021, outperforming the closest
competitor by 5.0% and 1.4%, respectively. ablation studies. in this study, we
performed comprehensive ablation studies to analyze the contributions of each
component and the performance of our method under different numbers of images
with subclass annotations. the performance of each component is individually
evaluated, and is listed in table 2. each component has demonstrated its
effectiveness in comparison to the naive modified u-net method. moreover, models
that incorporate more components generally outperform those with fewer
components. the effectiveness of the proposed hierarchicalmix is evident from
the comparisons made with models that use only image mixup or pseudo-labeling
for data augmentation, while the addition of separate normalization consistently
improves the model performance. furthermore, our method was competitive with a
fully supervised baseline, achieving comparable results with only 6.5% and 3.4%
subclass annotations on acdc and brats2021.
in this work, we proposed an innovative approach to address the problem of
efficient subclass segmentation in medical images, where limited subclass
annotations and sufficient superclass annotations are available. to the best of
our knowledge, this is the first work specifically focusing on this problem. our
approach leverages the hierarchical structure of categories to design network
architectures and data generation methods that enable the network to distinguish
between subclass categories more easily. specifically, we introduced a prior
concatenation module that enhances confidence in subclass segmentation by
concatenating predicted logits from the superclass classifier, a separate
normalization module that stretches the intra-class distance within the same
superclass to facilitate subclass segmentation, and a hierarchicalmix model that
generates high-quality pseudo labels for unlabeled samples by fusing only
similar superclass regions from labeled and unlabeled images. our experiments on
the acdc and brats2021 datasets demonstrated that our proposed approach
outperformed other compared methods in improving the segmentation accuracy.
overall, our proposed method provides a promising solution for efficient
fine-grained subclass segmentation in medical images.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43895-0_25.
despite the success of artificial intelligence (ai) in aiding diagnosis, its
application to medical education remains limited. trainee physicians require
several years of experience with a diverse range of clinical cases to develop
sufficient skills and expertise. however, designing educational materials solely
based on real-world data poses several challenges. for example, although small
but significant disease characteristics (e.g., depth of cancer invasion) can
sometimes alter diagnosis and treatment, collecting pairs with and without these
characteristics is cumbersome. another major challenge is longitudinal tracking
of pathological progression over time (e.g., from the early stage of cancer to
the advanced stage), which is difficult to understand because medical images are
often snapshots. privacy is also a concern since images of educational materials
are widely distributed. therefore, medical image editing that allows users to
generate their intended disease characteristics is useful for precise medical
education [3].image editing can synthesize low-or high-level image contents
[11]. our goal is to develop high-precision medical image editing according to
the fine-grained characteristics of individual diseases, rather than at the
level of disease categories. for example, even if two diseases belong to the
same disease category of "lung tumor," the impression of benign or malignant
will differ depending on fine-grained characteristics, such as whether the
margins are "smooth" or "spiculated." in this case, our approach is to edit the
tumor margins to be smooth or spiculated. these fine-grained characteristics
consist of low-to mid-level image features to distinguish the substructures of
organs and diseases, which we call anatomical elements.several types of image
editing techniques for medical imaging have been introduced, mainly using
generative adversarial networks [5] and, more recently, diffusion models [2].
nevertheless, editing specific anatomical elements remains a challenge [1,11].
latent space manipulation generates images by controlling latent feature axes
[4,14], but the editable attributes are often global rather than fine-grained.
conditional generation can precisely edit image content by using class or
segmentation labels. however, it requires manually provided labels [15] or
virtual models [18], which are labor-intensive. additionally, accurately
modeling certain fine-grained characteristics, such as the textual variations of
disease, can be a daunting task. image interpolation [17] requires actual images
with targeted content, which limits its applicability.here, we propose a novel
framework for image editing called u3-net that allows the generation of
anatomical elements with precise conditions. the core technique is
self-supervised segmentation, which aims to achieve pixel-wise clustering
without manually annotated labels [6,7]. as shown in fig. 1a,u3-net converts an
input image into a segmentation map corresponding to the anatomical elements.
once the user has completed editing, u3-net synthesizes an image in which the
targeted anatomical element has been modified. as a result, our synthesized
medical images can highlight hypothetical pathological changes and significant
clinical differences in a single image. for example, fig. 1b shows that whether
or not rectal cancer invades the muscularis propria (i.e., b-2 vs. b-3) affects
cancer staging (i.e., t1 vs. t2) as well as treatment strategy (i.e., endoscopic
resection vs. surgery). the distinction between mucinous and nonmucinous rectal
cancers (see fig. 1c) is also important to estimate the better or worse
prognosis of the disease. these synthetic images can help trainees intuitively
comprehend clinically significant findings and alleviate privacy concerns. five
expert physicians evaluated the edited images from a clinical perspective using
two datasets: a pelvic mri dataset and chest ct dataset.
-we propose a novel image-editing algorithm, u3-net, to synthesize images for
medical education via self-supervised segmentation. -u3-net can faithfully
synthesize intended anatomical elements according to the editing operation on
the segmentation labels. -evaluation by five expert physicians showed that the
edited images were natural as medical images with the intended features.
u3-net consists of three neural networks: encoder, decoder, and discriminator
(see fig. 2). the encoder achieves self-supervised segmentation with a feature
extraction (fe) module and a pixel-wise clustering (cl) module. we perform
pixel-wise clustering under the constraint of invariance to photometric and
geometric transformations [6], with the assumption that these transformations
should not change the clinical interpretation of the anatomical elements. given
a pair of differently transformed images, the fe module produces embedding maps
corresponding to the input images. the cl module then performs kmeans clustering
on the embedding maps to produce two interchangeable outputs: segmentation maps
and corresponding quantized embedding maps. these outputs are trained to be
consistent between the two views. the decoder then estimates the corresponding
images from the quantized embedding maps, while the discriminator forces the
decoder to produce more realistic images.
the training process for u3-net is two-stage. first, we train the encoder and
decoder (excluding the discriminator) to conduct k-class self-supervised
segmentation. to achieve pixel-wise clustering that is consistent between two
transformed views of the input images, we introduce four constraints:
intra-cluster pull force, inter-cluster push force, cross-view consistency, and
reconstruction loss.
we consider a sequence of image transformations [t 1 , . . . , t n ] specified
by the type (e.g., image rotation) and magnitude (e.g., degree of rotation) of
each transformation: cluster assignment and update: in the cl module, k-means
clustering in the first iteration initializes k mean vectors µ k ∈ r d . then,
the embedding vector of the i-th pixel e i∈{1,...,h×w } ∈ r d in the embedding
maps, e 1 and e 2 , is assigned to the cluster with the nearest mean vector as
follows:, where y i is the cluster index of the i-th pixel. by replacing
embedding vectors with their respective mean vectors, quantized embedding maps,
e q1 and e q2 , are generated g(e) = e q = [µ y1 , . . . , µ yh×w ] ∈ r d×h×w .
the cluster indices form the segmentation maps s = [y 1 , . . . , y h×w ] ∈ r
h×w , s 1 and s 2 . the mean vectors µ k are updated by using the exponential
moving average [9].intra-cluster pull force: for transformation-invariant
pixel-wise clustering, we define four loss terms. the first term, cluster loss,
forces the embedding vectors to adhere to the associated mean vector (see fig.
3), as defined:inter-cluster push force: the second term, distance loss, pushes
the distance between the mean vectors above a margin parameter m (see fig. 3),
as defined:where k a and k b indicate two different cluster indices.
the segmentation maps from the different views, s 1 and s 2 , should overlap
after re-transforming to align the coordinates. such a re-transform is composed
of inverse and forward geometric transformations:the inverse transformations of
the photometric transformations are not considered. using the re-transformed
segmentation maps, we impose a third term, cross-view consistency loss, which
forces the embedding vectors of one view to match the mean vector of the other
(see fig. 3), as defined:reconstruction loss: without user editing, the decoder
reconstructs the input images from quantized embedding maps h(e q ) = r ∈ r
c×h×w . we thus employ reconstruction loss, which minimizes the mean squared
error between the reconstructed and input images.learning objective: the
weighted sum of the loss functions is set to be minimized:
in the second stage, we train the decoder and discriminator (excluding the
encoder) to produce naturally appearing images from the quantized embedding
maps. learning objective: we impose generator loss l gen for the decoder to
produce more faithful images by deceiving the discriminator, and discriminator
loss l dis for the discriminator to judge the real or fake of the images as the
perpixel feedback [16]. we also add cutmix augmentation l cutmix and consistency
regularization l cons to the latter [16]. in this stage, the decoder and
discriminator are trained by alternately minimizing the following competing
objectives:
after training, the encoder can output a segmentation map from a testing
image.as shown in fig. 1a, when a user edits the segmentation map s → s by
changing the cluster indices y i → y i , the quantized embedding map is
subsequently updated e q → e q by reassigning the mean vectors according to the
edited indices µ yi → µ y i . finally, the decoder converts the quantized
embedding map into a synthetic image with the intended disease characteristics
h(e q ) = r ∈ r c×h×w . ssim, and psnr were 1.41 × 10 -2 ± 1.04 × 10 -2 , 7.40 ×
10 -1 ± 0.57 × 10 -1 , and 22.5 ± 2.7 in the pelvic mri testing dataset and 5.03
× 10 -4 ± 3.03 × 10 -4 , 9.08 × 10 -1 ± 0.34 × 10 -1 , and 38.6 ± 1.7 in the
chest ct testing dataset. subsequently, segmentation maps from the testing
images were edited to generate images with the intended characteristics (see
fig. 4cd). five expert physicians (two diagnostic radiologists, two colorectal
surgeons, and one thoracic surgeon) assessed them from a clinical perspective.
first, we tested whether the evaluators could identify real or synthesized
images from 20 images, which include ten real images and ten synthesized images.
the accuracies (i.e., the ratio of images correctly identified as real or
synthetic) were 0.69 ± 0.11 and 0.65 ± 0.11, for the pelvic mri and chest ct
testing datasets, respectively. note that when the synthetic images cannot be
distinguished at all, the accuracy should be 0.5. second, we presented image
captions explaining the radiological features, which also represented the
editing intention for the synthetic images. we asked the evaluators to rate each
presented image from a to c. a: the image is natural as a medical image, and the
caption is consistent with the image. b: the image is natural as a medical
image, but the caption is not consistent with the image. c : the image is not
natural as a medical image. this test was conducted after informing the
evaluators of the assumption that all 20 images could be synthetic, without
indicating which image was real or synthetic. as a result, the ratio of
synthetic images (vs. that of real images) categorized as a, b, and c were 0.80
± 0.15 (vs. 0.78 ± 0.20), 0.02 ± 0.04 (vs. 0.08 ± 0.07), and 0.18 ± 0.11 (vs.
0.14 ± 0.13) for the pelvic mri testing dataset, and 0.74 ± 0.28 (vs. 0.76 ±
0.30), 0.08 ± 0.09 (vs. 0.12 ± 0.15), and 0.18 ± 0.21 (vs. 0.12 ± 0.14) for the
chest ct testing dataset. there were no significant differences between real and
synthetic images (t-test: p > 0.05). consequently, the majority of the edited
images were natural-looking medical images with accurately reproduced disease
features.
in this study, we propose a medical image-editing framework to edit fine-grained
anatomical elements. the self-supervised segmentation extracted low-to midlevel
content of medical images, which corresponded well to the clinically meaningful
substructures of organs and diseases. the majority of the edited images with
intended characteristics were perceived as natural medical images by several
expert physicians. our medical image editing method can be applied to medical
education, which has been overlooked as an application of ai. future challenges
include improving scalability with fewer manual operations, validating
segmentation maps from a more objective perspective, and comparing our proposed
algorithm with existing methods, such as those based on superpixels [10].data
use declaration and acknowledgment: the pelvic mri and chest ct datasets were
collected from the national cancer center hospital. the study, data use, and
data protection procedures were approved by the ethics committee of the national
cancer center, tokyo, japan (protocol number 2016-496).our implementation and
all synthesized images will be available here: https://
github.com/kaz-k/medical-image-editing.
appearance loss: appearance loss combines mean squared loss l mse , focal
frequency loss l ffl [8], perceptual loss l lpips [19], and intermediate loss l
int , as follows: l app = w mse l mse + w ffl l ffl + w lpips l lpips + w int l
int , where intermediate loss l int refers to the l2 distance of the
intermediate features of the discriminator between the reconstructed and input
images.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43895-0_38.
implementation and datasets: all neural networks were implemented in python 3.8
using the pytorch library 1.10.0 [12] on an nvidia tesla a100 gpu running cuda
10.2. the encoder, decoder, and discriminator were implemented based on u-net
[13] (see supplementary information for details). the pelvic mri dataset with
rectal cancer contained 289 image series for training and 100 image series for
testing. for each image series, the min-max normalization converted the pixel
values to [-1, 1]. the chest ct dataset with lung cancer contained 500 image
series for training and 100 image series for testing. the ct values in the range
[-2048, 2048] were normalized to [-1, 1]. both were in-house datasets collected
from a single hospital. every image series comprises two-dimensional (2d)
consecutive slices, and we applied our algorithm on a per 2d slice
basis.self-supervised medical image segmentation: we began by optimizing the
hyperparameters to achieve self-supervised segmentation. appropriate
transformations were selected from six candidate functions: t 1 , random
horizontalflip, t 2 , randomaffine, t 3 , colorjitter, t 4 , randomgaussianblur,
t 5 , randomposterize, t 6 , randomgaussiannoise. because anatomical elements,
including the substructures of organs and diseases, are too detailed for human
annotators to segment, it was difficult to create ground-truth labels.
therefore, the training configuration was selected based on the consensus of two
expert radiologists with domain knowledge. by comparing different settings on
the pelvic mri training dataset (see supplementary information), the number of
segmentation classes of 10, the combination of t 1 , t 2 , and t 3 with moderate
magnitude, the weakly imposed reconstruction loss, and a certain value of the
margin parameter were considered suitable for self-supervised segmentation. in
particular, we found that reconstruction loss is essential for obtaining
segmentation maps corresponding to anatomical elements, although such a loss
term was not included in previous studies [6,7]. a similar configuration was
applied to the chest ct training dataset. the resultant segmentation maps are
shown in fig. 4ab. the anatomical substructures, including the histological
structure of the colorectal wall and subregions within the lung, corresponded
well with the segmentation maps in both the pelvic mri and chest ct testing
datasets. because our self-supervised segmentation extracts low-to mid-level
image content, a semantic object (e.g., rectum or lung cancer) typically
consists of multiple segmentation classes shared with other objects (see the
magnified images in fig. 4ab). these anatomical elements may be too detailed for
humans to annotate, demonstrating the necessity of self-supervised segmentation
for highprecision medical-image editing.
we measured the quality of image reconstruction using mean square error (mse),
structural similarity (ssim), and peak signal-to-noise ratio (psnr). the mean ±
standard deviations of mse,
multi-organ segmentation is a crucial step in medical image analysis that
enables physicians to perform diagnosis, prognosis, and treatment planning.
however, manual segmentation of large volume computed tomography (ct) and
magnetic resonance (mr) images is time-consuming and prone to high inter-rater
variability [30]. in recent years, deep convolutional neural networks (cnns)
have achieved state-of-the-art performance on a wide range of segmentation tasks
for natural images [12,16]. however, in the medical domain, there is often a
lack of labeled examples to optimally train a deep neural network from scratch.
since unlabeled medical images are comparatively easier to obtain in larger
quantities, an alternative strategy is to perform self-supervised learning and
generate pre-trained models from unlabeled datasets. self-supervised learning
involves automatically generating a supervisory signal from the data itself and
learning a representation by solving a pretext task.in computer vision, current
self-supervised learning methods can be broadly divided into discriminative
modeling and generative modeling. in earlier times, discriminative
self-supervised pretext tasks are designed as rotation prediction [15], jigsaw
solving [18], and relative patch location prediction [6], etc. recently,
contrastive learning achieves great success, whose core idea is to attract
different augmented views of the same image and repulse augmented views of
different images. based on this, moco [11] is proposed, which greatly shrink the
gap between self-supervised learning and fully-supervised learning. more
advanced techniques have emerged recently [3,9]. contrastive learning frameworks
have also shown promising results in the medical domain, achieving good
performance with few labeled examples [1]. generative modeling also provides a
feasible way for self-supervised pre-training [21,35]. recently, he et al.
propose mae [28] and yield a nontrivial and meaningful generative
self-supervisory task, by masking a high proportion of the input image. transfer
learning performance in downstream tasks outperforms supervised pre-training and
shows promising scaling behavior. in medical image domain, model genesis [36]
uses a "painting" operation to generate a new image by modifying the input
image. several selfsupervised learning approaches have also achieved
state-of-the-art performance in the medical domain on both classification and
segmentation tasks while significantly reducing annotation cost
[1,2,14,20,29,[31][32][33][34] however, most self-supervised pre-training
strategies are image [11] or patch [1] level, which are not capable of capturing
the detailed feature representations required for accurate medical segmentation.
to address this issue, in this paper, we propose a novel contrastive learning
framework that integrates localized region contrast (lrc) to enhance existing
self-supervised pre-training methods for medical image segmentation.our proposed
framework leverages felzenszwalb's algorithm [8] to formulate local regions and
defines a novel contrastive sampling loss to perform localized contrastive
learning. our main contributions include -we propose a standalone localized
contrastive learning module that can be integrated into most existing
pre-training strategy to boost multi-organ segmentation performance by learning
localized feature representations. -we introduce a novel localized contrastive
sampling loss for dense selfsupervised pre-training on local regions.-we conduct
extensive experiments on three multi-organ segmentation benchmarks and
demonstrate that our method consistently outperforms current supervised and
unsupervised pre-training approaches.
figure 1 illustrates our complete framework, which comprises two stages: the
contrastive pre-training stage and the fine-tuning stage. although lrc can be
integrated with most of the current popular pre-training strategies, for the
purpose of illustration, in this section, we demonstrate how to integrate our
lrc module with the classical global (image-level) contrast pre-training
strategy moco [11], using both its original global contrast and our localized
contrastive losses during the contrastive pre-training stage. during the
fine-tuning stage, we simply concatenate the local and global contrast models
and fine-tune the resulting model on a small target dataset. further details
about each stage are discussed in the following subsections.
in the pre-training stage, for each batch an image x q is randomly chosen from b
images as a query sample, and the rest of the images x n ∈ {x 1 , x 2 , ..., x b
} are considered as negative key samples, where n = q. to formulate a positive
key sample x p , elastic transforms are performed on the query sample x q
.global contrast. to explore global contextual information, we train a latent
encoder e g following the contrastive protocol in [11]. three sets of latent
embeddings z q , z p , z n are extracted by e g from x q , x p , x n
respectively. using dot product as a measure of similarity, a form of a
contrastive loss function called infonce [19] is considered:, where τ g is the
global temperature hyper-parameter per [27]. note that in the global contrast
branch, we only pre-train e g .
contrast. unlike global contrast, positive and negative pairs for local contrast
are only generated from input image x q and its transform x p . we differentiate
local regions and formulate the positive and negative pairs by using
felzenszwalb's algorithm. for an input image x, felzenszwalb's algorithm
provides k local regions r = {r 1 , r 2 , .., r k }, where r k is the k-th local
region cluster for image x. we then perform elastic transform for both the query
image x q and its local regions r q so that we have the augmented image x p = t
e (x q ) and its local regions r p = {r 1 p , r 2 p , .., r kp p }, where r k p
= t e (r k q ). note that k q = k p always holds since r p is a one-to-one
mapping from r q . following the widely used u-net [22] model design, the query
image x q and augmented image x p are then forwarded to a randomly initialized
u-net variant, which includes a convolutional encoder e l and a convolutional
decoder d l . we get corresponding feature maps f q and f p with the same
spatial dimensions as x q and x p and d channels from the last convolutional
layer of d l . afterwards, we sample n vectors with dimension d from the local
region r k q in f q , and formulate the sample mean, where f k,n q is the n-th
vector sampled from feature map f q within the k-th local region r k q . our
sampling strategy is straightforward: we sample random points with replacement
following a uniform distribution. we simply refer to this as "random sampling".
similarly, for feature map f p , its sample mean f k p can be provided following
the same random sampling process. each local region pair of f k q and f k p is
considered a positive pair. for the negative pairs, we sample both f q and f p
from the rest of the local regions {r 1 , r 2 , ..., r k-1 , r k+1 , ..., r k }.
the local contrastive loss can be defined as follows:where τ l is the local
temperature hyper-parameter. compared to the global contrast branch, in local
contrastive learning, we pre-train both e l and d l .
in the former pre-training stages, e g , e l , and d l are pre-trained with
global and local contrast strategy accordingly, with a large number of
unlabelled images. in the fine-tuning stage, we fine-tune the model with a
limited number of labelled images x f ∈ {x 1 , x 2 , ..., x f }, where f is the
size of the fine-tuning dataset.besides the two pre-trained encoders and one
decoder, a randomly initialized decoder d g is appended to the pre-trained e g
to ensure that the embeddings have the same dimensions prior to concatenation.
we combine local and global contrast models by concatenating the output of d g
and d l 's last convolutional layer, and fine-tune on the target dataset in an
end-to-end fashion. different levels of feature maps from encoders are
concatenated with corresponding layers of decoders through skip connections to
provide alternative paths for the gradient. dice loss is applied as in usual
multi-organ segmentation tasks.
during both global and local pre-training stages, we pre-train the encoders on
the abdomen-1k [17] dataset. it contains over one thousand ct images which
equates to roughly 240,000 2d slices. the ct images have been curated from 12
medical centers and include multi-phase, multi-vendor, and multi-disease
cases.although segmentation masks for liver, kidney, spleen, and pancreas are
provided in this dataset, we ignore these labels during pre-training since we
are following the self-supervised protocol.
during the fine-tuning stage, we perform extensive experiments on three datasets
with respect to different regions of the human body. abd-110 is an abdomen
dataset from [25] that contains 110 ct images from patients with various
abdominal tumors and these ct images were taken during the treatment planning
stage. we report the average dsc on 11 abdominal organs (large bowel, duodenum,
spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney,
stomach and gallbladder).thorax-85 is a thorax dataset from [5] that contains 85
thoracic ct images. we report the average dsc on 6 thoracic organs (esophagus,
trachea, spinal cord, left lung, right lung, and heart).han is from [24] and
contains 120 ct images covering the head and neck region. we report the average
dsc on 9 organs (brainstem, mandible, optical chiasm, left optical nerve, right
optical nerve, left parotid, right parotid, left submandibular gland, and right
submandibular gland).
all images are re-sampled to have spacing of 2.5 mm × 1.0 mm × 1.0 mm, with
respect to the depth, height, and width of the 3d volume. in the pre-training
stage, we apply elastic transform to formulate positive samples. in the global
contrast branch, we use the sgd optimizer to pre-train a resnet-50 [13] (for mae
[10], we use vit-base [7].) encoder e g for 200 epochs. in the local contrast
branch, we use the adam optimizer to pre-train both encoder e l and decoder d l
for 30 epochs. the dimension of sampled vectors d is 64 since f q and f p have
64 channels. in the fine-tuning stage, we use the adam optimizer to train the
whole framework in an end-to-end fashion. all optimizers in both pre-training
and fine-tuning stages are set to have momentum of 0.9 and weight decay of 10 -4
.
in table 1, we select 9 self-supervised pre-trained with 1 imagenet supervised
pre-trained networks and combine with our proposed localized region contrast
(lrc). through extensive experiments on 3 different datasets, we demonstrate lrc
is capable of enhancing these pre-training algorithms in a consistent way. we
use sørensen-dice coefficient (dsc) to measure our experimental results.
in fig. 2, we show segmentation results on abd-110, thorax-85, and han datasets
respectively. all the results are provided by models trained with target dataset
size |x t | = 10. by comparing (c) with (g) and (d) with (h), our method shows
significant improvement, particularly on the challenging han dataset. however,
due to limited space, we are only capable of demonstrating selected global
pre-training methods.
effect of additional parameters. additional parameters do bring performance
enhancement in machine learning. however, in number of samples n . in table 3,
we explore the effect of different number of samples n to the contrastive
sampling loss. when the sample mean f k is only averaged from a small number of
vectors, the capability of representing certain region level can be limited. in
the opposite, when the number of samples n is large, the sampling bias can be
high, since the number of pixels can be smaller than n . therefore, we need a
proper choice of n . with n = 50, our method demonstrates the best dsc score of
0.732.
in this paper, we propose a contrastive learning framework, which integrates a
novel localized contrastive sampling loss and enables the learning of
finegrained representations that are crucial for accurate segmentation of
complex structures. through extensive experiments on three multi-organ
segmentation datasets, we demonstrated that our approach consistently boosts
current supervised and unsupervised pre-training methods. lrc provides a
promising direction for improving the accuracy of medical image segmentation,
which is a crucial step in various clinical applications. overall, we believe
that our approach can significantly benefit the medical image analysis community
and pave the way for future developments in self-supervised learning for medical
applications.
w/ or wo/ lrc w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ random init 68.8 70.9
76.0 78.0 85.9 87.8 89 89.4 50.9 52.6 77.8 78.0
the demand for precise medical data analysis has led to the widespread use of
deep learning methods in the medical field. however, accompanied by the
promulgation of data acts and the strengthening of data privacy, it has become
increasingly challenging to train models in large-scale centralized medical
datasets. as one of the solutions, federated learning provides a new way out of
the dilemma and attracts significant attention from researchers.federated
learning (fl) [1,2] is a distributed machine learning paradigm in which all
clients train a global model collaboratively while preserving their data
locally. as a crucial core of them, the aggregation algorithm plays an important
role in releasing data potential and improving global model performance. fedavg
[1], as pioneering work, was a simple and effective aggregation algorithm, which
makes the proportions of local datasets size as the aggregation weights of local
models. but in the real world, not only the numbers of datasets held by clients
is different, but also their data distribution may be diverse, which leads to
the fact that the data in the federated learning is non-independent identically
distribution (non-iid). the naive aggregation algorithms maybe have worse
performance because of the non-iid data [3][4][5][6][7][8]. in medical image
segmentation, [9] and [10] took the lead in discussing the application and
safety of federated learning in brain tumor segmentation (brats). to solve the
non-iid challenges of fl in the medical image field, feddg [11] and fedmrcm [12]
were proposed to address the domain shift issue between the source domain and
the target domain, but the sharing of latent features may cause privacy
concerns. auto-fedrl [13] and auto-fedavg [14] were proposed to deal with the
non-iid problem by using an optimization algorithm to learn super parameters and
aggregate weights. ida [15] introduced the inverse distance of local models and
the average model of all clients to handle non-iid data. the work
[16][17][18][19] proposed corresponding aggregation methods from the
perspectives of clustering, frequency domain, bayesian, and representation
similarity analysis. more than this, the first computational competition on
federated learning, federated tumor segmentation (fets) challenge1 [20] was held
to measure the performance of different aggregation algorithms on glioma
segmentation [21][22][23][24]. leon et al. [25] proposed fedcostwavg get a
notable improvement compared to fedavg by including the cost function decreased
during the last round and won the challenge. however, most of these methods
improve the performance by adding other regular terms to the aggregation method,
without considering all factors as a whole, which may limit the performance of
the global model. different from the above methods, inspired by the concept of
the law of universal gravitation in physics, in this paper, we propose a novel
aggregation strategy, fedgrav, which unifies the differences in sample size and
the discrepancies of local models among clients by defining the concept of model
affinity. specifically, we take the client sample size as the mass of the local
model, and the discrepancies among the local models as their distance, which is
quantified from the topological perspective of neural networks. last, the
formula 1 is employed to calculate the affinity and explore the internal
correlation between the local models. the proposed method promotes a more
effective aggregation of local models by unifying the difference between sample
size and local model between clients.the primary contributions of this paper can
be summarized as: (1) we propose fedgrav, a novel aggregation strategy that
unifies the difference both in sample size and local model among clients by
defining the concept of model affinity; (2) we propose model graph distance, a
new method to quantify model differences from the perspective of neural network
topology. (3) we propose an aggregation algorithm that introduces the concept of
affinity and graph into federated learning, and the aggregation weights can be
adjusted adaptively; (4) the superior performance is achieved by the proposed
method, on the public cifar-10 and fets challenge datasets.
suppose k clients with private data cooperate to train a global model and share
the same neural network structure, 3d-unet [26], which is provided by the fets
challenge and kept unchanged. for the clients, every client trains a local model
w i for local e epochs and then delivers the local model to the server. the
server aggregates local models to a global model by computing the aggregation
weights with the proposed fedgrav and assigns it to all clients. specifically,
given k local models, we first make graph mapping to map the network model to
the topology graph, and then the graph distance is obtained after the graph
pruning and comparison. for the model affinity computation, fedgrav takes the
sample size of every client as the mass of the local model and combines the
given graph distance to calculate the affinity between models according to the
formula 1. after that, a symmetric model affinity matrix a ∈ r k×k is analyzed
to compute aggregation weights. last, the server aggregates local models to a
global model according to the aggregation weights and assigns it to all clients.
repeat and until t rounds or other limits. an overview of the method is shown in
fig. 1.
model affinity. inspired by the law of universal gravitation, we assume that
there is similar gravitation between any two local models. we define it as model
affinity in federated learning. it can be described that the affinity between
two local models is proportional to the sample size of the client corresponding
to the local model, and inversely proportional to the distance between two
models. the equation for model affinity takes the form:where a ik is the
affinity between i-th and k-th local models, n i and n k are the sample size of
i-th and k-th client, and d ik is the distance between two local models, which
is quantified from the perspective of neural network topology and will be
described in the following section. m is the affinity constant, it can be
simplified in the subsequent analysis, so this paper will not set specific
values for it. the model affinity depicts the internal correlation between two
local models, which lays the foundation for accurate aggregation weights.graph
distance. the distance is defined to quantify model differences. the differences
in local models reflect the discrepancies in the distribution of client data to
a certain extent. if the differences in local models can be accurately measured,
the more appropriate aggregation weights will be assigned to local models to
aggregate a better global model. the key motivation is to measure the internal
correlations of local models as accurately as possible. we explore the model
distance from the perspective of neural network topology in this paper and
define it as model graph distance. in fedgrav, the computation of graph distance
goes through the following steps:(1) graph mapping. suppose the server has
received local models trained by local data, and we map them into the
topological graph. inspired by [27], take the j-th convolutional layer of k-th
local model with 3d-unet structure as an example, whose kernel dimension is 3 27
. and then, we make every node w as scalar by averaging or summing, which can be
formulated as:it can be mapped into a graph whose structure is similar to the
full connection layer after the scalarization of the convolutional layer. given
a 3×3×3×c in ×c out convolutional layer, the dimensions of its input and output
are c in and c out respectively. so, we obtain a weight matrix w t ∈r cin×cout
after averaging or summing the weights of convolution kernel. we take the c in
and c out as the number of nodes, and the weight summation w sum is the edge
weight.(2) graph pruning. the server collects local models from clients and
makes the graph mapping on them to get k graphs which have the same structure
except for the edge weights. these graphs contain all the information of local
models, including the part of universality and the part of characteristics of
the client data. to make the graphs more distinctive, the graph pruning is
conducted.in detail, we differentiated these graphs by setting an adaptive
threshold δ, where the edge will be removed if the weight difference of each
layer between the local models and global model in the last round is less than
the threshold, otherwise, the edge will exist. it can be simplified
as:otherwise.(3)where in eq. 3, 0 denotes the edge is removed, w t kj denotes
edge weight of the j-th layer from the k-th graph in the t-th round, also the
weight summation of the j-th layer from the k-th local model in the t-th round,
w t-1 gj is the weight summation of the j-th layer from the global model in (t
-1)-th round. the threshold δ varies adaptively with the weights of local
models, and λ is the pruning ratio which is responsible for adjusting the degree
of pruning. after that we get k discriminative graphs(3) graph comparison. in
order to measure the degree of correlation between two graphs, we measure the
similarity between pairs of graphs by computing matching between their sets of
embeddings, where the pyramid match graph kernel [28] is employed. we take the
reciprocal of the correlation degree as the distance between them. the distance
is defined as follows:aggregation weights. according to the above process, the
affinity matrix a is obtained, which reports the correlation among local models
and is symmetric. the element a ik in matrix a denotes the affinity of g i and g
k . the elements in table 1. comparisons with other state-of-the-art methods on
the cifar-10 dataset.
fedavg [1] 88.37 ± 0.04 fedprox [6] 87.93 ± 0.19 fednova [29] 88.68 ± 0.26
auto-fedavg [14] in federated learning, clients send the updated local models
back to the server each round. in round t, α k is represented as α t k . the
global model w t+1 g is aggregated by the server:then, the server assigns the
global model w t g to all clients. repeat and until t rounds or other limits.
cifar-10. the first dataset to verify the validity of our algorithm is cifar-10.
we partition the training set into 8 clients with heterogeneous data by sampling
from a dirichlet distribution (α = 0.5) as in [10] to simulate the non-iid
distribution, and the test set in cifar-10 is considered as the global test set
to evaluate the performance of different algorithms. vgg-9 [30] is employed for
image classification, and the other detailed settings are as follows: initial
learning rate of 1e -2; total rounds of 100; local epochs of 20; batch size of
64; sgd optimizer for clients.
training data. the real-world dataset used in experiments is provided by the
fets challenge organizer, which is the training set of the whole dataset about
brain tumor segmentation. in order to evaluate the performance of fedgrav, we
partition the dataset composed of 341 data samples
experiment results on the cifar-10. we first validate the proposed method on the
cifar-10 dataset. table 1 shows the quantitative results of the state-of-the-art
fl methods in terms of the average accuracy, such as fedavg [1], fedprox [6],
fednova [29], and auto-fedavg [14]. as can be seen from the table, the proposed
fedgrav method outperforms the other competing fl aggregation methods including
auto-fedavg, a learning-based aggregation method, which indicates the potential
and superiority of fedgrav.experiment results on miccai fets2021 training
dataset. in order to verify the robustness of our method and its performance in
real-world data, we conduct the experiment on the miccai fets2021 training
dataset. we evaluate the performance of our algorithm by comparing six
indicators: the dice similarity coefficient(dsc) and hausdorff distance-95th
percentile(hd95) of whole tumor(wt), enhancing tumor(et), and tumor core(tc). as
is shown in table 2, we list the average results of fedavg,
fedcostwavg(shortened to fcw), the champion method of fets challenge 2021, and
the proposed fed-grav. different from the original fedcostwavg which changed the
activation function of networks, our re-implemented version made the network
unchanged to ensure a fair comparison. through the quantitative comparison in
table 2, we can find that the proposed method fedgrav has achieved the best
results in all indicators except the hd95 tc. moreover, compared with
fedcostwavg, fedgrav has significantly improved the evaluation of segmentation
performance, especially in the enhancing tumor segmentation.the visualization
results are shown in fig. 2. it can be seen that our fed-grav achieves better
segmentation results, even in the hard example, compared to fedcostwavg and
fedavg. the results proved that the proposed method fedgrav can explore the
correlations of local models better and achieved more excellent aggregation
performance compared with other methods.
to evaluate the effectiveness and find the better configuration of fedgrav, we
conduct the ablation study on the fets datasets, and the results are shown in
fig. 3. as we can see, the mean dsc shows a trend of rising first and then
falling, because more irrelevant and redundant information will be saved in the
model when pruning is not performed. the different values of λ denote the loose
degree of graphs, with the gradual increase of λ, the redundant information in
local models is gradually eliminated, and the unique information of each local
model is preserved. while, when the pruning ratio λ increases to a certain
extent, the models lack key information, which makes the model affinity
inaccurate, resulting in a decline in segmentation performance.
in this paper, we introduced fedgrav, a novel aggregation strategy inspired by
the law of universal gravitation in physics. fedgrav improves local model
aggregation by considering both the differences in sample size and discrepancies
among local models. it can adaptively adjust the aggregation weights and explore
the internal correlations of local models more effectively. we evaluated our
method on cifar-10 and real-world miccai federated tumor segmentation challenge
(fets) datasets, and the superior results demonstrated the effectiveness and
robustness of our fedgrav.
magnetic resonance imaging (mri) of the brain is an essential imaging modality
to accurately diagnose various neurological diseases ranging from inflammatory
t. pinetz and a. effland-are funded the german research foundation under
germany's excellence strategy -exc-2047/1 -390685813 and -exc2151 -390873048 and
r. haase is funded by a research grant (bonfor; o-194.0002.1). t. pinetz and e.
kobler-contributed equally to this work. lesions to brain tumors and metastases.
for accurate depictions of said pathologies, gadolinium-based contrast agents
(gbca) are injected intravenously to highlight brain-blood barrier dysfunctions.
however, these contrast agents are expensive and may cause nephrogenic systemic
fibrosis in patients with severely reduced kidney function [31]. moreover, [17]
reported that gadolinium accumulates inside patients with unclear health
consequences, especially after repeated application. the american college of
radiology recommends administering the lowest gbca dose to obtain the needed
clinical information [1].driven by this recommendation, several research groups
have recently published dose-reduction techniques focusing on maintaining image
quality. complementary to the development of higher relaxivity contrast agents
[28], virtual contrast [3,8] -replacing a large fraction of the gbca dose by
deep learninghas been proposed. these approaches typically acquire a
contrast-enhanced (ce) scan with a lower gbca dose along with non-ce scans,
e.g., t1w, t2w, flair, or adc. these input images are then processed by a deep
neural network (dnn) to replicate the corresponding standard-dose scan. while
promising, virtual contrast techniques have not been integrated into clinical
practice yet due to falsepositive signals or missed small lesions [3,23]. as
with all deep learning-based approaches, the availability of large datasets is
essential, which is problematic in the considered case since the additional ce
low-dose scan is not acquired in clinical routine exams. hence, there are no
public datasets to easily benchmark and compare different algorithms or evaluate
their performance. in general, the enhancement behavior of pathological tissues
at various gbca dosages has barely been researched due to a lack of data [12].in
recent years, generative models have been used to overcome data scarcity in the
computer vision and medical imaging community. frequently, generative
adversarial networks (gans) [9] are applied as state-of-the-art in image
generation [30] or semantic translation/interpolation [5,18,21]. in a nutshell,
the gan framework trains two competing dnns -the generator and the
discriminator. the generator learns a non-linear transformation of a predefined
noise distribution to fit the distribution of a target dataset, while the
discriminator provides feedback by simultaneously approximating a distance or
divergence between the generated and the target distribution. the choice of this
distance leads to the well-known different gan algorithms, e.g., wasserstein
gans [4,10], least squares gans [24], or non-saturating gans [9]. however, lucic
et al. [22] showed that this choice has only a minor impact on the
performance.learning conditional distributions between images can be
accomplished by additionally feeding a condition (additional scans, dose level,
etc.) into both the generator and discriminator. in particular, for
image-to-image translation tasks, these conditional gans have been successfully
applied using paired [14,25,27] and unpaired training data [35]. within these
methods, an additional content (cycle) loss typically penalizes pixel-wise
deviations (e.g., 1 ) from a corresponding reference to enforce structural
similarity, whereas a local adversarial loss (discriminator with local receptive
field) controls textural similarity. in addition, embeddings have been used to
inject metadata [7,18]. to study the gbca accumulation behavior, we collected
453 ce scans with non-standard gbca doses in the set of {10%, 20%, 33%} along
with the corresponding standard-dose (0.1 mmol/kg) scan after applying the
remaining contrast agent. using this dataset, we aim at the semantic
interpolation of the gbca signal at various fractional dose levels. to this end,
we use gans to learn the contrast enhancement behavior from the dataset
collective and thereby enable the synthesis of contrast signals at various dose
levels for individual cases. further, to minimize the smoothing effect [19] of
typical content losses (e.g., 1 or perceptual [16]), we develop a
noise-preserving content loss function based on the wasserstein distance between
paired image patches calculated using a sinkhornstyle algorithm. this novel loss
enables a faithful generation of noise, which is important for the
identification of enhancing pathologies and their usability as additional
training data.with this in mind, the contributions of this work are as
follows:-synthesis of gbca behavior at various doses using conditional gans,
-loss enabling interpolation of dose levels present in training data,
-noise-preserving content loss function to generate realistic synthetic images.
given a native image x na (i.e. without any contrast agent injection) and a ce
standard-dose image x sd , our conditional gan approach synthesizes ce lowdose
images xld for selected dose levels d ∈ d ⊂ [0, 1] from a uniform noise image z
∼ n (0, id), see fig. 1. to focus the generation on the contrast agent signal,
our model predicts residual images ŷld ; the corresponding low-dose can be
obtained by xld = x na + ŷld .for training and evaluation, we consider samples
(x na , x sd , y ld , d, b) of a dataset ds, where y ld = x ld -x na is the
residual image of a real ce low-dose scan x ld with dose level d ∈ d and b ∈
{1.5, 3} is the field-strength in tesla of the used scanner. to simplify
learning of the contrast accumulation behavior, we adapt the preprocessing
pipeline of brats [6]. further details of the dataset and the preprocessing are
in the supplementary material.
our approach is built on the insight that contrast enhancement is an inherently
local phenomenon and the necessary information for the synthesis task can be
extracted from a local neighborhood within an image. therefore, we use as
generator g θ a convolutional neural network (cnn) that is based on the u-net
[29] along with a local attention mechanism. the architecture design and the
implementation details can be found in the supplementary material. as
illustrated in fig. 1, the generator uses a 3d noise sample z ∼ n(0, id) along
with the native and sd images (x na , x sd ) as input. the synthesis is guided
by the metadata ( d b) , containing the artificial dose level d ∈ d as well as
the field strength of the corresponding scanner b ∈ {1.5, 3}. in particular, the
metadata is injected into every residual block of the generator using an
embedding, motivated by the recent success of diffusion-based models [13].to
learn this generator, a convolutional discriminator f φ is used, which is in
turn trained to distinguish the generated residual images ŷld with random dose
level d from the real residual images y ld with the associated real dose level
d. to make this a non-trivial task, label smoothing on the metadata is used,
i.e., the real dose is augmented by zero-mean additive gaussian noise with
standard deviation 0.05. the discriminator architecture essentially implements
the encoding side of the generator, however, no local attention layers are used
as suggested by [20]. like the generator, the discriminator is conditioned on
the metadata using an embedding, which is not shared between both networks.for
training of the generator θ and discriminator φ, we consider the losswhich
consists of a wasserstein gan loss l gan , a gradient penalty loss l gp , and a
content loss l c that are relatively weighted by scalar non-negative factors λ
gp and λ c . in detail, the wasserstein gan loss reads as lipschitz continuous
with factor 1 in its arguments as required by wasserstein gans [10]. here, ŷld =
g θ (z, ĉ) and the lipschitz penalty is evaluated at convex combinations of real
and synthetic images and condition tuples (essentially dose levels). finally,
using a distance c , the content loss l c (θ) := e (xna,xsd,yld,d,b)∼u (ds),z∼n
(0,id) c g θ (z, c) , y ld guides the generator g θ towards residual images in
the dataset. thus, it teaches the generator the principles of contrast
enhancement. typically, the 1 -norm is used as a distance function, which leads
to smooth results since it also penalizes deviations from the noise in y ld .
to generate realistic ce images, it is also important to retain the original
noise characteristics. therefore, we introduce a novel loss that accounts for
deviations in local statistics using optimal transport between empirical
distributions of paired patches, as illustrated in fig. 2. let x, x ∈ r n 3 be
patches of size n × n × n extracted from the same location of a real and
synthetic image, respectively. the wasserstein distance of the associated
empirical distributions using a transport plan t ∈ r n 3 ×n 3where 1 is the
vector of ones of size n 3 . in contrast to the element-wise difference
penalization of the 1 -distance, the wasserstein distance accounts for
mismatches between distributions. to illustrate this, let us, for instance,
assume that both patches are gaussian distributed (x ∼ n (μ, σ), x ∼ n (μ, σ)),
which is a coarse simplification of real mri noise [2]. in this case, the
wasserstein distance reduces to second-order momentum matching, i.e., w 2 (x, x)
= (μ -μ) 2 +(σ -σ) 2 . thus, the wasserstein distance generalizes this
distributional loss to any distribution within paired patches.to efficiently
solve problem (2), we use the inexact proximal point algorithm [34]. this
algorithm is parallelized and applied to all non-overlapping patch pairs, to
obtain our noise-preserving content losswhere p p extracts a local n × n × n
patch at location p ∈ p = {0, n, 2n, . . .} 3 using periodic boundary
conditions. note that we compute the expectation over offsets o ∈ o = {0, 1, . .
. , n 2 } 3 to avoid patching artifacts. in the numerical implementation, only a
single offset is sampled for time and memory constraints.
in this section, we evaluate the proposed conditional gan approach with a
particular focus on different content loss distance functions. all synthesis
models were trained on 250 samples acquired on 1.5t and 3t philips achieva
scanners and evaluated on 193 test cases, all collected at site 1 . further
details of the dataset, model and training can be found in the supplementary. in
our experiments, we observed that the choice of the content loss distance
function c (ŷ, y) strongly influences the performance. thus, we consider the
different cases:following johnsen et al. [16], h(x) is the vgg-16 model [32] up
to relu3_3.a qualitative comparison of the different distance functions c is
visualized in fig. 3. the first column depicts synthesized images using the 1
-norm as the distance function. these images depict a plausible contrast signal,
however, suffer from unrealistic smooth homogeneous regions. an improvement
thereof is shown by the perceptual content loss (vgg). the np-loss leads to a
further improvement not only in the contrast signal behavior but also in the
realism of the noise texture, cf. zoom regions in the lower corners.to highlight
the generalization capabilities, we depict in the bottom row of fig. 3 a sample
from site 2 , which was acquired using a philips ingenia scanner. moreover, the
gbca gadoterate was used, while our training data only consists of scans using
the gbca gadobutrol. nevertheless, all models present realistically synthesized
ld images. comparing the zooms of the ld images, we observe that our np-loss
leads to a better synthesis of noise and thereby to fig. 3. qualitative
comparison of synthesized images using different loss functions to the
corresponding reference xld. while the 1 loss yields smooth low-dose images, the
noise pattern is preserved to some extent using the vgg loss; our loss helps to
further retain the noise characteristics. more realistic ld images. in the 1 and
vgg columns, the noise is not faithfully synthesized, thus it is visually easy
to spot the enhancing pathological regions.for completeness, a quantitative
ablation of the considered distance functions on the test images of site 1 is
shown in table 1. although neither maximizing psnr nor ssim [33] is our
objective, we observe on-par performances of the perceptual (vgg) and our
proposed content loss (np) with the standard 1 distance function. using the sd
image, we define ce pixels as those pixels at which the intensity increases by
at least 10% compared to the native scan. an example of these ce regions is
illustrated in the supplementary. thus, the mean absolute error for ce pixels
(mae ce ) quantifies the enhancement behavior. further, we estimate the standard
deviation of the non-ce pixels and report the mae to the ground truth standard
deviation (mae σ ). as shown in table 1, our loss outperforms the other content
losses to a large extent on both metrics, proving its effectiveness for faithful
contrast enhancement and noise generation. further statistical analyses are
presented in the supplementary.table 1. quantitative comparison of the low-dose
synthesis methods. the central columns present metrics evaluated on the
synthesized low-dose images, whereas the right columns evaluate the effect of
purely synthesized data for training the standarddose prediction model [26].
note, that the psnr/ssim of the standard dose prediction model was always
evaluated on real ld images. the definitions of the mean absolute error on the
contrast enhancement (maece) and on the noise standard deviation (maeσ) are in
sect. next, we evaluate the effect of synthesized ld images on the performance
of a virtual contrast model (vcm). in particular, we consider the
state-of-the-art 2.5d u-net model [11,23,26], which predicts an sd image given a
corresponding native and ld image, see supplementary for further details. the
columns on the right of table 1 list the average psnr and ssim score on the real
33% ld subset of our test data from site 1 . the bottom row depicts the
performance if just real 33% ld images are used for training the vcm as an upper
bound. in contrast, the other entries on the right list the performance if only
synthesized ld images are used for training. both metrics show that the samples
synthesized using our np-loss model are superior to both 1 and vgg.to determine
the effectiveness of the ld synthesis models at different settings, we acquired
160 data samples from 1.5t and 3t philips ingenia scanners at site 2 . this site
used the gbca gadoterate, which has a lower relaxivity compared to gadobutrol
used at site 1 [15]. for 80 samples real ld images were acquired, which are used
for testing. using the vcm solely trained on the real 33% ld data of site 1
yields an average psnr and mae ce on the test samples of site 2 of 40.04 and
0.092, respectively. extending the training data for the vcm by synthesized ld
images from our model with np-loss, we get a significantly improvemed (p <
0.001) psnr score of 40.37 and mae ce of 0.075.finally, fig. 4 visualizes
synthesized ld images on the brats dataset [6] along with the associated vcm
outputs. comparing the predicted sd images xsd using 10% and 33% synthesized ld
images xld , we observe that the weakly enhancing tumor at the bottom zoom is
not preserved in the case of 10%, enabling evaluation of dose reduction methods
on known pathological regions. [6] along with the native (left) and real sd
image (right). we also included non-fractional dosage levels (17% and 47%) to
showcase the wide applicability of our algorithm. top: the tumor is well
contrasted in all xsd even for 10%. bottom: the subtle enhancement of the tumor
cannot be recovered from the 10% ld image.
in this work, we used conditional gans to synthesize contrast-enhanced images
using non-standard gbca doses. to this end, we introduced a novel
noisepreserving content loss motivated by optimal transport theory. numerous
numerical experiments showed that our content loss improves the faithful
synthesis of low-dose images. further, the performance of virtual contrast
models increases if training data is extended by synthesized images from our gan
model trained by the noise-preserving content loss.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43895-0_57.
ultrasound imaging is a very effective technique for breast lesion diagnosis,
which has high sensitivity. automatically detecting breast lesions is a
challenging problem with a potential to aid in improving the efficiency of
radiologists in ultrasound-based breast cancer diagnosis [18,21]. some of the
challenges associated with automatic breast lesion detection include blurry
boundaries and changeable sizes of breast lesions.most existing breast lesion
detection methods can be categorized into imagebased [10,11,16,17,19] and
video-based [1,9] breast lesion detection approaches. image-based breast lesion
detection approaches perform detection in each frame independently. compared to
image-based breast lesion detection approaches, methods based on videos are
capable of utilizing temporal information for improved detection performance.
for instance, chen et al. [1] exploited temporal coherence for semi-supervised
video-based breast lesion detection. recently, lin et al. [9] proposed a feature
aggregation network, termed as cva-net, that executes intra-video and
inter-video fusions at both video and clip levels based on attention blocks.
although the recent cva-net aggregates clip and video level features, we
distinguish two key issues that hamper its performance. first, the
self-attention based cross-frame feature fusion is a global-level operation and
it operates once before the encoder-decoder, thereby ignoring the useful local
information and in turn missing an effective deep feature fusion. second,
cva-net only performs one-frame prediction based on multiple frame inputs, which
is very time-consuming.to address the aforementioned issues, we propose a
spatial-temporal deformable attention based network, named stnet, for detecting
the breast lesions in ultrasound videos. within our stnet, we introduce a
spatial-temporal deformable attention module to fuse multi-scale
spatial-temporal information among different frames, and further integrate it
into each layer of the encoder and decoder. in this way, different from the
recent cva-net, our proposed stnet performs both deep and local feature fusion.
in addition, we introduce multiframe prediction with encoder feature shuffle
operation that shares the backbone and encoder features, and only perform
multi-frame prediction in the decoder. this enables us to significantly
accelerate the detection speed of the proposed approach. we conduct extensive
experiments on a public breast lesion ultrasound video dataset, named bluvd-186
[9]. the experimental results validate the efficacy of our proposed stnet that
has a superior detection performance. for example, our proposed stnet achieves a
map of 40.0% with an absolute gain of 3.9% in terms of detection accuracy, while
operating at two times faster, compared to the recent cva-net [9].
here, we describe our proposed spatial-temporal deformable attention based
framework, named stnet, for detecting breast lesions in the ultrasound videos.
figure 1(a) presents the overall architecture of our proposed stnet, which is
built on the end-to-end detector deformable detr [22]. within our stnet, we
introduce spatial-temporal deformable attention into the encoder and the
decoder. as in cva-net [9], we take six frames i k-1 , i k , i k+1 , i r1 , i r2
, i r3 from one ultrasound video as inputs, where there are three neighboring
frames i k-1 , i k , i k+1 and three randomly-selected frames i r1 , i r2 , i r3
. given these input frames, we use the backbone, such as resnet-50 [6], to
extract deep multi-scale features f k-1 , f k , f k+1 , f r1 , f r2 , f r3 .
afterwards, we introduce a spatial-temporal deformable attention based encoder
(st-encoder) to perform intra-frame and inter-frame multi-scale feature fusion.
then, we introduce a spatial-temporal deformable attention based decoder
(st-decoder) to generate output feature embeddings p k , which are fed to a
classifier and a box predictor for classification and bounding-box regression.
during inference, we take three neighboring frames and three randomly-selected
frames as the inputs, and simultaneously predict the results of three
neighboring frames using our encoder feature shuffle strategy. as a result, our
approach operates at a faster inference speed.
given a reference point, deformable attention [22] aggregates the features of a
group of key sampling points near it. compared to original transformer
selfattention [13], deformable attention has low-complexity along with a faster
convergence speed. motivated by this, we adopt deformable attention for breast
lesion detection and extend it to spatial-temporal deformable attention (stda).
our stda not only aggregates the features of current frame, but also aggregates
the features of the rest of the frames. figure 2 presents the structure of our
given a query feature and reference point, our stda not only fuses multi-scale
features within a frame, but also aggregates multi-scale features between
different frames.proposed stda. let f t = f l t l l=1 represent the set of
multi-scale feature maps at frame t, where f l t ∈ r c×h l ×w l is the feature
map at level l. given the query features p q and corresponding reference points
z q , the spatio-temporal multiscale attention is given as:where m represents
multi-head index and k is sampling point index. w m is a linear layer, a tlqk
indicates attention weight of sampling point, and δp tlqk indicates sample
offset of sampling point. φ l normalizes the coordinates p q by the scale of
feature map f l t . the sampling offset δp tlqk is predicted by the query
feature z q with a linear layer. the attention weight a tlqk is predicted by
feeding query feature z q to a linear layer and a softmax layer. as a result,
the sum of attention weights is equal to one as(compared to the standard
deformable attention, the proposed spatial-temporal deformable attention fully
exploits spatial information within frame and temporal information across
frames.
here, we integrate the proposed spatial-temporal deformable attention (stda)
into encoder and decoder (called st-encoder and st-decoder). as shown in fig.
1(b), st-encoder takes deep multi-scale feature maps f k-1 , f k , f k+1 , f r1
, f r2 , f r3 as inputs. afterwards, we employ stda to perform spatial and
temporal fusion and generate the fused multi-scale feature maps, where the query
corresponds to each pixel in multi-scale feature maps. then, the fused feature
map goes through a feed-forward network (ffn) to generate the output feature
mapssimilar to the original deformable detr, we adopt cascade structure to stack
six stda and ffn layers in st-encoder.the st-decoder takes the output feature
maps e k-1 , e k , e k+1 , e r1 , e r2 , e r3 and a set of learnable queries q ∈
r n ×c as inputs. the learnable queries first go through a self-attention layer.
afterwards, stda performs cross-attention operation between these feature maps
and the queries, where the key elements are these output feature maps of
st-encoder. then, we employ a ffn layer to generate the prediction features p k
∈ r n ×c . we also stack six self-attention, stda, and ffn layers in st-decoder
for deep feature extraction.
as discussed above, the proposed stnet adopts six frames to predict the results
of one frame. although stnet fully exploits temporal information for improved
breast lesion detection, it becomes time-consuming for multi-frame prediction.
to accelerate the detection speed, we introduce multi-frame prediction with
encoder feature shuffle during inference. instead of going through the entire
network several times, we first share deep multi-scale feature maps before
encoder and second perform the decoder several times for multi-frame prediction.
to perform multi-frame prediction only in the decoder, we propose the encoder
feature shuffle operation shown in fig. 1(d). by exchanging the order of
neighboring frame i k-1 , i k , i k+1 , the decoder can predict the results of
three neighboring frames, respectively. compared to the original stnet, the
proposed encoder feature shuffle strategy only employs decoder forward three
frames and accelerates the inference speed.
dataset. we conduct the experiments on the public bluvd-186 dataset [9],
comprising 186 videos including 112 malignant and 74 benign cases. the dataset
has totally 25,458 ultrasound frames, where the number of frames in a video
ranges from 28 to 413. the videos encompass a comprehensive tumor scan, from its
initial appearance to its largest section and eventual disappearance. all videos
were captured using philips tis l9-3 and logiq-e9. the grounding-truths in a
frame, including breast lesion bounding-boxes and corresponding categories, are
labeled by two pathologists, which have eight years of professional background
in the field of breast pathology. we adopt the same dataset splits as in table
1. state-of-the-art quantitative comparison of our approach with existing
methods in literature on the bluvd-186 dataset. our approach achieves a superior
performance on three different metrics. compared to the recent cva-net [9], our
approach obtains a gain of 3.9% in terms of overall ap. we show the best results
in bold.
type backbone ap ap50 ap75 gfl [7] image resnet-50 23.4 46.3 22.2 cascade rpn
[14] image resnet-50 24.8 42.4 27.3 faster r-cnn [12] image resnet-50 25.2 49.2
22.3 vfnet [20] image resnet-50 28.0 47.1 31.0 retinanet [8] image resnet-50
29.5 50.4 32.4dff [24] video resnet-50 25.8 48.5 25.1 fgfa [23] video resnet-50
26.1 49.7 27.0 selsa [15] video resnet-50 26.4 45.6 29.6 temporal roi align [5]
video resnet-50 29.0 49.9 33.1 mega [2] video resnet-50 32.3 57.2 35.7 cva-net
[9] video resnet-50 36. the previous work cva-net [9], to guarantee a fair
comparison. specifically, the testing set comprises 38 videos randomly selected
from all 186 videos, while the rest of the videos are used as the training
set.evaluation metrics. three commonly-used metrics are employed for performance
evaluation of breast lesion detection methods on the ultrasound videos, namely
average precision (ap), ap 50 , and ap 75 .implementation details. we employ the
resnet-50 [6] pre-trained on ima-genet [3], and use xavier [4] to initialize the
remaining network parameters. to enhance the diversity of training data, all
videos are randomly subjected to horizontal flipping, cropping, and resizing.
similar to that of cva-net, we employ a two-phase training strategy to achieve
better convergence. in the first phase, we employ adam optimizer to train the
model for 8 epochs. we then fine-tune the model for another 20 epochs with the
sgd optimizer. throughout both phases of training, we adopt the consistent
hyper-parameters, where the learning rate is 5 × 10 -5 and the weight decay is 1
× 10 -4 . we train the model on a single nvidia a100 gpu and set the batch size
as 1.
our proposed approach is compared with eleven state-of-the-art methods,
comprising image-based and video-based methods. we report the detection
performance of these state-of-the-art methods generated by cva-net [9].
specifically, cva-net acquires the detection performance of these methods by
utilizing their publicly available codes or re-implementing them if no publicly
available codes. quantitative comparisons. table 1 presents the state-of-the-art
quantitative comparison of our approach with the eleven existing breast lesion
video detection methods in literature. as a general trend, video-based methods
tend to yield higher average precision (ap), ap50, and ap75 scores compared to
image-based breast lesion detection methods. among the eleven existing methods,
the recent cva-net [9] achieves the best overall ap score of 36.1, ap50 score of
65.1, and ap75 score of 38.5. our proposed stnet method consistently outperforms
cva-net [9] on all three metrics (ap, ap50, and ap75). specifically, our stnet
achieves a significant improvement in the overall ap score from 36.1 to 40.0,
the ap50 score from 65.1 to 70.3, and the ap75 score from 38.5 to 43.3. the
significant improvement demonstrates the efficacy of our approach for detecting
breast lesions in ultrasound videos.qualitative comparisons. figure 3 presents
the qualitative breast lesion detection comparison between cva-net and our
proposed approach on an ultrasound video containing the benign breast lesions.
moreover, we show the ground truth of each frame on the third row for reference.
the first row of the figure shows that cva-net struggles to identify the breast
lesions in the second and third frames. further, although cva-net manages to
identify the breast lesions in the first and fifth frames, the classification
results are inaccurate (as highlighted by the blue rectangle in fig. 3). in
contrast, our stnet method in the second row of fig. 3 accurately detects the
breast lesions in all video frames and achieves accurate classification
performance for each frame. inference speed comparison. we present the inference
speed comparison between our proposed stnet and cva-net on an nvidia rtx 3090
gpu using the same environment. we use fps (frames per second) as the
performance metric. specifically, our proposed stnet achieves an averaged
inference speed of 21.84 fps, while cva-net achieves an averaged speed of 12.17
fps. our model operates around two times faster than cva-net, which we attribute
to the ability of our model to predict three frames simultaneously.
effectiveness of stda: to show the efficacy of our proposed stda, we perform
different ablation studies. the first baseline network, referred as "baseline +
single-frame", uses the original deformable detr and takes a single frame as
input. the second baseline network, referred as "baseline + multi-frame", uses
modified deformable detr with multi-head attention module to fuse six input
frames. for the third study, labeled "st-encoder + da-decoder", we retain the
encoder with stda in our model but replace the stda in the decoder with the
conventional deformable attention. similarly, in the fourth study, labeled
"da-encoder + st-decoder", we retain the decoder with stda in our model but
replace the stda in the encoder with the conventional deformable attention. as
shown in table 2, the results show that "st-encoder + da-decoder" and
"da-encoder + st-decoder" improve the ap by 4.7 and 5.6, respectively, compared
to "baseline + single-frame". this demonstrates that stda can effectively
perform intra-frame and inter-frame multi-scale feature fusion, even when only
partially adopted in the encoder or decoder. furthermore, our proposed stnet
improves the ap by 5.1 and 4.2 compared to "st-encoder + da-decoder" and
"da-encoder + st-decoder", respectively, indicating that the integration of stda
in both the encoder and decoder is crucial for achieving superior detection
performance.
we propose a novel breast lesion detection approach for ultrasound videos,
termed as stnet, which performs local spatial-temporal feature fusion and deep
feature aggregation in each stage of both encoder and decoder using our
spatial-temporal deformable attention module. additionally, we introduce the
encoder feature shuffle strategy that enables multi-frame prediction during
inference, thereby enabling us to accelerate the inference speed while
maintaining better detection performance. the experiments conducted on a public
breast lesion ultrasound video dataset show the efficacy of our stnet, resulting
in a superior detection performance while operating at a fast inference speed.
we believe stnet presents a promising solution and will help further promote
future research in the direction of efficient and accurate breast lesion
detection in videos.
over the past decade, we have observed the substantial success of convolutional
neural networks (cnns) in a multitude of grid-based medical imaging
applications, such as magnetic resonance imaging (mri) reconstruction [20,27]
and lesion segmentation [13,25]. despite the effectiveness of general inductive
biases like translation equivariance [15] and locality [16], the diverse nature
of the gradient field map of the qsm images presents normalized gradient vectors
(the darker the blue, the larger the gradient vector's magnitude). the two
rightmost columns display gradient magnitude maps vs and qsm value maps vu
processed by da-tr (see sect. diseases represented in medical images
necessitates highly domain-specific knowledge. consequently, the question of how
to incorporate domain-specific inductive biases, or priors, beyond general ones
into neural networks for medical image processing remains an open challenge.in
this study, we strive to answer this question by addressing the identification
problem associated with a specific type of multiple sclerosis (ms) lesion,
referred to as a chronic active lesion, or rim+ lesion. histopathology studies
characterize rim+ lesions by an iron-rich rim of activated macrophages and
microglia [2,6,9,14]. these lesions are visible with in-vivo quantitative
susceptibility mapping (qsm) [7,22] and phase imaging techniques [1,2]. notably,
they display a paramagnetic hyperintense rim at the edge (see fig. 1). despite
several efforts to tackle the issue [4,18,24], a clinically reliable solution
remains elusive.given the limited amount of data and high class imbalance, it's
more advantageous to explicitly incorporate domain knowledge into the network as
priors. as illustrated in fig. 1, rim+ lesions distinguish themselves from
rim-lesions in three primary aspects. firstly, rim+ lesions exhibit a
hyperintense ring-like structure at the lesion's edge on qsm. secondly, a higher
magnitude of gradients is observed near the edge of rim+ lesions, a feature not
present in rim-lesions. lastly, rim+ lesions are characterized by radially
oriented gradients at the edge, whereas rim-lesions lack such structured
orientations.in this work, we introduce the deep directed accumulator (deda), a
novel image processing operation. deda, symmetric to grid sampling within a
neural network's forward-backward framework, explicitly encodes the
aforementioned prior information. given a feature map and sampling grids, deda
creates an accumulator space, quantizes it into finite intervals, and
accumulates feature values. deda can also be viewed as a generalized discrete
radon transform, as it accumulates values between two discrete feature spaces.
our contributions are twofold: firstly, we present deda, a simple yet powerful
method that augments neural networks' representation capacity by explicitly
incorporating domain-specific priors. secondly, our experimental results on rim+
lesion identification demonstrate a notable improvement of 10.1% in partial area
under the receiver operating characteristic curve (proc auc) and a 10.2%
improvement in area under the precision recall curve (pr auc), outperforming
existing state-of-the-art methods.
numerous signal processing techniques, including the fourier transform, radon
transform, and hough transform, map discrete signals from image space to another
functional space. we call this new space accumulator space, where each cell's
value in the new space constitutes a weighted sum of values from all cells in
the original image space. for our purposes, an appealing feature of the
accumulator space is that local convolutions within it, like those in hough and
sinogram spaces, result in global aggregation of structural features, such as
lines, in the feature map space. this proves beneficial for incorporating
geometric priors into neural networks. differing from attention-based methods,
this convolution in accumulator space explicitly captures long-range information
through direct geometric prior parameterization.
the process of transforming an image to an accumulator space involves a critical
step, directed accumulation (da), in which a cell from the accumulator space is
pointed by multiple cells from the image space. figure 2, eq. (1) and eq. ( 3)
have shown that this da operation is a symmetric operation to the grid sampling
[12] within the forward-backward learning framework, where the backward pass of
da possesses the same structure as the forward pass of grid sampling if only one
sampling grid is given, and vice versa for the forward pass. in addition, da is
further generalized to allow multiple sampling grids to accumulate values from
the source feature map. here we briefly review the grid sampling method and then
derive the proposed deda.grid sampling: given a source feature map u ∈ r c×h×w ,
a sampling grid g ∈ r 2×h ×w = (g x , g y ) specifying pixel locations to read
from u, and a kernel function k() defining the image interpolation, then the
output value of a particular position (i, j) at the target feature map v ∈ r c×h
×w can be written as follows: where the kernel function k() can be replaced with
any other specified kernels, e.g. integer sampling kernel δ(here x + 0.5 rounds
x to the nearest integer and δ() is the kronecker delta function. the gradients
with respect to u and g for back propagation can be defined accordingly [12].the
number of grids), and a kernel function k(), the output value of a particular
position (i, j) at the target feature map v can be written as follows:it is
worth noting that the spatial dimension of the grid g[k] should be the same as
that of u, but the first dimension of g[k] can be an arbitrary number as long as
it aligns with the number of spatial dimensions of v, e.g. if givenbasically,
the deda operation in eq. ( 2) performs a tensor mapping by d : (u, g; k) → v,
where k is the sampling kernel. for simplicity, function d() will be used to
denote the deda forward for the rest of the paper.to allow back propagation for
training networks with deda, the gradients with respect to u are derived using
the chain rule as follows:the gradient tensor with respect to v is a. the
structure of eq. ( 3) reduces to eq. ( 1) when n = 1, indicating deda's symmetry
with grid sampling. given identical transformations for each channel c in deda's
forward and backward passes, we denote the feature map with spatial dimensions
alone henceforth. , where is a small real value to avoid zero denominator. the
mesh grids of u are denoted as m x (value range: (0, h -1)) and m y (value
range: (0, w -1)). we can then generate a set of sampling grids as
follows:whereand n = max(h, w ). now the deda-based transformation of rim
(da-tr) can be formulated as v s = d(s, g; k) and v u = d(u, g; k), where the
integer sampling kernel is used. it is worth noting that feature and gradient
magnitude values are accumulated separately due to differences of image
intensity and gradients between rim+ and rim-lesions (see fig. 1).
to gain more representation ability and capture long-range contextual
information, da-tr is applied to both intermediate feature maps and original
images. as can be seen from fig. 3, image patches of lesions are processed
through a set of convolutional layers with each consisting of a 3 × 3 × 3 or 1 ×
1 × 1 convolution, a batch normalization [11] and a relu activation function,
followed by a da-tr layer and a 1 × 1 × 1 convolutional layer. the first 1×1×1
conv layer is used to fuse feature maps and original image patches for better
feature embedding, and the second one is used to fuse deda transformed gradient
magnitude maps v s and feature maps v u . it is worth noting that only in-plane
rims are observed, and thus the da-tr is performed on the 2d feature map slices
along the axial direction.
for fair and consistent comparison, the dataset applied in the previous work
[24] was asked for and used to demonstrate the performance of the proposed
dedabased rim parameterization da-tr. a total of 172 subjects were included in
the dataset, and 177 lesions were identified as rim+ lesions and 3986 lesions
were identified as rim-lesions, please refer to [24] for more details about the
image acquisition and pre-processing.
comparator methods: three methods have been developed so far for rim+ lesion
identification, of which aprl [18] and rimnet [4] are on phase imaging and
qsmrim-net [24] is on qsm. in comparison with these methods, we use qsm along
with t2-flair images as the network inputs for rimnet and qsmrimnet, and use the
qsm image to extract first-order radiomic features for aprl. furthermore, we
applied residual networks (resnet) [10], vision transformer (vit) [8], swin
transformer [17], and nested transformer [28] as backbone architecture for our
application, and determined that resnet with 18 convolution layers works the
best. transformer-based networks with fewer inductive biases rely heavily on the
use of a large training dataset or depends strongly on the feature reuse [19],
as a result, these networks as well as cnns with deeper structures are prone to
overfit small datasets.implementation details: a stratified five-fold
cross-validation procedure was applied to train and validate the performance,
and all experiments including ablation study were carried out within this
setting. each lesion was cropped into patches with a fixed size of 32 × 32 × 8
voxels. random flipping, random affine transformation and random gaussian
blurring were used to augment our data. more details of the training procedure
can be found out in the supplementary materials.
lesion-wise results: to evaluate the performance of each method and produce
clinically relevant results, proc curves with false positive rates (fprs) in the
range of (0, 0.1) and pr curves of the different validation folds were
interpolated using piece-wise constant interpolation and averaged to show the
overall performance at the lesion level. for each curve, auc was computed
directly from the interpolated and averaged curves. the binary indicators of
rim+/rimlesions were generated by thresholding the model probabilities to
maximize the f 1 score, where f 1 = 2• precision • sensitivity precision +
sensitivity . in addition, accuracy, f1 score, sensitivity, specificity, and
precision were used to characterize the performance of each method. table 1 and
fig. 4 show the lesion-wise performance metrics of the proposed methods in
comparison with the other methods. da-tr-net outperformed the other competitors
in all evaluation metrics. with a slightly higher overall accuracy and
specificity with other methods, da-tr-net resulted in a 5.5%, 15.4% and 39.4%
improvement in f 1 score, 10.1%, 13.6% and 30.0% improvement in proc (fpr < 0.1)
auc, and 10.2%, 18.5% and 54.0% improvement in pr auc compared to qsmrimnet,
rimnet and aprl, respectively.
we also evaluated the performance at the subjectlevel. pearson's correlation
coefficient was used to measure the correlation model predicted count and human
expert count. mean squared error (mse) was also used to measure the averaged
accuracy for the model predicted count. figure 4a shows the scatter-plot for the
predicted count v.s. the human expert count, along with the identity line, and
the pearson's correlation coefficient (ρ) for da-tr-net was ρ = 0.93(95%ci :
0.90, 0.95) as can be seen from table 1, the pearson's correlations and mse for
the proposed da-tr-net was found higher than other competitors. this
demonstrates that the performance of da-tr-net at the subject-level is
statistically significantly higher than that of aprl, rim-net, and qsmrim-net
(table 2).
we conducted an ablation study to investigate the effects of each component
accompanied with da-tr. first, we examined the effects of applying the proposed
da-tr to the latent feature maps and raw images. second, we examined the effects
of using v u and v s , because rim+ lesions differ from rim-lesions in both
gradient magnitudes and values at the edge of the lesion. we then investigated
how multi-radius rim parameterization can affect the results, as the size of
rim+ lesions vary greatly with a radius from 5 to 15 among different subjects.
results from models #1, #2 and #4 show that the rim parametrization da-tr is
useful for rim+ identification, and da-tr used in the latent feature map space
performs even better. comparing model #3 and #4, one can see that accumulating
both gradient magnitudes and feature values is beneficial. the consistent
performance improvement from model #4 to #5 and from model #5 to #6 has
demonstrated the effectiveness of applying multi-radius rim parameterization.
more results on backbone networks can be found in the appendix.
medical images often require processing of a primary target or region of
interest (roi), such as rims, left ventricles, or tumors. these rois frequently
exhibit distinct geometric structures [26] or possess specific spatial
relationships [25] with their surroundings. capturing these characteristics
poses a challenge for modern neural networks, especially given limited and
imbalanced training data. while differentiable grid sampling [12] can tackle
some of these issues within a certain scope, another major class involving
transformations (e.g. hough transform [3]) that necessitate directed
accumulation is overlooked. our proposed deda bridges this gap, enabling the use
of image transformations with directed accumulation within a neural network.
this allows for the parametrization of geometric shapes and the modeling of
spatial correlations in a differentiable manner.while the study focuses on rim+
lesion identification, the proposed deda can be extended to other applications.
these include the utilization of polar transformation for skin lesion
recognition/segmentation, symmetric circular transformation for cardiac image
registration [23], parabola transformation for curvilinear structure
segmentation [21], and high-dimensional bilateral filtering [5].
we present deda, an image processing operation that helps parameterize rim and
effectively incorporates prior information into networks through a value
accumulation process. the experimental results demonstrate that deda surpasses
existing state-of-the-art methods in all evaluation metrics by a significant
margin. furthermore, deda's versatility extends beyond lesion identification and
can be applied in other image processing applications such as hough transform,
bilateral grid, and polar transform. we are excited about the potential of deda
to advance numerous medical applications and other image processing tasks.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43895-0 72.
deep learning techniques have achieved unprecedented success in the field of
medical image classification, but this is largely due to large amount of
annotated data [5,18,20]. however, obtaining large amounts of high-quality
annotated data is usually expensive and time-consuming, especially in the field
of pathology image processing [5,[12][13][14]18]. therefore, a very important
issue is how to obtain the highest model performance with a limited annotation
budget. the unlabeled sample pool contains k target categories (red-boxed
images) and l non-target categories (blue-boxed images). existing al methods
cannot accurately distinguish whether the samples are from the target classes or
not, thus querying a large number of non-target samples and wasting the
annotation budget, while our method can accurately query samples from the target
categories. (color figure online)active learning (al) is an effective approach
to address this issue from a data selection perspective, which selects the most
informative samples from an unlabeled sample pool for experts to label and
improves the performance of the trained model with reduced labeling cost
[1,2,9,10,16,17,19]. however, existing al methods usually work under the
closed-set assumption, i.e., all classes existing in the unlabeled sample pool
need to be classified by the target model, which does not meet the needs of some
real-world scenarios [11]. figure 1 shows an al scenario for pathology image
classification in an open world, which is very common in clinical practice. in
this scenario, the whole slide images (wsis) are cut into many small patches
that compose the unlabeled sample pool, where each patch may belong to tumor,
lymph, normal tissue, fat, stroma, debris, background, and many other
categories. however, it is not necessary to perform finegrained annotation and
classification for all categories in clinical applications. for example, in the
cell classification task, only patches of tumor, lymphatic and normal cells need
to be labeled and classified by the target model. since the nontarget patches
are not necessary for training the classifier, labeling them would waste a large
amount of budget. we call this scenario in which the unlabeled pool consists of
both target class and non-target class samples open-set al problem. most
existing al algorithms can only work in the closed-set setting. even worse, in
the open-set setting, they even query more non-target samples because these
samples tend to have greater uncertainty compared to the target class samples
[11]. therefore, for real-world open-set pathology image classification
scenarios, an al method that can accurately query the most informative samples
from the target classes is urgently needed.recently, ning et al. [11] proposed
the first al algorithm for open-set annotation in the field of natural images.
they first trained a network to detect target class samples using a small number
of initially labeled samples, and then modeled the maximum activation value
(mav) distribution of each sample using a gaussian mixture model [15] (gmm) to
actively select the most deterministic target class samples for labeling.
although promising performance is achieved, their detection of target class
samples is based on the activation layer values of the detection network which
has limited accuracy and high uncertainty with small initial training samples.in
this paper, we propose a novel al framework under an open-set scenario, and
denote it as openal, which cannot only query as many target class samples as
possible but also query the most informative samples from the target classes.
openal adopts an iterative query paradigm and uses a two-stage sample selection
strategy in each query. in the first stage, we do not rely on a detection
network to select target class samples and instead, we propose a feature-based
target sample selection strategy. specifically, we first train a feature
extractor using all samples in a self-supervised learning manner, and map all
samples to the feature space. there are three types of samples in the feature
space, the unlabeled samples, the target class samples labeled in previous
iterations, and the non-target class samples queried in previous iterations but
not being labeled. then we select the unlabeled samples that are close to the
target class samples and far from the non-target class samples to form a
candidate set. in the second stage, we select the most informative samples from
the candidate set by utilizing a model-based informative sample selection
strategy. in this stage, we measure the uncertainty of all unlabeled samples in
the candidate set using the classifier trained with the target class samples
labeled in previous iterations, and select the samples with the highest model
uncertainty as the final selected samples in this round of query. after the
second stage, the queried samples are sent for annotation, which includes
distinguishing target and non-target class samples and giving a fine-grained
label to every target class sample. after that, we train the classifier again
using all the fine-grained labeled target class samples.we conducted two
experiments with different matching ratios (ratio of the number of target class
samples to the total number of samples) on a public 9-class colorectal cancer
pathology image dataset. the experimental results demonstrate that openal can
significantly improve the query quality of target class samples and obtain
higher performance with equivalent labeling cost compared with the current
state-of-the-art al methods. to the best of our knowledge, this is the first
open-set al work in the field of pathology image analysis.
we consider the al task for pathology image classification in an open-set
scenario. the unlabeled sample pool p u consists of k classes of target samples
and l classes of non-target samples (usually, k < l). n iterative queries are
performed to query a fixed number of samples in each iteration, and the
objective is to select as many target class samples as possible from p u in each
query, while selecting as many informative samples as possible in the target
class samples. each queried sample is given to experts for labeling, and the
experts will give fine-grained category labels for target class samples, while
only giving a "non-target class samples" label for non-target class samples.
figure 2 illustrates the workflow of the proposed method, openal. openal
performs a total of n iterative queries, and each query is divided into two
stages. in stage 1, openal uses a feature-based target sample selection (ftss)
strategy to query the target class samples from the unlabeled sample pool to
form a candidate set. specifically, we first train a feature extractor with all
samples by self-supervised learning, and map all samples to the feature space.
then we model the distribution of all unlabeled samples, all labeled target
class samples from previous iterations, and all non-target class samples queried
in previous iterations in the feature space, and select the unlabeled samples
that are close to the target class samples and far from the non-target class
samples. in stage 2, openal adopts a model-based informative sample selection
(miss) strategy. specifically, we measure the uncertainty of all unlabeled
samples in the candidate set using the classifier trained in the last iteration,
and select the samples with the highest model uncertainty as the final selected
samples, which are sent to experts for annotation. after obtaining new labeled
samples, we train the classifier using all fine-grained labeled target class
samples with cross-entropy as the loss function. the ftss strategy is described
in sect. 2.2, and the miss strategy is described in sect. 2.3.
self-supervised feature representation. first, we use all samples to train a
feature extractor by self-supervised learning and map all samples to the latent
feature space. here, we adopt dino [3,4] as the self-supervised network because
of its outstanding performance.sample scoring and selection in the feature
space. then we define a scoring function on the base of the distribution of
unlabeled samples, labeled target class samples and non-target class samples
queried in previous iterations. every unlabeled sample in the current iteration
is given a score, and a smaller score indicates that the sample is more likely
to come from the target classes. the scoring function is defined in eq. 1.where
s i denotes the score of the unlabeled sample x u i . s ti measures the distance
between x u i and the distribution of features derived from all the labeled
target class samples. the smaller s ti is, the closer x u i is to the known
sample distribution of the target classes, and the more likely x u i is from a
target class. similarly, s wi measures the distance between x u i and the
distribution of features derived from all the queried non-target class samples.
the smaller s wi is, the closer x u i is from the known distribution of
non-target class samples, and the less likely x u i is from the target class.
after scoring all the unlabeled samples, we select the top ε% samples with the
smallest scores to form the candidate set. in this paper, we empirically take
twice the current iterative labeling budget (number of samples submitted to
experts for labeling) as the sample number of the candidate set. below, we give
the definitions of s ti and s wi .distance-based feature distribution modeling.
we propose a category and mahalanobis distance-based feature distribution
modeling approach for calculating s ti and s wi . the definitions of these two
values are slightly different, and we first present the calculation of s ti ,
followed by that of s wi .for all labeled target class samples from previous
iterations, their fine-grained labels are known, so we represent these samples
as different clusters in the feature space according to their true class labels,
where a cluster is denoted as c l t (t = 1, . . . , k). next, we calculate the
score s ti for z u i using the mahalanobis distance (md) according to eq. 2. md
is widely used to measure the distance between a point and a distribution
because it takes into account the mean and variance of the distribution, which
is very suitable for our scenario.where d(•) denotes the md function, μ t and σ
t are the mean and covariance of the samples in the target class t, and nom(•)
is the normalization function. it can be seen that s ti is essentially the
minimum distance of the unlabeled sample x u i to each target class cluster. for
all the queried non-target class samples from previous iterations, since they do
not have fine-grained labels, we first use the k-means algorithm to cluster
their features into w classes, where a cluster is denoted as c l w (w = 1, . . .
, w ).w is set to 9 in this paper. next, we calculate the score s wi for z u i
using the md according to eq. 4.) where μ w and σ w are the mean and covariance
of the non-target class sample features in the wth cluster. it can be seen that
s wi is essentially the minimum distance of z u i to each cluster of known
non-target class samples. the within-cluster selection and dynamic cluster
changes between rounds significantly enhance the diversity of the selected
samples and reduce redundancy.
to select the most informative samples from the candidate set, we utilize the
model-based informative sample selection strategy in stage 2. we measure the
uncertainty of all unlabeled samples in the candidate set using the classifier
trained in the last iteration and select the samples with the highest model
uncertainty as the final selected samples. the entropy of the model output is a
simple and effective way to measure sample uncertainty [7,8]. therefore, we
calculate the entropy of the model for the samples in the candidate set and
select 50% of them with the highest entropy as the final samples in the current
iteration.
to validate the effectiveness of openal, we conducted two experiments with
different matching ratios (the ratio of the number of samples in the target
class to the total number of samples) on a 9-class public colorectal cancer
pathology image classification dataset (nct-crc-he-100k) [6]. the dataset
contains a total of 100,000 patches of pathology images with fine-grained
labeling, with nine categories including adipose (adi 10%), background (back
11%), debris (deb 11%), lymphocytes (lym 12%), mucus (muc 9%), smooth muscle
(mus 14%), normal colon mucosa (norm 9%), cancer-associated stroma (str 10%),
and colorectal adenocarcinoma epithelium (tum, 14%). to construct the openset
datasets, we selected three classes, tum, lym and norm, as the target classes
and the remaining classes as the non-target classes. we selected these target
classes to simulate a possible scenario for pathological cell classification in
clinical practice. technically, target classes can be randomly chosen. in the
two experiments, we set the matching ratio to 33% (3 target classes, 6
non-target classes), and 42% (3 target classes, 4 non-target classes),
respectively.metrics. following [11], we use three metrics, precision, recall
and accuracy to compare the performance of each al method. we use precision and
recall to measure the performance of different methods in target class sample
selection.as defined in eq. 5, precision is the proportion of the target class
samples among the total samples queried in each query and recall is the ratio of
the number of the queried target class samples to the number of all the target
class samples in the unlabeled sample pool.where k m denotes the number of
target class samples queried in the mth query, l m denotes the number of
non-target class samples queried in the mth query, and n target denotes the
number of target class samples in the original unlabeled sample pool. obviously,
the higher the precision and recall are, the more target class samples are
queried, and the more effective the trained target class classifier will be. we
measure the final performance of each al method using the accuracy of the final
classifier on the test set of target class samples.competitors. we compare the
proposed openal to random sampling and five al methods, lfosa [11], uncertainty
[7,8], certainty [7,8], coreset [17] and ra [20], of which only lfosa [11] is
designed for open-set al. for all al methods, we randomly selected 1% of the
samples to label and used them as the initial labeled set for model
initialization. it is worth noting that the initial labeled samples contain
target class samples as well as non-target class samples, but the non-target
class samples are not fine-grained labeled. after each query round, we train a
resnet18 model of 100 epochs, using sgd as the optimizer with momentum of 0.9,
weight decay of 5e-4, initial learning rate of 0.01, and batchsize of 128. the
annotation budget for each query is 5% of all samples, and the length of the
candidate set is twice the budget for each query. for each method, we ran four
experiments and recorded the average results for four randomly selected seeds.
figure 3 a and b show the precision, recall and model accuracy of all comparing
methods at 33% and 42% matching ratios, respectively. it can be seen that openal
outperforms the other methods in almost all metrics and all query numbers
regardless of the matching ratio. particularly, openal significantly outperforms
lfosa [11], which is specifically designed for open-set al. the inferior
performance of the al methods based on the closed-set assumption is due to the
fact that they are unable to accurately identify more target class samples, thus
wasting a large amount of annotation budget. although lfosa [11] utilizes a
dedicated network for target class sample detection, the performance of the
detection network is not stable when the number of training samples is small,
thus limiting its performance. in contrast, our method uses a novel
feature-based target sample selection strategy and achieves the best
performance. upon analysis, our openal is capable of effectively maintaining the
balance of sample numbers across different classes during active learning. we
visualize the cumulative sampling ratios of openal for the target classes in
each round on the original dataset with a 33% matching ratio, as shown in fig.
4a. additionally, we visualize the cumulative sampling ratios of the lfosa
method on the same setting in fig. 4b. it can be observed that in the first 4
rounds, lym samples are either not selected or selected very few times. this
severe sample imbalance weakens the performance of lfosa compared to random
selection initially. conversely, our method selects target class samples with a
more bal-anced distribution. furthermore, we constructed a more imbalanced
setting for the target classes lym (6000 samples), norm (3000 samples), and tum
(9000 samples), yet the cumulative sampling ratios of our method for these three
target classes remain fairly balanced, as shown in fig. 4c.
to further validate the effectiveness of each component of openal, we conducted
an ablation test at a matching ratio of 33%. figure 3c shows the results, where
w/o s w indicates that the distance score of non-target class samples is not
used in the scoring of feature-based target sample selection (ftss), w/o s t
indicates that the distance score of target class samples is not used, w/o miss
means no model-based informative sample selection is used, i.e., the length of
the candidate set is directly set to the annotation budget in each query, and
only miss means no ftss strategy is used, but only uncertainty is used to select
samples.it can be seen that the distance modeling of both the target class
samples and the non-target class samples is essential in the ftss strategy, and
missing either one results in a decrease in performance. although the miss
strategy does not significantly facilitate the selection of target class
samples, it can effectively help select the most informative samples among the
samples in the candidate set, thus further improving the model performance with
a limited labeling budget. in contrast, when the samples are selected based on
uncertainty alone, the performance decreases significantly due to the inability
to accurately select the target class samples. the above experiments demonstrate
the effectiveness of each component of openal.
in this paper, we present a new open-set scenario of active learning for
pathology image classification, which is more practical in real-world
applications. we propose a novel al framework for this open-set scenario,
openal, which addresses the challenge of accurately querying the most
informative target class samples in an unlabeled sample pool containing a large
number of non-target samples. openal significantly outperforms state-of-the-art
al methods on real pathology image classification tasks. more importantly, in
clinical applications, on one hand, openal can be used to query informative
target class samples for experts to label, thus enabling better training of
target class classifiers under limited budgets. on the other hand, when applying
the classifier for future testing, it is also possible to use the feature-based
target sample selection strategy in the openal framework to achieve an open-set
classifier. therefore, this framework can be applied to both datasets containing
only target class samples and datasets also containing a large number of
non-target class samples during testing.
people perceive the world with signals from different modalities, which often
carry complementary information about varying aspects of an object or event of
interest. therefore, collecting and utilizing multimodal information is crucial
for artificial intelligence to understand the world around us. data collected
from various sensors (e.g., microphones, cameras, motion controllers) are used
to identify human activity [4]. moreover, multimodal medical images obtained
from different scanning protocols (e.g., computed tomography, magnetic resonance
imaging) are employed for disease diagnosis [12]. satisfactory performances have
been achieved with these multimodal data.in practical application, however,
modality missing is a common scenario. wirelessly connected sensors may
occasionally disconnect and temporarily be unable to send any data [3]. medical
images may be missing due to artifacts and diverse patient conditions [11]. in
these unexpected situations, any combinatorial subset of available modalities
can be given as input. to handle this, one intuitive solution is to train a
dedicated model on all possible subsets of available modalities [6,14,23].
however, these methods are ineffective and timeconsuming. another way is to
predict missing modalities and perform with the completed modalities [20]. but,
these approaches also require additional prediction networks for each missing
situation, and the quality of the recovered data directly affects the
performance, especially when there are only a few available modalities.
recently, fusing the available modalities into a shared representation received
wide attention. however, it is particularly challenging due to the varying
number of input modalities, which results in the n-to-one fusion
problem.currently, existing fusion strategies to tackle this challenge can be
broadly grouped into three categories: the arithmetic strategy, the selection
strategy and the convolution strategy. as shown in fig. 1(a), in the arithmetic
strategy, feature representations of available modalities are merged by an
arithmetic function, such as averaging, computing the first and second moments
or other designed formulas [10,13,17]. for the selection strategy, as shown in
fig. 1(b), each value of fused representation is selected from the values at the
corresponding position of the inputs. the selection rule can be defined as max,
min or probabilitybased [2,8,19]. although the above two fusion strategies are
easily scalable to various data missing situations, their fusion operation is
hard-coded. all available modalities contribute equally and their latent
correlations are neglected. unlike hard-coding the fusion operation, in the
convolution strategy, the convolutional fusion network automatically learns how
to fuse these feature representations, which is beneficial to exploiting the
correlation between multiple modalities. however, as shown in fig. 1(c), this
fusion strategy needs a constant number of data to meet the requirements of the
input channels in the convolutional network. therefore, it has to simulate
missing data by crudely zero-padding or replacing it with similar modalities,
which inevitably introduces a bias in computation and causes performance
degradation [5,18,25].transformer has achieved success in the field of computer
vision, demonstrating that self-attention mechanism has the ability to capture
the latent correlation of image tokens. however, no work has explored the
effectiveness of self-attention mechanism on the n-to-one fusion, where n is
variable during training, rather than fixed. furthermore, the calculation of
self-attention does not require a fixed number of tokens as input, which
represents a potential for handling missing data. therefore, we propose a
self-attention based fusion block (sfusion) to tackle the problems of the above
fusion strategies. as shown in fig. 1(d), sfusion can handle any number of input
data instead of fixing its number. in addition, sfusion is a learning-based
fusion strategy that consists of two components: the correlation extraction (ce)
module and the modal attention (ma) module. in the ce module, feature
representations extracted from available modalities are projected as tokens and
fed into the self-attention layers to learn multimodal correlations. based on
these correlations, a modal softmax function is proposed to generate weight maps
in the ma module. finally, it builds a shared feature representation by fusing
the varying inputs with the weight maps.the contributions of this work are:-we
propose sfusion, which is a data-dependent fusion strategy without impersonating
missing modalities. it can learn the latent correlations between different
modalities and builds a shared representation adaptively. -the sfusion is not
limited to specific deep learning architectures. it takes inputs from any kind
of upstream processing model and serves as the input of the downstream decision
model, which enables applying the sfusion to various backbone networks for
different tasks. -we provide qualitative and quantitative performance
evaluations on activity recognition with the shl [22] dataset and brain tumor
segmentation with the brats2020 [1] dataset. the results show the superiority of
sfusion over competing fusion strategies.
for multiple modalities, let k ∈ k ⊆ {1, 2, . . . , s} index a specific
modality, within the available modality set of k, where s is the number of all
possible modalities. given an input f k ∈ r b×c×r f , b and c denote the batch
size and the number of channels, respectively. r f represents the shape of
feature representation extracted from the k-th modality of a sample data, which
can be 1d (l), 2d (h×w), 3d (d×h×w) or higher-dimensional. in addition, i = {f k
|k ∈ k} denotes the input set of feature representations from all the available
modalities. our goal is to learn a fusion function f that can project i into a
shared feature representation f s , denoted as f (i) → f s . to achieve the
goal, we design an n-to-one fusion block, sfusion. the architecture is shown in
fig. 2, which consists of two modules: correlation extraction (ce) module and
modal attention (ma) module.
given the feature representationthen, we obtain the concatenation of all the
tokens z 0 ∈ r b×t ×c , where t = r × |k|, and |k| denotes the number of
available modalities. given z 0 , the stack of eight self-attention layers (sal)
are introduced to learn the latent multimodal correlations. each layer includes
a multi-head attention (mha) block and a fully connected feed-forward network
(ffn) [21]. layer normalization (ln) is applied before every block. the outputs
of the x-th (x ∈ [1, 2, . . . , 8]) layer can be describe as:(1)therefore, we
get z l ∈ r b×t ×c , which is the last sal output. by reverting z l to the size
of |k| × b × c × r f , we obtain the output i = {f k |k ∈ k} of ce as:where r(•)
and split(•) are the reshape and split operations, and i is the set of
calculated feature representations f k ∈ r b×c×r f which contains multimodal
correlations and has the same size as the original input f k .
given the calculated feature representations set i , the weight map m k is
generated with the modal attention mechanism. feature representations extracted
from different modalities are expected to have different weights for fusion at
the voxel level. therefore, we introduce a modal-wise and voxel-level softmax
function to generate the weight maps from i , as shown in fig. 3. we denote the
i-th voxel of f k and m k as v i k and m i k , respectively. e is the natural
logarithm. the value of weight map m k can be defined as:by element-wise
multiplying input feature map f k with the corresponding weight map m k and
summing all the modalities, we can obtain a fused feature map f s as:since the
sum of m i 1 , . . . m i |k| is 1, the value range of fused feature
representation f s remains stable to improve the robustness for variable input
modalities. moreover, the relative sizes of v i 1 , . . . v i |k| (contain the
latent multi-modal correlations learned from the ce module) are retained in the
corresponding weights. in particular, when only one modality is available, all
the values of the weight map are 1, which means f s = f k (k ∈ k, |k| = 1). in
this case, the input feature representation remains unchanged. it enables the
backbone network (the upstream processing model and the downstream decision
model) to enhance its capability to encode and decode information from different
modalities rather than relying on a particular one. it is crucial for variable
multimodal data analysis.
shl2019. the shl (sussex-huawei locomotion) challenge 2019 [22] dataset provides
data from seven sensors of a smartphone to recognize eight modes of locomotion
and transportation (activities), including still, walking, run, bike, car, bus,
train, and subway. the sensor data are collected from smartphones of a person
with four locations, including the bag, trousers front pocket, breast pocket and
hand. each location is called "bag", "hips", "torso", and "hand", respectively.
data acquired from the locations except the "hand" are given in the train
subset, while the validation subset provides the data of all four locations. in
the test subset, only unlabeled "hand" location data are available.brats2020.
the brats2020 [1] dataset provide four modality scans: t1ce, t1, t2, flair for
brain tumor segmentation. it contains 369 subjects. to better represent the
clinical application tasks, there are three mutually inclusive tumor regions:
the enhancing tumor (et), the tumor core (tc), and the whole tumor (wt) [1]. we
select 70% data as training data, while 10% and 20% as validation and test data
respectively. to prevent overfitting, two data augmentation techniques (randomly
flip the axes and rotate with a random angle in [-10 • , 10 • ]) are applied
during training. we apply z-score normalization [15] to the volumes individually
and randomly crop 128×128×128 patches as inputs to the networks.
embracenet. in the experiments on activity recognition, we compare sfusion with
embracenet [9], which employs a selection strategy (shown in fig. 1 (b)) by
generating feature masks (r 1 , r 2 , . . . , r 7 ) with the rule of giving
equal chances to all available modalities during each value selection. for a
fair comparison, as shown in fig. 4 (a), we adopt the same processing (p) and
decision (d) model as used in [9]. we obtain the performance of our fusion
strategy by replacing embracenet with sfusion. following [9] setting, the batch
size is set to 8. a crossentropy loss and the adam optimization method [16] with
β 1 = 0.9, β 2 = 0.999 are employed. the learning rate is initially set to 1 ×
10 -4 and reduced by a factor of 2 at every 1 × 10 5 steps. a total of 5 × 10 5
training steps are executed.
in the experiments on brain tumor segmentation, we compare sfusion with a gated
feature fusion block (gff) [5], which belongs to the convolution strategy (shown
in fig. 1(c)). as shown in fig. 4 (b), a feature disentanglement architecture is
employed. multimodal medical images are decomposed into the modality-invariant
content and the modality-specific appearance code by encoders e c and e a ,
respectively. the content codes (e.g., c 2 and c 3 , shown in fig. 4 (b)) of
missing modalities are simulated with zero values. then, all content codes are
fused into a shared representation c s by gff. given c s , the tumor
segmentation results are generated by the decoder d s . for a fair comparison,
we adopt the same encoders (e c i and e a i ) and decoders (d s and d r i ) as
used in [5]. we obtain the performance of our fusion strategy by replacing gff
with sfusion and removing the zero-padding operation. the training max_epoch is
set to 200. following [5] setting, the batch size is set to 1. adam [16] is
utilized with a learning rate of 1 × 10 -4 and progressively multiplies it by (1
-epoch / max_epoch) 0.9 . losses of l kl , l rec and l seg are employed as [5].
during training, to simulate real missing modalities scenarios, each training
patient's data is fixed to one of 15 possible missing cases. for a comprehensive
evaluation, we test the performance of all 15 cases for each test patient.our
implementations are on an nvidia rtx 3090(24g) with pytorch 1.8.1.
activity recognition. we compare sfusion with the embracenet [9] on shl2019. as
shown in table 1, we also compare the results of other fusion methods, which use
the same processing (p) model and decision (d) model as [9]. (1) in the early
fusion method, the data of seven sensors are concatenated along their c
dimension. the prediction results are obtained by inputting the concatenation
into a network of p and d in series. (2) for the intermediate fusion approach,
the embracenet is replaced with the concatenation of feature representations
along their r f dimension. (3) in the late fusion method, an independent network
of p and d in series is trained for each sensor, and then the decision is made
from the averaged softmax outputs. (4) in the confidence fusion model, the
embracenet is replaced with the confidence calculation and fusion layers in [7].
the results of different fusion methods on the validation data are presented in
table 1. our proposed sfusion outperforms the embracenet in all four smartphone
locations and improves the overall accuracy from 65.22% to 67.47%.brain tumor
segmentation. the quantitative segmentation results are shown in table 2.
compared with gff, the network integrated with sfusion achieves better average
performance over the 15 possible combinations in all three tasks. in particular,
sfusion outperforms gff for all the possible combinations in tc segmentation.
overall, sfusion achieves better dice scores in most situations (13,15,13
situations for wt, tc and et segmentation, respectively). in addition, we
conduct the statistical significance analysis. the number of situations with
significant improvement are 6, 10 and 8 for wt, tc and et, respectively. it is
provided by a wilcoxon test (p-values < 0.05). besides, we find no significant
drop in performance caused by sfusion. in addition, we compare the sf_fdgf
(where gff is replaced by sfusion) with current state-of-the-art methods. table
4 presents the average dice of 15 situations. for a fair comparison, we conduct
experiments on brats2018, adopt the same data partition as [24], and cite the
results in [24]. sf_fdgf achieves the best performance and verifies the
effectiveness of the sfusion.ablation experiments. the correlation extraction
(ce) module and the modal attention (ma) module are two key components in
sfusion. we evaluate the sfusion without ce and ma, respectively. sfusion
without ce denotes that feature representations are directly fed into the ma
module (fig. 2). sfusion without ma means that we directly add the calculated
feature representations (i ) up to get the fusion result. as shown in table 1,
we can find that sfusion without ce performs worse than other methods. compared
with embracenet, the improvement of sfusion without ma is inconspicuous. as
shown in table . 3, we present the averaged performance over the 15 possible
combinations on brats2020. it shows that both the ce and ma module lead to
performance improvement across all the tumor regions. therefore, ablation
experiments on two different tasks show that both ce and ma play an important
role in sfusion.
in this paper, we propose a self-attention based n-to-one fusion block sfusion
to tackle the problem of multimodal missing modalities fusion. as a
data-dependent fusion strategy, sfusion can automatically learn the latent
correlations between different modalities and builds a shared feature
representation. the entire fusion process is based on available data without
simulating missing modalities. in addition, sfusion has compatibility with any
kind of upstream processing model and downstream decision model, making it
universally applicable to different tasks. we show that it can be integrated
into existing backbone networks by replacing their fusion operation or block to
improve activity recognition and achieve brain tumor segmentation performance.
in particular, by integrating with sfusion, sf_fdgf achieves the
state-of-the-art performance. in the future, we will explore other tasks related
to variable multimodal fusion with sfusion.
embracenet †[9] 63.68 67.98 81.58 47.63 65.22 sfusion 67.41 68.91 85.22 48.35
67.47 sfusion w/o ce 56.82 63.14 74.69 46.70 60.33 sfusion w/o ma 65.01 67.95
83.49 47.52 65.99
automated image segmentation is a fundamental task in many medical imaging
applications, such as diagnosis [24], treatment planning [6], radiation therapy
planning, and tumor resection surgeries [7,12]. in the current literature,
numerous fully-supervised deep learning (dl) methods have become dominant in the
medical image segmentation task [5,16,18]. they can achieve their full potential
when trained on large amounts of fully annotated data, which is often
unavailable in the medical domain. medical data annotation requires expert
knowledge, and exhaustive labor, especially for volumetric images [17].
moreover, supervised dl-based methods are not sufficiently generalizable to
previously unseen classes.to address these limitations, few-shot segmentation
(fss) methods have been proposed [21][22][23]25], that segment an unseen class
based on just a few annotated samples. the main fss approaches use the idea of
meta-learning [9,11,13] and apply supervised learning to train a few-shot model.
however, to avoid overfitting and improve the generalization capability of fss
models, they rely on a large number of related tasks or classes. this can be
challenging as it may require a large amount of annotated data, which may not
always be available. although some works on fss techniques focus on training
with fewer data [4,20,26], they require re-training before applying to unseen
classes. to eliminate the need for annotated data during training and
re-training on unseen classes, some recent works have proposed self-supervised
fss methods for 3d medical images which use superpixel-based pseudo-labels as
supervision during training [8,19]. these methods design their self-supervised
tasks (support-query pairs) by applying a predefined transformation (e.g.,
geometric and intensity transformation) on a support image (i.e., a random slice
of a volume) to synthetically form a query one. thus, these methods do not take
into account intra-volume information and context that may be important for the
accurate segmentation of volumetric images during inference.we propose a novel
volume-informed self-supervised approach for few-shot 3d segmentation
(visa-fss). generally, visa-fss aims to exploit information beyond 2d image
slices by learning inter-slice information and continuous shape changes that
intrinsically exists among consecutive slices within a 3d image. to this end, we
introduce a novel type of self-supervised tasks (see sect. 2.2) that builds more
varied and realistic self-supervised fss tasks during training. besides of
generating synthetic queries (like [19] by applying geometric or intensity
transformation on the support images), we also utilize consecutive slices within
a 3d volume as support and query images. this novel type of task generation (in
addition to diversifying the tasks) allows us to present a 2.5d loss function
that enforces mask continuity between the prediction of adjacent queries. in
addition, to provide pseudo-labels for consecutive slices, we propose the
superpixel propagation strategy (spps). it propagates the superpixel of a
support slice into query ones by using flow field vectors that exist between
adjacent slices within a 3d image. we then introduce a novel strategy for
volumetric segmentation during inference that also exploits inter-slice
information within query volumes. it propagates a segmentation mask among
consecutive slices using the few-shot segmenter trained by visa-fss.
comprehensive experiments demonstrate the superiority of our method against
state-of-the-art fss approaches.
in this section, we introduce our proposed visa-fss for 3d medical image
segmentation. our method goes beyond 2d image slices and exploits intra-volume
information during training. to this end, visa-fss designs more varied and
realistic self-supervised fss tasks (support-query pairs) based on two types of
transformations: 1) applying a predefined transformation (e.g., geometric and
intensity transformation as used in [8,19]) on a random slice as support image
to synthetically make a query one, 2) taking consecutive slices in a 3d volume
as support and query images to learn continuous shape transformation that exists
intrinsically between consecutive slices within a volumetric image (see sect.
2.2). moreover, the volumetric view of task generation in the second type of
tasks allows us to go beyond 2d loss functions. thus, in sect. 2.2, we present a
2.5d loss function that enforces mask continuity between the prediction of
adjacent queries during training the few-shot segmenter. in this way, the
trained few-shot segmenter is able to effectively segment a new class in a query
slice given a support slice, regardless of whether it is in a different volume
(due to learning the first type of tasks) or in the same query volume (due to
learning the second type of tasks). finally, we propose a volumetric
segmentation strategy for inference time which is elaborated upon in sect. 2.3.
in fss, a training dataset d tr = {(x i , y i (l))} ntr i=1 , l ∈ l tr , and a
testing dataset d te = {(x i , y i (l))} nte i=1 , l ∈ l te are available, where
(x i , y i (l)) denotes an imagemask pair of the binary class l. l tr and l te
are the training and testing classes, respectively, and l tr ∩ l te = ∅. the
objective is to train a segmentation model on d tr that is directly applicable
to segment an unseen class l ∈ l te in a query image, x q ∈ d te , given a few
support set {(x j s , y j s (l))} p j=1 ⊂ d te . here, q and s indicate that an
image or mask is from a query or support set. to simplify notation afterwards,
we assume p = 1, which indicates the number of support images. during training,
a few-shot segmenter takes a support-query pair (s, q) as the input data, where
q = {(x i q , y i q (l))} ⊂ d tr , and s = {(x j s , y j s (l))} ⊂ d tr . then,
the model is trained according to the cross-entropy loss on each support-query
pair as follows: l(θ) = -log p θ (y q |x q , s). in this work, we model p θ (y q
|x q , s) using the prototypical network introduced in [19], called alpnet.
however, the network architecture is not the main focus of this paper, since our
visa-fss framework can be applied to any fss network. the main idea of visa-fss
is to learn a few-shot segmenter according to novel tasks designed in sect. 2.2
to be effectively applicable for volumetric segmentation.
there is a large level of information in a 3d medical image over its 2d image
slices, while prior fss methods [8,19] ignore intra-volume information for
creating their self-supervised tasks during training, although they are finally
applied to segment volumetric images during inference. previous approaches
employ a predefined transformation (e.g., geometric and intensity) to form
support-query pairs. we call these predefined transformations as synthetic
transformations. on the other hand, there is continuous shape transformation
that intrinsically exists between consecutive slices within a volume (we name
them realistic transformation). visa-fss aims to, besides synthetic
transformations, exploit realistic ones to learn more varied and realistic
tasks. figure 1 outlines a graphical overview of the proposed visa-fss
framework, which involves the use of two types of selfsupervised fss tasks to
train the few-shot segmenter. the two types of tasks are synthetic tasks and
realistic tasks: synthetic tasks. in the first type, tasks are formed the same
as in [8,19]. for each slice x i s , its superpixels are extracted by the
unsupervised algorithm [10], and its pseudo-mask is generated by randomly
selecting one of its superpixels as a pseudo-organ. thus, the support is formed
as s = (x i s , y i s (l)) ⊂ d tr , where l denotes the chosen superpixel. then,
after applying a random synthetic transformation t on s, the synthetic query
will be prepared, i.e., q s = (x i q , y i q (l)) = (t (x i s ), t (y i s (l))).
in this way, the (s, q s ) pair is taken as the input data of the few-shot
segmenter, presenting a 1-way 1-shot segmentation problem. a schematic view of a
representative (s, q s ) pair is given in the blue block of fig. 1.realistic
tasks. to make the second type of task, we take 2m adjacent slices of the
support image x i s , as our query images {x j q } j∈n (i) , where n (i) = {i-m,
..., i-1, i+ 1, i+ m}. these query images can be considered as real deformations
of the support image. this encourages the few-shot segmenter to learn
intra-volume information contrary to the first type of task. importantly,
pseudo-label generation for consecutive slices is the main challenge. to solve
this problem, we introduce a novel strategy called spps that propagates the
pseudo-label of the support image into query ones. specifically, we
consecutively apply flow field vectors that exist between adjacent image slices
on y i s (l) to generate pseudo-label y j q (l) as follows:) for j > m, and) for
j < m, where φ(x i , x j ) is the flow field vector between x i and x j , which
can be computed by deformably registering the two images using voxelmorph [2] or
vol2flow [3]. a schematic illustration of pseudo-label generation using spps is
depicted in the supplementary materials. the yellow block in fig. 1 demonstrates
a representative (s, q r ) pair formed using realistic tasks, where q r = {(x j
q , y j q (l))} j∈n (i) .loss function. the network is trained end-to-end in two
stages. in the first stage, we train the few-shot segmenter on both types of
synthetic and realistic tasks using the segmentation loss employed in [19] and
regularization loss defined in [25], which are based on the standard
cross-entropy loss. specifically, in each iteration, the segmentation loss l seg
can be followed as:, which is applied on a random query x i q (formed by
synthetic or realistic transformations) to predict the segmentation mask ŷi q
(l), where l ∈ l tr . the regularization loss l reg is defined to segment the
class l in its corresponding support image x i s , as follows:overall, in each
iteration, the loss function during the first-stage training isin the second
stage of training, we aim to exploit information beyond 2d image slices in a
volumetric image by employing realistic tasks.to this end, we define the 2.5d
loss function, l 2.5d , which enforces mask continuity among the prediction of
adjacent queries. the proposed l 2.5d profits the dice loss [18] to measure the
similarity between the predicted mask of 2m adjacent slices of the support image
x i s as follows:specifically, the loss function compares the predicted mask of
a query slice with the predicted mask of its adjacent slice and penalizes any
discontinuities between them. this helps ensure that the model produces
consistent and coherent segmentation masks across multiple slices, improving the
overall quality and accuracy of the segmentation. hence, in the second-stage
training, we train the network only on realistic tasks using the loss function:,
where λ 1 is linearly increased from 0 to 0.5 every 1000th iteration during
training. finally, after self-supervised learning, the few-shot segmenter can be
directly utilized for inference on unseen classes.
during inference, the goal is to segment query volumes based on a support volume
with only a sparse set of human-annotated slices, while the few-shot segmenter
is trained with 2d images. to evaluate 2d segmentation on 3d volumetric images,
we take inspiration from [21] and propose the volumetric segmentation
propagation strategy (vsps). assume, x s = {x 1 s , x 2 s , ..., x ns s } and x
q = {x 1 q , x 2 q , ..., x nq q } denote support and query volumes, comprising
of n s and n q consecutive slices, respectively. we follow the same setting as
[8,19,21] in which slices containing semantic class l are divided into k
equally-spaced groups, including [x 1 s , x 2 s , ..., x k s ] in the support,
and [x 1 q , x 2 q , ..., x k q ] in the query volume, where x k indicates the
set of slices in the k th group. suppose, in each of the k groups in the support
volume, the manual annotation of the middle slice [(x c s ) 1 , (x c s ) 2 ,
..., (x c s ) k ] are available as in [8,19,21]. for volumetric segmentation,
previous methods [8,19,21], for each group k ∈ {1, ..., k}, pair the annotated
center slice in the support volume with all the unannotated slices of the
corresponding group in the query volume. more precisely, ((x c s ) k , (y c s )
k ) is considered as the support for all slices in x k q , where (y c s ) k is
annotation of the center slice (x c s ) k . finally, they use the 2d few-shot
segmenter to find the mask of each of the query slices individually and
therefore segment the whole query volume accordingly. in this work, we exploit
the vsps algorithm, which is based on two steps. in the first step, an
inter-volume task is constructed to segment the center slice of each group in
the query volume. more precisely, the center slice of each query group, (x c q )
k , is segmented using ((x c s ) k , (y c s ) k ) as the support. then, by
employing the volumetric view even in the inference time, we construct
intra-volume tasks to segment other slices of each group. formally, vsps
consecutively segments each (x j q ) k ∈ x k q , starting (x c q ) k , with
respect to the image-mask pair of its previous slice, i.e., ((x j-1 q ) k , (ŷ
j-1 q ) k ).in fact, we first find the pseudo-mask of (x c q ) k using the 2d
few-shot segmenter and consequently consider this pseudo-annotated slice as the
support for all other slices in x k q . it is worth mentioning that our task
generation strategy discussed in sect. 2.2 is capable of handling such
intra-volume tasks. further details of the vsps algorithm are brought in the
supplementary materials.
to unify experiment results, we follow the evaluation protocol established by
[19], such as hyper-parameters, data preprocessing techniques, evaluation metric
(i.e., dice score), and compared methods. the architecture and implementation of
the network are exactly the same as developed in [19]. moreover, during
inference, a support volume with 3 annotated slices (i.e., k = 3) is used as a
reference to segment each query volume, the same as in [19]. also, we set m = 3,
taking 3 adjacent slices of the support image as consecutive query images.
however, the effect of this hyper-parameter is investigated in the supplementary
materials.dataset. following [8,19], we perform experiments on two common
medical benchmarks, including abdominal ct image scans from miccai 2015
multi-atlas abdomen labeling challenge [15] and abdominal mri image scans from
isbi 2019 combined healthy abdominal organ segmentation challenge [14]. in
addition, in all experiments, average results are reported according to 5-fold
cross-validation on four anatomical structures the same as in [8,19], including
left kidney (lk), right kidney (rk), spleen, and liver.
comparison with existing approaches. table 1 compares visa-fss with
state-of-the-art fss methods in terms of dice, including: vanilla panet [25],
se-net [21], ssl-rpnet [23], ssl-alpnet [19], and crapnet [8]. vanilla panet and
se-net are baselines on natural and medical images, respectively, which utilize
manual annotations for training. ssl-rpnet, ssl-alpnet, and crap-net are
self-supervised methods that construct their fss tasks using synthetic
transformations (e.g., geometric and intensity) in the same way, and are only
different in the network architecture. as demonstrated, visa-fss outperforms
vanilla panet and se-net without using any manual annotation in its training
phase. moreover, the performance gains of visa-fss compared with ssl-rpnet,
ssl-alpnet, and crapnet highlight the benefit of learning continuous shape
transformation among consecutive slices within a 3d image for volumetric
segmentation. also, the performance of visa-fss was evaluated using hausedorff
distance and surface dice metrics on ct and mri datasets. on the ct dataset,
visa-fss reduced sslalpnet's hausedorff distance from 30.07 to 23.62 effect of
task generation. to investigate the effect of realistic tasks in selfsupervised
fss models, we perform an ablation study on the absence of this type of task.
the experiment results are given in rows (a) and (b) of table 2. as expected,
performance gains can be observed when both synthetic and realistic tasks are
employed during training. this can highlight that the use of more and diverse
tasks improves the performance of fss models. of note, to generate pseudo-label
for consecutive slices, instead of spps, we can also employ supervoxel
generation strategies like the popular slic algorithm [1]. however, we observed
that by doing so the performance is 66.83 in the term of mean dice score,
under-performing spps (row (b) in table 2) by about 8%. it can be inferred that
contrary to slic, spps implicitly takes pseudolabel shape continuity into
account due to its propagation process, which can help construct effective
realistic tasks. to intuitively illustrate this issue, visual comparison of some
pseudo-labels generated by slic and spps is depicted in the supplementary
materials. in addition, to demonstrate the importance of the 2.5d loss function
defined in eq. 1 during training, we report the performance with and without l
2.5d in table 2 (see row (d) and (e)). we observe over 1% increase in the
average dice due to applying the 2.5d loss function.
this work introduces a novel framework called visa-fss, which aims to perform
few-shot 3d segmentation without requiring any manual annotations during
training. visa-fss leverages inter-slice information and continuous shape
changes that exist across consecutive slices within a 3d image. during training,
it uses consecutive slices within a 3d volume as support and query images, as
well as support-query pairs generated by applying geometric and intensity
transformations. this allows us to exploit intra-volume information and
introduce a 2.5d loss function that penalizes the model for making predictions
that are discontinuous among adjacent slices. finally, during inference, a novel
strategy for volumetric segmentation is introduced to employ the volumetric view
even during the testing time.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43895-0_11.
skin cancer is one of the most common cancers all over the world. serious skin
diseases such as melanoma can be life-threatening, making early detection and
treatment essential [3]. as computer-aided diagnosis matures, recent advances
with deep learning techniques such as cnns have significantly improved the
performance of skin lesion classification [7,8]. however, as data-hungry
approaches, deep learning models require large balanced and high-quality
datasets to meet the in scl, head classes are overtreated leading to
optimization concentrating on head classes. by contrast, ecl utilizes the
proxies to enhance the learning of tail classes and treats all classes equally
according to balanced contrastive theory [24]. moreover, the enriched relations
in samples and proxies are helped for better representations.accuracy and
robustness requirements in applications, which is hard to suffice due to the
long-tailed occurrence of diseases in the real-world. long-tailed problem is
usually caused by differences in incidence rate and difficulties in data
collection. some diseases are common while others are rare, making it difficult
to collect balanced data [13]. this will cause the head classes to account for
the majority of the samples and the tail classes only have small portions. thus,
existing public skin datasets usually suffer from imbalanced problems which then
results in class bias of classifier, for example, poor model performance
especially on tail lesion types.to tackle the challenge of learning unbiased
classifiers with imbalanced data, many previous works focus on three main ideas,
including re-sampling data [1,18], re-weighting loss [2,15,22] and re-balancing
training strategies [10,23]. resampling methods over-sample tail classes or
under-sample head classes, reweighting methods adjust the weights of losses on
class-level or instance-level, and re-balancing methods decouple the
representation learning and classifier learning into two stages or assign the
weights between features from different sampling branches [21]. despite the
great results achieved, these methods either manually interfere with the
original data distribution or improve the accuracy of minority classes at the
cost of reducing that of majority classes [12,13].recently, contrastive learning
(cl) methods pose great potential for representation learning when trained on
imbalanced data [4,14]. among them, supervised contrastive learning (scl) [11]
aggregates semantically similar samples and separates different classes by
training in pairs, leading to impressive success in long-tailed classification
of both natural and medical images [16]. however, there still remain some
defects: (1) current scl-based methods utilize the information of minority
classes insufficiently. since tail classes are sampled with low probability,
each training mini-batch inherits the long-tail distribution, making parameter
updates less dependent on tail classes. (2) scl loss focuses more on optimizing
the head classes with much larger gradients than tail classes, which means tail
classes are all pushed farther away from heads [24]. (3) most methods only
consider the impact of sample size ("imbalanced data") on the classification
accuracy of skin diseases, while ignoring the diagnostic difficulty of the
diseases themselves ("imbalanced diagnosis difficulty").to address the above
issues, we propose a class-enhancement contrastive learning (ecl) method for
skin lesion classification, differences between scl and ecl are illustrated in
fig. 1. for sufficiently utilizing the tail data information, we attempt to
address the solution from a proxy-based perspective. a proxy can be regarded as
the representative of a specific class set as learnable parameters. we propose a
novel hybrid-proxy model to generate proxies for enhancing different classes
with a reversed imbalanced strategy, i.e., the fewer samples in a class, the
more proxies the class has. these learnable proxies are optimized with a cycle
update strategy that captures original data distribution to mitigate the quality
degradation caused by the lack of minority samples in a mini-batch. furthermore,
we propose a balanced-hybrid-proxy loss, besides introducing balanced
contrastive learning (bcl) [24]. the new loss treats all classes equally and
utilizes sample-to-sample, proxy-to-sample and proxy-to-proxy relations to
improve representation learning. moreover, we design a balanced-weighted
crossentropy loss which follows a curriculum learning schedule by considering
both imbalanced data and diagnosis difficulty.our contributions can be
summarized as follows: (1) we propose an ecl framework for long-tailed skin
lesion classification. information of classes are enhanced by the designed
hybrid-proxy model with a cycle update strategy. (2) we present a
balanced-hybrid-proxy loss to balance the optimization of each class and
leverage relations among samples and proxies. (3) a new balancedweighted
cross-entropy loss is designed for an unbiased classifier, which considers both
"imbalanced data" and "imbalanced diagnosis difficulty". (4) experimental
results demonstrate that the proposed framework outperforms other
state-of-theart methods on two imbalanced dermoscopic image datasets and the
ablation study shows the effectiveness of each element.
the overall end-to-end framework of ecl is presented in fig. 2. the network
consists of two parallel branches: a contrastive learning (cl) branch for
representative learning and a classifier learning branch. the two branches take
in different augmentations t i , i ∈ {1, 2} from input images x and the backbone
is shared between branches to learn the features xi , i ∈ {1, 2}. we use a fully
connected layer as a logistic projection for classification g(•) : x → ỹ and a
one-hidden layer mlp h(•) : x → z ∈ r d as a sample embedding head where d
denotes the dimension. l 2 -normalization is applied to z by using inner product
as distance measurement in cl. both the class-dependent proxies generated by
hybrid-proxy model and the embeddings of samples are used to calculate
balanced-weighted cross-entropy loss, thus capturing the rich relations of
samples and proxies. for better representation, we design a cycle update
strategy to optimize the proxies' parameters in hybrid-proxy model, together
with a curriculum learning schedule for achieving unbiased classifiers. the
details are introduced as follows.
the proposed hybrid-proxy model consists of a set of class-dependent proxies
andn p c is the proxy number in this class. since samples in a mini-batch follow
imbalanced data distribution, these proxies are designed to be generated in a
reversed imbalanced way by giving more representative proxies of tail classes
for enhancing the information of minority samples. let us denote the sample
number of class c as n c and the maximum in all classes as n max . the proxy
number n p c can be obtained by calculating the imbalanced factor nmax nc of
each class:in this way, the tail classes have more proxies while head classes
have less, thus alleviating the imbalanced problem in a mini-batch.as we know, a
gradient descent algorithm will generally be executed to update the parameters
after training a mini-batch of samples. however, when dealing with an imbalanced
dataset, tail samples in a batch contribute little to the update of their
corresponding proxies due to the low probability of being sampled. so how to get
better representative proxies? here we propose a cycle update strategy for the
optimization of the parameters. specifically, we introduce the gradient
accumulation method into the training process to update proxies asynchronously.
the proxies are updated only after a finished epoch that all data has been
processed by the framework with the gradients accumulated. with such a strategy,
tail proxies can be optimized in a view of whole data distribution, thus playing
better roles in class information enhancement. algorithm 1 presents the details
of the training process.
to tackle the problem that scl loss pays more attention on head classes, we
introduce bcl and propose balanced-hybrid-proxy loss to treat classes equally.
given a batch of samples b = (xbe the feature embeddings in a batch and b
denotes the batch size. for an anchor sample z i ∈ z in class c, we unify the
positive image set as z + = {z j |y j = y i = c, j = i}. also for an anchor
proxy p c i , we unify all positive proxies as p + . the proposed
balanced-hybrid-proxy loss pulls points (both samples and proxies) in the same
class together, while pushes apart samples from different classes in embedding
space by using dot product as a similarity measure, which can be formulated as
follows:where b c means the sample number of class c in a batch, τ is the
temperature parameter. in addition, we further define z c and p c as a subset
with the label c of z and p respectively. the average operation in the
denominator of balancedhybrid-proxy loss can effectively reduce the gradients of
the head classes, making an equal contribution to optimizing each class. note
that our loss differs from bcl as we enrich the learning of relations between
samples and proxies. sampleto-sample, proxy-to-sample and proxy-to-proxy
relations in the proposed loss have the potential to promote network's
representation learning. moreover, as the skin datasets are often small, richer
relations can effectively help form a high-quality distribution in the embedding
space and improve the separation of features.
taking both "imbalanced data" and "imbalanced diagnosis difficulty" into
consideration, we design a curriculum schedule and propose balanced-weighted
cross-entropy loss to train an unbiased classifier. the training phase are
divided into three stages. we first train a general classifier, then in the
second stage we assign larger weight to tail classes for "imbalanced data". in
the last stage, we utilize the results on the validation set as the diagnosis
difficulty indicator of skin disease types to update the weights for "imbalanced
diagnosis difficulty". the loss is given by:where w denotes the weight and ỹ
denotes the network prediction. we assume f e c is the evaluation result of
class c on validation set after epoch e and we use f1-score in our experiments.
the network is trained for e epochs, e 1 and e 2 are hyperparameters for stages.
the final loss is given by loss = λl bhp +μl bw ce where λ and μ are the
hyperparameters which control the impact of losses.
dataset and evaluation metrics. we evaluate the ecl on two publicly available
dermoscopic datasets isic2018 [5,19] and isic2019 [5,6,19]. the 2018 we adopt
five metrics for evaluation: accuracy (acc), average precision (pre), average
sensitivity (sen), macro f1-score (f1) and macro area under curve (auc). acc and
f1 are considered as the most important metrics in this task.implementation
details. the proposed algorithm is implemented in python with pytorch library
and runs on a pc equipped with an nvidia a100 gpu. we use resnet50 [9] as
backbone and the embedding dimension d is set to 128. we use sgd as the
optimizer with the weight decay 1e-4. the initial learning rate is set to 0.002
and decayed by cosine schedule. we train the network for 100 epochs with a batch
size of 64. the hyperparameters e 1 , e 2 , τ , λ, and μ are set to 20, 50,
0.01, 1, and 2 respectively. we use the default data augmentation strategy on
imagenet in [9] as t 1 for classification branch. and for cl branch, we add
random grayscale, rotation, and vertical flip in t 1 as t 2 to enrich the data
representations. meanwhile, we only conduct the resize operation to ensure input
size 224 × 224 × 3 during testing process. the models with the highest acc on
validation set are chosen for testing. we conduct experiments in 3 independent
runs and report the standard deviations in the supplementary material.
quantitative results. to evaluate the performance of our ecl, we compare our
method with 10 advanced methods. among them, focal loss [15], ldam-drw [2],
logit adjust [17], and mwnl [22] are the re-weighting loss methods. bbn [23] is
the methods based on re-balancing training strategy while hybrid-sc [20], scl
[11,16], bcl [24], tsc [14] and ours are the cl-based methods. moreover, mwnl
and scl have been verified to perform well in the skin disease classification
task. to ensure fairness, we re-train all methods by rerun their released codes
on our divided datasets with the same experimental settings. we also confirmed
that all models have converged and choose the best eval checkpoints. the results
are shown in table 1. it can be seen that ecl has a significant advantage with
the highest level in most metrics on two datasets. noticeably, our ecl
outperforms other imbalanced methods by great gains, e.g., 2.56% in pre on
isic2018 compared with scl and 4.33% in f1 on isic2019 dataset compared with
tsc. furthermore, we draw the confusion matrixes after normalization in fig. 3,
which illustrate that ecl has significantly improved most of the categories,
from minority to majority. s2). first, we directly move the contrastive learning
(cl) branch and replaced the balanced-weighted cross-entropy (bwce) loss with
cross-entropy (ce) loss. we can see from the results that adding cl branch can
significantly improve the network's data representation ability with better
performance than only adopting a classifier branch. and our bwce loss can help
in learning a more unbiased classifier with an improvement of 2.7% in f1
compared to ce in dual branch setting. then we train the ecl w/o cycle update
strategy. the overall performance of the network has declined compared with
training w/ the strategy, indicating that this strategy can better enhance
proxies learning through the whole data distribution. in the end, we also set
the proxies' number of different classes equal to explore whether the
classification ability of the network is improved due to the increase in the
number of proxies. with more proxies, metrics fluctuate and do not increase
significantly. however, the result of using proxies generated by reversed
balanced way in hybrid-proxy model (hpm) outperforms equal proxies in nearly all
metrics, which proves that giving more proxies to tail classes can effectively
enhance and enrich the information.
in this work, we present a class-enhancement contrastive learning framework,
named ecl, for long-tailed skin lesion classification. hybrid-proxy model and
balanced-hybrid-proxy loss are proposed to tackle the problem that scl-based
methods pay less attention to the learning of tail classes. class-dependent
proxies are generated in hybrid-proxy model to enhance information of tail
classes, where rich relations between samples and proxies are utilized to
improve representation learning of the network. furthermore, balanced-weighted
cross-entropy loss is designed to help train an unbiased classifier by
considering both "imbalanced data" and "imbalanced diagnosis difficulty".
extensive experiments on isic2018 and isic2019 datasets have demonstrated the
effectiveness and superiority of ecl over other compared methods.
input: 7 else 8 loss(θ, φ) = λlbhp ( z , p) + μlbw ce ({yi, ỹi} b ) 9 grad t θ =
∇ θ loss(θ), grad t φ = ∇ φ loss(φ) // calculate gradients 10 θ ← θlr * grad t θ
// update parameters θ of model 11 φ ← φ -t t lr * grad t φ // update parameters
φ of p 12 if e > e2 then 13 f e = v alidate(model, x val )
ablation study. to further verify the effectiveness of the designs in ecl, we
conduct a detailed ablation study shown in table2(the results on isic2018 are
shown in supplementary material table
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43895-0 23.
deep learning has achieved promising performance in computer-aided diagnosis
[1,12,14,24], but it relies on large-scale labeled data to train, which is
challenging in medical imaging due to label scarcity and high annotation cost
[3,25]. specifically, expert annotations are required for medical data, which
can be costly and time-consuming, especially in tasks such as 3d image
segmentation.transferring pre-trained models to downstream tasks is an effective
solution for addressing the label-limited problem [8], but fine-tuning the full
network with small downstream data is prone to overfitting [16]. recently,
prompt tuning [5,18] is emerging from natural language processing (nlp), which
introduces additional tunable prompt parameters to the pre-trained model and
updates only prompt parameters using supervision signals obtained from a few
downstream training samples while keeping the entire pre-trained unchanged. by
tuning only a few parameters, prompt tuning makes better use of pre-trained
knowledge. it avoids driving the entire model with few downstream data, which
enables it to outperform traditional fine-tuning in limited labeled data.
building on the recent success of prompt tuning in nlp [5], instead of designing
text prompts and transformer models, we explore visual prompts on convolutional
neural networks (cnns) and the potential to address data limitations in medical
imaging.however, previous prompt tuning research [18,28], whether on language or
visual models, has focused solely on the model-centric approach. for instance,
coop [29] models a prompt's context using a set of learnable vectors and
optimizes it on a few downstream data, without discussing what kind of samples
are more suitable for learning prompts. vpt [13] explores prompt tuning with a
vision transformer, and spm [17] attempts to handle downstream segmentation
tasks through prompt tuning on cnns, which are also model-centric. however, in
downstream tasks with limited labeled data, selective labeling as a data-centric
method is crucial for determining which samples are valuable for learning,
similar to active learning (al) [23]. in al, given the initial labeled data, the
model actively selects a subset of valuable samples for labeling and improves
performance with minimum annotation effort. nevertheless, directly combining
prompt tuning with al presents several problems. first, unlike the task-specific
models trained with initial data in al, the task-agnostic pre-trained model
(e.g., trained by related but not identical supervised or self-supervised task)
is employed for data selection with prompt tuning. second, in prompt tuning, the
pre-trained model is frozen, which may render some al methods inapplicable, such
as those previously based on backbone gradient [9] and feature [19]. third,
merging prompt tuning with al takes work. their interplay must be considered.
however, previous al methods [27] did not consider the existence of prompts or
use prompts to estimate sample value.therefore, this paper proposes the first
framework for selective labeling and prompt tuning (slpt), combining
model-centric and data-centric methods to improve performance in medical
label-limited scenarios. we make three main contributions: (1) we design a novel
feature-aware prompt updater embedded in the pre-trained model to guide prompt
tuning in deep layers. (2) we propose a diversified visual prompt tuning
mechanism that provides multi-prompt-based discrepant predictions for selective
labeling. (3) we introduce the tesla strategy which includes both unsupervised
diversity selection via task-agnostic features and supervised selection
considering prompt-based uncertainty. the results show that slpt outperforms
fine-tuning with just 6% of tunable parameters and achieves 94% of full-data
performance by selecting only 5% of labeled data.
given a task-agnostic pre-trained model and unlabeled data for an initial
medical task, we propose slpt to improve model performance. slpt consists of
three components, as illustrated in fig. 1: (a) a prompt-based visual model, (b)
diversified visual prompt tuning, and (c) tandem selective labeling.
specifically, with slpt, we can select valuable data to label and tune the model
via prompts, which helps the model overcome label-limited medical scenarios.
the pre-trained model, learned by supervised or unsupervised training, is a
powerful tool for improving performance on label-limited downstream tasks.
finetuning a large pre-trained model with limited data may be suboptimal and
prone to overfitting [16]. to overcome this issue, we draw inspiration from nlp
[18] and explore prompt tuning on visual models. in order to facilitate prompt
tuning on the model's deep layers, we introduce the feature-aware prompt updater
(fpu). fpus are inserted into the network to update deep prompts and features.
in fig. 1(a), an fpu receives two inputs, feature map f out i-1 and prompt p i-1
, of the same shape, and updates to f i and p i through two parallel branches.
in the feature branch, f out i-1 and p i-1 are concatenated and fed into a 1x1
convolution and fusion module. the fusion module utilizes aspp [7] to extract
multi-scale contexts. then a se [11] module for channel attention enhances
context by channel. finally, the attention output and f out i-1 are element-wise
multiplied and added to obtain the updated feature f i . in the prompt branch,
the updated feature f i is concatenated with the previous prompt p i-1 , and a
parameter-efficient depth-separable convolution is employed to generate the
updated prompt p i .to incorporate fpu into a pre-trained model, we consider the
model comprising n modular m i (i = 1, ..., n ) and a head output layer. after
each m i , we insert an f p u i . given the input f in i-1 and prompt p i-1 , we
have the output feature f i , updated prompt p i and prediction y as
follows:where input x = f 0 , fpu and head are tuned while m i is not tunable.
inspired by multi-prompt learning [18] in nlp, we investigate using multiple
visual prompts to evaluate prompt-based uncertainty. however, initializing and
optimizing k prompts directly can significantly increase parameters and may not
ensure prompt diversity. to address these challenges, we propose a diversified
visual prompt tuning approach. as shown in fig. 1(b), our method generates k
prompts p k ∈ r 1×d×h×w from a meta promptthrough k different upsampling and
convolution operations upconv k . p m is initialized from the statistical
probability map of the foreground category, similar to [17]. specifically, we
set the foreground to 1 and the background to 0 in the groundtruth mask, and
then average all masks and downsample to 1to enhance prompt diversity, we
introduce a prompt diversity loss l div that regularizes the cosine similarity
between the generated prompts and maximizes their diversity. this loss is
formulated as follows:where p k1 and p k2 represent the k 1 -th and k 2 -th
generated prompts, respectively, and || • || 2 denotes the l2 norm. by
incorporating the prompt diversity loss, we aim to generate a set of diverse
prompts for our visual model. in nlp, using multiple prompts can produce
discrepant predictions [2] that help estimate prompt-based uncertainty. drawing
inspiration, we propose a visual prompt tuning approach that associates diverse
prompts with discrepant predictions. to achieve this, we design k different data
augmentation, heads, and losses based on corresponding k prompts. by varying
hyperparameters, we can achieve different data augmentation strengths,
increasing the model's diversity and generalization. different predictions y k
are generated by k heads, each where k = 1, ..., k, m f p u is the pre-trained
model with fpu, ce is the crossentropy loss, and λ 1 = λ 2 = λ 3 = 1 weight each
loss component. y represents the ground truth and l is the total loss.
previous studies overlook the critical issue of data selection for downstream
tasks, especially when available labels are limited. to address this challenge,
we propose a novel strategy called tesla. tesla consists of two tandem steps:
unsupervised diversity selection and supervised uncertainty selection. the first
step aims to maximize the diversity of the selected data, while the second step
aims to select the most uncertain samples based on diverse prompts.step 0:
unsupervised diversity selection. since we do not have any labels in the initial
and our pre-trained model is task-agnostic, we select diverse samples to cover
the entire dataset. to achieve this, we leverage the pre-trained model to obtain
feature representations for all unlabeled data. although these features are
task-independent, they capture the underlying relationships, with similar
samples having closer feature distances. we apply the k-center method from
coreset [22], which identifies the b samples that best represent the diversity
of the data based on these features. these selected samples are then annotated
and serve as the initial dataset for downstream tasks.step 1: supervised
uncertainty selection. after prompt tuning with the initial dataset, we obtain a
task-specific model that can be used to evaluate data value under supervised
training. since only prompt-related parameters can be tuned while others are
frozen, we assess prompt-based uncertainty via diverse prompts, considering
inter-prompts uncertainty and intra-prompts uncertainty.in the former, we
compute the multi-prompt-based divergence map d, given k probability predictions
y k through k diverse prompts p k , as follows:where kl refers to the kl
divergence [15]. then, we have the divergence score s d = mean(d), which
reflects inter-prompts uncertainty.in the latter, we evaluate intra-prompts
uncertainty by computing the mean prediction of the prompts and propose to
estimate prompt-based gradients as the model's performance depends on the update
of prompt parameters θ p . however, for these unlabeled samples, computing their
supervised loss and gradient directly is not feasible. therefore, we use the
entropy of the model's predictions as a proxy for loss. specifically, we
calculate the entropy-based prompt gradient score s g for each unlabeled sample
as follows:to avoid manual weight adjustment, we employ multiplication instead
of addition. we calculate our uncertainty score s as follows:where max(•) finds
the maximum value. we sort the unlabeled data by their corresponding s values in
ascending order and select the top b data to annotate.
datasets and pre-trained model. we conducted experiments on automating liver
tumor segmentation in contrast-enhanced ct scans, a crucial task in liver cancer
diagnosis and surgical planning [1]. although there are publicly available liver
tumor datasets [1,24], they only contain major tumor types and differ in image
characteristics and label distribution from our hospital's data. deploying a
model trained from public data to our hospital directly will be
problematic.collecting large-scale data from our hospital and training a new
model will be expensive. therefore, we can use the model trained from them as a
starting point and use slpt to adapt it to our hospital with minimum cost. we
collected a dataset from our in-house hospital comprising 941 ct scans with
eight categories: hepatocellular carcinoma, cholangioma, metastasis,
hepatoblastoma, hemangioma, focal nodular hyperplasia, cyst, and others. it
covers both major and rare tumor types. our objective is to segment all types of
lesions accurately. we utilized a pre-trained model for liver segmentation using
supervised learning on two public datasets [24] with no data overlap with our
downstream task. the nnunet [12] was used to preprocess and sample the data into
24 × 256 × 256 patches for training. to evaluate the performance, we employed a
5-fold crossvalidation (752 for selection, 189 for test).metrics. we evaluated
lesion segmentation performance using pixel-wise and lesion-wise metrics. for
pixel-wise evaluation, we used the dice per case, a commonly used metric [1].
for lesion-wise evaluation, we first do connected component analysis to
predicted and ground truth masks to extract lesion instances, and then compute
precision and recall per case [20]. a predicted lesion is regarded as a tp if
its overlap with ground truth is higher than 0.2 in dice. competing approaches.
in the prompt tuning experiment, we compared our method with three types of
tuning: full parameter update (fine-tuning, learn-from-scratch), partial
parameter update (head-tuning, encoder-tuning, decoder-tuning), and prompt
update (spm [17]). in the unsupervised diversity selection experiment, we
compared our method with random sampling. in the supervised uncertainty
selection experiment, we compared our method with random sampling, diversity
sampling (coreset [22], corecgn [6]), and uncertainty sampling (entropy, mc
dropout [10], ensemble [4], uncertaingcn [6], ent-gn [26]). unlike ensemble, our
method was on multi-prompt-based heads. furthermore, unlike ent-gn, which
computed the entropy-based gradient from a single prediction, we calculated a
stable entropy from the muti-prompt-based mean predictions and solely considered
the prompt gradient.training setup. we conducted the experiments using the
pytorch framework on a single nvidia tesla v100 gpu. the nnunet [12] framework
was used for 3d lesion segmentation with training 500 epochs at an initial
learning rate of 0.01. we integrated 13 fpus behind each upsampling or
downsampling of nnunet, adding only 2.7m parameters. during training, we set k =
3 and employed diverse data augmentation techniques such as scale, elastic,
rotation, and mirror. three sets of tl parameters is (α 1,2,3 = 0.5,0.7,0.3, β
1,2,3 = 0.5,0.3,0.7). to ensure fairness and eliminate model ensemble effects,
we only used the model's prediction with k = 1 during testing. we used fixed
random seeds and 5-fold cross-validation for all segmentation experiments.
evaluation of prompt tuning. since we aim to evaluate the efficacy of prompt
tuning on limited labeled data in table 1, we create a sub-dataset of
approximately 5% (40/752) from the original dataset. specifically, we calculate
the class probability distribution vector for each sample based on the pixel
class in the mask and use coreset with these vectors to select 40 class-balanced
samples. using this sub-dataset, we evaluated various tuning methods for limited
1. fine-tuning all parameters served as the strongest baseline, but our method,
which utilizes only 6% tunable parameters, outperformed it by 5.4%. although spm
also outperforms fine-tuning, our methods outperform spm by 1.18% and save 0.44m
tunable parameters with more efficient fpu. in cases of limited data,
fine-tuning tends to overfit on a larger number of parameters, while prompt
tuning does not. the pre-trained model is crucial for downstream tasks with
limited data, as it improves performance by 9.52% compared to
learn-from-scratch. among the three partial tuning methods, the number of tuning
parameters positively correlates with the model's performance, but they are
challenging to surpass fine-tuning.evaluation of selective labeling. we
conducted steps 0 (unsupervised selection) and 1 (supervised selection) from the
unlabeled 752 data and compared our approach with other competing methods, as
shown in table 2. in step 0, without any labeled data, our diversity selection
outperformed the random baseline by 1.86%. building upon the 20 data points
selected by our method in step 0, we proceeded to step 1, where we compared our
method with eight other data selection strategies in supervised mode. as a
result, our approach outperformed other methods because of prompt-based
uncertainty, such as ent-gn and ensemble, by 2.05% and 1.46%, respectively. our
approach outperformed coreset by 6.05% and coregcn by 5.43%. we also
outperformed uncertaingcn by 1.93%. mc dropout and entropy underperformed in our
prompt tuning, likely due to the difficulty of learning such uncertain data with
only a few prompt parameters. notably, our method outperformed random sampling
by 10.28%. these results demonstrate the effectiveness of our data selection
approach in practical tasks.ablation studies. we conducted ablation studies on s
d and s g in tesla. as shown in table 2, the complete tesla achieved the best
performance, outperforming the version without s d by 1.84% and the version
without s g by 1.98%. it shows that each component plays a critical role in
improving performance.
we proposed a pipeline called slpt that enhances model performance in
labellimited scenarios. with only 6% of tunable prompt parameters, slpt
outperforms fine-tuning due to the feature-aware prompt updater. moreover, we
presented a diversified visual prompt tuning and a tesla strategy that combines
unsupervised and supervised selection to build annotated datasets for downstream
tasks. slpt pipeline is a promising solution for practical medical tasks with
limited data, providing good performance, few tunable parameters, and low
labeling costs. future work can explore the potential of slpt in other domains.
recent years have witnessed the remarkable success of deep learning in medical
image segmentation. however, although the performance of deep learning models
even surpasses the accuracy of human exports on some segmentation tasks, two
challenges still persist. (1) different segmentation tasks are usually tackled
separately by specialized networks (see fig. 1(a)), leading to distributed
research efforts. (2) most segmentation tasks face the limitation of a small
labeled dataset, especially for 3d segmentation tasks, since pixel-wise 3d image
annotation is labor-intensive, time-consuming, and susceptible to operator bias.
train one model on n datasets using task-specific prompts. we use purple to
highlight where to add the task-related information.several strategies have been
attempted to address both challenges. first, multi-head networks (see fig. 1(b))
were designed for multiple segmentation tasks [4,7,21]. a typical example is
med3d [4], which contains a shared encoder and multiple task-specific decoders.
although they benefit from the encoder parameter-sharing scheme and the rich
information provided by multiple training datasets, multi-head networks are
less-suitable for multi-task co-training, due to the structural redundancy
caused by the requirement of preparing a separate decoder for each task. the
second strategy is the multi-class model, which formulates multiple segmentation
tasks into a multi-class problem and performs it simultaneously. to achieve
this, the clip-driven universal model [16] (see fig. 1(c)) introduces the text
embedding of all labels as external knowledge, obtained by feeding medical
prompts to clip [5]. however, clip has limited ability to generalize in medical
scenarios due to the differences between natural and medical texts. it is
concluded that the discriminative ability of text prompts is weak in different
tasks, and it is difficult to help learn task-specific semantic information. the
third strategy is dynamic convolution. dodnet [29] and its variants [6,17,25]
present universal models, which can perform different segmentation tasks based
on using task encoding and a controller to generate dynamic convolutions (see
fig. 1(d)). the limitations of these models are two-fold. (1) different tasks
are encoded as one-hot vectors, which are mutually orthogonal, ignoring the
correlations among tasks. (2) the task-related information (i.e., dynamic
convolution parameters) is introduced at the end of the decoder. it may be too
late for the model to be 'aware' of the ongoing task, making it difficult to
decode complex targets.in this paper, we propose a prompt-driven universal
segmentation model (uniseg) to segment multiple organs, tumors, and vertebrae on
3d medical images with diverse modalities and domains. uniseg contains a vision
encoder, a fusion and selection (fuse) module, and a prompt-driven decoder. the
fuse module is devised to generate the task-specific prompt, which enables the
model to be 'aware' of the ongoing task (see fig. 1(e)). specifically, since
prompt learning has a proven ability to represent both task-specific and
task-invariant knowledge [24], a learnable universal prompt is designed to
describe the correlations among tasks. then, the universal prompt and the
features extracted by the vision encoder are fed to the fuse module to generate
task prompts for all tasks. the task-specific prompt is selected according to
the ongoing task. moreover, to introduce the prompt information to the model
early, we move the task-specific prompt from the end of the decoder to the start
of the decoder (see fig. 2). thanks to both designs, we can use a single decoder
and a segmentation head to predict various targets under the supervision of the
corresponding ground truths. we collected 3237 volumetric data with three
modalities (ct, mr, and pet) and various targets (eight organs, vertebrae, and
tumors) from 11 datasets as the upstream dataset. on this dataset, we evaluated
our uniseg model against other universal models, such as dodnet and the
clip-driven universal model. we also compared uniseg to seven advanced
single-task models, such as cotr [26], nnformer [30], and nnunet [12], which are
trained independently on each dataset. furthermore, to verify its generalization
ability on downstream tasks, we applied the trained uniseg to two downstream
datasets and compared it to other pre-trained models, such as mg [31], desd
[28], and unimiss [27]. our results indicate that uniseg outperforms all
competing methods on 11 upstream tasks and two downstream tasks.our
contributions are three-fold: (1) we design a universal prompt to describe the
correlations among different tasks and use it to generate task prompts for all
tasks. (2) we utilize the task-related prompt information as the input of the
decoder, facilitating the training of the whole decoder, instead of just the
last few layers. (3) the proposed uniseg can be trained on and applied to
various 3d medical image tasks with diverse modalities and domains, providing a
highquality pre-trained 3d medical image segmentation model for the community.
let {d 1 , d 2 , ..., d n } be n datasets. here,j=1 represents that the i-th
dataset has a total of n i image-label pairs, and x ij and y ij are the image
and the corresponding ground truth, respectively. straightforwardly, n tasks can
be completed by training n models on n datasets, respectively. this solution
faces the issues of ( 1) designing an architecture for each task, (2)
distributing research effort, and (3) dropping the benefit of rich information
from other tasks. therefore, we propose a universal framework called uniseg to
solve multiple tasks with a single model, whose architecture was shown in fig.
2.
the main architecture of uniseg is based on nnunet [12], which consists of an
encoder and a decoder shared by different tasks. the encoder has six stages,
each containing two convolutional blocks, to extract features and gradually
reduce the feature resolution. the convolutional block includes a convolutional
layer followed by instance normalization and a reakyrelu activation, and the
first convolution layer of each stage is usually set to reduce the resolution
with a stride of 2, except for the first stage. to accept the multi-modality
inputs, we reform the first convolution layer and set up three different
convolution layers to handle the input with one, two, or four channels,
respectively. after the encoder process, we obtain the sample-specific
features32 , where c is the number of channels and d, h, and w are the depth,
height, and width of the input, respectively. symmetrically, in each stage of
the decoder, the upsampling operation implemented by a transposed convolution
layer is applied to the input feature map to improve its resolution and reduce
its channel number. the upsampled feature map is concatenated with the output of
the corresponding encoder stage and then fed to a convolutional block. after the
decoder process, the output of each decoder stage is passed through a
segmentation head to predict segmentation maps for deep supervision, which is
governed by the sum of the dice loss and cross-entropy loss. note that the
channel number of multi-scale segmentation maps is set to the maximum number of
classes among all tasks.
following the simple idea that everything is correlated, we believe that the
correlations among different segmentation tasks must exist undoubtedly, though
they are ignored by dodnet which uses a set of orthogonal and one-hot task
codes. considering the correlations among tasks are extremely hard to handcraft,
we propose a learnable prompt called universal prompt to describe them and use
that prompt to generate task prompts for all tasks, aiming to encourage
interaction and fusion among different task prompts. we define the shape of the
universal prompt as32 , where n is the number of tasks.
before building a universal network, figuring out a way to make the model
'aware' of the ongoing task is a must. dodnet adopts a one-hot vector to encode
each task, and the clip-driven universal model [16] uses masked back-propagation
to optionally optimize the task-related segmentation maps. by contrast, we first
obtain n features by passing the concatenation of f uni and f through three
convolutional blocks, shown as followswhere f taski denotes the prompt features
belonging to the i-th task, cat(, ) is a concatenation operation, f (•) denotes
the feed forward process, and split(•) n means splitting features along the
channel to obtain n features with the same shape. then, we select the target
features, called task-specific prompt f tp , from {f task1 , f task2 , ..., f
taskn } according to the ongoing task. finally, we concatenate f and selected f
tp as the decoder input. in this way, we introduce task-related prior
information into the model, aiming to boost the training of the whole decoder
rather than only the last few convolution layers.
after training uniseg on upstream datasets, we transfer the pre-trained
encoderdecoder and randomly initialized segmentation heads to downstream tasks.
the model is fine-tuned in a fully supervised manner to minimize the sum of the
dice loss and cross-entropy loss.3 experiments and results
datasets. for this study, we collected 11 medical image segmentation datasets as
the upstream dataset to train our uniseg and single-task models. the liver and
kidney datasets are from lits [3] and kits [11], respectively. the hepatic
vessel (hepav), pancreas, colon, lung, and spleen datasets are from medical
segmentation decathlon (msd) [1]. verse20 [19], prostate [18], brats21 [2], and
autopet [8] datasets have annotations of the vertebrae, prostate, brain tumors,
and whole-body tumors, respectively. we used the binary version of the verse20
dataset, where all foreground classes are regarded as one class. moreover, we
dropped the samples without tumors in the autopet dataset. meanwhile, we use
btcv [14] and vs datasets [20] as downstream datasets to verify the ability of
uniseg to generalize to other medical image segmentation tasks. btcv contains
the annotations of 13 abdominal organs, including the spleen (sp), right kidney
(rki), left kidney (lki), gallbladder (gb), esophagus (es), liver (li), stomach
(st), aorta(ao), inferior vena cava (ivc), portal vein and splenic vein (psv),
pancreas (pa), right adrenal gland (rag), and left adrenal gland (lag). the vs
dataset contains the annotations of the vestibular schwannoma. more details are
shown in table 1.evaluation metric. the dice similarity coefficient (dice) that
measures the overlap region of the segmentation prediction and ground truth is
employed to evaluate the segmentation performance.
both pre-training on eleven upstream datasets and fine-tuning on two downstream
datasets were implemented based on the nnunet framework [12]. during
pre-training, we adopted the sgd optimizer and set the batch size to 2, the
initial learning rate to 0.01, the default patch size to 64 × 192 × 192, and the
maximum training epoch to 1000 with a total of 550,000 iterations. moreover, we
adopted a uniform sampling strategy to sample training data from upstream
datasets. in the inference stage, we employed the sliding window strategy, in
which the shape of the window is the same as the training patch size, to obtain
the whole average segmentation map. during fine-tuning, we set the batch size to
2, the initial learning rate to 0.01, the default patch size to 48 × 192 × 192,
and the maximum training iterations to 25,000 for all downstream datasets. the
sliding window strategy was also employed when inference on downstream tasks.
comparing to single-task and universal models. our uniseg was compared with
advanced single-task models and universal models. the former includes unetr [9],
nnformer [30], pvtv2-b1 [23], cotr [26], uxnet [15], swin unetr [22], and nnunet
[12]. the latter includes dodnet [29], clip dodnet, which replaces the one-hot
vectors with clip embeddings obtained by following [16], and clip-driven
universal model [16]. for a fair comparison, the maximum training iterations of
single-task models on each task are 50,000, and the patch size is 64 × 192 ×
192, except for swin unetr, whose patch size is 64 × 160 × 160 due to the
limitation of gpu memory. the backbones of the competing universal models and
our uniseg are the same. as shown in table 2, our uniseg achieves the highest
dice on eight datasets, beating the second-best models by 1.9%, 0.7%, 0.8%,
0.4%, 0.4%, 1.0%, 0.3%, 1.2% on the liver, kidney, hepav, pancreas, colon, lung,
prostate, and autopet datasets, respectively. moreover, uniseg also presents
superior performance with an average margin of 1.0% and 1.6% on eleven datasets
compared to the second-best universal model and single-task model, respectively,
demonstrating its superior performance.comparing to other pre-trained models. we
compared our uniseg with advanced unsupervised pre-trained models, such as mg
[31], smit [13], unimiss [27], desd [28], and gvsl [10], and supervised
pre-trained models, such as autopet and dodnet [29]. the former are officially
released with different backbones while the latter are trained using the
datasets and backbone used in our uniseg. to verify the benefit of training on
multiple datasets, we also report the performance of the models per-trained on
autopet and brats21, respectively. the results in table 3 reveal that almost all
pre-trained models achieve performance gains over their baselines, which were
trained from scratch. more important, thanks to the powerful baseline and small
gap between the pretext and downstream tasks, uniseg achieves the best
performance and competitive performance gains on downstream datasets,
demonstrating that it has learned a strong representations ability. furthermore,
another advantage of uniseg against other unsupervised pre-trained models is
that it is more resource-friendly, requiring only one gpu of 11 gb memory for
implementation, while unsupervised pre-trained models usually require tremendous
computational resources, such as eight and four v100 for unimiss and smit,
respectively.comparison of different variants. we attempted three uniseg
variants, including fixed prompt, multiple prompts, and uniseg-t, as shown in
fig. 3. the results in table 4 suggest that (1) learnable universal prompt is
helpful for building valuable prompt features; (2) using one universal prompt
instead of multiple task-independent prompts boosts the interaction and fusion
among all tasks, resulting in better performance; (3) adding task-related
information in advance facilitates handling complex prediction situations.
this study proposes a universal model called uniseg (a single model) to perform
multiple organs, tumors, and vertebrae segmentation on images with multiple
modalities and domains. to solve two limitations existing in preview universal
models, we design the universal prompt to describe correlations among all tasks
and make the model 'aware' of the ongoing task early, boosting the training of
the whole decoder instead of just the last few layers. thanks to both designs,
our uniseg achieves superior performance on 11 upstream datasets and two
downstream datasets, setting a new record. in our future work, we plan to design
a universal model that can effectively process multiple dimensional data.
of single-task models and universal models on eleven datasets. we use dice (%)
on each dataset and mean dice (%) on all datasets as metrics. the best results
on each dataset are in bold.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1_49.
histopathological image analysis is an important step towards cancer diagnosis.
however, shortage of pathologists worldwide along with the complexity of
histopathological data make this task time consuming and challenging. therefore,
developing automatic and accurate histopathological image analysis methods that
leverage recent progress in deep learning has received significant attention in
recent years. in this work, we investigate the problem of diagnosing colorectal
cancer, which is one of the most common reason for cancer deaths around the
world and particularly in europe and america [23].existing deep learning-based
colorectal tissue classification methods [18,21,22] typically require large
amounts of annotated histopathological training data for all tissue types to be
categorized. however, obtaining large amount of training data is challenging,
especially for rare cancer tissues. to this end, it is desirable to develop a
few-shot colorectal tissue classification method, which can learn from seen
tissue classes having sufficient training data, and be able to transfer this
knowledge to unseen (novel) tissue classes having only a few exemplar training
images.while generative adversarial networks (gans) [6] have been utilized to
synthesize images, they typically need to be trained using large amount of real
images of the respective classes, which is not feasible in aforementioned
few-shot setting. therefore, we propose a few-shot (fs) image generation
approach for generating high-quality and diverse colorectal tissue images of
novel classes using limited exemplars. moreover, we demonstrate the
applicability of these generated images for the challenging problem of fs
colorectal tissue classification.
we propose a few-shot colorectal tissue image generation framework, named
xm-gan, which simultaneously focuses on generating highquality yet diverse
images. within our tissue image generation framework, we introduce a novel
controllable fusion block (cfb) that enables a dense aggregation of local
regions of the reference tissue images based on their congruence to those in the
base tissue image. our cfb employs a cross-attention based feature aggregation
between the base (query) and reference (keys, values) tissue image features.
such a cross-attention mechanism enables the aggregation of reference features
from a global receptive field, resulting in locally consistent features.
consequently, colorectal tissue images are generated with reduced artifacts.to
further enhance the diversity and quality of the generated tissue images, we
introduce a mapping network along with a controllable cross-modulated layer
normalization (cln) within our cfb. our mapping network generates 'metaweights'
that are a function of the global-level features of the reference tissue image
and the control parameters. these meta-weights are then used to compute the
modulation weights for feature re-weighting in our cln. this enables the
cross-attended tissue image features to be re-weighted and enriched in a
controllable manner, based on the reference tissue image features and associated
control parameters. consequently, it results in improved diversity of the tissue
images generated by our transformer-based framework (see fig. 3).we validate our
xm-gan on the fs colorectral tissue image generation task by performing
extensive qualitative, quantitative and subject specialist (pathologist) based
evaluations. our xm-gan generates realistic and diverse colorectal tissue images
(see fig. 3). in our subject specialist (pathologist) based evaluation,
pathologists could differentiate between our xm-gan generated colorectral tissue
images and real images only 55% time. furthermore, we evaluate the effectiveness
of our generated tissue images by using them as data augmentation during
training of fs colorectal tissue image classifier, leading to an absolute gain
of 4.4% in terms of mean classification accuracy over the vanilla fs classifier.
the ability of generative models [6,15] to fit to a variety of data
distributions has enabled great strides of advancement in tasks, such as image
generation [3,12,13,19], and so on. despite their success, these generative
models typically require large amount of data to train and avoid overfitting. in
contrast, fewshot (fs) image generation approaches [2,4,7,9,16] strive to
generate natural images from disjoint novel categories from the same domain as
in the training. existing fs natural image generation approaches can be broadly
divided into three categories based on transformation [1], optimization [4,16]
and fusion [7,9,10]. the transformation-based approach learns to perform
generalized data augmentations to generate intra-class images from a single
conditional image. on the other hand, optimization-based approaches typically
utilize meta-learning techniques to adapt to a different image generation task
by optimizing on a few reference images from the novel domain. different from
these two paradigms that are better suited for simple image generation task,
fusion-based approaches first aggregate latent features of reference images and
then employ a decoder to generate same class images from these aggregated
features.our approach: while the aforementioned works explore fs generation in
natural images, to the best of our knowledge, we are the first to investigate fs
generation in colorectal tissue images. in this work, we look into multi-class
colorectal tissue analysis problem, with low and high-grade tumors included in
the set. the corresponding dataset [14] used in this study is widely employed
for multi-class texture classification in colorectal cancer histology and
comprises eight types of tissue: tumor epithelium, simple stroma, complex
stroma, immune cells, debris, normal mucosal glands, adipose tissue and
background (no tissue). generating colorectal tissue images of these diverse
categories is a challenging task, especially in the fs setting. generating
realistic and diverse tissue images require ensuring both global and local
texture consistency (patterns). our xm-gan densely aggregates features [5,20]
from all relevant local regions of the reference tissue images at a
global-receptive field along with a controllable mechanism for modulating the
tissue image features by utilizing meta-weights computed from the input
reference tissue image features. as a result, this leads to high-quality yet
diverse colorectal tissue image generation in fs setting.
problem formulation: in our few-shot colorectal tissue image generation
framework, the goal is to generate diverse set of images from k input examples x
of a unseen (novel) tissue classes. let d s and d u be the set of seen and
unseen classes, respectively, where d s ∩ d u = ∅. in the training stage, we
sample images from d s and train the model to learn transferable generation
ability to produce new tissue images for unseen classes. during inference, given
k images from an unseen class in d u , the trained model strives to produce
diverse yet plausible images for this unseen class without any further
fine-tuning. overall architecture: figure 1 shows the overall architecture of
our proposed framework, xm-gan. here, we randomly assign a tissue image from x
as a base image x b , and denote the remaining k-1 tissue images as reference {x
ref i } k-1 i=1 . given the input images x, we obtain feature representation of
the base tissue image and each reference tissue image by passing them through
the shared encoder f e . next, the encoded feature representations h are input
to a controllable fusion block (cfb), where cross-attention [20] is performed
between the base and reference features, h b and h ref i , respectively. within
our cfb, we introduce a mapping network along with a controllable
cross-modulated layer normalization (cln) to compute meta-weights w i , which
are then used to generate the modulation weights used for re-weighting in our
cln. the resulting fused representation f is input to a decoder f d to generate
tissue image x. the whole framework is trained following the gan [17] paradigm.
in addition to l adv and l cl , we propose to use a guided perceptual loss term
l p , utilizing the control parameters α i . next, we describe our cfb in
detail.
figure 2 shows the architecture of our proposed cfb, comprises of a shared
crosstransformer followed by a feature fusion mechanism. here, the
cross-transformer is based on multi-headed cross-attention mechanism that
densely aggregates relevant input image features, based on pairwise attention
scores between each position in the base tissue image with every region of the
reference tissue image. the query embeddings q m ∈ r n×d are computed from the
base features h b ∈ r n×d , while keys k m i ∈ r n×d and values v m i ∈ r n×d
are obtained from the reference features h ref i ∈ r n×d , where d = d /m with m
as the number of attention heads. next, a cross-attention function maps the
queries to outputs r m i using the keyvalue pairs. finally, the outputs r m i
from all m heads are concatenated and processed by a learnable weight matrix w ∈
r d×d to generate cross-attended features c i ∈ r n×d given byfig. 2.
cross-attending the base and reference tissue image features using controllable
cross-modulated layer norm (cln) in our cfb.here, a reference feature h ref i ,
noise z and control parameter αi are input to a mapping network for generating
meta-weights wi. the resulting wi modulates the features via λ(wi) and β(wi) in
our cln. as a result of this controllable feature modulation, the output
features fi enable the generation of tissue images that are diverse yet aligned
with the semantics of the input tissue images.next, we introduce a controllable
feature modulation mechanism in our crosstransformer to further enhance the
diversity and quality of generated images.controllable feature modulation: the
standard cross-attention mechanism, described above, computes locally consistent
features that generate images with reduced artifacts. however, given the
deterministic nature of the cross-attention and the limited set of reference
images, simultaneously generating diverse and high-quality images in the
few-shot setting is still a challenge. to this end, we introduce a controllable
feature modulation mechanism within our cfb that aims at improving the diversity
and quality of generated images. the proposed modulation incorporates
stochasticity as well as enhanced control in the feature aggregation and
refinement steps. this is achieved by utilizing the output of a mapping network
for modulating the visual features in the layer normalization modules in our
crosstransformer.mapping network: the meta-weights w i ∈ r d are obtained by the
mapping network as,where ψ α (•) and ψ z (•) are linear transformations, z ∼ n
(0, 1) is a gaussian noise vector, and α i is control parameter. g ref i is
global-level feature computed from the reference features h ref i through a
linear transformation and a global average pooling operation. the meta-weights w
i are then used for modulating the features in our cross-modulated layer
normalization, as described below.
our cln learns sample-dependent modulation weights for normalizing features
since it is desired to generate images that are similar to the few-shot samples.
such a dynamic modulation of features enables our framework to generate images
of high-quality and diversity. to this end, we utilize the meta-weights w i for
computing the modulation parameters λ and β in our layer normalization modules.
with the cross-attended feature c i as input, our cln modulates the input to
produce an output feature o i ∈ r n×d , given bywhere μ and σ 2 are the
estimated mean and variance of the input c i . here, λ(w i ) is computed as the
element-wise multiplication between meta-weights w i and sample-independent
learnable weights λ ∈ r d , as λ w i . a similar computation is performed for
β(w i ). consequently, our proposed normalization mechanism achieves a
controllable modulation of the input features based on the reference image
inputs and enables enhanced diversity and quality in the generated images. the
resulting features o i are then passed through a feed-forward network (ffn)
followed by another cln for preforming point-wise feature refinement, as shown
in fig. 2. afterwards, the cross-attended features f i are aggregated using
control parameters α i to obtain the fused feature representationfinally, the
decoder f d generates the final image x.
training: the whole framework is trained end-to-end following the hinge version
gan [17] formulation. with generator f g denoting our encoder, cfb and decoder
together, and discriminator f dis , the adversarial loss l adv is given
byandadditionally, to encourage the generated image x to be perceptually similar
to the reference images based on the specified control parameters α, we use a
parameterized formulation of the standard perceptual loss [11], given
bywheremoreover, a classification loss l cl enforces that the images generated
by the decoder are classified into the corresponding class of the input few-shot
samples.our xm-gan is then trained using the formulation:where η p and η cl are
hyperparameters for weighting the loss terms.inference: during inference,
multiple high-quality and diverse images x are generated by varying the control
parameter α i for a set of fixed k-shot samples. while a base image x b and α i
can be randomly selected, our framework enables a user to have control over the
generation based on the choice of α i values.
we conduct experiments on human colorectal cancer dataset [14]. the dataset
consist of 8 categories of colorectal tissues, tumor, stroma, lymph, complex,
debris, mucosa, adipose, and empty with 625 per categories. to enable few-shot
setting, we split the 8 categories into 5 seen (for training) and 3 unseen
categories (for evaluation) with 40 images per category. we evaluate our
approach using two metrics: frèchet inception distance (fid) [8] and learned
perceptual image patch similarity (lpips) [24]. our encoder f e and decoder f d
both have five convolutional blocks with batch normalization and leaky-relu
activation, as in [7]. the input and generated image size is 128 × 128. the
linear transformation ψ(•) is implemented as a 1 × 1 convolution with input and
output channels set to d. the weights η p and η cl are set to 50 and 1. we set k
= 3 in all the experiments, unless specified otherwise. our xm-gan is trained
with a batchsize of 8 using the adam optimizer and a fixed learning rate of 10
-4 .
fs tissue image generation: in tab. 1, we compare our xm-gan approach for fs
tissue image generation with state-of-the-art lofgan [7] on [14] dataset.our
proposed xm-gan that utilizes dense aggregation of relevant local information at
a global receptive field along with controllable feature modulation outperforms
lofgan with a significant margin of 30.1, achieving fid score of 55.8.
furthermore, our xm-gan achieves a better lpips score. in fig. 3, we present a
qualitative comparison of our xm-gan with lofgan [7].
here, we present our ablation study to validate the merits of the proposed
contributions. table 3 shows the baseline comparison on the [14] dataset. our
baseline comprises an encoder, a standard cross-transformer with standard layer
normalization (ln) layers and a decoder. this is denoted as baseline.baseline+pl
refers to extending the baseline by also integrating the standard perceptual
loss. we conduct an additional experiment using random values of α i s.t. i α i
= 1 for computing the fused feature f and parameterized perceptual loss (eq. 5).
we refer to this as baseline+ppl. our final xm-gan referred here as
baseline+ppl+cln contains the novel cfb. within our cfb, we also validate the
impact of the reference features for feature modulation by computing the
meta-weights w i using only the gaussian noise z in eq. 2. this is denoted here
as baseline+ppl+cln † . our approach based on the novel cfb achieves the best
performance amongst all baselines.
we conducted a study with a group of ten pathologists having an average subject
experience of 8.5 years. each pathologist is shown a random set of 20 images (10
real and 10 xm-gan generated) and asked to identify whether they are real or
generated. the study shows that pathologists could differentiate between the
ai-generated and real images only 55% time, which is comparable with a random
prediction in a binary classification problem, indicating the ability of our
proposed generative framework to generate realistic colorectal images.
we proposed a few-shot colorectal tissue image generation approach that
comprises a controllable fusion block (cfb) which generates locally consistent
features by performing a dense aggregation of local regions from reference
tissue images based on their similarity to those in the base tissue image. we
introduced a mapping network together with a cross-modulated layer
normalization, within our cfb, to enhance the quality and diversity of generated
images. we extensively validated our xm-gan by performing quantitative,
qualitative and human-based evaluations, achieving state-of-the-art results.
the covid-19 pneumonia pandemic has posed an unprecedented global health crisis,
with lung imaging as a crucial tool for identifying and managing affected
individuals [16]. the commonly used imaging modalities for covid-19 diagnosis
are chest x-rays and chest computerized tomography (ct). the latter has been the
preferred method for detecting acute lung manifestations of the virus due to its
exceptional imaging quality and ability to produce a 3d view of the lungs.
effective segmentation of covid-19 infections using ct can provide valuable
insights into the disease's development, prediction of the pathological stage,
and treatment response beyond just screening for covid-19 cases. however, the
current method of visual inspection by radiologists for segmentation is
time-consuming, requires specialized skills, and is unsuitable for large-scale
screening. automated segmentation is crucial, but it is also challenging due to
three factors: the infected regions often vary in shape, size, and location,
appear similar to surrounding tissues, and can disperse within the lung cavity.
the success of deep convolutional neural networks (dcnns) in image segmentation
has led researchers to apply this approach to covid-19 segmentation using ct
scans [7,14,17]. however, dcnns require large-scale annotated data to explore
feature representations effectively. unfortunately, publicly available ct scans
with pixel-wise annotations are relatively limited due to high imaging and
annotation costs and data privacy concerns. this limited data scale currently
constrains the potential of dcnns for covid-19 segmentation using ct scans.in
comparison to ct scans, 2d chest x-rays are a more accessible and costeffective
option due to their fast imaging speed, low radiation, and low cost, especially
during the early stages of the pandemic [21]. for example, the chestxray dataset
[18] contains about 112,120 chest x-rays used to classify common thoracic
diseases. chestxr dataset [1] contains 17,955 chest x-rays used for covid-19
recognition. we advocate using chest x-ray datasets such as chestxray and
chestxr may benefit covid-19 segmentation using ct scans because of three
reasons: (1) supplement limited ct data and contribute to training a more
accurate segmentation model; (2) provide large-scale chest x-rays with labeled
features, including pneumonia, thus can help the segmentation model to recognize
patterns and features specific to covid-19 infections; and (3) help improve the
generalization of the segmentation model by enabling it to learn from different
populations and imaging facilities. inspired by this, in this study, we propose
a new learning paradigm for covid-19 segmentation using ct scans, involving
training the segmentation model using limited ct scans with pixelwise
annotations and unpaired chest x-ray images with image-level labels.to achieve
this, an intuitive solution is building independent networks to learn features
from each modality initially. afterward, late feature fusion, coattention or
cross-attention modules are incorporated to transfer knowledge between ct and
x-ray [12,13,22,23]. however, this solution faces two limitations. first,
building modality-specific networks may cause insufficient interaction between
ct and x-ray, limiting the model's ability to integrate information effectively.
although "chilopod"-shaped multi-modal learning [6] has been proposed to share
all cnn kernels across modalities, it is still limited when the different
modalities have a significant dimension gap. second, the presence of unpaired
data, specifically ct and x-ray data, in the feature fusion/crossattention
interaction can potentially cause the model to learn incorrect or irrelevant
information due to the possible differences in their image distributions and
objectives, leading to reduced covid-19 segmentation accuracy. it's worth noting
that the method using paired multimodal data [2] is not suitable for our
application scenario, and the latest unpaired cross-modal [3] requires
pixel-level annotations for both modalities, while our method can use x-ray
images with image-level labels for training.this paper proposes a novel unpaired
cross-modal interaction (uci) learning framework for covid-19 segmentation,
which aims to learn strong representations from limited dense annotated ct scans
and abundant image-level annotated x-ray images. the uci framework learns
representations from both segmentation and classification tasks. it includes
three main components: a multimodal encoder for image representations, a
knowledge condensation and interaction module for unpaired cross-modal data, and
task-specific networks. the encoder contains modality-specific patch embeddings
and shared transformer layers. this design enables the network to capture
optimal feature representations for both ct and x-ray images while maintaining
the ability to learn shared representations between the two modalities despite
dimensional differences. to address the challenge of information interaction
between unpaired cross-modal data, we introduce a momentum-updated prototype
learning strategy to condense modality-specific knowledge. this strategy groups
similar representations into the same prototype and iteratively updates the
prototypes with a momentum term to capture essential information in each
modality. therewith, a knowledge-guided interaction module is developed that
accepts the learned prototypes, enabling the uci to better capture critical
features and relationships between the two modalities. finally, the
task-specific networks, including the segmentation decoder and classification
head, are presented to learn from all available labels. the proposed uci
framework has significantly improved performance on the public covid-19
segmentation benchmark [15], thanks to the inclusion of chest x-rays.the main
contributions of this paper are three-fold: (1) we are the first to employ
abundant x-ray images with image-level annotations to improve covid-19
segmentation on limited ct scans, where the ct and x-ray data are unpaired and
have potential distributional differences; (2) we introduce the knowledge
condensation and interaction module, in which the momentum-updated prototype
learning is offered to concentrate modality-specific knowledge, and a
knowledgeguided interaction module is proposed to harness the learned knowledge
for boosting the representations of each modality; and (3) our experimental
results demonstrate our uci learning method's effectiveness and strong
generalizability in covid-19 segmentation and the potential for related disease
screening. this suggests that the proposed framework can be a valuable tool for
medical practitioners in detecting and identifying covid-19 and other associated
diseases.
the proposed uci aims to explore effective representations for covid-19
segmentation by leveraging both limited dense annotated ct scans and abundant
image-level annotated x-rays. figure 1 illustrates the three primary components
of the uci framework: a multi-modal encoder used to extract features from each
modality, the knowledge condensation and interaction module used to model
unpaired cross-modal dependencies, and task-specific heads designed for
segmentation and classification purposes.
the multi-modal encoder f(•) consists of three stages of blocks, with
modalityspecific patch embedding layers and shared transformer layers in each
block, capturing modality-specific and shared patterns, which can be more robust
and discriminative across modalities. notice that due to the dimensional gap
between ct and x-ray, we use the 2d convolution block as patch embedding for
x-rays and the 3d convolution block as patch embedding for cts. in each stage,
the patch embedding layers down-sample the inputs and generate the sequence of
modality-specific embedded tokens. the resultant tokens, combined with the
learnable positional embedding, are fed into the shared transformer layers for
long-term dependency modeling and learning the common patterns. more details
about architecture can be found in the appendix.given a ct volume x ct , and a
chest x-ray image x cxr , we denote the output feature sequence of the
multi-modal encoder aswhere c ct and c cxr represent the channels of ct and
x-ray feature sequence.n ct and n cxr means the length of ct and x-ray feature
sequence.
knowledge condensation. it is difficult to directly learn cross-modal
dependencies using the features obtained by the encoder because ct and x-ray
data were collected from different patients. this means that the data may not
have a direct correspondence between two modalities, making it challenging to
capture their relationship. as shown in fig. 1(a), we design a knowledge
condensation (kc) module by introducing a momentum-updated prototype learning
strategy to condensate valuable knowledge in each modality from the learned
features. for the x-ray modality, given its prototypes p cxr = {p cxr 1 , p cxr
2 , ..., p cxr k } initialized randomly and the feature sequence f cxr , kc
module first reduces the spatial resolution of f cxr and groups the reduced f
cxr into k prototypes by calculating the distance between each feature point and
prototypes, shown as followswhere c cxr i suggests the feature points closing to
the i-th prototype. σ(•) represents a linear projection to reduce the feature
sequence length to relieve the computational burden. then we introduce a
momentum learning function to update the prototypes with c cxr i , which means
that the updates at each iteration not only depend on the current c cxr i but
also consider the direction and magnitude of the previous updates, defined
aswhere λ is the momentum factor, which controls the influence of the previous
update on the current update. similarly, the prototypes p ct for ct modality can
be calculated and updated with the feature set f ct . the prototypes effectively
integrate the informative features of each modality and can be considered
modality-specific knowledge to improve the subsequent cross-modal interaction
learning. the momentum term allows prototypes to move more smoothly and
consistently towards the optimal position, even in the presence of noise or
other factors that might cause the prototypes to fluctuate. this can result in a
more stable learning process and more accurate prototypes, thus contributing to
condensate the knowledge of each modality better.knowledge-guided interaction.
the knowledge-guided interaction (ki) module is proposed for unpaired
cross-modality learning, which accepts the learned prototypes from one modality
and features from another modality as inputs. as shown in fig. 1(b), the ki
module contains two multi-head attention (mha) blocks. take ct features f ct and
x-ray prototypes p cxr as input example, the first block considers p cxr as the
query and reduced f ct as the key and value of the attention. it embeds the
x-ray prototypes through the calculated affinity map between f ct and p cxr ,
resulting in the adapted prototype p cxr . the first block can be seen as a
warm-up to make the prototype adapt better to the features from another
modality. the second block treats f ct as the query and the concatenation of
reduced f ct and p cxr as the key and value, improving the f ct through the
adapted prototypes. similarly, for the f cxr and p ct as inputs, the ki module
is also used to boost the x-ray representations. inspired by the knowledge
prototypes, ki modules boost the interaction between the two modalities and
allow for the learning of strong representations for covid-19 segmentation and
x-ray classification tasks.
the outputs of the ki module are fed into two multi-task heads -one decoder for
segmentation and one prediction head for classification respectively. the
segmentation decoder has a symmetric structure with the encoder, consisting of
three stages. in each stage, the input feature map is first up-sampled by the 3d
patch embedding layer, and then refined by the stacked transformer layers.
besides, we also add skip connections between the encoder and decoder to keep
more low-level but high-resolution information. the decoder includes a
segmentation head for final prediction. this head includes a transposed
convolutional layer, a conv-in-leakyrelu, and a convolutional layer with a
kernel size of 1 and the output channel as the number of classes. the
classification head contains a linear layer with the output channel as the
number of classes for prediction. we use the deep supervision strategy by adding
auxiliary segmentation losses (i.e., the sum of the dice loss and cross-entropy
loss) to the decoder at different scales. the cross-entropy loss is used to
optimize the classification task.
we used the public covid-19 segmentation benchmark [15] to verify the proposed
uci. it is collected from two public resources [5,8] on chest ct images
available on the cancer imaging archive (tcia) [4]. all ct images were acquired
without intravenous contrast enhancement from patients with positive reverse
transcription polymerase chain reaction (rt-pcr) for sars-cov-2. in total, we
used 199 ct images including 149 training images and 50 test images. we also
used two chest x-ray-based classification datasets including chestx-ray14 [18]
and chestxr [1] to assist the uci training. the chestx-ray14 dataset comprises
112,120 x-ray images showing positive cases from 30,805 patients, encompassing
14 disease image labels pertaining to thoracic and lung ailments. an image may
contain multiple or no labels. the chestxr dataset consists of 21,390 samples,
with each sample classified as healthy, pneumonia, or covid-19.
for ct data, we first truncated the hu values of each scan using the range of
[-958, 327] to filter irrelevant regions, and then normalized truncated voxel
values by subtracting 82.92 and dividing by 136.97. we randomly cropped
subvolumes of size 32 × 256 × 256 as the input and employed the online data
augmentation like [10] to diversify the ct training set. for chest x-ray data,
we set the size of input patches to 224 × 224. we employ the online data
argumentation, including random cropping and zooming, random rotation, and
horizontal/vertical flip, to enlarge the x-ray training dataset. we follow the
extension of [20] for weight initialization and use the adamw optimizer [11] and
empirically set the initial learning rate to 0.0001, batch size to 2 and 32 for
segmentation and classification, maximum iterations to 25w, momentum factor λ to
0.99, and the number of prototypes k to 256.to evaluate the covid-19
segmentation performance, we utilized six metrics, including the dice similarity
coefficient (dsc), intersection over union (iou), sensitivity (sen), specificity
(spe), hausdorff distance (hd), and average surface distance (asd). these
metrics provide a comprehensive assessment of the segmentation quality. the
overlap-based metrics, namely dsc, iou, sen, and spe, range from 0 to 1, with a
higher score indicating better performance. on the other hand, hd and asd are
shape distance-based metrics that measure the dissimilarity between the surfaces
or boundaries of the segmentation output and the ground truth. for hd and asd, a
lower value indicates better segmentation results.
table 1 gives the performance of our models and four advanced competing ones,
including nnunet [10], cotr [19], nnformer [24], and swin unetr [9] in covid-19
lesion segmentation. the results demonstrate that our uci, which utilizes
inexpensive chest x-rays, outperforms all other methods consistently and
significantly, as evidenced by higher dice and iou scores. this suggests that
the segmentation outcomes generated by our models are in good agreement with the
ground truth. notably, despite chestxr being more focused on covid-19
recognition, the uci model aided by the chestx-ray14 dataset containing 80k
images performs better than the uci model using the chestxr dataset with only
16k images. this suggests that having a larger auxiliary dataset can improve the
segmentation performance even if it is not directly related to the target task.
the results also further prove the effectiveness of using a wealth of chest
x-rays to assist the covid-19 segmentation under limited cts. finally, our uci
significantly reduces hd and asd values compared to competing approaches. this
reduction demonstrates that our segmentation results provide highly accurate
boundaries that closely match the ground-truth boundaries.
ablations. we perform ablation studies over each component of uci, including the
multi-modal encoder, knowledge condensation (kc) and knowledge interaction (ki)
models, as listed in fig. 2. we set the maximum iterations to 8w and use
chestx-ray14 as auxiliary data for all ablation experiments. we compare five
variants of our uci: (1) baseline: trained solely on densely annotated ct
images;(2) w/o shared encoder: replacing the multi-modal encoder with two
independent encoders, each designed to learn features from a separate modality;
(3) w/o kc: removing the prototype and using the features before kc for
interaction; (4) w/o kc & ki: only with encoder to share multi-modal
information; and (5) w/o warm-up: removing the prototype warm-up in ki. figure 2
reveals several noteworthy conclusions. firstly, our uci model, which jointly
uses chest x-rays, outperforms the baseline segmentation results by up to 1.69%,
highlighting the effectiveness of using cheap large-scale auxiliary images.
secondly, using only a shared encoder for multi-modal learning (uci w/o kc & ki)
can still bring a segmentation gain of 0.96%, and the multi-modal encoder
outperforms building independent modality-specific networks (uci w/o shared
encoder), underscoring the importance of shared networks. finally, our results
demonstrate the effectiveness of the prototype learning and prototype warm-up
steps.hyper-parameter settings. to evaluate the impact of hyper-parameter
settings on covid-19 segmentation, we conducted an investigation of the number
of prototypes (k) and the number of momentum factors (λ). figure 3 illustrates
the dice scores obtained on the test set for different values of k and λ,
providing insights into the optimal settings for these hyper-parameters.
our study introduces uci, a novel method for improving covid-19 segmentation
under limited ct images by leveraging unpaired x-ray images with imagelevel
annotations. especially, uci includes a multi-modal shared encoder to capture
optimal feature representations for ct and x-ray images while also learning
shared representations between the two modalities. to address the challenge of
information interaction between unpaired cross-modal data, uci further develops
a kc and ki module to condense modality-specific knowledge and facilitates
cross-modal interaction, thereby enhancing segmentation training. our
experiments demonstrate that the uci method outperforms existing segmentation
models for covid-19 segmentation.
fig. 2. effectiveness of each module in uci.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1 58.
instances with complex shapes arise in many biomedical domains, and their
morphology carries critical information. for example, the structure of gland
tissues in microscopy images is essential in accessing the pathological stages
for cancer diagnosis and treatment. these instances, however, are usually
closely in touch with each other and have non-convex structures with parts of
varying widths (fig. 1a), posing significant challenges for existing
segmentation methods.in the biomedical domain, most methods [3,4,13,14,22] first
learns intermediate representations and then convert them into masks with
standard segmentation algorithms like connected-component labeling and watershed
transform. these representations are not only efficient to predict in one model
forward pass but also able to capture object geometry (i.e., precise instance
boundary), which are hard for top-down methods using low-resolution features for
mask generation. however, existing representations have several restrictions.
for example, boundary map is usually learned as a pixel-wise binary
classification task, which makes the model conduct relatively local predictions
and consequently become vulnerable to small errors that break the connectivity
between adjacent instances (fig. 1b). to improve the boundary map, deep
watershed transform (dwt) [1] predicts the euclidean distance transform (dt) of
each pixel to the instance boundary. this representation is more aware of the
structure for convex objects, as the energy value for centers is significantly
different from pixels close to the boundary. however, for objects with
non-convex morphology, the boundary-based distance transform produces multiple
local optima in the energy landscape (fig. 1c), which tends to break the
intra-instance connectivity when applying thresholding and results in
over-segmentation.to preserve the connectivity of instances while keeping the
precise instance boundary, in this paper, we propose a novel representation
named skeleton-aware distance transform (sdt). our sdt incorporate object
skeleton, a concise and connectivity-preserving representation of object
structure, into the traditional boundary-based distance transform (dt) (fig.
1d). in quantitative evaluations, we show that our proposed sdt achieves leading
performance on histopathology image segmentation for instances with various
sizes and complex structures. specifically, under the hausdorff distance for
evaluating shape similarity, our approach improves the previous state-of-the-art
method by relatively 10.6%.
instance segmentation. bottom-up instance segmentation approaches have become de
facto for many biomedical applications due to the advantage in segmenting
objects with arbitrary geometry. u-net [14] and dcan [4] use fully convolutional
models to predict the boundary map of instances. since the boundary map is not
robust to small errors that can significantly change instance structure,
shape-preserving loss [22] adds a curve fitting step in the loss function to
enforce boundary connectivity. in order to further distinguish closely touching
instances, deep watershed transform (dwt) [1] predicts the distance transform
(dt) that represents each pixel as its distance to the closest boundary.
however, for complex structure with parts of varying width, the boundary-based
dt tends to produce relatively low values for thin connections and consequently
causes oversegmentation. compared to dwt, our sdt incorporates object skeleton
(also known as medial axis) [2,8,24] that concisely captures the topological
connectivity into standard dt to enforce both the geometry and
connectivity.object skeletonization. object skeleton [15] is a one-pixel wide
representation of object masks that can be calculated by topological thinning
[8,12,24] or medial axis transform [2]. the vision community has been working on
direct object skeletonization from images [7,9,16,19]. among the works, only
shen et al. [16] shows the application of the skeleton on segmenting
single-object images. we instead focus on the more challenging instance
segmentation task with multiple objects closely touching each other. object
skeletons are also used to correct errors in pre-computed segmentation masks
[11]. our sdt framework instead use the skeleton in the direct segmentation from
images.
given an image, we aim to design a new representation e for a model to learn,
which is later decoded into instances with simple post-processing. specifically,
a good representation for capturing complex-structure masks should have two
desired properties: precise geometric boundary and robust topological
connectivity.let ω denote an instance mask, and γ b be the boundary of the
instance (pixels with other object indices in a small local neighborhood). the
boundary (or affinity) map is a binary representation where e| γ b = 0 and e|
ω\γ b = 1. taking the merits of dt in modeling the geometric arrangement and
skeleton in preserving connectivity, we propose a new representation e that
satisfies:here e| ω\γs < e| γs = 1 indicates that there is only one global
maximum for each instance, and the value is assigned to a pixel if and only if
the pixel is on the skeleton. this property avoids ambiguity in defining the
object interior and preserve connectivity. besides, e| ω\γ b > e| γ b = 0
ensures that boundary is distinguishable as the standard dt, which produces
precise geometric boundary.for the realization of e, let x be a pixel in the
input image, and d be the metric, e.g., euclidean distance. the energy function
for distance transform (dt) is defined as e dt (x) = d(x, γ b ), which starts
from 0 at object boundary and increases monotonically when x is away from the
boundary. similarly, we can define an energy function d(x, γ s ) representing
the distance from the skeleton. it vanishes to 0 when the pixel approaches the
object skeleton. formally, we define the energy function of the skeleton-aware
distance transform (fig. 2) aswhere α controls the curvature of the energy
surface1 . when 0 < α < 1, the function is concave and decreases faster when
being close to the boundary, and vice versa when α > 1. in the ablation studies,
we demonstrate various patterns of the model predictions given different
α.besides, since common skeletonization algorithms can be sensitive to small
perturbations on the object boundary and produce unwanted branches, we smooth
the masks before computing the object skeleton by gaussian filtering and
thresholding to avoid complex branches.learning strategy. given the ground-truth
sdt energy map, there are two ways to learn it using a cnn model. the first way
is to regress the energy using l 1 or l 2 loss. in the regression mode, the
output is a single-channel image. the second way is to quantize the [0, 1]
energy space into k bins and rephrase the regression task into a classification
task [1,18], which makes the model robust to small perturbations in the energy
landscape. for the classification mode, the model output has (k +1) channels
with one channel representing the background region. we fix the bin size to 0.1
without tweaking, making k = 10. softmax is applied before calculating the
cross-entropy loss. we test both learning strategies in the experiments to
illustrate the optimal setting for sdt.
network architecture. directly learning the energy function with a fully
convolutional network (fcn) can be challenging. previous approaches either first
regress an easier direction field representation and then use additional layers
to predict the desired target [1], or take the multi-task learning approach to
predict additional targets at the same time [16,21,22]. fortunately, with recent
progress in fcn architectures, it becomes feasible to learn the target energy
map in an end-to-end fashion. specifically, in all the experiments, we use a
deeplabv3 model [5] with a resnet [6] backbone to directly learn the sdt energy
without additional targets (fig. 3, training phase). we also add a coordconv
[10] layer before the 3rd stage in the backbone network to introduce spatial
information into the segmentation model.
there is an inconsistency problem in object skeleton generation: part of the
complete instance skeleton can be different from the skeleton of the instance
part (fig. 4). some objects may touch the image border due to either a
restricted field of view (fov) of the imaging devices or spatial data
augmentation like the random crop. if pre-computing the skeleton, we will get
local skeleton (fig. 4c) for objects with missing masks due to imaging
restrictions, and partial skeleton (fig. 4b) due to spatial data augmentation,
which causes ambiguity. therefore we calculate the local skeleton for sdt
on-the-fly after all spatial transformations instead of pre-computing to prevent
the model from hallucinating the structure of parts outside of the currently
visible region. in inference, we always run predictions on the whole images to
avoid inconsistent predictions. we use the skeletonization algorithm in lee et
al. [8], which is less sensitive to small perturbations and produces skeletons
with fewer branches.instance extraction from sdt. in the sdt energy map, all
boundary pixels share the same energy value and can be processed into segments
by direct thresholding and connected component labeling, similar to dwt [1].
however, since the prediction is never perfect, the energy values along closely
touching boundaries are usually not sharp and cause split-errors when applying a
higher threshold or merge-errors when applying a lower threshold. therefore we
utilize a skeleton-aware instance extraction (fig. 3, inference phase) for sdt.
specifically, we set a threshold θ = 0.7 so that all pixels with the predicted
energy bigger than θ are labeled as skeleton pixels. we first perform connected
component labeling of the skeleton pixels to generate seeds and run the
watershed algorithm on the reversed energy map using the seeds as basins (local
optima) to generate the final segmentation. we also follow previous works [4,22]
and refine the segmentation by hole-filling and removing small spurious objects.
accurate instance segmentation of gland tissues in histopathology images is
essential for clinical analysis, especially cancer diagnosis. the diversity of
object appearance, size, and shape makes the task challenging.dataset and
evaluation metric. we use the gland segmentation challenge dataset [17] that
contains colored light microscopy images of tissues with a wide range of
histological levels from benign to malignant. there are 85 and 80 images in the
training and test set, respectively, with ground truth annotations provided by
pathologists. according to the challenge protocol, the test set is further
divided into two splits with 60 images of normal and 20 images of abnormal
tissues for evaluation. three evaluation criteria used in the challenge include
instance-level f1 score, dice index, and hausdorff distance, which measure the
performance of object detection, segmentation, and shape similarity,
respectively. for the instance-level f1 score, an iou threshold of 0.5 is used
to decide the correctness of a prediction.
we compare sdt with previous state-of-the-art segmentation methods, including
dcan [4], multi-channel network (mcn) [21], shape-preserving loss (spl) [22] and
fullnet [13]. we also compare with suggestive annotation (sa) [23], and sa with
model quantization (qsa) [20], which use multiple fcn models to select
informative training samples from the dataset. with the same training settings
as our sdt, we also report the performance of skeleton with scales (ss) and
traditional distance transform (dt). training and inference. since the training
data is relatively limited due to the challenges in collecting medical images,
we apply pixel-level and spatial-level augmentations, including random
brightness, contrast, rotation, crop, and elastic transformation, to alleviate
overfitting. we set α = 0.8 for our sdt in eq. 2. we use the classification
learning strategy and optimize a model with 11 output channels (10 channels for
energy quantized into ten bins and one channel for table 1. comparison with
existing methods on the gland segmentation. our sdt achieves better or on par f1
score and dice index, and significantly better hausdorff distance for evaluating
shape similarity. dt and ss represent distance transform and skeleton with
scales. [4] 0.912 0.716 0.897 0.781 45.42 160.35 mcn [21] 0.893 0.843 0.908
0.833 44.13 116.82 spl [22] 0.924 0.844 0.902 0.840 49.88 106.08 sa [23] 0.921
0.855 0.904 0.858 44.74 96.98 fullnet [13] specifically for ss, we set the
number of output channels to two, with one channel predicting skeleton
probability and the other predicting scales. since the scales are non-negative,
we add a relu activation for the second channel and calculate the regression
loss. masks are generated by morphological dilation. we do not quantize the
scales as dt and sdt since even ground-truth scales can yield masks unaligned
with the instance boundary with quantization.
our sdt framework achieves state-of-the-art performance on 5 out of 6 evaluation
metrics on the gland segmentation dataset (table 1). with the better
distinguishability of object interior and boundary, sdt can unambiguously
separate closely touching instances (fig. 5, first two rows), performs better
than previous methods using object boundary representations [4,22]. besides,
under the hausdorff distance for evaluating shape-similarity between
ground-truth and predicted masks, our sdt reports an average score of 44.82
across two test splits, which improves the previous state-of-the-art approach
(i.e., fullnet with an average score of 50.15) by 10.6%. we also notice the
different sensitivities of the three evaluation metrics. taking the instance d
(fig. 5, 3rd row) as an example: both sdt and fullnet [13] have 1.0 f1-score
(iou threshold 0.5) for the correct detection; sdt has a slightly higher dice
index (0.956 vs. 0.931) for better pixel-level classification; and our sdt has
significantly lower hausdorff distance (24.41 vs. 48.81) as sdt yields a mask
with much more accurate morphology.
loss function. we compare the regression mode using l1 and l2 losses with the
classification mode using cross-entropy loss. there is a separate channel for
background under the classification mode where the energy values are quantized
into bins. however, for regression mode, if the background value is 0, we need
to use a threshold τ > 0 to decide the foreground region, which results in
shrank masks. to separate the background region from the foreground objects, we
assign an energy value of -b to the background pixels (b ≥ 0). to facilitate the
regression, given the predicted value ŷi for pixel i, we apply a sigmoid
function (σ) and affine transformation so that ŷ i = (1+b) • σ(ŷ i )b has a
range of (-b, 1). we set b = 0.1 for the experiments. we show that under the
same settings, the model trained with quantized energy reports the best results
(table 2). we also notice that the model trained with l 1 loss produces a much
sharper energy surface than the model trained with l 2 loss, which is
expected.curvature. we also compare different α in eq. 2 that controls the
curvature of the energy landscape. table 2 shows that α = 0.8 achieves the best
overall performance, which is slightly better than α = 1.0. decreasing α to 0.6
introduces more merges and make the results worse.global/local skeleton. in
sect. 2.2 we show the inconsistency problem of global and local skeletons. in
this study, we set α = 0.8 and let the model learn the pre-computed sdt energy
for the training set. the results show that pre-computed sdt significantly
degrades performance (table 2). we argue this is because pre-computed energy not
only introduces inconsistency for instances touching the image border but also
restricts the diversity of sdt energy maps.
in this paper, we introduce the skeleton-aware distance transform (sdt) to
capture both the geometry and topological connectivity of instance masks with
complex shapes. for multi-class problems, we can use class-aware semantic
segmentation to mask the sdt energy trained for all objects that is agnostic to
their classes. we hope this work can inspire more research on not only better
representations of object masks but also novel models that can better predict
those representations with shape encoding. we will also explore the application
of sdt in the more challenging 3d instance segmentation setting.
image segmentation is one of the most fundamental and popular tasks in medical
image analysis. it is widely applied to parse organs, bones, soft tissues, or
lesions in n -d medical images. conventional methods rely on statistics of image
intensities or object shapes to infer the boundaries of the target regions [8].
recently, the convolutional neural networks (cnn) demonstrated superior
performance in multiple tasks. cnns are inherently translation-invariant with
inneighborhood computation, which makes training efficient and deployment
effective. for instance, the u-shaped encoder-decoder cnn is greatly favored
among segmentation models due to its simplicity and effectiveness [16]. despite
the success, adding new components to existing models in pursuit of better
performance and efficiency is always an ongoing effort in different research
fields.inspired by the advancement from related domains, e.g., natural language
processing (nlp), transformers [24] have been successfully introduced to image
processing and computer vision [7]. the transformer block is crafted with
longrange dependency inside sequences with marginal inductive bias. to
incorporate a transformer into image analysis models, images are divided into
patches with equal size and serialized as a sequence of tokens, so that the
transformer based models can treat n -d images in the same way as 1-d sentences.
such operation explicitly destroys neighborhood relationships in the images, and
instead learnable position embeddings are added to encourage the learning of
flexible patch interaction. in addition to supervised tasks, transformer-based
models have also been shown to achieve superior performance in pre-training with
largescale (labeled/unlabeled) data sets [11].most existing neural architectures
are designed with strong human heuristics. neural architecture search (nas) has
been proposed in an attempt to reduce dependency on such heuristics while
optimizing model performance for given tasks. given target constraints, it is
capable of optimizing multiple objectives (e.g., accuracy, memory consumption,
latency, etc.) of the neural network models at the same time. nowadays, nas has
been widely applied for many applications in medical imaging including image
classification and segmentation.existing nas works have been focusing on
optimization with convolutional deep learning components [18,31]. our proposed
nas method, named dast, on the other hand learns the relationship between
convolutions and transformers within the search space of segmentation networks.
during architecture searching, those two operations can be placed at different
scale resolutions and levels for performance optimization. intrinsically, it
shall benefit from inductive biases of these two popular deep learning
ingredients. meanwhile, dast is also equipped with capacities of optimizing
memory consumption of the searched architecture, so that the input shape of the
neural network can be properly adjusted according to the available computing
resource, and long-range dependency of transformers can be visualized through
attention matrices. we evaluated our proposed algorithm on two public data sets
with excellent performance.
neural architecture search tries to find optimal global model structures and
local operations from large search spaces for different applications. searching
algorithms, including reinforcement learning and genetic algorithms [2,27,31],
have been proposed for different search spaces. these approaches usually require
large-scale computing resources to train a large number of independent neural
networks, which makes them less practical when applied to large-scale data sets.
on the contrary, differentiable neural architecture search (darts) aims to boost
search efficiency and reduce computation budgets via continuous relaxation in
the optimization [6,9,12,17,18,26,30]. it defines a large super-net containing
all network candidates with learnable intermediate path weights, such that
optimizing model architecture is equivalent to optimizing and binarizing those
path weights. however, most existing nas algorithms in medical imaging rely
heavily on fully convolution-based search spaces, which may limit its receptive
field.transformer based neural network has been recently introduced to medical
imaging domain for various applications following the success of vision
transformer (vit) in computer vision [7]. the direct extension applies vit to 2d
medical image analysis [21,23]. some works have adopted vit for 3d medical image
segmentation [3,4,10,22,25,29] via serializing 3d images as sequences of
patches/cubes. most works rely on conventional designs of neural architectures
and replace the convolution operations with transformers. for instance, the
segmentation networks are always in "symmetric" encoder-decoder structure.
however, from a network design's perspective, it is not trivial to find the
right balance between convolutions and transformers inside the architecture.
our nas algorithm, dast, is an intuitive extension of dints [12] for 3d medical
image segmentation. like other darts type of algorithms, it requires continuous
relaxation of the super-net search space for gradient descent optimization.
unlike other nas algorithms, it searches for a multi-path neural network and
optimizes the searched architecture with additional memory constraints. large
image inputs can fit the searched architecture with low memory consumption,
which helps to build long-range dependencies for transformers. differentiable
search space. following dints [12], the main search space is defined as r × l
grids with r resolution scales and l levels from input image to output
segmentation shown in fig. 1. the grid node n i,j (at resolution i and level j)
has directed connections towards neighboring nodes n i-1,j+1 , n i,j+1and n
i+1,j+1 . each edge e of connection is a weighted combination of outputs from
operations o, and the pool of o includes skip-connection, convolution and
transformer. for neighboring nodes at different levels, additional ×2
up-sampling or down-sampling is added in e, as well as convolutions to match
feature channel numbers. there is no additional operation when passing features
between nodes at the same level. to optimize the architecture, two different
types of weights are introduced. each edge has weight w e , and each operation
has weight w o . then the searched architecture can be defined when all w e and
w o are binarized.the stem cells are concatenated at input and output of the
search space. the input stem cells down-samples image toward different
resolution scales and fit image features to the search space. the output stem
cells up-samples multiscale features out of search space with necessary
concatenation to produce multichannel probability maps. transformer. we
introduce transformer [24] into the search space as a candidate of the operation
pool as shown in fig. 1. the input and output of transformer are with dimension
c × n (c is feature dimension and n is length of the sequence). however, this
dimension is not suitable for networks with high dimensional image features.
therefore, we add a convolutional projector p [5] before the transformer, aiming
to resize and project features x c×h×w×d to a smaller size with the number of
channels matching the number of tokens required by the transformer (h, w, d
denotes height, width, depth of 3d patches). another input of projector is a
learnable 3d positional encoding p shown in eq. 1.the positional embedding p in
eq. 2 is initialized as a normalized 3d position map. it is efficient to compute
with only 3 channels.the full transformer with an encoder and a decoder is
adopted to process the output of the projector. the decoder takes additional
learnable query embedding as input. the output of the transformer shares the
same dimension/shape with the input. next, we need to add another reversed
projector to map the 1-d feature map back to the 3d shape of input. segmentation
attention. to further understand the self-attention scheme, we embed an
additional multi-head self-attention layer a after the transformer.a uses the
feature maps from the transformer as the semantic query q, and the features from
the transformer encoder as key k. unlike multi-head self-attention layers inside
the transformer, a does not have residual connection. thus, a is enforced to
learn meaningful attention weights for segmentation tasks. the attention weights
are directly multiplied with intermediate features maps, which can be reshaped
from 1-d to 3-d for visual interpretation. memory estimation. like dints [12],
the memory budget constraints were proposed as part of loss functions to
optimize memory usage at training and inference. it requires to estimate peak
memory usage in each operation given the fixed input shape. since the token is
designed to have the same dimension c as the channel dimension at different
resolution scales, the memory consumption estimation of entire transformer is
shown in eq. 3:in practice, the number of token l is fixed as 512 to avoid
potential memory explosion. eight heads are used in each multi-head
self-attention layer. number of operations n is approximately estimated as 15
including convolutions, batch normalization, linear operations, layer
normalization, and multi-head selfattention.
we adopted large-scale data sets task07 pancreas from medical segmentation
decathlon (msd) [1] as used in [12] for architecture searching, and kits'19
[13,14] to validate the searched architectures. both are very challenging
applications involving various fields of view of ct volumes and different types
of pathology. the pancreas data set has 3-class segmentation labels (background,
pancreas and tumor) for 282 ct volumes. we adopt entire labeled set for nas with
the same data split as [12]: 114 volumes for model training, 114 volumes for
architecture search, and 54 volumes for model validation. a different 4 : 1 data
split is used for experiments of training from scratch. the kits'19 data set has
3-class segmentation labels (background, kidney and tumor) for 210 ct volumes,
and an additional standalone 90 test volumes (with hidden ground truth) for the
public leaderboard. we train the searched models with 5-fold data split, and
verify the model performance on the test set using the public leaderboard. all
data sets are re-sampled into the isotropic voxel spacing 1.0 mm for both images
and labels. for both ct data sets, the voxel intensities of the images are
normalized to the range [0, 1] according to the 5 th and 95 th percentile of
overall foreground intensities.a combination of dice loss and cross-entropy loss
is adopted to minimize both global and pixel-wise distance between ground truth
and predictions. the nas is conducted through conventional bi-level optimization
following implementation 1 . we use the same definition of search space with 4
resolution scales and 12 levels. for cell level searching, we only use three
different operations: skip-connection, convolution and transformer. for model
training, we use a large input patch shape 160 3 , and the batch size at each
gpu is 2. the training settings (like data augmentation, optimizer, etc.) are
very similar to the model searching. we keep a constant learning rate 1e -3 to
train the model from scratch for 40, 000 iterations. our experiments are
conducted using monai and trained on eight nvidia v100 gpus with 32 gb of
memory. for searching or training, the time cost is ∼ 15 hours including
training and validation on-the-fly (similar as [12]).
since dints is the closest work to dast, we directly compare the performance on
dints's searching tasks with the same data split. then we re-train the searched
architectures from both methods from scratch. as shown in fig. 2, dast has
better training convergence and validation accuracy compared to dints. the
default model input shapes of dast and dints are different, so we experiment
with various combinations. with input shape 160 3 , dast converges faster than
dins-160 with a better validation curve. the same conclusion can be made with
input shape 96 3 . based on the results, training with smaller input shape would
make the training process harder. dast consistently has better performance than
dints under different settings.
to verify the effectiveness and generalization of our searched architectures
from dast, we validate the searched architecture (from pancreas data set) on
this challenging task. metrics for kidneys and tumors are the average dice score
per case. finally, we evaluate our single-fold model as well as the ensemble
from 5 cross-validation models on the public test leaderboard2 .table 1. kits'19
challenge test-set performance evaluation for kidney and tumor segmentation in
terms of the average dice score per case. the evaluation results of our method
are copied directly from the public leaderboard. based on the results from the
public leaderboard in table . 1, our single-fold model and ensemble of five
models achieve excellent performance compared to all other entries in the
challenge shown in the table . 1. the nnu-net [16] is the best among all other
entries, but the method utilized 20 u-net models with training strategies to
achieve the ensemble result for the challenge. some other entries rely on
cascaded models, which use more complex and intensive training mechanisms. on
the contrary, dast shows great simplicity when transferring a searched
architecture to a new task. it is important to point out that the performance of
our models is not only the best of all entries with publications, but also the
best of all public entries (around 2, 000) on the test leaderboard.
memory constraints. we provide the option to change the parameter controlling
memory consumption budget in the loss function with different values shown in
fig. 3. from the results, we can observe that given different values, the
searched models have a clear trend: for the model with the highest memory (λ =
0.8), transformers are distributed at different resolution scales. as memory
constraints increase (where λ is reduced), transformers are more towards lower
fig. 4. four attention weights visualized with ct images and model predictions.
the first row is from a transformer layer (r = 4, l = 8), and the second row is
from another transformer layer at higher resolution (r = 3, l = 10). in each
case, the left side is the original image, the middle one is the overlaid
display with the attention weights, and the right side is the overlaid display
of the attention weights and segmentation masks. resolution scales. it agrees
with our expectation since transformers normally consume more gpu memory than
convolutions due to several linear operations in large token dimension. on the
other hand, more convolutions are chosen than transformers, which implicitly
suggests that this balance between convolutions and transformers is better for
feature learning in segmentation. another benefit shown in fig. 2 is that
re-training architectures with lower memory constraints would not hurt the model
performance. it is encouraging to see that such combination of convolution and
transformer retains the same-level performance with lower gpu memory costs and
receptive field of the entire model input. attention mechanism. we visualize the
attention weights computed by a dedicated self-attention operation in the
transformer. the attention weight is with shape 512 × 4096 = 512 × 16 3 . then
we take the average from the channel dimension and resize it to a volume with
shape 160 3 by trilinear interpolation (for visualization). we can see that the
self-attention weights of the kidney segmentation consistently focus on the
lower spine or pelvis areas at different transformer layers as evidence of
long-range dependency (fig. 4). one potential explanation could be that kidneys
are located around those areas and both kidneys are on the opposite sides of the
spine. so the information over there can help roughly identify the kidneys from
the whole-body ct. especially specific bones are good bio-markers with high
intensity values in ct. furthermore, to the best of our knowledge, it is the
first time that the multi-head self-attention is visualized for 3d medical image
segmentation.
in this study, we observe that dast is able to find effective and concise
relationships between convolutions and transformers in a single neural network
model. the optimized connections between operations improves the model
effectiveness in various applications. such models benefit from the different
inductive biases introduced by these two operations. adding a memory constraint
loss as an additional objective can lower memory consumption for the searched
architecture. transformers will then benefit more from long-range dependencies
of larger input patches. we hope this perspective will be helpful for different
applications in medical imaging.
the success of deep neural networks heavily relies on the availability of large
and diverse annotated datasets across a range of computer vision tasks. to learn
a strong data representation for robust and performant medical image
segmentation, huge datasets with either many thousands of annotated data
structures or less specific self-supervised pretraining objectives with
unlabeled data are needed [29,33]. the annotation of 3d medical images is a
difficult and laborious task. thus, depending on the task, only a bare minimum
of images and target structures is usually annotated. this results in a
situation where a zoo of partially labeled datasets is available to the
community. recent efforts have resulted in a large dataset of >1000 ct images
with >100 annotated classes each, thus providing more than 100,000 manual
annotations which can be used for pre-training [30]. focusing on such a dataset
prevents leveraging the potentially precious additional information of the above
mentioned other datasets that are only partially annotated. integrating
information across different datasets potentially yields a higher variety in
image acquisition protocols, more anatomical target structures or details about
them as well as information on different kinds of pathologies. consequently,
recent advances in the field allowed utilizing partially labeled datasets to
train one integrated model [21]. early approaches handled annotations that are
present in one dataset but missing in another by considering them as background
[5,27] and penalizing overlapping predictions by taking advantage of the fact
that organs are mutually exclusive [7,28]. some other methods only predicted one
structure of interest for each forward pass by incorporating the class
information at different stages of the network [4,22,31]. chen et al. trained
one network with a shared encoder and separate decoders for each dataset to
generate a generalized encoder for transfer learning [2]. however, most
approaches are primarily geared towards multi-organ segmentation as they do not
support overlapping target structures, like vessels or cancer classes within an
organ [6,8,12,23]. so far, all previous methods do not convincingly leverage
cross-dataset synergies. as liu et al. pointed out, one common caveat is that
many methods force the resulting model to average between distinct annotation
protocol characteristics [22] by combining labels from different datasets for
the same target structure (visualized in fig. 1 b)). hence, they all fail to
reach segmentation performance on par with cutting-edge single dataset
segmentation methods. to this end, we introduce multitalent (multi dataset
learning and pre-training), a new, flexible, multi-dataset training method: 1)
multitalent can handle classes that are absent in one dataset but annotated in
another during training. 2) it retains different annotation protocol
characteristics for the same target structure and 3) allows for overlapping
target structures with different level of detail such as liver, liver vessel and
liver tumor. overall, mul-titalent can include all kinds of new datasets
irrespective of their annotated target structures.multitalent can be used in two
scenarios: first, in a combined multi-dataset (md) training to generate one
foundation segmentation model that is able to predict all classes that are
present in any of the utilized partially annotated datasets, and second, for
pre-training to leverage the learned representation of this foundation model for
a new task. in experiments with a large collection of abdominal ct datasets, the
proposed model outperformed state-of-the-art segmentation networks that were
trained on each dataset individually as well as all previous methods that
incorporated multiple datasets for training. interestingly, the benefits of
multitalent are particularly notable for more difficult classes and pathologies.
in comparison to an ensemble of single dataset solutions, multitalent comes with
shorter training and inference times. additionally, at the example of three
challenging datasets, we demonstrate that fine-tuning multitalent yields higher
segmentation performance than training from scratch or initializing the model
parameters using unsupervised pretraining strategies [29,33]. it also surpasses
supervised pretrained and fine-tuned state-of-the art models on most tasks,
despite requiring orders of magnitude less annotations during pre-training.
we introduce multitalent, a multi dataset learning and pre-training method, to
train a foundation medical image segmentation model. it comes with a novel
dataset and class adaptive loss function. the proposed network architecture
enables the preservation of all label properties, learning overlapping classes
and the simultaneous prediction of all classes. furthermore, we introduce a
training schedule and dataset preprocessing which balances varying dataset size
and class characteristics.
we begin with a dataset collection of k datasets, where c (k) ⊆ c is the label
set associated to dataset d (k) . even if classes from different datasets refer
to the same target structure we consider them as unique, since the exact
annotation protocols and labeling characteristics of the annotations are unknown
and can vary between datasets: c (k) ∩ c (j) = ∅, ∀k = j. this implies that the
network must be capable of predicting multiple classes for one voxel to account
for the inconsistent class definitions.
network modifications. we employ three different network architectures, which
are further described below, to demonstrate that our approach is applicable to
any network topology. to solve the label contradiction problem we decouple the
segmentation outputs for each class by applying a sigmoid activation function
instead of the commonly used softmax activation function across the dataset. the
network shares the same backbone parameters θ but it has independent
segmentation head parameters θ c for each class. the sigmoid probabilities for
each class are defined as ŷc = f (x, θ, θ c ). this modification allows the
network to assign multiple classes to one pixel and thus enables overlapping
classes and the conservation of all label properties from each dataset.
consequently, the segmentation of each class can be thought of as a binary
segmentation task. while the regular dice loss is calculated for each image
within a batch, we calculate the dice loss jointly for all images of the input
batch. this regularizes the loss if only a few voxels of one class are present
in one image and a larger area is present in another image of the same batch.
thus, an inaccurate prediction of a few pixels in the first image has a limited
effect on the loss. in the following, we unite the sum over the image voxels i
and the batch b to z . we modify the loss function to be calculated only for
classes that were annotated in the corresponding partially labeled dataset
[5,27], in the following indicated by 1and 0 otherwise. instead of averaging, we
add up the loss over the classes. hence, the loss signal for each class
prediction does not depend on the number of other classes within the batch. this
compensates for the varying number of annotated classes in each dataset.
otherwise, the magnitude of the loss e.g. for the liver head from d1 (2 classes)
would be much larger as for d7 (13 classes). gradient clipping captures any
potential instability that might arise from a higher loss magnitude:network
architectures. to demonstrate the general applicability of this approach, we
applied it to three segmentation networks. we employed a 3d u-net [24], an
extension with additional residual blocks in the encoder (resenc u-net), that
demonstrated highly competitive results in previous medical image segmentation
challenges [14,15] and a recently proposed transformer based architecture
(swinunetr [29]). we implemented our approach in the nnu-net framework [13].
however, the automatic pipeline configuration from nnu-net was not used in favor
of a manually defined configuration that aims to reflect the peculiarities of
each of the datasets, irrespective of the number of training cases they contain.
we manually selected a patch size of [96,192,192] and image spacing of 1mm in
plane and 1.5mm for the axial slice thickness, which nnu-net used to
automatically create the two cnn network topologies. for the swinunetr, we
adopted the default network topology.multi-dataset training setup. we trained
multitalent with 13 public abdominal ct datasets with a total of 1477 3d images,
including 47 classes (multi-dataset (md) collection)
[1,3,9,11,[18][19][20]25,26]. detailed information about the datasets, can be
found in the appendix in table 3 and fig. 3, including the corresponding
annotated classes. we increased the batch size to 4 and the number of training
epochs to 2000 to account for the high number of training images. to compensate
for the varying number of training images in each dataset, we choose a sampling
probability per case that is inversely proportional to √ n, where n is the
number of training cases in the corresponding source dataset. apart from that,
we have adopted all established design choices from nnu-net to ensure
reproducibility and comparability.transfer learning setup. we used the btcv
(small multi organ dataset [19]), amos (large multi organ dataset [16]) and
kits19 (pathology dataset [11]) datasets to evaluate the generalizability of the
multitalent features in a pre-training and fine tuning setting. naturally, the
target datasets were excluded from the respective pre-training. fine tuning was
performed with identical configuration as the source training, except for the
batch size which was set to 2. we followed the fine-tuning schedule proposed by
kumar et al. [17]. first, the segmentation heads were warmed up over 10 epochs
with linearly increasing learning rate, followed by a whole-network warm-up over
50 epochs. finally, we continued with the standard nnu-net training schedule.
as a baseline for the multitalent, we applied the 3d u-net generated by the
nnu-net without manual intervention to each dataset individually. furthermore,
we trained a 3d u-net, a resenc u-net and a swinunetr with the same network
topology, patch and batch size as our multitalent for each dataset. all baseline
networks were also implemented within the nnu-net framework and follow the
default training procedure. additionally, we compare multitalent with related
work on the public btcv leaderboard in table 1. furthermore, the utility of
features generated by multitalent is compared to supervised and unsupervised
pre-training baselines. as supervised baseline, we used the weights resulting
from training the three model architectures on the totalsegmentator dataset,
which consists of 1204 images and 104 classes [30], resulting in more than 10 5
annotated target structures. in contrast, mul-titalent is only trained with
about 3600 annotations. we used the same patch size, image spacing, batch size
and number of epochs as for the multitalent training. as unsupervised baseline
for the cnns, we pre-trained the networks on the multi-dataset collection based
on the work of zhou et al. (model genesis [33]). finally, for the swinunetr
architecture, we compared the utility of the weights from our multitalent with
the ones provided by tan et al. who performed self-supervised pre-training on
5050 ct images. this necessitated the use of the original (org.) implementation
of swinunetr because the recommended settings for fine tuning were used. this
should serve as additional external validation of our model. to ensure fair
comparability, we did not scale up any models. despite using gradient
checkpointing, the swinunetr models requires roughly 30 gb of gpu memory,
compared to less than 17 gb for the cnns.
multi-dataset training results are presented in fig. 2. in general, the
convolutional architectures clearly outperform the transformer-inspired
swinunetr. multitalent improves the performance of the purely convolutional
architectures (u-net and resenc u-net) and outperforms the corresponding
baseline models that were trained on each dataset individually. since a simple
average over all classes would introduce a biased perception due to the highly
varying numbers of images and classes, we additionally report an average over
all datasets. for example, dataset 7 consists of only 30 training images but has
13 classes, whereas 4 in the appendix provides all results for all classes.
averaged over all datasets, the multitalent gains 1.26 dice points for the
resenc u-net architecture and 1.05 dice points for the u-net architecture.
compared to the default nnu-net, configured without manual intervention for each
dataset, the improvements are 1.56 and 0.84 dice points. additionally, in fig. 2
we analyzed two subgroups of classes. the first group includes all "difficult"
classes for which the default nnu-net has a dice smaller than 75 (labeled by a
"d" in table 4 in the appendix). the second group includes all cancer classes
because of their clinical relevance. both class groups, but especially the
cancer classes, experience notable performance improvements from multitalent.
for the official btcv test set in table 1, multitalent outperforms all related
work that have also incorporated multiple datasets during training, proving that
multitalent is substantially superior to related approaches. the advantages of
multitalent include not only better segmentation results, but also considerable
time savings for training and inference due to the simultaneous prediction of
all classes. the training is 6.5 times faster and the inference is around 13
times faster than an ensemble of models trained on 13 datasets.transfer learning
results are found in table 2, which compares the finetuned 5-fold
cross-validation results of different pre-training strategies for three
different models on three datasets. the multitalent pre-training is highly
beneficial for the convolutional models and outperforms all unsupervised
baselines. although multitalent was trained with a substantially lower amount of
manually annotated structures ( ˜3600 vs. ˜10 5 annotations), it also exceeds
the supervised pre-training baseline. especially for the small multi-organ
dataset, which only has 30 training images (btcv), and for the kidney tumor
(kits19), the multi-talent pre-training boosts the segmentation results. in
general, the results show that supervised pre-training can be beneficial for the
swinunetr as well, but pre-training on the large totalsegmentator dataset works
better than the md pre-training. for the amos dataset, no pre-training scheme
has a substantial impact on the performance. we suspect that it is a result of
the dataset being saturated due to its large number of training cases. the
resenc u-net pretrained with multitalent, sets a new state-of-the-art on the
btcv leaderboard1 (table 1). allows including any publicly available datasets
(e.g. amos and totalsegmentator). this paves the way towards holistic whole body
segmentation model that is even capable of handling pathologies.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1_62.
the success of deep neural networks (dnns) mainly depends on the large number of
samples and the high-quality labels. however, either of them is very difficult
to be obtained for conducting medical image analysis with dnns. in particular,
obtaining high-quality labels needs professional experience so that corrupted
labels can often be found in medical datasets, which can seriously degrade the
effectiveness of medical image analysis. moreover, sample annotation needs
expensive cost. hence, correcting corrupted labels might be one of effective
solutions to solve the issues of high-quality labels.numerous works have been
proposed to tackle the issue of corrupted labels. based on whether correcting
corrupted labels, previous methods can be roughly divided into two categories,
i.e., robustness-based methods [12,17] and label correction methods [14,22].
robustness-based methods are designed to utilize various techniques, such as
dropout, augmentation and loss regularization, to avoid the adverse impact of
corrupted labels, thereby outputting a robust model. label correction methods
are proposed to first detect corrupted labels and then correct them. for
example, co-correction [9] simultaneously trains two models and corrects labels
for medical image analysis, and lcc [5] first regards the outputs of dnn as the
class probability of the training samples and then changes the labels of samples
with low class probability. label correction methods are significant for disease
diagnosis, because physicians can double check the probably mislabeled samples
to improve diagnosis accuracy. however, current label correction methods still
have limitations to be addressed. first, they cannot detect and correct all
corrupted labels, and meanwhile they usually fail to consider boosting the
robustness of the model itself, so that the effectiveness of dnns is possibly
degraded. second, existing label correction methods often ignore to take into
account the relationship among the samples so that influencing the effectiveness
of label correction.to address the aforementioned issues, in this paper, we
propose a new co-assistant framework, namely co-assistant networks for label
correction (cnlc) (shown in fig. 1), which consists of two modules, i.e., noise
detector and noise cleaner. specifically, the noise detector first adopts a
convolutional neural network (cnn [6,20]) to predict the class probability of
samples, and then the loss is used to partition all the training samples for
each class into three subgroups, i.e., clean samples, uncertain samples and
corrupted samples. moreover, we design a robust loss (i.e., a resistance loss)
into the cnn framework to avoid model overfitting on corrupted labels and thus
exploring the first issue in previous label correction methods. the noise
cleaner constructs a graph convolutional network (gcn [18,19]) model for each
class to correct the corrupted labels. during the process of noise cleaner, we
consider the relationship among samples (i.e., the local topology structure
preservation by gcn) to touch the second issue in previous methods. in
particular, our proposed cnlc iteratively updates the noise detector and the
noise cleaner, which results in a bi-level optimization problem [4,10] compared
to previous methods, the contributions of our method is two-fold. first, we
propose a new label correction method (i.e., a co-assistant framework) to boost
the model robustness for medical image analysis by two sequential modules.
either of them adaptively adjusts the other, and thus guaranteeing to output a
robust label correction model. second, two sequential modules in our framework
results in a bi-level optimization problem. we thus design a bi-level
optimization algorithm to solve our proposed objective function.
in this section, our proposed method first designs a noise detector to
discriminate corrupted samples from all samples, and then investigates a noise
cleaner to correct the detected corrupted labels.
noise detector is used to distinguish corrupted samples from clean samples. the
prevailing detection method is designed to first calculate the loss of dnns on
all training samples and then distinguish corrupted samples from clean ones
based on their losses. specifically, the samples with small losses are regarded
as clean samples while the samples with large losses are regarded as corrupted
samples.different from previous literature [6,7], our noise detector involves
two steps, i.e., cnn and label partition, to partition all training samples for
each class into three subgroups, i.e., clean samples, uncertain samples and
corrupted samples. specifically, we first employ cnn with the cross-entropy loss
as the backbone to obtain the loss of all training samples. since the
cross-entropy loss is easy to overfit on corrupted labels without extra
noise-tolerant term [1,21], we change it to the following resistant loss in
cnn:where b is the number of samples in each batch, p t i [j] represents the
j-th class prediction of the i-th sample in the t-th epoch, ỹi ∈ {0, 1, ..., c
-1} denotes the corrupted label of the i-th sample x i , c denotes the number of
classes and λ(t) is a time-related hyper-parameter. in eq. ( 1), the first term
is the cross-entropy loss. the second term is the resistance loss which is
proposed to smooth the update of model parameters so that preventing model
overfitting on corrupted labels to some extent [12].in label partition, based on
the resistant loss in eq. ( 1), the training samples for each class are divided
into three subgroups, i.e., clean samples, uncertain fig. 2. cross-entropy loss
distribution and gmm probability on different noise rates on breakhis [13].
"clean" denotes the samples with the ground-truth labels, while "corrupted"
denotes the samples with the corrupted labels.samples, and corrupted samples.
specifically, the samples with n 1 smallest losses are regarded as clean samples
and the samples with n 1 largest loss values are regarded as corrupted samples,
where n 1 is experimentally set as 5.0% of all training sample for each class.
the rest of the training samples for each class are regarded as uncertain
samples.in noise detector, our goal is to identify the real clean samples and
real corrupted samples, which are corresponded to set as positive samples and
negative samples in noise cleaner. if we select a large number of either clean
samples or corrupted samples (e.g., larger than 5.0% of all training samples),
they may contain false positive samples or false negative samples, so that the
effectiveness of the noise cleaner will be influenced. as a result, our noise
detector partitions all training samples for each class into three subgroups,
including a small proportion of clean samples and corrupted samples, as well as
uncertain samples.
noise cleaner is designed to correct labels of samples with corrupted labels.
recent works often employ dnns (such as cnn [8] and mlp [15]) to correct the
corrupted labels. first, these methods ignore to take into account the
relationship among the samples, such as local topology structure preservation,
i.e., one of popular techniques in computer vision and machine learning, which
ensures that nearby samples have similar labels and dissimilar samples have
different labels. in particular, based on the partition mentioned in the above
section, the clean samples within the same class should have the same label and
the corrupted samples should have different labels from clean samples within the
same class. this indicates that it is necessary to preserve the local topology
structure of samples within the same class. second, in noise detector, we only
select a small proportion of clean samples and corrupted samples for the
construction of noise cleaner. limited number of samples cannot guarantee to
build robust noise cleaner. in this paper, we address the above issues by
employing semi-supervised learning, i.e., a gcn for each class, which keeps the
local topology structure of samples on both labeled samples and unlabeled
samples. specifically, our noise cleaner includes three components, i.e., noise
rate estimation, class-based gcns, and corrupted label correction.the inputs of
each class-based gcn include labeled samples and unlabeled samples. the labeled
samples consist of positive samples (i.e., the clean samples of this class with
the new label z ic = 1 for the i-th sample in the c-th class) and negative
samples (i.e., the corrupted samples of this class with the new label z ic = 0).
the unlabeled samples include a subset of the uncertain samples from all classes
and corrupted samples of other classes. we follow the principle to select
uncertain samples for each class, i.e., the higher the resistant loss in eq. (
1), the higher the probability of the sample belonging to corrupted samples.
moreover, the number of uncertain samples is determined by noise rate
estimation.given the resistant loss in eq. ( 1), in noise rate estimation, we
estimate the noise rate of the training samples by employing a gaussian mixed
model (gmm) composed of two gaussian models. as shown in fig. 2, we observe that
the mean value of gaussian model for corrupted samples is greater than that of
gaussian model for clean samples. thus, the gaussian model with a large mean
value is probably the curve of corrupted labels. based on this, given two
outputs of the gmm model for the i-th sample, its output with a larger mean
value and the output with a smaller mean value, respectively, are denoted as m
i,1 and m i,2 , so the following definition v i is used to determine if the i-th
samples is noise:hence, the noise rate r of training samples is calculated
by:where n represents the total number of samples in training dataset. supposing
the number of samples in the c-th class is s c , the number of uncertain samples
of each class is s c × r -n 1 . hence, the total number of unlabeled samples for
each class is n × r -n 1 in noise cleaner. given 2 × n 1 labeled samples and n ×
r -n 1 unlabeled samples, the classbased gcn for each class conducts
semi-supervised learning to predict n × r samples, including n × r -n 1
unlabeled samples and n 1 corrupted samples for this class. the semi-supervised
loss l ssl includes a binary cross-entropy loss l bce for labeled samples and an
unsupervised loss l mse [8] for unlabeled samples, i.e., l ssl = l bce + l mse ,
where l bce and l mse are defined as:where q t ic denotes the prediction of the
i-th sample in the t-th epoch for the class c, qt ic is updated by qt ic =,
where (t) is related to time [8].in corrupted label correction, given c
well-trained gcns and the similarity scores on each class for a subset of
uncertain samples and all corrupted samples, their labels can be determined by:
ỹi = argmax 0≤c≤c-1 (q ic ) .(6)
the optimization of the noise detector is associated with the corrupted label
set ỹ, which is determined by noise cleaner. similarly, the embedding of all
samples e is an essential input of the noise cleaner, which is generated by the
noise detector. as the optimizations of two modules are nested, the objective
function of our proposed method is the following bi-level optimization problem:
in this paper, we construct a bi-level optimization algorithm to search optimal
network parameters of the above objective function. specifically, we optimize
the noise detector to output an optimal feature matrix e * , which is used for
the construction of the noise cleaner. furthermore, the output ỹ * of the noise
cleaner is used to optimize the noise detector. this optimization process
alternatively optimize two modules until the noise cleaner converges. we list
the optimization details of our proposed algorithm in the supplemental
materials.
the used datasets are breakhis [13], isic [3], and nihcc [16]. breakhis consists
of 7,909 breast cancer histopathological images including 2,480 benigns and
5,429 malignants. we randomly select 5,537 images for training and 2,372 ones
for testing. isic has 12,000 digital skin images where 6,000 are normal and
6,000 are with melanoma. we randomly choose 9,600 samples for training and the
remaining ones for testing. nihcc has 10,280 frontal-view x-ray images, where
5,110 are normal and 5,170 are with lung diseases. we randomly select 8,574
images for training and the rest of images for testing. in particular, the
random selection in our experiments guarantees that three datasets (i.e., the
training set, the testing set, and the whole set) have the same ratio for each
class. moreover, we assume that all labels in the used raw datasets are clean,
so we add corrupted labels with different noise rates = {0, 0.2, 0.4} into these
datasests, where = 0 means that all labels in the training set are clean.we
compare our proposed method with six popular methods, including one fundamental
baseline (i.e., cross-entropy (ce)), three robustness-based methods (i.e.,
co-teaching (ct) [6], nested co-teaching (nct) [2] and self-paced resistance
learning (sprl) [12]), and two label correction methods (i.e., co-correcting
(cc) [9] and self-ensemble label correction (selc) [11]). for fairness, in our
experiments, we adopt the same neural network for all comparison methods based
on their public codes and default parameter settings. we evaluate the
effectiveness of all methods in terms of four evaluation metrics, i.e.,
classification accuracy (acc), specificity (spe), sensitivity (sen) and area
under the roc curve (auc).
table 1 presents the classification results of all methods on three datasets.
due to the space limitation, we present the results at = 0.0 of all methods in
the supplemental materials. first, our method obtains the best results, followed
by ct, nct, sprl, celc, cc, and ce, on all datasets in terms of four evalua-
this might be because our proposed method not only utilizes a robust method to
train a cnn for distinguishing corrupted labels from clean labels, but also
corrects them by considering their relationship among the samples within the
same class. second, all methods outperform the fundamental baseline (i.e., ce)
on all cases. for example, the accuracy of cc improves by 4.8% and 28.2%
compared with ce at = 0.2 and = 0.4, respectively, on isic. the reason is that
the cross-entropy loss easily results in the overfitting issue on corrupted
labels.
to verify the effectiveness of the noise cleaner, we compare our method with the
following comparison methods: 1) w/o nc: without noise cleaner, and 2) mlp:
replace gcn with multi-layer perceptron, i.e., without considering the
relationship among samples. due to the space limitation, we only show results on
isic, which is listed in the first and second rows of table 2. the methods with
noise cleaner (i.e., mlp and cnlc) outperform the method without noise cleaner
w/o nc. for example, cnlc improves by 4.2% compared with w/o nc at = 0.4. thus,
the noise cleaner plays an critical role in cnlc. additionally, cnlc obtains
better performance than mlp because it considers the relationship among samples.
both of the above observations verify the conclusion mentioned in the last
section again.to verify the effectiveness of the resistance loss in eq. ( 1), we
remove the second term in eq. ( 1) to have a new comparison method cnlc-rl and
list the results in the third row of table 2. obviously, cnlc outperforms
cnlc-rl. for example, cnlc improves by 1.0% and 2.3%, respectively, compared to
cnlc-rl, in terms of four evaluation metrics at = 0.2 and = 0.4. the reason is
that the robustness loss can prevent the model from overfitting on corrupted
labels, and thus boosting the model robustness. this verifies the effectiveness
of the resistance loss defined in eq. (1) for medical image analysis, which has
been theoretically and experimentally verified in the application of natural
images [12].
in this paper, we proposed a novel co-assistant framework, to solve the problem
of dnns with corrupted labels for medical image analysis. experiments on three
medical image datasets demonstrate the effectiveness of the proposed framework.
although our method has achieved promising performance, its accuracy might be
further boosted by using more powerful feature extractors, like pre-train models
on large-scale public datasets or some self-supervised methods, e.g.,
contrastive learning. in the future, we will integrate these feature extractors
into the proposed framework to further improve its effectiveness.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1_16.
computed tomography (ct) and magnetic resonance (mr) are two widely used imaging
techniques in clinical practice. ct imaging uses x-rays to produce detailed,
cross-sectional images of the body, which is particularly useful for imaging
bones and detecting certain types of cancers with fast imaging speed. however,
ct imaging has relatively high radiation doses that can pose a risk of radiation
exposure to patients. low-dose ct techniques have been developed to address this
concern by using lower doses of radiation, but the image quality is degraded
with increased noise, which may compromise diagnostic accuracy [9].mr imaging,
on the other hand, uses a strong magnetic field and radio waves to create
detailed images of the body's internal structures, which can produce
high-contrast images for soft tissues and does not involve ionizing radiation.
this makes mr imaging safer for patients, particularly for those who require
frequent or repeated scans. however, mr imaging typically has a lower resolution
than ct [18], which limits its ability to visualize small structures or
abnormalities.motivated by the aforementioned, there is a pressing need to
improve the quality of low-dose ct images and low-resolution mr images to ensure
that they provide the necessary diagnostic information. numerous algorithms have
been developed for ct and mr image enhancement, with deep learning-based methods
emerging as a prominent trend [5,14], such as using the conditional generative
adversarial network for ct image denoising [32] and convolutional neural network
for mr image super-resolution (sr) [4].these algorithms are capable of improving
image quality, but they have two significant limitations. first, paired images
are required for training, e.g., low-dose and full-dose ct images;
low-resolution and high-resolution mr images). however, acquiring such paired
data is challenging in real clinical scenarios. although it is possible to
simulate low-quality images from high-quality images, the models derived from
such data may have limited generalization ability when applied to real data
[9,14]. second, customized models are required for each task. for example, for
mr super-resolution tasks with different degradation levels (i.e., 4x and 8x
downsampling), one may need to train a customized model for each degradation
level and the trained model cannot generalize to other degradation levels.
addressing these limitations is crucial for widespread adoption in clinical
practice.recently, pre-trained diffusion models [8,11,21] have shown great
promise in the context of unsupervised natural image reconstruction [6,7,12,28].
however, their applicability to medical images has not been fully explored due
to the absence of publicly available pre-trained diffusion models tailored for
the medical imaging community. the training of diffusion models requires a
significant amount of computational resources and training images. for example,
openai's improved diffusion models [21] took 1600-16000 a100 hours to be trained
on the imagenet dataset with one million images, which is prohibitively
expensive. several studies have used diffusion models for low-dose ct denoising
[30] and mr image reconstruction [22,31], but they still rely on paired
images.in this paper, we aim at addressing the limitations of existing image
enhancement methods and the scarcity of pre-trained diffusion models for medical
images. specifically, we provide two well-trained diffusion models on full-dose
ct images and high-resolution heart mr images, suitable for a range of
applications including image generation, denoising, and super-resolution.
motivated by the existing plug-and-play image restoration methods [26,34,35] and
denoising diffusion restoration and null-space models (ddnm) [12,28], we further
introduce a paradigm for plug-and-play ct and mr image denoising and
super-resolution as shown in fig. 1. notably, it eliminates the need for paired
data, enabling greater scalability and wider applicability than existing
paired-image dependent methods. moreover, it eliminates the need to train a
customized model for each task. our method does not need additional training on
specific tasks and can directly use the single pre-trained diffusion model on
multiple medical image enhancement tasks. the pre-trained diffusion models and
pytorch code of the present method are publicly available at
https://github.com/bowang-lab/ dpm-medimgenhance.
this section begins with a brief overview of diffusion models for image
generation and the mathematical model and algorithm for general image
enhancement. we then introduce a plug-and-play framework that harnesses the
strengths of both approaches to enable unsupervised medical image enhancement.
image generation models aim to capture the intrinsic data distribution from a
set of training images and generate new images from the model itself. we use
ddpm [11] for unconditional medical image generation, which contains a diffusion
(or forward) process and a sampling (or reverse) process. the diffusion process
gradually adds random gaussian noise to the input image x 0 , following a markov
chain with transition kernel q(xwhere t ∈ {1, • • • , t } represents the current
timestep, x t and x t-1 are adjacent image status, andis a predefined noise
schedule. furthermore, we can directly obtain x t based on x 0 at any timestep t
by:where α t := 1-β t , ᾱt := π t s=1 α s , and ∼ n (0, i). this property
enables simple model training where the input is the noisy image x t and the
timestep t and the output is the predicted noise θ (θ denotes model parameters).
intuitively, a denoising network is trained with the mean square loss e t,x ||θ
(x t , t)|| 2 . the sampling process aims to generate a clean image from
gaussian noise x t ∼ n (0, i), and each reverse step is defined by:
in general, image enhancement tasks can be formulated by:where y is the degraded
image, h is a degradation matrix, x is the unknown original image, and n is the
independent random noise. this model can represent various image restoration
tasks. for instance, in the image denoising task, h is the identity matrix, and
in the image super-resolution task, h is the downsampling operator. the main
objective is to recover x by solving the minimization problem:x * = arg minwhere
the first data-fidelity term keeps the data consistency and the second
dataregularization term r(x) imposes prior knowledge constraints on the
solution. this problem can be solved by the iterative denoising and backward
projections (idbp) algorithm [26], which optimizes the revised equivalent
problem:whereis the pseudo inverse of the degradation matrix h and f h t h := f
t h t hf. specifically, x * and ŷ * can be alternatively estimated by solving
min x ŷx 2 2 + r(x) and min ŷ ŷx 2 2 s.t. h ŷ = y. estimating x * is essentially
a denoising problem that can be solved by a denoising operator and ŷ * has a
closed-form solution:intuitively, idbp iteratively estimates the original image
from the current degraded image and makes a projection by constraining it with
prior knowledge.although idbp offers a flexible way to solve image enhancement
problems, it still requires paired images to train the denoising operator [26].
we introduce a plug-and-play framework by leveraging the benefits of the
diffusion model and idbp algorithm. here we highlight two benefits: (1) it
removes the need for paired images; and (2) it can simply apply the single
pre-trained diffusion model across multiple medical image enhancement tasks.
first, we reformulate the ddpm sampling process [11] x t-1 ∼ p θ (x t-1 |x t ) =
n (x t-1 ; μ θ (x t , t), β t i) into:andintuitively, each sampling iteration
has two steps. the first step estimates the denoised image x 0|t based on the
current noisy image x t and the trained denoising network θ (x t , t). the
second step generates a rectified image x t-1 by taking a weighted sum of x 0|t
and x t and adding a gaussian noise perturbation.as mentioned in eq. ( 3), our
goal is to restore an unknown original image x 0 from a degraded image y. thus,
the degraded image y needs to be involved in the sampling process. inspired by
the iteration loop in idbp, we project the estimated x 0|t on the hyperplane y =
hx:it can be easily proved that h x0|t = hh † y + hx 0|t -hh † hx 0|t = y. by
replacing x 0|t with x0|t in eq. ( 8), we have:algorithm 1 shows the complete
steps for image enhancement, which inherit the denoising operator from ddpm and
the projection operator from idbp. the former employs the strong denoising
capability in the diffusion model and the latter can make sure that the
generated results match the input image. notably, the final algorithm is
equivalent to ddnm [28], but it is derived from different perspectives.
require: pre-trained ddpm θ , low-quality image y, degradation operator h 1:
initialize xt ∼ n (0, i). 2: for t = t to 1 do 3:
dataset. we conducted experiments on two common image enhancement tasks:
denoising and sr. to mimic the real-world setting, the diffusion models were
trained on a diverse dataset, including images from different centers and
scanners. the testing set (e.g., mr images) is from a new medical center that
has not appeared in the training set. experiments show that our model can
generalize to these unseen images. specifically, the denoising task is based on
the aapm low dose ct grand challenge abdominal dataset [19], which can be also
used for sr [33]. the heart mr sr task is based on three datasets: acdc [1],
m&ms1-2 [3], and cmrxmotion [27]. notably, the presented framework eliminates
the requirement of paired data. for the ct image enhancement task, we trained a
diffusion model [21] based on the full-dose dataset that contains 5351 images,
and the hold-out quarter-dose images were used for testing. for the mr
enhancement task, we used the whole acdc [1] and m&ms1-2 [3] for training the
diffusion model and the cmrxmotion [27] dataset for testing. the testing images
were downsampled by operator h with factors of 4× and 8× to produce
low-resolution images, and the original images served as the ground
truth.evaluation metrics. the image quality was quantitatively evaluated by the
peak signal-to-noise ratio (psnr), structural similarity index (ssim) [29], and
visual information fidelity (vif) [24], which are widely used measures in
medical image enhancement tasks [9,17]. implementation details. we followed the
standard configuration in [21] to train the diffusion model from scratch.
specifically, the diffusion step used a linear noise schedule β ∈ [1e -4, 0.02]
and the number of diffusion timesteps was t = 1000. the input image size was
normalized to 256 × 256 and the 2d u-net [23] was optimized by adam [13] with a
batch size of 16 and a learning rate of 10 -4 , and an exponential moving
average (ema) over model parameters with a rate of 0.9999. all the models were
trained on a100 gpu and the total training time was 16 d. the implementation was
based on ddnm [28]. for an efficient sampling, we used ddim [25] with 100
diffusion steps. we followed the degradation operator settings in ddnm.
specifically, we used the identity matrix i as the degradation operator for the
denoising task and scaled the projection difference h † (hx 0|t -y) with
coefficient σ to balance the information from measurement y and denoising output
x 0|t . the downsampling operator implemented with torch.nn.adaptiveavgpool2d
for the super-resolution task. the pseudo-inverse operator h † is i for the
denoising task and upsampling operator for the sr task.comparison with other
methods. the pseudo-inverse operator h † was used as the baseline method,
namely, x * = h † y. we also compared the present method with one commonly used
image enhancement method dip [10] and two recent diffusion model-based methods:
ivlr [6], which adopted low-frequency information from measurement y to guide
the generation process towards a narrow data manifold, and dps [7], which
addressed the intractability of posterior sampling through laplacian
approximation. notably, dps used 1000 sampling steps while we only used 100
sampling steps.
low-dose ct image enhancement. the presented method outperformed all other
methods on the denoising task in all metrics, as shown in table 1, with average
psnr, ssim, and vif of 28.3, 0.803, and 0.510, respectively. supplementary fig.
1 (a) visually compares the denoising results, showing that the presented method
effectively removes the noise and preserves the anatomical details, while other
methods either fail to suppress the noise or result in loss of tissue
information. we also used the same pre-trained diffusion model for
simultaneously denoising and sr by setting h as the downsampling operator. our
method still achieves better performance across all metrics as shown in table 2.
by visually comparing the enhancement results in fig. 1 (b) and (c), our results
can reconstruct more anatomical details even in the challenging noisy 8× sr
task. in contrast, dip tends to smooth tissues and ilvr and dps fail to recover
the tiny structures such as liver vessels in the zoom-in regions. mr image
enhancement. to demonstrate the generality of the presented method, we also
applied it for the heart mr image 4× and 8× sr tasks, and the quantitative
results are presented in table 3. our results still outperformed ivlr and dps in
all metrics. dip obtains slightly better scores in psnr for the 4× sr task and
psnr and ssim for the 8× sr tasks, but visualized image quality is significantly
worse than our results as shown in supplementary fig. 2, e.g., many anatomical
structures are smoothed. this is because perceptual and distortion qualities are
in opposition to each other as theoretically proven in [2]. dip mainly
prioritizes the distortion measures for the noise-free sr tasks while our
results achieve a better trade-off between the perceptual and distortion
quality.
in summary, we have provided two well-trained diffusion models for abdomen ct
and heart mr, and introduced a plug-and-play framework for image enhancement.
our experiments have demonstrated that a single pre-trained diffusion model
could address different degradation levels without customized models. however,
there are still some limitations to be solved. the degradation operator and its
pseudo-inverse should be explicitly given, which limits its application in tasks
such as heart mr motion deblurring. although the present method is in general
applicable for 3d images, training the 3d diffusion model still remains
prohibitively expensive. moreover, the sampling process currently requires
multiple network inferences, but it could be solved with recent advances in
one-step generation models [15] and faster algorithms [16]. despite these
limitations, the versatile and scalable nature of the presented paradigm has
great potential to revolutionize medical image enhancement tasks. future work
could focus on developing more efficient algorithms for 3d diffusion models and
expanding this paradigm to more clinical applications such as low-dose pet
denoising.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1_1.
convolutional neural networks (cnns) and visual transformers (vits) have become
the standard in semantic segmentation, achieving state-of-the-art results in
many applications [1,16,24]. however, supervised training of cnns and vits
requires large amounts of annotated data, where each pixel in the image is
labeled with the category it belongs to. in the medical domain, obtaining these
annotations can be costly and time-consuming as it requires expertise and domain
knowledge that is often scarcely available [6]. in addition, medical image
annotations can be affected by human bias and poor inter-annotator agreement
[23], further complicating the process. despite efforts to obtain labels through
automated mining [31] and crowd-sourcing methods [11], the quality of datasets
gathered using these methods remains challenging due to often high levels of
label noise.for instance, the fitzpatrick 17k dataset, commonly used in
dermatology research, contains non-skin images and noisy annotations. in a
random sample of 504 images, 5.4% were labeled incorrectly or as other classes
[10]. the dataset was scraped from online atlases, which makes it vulnerable to
inaccuracies and noise [10]. noisy labels are and will continue to be, a problem
in medical datasets. this is a concern as label noise has been shown to decrease
the accuracy of supervised models [20,22,35], making it a key area of focus for
both research and practical applications.previous literature has explored many
methods to mitigate the problem of noisy labels in deep learning. these methods
can be broadly categorized into label correction [27,28,32], loss function
correction based on an estimated noise transition matrix [21,29,33], and robust
loss functions [2,18,30,34]. compared to the first two approaches, which may
suffer from inaccurate estimates of the noise transition matrix, robust loss
functions enable joint optimization of model parameters and variables related to
the noise model and have shown promising results in classification tasks [8,34].
despite these advances, semantic segmentation with noisy labels is relatively
understudied. existing research in this area has focused on the development of
noise-resistant network architectures [15], the incorporation of domain-specific
prior knowledge [29], or more recent strategies that update the noisy masks
before memorization [17].although previous methods have shown robustness in
semantic segmentation, they often have limitations, such as more
hyper-parameters, modifications to the network architecture, or complex training
procedures. in contrast, robust loss functions offer a much simpler solution as
they could be incorporated with a simple change in a single modeling component.
however, their effectiveness has not been thoroughly investigated.in this work,
we show that several traditional robust loss functions are vulnerable to
memorizing noisy labels. to overcome this problem, we introduce a novel robust
loss function, the t-loss, which is inspired by the negative loglikelihood of
the student-t distribution. the t-loss, whose simplest formulation features a
single parameter, can adaptively learn an optimal tolerance level to label noise
directly during backpropagation, eliminating the need for additional
computations such as the expectation maximization (em) steps.to evaluate the
effectiveness of the t-loss as a robust loss function for medical semantic
segmentation, we conducted experiments on two widely-used benchmark datasets in
the field: one for skin lesion segmentation and the other for lung segmentation.
we injected different levels of noise into these datasets that simulate typical
human labeling errors and trained deep learning models using various robust loss
functions. our experiments demonstrate that the t-loss outperforms these robust
state-of-the-art loss functions in terms of segmentation accuracy and
robustness, particularly under conditions of high noise contamination. we also
observed that the t-loss could adaptively learn the optimal tolerance level to
label noise which significantly reduces the risk of memorizing noisy labels.this
research is divided as follows: sect. 2 introduces the motivation behind our
t-loss and provides its mathematical derivation. section 3 covers the datasets
used in our experiments, the implementation and training details of t-loss, and
the metrics used for comparison. section 4 presents the main findings of our
study, including the results of the t-loss and the baselines on both datasets
and an ablation study on the parameter of t-loss. finally, in sect. 5, we
summarize our contributions and the significance of our study for the field.
let x i ∈ r c×w×h be an input image and y i ∈ {0, 1} w×h be its noisy annotated
binary segmentation mask, where c represents the number of channels, w the
image's width, and h its height. given a set of images {x 1 , . . . , x n } and
corresponding masks {y 1 , . . . , y n }, our goal is to train a model f w with
parameters w such that f w (x) approximates the accurate binary segmentation
mask for any given image x.to this end we note that, heuristically, assuming
error terms to follow a student-t distribution (as suggested e.g. in [19])
allows for significantly larger noise tolerance with respect to the usual
gaussian form. recall that the student-t distribution for a d-dimensional
variable y is defined by the probability density function (pdf)where µ and σ are
respectively the mean and the covariance matrix of the associated multivariate
normal distribution, ν is the number of degrees of freedom, and | • | indicates
the determinant (see e.g. [3]). from this expression, we see that the tails of
the student-t distribution follow a power law that is indeed heavier compared to
the usual negative quadratic exponential. for this reason, it is well known to
be robust to outliers [7,26].since the common mean squared error (mse) loss is
derived by minimizing the negative log-likelihood of the normal distribution, we
choose to apply the same transformation and getthe functional form of our loss
function for one image is then obtained with the identification y = y i and the
approximation µ = f w (x i ), and aggregated withequation ( 2) has d(d +1)/2
free parameters in the covariance matrix, which should be estimated from the
data. in the case of images, this can easily be in the order of 10 4 or larger,
which makes a general computation highly non-trivial and may deteriorate the
generalization capabilities of the model. for these reasons, we take σ to be the
identity matrix i d , despite knowing that pixel annotations in an image are not
independent. the loss term for one image simplifies toto clarify the relation
with known loss functions, let δ = |y i -f w (x i )|, and fix the value of ν.
for δ → 0, the functional dependence from δ reduces to a linear function of δ 2
, i.e. mse. for large values of δ, though, eq. ( 4) is equivalent to log δ, thus
penalizing large deviations even less than the much-advocated robust mean
absolute error (mae). the scale of this transition, the sensitivity to outliers,
is regulated by the parameter ν.we optimize the parameter ν jointly with w using
gradient descent. to this end, we reparametrize ν = e ν + where is a safeguard
for numerical stability. loss functions with similar dynamic tolerance
parameters were also studied in [2] in the context of regression, where using
the student-t distribution is only mentioned in passing.
in this section, we demonstrate the robustness of the t-loss for segmentation
tasks on two public image collections from different medical modalities, namely
isic [5] and shenzhen [4,13,25]. in line with the literature, we use simulated
label noise in our tests, as no public benchmark with real label noise exists
[15].
the isic 2017 dataset [5] is a well-known public benchmark of dermoscopy images
for skin cancer detection. it contains 2000 training and 600 test images with
corresponding lesion boundary masks. the images are annotated with lesion type,
diagnosis, and anatomical location metadata. the dataset also includes a list of
lesion attributes, such as size, shape, and color. we resized the images to 256
× 256 pixels for our experiments.shenzhen [4,13,25] is a public dataset
containing 566 frontal chest radiographs with corresponding lung segmentation
masks for tuberculosis detection. since there is not a predefined split for
shenzhen as in isic, to ensure representative training and testing sets, we
stratified the images by their tuberculosis and normal lung labels, with 70% of
the data for training and the remaining 30% for testing. resulting in 296
training images and 170 test images. all images were resized to 256 × 256
pixels.without a public benchmark with real noisy and clean segmentation masks,
we artificially inject additional mask noise in these two datasets to test the
model's robustness to low annotation quality. this simulates the real risk of
errors due to factors like annotator fatigue and difficulty in annotating
certain images. in particular, we follow [15], randomly sample a portion of the
training data with probability α ∈ {0.3, 0.5, 0.7}, and apply morphological
transformations with noise levels controlled by β ∈ {0.5, 0.7}1 . the
morphological transformations included erosion, dilation, and affine
transformations, which respectively reduced, enlarged, and displaced the
annotated area.
we train a nnu-net [12] as a segmentation network from scratch. to increase
variations in the training data, we augment them with random mirroring,
flipping, and gamma transformations. the t-loss was initialized with ν = 0 and =
10 -8 . the nnu-net was trained for 100 epochs using the adam optimizer with a
learning rate of 10 -3 and a batch size of 16 for the isic dataset and 8 for the
shenzhen dataset. the network was trained on a single nvidia tesla v100 with 32
gb of memory.the model is trained using noisy masks. however, by using the
ground truth for the corresponding noisy mask, we can evaluate the robustness of
the model and measure noisy-label memorization. this is done by analyzing the
dice score of the model's prediction compared to the actual ground truth.in
addition to the t-loss, we train several other losses for comparison. our
analysis includes some traditional robust losses, such as mean absolute error
(mae), reverse cross entropy (rce), normalized cross entropy (nce), and
normalized generalized cross entropy (ngce), as well as more recent methods,
such as generalized cross entropy (gce) [34], symmetrical cross entropy (sce)
[30], and active-passive loss (apl) [18]. for apl, in particular, we consider
three combinations: 1) nce+rce, 2) ngce+mae, and 3) ngce+rce. we consider the
mean of the predictions for the last 10 epochs with a fixed number of epochs and
report its mean and standard deviation over 3 different random seeds.finally, we
complete our evaluation with statistical significance tests. we use the anova
test [9] to compare the differences between the means of the dice scores and
obtain a p-value. in addition, if the difference is significant, we perform the
tukey post-hoc test [14] to determine which means are different. we assume
statistical significance for p-values of less than p = 0.05 and denote this with
a .
we present experimental results for the skin lesion segmentation task on the
isic dataset in table 1. our results show that conventional losses perform well
with no noise or under low noise levels, but their performance decreases
significantly with increasing noise levels due to the memorization of noisy
labels. this can be observed from the training dice scores in fig. 1, where
traditional robust losses overfit data in later stages of learning while metrics
for the t-loss do not deteriorate. our method achieves a dice score of 0.788 ±
0.007 even for the most extreme noise scenario under exam. examples of the
obtained masks can be seen in the supplementary material. t-loss (ours) 0.825(5)
0.809(6) 0.804(5) 0.800(11) 0.790(5) 0.788(7) 0.761( 6)
the results of lung segmentation for the shenzhen test set are reported in table
2. similar to the isic dataset, all considered robust losses perform well at low
noise levels. however, as the noise level increases, their dice scores
deteriorate. on the other hand, the t-loss stands out by consistently achieving
the highest dice score, even in the most challenging scenarios. the statistical
test results also support this claim, with the t-loss being significantly
superior to the other methods.
the value of ν is crucial for the model's performance, as it controls the
sensitivity to label noise. to shed light on this mechanism, we study the
behavior of ν during training for different label noise levels and
initializations on the isic dataset. as seen in fig. 2, ν dynamically adjusts
annotation noise tolerance in the early stages of training, independently of its
initial value. the plots demonstrate that t-loss (ours) 0.949(1) 0.948(1)
0.939(1) 0.914(5) 0.904(8) 0.896(7) 0.870 (31) ν clearly converges to a stable
solution during training, with initializations far from this solution only
mildly prolonging the time needed for convergence and having no significant
effect on the final dice score.
in this contribution, we introduced the t-loss, a loss function based on the
negative log-likelihood of the student-t distribution. the t-loss offers the
great advantage of controlling sensitivity to outliers through a single
parameter that is dynamically optimized. our evaluation on public medical
datasets for skin lesion and lung segmentation demonstrates that the t-loss
outperforms other robust losses by a statistically significant margin. while
other robust losses are vulnerable to noise memorization for high noise levels,
the t-loss can reabsorb this form of overfitting into the tolerance level ν. our
loss function also features remarkable independence to different noise types and
levels.it should be noted that other methods, such as [15] offer better
performance for segmentation on the isic dataset with the same synthetic noisy
labels, while the t-loss offers a simple alternative. the trade-off in terms of
performance, computational cost, and ease of adaption to different scenarios
remains to be investigated. similarly, combinations of the t-loss with
superpixels and/or iterative label refinement procedures are still to be
explored.the t-loss provides a robust solution for binary segmentation of
medical images in the presence of high levels of annotation noise, as frequently
met in practice e.g. due to annotator fatigue or inter-annotator disagreements.
this may be a key feature in achieving good generalization in many medical image
segmentation applications, such as clinical decision support systems. our
evaluations and analyses provide evidence that the t-loss is a reliable and
valuable tool in the field of medical image analysis, with the potential for
broader application in other domains.
we declare that we have used the isic dataset [5] under the apache license 2.0,
publicly available, and the shenzhen dataset [4,13,25] public available under
the cc by-nc-sa 4.0 license.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1 68.
when training supervised computer vision models, we typically focus on improving
their predictive performance, yet equally important for safety-critical tasks is
their ability to express meaningful uncertainties about their own predictions
[4]. in the context of machine learning, we often distinguish two types of
uncertainties: epistemic and aleatoric [13]. briefly speaking, epistemic
uncertainty arises from imperfect knowledge of the model about the problem it is
trained to solve, whereas aleatoric uncertainty describes ignorance regarding
the data used for learning and making predictions. for example, if a classifier
has learned to predict the presence of cancerous tissue on a colon
histopathology, and it is tasked with making a prediction on a breast biopsy it
may display epistemic uncertainty, as it was never trained for this problem
[21]. nonetheless, if we ask the model about a colon biopsy with ambiguous
visual content, i.e. a hard-todiagnose image, then it could express aleatoric
uncertainty, as it may not know how to solve the problem, but the ambiguity
comes from the data. this distinction between epistemic and aleatoric is often
blurry, because the presence of one of them does not imply the absence of the
other [12]. also, under strong epistemic uncertainty, aleatoric uncertainty
estimates can become unreliable [31].producing good uncertainty estimates can be
useful, e.g. to identify test samples where the model predicts with little
confidence and which should be reviewed [1]. a straightforward way to report
uncertainty estimates is by interpreting the output of a model (maximum of its
softmax probabilities) as its predictive confidence. when this confidence aligns
with the actual accuracy we say that the model is calibrated [8]. model
calibration has been studied for a long time, with roots going back to the
weather forecasting field [3]. initially applied mostly for binary
classification systems [7], the realization that modern neural networks tend to
predict over-confidently [10] has led to a surge of interest in recent years
[8]. broadly speaking, one can attempt to promote calibration during training,
by means of a post-processing stage, or by model ensembling.training-time
calibration. popular training-time approaches consist of reducing the predictive
entropy by means of regularization [11], e.g. label smoothing [25] or mixup
[30], or loss functions that smooth predictions [26]. these techniques often
rely on correctly tuning a hyper-parameter controlling the trade-off between
discrimination ability and confidence, and can easily achieve better calibration
at the expense of decreasing predictive performance [22]. examples of medical
image analysis works adopting this approach are difference between confidence
and accuracy regularization [20] for medical image diagnosis, or
spatially-varying and margin-based label smoothing [14,27], which extend and
improve label smoothing for biomedical image segmentation tasks.post-hoc
calibration. post-hoc calibration techniques like temperature scaling [10] and
its variants [6,15] have been proposed to correct over or underconfident
predictions by applying simple monotone mappings (fitted on a heldout subset of
the training data) on the output probabilities of the model. their greatest
shortcoming is the dependence on the i.i.d. assumption implicitly made when
using validation data to learn the mapping: these approaches suffer to
generalize to unseen data [28]. other than that, these techniques can be
combined with training-time methods and return compounded performance
improvements.model ensembling. a third approach to improve calibration is to
aggregate the output of several models, which are trained beforehand so that
they have some diversity in their predictions [5]. in deep learning, model
ensembles are considered to be the most successful method to generate meaningful
uncertainty estimates [16]. an obvious weakness of deep ensembles is the
requirement of training and then keeping for inference purposes a set of models,
which results in a computational overhead that can be considerable for larger
architectures. examples of applying ensembling in medical image computing
include [17,24].in this work we achieve model calibration by means of multi-head
models trained with diverse loss functions. in this sense, our approach is
closest to some recent works on multi-output architectures like [21], where a
multi-branch cnn is trained on histopathological data, enforcing specialization
of the different heads by backpropagating gradients through branches with the
lowest loss. compared to our approach, ensuring correct gradient flow to avoid
dead heads requires ad-hoc computational tricks [21]; in addition, no analysis
on model calibration on in-domain data or aleatoric uncertainty was developed,
focusing instead on anomaly detection. our main contribution is a multi-head
model that i) exploits multi-loss diversity to achieve greater confidence
calibration than other learning-based methods, while ii) avoiding the use of
training data to learn post-processing mappings as most post-hoc calibration
methods do, and iii) sidesteping the computation overhead of deep ensembles.
in this section we formally introduce multi-head models [19], and justify the
need for enforcing diversity on them. detailed derivations of all the results
below are provided in the online supplementary materials.
consider a k-class classification problem, and a neural network u θ taking an
image x and mapping it onto a representation u θ (x) ∈ r n , which is linearly
transformed by f into a logits vector z = f (u θ (x)) ∈ r k . this is then
mapped into a vector of probabilities p ∈ [0, 1] k by a softmax operation p =
σ(z), where p j = e zj / i e zi . if the label of x was y ∈ {1, ..., k}, we can
measure the error associated to prediction p with the cross-entropy loss l ce
(p, y) = -log(p y ).we now wish to implement a multi-head ensemble model like
the one shown in fig. 1. for this, we replace f by m different branches f 1 ,
..., f m , each of them still taking the same input but mapping it to different
logits z m = f m (u θ (x)). the resulting probability vectors p m = σ(z m ) are
then averaged to obtain a final prediction p μ = (1/m ) m p m . we are
interested in backpropagating the loss l ce (p μ , y) = -log(p μ y ) to find the
gradient at each branch, ∇ z m l ce (p μ , y). property 1: for the m-head
classifier in fig. 1, the derivative of the crossentropy loss at head f m with
respect to z m is given by where y is a one-hot representation of the label
y.from eq. (1) we see that the gradient in branch m will be scaled depending on
how much probability mass p m y is placed by f m on the correct class relative
to the total mass placed by all heads. in other words, if every head learned to
produce a similar prediction (not necessarily correct) for a particular sample,
then the optimization process of this network would result in the same updates
for all of them. as a consequence, diversity in the predictions that make up the
output p μ of the network would be damaged.
in view of the above, one way to obtain more diverse gradient updates in a
multihead model during training could be to supervise each head with a different
loss function. to this end, we will apply the weighted cross-entropy loss, given
by l ω -ce (p, y) = -ω y log(p μ y ), where ω ∈ r k is a weight vector. in our
case, we assign to each head a different weight vector ω m (as detailed below),
in such a way that a different loss function l ω m -ce will supervise the
intermediate output of each branch f m , similar to deep supervision strategies
[18] but enforcing diversity. the total loss of the complete model is the
addition of the per-head losses and the overall loss acting on the average
prediction:where p = (p 1 , ..., p m ) is an array collecting all the
predictions the network makes. since l ω -ce results from just multiplying by a
constant factor the conventional ce loss, we can readily calculate the gradient
of l mh at each branch.property 2: for the multi-loss multi-head classifier
shown in fig. 1, the gradient of the multi-head loss l mh at branch f m is given
by:note that having equal weight vectors in all branches fails to break the
symmetry in the scenario of all heads making similar predictions. indeed, if for
any two given heads f mi , f mj we have ω mi = ω mj and p mi ≈ p mj , i.e. p m ≈
p μ ∀m, then the difference in norm of the gradients of two heads would be:it
follows that we indeed require a different weight in each branch. in this work,
we design a weighting scheme to enforce the specialization of each head into a
particular subset of the categories {c 1 , ..., c k } in the training set. we
first assume that the multi-head model has less branches than the number of
classes in our problem, i.e. m ≤ k, as otherwise we would need to have different
branches specializing in the same category. in order to construct the weight
vector ω m , we associate to branch f m a subset of n/k categories, randomly
selected, for specialization, and these are weighed with ω m j = k. then, the
remaining categories in ω m receive a weight of ω m j = 1/k. for example, in a
problem with 4 categories and 2 branches, we could haveif n is not divisible by
k, the reminder categories are assigned for specialization to random branches.
when measuring model calibration, the standard approach relies on observing the
test set accuracy at different confidence bands b. for example, taking all test
samples that are predicted with a confidence around c = 0.8, a well-calibrated
classifier would show an accuracy of approximately 80% in this test subset. this
can be quantified by the expected calibration error (ece), given by:where s b s
form a uniform partition of the unit interval, and acc(b s ), conf(b s ) are
accuracy and average confidence (maximum softmax value) for test samples
predicted with confidence in b s .in practice, the ece alone is not a good
measure in terms of practical usability, as one can have a perfectly
ece-calibrated model with no predictive power [29]. a binary classifier in a
balanced dataset, randomly predicting always one class with c = 0.5 +
confidence, has a perfect calibration and 50% accuracy. proper scoring rules
like negative log-likelihood (nll) or the brier score are alternative
uncertainty quality metrics [9] that capture both discrimination ability and
calibration: a model must be both accurate and calibrated to achieve a low psr
value. we report nll, and also standard accuracy, which contrary to ece can be
high even for badly-calibrated models. finally, we show as summary metric the
average rank when aggregating rankings of ece, nll, and accuracy.
we now describe the data we used for experimentation, carefully analyze
performance for each dataset, and end up with a discussion of our findings.
we conducted experiments on two datasets: 1) the chaoyang dataset1 , which
contains colon histopathology images. it has 6,160 images unevenly distributed
in 4 classes (29%, 19%, 37%, 15%), with some amount of label ambiguity,
reflecting high aleatoric uncertainty. as a consequence, the best model in the
original reference [32], applying specific techniques to deal with label noise,
achieved an accuracy of 83.4%. 2) kvasir2 , a dataset for the task of endoscopic
image classification. the annotated part of this dataset contains 10,662 images,
and it represents a challenging classification problem due a high amount of
classes (23) and highly imbalanced class frequencies [2]. for the sake of
readability we do not show measures of dispersion, but we add them to the
supplementary material (appendix b), together with further experiments on other
datasets.we implement the proposed approach by optimizing several popular neural
network architectures, namely a common resnet50 and two more recent models: a
convnext [23] and a swin-transformer [23]. all models are trained for 50 epochs,
which was observed enough for convergence, using stochastic gradient descent
with a learning rate of l = 1e-2. code to reproduce our results and
hyperparameter specifications are shared at https://github.com/agaldran/
mhml_calibration.
notation: we train three different multi-head classifiers: 1) a 2-head model
where each head optimizes for standard (unweighted) ce, referred to as 2hsl (2
heads-single loss); 2) a 2-head model but with each head minimizing a
differently weighed ce loss as described in sect. 2.2. we call this model 2hml
(2 heads-multi loss)); 3) finally, we increase the number of heads to four, and
we refer to this model as 4hml. for comparison, we include a standard singleloss
one-head classifier (sl1h), plus models trained with label smoothing (ls [25]),
margin-based label smoothing (mbls [22]), mixup [30], and using the dca loss
[20]. we also show the performance of deep ensembles (d-ens [16]). we analyze
the impact of temperature scaling [10] in appendix a.what we expect to see:
multi-head multi-loss models should achieve a better calibration (low ece) than
other learning-based methods, ideally approaching deep ensembles calibration. we
also expect to achieve good calibration without sacrificing predictive
performance (high accuracy). both goals would be reflected jointly by a low nll
value, and by a better aggregated ranking. finally we would ideally observe
improved performance as we increase the diversity (comparing 2hsl to 2hml) and
as we add heads (comparing 2hml to 4hml).chaoyang: in table 1 we report the
results on the chaoyang dataset. overall, accuracy is relatively low, since this
dataset is challenging due to label ambiguity, and therefore calibration
analysis of aleatoric uncertainty becomes meaningful here. as expected, we see
how deep ensembles are the most accurate method, also with the lowest nll, for
two out of the three considered networks. however, we also observe noticeable
differences between other learning-based calibration techniques and multi-head
architectures. namely, all other calibration methods achieve lower ece than the
baseline (sl1h) model, but at the cost of a reduced accuracy. this is actually
captured by nll and rank, which become much higher for these approaches. in
contrast, 4hml achieves the second rank in two architectures, only behind deep
ensembles when using a resnet50 and a swin-transformer, and above any other 2hml
with a convnext, even outperforming deep ensembles in this case. overall, we can
see a pattern: multi-loss multi-head models appear to be extremely
well-calibrated (low ece and nll values) without sacrificing accuracy, and as we
diversify the losses and increase the number of heads we tend to improve
calibration. kvasir: next, we show in table 2 results for the kvasir dataset.
deep ensembles again reach the highest accuracy and excellent calibration.
interestingly, methods that smooth labels (ls, mbls, mixup) show a strong
degradation in calibration and their ece is often twice the ece of the baseline
sl1h model. we attribute this to class imbalance and the large number of
categories: smoothing labels might be ineffective in this scenario. note that
models minimizing the dca loss do manage to bring the ece down, although by
giving up accuracy. in contrast, all multi-head models improve calibration while
maintaining accuracy. remarkably, 4hml obtains lower ece than deep ensembles in
all cases. also, for two out of the three architectures 4hml ranks as the best
method, and for the other one 2hml reaches the best ranking.
multi-head multi-loss networks are classifiers with enhanced calibration and no
degradation of predictive performance when compared to their single-head
counterparts. this is achieved by simultaneously optimizing several output
branches, each one minimizing a differently weighted cross-entropy loss. weights
are complementary, ensuring that each branch is rewarded for becoming
specialized in a subset of the original data categories. comprehensive
experiments on two challenging datasets with three different neural networks
show that multi-head multi-loss models consistently outperform other
learning-based calibration techniques, matching and sometimes surpassing the
calibration of deep ensembles.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1_11.
deep learning models have achieved remarkable success in segmenting anatomy and
lesions from medical images but often rely on large-scale manually annotated
datasets [1][2][3]. this is challenging when working with volumetric medical
data as voxelwise labeling requires a lot of time and expertise. interactive
segmentation models address this issue by utilizing weak labels, such as clicks,
instead of voxelwise annotations [5][6][7]. the clicks are transformed into
guidance signals, e.g., gaussian heatmaps or euclidean/geodesic distance maps,
and used together with the image as a joint input for the interactive model.
annotators can make additional clicks in missegmented areas to iteratively
refine the segmentation mask, which often significantly improves the prediction
compared to non-interactive models [4,13]. however, prior research on choosing
guidance signals for interactive models is limited to small ablation studies
[5,8,9]. there is also no systematic framework for comparing guidance signals,
which includes not only accuracy but also efficiency and the ability to
iteratively improve predictions with new clicks, which are all important aspects
of interactive models [7]. we address these challenges with the following
contributions:1. we compare 5 existing guidance signals on the autopet [1] and
msd spleen [2] datasets and vary various hyperparameters. we show which
parameters are essential to tune for each guidance and suggest default values.
2. we introduce 5 guidance evaluation metrics (m1)-(m5), which evaluate the
performance, efficiency, and ability to improve with new clicks. this provides a
systematic framework for comparing guidance signals in future research. 3. based
on our insights from 1., we propose novel adaptive gaussian heatmaps, which use
geodesic distance values around each click to set the radius of each heatmap.
our adaptive heatmaps mitigate the weaknesses of the 5 guidances and achieve the
best performance on autopet [1] and msd spleen [2].
previous work comparing guidance signals has mostly been limited to small
ablation studies. sofiiuk et al. [9] and benenson et al. [8] both compare
euclidean distance maps with solid disks and find that disks perform better.
however, neither of them explore different parameter settings for each guidance
and both work with natural 2d images. dupont et al. [12] note that a
comprehensive comparison of existing guidance signals would be helpful in
designing interactive models. the closest work to ours is mideepseg [5], which
proposes a user guidance based on exponentialized geodesic distances and compare
it to existing guidance signals. however, they use only initial clicks and do
not add iterative corrective clicks to refine the segmentation. in contrast to
previous work, our research evaluates the influence of hyperparameters for
guidance signals and assesses the guidances' efficiency and ability to improve
with new clicks, in addition to accuracy. while some previous works [20][21][22]
propose using a larger radius for the first click's heatmap, our adaptive
heatmaps offer a greater flexibility by adjusting the radius at each new click
dynamically.
we define the five guidance signals over a set of n clicks c = {c 1 , c 2 , ...,
c n } where c i = (x i , y i , z i ) is the i th click. as disks and heatmaps
can be computed independently for each click, they are defined for a single
click c i over 3d voxels v = (x, y, z) in the volume. the disk signal fills
spheres with a radius σ centered around each click c i , which is represented by
the equation in eq. (1).the gaussian heatmap applies gaussian filters centered
around each click to create softer edges with an exponential decrease away from
the click (eq. ( 2)).the euclidean distance transform (edt) is defined in eq. (
3) as the minimum euclidean distance between a voxel v and the set of clicks c.
it is similar to the disk signal in eq. ( 1), but instead of filling the sphere
with a constant value it computes the distance of each voxel to the closest
click point.the geodesic distance transform (gdt) is defined in eq. (4) as the
shortest path distance between each voxel in the volume and the closest click in
the set c [14]. the shortest path in gdt also takes into account intensity
differences between voxels along the path. we use the method of asad et al. [10]
to compute the shortest path which is denoted as φ in eq. (4).we also examine
the exponentialized geodesic distance (exp-gdt) proposed in mideepseg [5] that
is defined in eq. ( 5) as an exponentiation of gdt:note: we normalize signals to
[0, 1] and invert intensity values for euclidean and geodesic distances d(x) by
1d(x) for better highlighting of small distances. we define our adaptive
gaussian heatmaps ad-heatmap(v, c i , σ i ) via:here, n ci is the 9-neighborhood
of c i , a = 13 limits the maximum radius to 13, and b = 0.15 is set empirically
1 (details in supplementary). the radius σ i is smaller for higher x, i.e., when
the mean geodesic distance in the neighboring voxels is high, indicating large
intensity changes such as edges. this leads to a more precise guidance with a
smaller radius σ i near edges and a larger radius in homogeneous areas such as
clicks in the center of the object of interest. an example of this process and
each guidance signal can be seen in fig. 1a).
we use the deepedit [11] model with a u-net backbone [15] and simulate a fixed
number of clicks n during training and evaluation. for each volume, n clicks are
iteratively sampled from over-and undersegmented predictions of the model as in
[16] and represented as foreground and background guidance signals. we
implemented our experiments with monai label [23] and will release our code.we
trained and evaluated all of our models on the openly available autopet [1] and
msd spleen [2] datasets. msd spleen [2] contains 41 ct volumes with voxel size
0.79×0.79×5.00mm 3 and average resolution of 512×512×89 voxels with dense
annotations of the spleen. autopet [1] consists of 1014 pet/ct volumes with
annotated tumor lesions of melanoma, lung cancer, or lymphoma. we discard the
513 tumor-free patients, leaving us with 501 volumes. we also only use pet data
for our experiments. the pet volumes have a voxel size of 2.0 × 2.0 × 2.0mm 3
and an average resolution of 400 × 400 × 352 voxels.
we keep these parameters constant for all models: learning rate = 10 -5 ,
#clicks n = 10, dice cross-entropy loss [24], and a fixed 80-20
training-validation split (d train /d val ). we apply the same data augmentation
transforms to all models and simulate clicks as proposed in sakinis et al. [16].
we train using one a100 gpu for 20 and 100 epochs on autopet [1] and msd spleen
[2] respectively.we vary the following four hyperparameters (h1)-(h4): (h1)
sigma. we vary the radius σ of disks and heatmaps in eq. ( 1) and ( 2) and also
explore how this parameter influences the performance of the distancebased
signals in eq. ( 3)- (5). instead of initializing the seed clicks c as
individual voxels c i , we initialize the set of seed clicks c as all voxels
within a radius σ centered at each c i and then compute the distance transform
as in eq. ( 3)-( 5).(h2) theta. we explore how truncating the values of
distance-based signals in eq. ( 3)-( 5) affects the performance. we discard the
top θ ∈ {10%, 30%, 50%} of the distance values and keep only smaller distances
closer to the clicks making the guidance more precise. unlike mideepseg [5], we
compute the θ threshold for each image individually, as fixed thresholds may not
be suitable for all images.(h3) input adaptor. we test three methods for
combining guidance signals with input volumes proposed by sofiuuk et al. [9]
-concat, distance maps fusion (dmf), and conv1s. concat combines input and
guidance by concatenating their channels. dmf additionally includes 1 × 1 conv.
layers to adjust the channels to match the original size in the backbone. conv1s
has two branches for the guidance and volume, which are summed and fed to the
backbone. (h4) probability of interaction. we randomly decide for each volume
whether to add the n clicks or not, with a probability of p, in order to make
the model more independent of interactions and improve its initial segmentation.
all the hyperparameters we vary are summarized in table 1. each combination of
hyperparameters corresponds to a separately trained deepedit [11] model.
we use 5 metrics (m1)-(m5) (table 2) to evaluate the validation performance.
overlap of the guidance g with the ground-truth mask m : |m ∩g| |g| . this
estimates the guidance precision as corrective clicks are often near boundaries,
and if guidances are too large, such as disks with a large σ, there is a large
overlap with the background outside the boundary
we first train a deepedit [11] model for each (σ, θ) pair and set p = 100% and
the input adaptor to concat to constrain the parameter space.(h1) sigma. results
in fig. 1b) show that on msd spleen [2], the highest dice scores are at σ = 5,
with a slight improvement for two samples at σ = 1, but performance decreases
for higher values σ > 5. on autopet [1], σ = 5 and two samples with σ = 0 show
the best performance, while higher values again demonstrate a significant
performance drop. figure 1c) shows that the best final and initial dice for
disks and heatmaps are with σ = 1 and σ = 0. geodesic maps exhibit lower dice
scores for small σ < 5 and achieve the best performance for σ = 5 on both
datasets. larger σ values lead to a worse initial dice for all guidance signals.
differences in results for different σ values are more pronounced in autopet [1]
as it is a more challenging dataset [17][18][19]25].(h2) theta. we examine the
impact of truncating large distance values for the edt and gdt guidances from
eq. ( 3) and ( 4). figure 1a) shows that the highest final dice scores are
achieved with θ = 10 for msd spleen [2]. on autopet [1], the scores are
relatively similar when varying θ with a slight improvement at θ = 10. the
results in fig. 1d) also confirm that θ = 10 is the optimal parameter for both
datasets and that not truncating values on msd spleen [2], i.e. θ = 0, leads to
a sharp drop in performance.for our next experiments, we fix the optimal (σ, θ)
pair for each of the five guidances (see table 3) and train a deepedit [11]
model for all combinations of input adaptors and probability of interaction.(h3)
input adaptor. we look into different ways of combining guidance signals with
input volumes using the input adaptors proposed by sofiuuk et al. [9]. the
results in fig. 1e) indicate that the best performance is achieved by simply
concatenating the guidance signal with the input volume. this holds true for
both datasets and the difference in performance is substantial.(h4) probability
of interaction. figure 1e) shows that p ∈ {75%, 100%} results in the best
performance on msd spleen [2], with a faster convergence rate for p = 75%.
however, with p = 50%, the performance is worse than the non-interactive
baseline (p = 0%). on autopet [1], the results for all p values are similar, but
the highest dice is achieved with p = 100%. note that p = 100% results in lower
initial dice scores and requires more interactions to converge, indicating that
the models depend more on the interactions. for the rest of our experiments, we
use the optimal hyperparameters for each guidance in table 1.
the comparison of the guidance signals using our five metrics (m1)-(m5) can be
seen in fig. 2. although the concrete values for msd spleen [2] autopet [1] are
different, the five metrics follow the same trend on both datasets.(m1) initial
and (m2) final dice. overall, all guidance signals improve their
initial-to-final dice scores after n clicks, with autopet [1] showing a gap
between disks/heatmaps and distance-based signals. moreover, geodesic-based
signals have lower initial scores on both datasets and require more
interactions.(m3) consistent improvement. the consistent improvement is ≈ 65%
for both datasets, but it is slightly worse for autopet [1] as it is more
challenging. heatmaps and disks achieve the most consistent improvement, which
means they are more precise in correcting errors. in contrast, geodesic
distances change globally with new clicks as the whole guidance must be
recomputed. these changes may confuse the model and lead to inconsistent
improvement.(m4) overlap with ground truth. heatmaps, disks, and edt have a
significantly higher overlap with the ground truth compared to geodesicbased
signals, particularly on autopet [1]. gdt incorporates the changes in voxel
intensity, which is not a strong signal for lesions with weak boundaries in
autopet [1], resulting in a smaller overlap with the ground truth. the guidances
are ranked in the same order in (m3) and in (m4) for both datasets. thus, a good
overlap with the ground truth can be associated with precise corrections.(m5)
efficiency. efficiency is much higher on msd spleen [2] compared to autopet [1],
as autopet has a ×2.4 larger mean volume size. the time also includes the
sampling of new clicks for each simulated interaction. disks are the most
efficient signal, filling up spheres with constant values, while heatmaps are
slightly slower due to applying a gaussian filter over the disks. distance
transform-based guidances are the slowest on both datasets due to their
complexity, but all guidance signals are computed in a reasonable time (<1
s).adaptive heatmaps: results. varying (h1)-(h4) and examining (m1)-(m5), we
find disks/heatmaps as the best signals, but with inflexibility near edges due
to their fixed radius (fig. 1a)). using gdt as a proxy signal to adapt σ i for
each click c i mitigates this weakness by imposing large σ i in homogeneous
areas and small, precise σ i near edges (fig. 1a)). this results in
substantially higher consistent improvement and overlap with ground truth and
the best initial and final dice (table 3). thus, our comparative study has led
to the creation of a more consistent and flexible signal with a slight
performance boost, albeit with an efficiency cost due to the need to compute
both gdt and heatmaps.
our comparative experiments yield insights into tuning existing guiding signals
and designing new ones. we find that smaller radiuses (σ ≤ 5), a small threshold
(θ = 10%), more iterations with interactions (p ≥ 75%), and traditional
concatenation should be used. weaknesses in existing signals include overly
large radiuses near edges and inconsistent improvement for geodesic-based
signals that change with each click. this analysis inspires our adaptive
heatmaps, which adapt the radiuses of the heatmaps according to the geodesic
values around the clicks, mitigating the inflexibility and inconsistency of
existing guidances. we emphasize the importance of guidance representation in
clinical applications, where a consistent and robust model is critical. our
study provides an overview of potential pitfalls, important parameters to tune,
and how to design future guidance signals, along with proposed metrics for
systematic comparison.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1 61.
the recent advancements in transformer-based models have revolutionized the
field of natural language processing and have also shown great promise in a wide
range of computer vision tasks [5]. as a notable example, the vision transformer
(vit) model utilizes multi-head self-attention (msa) blocks to globally model
the interactions between semantic tokens created by treating local image patches
as individual elements [7]. this approach stands in contrast to cnns, which
hierarchically increase their receptive field from local to global to capture a
global semantic representation. nevertheless, recent studies [3,20] have shown
that vit models struggle to capture high-frequency components of images, which
can limit their ability to detect local textures and it is vital for many
diagnostic and prognostic tasks. this weakness in local representation can be
attributed to the way in which vit models process images. vit models split an
image into a sequence of patches and model their dependencies using a
self-attention mechanism, which may not be as effective as the convolution
operation used in cnn models in extracting local features within receptive
fields. this difference in how vit and cnn process images may explain the
superior performance of cnn models in local feature extraction [1,8]. innovative
approaches have been proposed in recent years to address the insufficient local
texture representation within transformer models. one such approach is the
integration of cnn and vit features through complementary methods, aimed at
seamlessly blending the strengths of both in order to compensate for any
shortcomings in local representation [5]. [5] is one of the earliest approaches
incorporating the transformer layers into the cnn bottleneck to model both local
and global dependency using the combination of cnn and vit models. heidari et
al. [11] proposed a novel solution called hiformer, which leverages a swin
transformer module and a cnn-based encoder to generate two multi-scale feature
representations, which are then integrated via a double-level fusion module.
unetr [10] used a transformer to create a powerful encoder with a cnn decoder
for 3d medical image segmentation. by bridging the cnnbased encoder and decoder
with the transformer, cotr [26], and transbts [22], the segmentation performance
in low-resolution stages was improved. despite these advances, there remain some
limitations in these methods such as computationally inefficiency (e.g.,
transunet model), the requirement of a heavy cnn backbone (e.g., hiformer), and
the lack of consideration for multi-scale information. these limitations have
resulted in less effective network learning results in the field of medical
image segmentation.
the redesign of the self-attention mechanism within pure transformer models is
another method aiming to augment feature repre-sentation to enhance the local
feature representation ultimately. in this direction, swin-unet [4] utilizes a
linear computational complexity swin transformer [14] block in a u-shaped
structure as a multi-scale backbone. missformer [12] besides exploring the
efficient transformer [25] counterpart to diminish the parameter overflow of
vision transformers, applies a non-invertible downsampling operation on input
blocks transformer to reduce the parameters. d-former [24] is a pure
transformer-based pipeline that comprises a double attention module to capture
locally fine-grained attention and interaction with different units in a dilated
manner through its mechanism.
recent research has revealed that traditional self-attention mechanisms, while
effective in addressing local feature discrepancies, have a tendency to overlook
important high-frequency information such as texture and edge details [21]. this
is especially problematic for tasks like tumor detection, cancer-type
identification through radiomics analysis, as well as treatment response
assessment, where abnormalities often manifest in texture. moreover,
self-attention mechanisms have a quadratic computational complexity and may
produce redundant features [18].our contributions: ➊ we propose
laplacian-former, a novel approach that includes new efficient attention
(ef-att) consisting of two sub-attention mechanisms: efficient attention and
frequency attention. the efficient attention mechanism reduces the complexity of
self-attention to linear while producing the same output. the frequency
attention mechanism is modeled using a laplacian pyramid to emphasize each
frequency information's contribution selectively. then, a parametric frequency
attention fusion strategy to balance the importance of shape and texture
features by recalibrating the frequency features. these two attention mechanisms
work in parallel. ➋ we also introduce a novel efficient enhancement multi-scale
bridge that effectively transfers spatial information from the encoder to the
decoder while preserving the fundamental features. ➌ our method not only
alleviates the problem of the traditional self-attention mechanism mentioned
above, but also it surpasses all its counterparts in terms of different
evaluation metrics for the tasks of medical image segmentation.
in our proposed network, illustrated in fig. 1, taking an input image x ∈ r h×w
×c with spatial dimensions h and w , and c channels, it is first passed through
a patch embedding module to obtain overlapping patch tokens of size 4 × 4 from
the input image. the proposed model comprises four encoder blocks, each
containing two efficient enhancement transformer layers and a patch merging
layer that downsamples the features by merging 2 × 2 patch tokens and increasing
the channel dimension. the decoder is composed of three efficient enhancement
transformer blocks and four patch-expanding blocks, followed by a segmentation
head to retrieve the final segmentation map. laplacian-former then employs a
novel efficient enhancement multi-scale bridge to capture local and global
correlations of different scale features and effectively transfer the underlying
features from the encoder to the decoder.
in medical imaging, it is important to distinguish different structures and
tissues, especially when tissue boundaries are ill-defined. this is often the
case for accurate segmentation of small abnormalities, where high-frequency
information plays a critical role in defining boundaries by capturing both
textures and edges. inspired by this, we propose an efficient enhancement
transformer block that incorporates an efficient frequency attention (ef-att)
mechanism to capture contextual information of an image while recalibrating the
representation space within an attention mechanism and recovering high-frequency
details.our efficient enhancement transformer block first takes a layernorm (ln)
from the input x. then it applies the ef-att mechanism to capture contextual
information and selectively include various types of frequency information while
using the laplacian pyramid to balance the importance of shape and texture
features. next, x and diversity-enhanced shortcuts are added to the output of
the attention mechanism to increase the diversity of features. it is proved in
[19] that as transformers become deeper, their features become less varied,
which restrains their representation capacity and prevents them from attaining
optimal performance. to address this issue, we have implemented an augmented
short- cut method from [9], a diversity-enhanced shortcut (des), employing a
kronecker decomposition-based projection. this approach involves inserting
additional paths with trainable parameters alongside the original shortcut x,
which enhances feature diversity and improves performance while requiring
minimal hardware resources. finally, we apply layernorm and mix-ffn [25] to the
resulting feature representation to enhance its power. this final step completes
our efficient enhancement transformer block, as illustrated in fig. 2.
the traditional self-attention block computes the attention score s using query
(q) and key (k) values, normalizes the result using softmax, and then multiplies
the normalized attention map with value (v):where d k is the embedding
dimension. one of the main limitations of the dotproduct mechanism is that it
generates redundant information, resulting in unnecessary computational
complexity. shen et al. [18] proposed to represent the context more effectively
by reducing the computational burden from o(n 2 ) to linear form o(d 2 n):their
approach involves applying the softmax function (ρ) to the key and query vectors
to obtain normalized scores and formulating the global context by multiplying
the key and value matrix. they demonstrate that efficient attention e can
provide an equivalent representation of self-attention while being
computationally efficient. by adopting this approach, we can alleviate the
issues of feature redundancy and computational complexity associated with
self-attention.wang et al. [21] explored another major limitation of the
self-attention mechanism, where they demonstrated through theoretical analysis
that self-attention operates as a low-pass filter that erases high-frequency
information, leading to a loss of feature expressiveness in the model's deep
layers. authors found that the softmax operation causes self-attention to keep
low-frequency information and loses its fine details. motivated by this, we
propose a new frequency recalibration technique to address the limitations of
self-attention, which only focuses on low-frequency information (which contains
shape information) while ignoring the higher frequencies that carry texture and
edge information. first, we construct a laplacian pyramid to determine the
different frequency levels of the feature maps. the process begins by extracting
(l + 1) gaussian representations from the encoded feature using different
variance values of the gaussian function:where x refers to the input feature
map, (i, j) corresponds to the spatial location within the encoded feature map,
the variable σ l denotes the variance of the gaussian function for the l-th
scale, and the symbol * represents the convolution operator. the pyramid is then
built by subtracting the l-th gaussian function (g l ) output from the (l +
1)-th output (g l -g l+1 ) to encode frequency information at different scales.
the laplacian pyramid is composed of multiple levels, each level containing
distinct types of information. to ensure a balanced distribution of low and
high-frequency information in the model, it is necessary to efficiently
aggregate the features from all levels of the frequency domain. hence, we
present frequency attention that involves multiplying the key and value of each
level (x l ) to calculate the attention score and then fuses the resulting
attention scores of all levels using a fusion module, which performs summation.
the resulting attention score is multiplied by query (q) to obtain the final
frequency attention result, which subsequently concatenates with the efficient
attention result and applies the depth-wise convolution with the kernel size of
2×1×1 in order to aggregate both information and recalibrate the feature map,
thus allowing for the retrieval of high-frequency information.
it is widely known that effectively integrating multi-scale information can lead
to improved performance [12]. thus, we introduce the efficient enhancement
multi-scale bridge as an alternative to simply concatenating the features from
the encoder and decoder layers. the proposed bridge, depicted in fig. 1,
delivers spatial information to each decoder layer, enabling the recovery of
intricate details while generating output segmentation masks. in this approach,
we aim to calculate the efficient attention mechanism for each level and fuse
the multiscale information in their context; thus, it is important that all
levels' embedding dimension is of the same size. therefore, in order to
calculate the global context (g i ), we parametrize the query and value of each
level using a convolution 1 × 1 where it gets the size of mc and outputs c,
where m equals 1, 2, 5, and 8 for the first to fourth levels, respectively. we
multiply the new key and value to each other to attain the global context. we
then use a summation module to aggregate the global context of all levels and
reshape the query for matrix multiplication with the augmented global context.
taking the second level with the dimension of h 8 × w 8 × 2c, the key and value
are mapped to ( h 8 × w 8 × 2c and feed it through an ln and mix-ffn module with
a skip connection to empower the feature representations. the resulting output
is combined with the expanded feature map, and then projected using a linear
layer onto the same size as the encoder block corresponding to that level.
our proposed technique was developed using the pytorch library and executed on a
single rtx 3090 gpu. a batch size of 24 and a stochastic gradient descent
algorithm with a base learning rate of 0.05, a momentum of 0.9, and a weight
decay of 0.0001 was utilized during the training process, which was carried out
for 400 epochs. for the loss function, we used both cross-entropy and dice
losses (loss = γ • l dice + (1γ) • l ce ), γ set to 0.6 empirically.
we tested our model using the synapse dataset [13], which comprises 30 cases of
contrast-enhanced abdominal clinical ct scans (a total of 3,779 axial slices).
each ct scan consists of 85 ∼ 198 slices of the in-plane size of 512 × 512 and
has annotations for eight different organs. we followed the same preferences for
data preparation analogous to [5]. we also followed [2] experiments to evaluate
our method on the isic 2018 skin lesion dataset [6] with 2,694 images. synapse
multi-organ segmentation: table 1 presents a comparison of our proposal with
previous sota methods using the dsc and hd metrics across eight abdominal
organs. laplacian-former clearly outperforms sota cnnbased methods. we
extensively evaluated efficientformer (effformer) plus another drift of
laplacian-former without utilizing the bridge connections to endorse the
superiority of laplacian-former. laplacian-former exhibits superior learning
ability on the dice score metric compared to other transformer-based models,
achieving an increase of +1.59% and +2.77% in dice scores compared to hiformer
and swin-unet, respectively. figure 3 illustrates a qualitative result of our
method for different organ segmentation, specifically we can observe that the
lalacianformer produces a precise boundary segmentation on gallbladder, liver,
and stomach organs. it is noteworthy to mention that our pipeline, as a pure
transformer-based architecture trained from scratch without pretraining weights,
outperforms all previously presented network architectures.skin lesion
segmentation: table 2a shows the comparison results of our proposed method,
laplacian-former, against leading methods on the skin lesion segmentation
benchmark. our approach outperforms other competitors across most evaluation
metrics, indicating its excellent generalization ability across different
datasets. in particular, our approach performs better than hybrid methods such
as tmu-net [15] and pure transformer-based methods such as swin-unet [4].our
method achieves superior performance by utilizing the frequency attention in a
pyramid scale to model local textures. specifically, our frequency attention
emphasizes the fine details and texture characteristics that are indicative of
skin lesion structures and amplifies regions with significant intensity
variations, thus accentuating the texture patterns present in the image and
resulting in better performance. in addition, we provided the spectral response
of laplacianformer vs. standard transformer in identical layers in table 2b. it
is evident standard design frequency response in deep layers of structure
attenuates more than the laplacianformer, which is a visual endorsement of the
capability of laplacian- former for its ability to preserve high-frequency
details. the supplementary provides more visualization results.
in this paper, we introduce laplacian-former, a novel standalone
transformerbased u-shaped architecture for medical image analysis. specifically,
we address the transformer's inability to capture local context as
high-frequency details, e.g., edges and boundaries, by developing a new design
within a scaled dot attention block. our pipeline benefits the multi-resolution
laplacian module to compensate for the lack of frequency attention in
transformers. moreover, while our design takes advantage of the efficiency of
transformer architectures, it keeps the parameter numbers low.
although machine learning-based classification systems have achieved significant
breakthroughs in various research and practical areas, their clinical
application is still lacking. a primary reason is the lack of reliability, i.e.
failure cases produced by the system, which predominantly occur when deployment
data differs from the data it was trained on, a phenomenon known as distribution
shifts. in medical applications, these shifts can be caused by image corruption
("corruption shift"), unseen variants of pathologies ("manifestation shift"), or
deployment in new clinical sites with different scanners and protocols
("acquisition shift") [4]. the robustness of a classifier, i.e. its ability to
generalize across these shifts, is extensively studied in the computer vision
community with a variety of recent benchmarks covering nuanced realistic
distribution shifts [13,15,19,27], and is also studied in isolated cases in the
biomedical community [2,3,33]. despite these efforts, perfect classifiers are
not to be expected, thus a second mitigation strategy is to detect and defer the
remaining failures, thus preventing failures to be silent. this is done by means
of confidence scoring functions (csf) of different types as studied in the
fields of misclassification detection (misd) [5,11,23], out-of-distribution
detection (ood-d) [6,7,11,20,21,32], selective classification (sc) [9,10,22],
and predictive uncertainty quantification (puq) [18,25].we argue, that silent
failures, which occur when test cases break both the classifier and the csf, are
a significant bottleneck in the clinical translation of ml systems and require
further attention in the medical community.note that the task of silent failure
prevention is orthogonal to calibration, as, for example, a perfectly calibrated
classifier can still yield substantial amounts of silent failures and vice versa
[15].bernhardt et al. [3] studied failure detection on several biomedical
datasets, but only assessed the performance of csfs in isolation without
considering the classifier's ability to prevent failures. moreover, their study
did not include distribution shifts thus lacking a wide range of realistic
failure sources. jaeger et al. [15], on the other hand, recently discussed
various shortcomings in current research on silent failures including the common
lack of distribution shifts and the lack of assessing the classifier and csf as
a joint system. however, their study did not cover tasks from the biomedical
domain.in this work, our contribution is twofold: 1) building on the work of
jaeger et al. [15], we present the first comprehensive study of silent failure
prevention in the biomedical field. we compare various csfs under a wide range
of distribution shifts on four biomedical datasets. our study provides valuable
insights and the underlying framework is made openly available to catalyze
future research in the community. 2) since the benchmark reveals that none of
the predominant csfs can reliably prevent silent failures in biomedical tasks,
we argue that a deeper understanding of the root causes in the data itself is
required. to this end, we present sf-visuals, a visualization tool that
facilitates identifying silent failures in a dataset and investigating their
causes (see fig. 1). our approach contributes to recent research on visual
analysis of failures [13], which has not focused on silent failures and
distribution shifts before.
benchmark for silent failure prevention under distribution shifts. we follow the
spirit of recent robustness benchmarks, where existing datasets have been
enhanced by various distribution shifts to evaluate methods under a wide range
of failure sources and thus simulate real-world application [19,27]. to our
knowledge, no such comprehensive benchmark currently exists in the biomedical
domain. specifically, we introduce corruptions of various intensity levels to
the images in four datasets in the form of brightness, motion blur, elastic
transformations and gaussian noise. we further simulate acquisition shifts and
manifestation shifts by splitting the data into "source domain" (development
data) and "target domain" (deployment data) according to sub-class information
from the meta-data such as lesion subtypes or clinical sites. dermoscopy
dataset: we combine data from isic 2020 [26], derma 7 point [17], ph2 [24] and
ham10000 [30] and map all lesion sub-types to the super-classes "benign" or
"malignant". we emulate two acquisition shifts by defining either images from
the memorial sloan kettering cancer center (mskcc) or hospital clinic barcelona
(hcb) as the target domain and the remaining images as the source domain.
further, a manifestation shift is designed by defining the lesion subtypes
"keratosis-like" (benign) and "actinic keratosis" (malignant) as the target
domain. chest x-ray dataset: we pool the data from chexpert [14], nih14 [31] and
mimic [16], while only retaining the classes common to all three. next, we
emulate two acquisition shifts by defining either the nih14 or the chexpert data
as the target domain. fc-microscopy dataset: the rxrx1 dataset [28] represents
the fluorescence cell microscopy domain. since the images were acquired in 51
deviating acquisition steps, we define 10 of these batches as target-domain to
emulate an acquisition shift. lung nodule ct dataset: we create a simple 2d
binary nodule classification task based on the 3d lidc-idri data [1] by
selecting the slice with the largest annotation per nodule (±two slices
resulting in 5 slices per nodule). average malignancy ratings (four raters per
nodule, scores between 1 and 5) > 2 are considered malignant and all others as
benign. we emulate two manifestation shifts by defining nodules with high
spiculation (rating > 2), and low texture (rating < 3) as target domains.the
datasets consist only of publicly available data, our benchmark provides scripts
to automatically generate the combined datasets and distribution shifts.the
sf-visuals tool: visualizing silent failures. the proposed tool is based on
three simple operations, that enable effective and intuitive analysis of silent
failures in datasets across various csfs: 1) interactive scatter plots: see
example in fig. 1b. we first reduce the dimensionality of the classifier's
latent space to 50 using principal component analysis and use t-sne to obtain
the final 3-dimensional embedding. interactive functionality includes coloring
dots via pre-defined schemes such as classes, distribution shifts, classifier
confusion matrix, or csf confusion matrix. the associated images are displayed
upon selection of a dot to establish a direct visual link between input space
and embedding. 2) concept cluster plots: see examples in fig. 1c. to abstract
away from individual points in the scatter plot, concepts of interest, such as
classes or distribution shifts can be defined and visualized to identify
conceptual commonalities and differences in the data as perceived by the model.
therefore, k-means clustering is applied to the 3-dimensional embedding. nine
clusters are identified per concept and the resulting plots show the
closest-to-center image per cluster as a visual representation of the concept.
3) silent failure visualization: see examples in fig. 2. we sort all failures by
the classifier confidence and by default show the images associated with the
top-two most confident failures. for corruption shifts, we further allow
investigating the predictions on a fixed input image over varying intensity
levels.based on these visualizations, the functionality of sf-visuals is
three-fold: 1) visual analysis of the dataset including distribution shifts. 2)
visual analysis of the general behavior of various csfs on a given task 3)
visual analysis of individual silent failures in the dataset for various csfs.
evaluating silent failure prevention: we follow jaeger et al. [15] in evaluating
silent failure prevention as a joint task of the classifier and the csf. the
area under the risk-coverage curve aurc reflects this task, since it considers
both the classifier's accuracy as well as the csf's ability to detect failures
by assigning low confidence scores. thus, it can be interpreted as a silent
failure rate or the error rate averaged over steps of filtering cases one by one
according to their rank of confidence score (low to high). exemplary
risk-coverage curves are shown in appendix fig. 3. compared confidence scoring
functions: we compare the following csfs: the maximum softmax response (msr) and
the predictive entropy computed from the classifier's softmax output, three
predictive uncertainty measures based on monte-carlo dropout (mcd) [8], namely
mean softmax (mcd-msr), predictive entropy (mcd-pe) and expected entropy
(mcd-ee), confidnet [5], which is trained as an extension to the classifier,
deepgamblers (dg) that learns a confidence like reservation score (dg-res) [22]
and the work of devries et al. [6]. training settings: on each dataset, we
employ the classifier behind the respective leading results in literature: for
chest xray data we use densenet121 [12], for dermoscopy data we use
efficientnet-b4 [29] and for fluorescence cell microscopy and lung nodule ct
data we us densenet161 [12]. we select the initial learning rate between 10 -3
and 10 -5 and weight decay between 0 and 10 -5 via grid search and optimize for
validation accuracy. all models were trained with dropout. all hyperparameters
can be found in appendix table 3.
table 1 shows the results of our benchmark for silent failure prevention in the
biomedical domain and provides the first overview of the current state of the
reliability of classification systems in high-stake biomedical applications.
sources. this result is generally consistent with previous findings in bernhard
et al. [3] and jaeger et al. [15], but is shown for the first time for a diverse
range of realistic biomedical failure sources. previously proposed methods do
not outperform msr baselines even in the settings they have been proposed for,
e.g. devries et al. under distribution shifts, or confidnet and dg-res for
i.i.d. testing. mcd and loss attenuation are able to improve the msr. mcd-msr is
the overall best performing method indicating that mcd generally improves the
confidence scoring ability of softmax outputs on these tasks. interestingly, the
dg loss attenuation applied to mcd-msr, dg-mcd-msr, which has not been part of
the original dg publication but was first tested in jaeger et al. [15], shows
the best results on i.i.d. testing on 3 out of 4 tasks. however, the method is
not reliable across all settings, falling short on manifestation shifts and
corruptions on the lung nodule ct dataset. effects of particular shifts on the
reliability of a csf might be interdependent. when looking beyond the averages
displayed in table 1 and analyzing the results of individual clinical centers,
corruptions and manifestation shifts, one remarkable pattern can be observed: in
various cases, the same csf showed opposing behavior between two variants of the
same shift on the same dataset. for instance, devries et al. outperforms all
other csfs for one clinical site (mskcc) as target domain, but falls short on
the other one (hcb).on the chest x-ray dataset, mcd worsens the performance for
darkening corruptions across all csfs and intensity levels, whereas the opposite
is observed for brightening corruptions. further, on the lung nodule ct dataset,
dg-mcd-res performs best on bright/dark corruptions and the spiculation
manifestation shift, but worst on noise corruption and falls behind on the
texture manifestation shift. these observations indicate trade-offs, where,
within one distribution shift, reliability against one domain might induce
susceptibility to other domains. current systems are not generally reliable
enough for clinical application. although csfs can mitigate the rate of silent
failures (see appendix fig. 3), the reliability of the resulting classification
systems is not sufficient for high-stake applications in the biomedical domain,
with substantial rates of silent failure in three out of four tasks. therefore,
a deeper understanding of the root causes of these failures is needed.
sf-visuals enables comprehensive analysis of silent failures. figure 1 vividly
demonstrates the added benefit of the proposed tool. first, an interactive
scatter plot (fig. 1b, left) provides an overview of the mskcc acquisition shift
on the dermoscopy dataset and reveals a severe change of the data distribution.
for instance, some malignant lesions of the target domain (purple dots) are
located deep within the "benign" cluster. figure 1c provides a concept cluster
plot that visually confirms how some of these lesions (purple dot) share
characteristics of the benign cluster of the source domain (turquoise dot), such
as being smaller, brighter, and rounder compared to malignant source-lesions
(blue dot). the right-hand plot of fig. 1b reveals that these cases have in fact
caused silent failures (red crosses) and visual inspection (see arrow and fig.
1a) confirms the hypothesis that these failures have been caused by the fact
that the acquisition shift introduced malignant target-lesions that exhibit
benign characteristics. figure 1b in both examples, the brightening of the image
leads to a malignant lesion taking on benign characteristics (brighter and
smoother skin on the dermoscopy data, decreased contrast between lesion and
background on the lung nodule ct data). acquisition shift: additionally to the
example in fig. 1, fig. 2e shows how the proposed tool visualizes an acquisition
shift on the chest x-ray data. while this reveals an increased blurriness in the
target domain, it is difficult to derive further insights involving specific
pathologies without a clinical expert. figure 2h shows a classification failure
from a target clinical center together with the model's confidence as measured
by msr and dg. while msr assigns the prediction low confidence thereby catching
the failure, dg assigns high confidence for the same model and prediction,
causing a silent failure. this example shows how the tool allows the comparison
of csfs and can help to identify failure modes specific to each csf.
manifestation shift: on the dermoscopy data (fig. 2g), we see how a
manifestation shift can cause silent failures. the benign lesions in the target
domain are similar to the malignant lesions in the source domain (rough skin,
irregular shapes), and indeed the two failures in the target domain seem to fall
into this trap. on the lung nodule ct data (fig. 2f), we observe a visual
distinction between the spiculated target domain (spiked surface) and the
non-spiculated source domain (smooth surface).
we see two major opportunities for this work to make an impact on the community.
1) we hope the revealed shortcomings of current systems on biomedical tasks in
combination with the deeper understanding of csf behaviors granted by sf-visuals
will catalyze research towards a new generation of more reliable csfs. 2) this
study shows that in order to progress towards reliable ml systems, a deeper
understanding of the data itself is required. sf-visuals can help to bridge this
gap and equip researchers with a better intuition of when and how to employ ml
systems for a particular task.
sf-visuals generates insights across tasks and distribution shifts. i.i.d. (no
shift):this analysis reveals how simple class clustering (no distribution shifts
involved) can help to gain intuition on the most severe silent failures
(examples selected as the two highest-confidence failures). on the lung nodule
ct data (fig.2a), we see how the classifier and csf break down when a malignant
sample (typically: small bright, round) exhibits characteristics typical to
benign lesions (larger, less cohesive contour, darker) and vice versa. this
pattern of contrary class characteristics is also observed on the dermoscopy
dataset (2c). the failure example at the top is particularly severe, and
localization in the scatter plot reveals a position deep inside the 'benign'
cluster indicating either a severe sampling error in the dataset (e.g.
underrepresented lesion subtype) or simply a wrong label. corruption shift:
figs.2b and 2dshow for the lung nodule ct data and the dermoscopy data,
respectively, how corruptions can lead to silent failures in low-confident
predictions.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1 39.
detecting out-of-distribution (ood) samples is crucial in real-world
applications of machine learning, especially in medical imaging analysis where
misdiagnosis can pose significant risks [7]. recently, deep neural networks,
particularly resnets [9] and u-nets [15], have been widely used in various
medical imaging applications such as classification and segmentation tasks,
achieving state-ofthe-art performance. however, due to the typical
overconfidence seen in neural networks [8,18], deep learning with uncertainty
estimation is becoming increasingly important in ood detection.deep
learning-based ood detection methods with uncertainty estimation, such as
evidential deep learning (edl) [10,17] and its variants [2,[11][12][13]24], have
shown their superiority in terms of computational performance, efficiency, and
extensibility. however, most of these methods consider identifying outliers that
significantly differ from training samples(e.g. natural images collected from
imagenet [5]) as ood samples [1]. these approaches overlook the inherent near
ood problem in medical images, in which instances belong to categories or
classes that are not present in the training set [21] due to the differences in
morbidities. failing to detect such near ood samples poses a high risk in
medical application, as it can lead to inaccurate diagnoses and treatments. some
recent works have been proposed for near ood detection based on density models
[20], preprocessing [14], and outlier exposure [16]. nevertheless, all of these
approaches are susceptible to the quality of the training set, which cannot
always be guaranteed in clinical applications.to address this limitation, we
propose an evidence reconciled neural network (ernn), which aims to reliably
detect those samples that are similar to the training data but still with
different distributions (near ood), while maintain accuracy for in-distribution
(id) classification. concretely, we introduce a module named evidence reconcile
block (erb) based on evidence offset. this module cancels out the conflict
evidences obtained from the evidential head, maximizes the uncertainty of
derived opinions, thus minimizes the error of uncertainty calibration in ood
detection. with the proposed method, the decision boundary of the model is
restricted, the capability of medical outlier detection is improved and the risk
of misdiagnosis in medical images is mitigated. extensive experiments on both
isic2019 dataset and in-house pancreas tumor dataset demonstrate that the
proposed ernn significantly improves the reliability and accuracy of ood
detection for clinical applications. code for ernn can be found at
https://github.com/kelladoe/ernn.
in this section, we introduce our proposed evidence reconciled neural network
(ernn) and analyze its theoretical effectiveness in near ood detection. in our
approach, the evidential head firstly generates the original evidence to support
the classification of each sample into the corresponding class. and then, the
proposed evidence reconcile block (erb) is introduced, which reforms the derived
evidence representation to maximize the uncertainty in its relevant opinion and
better restrict the model decision boundary. more details and theorical analysis
of the model are described below.
traditional classifiers typically employ a softmax layer on top of feature
extractor to calculate a point estimation of the classification result. however,
the point estimates of softmax only ensure the accuracy of the prediction but
ignore the confidence of results. to address this problem, edl utilizes the
dirichlet distribution as the conjugate prior of the categorical distribution
and replaces the softmax layer with an evidential head which produces a
non-negative output as evidence and formalizes an opinion based on evidence
theory to explicitly express the uncertainty of generated evidence. wherebased
on the fact that the parameters of the categorical distribution should obey
dirichlet distribution, the model prediction ŷ and the expected cross entropy
loss l ece on dirichlet distributioncan be inferred as:(2)
in case of ood detection, since the outliers are absent in the training set, the
detection is a non-frequentist situation. referring to the subjective logic
[10], when a variable is not governed by a frequentist process, the statical
accumulation of supporting evidence would lead to a reduction in uncertainty
mass. therefore, traditional evidence generated on the basis accumulation is
inapplicable and would lead to bad uncertainty calibration in ood detection.
moreover, the higher the similarity between samples, the greater impact of
evidence accumulation, which results in a dramatic performance degradation in
medical near ood detection.to tackle the problem mentioned above, we propose an
evidence reconcile block (erb) that reformulates the representation of original
evidence and minimizes the deviation of uncertainty in evidence generation. in
the proposed erb, different pieces of evidence that support different classes
are canceled out by transforming them from subjective opinion to epistemic
opinion and the theoretical maximum uncertainty mass is obtained.as shown in
fig. 1, the simplex corresponding to k-class opinions has k dimensions
corresponding to each category and an additional dimension representing the
uncertainty in the evidence, i.e., vacuity in edl. for a given opinion ω, its
projected predictive probability is shown as p with the direction determined by
prior a. to ensure the consistency of projection probabilities, epistemic
opinion ω should also lie on the direction of projection and satisfy that at
least one belief mass of ω is zero, corresponding to a point on a side of the
simplex. let ü denotes the maximum uncertainty, it should satisfy:since a is a
uniform distribution defined earlier, the transformed belief mass can be
calculated as: b = bb min , where b min is the minimum value in the original
belief mass b. similarly, the evidence representation ë in our erb, based on
epistemic opinion ω, can be formulated as:]a = e -min i e, f or i ∈ {1, . . . ,
k}.(after the transformation by erb, the parameters α = ë + 1 of dirichlet
distribution associate with the reconciled evidence can be determined, and the
reconciled evidential cross entropy loss l rece can be inferred as (6), in which
s = k i=1 αi .by reconciling the evidence through the transformation of
epistemic opinion in subjective logic, this model can effectively reduce errors
in evidence generation caused by statistical accumulation. as a result, it can
mitigate the poor uncertainty calibration in edl, leading to better error
correction and lower empirical loss in near ood detection, as analysized in
sect. 2.3. as shown in fig. 2, we utilize samples from three gaussian
distributions to simulate a 3-classification task and generate evidences based
on the probability density of each class. when using the traditional cnn to
measure the uncertainty of the output with predictive entropy, the model is
unable to distinguish far ood due to the normalization of softmax. while the
introduction of evidence representation in the vacuity of edl allows effective
far ood detections. however, due to the aforementioned impact of evidence
accumulation, we observe that the edl has a tendency to produce small
uncertainties for outliers close to in-distribution (id) samples, thus leading
to failures in detecting near ood samples. our proposed method combines the
benefits of both approaches, the evidence transformed by erb can output
appropriate uncertainty for both near and far ood samples, leading to better
identification performance for both types of outliers.
to further analyze the constraint of the proposed model in ood detection, we
theoretically analyze the difference between the loss functions before and after
the evidence transformation, as well as why it can improve the ability of near
ood detection. detailed provements of following propositons are provided in
supplements.proposition 1. for a given sample in k-classification with the label
c andthe misclassified id samples with p c = α c /s ≤ 1/k are often located at
the decision boundary of the corresponding categories. based on proposition 1,
the reconciled evidence can generate a larger loss, which helps the model focus
more on the difficult-to-distinguish samples. therefore, the module can help
optimize the decision boundary of id samples, and promote the ability to detect
near ood.proposition 2. for a given sample in k-classification with the label c
and k i=1 α i = s, for any α c > s k , l rece < l ece is satisfied. due to the
lower loss derived from the proposed method, we achieve better classification
accuracy and reduce empirical loss, thus the decision boundary can be better
represented for detecting outliers. proposition 3. for a given sample in
k-classification and dirichlet distribution parameter α, when all values of α
equal to const α, l rece ≥ l ece is satisfied.during the training process, if
the prediction p of id samples is identical to the ideal ood outputs, the
proposed method generates a greater loss to prevent such evidence from
occurring. this increases the difference in predictions between id and ood
samples, thereby enhancing the ability to detect ood samples using prediction
entropy.in summary, the proposed evidence reconciled neural network (ernn)
optimizes the decision boundary and enhances the ability to detect near ood
samples. specifically, our method improves the error-correcting ability when the
probability output of the true label is no more than 1/k, and reduces the
empirical loss when the probability output of the true label is greater than
1/k. furthermore, the proposed method prevents model from generating same
evidence for each classes thus amplifying the difference between id and ood
samples, resulting in a more effective near ood detection.
datasets. we conduct experiments on isic 2019 dataset [3,4,19] and an inhouse
dataset. isic 2019 consists of skin lesion images in jpeg format, which are
categorized into nv (12875), mel (4522), bcc (3323), bkl (2624), ak (867), scc
(628), df (239) and vasc (253), with a long-tailed distribution of classes. in
line with the settings presented in [14,16], we define df and vasc, for which
samples are relatively scarce as the near-ood classes. the in-house pancreas
tumor dataset collected from a cooperative hospital is composed of eight
classes: pdac (302), ipmn (71), net (43), scn (37), asc (33), cp (6), mcn (3),
and panin (1). for each sequence, ct slices with the largest tumor area are
picked for experiment. similarly, pdac, ipmn and net are chosen as id classes,
while the remaining classes are reserved as ood categories.implementations and
evaluation metrics. to ensure fairness, we used pretrained resnet34 [9] as
backbone for all methods. during our training process, the images were first
resized to 224 × 224 pixels and normalized, then horizontal and vertical flips
were applied for augmentation. the training was performed using one geforce rtx
3090 with a batch size of 256 for 100 epochs using the adamw optimizer with an
initial learning rate of 1e-4 along with exponential decay. note that we
employed five-fold cross-validation on all methods, without using any additional
ood samples during training. furthermore, we selected the precision(pre),
recall(rec), and f1-score(f1) as the evaluation metrics for id samples, and used
the area under receiver operator characteristic (auroc) as ood evaluation
metric, in line with the work of [6].
in the experiment, we compare the ood detection performance of ernn to several
uncertainty-based approaches:• prototype network described in [22], where the
prototypes of classes are introduced and the distance is utilized for
uncertainty estimation. • prior network described in [12], in which the second
order dirichlet distribution is utlized to estimate uncertainty. • evidential
deep learning described in [17], introduces evidence representation and
estimates uncertainty through subjective logic. • posterior network described in
[2], where density estimators are used for generating the parameters of
dirichlet distributions.inspired by [14], we further compare the proposed method
with mixup-based methods:• mixup: as described in [23], mix up is applied to all
samples.• mt-mixup: mix up is only applied to mid-class and tail-class samples.•
mtmx-prototype: on the basis of mt mixup, prototype network is also applied to
estimate uncertainty.the results on two datasets are shown in table 1. we can
clearly observe that ernn consistently achieves better ood detection performance
than other uncertainty-based methods without additional data augmentation. even
with using mixup, ernn exhibits near performance with the best method
(mtmx-prototype) on isic 2019 and outperforms the other methods on in-house
datasets. all of the experimental results verify that our ernn method improves
ood detection performance while maintaining the results of id classification
even without any changes to the existing architecture.
in this section, we conduct a detailed ablation study to clearly demonstrate the
effectiveness of our major technical components, which consist of evaluation of
evidential head, evaluation of the proposed evidence reconcile block on both
isic 2019 dataset and our in-house pancreas tumor dataset. since the evidence
reconcile block is based on the evidential head, thus there are four
combinations, but only three experimental results were obtained. as shown in
table 2, it is clear that a network with an evidential head can improve the ood
detection capability by 6% and 1% on isic dataset and in-house pancreas tumor
dataset respectively. furthermore, introducing erb further improves the ood
detection performance of ernn by 1% on isic dataset. and on the more challenging
inhouse dataset, which has more similarities in samples, the proposed method
improves the auroc by 2.3%, demonstrating the effectiveness and robustness of
our model on more challenging tasks.
in this work, we propose a simple and effective network named evidence
reconciled nueral network for medical ood detection with uncertainty estimation,
which can measure the confidence in model prediction. our method addresses the
failure in uncertainty calibration of existing methods due to the similarity of
near ood with id samples. with the evidence reformation in the proposed evidence
reconcile block, the error brought by accumulative evidence generation can be
mitigated. compared to existing state-of-the-art methods, our method can achieve
competitive performance in near ood detection with less loss of accuracy in id
classification. furthermore, the proposed plug-and-play method can be easily
applied without any changes of network, resulting in less computation cost in
identifying outliers. the experimental results validate the effectiveness and
robustness of our method in the medical near ood detection problem.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1_30.
lung cancer is the main cause of cancer death worldwide [18]. pulmonary nodules
and masses are both features present in computed tomography images that aid in
the diagnosis of lung cancer. the primary difference is that a nodule is smaller
than 30 mm in diameter, while a mass is larger than 30 mm [22]. early detection
of these features is crucial to aid physicians in making a diagnosis of z. li
and j. yang-equal contributions. visualization on results of four large-scale
mass segmentation given by nnu-net baseline [7]. compared with the ground-truth
segmentation, the recall rate for these four samples is 46.29%, 58.34%, 79.51%,
and 68.51%, respectively. this is significantly lower than the mean value of
81.68%. (b): statistics of the number of nodules at different scales in three
datasets. the range of nodule diameter corresponding to micro, small, medium,
and mass is (0, 10], (10,20], (20,30], [30, ∞), respectively. (c) : the
distribution of recall rate with respect to the nodule size. existing methods
have low recall rates for the segmentation of large scale nodules and
masses.benign or malignant tumors [27] and determining follow-up treatment.
lesion segmentation can be utilized to evaluate two important factors: the
volume of the lesion and its growth rate [5,6,8,12]. furthermore, obtaining
accurate information regarding the nodule can assist in determining the
appropriate resection method and surgical margin required to preserve as much
lung function as possible. [14,17].segmenting nodules is a tedious task that
requires significant human labor. computer aided diagnosis (cad) systems can
significantly reduce such heavy workloads. the accuracy of the existing nodule
detection model reaches 96.1% [9] accuracy. however, the accuracy of the 3d
nodule segmentation model is prone to significantly decline in the application,
regardless of whether its structure is based on cnn or transformer [2]. as shown
in fig. 1(a-c), the recall rate of the large-scale nodule and mass is usually
lower than the average level. the main reason is that the lesion scale in the
two public datasets are relatively small, which matches the fact few patients
have very large nodule or mass. this makes the pulmonary nodule and mass
segmentation task resemble a long-tail problem rather than a mere large scale
span problem. this leads to unsatisfactory results when segmenting large lesions
that require more accurate delineation [26].several studies have proposed
solutions to tackle the large scale span challenges at both the input and
feature level. for instance, some approaches adopt multi-scale inputs [4], where
the input images are resized to different resolu-tion ratios. some other methods
leverage multi-scale feature maps to capture information from different scales,
such as cross-scale feature fusion [19] or using multi-scale convolutional
filters [3]. furthermore, the attention mechanisms [23] has also been utilized
to emphasize the features that are more relevant for segmentation. though these
methods have achieved impressive performance, they still struggle to accurately
segment the extremely imbalanced multi-scale lesions.recently, some click-based
lesion segmentation methods [19][20][21] introduce the click at the input or
feature level and modify the network accordingly, resulting in higher accuracy
results. yet, the click input does not provide the scale information of lesions
for the network.in this paper, we propose a scale-aware test-time click
adaptation (sattca) method, which simply utilizes easily obtainable lesion click
(i.e., the center detected nodule) to adjust the parameters of the network
normalization layers [24] during testing. note that we do not need to exploit
any data from the training set. specifically, we expand the click into an
ellipsoid mask, which supervises the test-time adaptation. this helps to improve
the segmentation performance of large-scale nodules and masses. additionally, we
also propose a multi-scale input encoder to further address the problem of
imbalanced lesion scales. experimental results on two public datasets and one
in-house dataset demonstrate that the proposed method outperforms existing
methods with different backbones.
for pulmonary nodule and mass segmentation, existing methods mostly rely on
regions of interest (roi) obtained by lesion detection networks. a set of 3d roi
inputs i can be represented as i ∈ r d×h×w with size (d, h, w ), along with its
corresponding segmentation ground truth of nodules and masses represented by s ∈
(0, 1)d×h×w . typically, a neural network with weighted parameters θ is trained
to predict the lesion area ŝ = θ(i), with the goal of minimizing the loss
function l (s, ŝ). the stochastic gradient descent (sgd) and the automatic data
acquisition module weight decay (adamw) optimizers are usually used to optimize
the weighted parameters.for each roi input, the center point c of the lesion,
which is represented as2 ) in cartesian coordinate system, can be used as a
reference point to assist the network in improving segmentation performance.
this can be achieved either through an artificial or automatic approach, for
instance, by adding click channels directly to the input or by adding a prior
encoder to the network as demonstrated by the methods [19,20]. however,
incorporating clicks in this way does not focus on addressing the extremely
imbalanced lesion scales.
the network structure of the proposed method, as shown in fig. 2, is enhanced
with a multi-scale (ms) input encoder to address the issue of multi-scale
lesions. we first get the predicted segmentation ŝi and compute its minimum 3d
bounding box b from the trained model. then we generate an ellipsoid mask mi
(around the center ci of the detected nodule) whose size is proportional to the
size of b to supervise the parameter updating during test-time adaptation. our
sattca method is applicable to backbones on cnn and transformer. we also adopt a
multiscale input encoder to further improve the segmentation performance of
nodules and masses with different scales.to achieve this, we employ a clipping
strategy to adjust the proportion of foreground and background in the input
image, producing a group of input images with dimensions of 64 × 96 × 96, 32 ×
48 × 48, and 16 × 24 × 24. these images are then passed through three
convolution paths. the feature maps are concatenated as they are down-sampled to
the same scale. the subsequent modules can be based on either cnn or transformer
structures. the multi-scale input encoder allows the network to capture more
scale information of the nodules and masses, thus mitigating the problem of
large lesion scale span.
in clinical scenarios, the neural network for assisted diagnosis is generally a
pretraining model. due to differences in the statistical distribution of
pulmonary nodule scale in image data from different medical centers, the
segmentation results of some images, especially for large nodules, are worse
than expected. for such scenarios, we propose the scale-aware test-time click
adaptation method, which can improve the performance of segmentation results for
largescale nodules and masses by adjusting some of the network parameters during
testing. the pipeline of the proposed method is shown in fig. 2. first, we use
the pre-trained network to pre-segment the input ct from the test set, getting
ŝi = θwhere n is the number of samples in the test set. then we make a
projection on the main connected region of ŝi along three coordinate axes to
obtain the size of the bounding box b i = (d, w, h) of the pre-segmentation
result, and generate an ellipsoid m i with three axes length proportional to the
corresponding side length of the bounding box b i . more formally, the
coordinates of any foreground voxel point v : (x, y, z) in m i meets the
following requirement:where r represents the mapping function between the axis
length of the ellipsoid and the side length of the bounding box b i . taking the
x-axis as an example, r(d) is given by: r (d) = min (0.02to account for the
introduction of error information at some voxels during adaptive click
adjustment, we develop a mapping function to generate m i adaptively based on
the size of nodules and masses. if the nodule's length and diameter are less
than 7 mm, m i degenerates into a voxel. when the predicted nodule size ranges
from 7 mm to 40 mm, the axial length of b i and the side length of the bounding
box follow a quadratic nonlinear relationship. if the predicted nodule size is
greater than 40 mm, the axial length of m i has a linear relationship with the
side length. to determine the super parametric values for the mapping function
r, we perform cross-validation on three datasets.
we use the foreground range of adaptively adjusted ellipsoid m i to mask ŝi to
obtain a masked segmentation ŝm i . then we use m i to adjust the normalization
layer parameters in the network during testing [24]. the test-time loss function
l tt is the weighted sum of the binary cross-entropy loss l bce and the dice
loss with sigmoid l dice of m i and ŝm i , and the information entropy loss l
ent of ŝi . formally, l tt is given by:where σ and γ are hyper-parameters set to
0.5 and 1 in all experiments, respectively. the sum of the first two equations
is referred to as click loss l click .
we experiment on two public datasets and one in-house dataset. all three
datasets are divided into training, validation, and test sets using a 7:1:2
ratio.
the lidc dataset is a publicly available lung ct image database containing 1018
scans, developed by the lung image database consortium (lidc). all pulmonary
nodules and masses in the dataset have been annotated by multiple raters. to
generate the ground truth for each nodule and mass, we combined the segmentation
annotations from different raters. overall, we selected a total of 1625 nodules
and masses that were annotated by more than three raters from the lidc dataset
for the experiment.
the lndb dataset published in 2019, comprises 294 ct scans collected between
2016 and 2018. each ct scan in the dataset has been segmented by at least one
radiologist. the nodules included in this dataset are larger than 3 mm. the mean
scale of the lesion in lndb dataset is the shortest among the three datasets. we
adopt 1968 nodules and masses from the lndb dataset.
the in-house data (ours) contains 4055 ct scans and 6864 nodules and masses.
every ct scans are annotated with voxel-level nodule masks by radiologists. we
exclude nodules and masses with diameters larger than 64 mm or smaller than 2
mm, as the diameter of the largest mass in the public dataset is no more than 64
mm.
the performance of the nodule segmentation is evaluated by three metrics:
volume-based dice similarity coefficient (dsc), surface-based normalized surface
dice (nsd) [13], and recall rate, which calculates the shape similarity between
predictions and ground truth.
the roi of the lesion is a patch cropped around nodules or masses from the
original ct scans with shape 64 × 96 × 96. during pre-processing, hounsfield
units (hu) values in all patches are first clipped to the range of [-1350, 150].
min-max normalization is then applied, scaling hu values into the range of [0,
1]. all models are trained using adamw [11] optimizer, cosine annealing learning
rate schedule [10]
we adopt nnunet [7] and transbts [25] as the backbone to evaluate the proposed
method on pulmonary nodule and mass segmentation. nnunet is a robust baseline
with a complete cnn structure. its adaptive framework makes it wellsuited for
pulmonary nodule segmentation. transbts is a 3d medical image segmentation
network with a hybrid architecture of transformer and cnn. it incorporates
long-range dependencies into the traditional cnn structure to achieve a larger
receptive field. the experimental results presented in table 1, consistently
demonstrate that the cnn-based network can achieve better results in multiscale
pulmonary nodule and mass segmentation tasks across all three datasets. this is
mainly due to the fact that large receptive fields may involve background
features that are not conducive to segmentation inference for micro or small
nodules. in datasets such as lidc and in-house, where the number imbalance of
multi-scale lesion phenomena is more notable, the multi-input method
consistently outperforms the other two baselines. we also implemented
comparative experiments with other click methods [20] and [19]. as depicted in
table 1, the experimental results show that using a point and a fixed range of
gaussian intensity expansion in the case of large fluctuations in the size of
the nodules does not take effect in improving the segmentation performance. the
inferior segmentation results of [19] can be attributed to the fact that when it
fuses features of different depths and scales, the number of channels in the
feature map remains the same, and some of the up-sampling or down-sampling
strides are too large, leading to redundancy in shallow features and a lack of
deep features. moreover, the sattca improves the dice coefficient and
surface-based normalized surface dice of segmentation results in both networks.
in particular, as demonstrated in fig. 3, the recall rate of large nodule
segmentation is significantly improved.we further analyze the performance of
sattca. firstly, we present the quantitative comparison in table 2, where we
group the nodules and masses in each dataset at 10 mm intervals and calculate
the average segmentation performance differences of the nodules in each scale
group. the statistical results show that the proposed sattca significantly
improves the recall rate of the segmentation on large nodules and masses. as
shown in fig. 3(a), for nodules smaller than 20 mm, both ttca and sattca
effectively increase the recall rate of predicted segmentation. for the medium
nodule and mass, our sattca proves to be more effective in improving
segmentation performance. fig. 3(b) shows the mean recall rate for lesions at
every scale. the difference between the two scatter diagrams indicates that the
proposed sattca effectively alleviates the issue of extremely imbalanced lesion
scales, and improves the segmentation performance for large lesions. in
addition, for ten epochs of tta, the inference time of each sample will increase
approximately one second comparing with baseline.
this paper introduces a novel approach called the scale-aware test-time click
adaptation for nodule and mass segmentation, which aims to address the issue of
extremely imbalanced lesion scale and poor segmentation performance on
largescale nodules and masses. the network parameters are adapted at the
instance level according to the scale-aware click during testing without
altering the model architecture. this allows the network to achieve high recall
for large-scale lesions. then, a multi-scale input encoder is also proposed to
enhance the segmentation performance of multi-scale nodules and masses.
extensive experiments on two public datasets and one in-house dataset
demonstrate that though sattca increases inference time for each sample by about
one second, it outperforms the corresponding baseline and click-based methods
with different backbones.
colorectal cancer (crc) has become a major threat to health worldwide. since
most crcs originate from colorectal polyps, early screening for polyps is
necessary. given its significance, automatic polyp segmentation models
[5,8,16,18] have been designed to aid in screening. for example, acsnet [21],
hrenet [14], ldnet [20] and ccbanet [11] propose to use convolutional neural
networks to extract multi-scale contexts for robust predictions. lodnet [2],
pranet [5], and msnet [23] aim to improve the model's discrimination of polyp
boundaries. sanet [19] eliminates the distribution gap between the training set
and the testing set, thus improving the model generalization. recently, tganet
[15] introduces text embeddings to enhance the model's discrimination.
furthermore, transfuse [22], ppformer [1], and polyp-pvt [3] introduce the
transformer [4] backbone to extract global contexts, achieving a significant
performance gain.all above models are fully supervised and require pixel-level
annotations. however, pixel-by-pixel labeling is time-consuming and expensive,
which hampers practical clinical usage. besides, many polyps do not have
welldefined boundaries. pixel-level labeling inevitably introduces subjective
noise. to address the above limitations, a generalized polyp segmentation model
is urgently needed. in this paper, we achieve this goal by a weakly supervised
polyp segmentation model (named weakpolyp) that only uses coarse bounding box
annotations. figure 1(a) shows the differences between our weakpolyp and fully
supervised models. compared with fully supervised ones, weakpolyp requires only
a bounding box for each polyp, thus dramatically reducing the labeling cost.
more meaningfully, weakpolyp can take existing large-scale polyp detection
datasets to assist the polyp segmentation task. finally, weakpolyp does not
require the labeling for polyp boundaries, avoiding the subjective noise at
source. all these advantages make weakpolyp more clinically practical.however,
bounding box annotations are much coarser than pixel-level ones, which can not
describe the shape of polyps. simply adopting these box annotations as
supervision introduces too much background noise, thereby leading to suboptimal
models. as a solution, boxpolyp [18] only supervises the pixels with high
certainty. however, it requires a fully supervised model to predict the
uncertainty map. unlike boxpolyp, our weakpolyp completely follows the weakly
supervised form that requires no additional models or annotations. surprisingly,
just by redesigning the supervision loss without any changes to the model
structure, weakpolyp achieves comparable performance to its fully supervised
counterpart. figure 1(b) visualizes some predicted results by
weakpolyp.weakpolyp is mainly enabled by two novel components: mask-to-box (m2b)
transformation and scale consistency (sc) loss. in practice, m2b is applied to
transform the predicted mask into a box-like mask by projection and
backprojection. then, this transformed mask is supervised by the bounding box
annotation. this indirect supervision avoids the misleading of box-shape bias of
annotations. however, many regions in the predicted mask are lost in the
projection and therefore get no supervision. to fully explore these regions, we
propose the sc loss to provide a pixel-level self-supervision while requiring no
annotations at all. specifically, the sc loss explicitly reduces the distance
between predictions of the same image at different scales. by forcing feature
alignment, it inhibits the excessive diversity of predictions, thus improving
the model generalization.in summary, our contributions are three-fold: (1) we
build the weakpolyp model completely based on bounding box annotations, which
largely reduces the labeling cost and achieves a comparable performance to full
supervision. (2) we propose the m2b transformation to mitigate the mismatch
between the prediction and the supervision, and design the sc loss to improve
the robustness of the model against the variability of the predictions. (3) our
proposed weakpolyp is a plug-and-play option, which can boost the performances
of polyp segmentation models under different backbones.
model components. fig. 2 depicts the components of weakpolyp, including the
segmentation phase and the supervision phase. for the segmentation phase, we
adopt res2net [6] as the backbone. for input image i ∈ r h×w , res2net extracts
four scales of featuresconsidering the computational cost, only f 2 , f 3 and f
4 are utilized. to fuse them, we first apply a 1 × 1 convolutional layer to
unify the channels of f 2 , f 3 , f 4 and then use the bilinear upsampling to
unify their resolutions. after being transformed to the same size, f 2 , f 3 , f
4 are added together and fed into one 1 × 1 convolutional layer for final
prediction. instead of the segmentation phase, our contributions primarily lie
in the supervision phase, including mask-to-box (m2b) transformation and scale
consistency (sc) loss. notably, both m2b and sc are independent of the specific
model structure.model pipeline. for each input image i, we first resize it into
two different scales: i 1 ∈ r s1×s1 and i 2 ∈ r s2×s2 . then, i 1 and i 2 are
sent to the segmentation model and get two predicted masks p 1 and p 2 , both of
which have been resized to the same size. next, an sc loss is proposed to reduce
the distance between p 1 and p 2 , which helps suppress the variation of the
prediction. finally, to fit the bounding box annotations (b), p 1 and p 2 are
sent to m2b and converted into box-like masks t 1 and t 2 . with t 1 /t 2 and b,
we calculate the binary cross entropy (bce) loss and dice loss, without worrying
about noise interference. fig. 2. the framework of our proposed weakpolyp model,
which consists of the segmentation phase and the supervision phase. the
segmentation phase predicts the polyp mask for each input firstly, and the
supervision phase uses the coarse box annotation to guide previous predicted
mask. note that our contributions mainly lie in the supervision phase, where the
proposed m2b transformation converts the predicted mask into a box mask to
accommodate the bounding box annotation. besides, another proposed sc loss is
introduced to provide dense supervision from multi-scales, which improves the
consistency of predictions.
one naive method to achieve the weakly supervised polyp segmentation is to use
the bounding box annotation b to supervise the predicted mask p 1 /p 2 .
unfortunately, models trained in this way show poor generalization. because
there is a strong box-shape bias in b. training with this bias, the model is
forced to predict the box-shape mask, unable to maintain the polyp's contours.
to solve this, we innovatively use b to supervise the bounding box mask (i.e.,t
1 /t 2 ) of p 1 /p 2 , rather than p 1 /p 2 itself. this indirect supervision
separates p 1 /p 2 from b so that p 1 /p 2 is not affected by the shape bias of
b while obtaining the position and extent of polyps. but how to implement the
transformation from p 1 /p 2 to t 1 /t 2 ? we design the m2b module, which
consists of two steps: projection and back-projection, as shown in fig.
2.projection. as shown in eq. 1, given a predicted mask p ∈ [0, 1] h×w , we
project it horizontally and vertically into two vectors p w ∈ [0, 1] 1×w and p h
∈ [0, 1] h×1 . in this projection, instead of using mean pooling, we use max
pooling to pick the maximum value for each row/column in p . because max pooling
can completely remove the shape information of the polyp. after projection, only
the position and scope of the polyp are stored in p w and p h . back-projection.
based on p w and p h , we construct the bounding box mask of the polyp by
back-projection. as shown in eq. 2, p w and p h are first repeated into p w and
p h with the same size as p . then, we element-wisely take the minimum of p w
and p h to achieve the bounding box mask t . as shown in fig. 2, t no longer
contains the contours of the polyp.supervision. by m2b, p 1 and p 2 are
transformed into t 1 and t 2 , respectively. because both t 1 /t 2 and b are
box-like masks, we directly calculate the supervision loss between them without
worrying about the misguidance of box-shape bias. specifically, we follow [5,19]
to adopt bce loss l bce and dice loss l dice for model supervision, as shown in
eq. 3.priority. by simple transformation, m2b turns the noisy supervision into a
noise-free one, so that the predicted mask is able to preserve the contours of
the polyp. notably, m2b is differentiable, which can be easily implemented with
pytorch and plugged into the model to participate in gradient backpropagation.
in m2b, most pixels in p are ignored in the projection, thus only a few pixels
with high response values are involved in the supervision loss. this sparse
supervision may lead to non-unique predictions. as shown in fig. 3, after m2b
projection, five predicted masks with different response values can be
transformed into the same bounding box mask. therefore, we consider introducing
the sc loss to achieve dense supervision without annotations, which reduces the
degree of freedom of predictions.method. as shown in fig. 2, due to the
non-uniqueness of the prediction and the scale difference between i 1 and i 2 ,
p 1 and p 2 differ in response values. but come from the same image i 1 . they
should be exactly the same. given this, as shown in eq. 4, we build the dense
supervision l sc by explicitly reducing the distance between p 1 and p 2 , where
(i, j) is the pixel coordinates. note that only pixels inside bounding box are
involved in l sc to emphasize more on polyp regions. despite its simplicity, l
sc brings pixel-level constraints to compensate for the sparsity of l sum , thus
reducing the variety of predictions.
as shown in eq. 5, combining l sum and l sc together, we get weakpolyp model.
note that weakpolyp simply replaces the supervision loss without making any
changes to the model structure. therefore, it is general and can be ported to
other models. besides, l sum and l sc are only used during training. in
inference, they will be removed, thus having no effect on the speed of the
model.
datasets. two large polyp datasets are adopted to evaluate the model
performance, including sun-seg [9] and polyp-seg. sun-seg originates compared
with fully supervised methods. table . 3 shows our weakpolyp is even superior to
many previous fully supervised methods: pranet [5], sanet [19], 2/3d [12] and
pns+ [9], which shows the excellent application prospect of weakly supervised
learning in the polyp field.
limited by expensive labeling cost, pixel-level annotations are not readily
available, which hinders the development of the polyp segmentation field. in
this paper, we propose the weakpolyp model completely based on bounding box
annotations. weakpolyp requires no pixel-level annotations, thus avoiding the
interference of subjective noise labels. more importantly, weakpolyp even
achieves a comparable performance to the fully supervised models, showing the
great potential of weakly supervised learning in the polyp segmentation field.in
future, we will introduce temporal information into weakly supervised polyp
segmentation to further reduce the model's dependence on labeling.
interestingly, weakpolyp even surpasses the fully supervised model on sun-seg,
which indicates that there is a lot of noise in the pixel-level annota-
axillary lymph node (aln) metastasis is a severe complication of cancer that can
have devastating consequences, including significant morbidity and mortality.
early detection and timely treatment are crucial for improving outcomes and
reducing the risk of recurrence. in breast cancer diagnosis, accurately
segmenting breast lesions in ultrasound (us) videos is an essential step for
computer-aided diagnosis systems, as well as breast cancer diagnosis and
treatment. however, this task is challenging due to several factors, including
blurry lesion boundaries, inhomogeneous distributions, diverse motion patterns,
and dynamic changes in lesion sizes over time [12]. the work presented in [10]
proposed the first pixel-wise annotated benchmark dataset for breast lesion
segmentation in us videos, but it has some limitations. although their efforts
were commendable, this dataset is private and contains only 63 videos with 4,619
annotated frames. the small dataset size increases the risk of overfitting and
limits the generalizability capability. in this work, we collected a
larger-scale us video breast lesion segmentation dataset with 572 videos and
34,300 annotated frames, of which 222 videos contain aln metastasis, covering a
wide range of realistic clinical scenarios. please refer to table 1 for a
detailed comparison between our dataset and existing datasets.although the
existing benchmark method dpstt [10] has shown promising results for breast
lesion segmentation in us videos, it only uses the ultrasound image to read
memory for learning temporal features. however, ultrasound images suffer from
speckle noise, weak boundaries, and low image quality. thus, there is still
considerable room for improvement in ultrasound video breast lesion
segmentation. to address this, we propose a novel network called frequency and
localization feature aggregation network (fla-net) to improve breast lesion
segmentation in ultrasound videos. our fla-net learns frequency-based temporal
features and then uses them to predict auxiliary breast lesion location maps to
assist the segmentation of breast lesions in video frames. additionally, we
devise a contrastive loss to enhance the breast lesion location similarity of
video frames within the same ultrasound video and to prohibit location
similarity of different ultrasound videos. the experimental results
unequivocally showcase that our network surpasses state-of-the-art techniques in
the realm of both breast lesion segmentation in us videos and two video polyp
segmentation benchmark datasets (fig. 1).
to support advancements in breast lesion segmentation and aln metastasis
prediction, we collected a dataset containing 572 breast lesion ultrasound
videos with 34,300 annotated frames. table 1 summarizes the statistics of
existing breast lesion us video datasets. among 572 videos, 222 videos with aln
metastasis. nine experienced pathologists were invited to manually annotate
breast lesions at each video frame. unlike previous datasets [10,12], our
dataset has a reserved validation set to avoid model overfitting. the entire
dataset is partitioned into training, validation, and test sets in a proportion
of 4:2:4, yielding a total of 230 training videos, 112 validation videos, and
230 test videos for comprehensive benchmarking purposes. moreover, apart from
the segmentation annotation, our dataset also includes lesion bounding box
labels, which enables benchmarking breast lesion detection in ultrasound videos.
more dataset statistics are available in the supplementary.
figure 2 provides a detailed illustration of the proposed frequency and
localization feature aggregation network (fla-net). when presented with an
ultrasound frame denoted as i t along with its two adjacent video frames (i t-1
and i t-2 ), our initial step involves feeding them through an encoder,
specifically the res2net50 architecture [6], to acquire three distinct features
labeled as f t , f t-1 , and f t-2 . then, we devise a frequency-based feature
aggregation (ffa) module to integrate frequency features of each video frame.
after that, we pass the output features o t of the ffa module into two decoder
branches (similar to the unet decoder [14]): one is the localization branch to
predict the localization map of the breast lesions, while another segmentation
branch integrates the features of the localization branch to fuse localization
feature for segmenting breast lesions. moreover, we devise a location-based
contrastive loss to regularize the breast lesion locations of inter-video frames
and intra-video frames.
according to the spectral convolution theorem in fourier theory, any
modification made to a single value in the spectral domain has a global impact
on all the original input features [1]. this theorem guides the design of ffa
module, which has a global receptive field to refine features in the spectral
domain. as shown in fig. 2, our ffa block takes three features (f t ∈ r c×h×w ,
f t-1 ∈ r c×h×w , and f t-2 ∈ r c×h×w ) as input. to integrate the three input
features and extract relevant information while suppressing irrelevant
information, our ffa block first employs a fast fourier transform (fft) to
transform the three input features into the spectral domain, resulting in three
corresponding spectral domain features ( ft ∈ c c×h×w , ft-1 ∈ c c×h×w , and
ft-2 ∈ c c×h×w ), which capture the frequency information of the input features.
note that the current spectral features ( ft , ft-1 , and ft-2 ) are complex
numbers and incompatible with the neural layers. therefore we concatenate the
real and imaginary parts of these complex numbers along the channel dimension
respectively and thus obtain three new tensors (x t ∈ r 2c×h×w , x t-1 ∈ r
2c×h×w , and x t-2 ∈ r 2c×h×w ) with double channels. afterward, we take the
current frame spectral-domain features x t as the core and fuse the
spatial-temporal information from the two auxiliary spectral-domain features (x
t-1 and x t-2 ), respectively. specifically, we first group three features into
two groups ({x t , x t-1 } and {x t , x t-2 }) and develop a channel attention
function ca(•) to obtain two attention maps. the ca(•) passes an input feature
map to a feature normalization, two 1×1 convolution layers conv(•), a relu
activation function δ(•), and a sigmoid function σ(•) to compute an attention
map. then, we element-wise multiply the obtained attention map from each group
with the input features, and the multiplication results (see y 1 and y 2 ) are
then transformed into complex numbers by splitting them into real and imaginary
parts along the channel dimension. after that, inverse fft (ifft) operation is
employed to transfer the spectral features back to the spatial domain, and then
two obtained features at the spatial domain are denoted as z 1 and z 2 .
finally, we further element-wisely add z 1 and z 2 and then pass it into a
"bconv" layer to obtain the output feature o t of our ffa module.
mathematically, o t is computed by o t = bconv(z 1 + z 2 ), where "bconv"
contains a 3 × 3 convolution layer, a group normalization, and a relu activation
function.
after obtaining the frequency features, we introduce a two-branch decoder
consisting of a segmentation branch and a localization branch to incorporate
temporal features from nearby frames into the current frame. each branch is
built based on the unet decoder [14] with four convolutional layers. let d 1 s
and d 2 s denote the features at the last two layers of the segmentation decoder
branch, and d 1 l and d 2 l denote the features at the last two layers of the
localization decoder branch. then, we pass d 1 l at the localization decoder
branch to predict a breast lesion localization map. then, we element-wisely add
d 1 l and d 1 s , and elementwisely add d 2 l and d 2 s , and pass the addition
result into a "bconv" convolution layer to predict the segmentation map s t of
the input video frame i t .location ground truth. instead of formulating it as a
regression problem, we adopt a likelihood heatmap-based approach to encode the
location of breast lesions, since it is more robust to occlusion and motion
blur. to do so, we compute a bounding box of the annotated breast lesion
segmentation result, and then take the center coordinates of the bounding box.
after that, we apply a gaussian kernel with a standard deviation of 5 on the
center coordinates to generate a heatmap, which is taken as the ground truth of
the breast lesion localization.
note that the breast lesion locations of neighboring ultrasound video frames are
close, while the breast lesion location distance is large for different
ultrasound videos, which are often obtained from different patients. motivated
by this, we further devise a location-based contrastive loss to make the breast
lesion locations at the same video to be close, while pushing the lesion
locations of frames from different videos away. by doing so, we can enhance the
breast lesion location prediction in the localization branch. hence, we devise a
location-based contrastive loss based on a triplet loss [15], and the definition
is given by:where α is a margin that is enforced between positive and negative
pairs. h t and h t-1 are predicted heatmaps of neighboring frames from the same
video. n t denotes the heatmap of the breast lesion from a frame from another
ultrasound video. hence, the total loss l total of our network is computed
by:where g h t and g s t denote the ground truth of the breast lesion
segmentation and the breast lesion localization. we empirically set weights
implementation details. to initialize the backbone of our network, we pretrained
res2net-50 [6] on the imagenet dataset, while the remaining components of our
network were trained from scratch. prior to inputting the training video frames
into the network, we resize them to 352 × 352 dimensions. our network is
implemented in pytorch and employs the adam optimizer with a learning rate of 5
× 10 -5 , trained over 100 epochs, and a batch size of 24. training is conducted
on four geforce rtx 2080 ti gpus. for quantitative comparisons, we utilize
various metrics, including the dice similarity coefficient (dice), jaccard
similarity coefficient (jaccard), f1-score, and mean absolute error (mae).
we conduct a comparative analysis between our network and nine state-of-theart
methods, comprising four image-based methods and five video-based methods. four
image-based methods are unet [14], unet++ [19], transunet [4], and setr [18],
while five video-based methods are stm [13], afb-urr [11], pns+ [9], dpstt [10],
and dcfnet [16]. to ensure a fair and equitable comparison, we acquire the
segmentation results of all nine compared methods by utilizing either their
publicly available implementations or by implementing them ourselves.
additionally, we retrain these networks on our dataset and fine-tune their
network parameters to attain their optimal segmentation performance, enabling
accurate and meaningful comparisons.quantitative comparisons. the quantitative
results of our network and the nine compared breast lesion segmentation methods
are summarized in table 2. analysis of the results reveals that, in terms of
quantitative metrics, video-based methods generally outperform image-based
methods. among nine compared methods, dcfnet [16] achieves the largest dice,
jaccard, and f1-score results, while pns+ [9] and dpstt [10] have the smallest
mae score. more importantly, our fla-net further outperforms dcfnet [16] in
terms of dice, jaccard, and f1-score metrics, and has a superior mae performance
over pns+ [9] and dpstt [10]. specifically, our fla-net improves the dice score
from 0.762 to 0.789, the jaccard score from 0.659 to 0.687, the f1-score result
from 0.799 to 0.815, and the mae score from 0.036 to 0.033. metrics unet [14]
unet++ [19] resunet [7] acsnet [17] pranet [5] pnsnet [8] ours qualitative
comparisons. figure 3 visually presents a comparison of breast lesion
segmentation results obtained from our network and three other methods across
various input video frames. apparently, our method accurately segments breast
lesions of the input ultrasound video frames, although these target breast
lesions have varied sizes and diverse shapes in the input video frames.
to evaluate the effectiveness of the major components in our network, we
constructed three baseline networks. the first one (denoted as "basic") removed
the localization encoder branch and replaced our fla modules with a simple
feature concatenation and a 1 × 1 convolutional layer. the second and third
baseline networks (named "basic+fla" and "basic+lb") incorporate the fla module
and the localization branch into the basic network, respectively. table 3
reports the quantitative results of our method and three baseline networks. the
superior metric performance of "basic+fla" and "basic+lb" compared to "basic"
clearly indicates that our fla module and the localization encoder branch
effectively enhance the breast lesion segmentation performance in ultrasound
videos. then, the superior performance of "basic+fla+lb" over "basic+fla" and
"basic+lb" demonstrate that combining our fla module and the localization
encoder branch can incur a more accurate segmentation result. moreover, our
method has larger dice, jaccard, f1-score results and a smaller mae result than
"basic+fla+lb", which shows that our location-based contrastive loss has its
contribution to the success of our video breast lesion segmentation method.
to further evaluate the effectiveness of our fla-net, we extend its application
to the task of video polyp segmentation. following the experimental protocol
employed in a recent study on video polyp segmentation [8], we retrain our
network and present quantitative results on two benchmark datasets, namely
cvc-300-tv [2] and cvc-612-v [3]. table 4 showcases the dice, iou, s α , e φ ,
and mae results achieved by our network in comparison to state-of-the-art
methods on these two datasets. our method demonstrates clear superiority over
state-ofthe-art methods in terms of dice, iou, e φ , and mae on both the
cvc-300-tv and cvc-612-v datasets. specifically, our method enhances the dice
score from 0.840 to 0.874, the iou score from 0.745 to 0.789, the e φ score from
0.921 to 0.969, and reduces the mae score from 0.013 to 0.010 for the cvc-300-tv
dataset. similarly, for the cvc-612-v dataset, our method achieves improvements
of 0.012, 0.014, 0.019, and 0 in dice, iou, e φ , and mae scores, respectively.
although our s α results (0.907 on cvc-300-tv and 0.920 on cvc-612-v) take the
2nd rank, they are very close to the best s α results, which are 0.909 on
cvc-300-tv and 0.923 on cvc-612-v. hence, the superior metric results obtained
by our network clearly demonstrate its ability to accurately segment polyp
regions more effectively than state-of-the-art video polyp segmentation methods.
in this study, we introduce a novel approach for segmenting breast lesions in
ultrasound videos, leveraging a larger dataset consisting of 572 videos
containing a total of 34,300 annotated frames. we introduce a frequency and
location feature aggregation network that incorporates frequency-based temporal
feature learning, an auxiliary prediction of breast lesion location, and a
location-based contrastive loss. our proposed method surpasses existing
state-of-the-art techniques in terms of performance on our annotated dataset as
well as two publicly available video polyp segmentation datasets. these outcomes
serve as compelling evidence for the effectiveness of our approach in achieving
accurate breast lesion segmentation in ultrasound videos.
semantic segmentation, an essential component of computer-aided medical image
analysis, identifies and highlights regions of interest in various diagnosis
tasks. however, this often becomes complicated due to various factors involving
image modality and acquisition along with pathological and biological variations
[18]. the application of deep learning in this domain has thus certainly
benefited in this regard. most notably, ever since its introduction, the unet
model [19] has demonstrated astounding efficacy in medical image segmentation.
as a result, unet and its derivatives have become the de-facto standard [25].the
original unet model comprises a symmetric encoder-decoder architecture (fig. 1a)
and employs skip-connections, which provide the decoder spatial information
probably lost during the pooling operations in the encoder. although this
information propagation through simple concatenation improves the performance,
there exists a likely semantic gap between the encoder-decoder feature maps.
this led to the development of a second class of unets (fig. 1b). u-net++ [26]
leveraged dense connections and multiresunet [11] added additional convolutional
blocks along the skip connection as a potential remedy.till this point in the
history of unet, all the innovations were performed using cnns. however, the
decade of 2020 brought radical changes in the computer vision landscape. the
long-standing dominance of cnns in vision was disrupted by vision transformers
[7]. swin transformers [15] further adapted transformers for general vision
applications. thus, unet models started adopting transformers [5]. swin-unet [9]
replaced the convolutional blocks with swin transformer blocks and thus
initiated a new class of models (fig. 1c). nevertheless, cnns still having
various merits in image segmentation, led to the development of fusing those two
[2]. this hybrid class of unet models (fig. 1d) employs convolutional blocks in
the encoder-decoder and uses transformer layers along the skip connections.
uctransnet [22] and mctrans [24] are two representative models of this class.
finally, there have also been attempts to develop all-transformer unet
architectures (fig. 1e), for instance, smeswin unet [27] uses transformer both
in encoder-decoder blocks and the skip-connection.very recently, studies have
begun rediscovering the potential of cnns in light of the advancements brought
by transformers. the pioneering work in this regard is 'a convnet for the
20202020ss' [16], which explores the various ideas introduced by transformers
and their applicability in convolutional networks. by gradually incorporating
ideas from training protocol and micro-macro design choices, this work enabled
resnet models to outperform swin transformer models.in this paper, we ask the
same question but in the context of unet models. we investigate if a unet model
solely based on convolution can compete with the transformer-based unets. in
doing so, we derive motivations from the transformer architecture and develop a
purely convolutional unet model. we propose a patch-based context aggregation
contrary to window-based self-attention. in addition, we innovate the skip
connections by fusing the feature maps from multiple levels of encoders.
extensive experiments on 5 benchmark datasets suggest that our proposed
modifications have the potential to improve unet models.
firstly, we analyze the transformer-based unet models from a high-level.
deriving motivation and insight from this, we design two convolutional blocks to
simulate the operations performed in transformers. finally, we integrate them in
a vanilla unet backbone and develop our proposed acc-unet architecture.
leveraging the long-range dependency of self-attention. transformers can compute
features from a much larger view of context through the use of (windowed)
self-attention. in addition, they improve expressivity by adopting inverted
bottlenecks, i.e., increasing the neurons in the mlp layer. furthermore, they
contain shortcut connections, which facilitate the learning [7].adaptive
multi-level feature combination through channel attention. transformer-based
unets fuse the feature maps from multiple encoder levels adaptively using
channel attention. this generates enriched features due to the combination of
various regions of interest from different levels compared to simple
skip-connection which is limited by the information at the current level
[22].based on these observations, we modify the convolutional blocks and
skipconnections in a vanilla unet model to induce the capabilities of long-range
dependency and multi-level feature combinations.
we first explore the possibility of inducing long-range dependency along with
improving expressivity in convolutional blocks. we only use pointwise and
depthwise convolutions to reduce the computational complexity [8].in order to
increase the expressive capability, we propose to include inverted bottlenecks
in convolutional blocks [16], which can be achieve by increasing the number of
channels from c in to c inv = c in * inv_fctr using pointwise convolution. since
these additional channels will increase the model complexity, we use 3 × 3
depthwise convolution to compensate. an input feature map x in ∈ r cin,n,m is
thus transformed to x 1 ∈ r cinv,n,m as (fig. 2b)next, we wish to emulate
self-attention in our convolution block, which at its core is comparing a pixel
with the other pixels in its neighborhood [15]. this comparison can be
simplified by comparing a pixel value with the mean and maximum of its
neighborhood. therefore, we can provide an approximate notion of neighborhood
comparison by appending the mean and max of the neighboring pixel features.
consecutive pointwise convolution can thus consider these and capture a
contrasting view. since hierarchical analysis is beneficial for images [23],
instead of computing this aggregation in a single large window, we compute this
in multiple levels hierarchically, for example,patches. for k = 1, it would be
the ordinary convolution operation, but as we increase the value of k, more
contextual information will be provided, bypassing the need for larger
convolutional kernels. thus, our proposed hierarchical neighborhood context
aggregation enriches feature map x 1 ∈ r cinv,n,m with contextual information as
x 2 ∈ r cinv * (2k-1),n,m (fig. 2b), where || corresponds to concatenation along
the channel dimensionnext, similar to the transformer, we include a shortcut
connection in the convolution block for better gradient propagation. hence, we
perform another pointwise convolution to reduce the number of channels to c in
and add with the input feature map. thus, x 2 ∈ r cinv * (2k-1),n,m becomes x 3
∈ r cin,n,m (fig. 2b)finally, we change the number of filters to c out , as the
output, using pointwise convolution (fig. 2b)thus, we propose a novel
hierarchical aggregation of neighborhood context (hanc) block using convolution
but bringing the benefits of transformers. the operation of this block is
illustrated in fig. 2b.
next, we investigate the feasibility of multi-level feature combination, which
is the other advantage of using transformer-based unets.transformer-based skip
connections have demonstrated effective feature fusion of all the encoder levels
and appropriate filtering from the compiled feature maps by the individual
decoders [22,24,27]. this is performed through concatenating the projected
tokens from different levels [22]. following this approach, we resize the
convolutional feature maps obtained from the different encoder levels to make
them equisized and concatenate them. this provides us with an overview of the
feature maps across the different semantic levels. we apply pointwise
convolution operation to summarize this representation and merge with the
corresponding encoder feature map. this fusion of the overall and individual
information is passed through another convolution, which we hypothesize enriches
the current level feature with information from other level features.for the
features, x 1 , x 2 , x 3 , x 4 from 4 different levels, the feature maps can be
enriched with multilevel information as (fig. 2d)here, resize i (x j ) is an
operation that resizes x j to the size of x i and c tot = c 1 + c 2 + c 3 + c 4
. this operation is done individually for all the different levels.we thus
propose another novel block named multi level feature compilation (mlfc), which
aggregates information from multiple encoder levels and enriches the individual
encoder feature maps. this block is illustrated in fig. 2d.
therefore, we propose fully convolutional acc-unet (fig. 2a). we started with a
vanilla unet model and reduced the number of filters by half. then, we replaced
the convolutional blocks from the encoder and decoder with our proposed hanc
blocks. we considered inv_fctr = 3, other than the last decoder block at level 3
(inv_fctr = 34) to mimic the expansion at stage 3 of swin transformer. k = 3,
which considers up to 4 × 4 patches, was selected for all but the bottleneck
level (k = 1) and the one next to it (k = 2). next, we modified the skip
connections by using residual blocks (fig. 2c) to reduce semantic gap [11] and
stacked 3 mlfc blocks. all the convolutional layers were batch-normalized [12],
activated by leaky-relu [17] and recalibrated by squeeze and excitation [10].to
summarize, in a unet model, we replaced the classical convolutional blocks with
our proposed hanc blocks that perform an approximate version of self-attention
and modified the skip connection with mlfc blocks which consider the feature
maps from different encoder levels. the proposed model has 16.77 m parameters,
roughly a 2m increase than the vanilla unet model.
in order to evaluate acc-unet, we conducted experiments on 5 public datasets
across different tasks and modalities. we used isic-2018 [6,21] (dermoscopy,
2594 images), busi [3](breast ultrasound, used 437 benign and 210 malignant
images similar to [13]), cvc-clinicdb [4] (colonoscopy, 612 images), covid [1]
(pneumonia lesion segmentation, 100 images), and glas [20] (gland segmentation,
85 training, and 80 test images). all the images and masks were resized to 224 ×
224. for the glas dataset, we considered the original test split as the test
data, for the other datasets we randomly selected 20% of images as test data.
the remaining 60% and 20% images were used for training and validation and the
experiments were repeated 3 times with different random shuffling.
we implemented acc-unet model in pytorch and used a workstation equipped with
amd epyc 7443p 24-core cpu and nvidia rtx a6000 (48g) gpu for our experiments.
we designed our training protocol identical to previous works [22], except for
using a batch size of 12 throughout our experiments [27]. the models were
trained for 1000 epochs [27] and we employed an early stopping patience of 100
epochs. we minimized the combined cross-entropy and dice loss [22] using the
adam [14] optimizer with an initial learning rate of 10 -3 , which was adjusted
through cosine annealing learning rate scheduler [13] 1 . we performed online
data augmentations in the form of random flipping and rotating [22].
we evaluated acc-unet against unet, multiresunet, swin-unet, uctransnet,
smeswin-unet, i.e., one representative model from the 5 classes of unet,
respectively (fig. 1). table 1 presents the dice score obtained on the test
sets. the results show an interesting pattern. apparently, for the comparatively
larger datasets (isic-18) transformer-based swin-unet was the 2nd best method,
as transformers require more data for proper training [2]. on the other end of
the spectrum, lightweight convolutional model (multiresunet) achieved the 2nd
best score for small datasets (glas). for the remaining datasets, hybrid model
(uctransnet) seemed to perform as the 2 nd best method. smeswin-unet fell behind
in all the cases, despite having such a large number of parameters, which in
turn probably makes it difficult to be trained on small-scale datasets.however,
our model combining the design principles of transformers with the inductive
bias of cnns seemed to perform best in all the different categories with much
lower parameters. compared to much larger state-of-the-art models, for the 5
datasets, we achieved 0.13%, 0.10%, 0.63%, 0.90%, 0.27% improvements in dice
score, respectively. thus, our model is not only accurate, but it is also
efficient in using the moderately small parameters it possesses. in terms of
flops, our model is comparable with convolutional unets, the transformer-based
unets have smaller flops due to the massive downsampling at patch partitioning.
in addition to, achieving higher dice scores, apparently, acc-unet generated
better qualitative results. figure 3 presents a qualitative comparison of
acc-unet with the other models. each row of the figure comprises one example
from each of the datasets and the segmentation predicted by acc-unet and the
ground truth mask are presented in the rightmost two columns. for the 1 st
example from the isic-18 dataset, our model did not oversegment but rather
followed the lesion boundary. in the 2 nd example from cvc-clinicdb, our model
managed to distinguish the finger from the polyp almost perfectly. next in the 3
rd example from busi, our prediction filtered out the apparent nodule region on
the left, which was predicted as a false positive tumor by all the other models.
similarly, in the 4 th sample from the covid dataset, we were capable to model
the gaps in the consolidation of the left lung visually better, which in turn
resulted in 2.9% higher dice score than the 2 nd best method. again, in the
final example from the glas dataset, we not only successfully predicted the
gland at the bottom right corner but also identified the glands at the top left
individually, which were mostly missed or merged by the other models,
respectively.
we performed an ablation study on the cvc-clinicdb dataset to analyze the
contributions of the different design choices in our roadmap (fig. 4). we
started with a unet model with the number of filters halved as our base model,
which results in a dice score of 87.77% with 7.8m parameters. using depthwise
convolutional along with increasing the bottleneck by 4 raised the dice score to
88.26% while slightly reducing the parameters to 7.5m . next, hanc block was
added with k = 3 throughout, which increased the number of parameters by 340%
for an increase of 1.1% dice score. shortcut connections increased the
performance by 2.16%. we also slowly reduced both k and inv_fctr which reduced
the number of parameters without any fall in performance. finally, we added the
mlfc blocks (4 stacks) and gradually optimized k and inv_fctr along with
dropping one mlfc stage, which led to the development of acc-unet. some other
inter- esting ablations were acc-unet without mlfc (dice 91.9%) or hanc (dice
90.96%, with 25% more filters to keep the number of parameters comparable).
acknowledging the benefits of various design paradigms in transformers, we
investigate the suitability of similar ideas in convolutional unets. the
resultant acc-unet possesses the inductive bias of cnns infused with long-range
and multi-level feature accumulation of transformers. our experiments reveals
this amalgamation indeed has the potential to improve unet models. one
limitation of our model is the slowdown from concat operations (please see
supplementary materials), which can be solved by replacing them. in addition,
there are more innovations brought by transformers [16], e.g., layer
normalization, gelu activation, adamw optimizer, these will be explored further
in our future work.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1_66.
prostate cancer is a leading cause of cancer-related deaths in adult males, as
reported in studies, such as [17]. a common treatment option for prostate cancer
is external beam radiation therapy (ebrt) [4], where ct scanning is a
cost-effective tool for the treatment planning process compared with the more
expensive magnetic resonance imaging (mri). as a result, precise prostate
segmentation in ct images becomes a crucial step, as it helps to ensure that the
radiation doses are delivered effectively to the tumor tissues while minimizing
harm to the surrounding healthy tissues.due to the relatively low spatial
resolution and soft tissue contrast in ct images compared to mri images, manual
prostate segmentation in ct images can be time-consuming and may result in
significant variations between operators [10]. several automated segmentation
methods have been proposed to alleviate these issues, especially the fully
convolutional networks (fcn) based u-net [19] (an encoder-decoder architecture
with skip connections to preserve details and extract local visual features) and
its variants [14,23,26]. despite good progress, these methods often have
limitations in capturing long-range relationships and global context information
[2] due to the inherent bias of convolutional operations. researchers naturally
turn to vit [5], powered with self-attention (sa), for more possibilities:
transunet first [2] adapts vit to medical image segmentation tasks by connecting
several layers of the transformer module (multi-head sa) to the fcn-based
encoder for better capturing the global context information from the high-level
feature maps. transfuse [25] and medt [21] use a combined fcn and transformer
architecture with two branches to capture global dependency and low-level
spatial details more effectively. swin-unet [1] is the first u-shaped network
based purely on more efficient swin transformers [12] and outperforms models
with fcn-based methods. unetr [6] and siwnunetr [20] are transformer
architectures extended for 3d inputs.in spite of the improved performance for
the aforementioned vit-based networks, these methods utilize the standard or
shifted-window-based sa, which is the fine-grained local sa and may overlook the
local and global interactions [18,24]. as reported by [20], even pre-trained
with a massive amount of medical data using self-supervised learning, the
performance of prostate segmentation task using high-resolution and better soft
tissue contrast mri images has not been completely satisfactory, not to mention
the lower-quality ct images. additionally, the unclear boundary of the prostate
in ct images derived from the low soft tissue contrast is not properly addressed
[7,22].recently, focal transformer [24] is proposed for general computer vision
tasks, in which focal self-attention is leveraged to incorporate both
fine-grained local and coarse-grained global interactions. each token attends
its closest surrounding tokens with fine granularity, and the tokens far away
with coarse granularity; thus, focal sa can capture both short-and long-range
visual dependencies efficiently and effectively. inspired by this work, we
propose the focalunetr (focal u-net transformers), a novel focal transformer
architecture for ctbased medical image segmentation (fig. 1a). even though prior
works such as psi-net [15] incorporates additional decoders to enhance boundary
detection and distance map estimation, they either lack the capacity for
effective global context capture through fcn-based techniques or overlook the
significance of considering the randomness of the boundary, particularly in poor
soft tissue contrast ct images for prostate segmentation. in contrast, our
approach utilizes a multi-task learning strategy that leverages a gaussian
kernel over the boundary of the ground truth segmentation mask [11] as an
auxiliary boundary-aware contour regression task (fig. 1b). this serves as a
regularization term for the main task of generating the segmentation mask. and
the auxiliary task enhances the model's generalizability by addressing the
challenge of unclear boundaries in low-contrast ct images. in this paper, we
make several new contributions. first, we develop a novel focal transformer
model (focalunetr) for ct-based prostate segmentation, which makes use of focal
sa to hierarchically learn the feature maps accounting for both short-and
long-range visual dependencies efficiently and effectively. second, we also
address the challenge of unclear boundaries specific to ct images by
incorporating an auxiliary task of contour regression. third, our methodology
advances state-of-the-art performance via extensive experiments on both
realworld and benchmark datasets.
our focalunetr architecture (fig. 1) follows a multi-scale design similar to
[6,20], enabling us to obtain hierarchical feature maps at different stages. the
input medical image x ∈ r c×h×w is first split into a sequence of tokens with
dimension h h × w w , where h, w represent spatial height and width,
respectively, and c represents the number of channels. these tokens are then
projected into an embedding space of dimension d using a patch of resolution (h
, w ). the sa is computed at two focal levels [24]: fine-grained and
coarse-grained, as illustrated in fig. 2a. the focal sa attends to fine-grained
tokens locally, while summarized tokens are attended to globally (reducing
computational cost). we perform focal sa at the window level, where a feature
map of x ∈ r d×h ×w with spatial size h × w and d channels is partitioned into a
grid of windows with size s w × s w . for each window, we extract its
surroundings using focal sa.for window-wise focal sa [24], there are three terms
{l, s w , s r }. focal level l is the number of granularity levels for which we
extract the tokens for our focal sa. we present an example, depicted in fig. 2b,
that illustrates the use of two focal levels (fine and coarse) for capturing the
interaction of local and global context for optimal boundary-matching between
the prediction and the ground truth for prostate segmentation. focal window size
s l w is the size of the sub-window on which we get the summarized tokens at
level l ∈ {1, . . . , l}. focal region size s l r is the number of sub-windows
horizontally and vertically in attended regions at level l. the focal sa module
proceeds in two main steps, subwindow pooling and attention computation. in the
sub-window pooling step, an input feature map x ∈ r d×h ×w is split into a grid
of sub-windows with size {s l w , s l w }, followed by a simple linear layer f l
p to pool the sub-windows spatially. the pooled feature maps at different levels
l provide rich information at both fine-grained and coarse-grained, where)×(s l
w ×s l w ) . after obtaining the pooled feature mapsx l l 1 , we calculate the
query at the first level and key and value for all levels using three linear
projection layers f q , f k , and f v :for the queries inside the i-th window q
i ∈ r d×sw×sw , we extract the s l r × s l r keys and values from k l and v l
around the window where the query lies in and then gather the keys and values
from all l to obtainfinally, a relative position bias is added to compute the
focal sa forwhere b = {b l } l 1 is the learnable relative position bias [24].
the encoder utilizes a patch size of 2×2 with a feature dimension of 2×2×1 = 4
(i.e., a single input channel ct) and a d-dimensional embedding space. the
overall architecture of the encoder comprises four stages of focal transformer
blocks, with a patch merging layer applied between each stage to reduce the
resolution by a factor of 2. we utilize an fcn-based decoder (fig. 1a) with skip
connections to connect to the encoder at each resolution to construct a
"ushaped" architecture for our ct-based prostate segmentation task. the output
of the encoder is concatenated with processed input volume features and fed into
a residual block. a final 1 × 1 convolutional layer with a suitable activation
function, such as softmax, is applied to obtain the required number of
class-based probabilities.
for the main task of mask prediction (as illustrated in fig. 1a), a combination
of dice loss and cross-entropy loss is employed to evaluate the concordance of
the predicted mask and the ground truth on a pixel-wise level. the objective
function for the segmentation head is given by: l seg = l dice (p i , g) + l ce
(p i , g), where pi represents the predicted probabilities from the main task
and g represents the ground truth mask, both given an input image i. the
predicted probabilities, pi , are derived from the main task through the
application of the focalunetr model to the input ct image.to address the
challenge of unclear boundaries in ct-based prostate segmentation, an auxiliary
task is introduced for the purpose of predicting boundaryaware contours to
assist the main prostate segmentation task. this auxiliary task is achieved by
attaching another convolution head after the extracted feature maps at the final
stage (see fig. 1b). the boundary-aware contour, or the induced
boundary-sensitive label, is generated by considering pixels near the boundary
of the prostate mask. to do this, the contour points and their surrounding
pixels are formulated into a gaussian distribution using a kernel with a fixed
standard deviation of σ (in this specific case, e.g., σ = 1.6) [7,11,13]. the
resulting contour is a heatmap in the form of a heatsum function [11]. we
predict this heatmap with a regression task trained by minimizing mean-squared
error instead of treating it as a single-pixel boundary segmentation problem.
given the ground truth of contour g c i , induced from the segmentation mask for
input image i, and the reconstructed output probability pc i , we use the
following loss function:where n is the total number of images for each batch.
this auxiliary task is trained concurrently with the main segmentation task.a
multi-task learning approach is adopted to regularize the main segmentation task
through the auxiliary boundary prediction task. the overall loss function is a
combination of l seg and l reg : l tol = λ 1 l seg + λ 2 l reg , where λ 1 and λ
2 are hyper-parameters that weigh the contribution of the mask prediction loss
and contour regression loss, respectively, to the overall loss. the optimal
setting of λ 1 = λ 2 = 0.5 is determined by trying different settings.
to evaluate our method, we use a large private dataset with 400 ct scans and a
large public dataset with 300 ct scans (amos [9]). as far as we know, the amos
dataset is the only publicly available ct dataset including prostate ground
truth. we randomly split the private dataset with 280 scans for training, 40 for
validation, and 80 for testing. the amos dataset has 200 scans for training and
100 for testing [9]. although the amos dataset includes the prostate class, it
mixes the prostate (in males) and the uterus (in females) into one single class
labeled pro/ute. we filter out ct scans missing the pro/ute ground-truth
segmentation.regarding the architecture, we follow the hyperparameter settings
suggested in [24], with 2 focal levels, transformer blocks of depths [2,2,6,2],
and head numbers [4,8,16,32] for each of the four stages. we then create
focalunetr-s and focalunetr-b with d as 48 and 64, respectively. these settings
have 27.3 m and 48.3 m parameters, which are comparable to other
state-of-the-art models in size.for the implementation, we utilize a server
equipped with 8 nvidia a100 gpus, each with 40 gb of memory. all experiments are
conducted in pytorch, and each model is trained on a single gpu. we interpolate
all ct scans into an isotropic voxel spacing of [1.0 × 1.0 × 1.5] mm for both
datasets. houndsfield unit (hu) range of [-50, 150] is used and normalized to
[0, 1]. subsequently, each ct scan is cropped to a 128 × 128 × 64 voxel patch
around the prostate area, which is used as input for 3d models. for 2d models,
we first slice each voxel patch in the axial direction into 64 slices of 128 ×
128 images for training and stack them back for evaluation. for the private
dataset, we train models for 200 epochs using the adamw optimizer with an
initial learning rate of 5e -4 . an exponential learning rate scheduler with a
warmup of 5 epochs is applied to the optimizer. the batch size is set to 24 for
2d models and 1 for 3d models. we use random flip, rotation, and intensity
scaling as augmentation transforms with probabilities of 0.1, 0.1, and 0.2,
respectively. we also tried using 10% percent of amos training set as validation
data to find a better training parameter setting and re-trained the model with
the full training set. however, we did not get improved performance compared
with directly applying the training parameters learned from tuning the private
dataset. we report the dice similarity coefficient (dsc, %), 95% percentile
hausdorff distance (hd, mm), and average symmetric surface distance (assd, mm)
metrics.
comparison with state-of-the-art methods. to demonstrate the effectiveness of
focalunetr, we compare the ct-based prostate segmentation performance with three
2d u-net-based methods: u-net [19], unet++ [26], and attention u-net (attunet)
[16], two 2d transformer-based segmentation methods: transunet [2] and swin-unet
[1], two 3d u-net-based methods: u-net (3d) [3] and v-net [14], and two 3d
transformer-based models: unetr [6] and siwnunetr [20]. nnunet [8] is used for
comparison as well. both 2d and 3d models are included as there is no conclusive
evidence for which type is better for this task [22]. all methods (except
nnunet) follow the same settings as focalunetr and are trained from scratch.
transunet and swin-unet are the only methods that are pre-trained on imagenet.
detailed information regarding the number of parameters, flops, and average
inference time can be found in the supplementary materials.quantitative results
are presented in table 1, which shows that the proposed focalunetr, even without
co-training, outperforms other fcn and transformer baselines (2d and 3d) in both
datasets for most of the metrics. the amos dataset mixes the
prostate(males)/uterus(females, a relatively small portion). the morphology of
the prostate and uterus is significantly different. consequently, the models may
struggle to provide accurate predictions for this specific portion of the
uterus. thus, the overall performance of focalunetr is overshadowed by this
challenge, resulting in only moderate improvement over the baselines on the amos
dataset. however, the performance margin significantly improves when using the
real-world (private) dataset. when co-trained with the auxiliary contour
regression task using the multi-task training strategy, the performance of
focalunetrs is further improved. in summary, these observations indicate that
incorporating focalunetr and multi-task training qualitative results of several
representative methods are visualized in fig. 3. the figure shows that our
focalunetr-b and focalunetr-b* generate more accurate segmentation results that
are more consistent with the ground truth than the results of the baseline
models. all methods perform well for relatively easy cases (1 st row in fig. 3),
but the focalunetrs outperform the other methods. for more challenging cases
(rows 2-4 in fig. 3), such as unclear boundaries and mixed pro/ute labels,
focalunetrs still perform better than other methods. additionally, the
focalunetrs are less likely to produce false positives (see more in
supplementary materials) for ct images without a foreground ground truth, due to
the focal sa mechanism that enables the model to capture global context and
helps to identify the correct boundary and shape of the prostate. overall, the
focalunetrs demonstrate improved segmentation capabilities while preserving
shapes more precisely, making them promising tools for clinical applications.
ablation study. to better examine the efficacy of the auxiliary task for
focalunetr, we selected different settings of λ 1 and λ 2 for the overall loss
function l tol on the private dataset. the results (table 2) indicate that as
the value of λ 2 is gradually increased and that of λ 1 is correspondingly
decreased (thereby increasing the relative importance of the auxiliary contour
regression task), segmentation performance initially improves. however, as the
ratio of contour information to segmentation mask information becomes too
unbalanced, performance begins to decline. thus, it can be inferred that the
optimal setting for these parameters is when both λ 1 and λ 2 are set to 0.5.
in summary, the proposed focalunetr architecture has demonstrated the ability to
effectively capture local visual features and global contexts in ct images by
utilizing the focal self-attention mechanism. the auxiliary contour regression
task has also been shown to improve the segmentation performance for unclear
boundary issues in low-contrast ct images. extensive experiments on two large ct
datasets have shown that the focalunetr outperforms state-ofthe-art methods for
the prostate segmentation task. future work includes the evaluation of other
organs and extending the focal self-attention mechanism for 3d inputs.
segmentation is key in medical image analysis and is primarily achieved with
pixel-wise classification neural networks [4,14,20]. recently, methods that use
contours defined by points [10,13] have been shown more suitable for organs with
a regular shape (e.g. lungs, heart) while predicting the organ outline similarly
to how experts label data [10,13]. while various uncertainty methods have been
investigated for both pixel-wise image segmentation [6,16,29] and landmark
regression [25,27], few uncertainty methods for point-defined contours in the
context of segmentation exists to date. uncertainty can be epistemic or
aleatoric by nature [16]. epistemic uncertainty models the network uncertainty
by defining the network weights as a probabilistic distribution instead of a
single value, with methods such as bayesian networks [5], mc dropout [11,12] and
ensembles [19]. aleatoric uncertainty is the uncertainty in the data. most
pixel-wise segmentation methods estimate perpixel aleatoric uncertainty by
modeling a normal distribution over each output logit [16]. for regression, it
is common practice to assume that each predicted output is independent and
identically distributed, and follows an univariate normal distribution. in that
case, the mean and variance distribution parameters μ and σ are learned with a
loss function that maximizes their log-likelihood [16].other methods estimate
the aleatoric uncertainty from multiple forward passes of test-time augmentation
[1,29]. some methods do not explicitly model epistemic nor aleatoric
uncertainty, but rather use custom uncertainty losses [9] or add an auxiliary
confidence network [8]. other works predict uncertainty based on an encoded
prior [15] or by sampling a latent representation space [3,18]. the latter
however requires a dataset containing multiple annotations per image to obtain
optimal results.previous methods provide pixel-wise uncertainty estimates. these
estimates are beneficial when segmenting abnormal structures that may or may not
be present. however, they are less suited for measuring uncertainty on organ
delineation because their presence in the image are not uncertain.in this work,
we propose a novel method to estimate aleatoric uncertainty of point-wise
defined contours, independent on the model's architecture, without compromising
the contour estimation performance. we extend state-of-the-art point-regression
networks [21] by modeling point coordinates with gaussian and skewed-gaussian
distributions, a novel solution to predict asymmetrical uncertainty. conversely,
we demonstrate that the benefits of point-based contouring also extend to
uncertainty estimation with highly interpretable results.
let's consider a dataset made of n pairs {x i , y k i } n i=1 , each pair
consisting of an image x i ∈ r h×w of height h and width w , and a series of k
ordered points y k i , drawn by an expert. each point series defines the contour
of one or more organs depending on the task. a simple way of predicting these k
points is to regress 2k values (x-y coordinates) with a cnn, but doing so is
sub-optimal due to the loss of spatial coherence in the output flatten layer
[21]. as an alternative, nabili et al. proposed the dsnt network (differentiable
spatial to numerical transform) designed to extract numerical coordinates of k
points from the prediction of k heatmaps [21] (c.f. the middle plots in fig. 1
for an illustration).inspired by this work, our method extends to the notion of
heatmaps to regress univariate, bivariate, and skew-bivariate uncertainty
models.
univariate model -in this approach, a neural network f θ (•) is trained to
generate k heatmaps z k ∈ r h×w which are normalized by a softmax function so
that their content represents the probability of presence of the center c k of
each landmark point. two coordinate maps i ∈ r h×w and j ∈ r h×w , where
, are then combined to these heatmaps to regress the final position μ k and the
corresponding variance (σ kx , σ k y ) of each landmark point through the
following two equations:where •, • f is the frobenius inner product, corresponds
to the hadamard product, and (σ ky ) 2 is computed similarly. thus, for each
image x i , the neural network f θ (x i ) predicts a tuple (μ i , σ i ) with μ i
∈ r 2k and σ i ∈ r 2k through the generation of k heatmaps. the network is
finally trained using the following univariate aleatoric loss adapted from [16]
where y k i is the k th reference landmark point of image x i . bivariate model
-one of the limitations of the univariate model is that it assumes no x-y
covariance on the regressed uncertainty. this does not hold true in many cases,
because the uncertainty can be oblique and thus involve a nonzero x-y
covariance. to address this, one can model the uncertainty of each point with a
2 × 2 covariance matrix, σ, where the variances are expressed with eq. 2 and the
covariance is computed as follows:the network f θ (x i ) thus predicts a tuple
(μ i , σ i ) for each image x i , with μ i ∈ r k×2 and σ i ∈ r k×2×2 . we
propose to train f θ using a new loss function l n2 :asymmetric model -one
limitation of the bivariate method is that it models a symmetric uncertainty, an
assumption that may not hold in some cases as illustrated on the right side of
fig. 2. therefore we developed a third approach based on a bivariate skew-normal
distribution [2]:where φ n is a multivariate normal, φ 1 is the cumulative
distribution function of a unit normal, σ = ω σω and α ∈ r n is the skewness
parameter. note that this is a direct extension of the multivariate normal as
the skew-normal distribution is equal to the normal distribution when α = 0.the
corresponding network predicts a tuple (μ, σ, α) with μ ∈ r k×2 , σ ∈ r k×2×2
and α ∈ r k×2 . the skewness output α is predicted using a sub-network whose
input is the latent space of the main network (refer to the supplementary
material for an illustration). this model is trained using a new loss function
derived from the maximum likelihood estimate of the skew-normal distribution:
as shown in fig. 2, the predicted uncertainty can be pictured in two ways: (i)
either by per-point covariance ellipses [left] and skewed-covariance profiles
[right] or (ii) by an uncertainty map to express the probability of wrongly
classifying pixels which is highest at the border between 2 classes. in our
formalism, the probability of the presence of a contour (and thus the separation
between 2 classes) can be represented by the component of the uncertainty that
is perpendicular to the contour. we consider the perpendicular normalized
marginal distribution at each point (illustrated by the green line). this
distribution also happens to be a univariate normal [left] or skew-normal
[right] distribution [2].from these distributions, we draw isolines of equal
uncertainty on the inside and outside of the predicted contour. by aggregating
multiple isolines, we construct a smooth uncertainty map along the contours
(illustrated by the white-shaded areas). please refer to the supp. material for
further details on this procedure.3 experimental setup
camus. the camus dataset [20] contains cardiac ultrasounds from 500 patients,
for which two-chamber and four-chamber sequences were acquired.manual
annotations for the endocardium and epicardium borders of the left ventricle
(lv) and the left atrium were obtained from a cardiologist for the end-diastolic
(ed) and end-systolic (es) frames. the dataset is split into 400 training
patients, 50 validation patients, and 50 testing patients. contour points were
extracted by finding the basal points of the endocardium and epicardium and then
the apex as the farthest points along the edge. each contour contains 21
points.private cardiac us. this is a proprietary multi-site multi-vendor dataset
containing 2d echocardiograms of apical two and four chambers from 890 patients.
data comes from patients diagnosed with coronary artery disease, covid, or
healthy volunteers. the dataset is split into a training/validation set (80/20)
and an independent test set from different sites, comprised of 994
echocardiograms from 684 patients and 368 echocardiograms from 206 patients,
respectively. the endocardium contour was labeled by experts who labeled a
minimum of 7 points based on anatomical landmarks and add as many other points
as necessary to define the contour. we resampled 21 points equally along the
contour.jsrt. the japanese society of radiological technology (jsrt) dataset
consists of 247 chest x-rays [26]. we used the 120 points for the lungs and
heart annotation made available by [10]. the set of points contains specific
anatomical points for each structure (4 for the right lung, 5 for the left lung,
and 4 for the heart) and equally spaced points between each anatomical point. we
reconstructed the segmentation map with 3 classes (background, lungs, heart)
with these points and used the same train-val-test split of 70%-10%-20% as [10].
we used a network based on enet [24] for the ultrasound data and on deeplabv3
[7] for the jsrt dataset to derive both the segmentation maps and regress the
per-landmark heatmaps. images were all reshaped to 256 × 256 and b-splines were
fit on the predicted landmarks to represent the contours.training was carried
out with the adam optimizer [17] with a learning rate of 1 × 10 -3 and with
ample data augmentation (random rotation and translations, brightness and
contrast changes, and gamma corrections). models were trained with early
stopping and the models with best validation loss were retained for testing.
to assess quality of the uncertainty estimates at image and pixel level we
use:correlation. the correlation between image uncertainty and dice was computed
using the absolute value of the pearson correlation score. we obtained image
uncertainty be taking the sum of the uncertainty map and dividing it by the
number of foreground pixels.
. this common uncertainty metric represents the probability if a classifier
(here a segmentation method) of being correct by computing the worst case
difference between its predicted confidence and its actual accuracy
[23].uncertainty error mutual-information. as proposed in [15], uncertainty
error mutual-information measures the degree of overlap between the
unthresholded uncertainty map and the pixel-wise error map.
we computed uncertainty estimates for both pixel-wise segmentation and contour
regression methods to validate the hypothesis that uncertainty prediction is
better suited to per-landmark segmentation than per-pixel segmentation methods.
for a fair comparison, we made sure the segmentation models achieve similar
segmentation performance, with the average dice being .90 ± .02 for camus, .86 ±
.02 for private us., and .94 ± .02 for jsrt.for the pixel-wise segmentations, we
report results of a classical aleatoric uncertainty segmentation method [16] as
well as a test time augmentation (tta) method [29]. for tta, we used the same
augmentations as the ones used during training. we also computed epistemic
uncertainty with mc-dropout [6] for which we selected the best results of 10%,
25%, and 50% dropout rates. the implementation of mc-dropout for regression was
trained with the dsnt layer [21] and mean squared error as a loss function.as
for the landmark prediction, since no uncertainty estimation methods have been
proposed in the literature, we adapted the mc-dropout method to it. we also
report results for our method using univariate, (n 1 ), bivariate, (n 2 ) and
bivariate skew-normal distributions (sn 2 ).the uncertainty maps for tta and
mc-dropout (i.e. those generating multiple samples) were constructed by
computing the pixel-wise entropy of multiple fig. 4. reliability diagrams [22]
for the 3 datasets. for uncertainty (u) bounded by 0 and 1, confidence (c) is
defined as c = 1u [28] forward passes. it was found that doing so for the
aleatoric method produces better results than simply taking the variance. the
uncertainty map for the landmark predictions was obtained with the method
described in sect. 2.2.quantitative results are presented in table 1 and
qualitative results are shown in fig. 3. as can be seen, our uncertainty
estimation method is globally better than the other approaches except for the
correlation score on the camus dataset which is slightly larger for tta.
furthermore, our point-based aleatoric uncertainty better detects regions of
uncertainty consistently, as reflected in the mutual information (mi) metric.
the reliability diagrams in fig. 4 show that our method is systematically better
aligned to perfect calibration (dashed line) for all datasets, which explains
why our method has a lower mce. with the exception of the private cardiac us
dataset, the skewed normal distribution model shows very similar or improved
results for both correlation and mutual information compared to the univariate
and bivariate models. it can be noted, however, that in specific instances, the
asymmetric model performs better on private cardiac us dataset (c.f. column 2
and 3 in fig. 3). this confirms that it is better capturing asymmetric errors
over the region of every contour point.
the results reported before reveal that approaching the problem of segmentation
uncertainty prediction via a regression task, where the uncertainty is expressed
in terms of landmark location, is globally better than via pixel-based
segmentation methods. it also shows that our method (n 1 , n 2 and sn 2 ) is
better than the commonly-used mc-dropout. it can also be said that our method is
more interpretable as is detailed in sect. 2.2 and shown in fig. 3.the choice of
distribution has an impact when considering the shape of the predicted contour.
for instance, structures such as the left ventricle and the myocardium wall in
the ultrasound datasets have large components of their contour oriented along
the vertical direction which allows the univariate and bivariate models to
perform as well, if not better, than the asymmetric model. however, the lungs
and heart in chest x-rays have contours in more directions and therefore the
uncertainty is better modeled with the asymmetric model.furthermore, it has been
demonstrated that skewed uncertainty is more prevalent when tissue separation is
clear, for instance, along the septum border (camus) and along the lung contours
(jsrt). the contrast between the left ventricle and myocardium in the images of
the private cardiac us dataset is small, which explains why the simpler
univariate and bivariate models perform well. this is why on very noisy and
poorly contrasted data, the univariate or the bivariate model might be
preferable to using the asymmetric model.while our method works well on the
tasks presented, it is worth noting that it may not be applicable to all
segmentation problems like tumour segmentation. nevertheless, our approach is
broad enough to cover many applications, especially related to segmentation that
is later used for downstream tasks such as clinical metric estimation. future
work will look to expand this method to more general distributions, including
bi-modal distributions, and combine the aleatoric and epistemic uncertainty to
obtain the full predictive uncertainty.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1_21.
the shortage of labeled data is a significant challenge in medical image
segmentation, as acquiring large amounts of labeled data is expensive and
requires specialized knowledge. this shortage limits the performance of existing
segmentation models. to address this issue, researchers have proposed various
semi-supervised learning (ssl) techniques that incorporate both labeled and
unlabeled data to train models for both natural [2,4,12,13,15,16] and medical
images [10,11,14,[18][19][20][21]. however, most of these methods do not
consider the class imbalance issue, which is common in medical image datasets.
for example, multi-organ segmentation from ct scans requires to segment
esophagus, right adrenal gland, left adrenal gland, etc., where the class ratio
is quite imbalanced; see fig 1(a). as for liver tumor segmentation from ct
scans, usually the ratio for liver and tumor is larger than 16:1.recently, some
researchers proposed class-imbalanced semi-supervised methods [1,10] and
demonstrated substantial advances in medical image segmentation tasks.
concretely, basak et al. [1] introduced a robust class-wise sampling strategy to
address the learning bias by maintaining performance indicators on the fly and
using fuzzy fusion to dynamically obtain the class-wise sampling rates. however,
the proposed indicators can not model the difficulty well, and the benefits may
be overestimated due to the non-representative datasets used (fig. 1(a)). lin et
al. [10] proposed cld to address the data bias by weighting the overall loss
function based on the voxel number of each class. however, this method fails due
to the easily over-fitted cps (cross pseudo supervision) [4] baseline, ignoring
unlabeled data in weight estimation and the fixed class-aware weights.in this
work, we explore the importance of heterogeneity in solving the over-fitting
problem of cps (fig. 2) and propose a novel dhc (dual-debiased heterogeneous
co-training) framework with two distinct dynamic weighting strategies leveraging
both labeled and unlabeled data, to tackle the class imbalance issues and
drawbacks of the cps baseline model. the key idea of heterogeneous co-training
is that individual learners in an ensemble model should be both accurate and
diverse, as stated in the error-ambiguity decomposition [8].to achieve this, we
propose distdw (distribution-aware debiased weighting) and diffdw (diff
iculty-aware debiased weighting) strategies to guide the two sub-models to
tackle different biases, leading to heterogeneous learning directions.
specifically, distdw solves the data bias by calculating the imbalance ratio
with the unlabeled data and forcing the model to focus on extreme minority
classes through careful function design. then, after observing the inconsistency
between the imbalance degrees and the performances (see fig. 1(b)), diffdw is
designed to solve the learning bias. we use the labeled samples and the
corresponding labels to measure the learning difficulty from learning speed and
dice value aspects and slow down the speeds of the easier classes by setting
smaller weights. distdw and diffdw are diverse and have complementary properties
(fig. 1(c)), which satisfies the design ethos of a heterogeneous framework.the
key contributions of our work can be summarized as follows: 1) we first state
the homogeneity issue of cps and improve it with a novel dual-debiased
heterogeneous co-training framework targeting the class imbalance issue; 2) we
propose two novel weighting strategies, distdw and diffdw, which effectively
solve two critical issues of ssl: data and learning biases; 3) we introduce two
public datasets, synapse [9] and amos [7], as new benchmarks for
class-imbalanced semi-supervised medical image segmentation. these datasets
include sufficient classes and significant imbalance ratios (> 500 : 1), making
them ideal for evaluating the effectiveness of class-imbalance-targeted
algorithm designs.
figure 3 shows the overall framework of the proposed dhc framework. dhc
leverages the benefits of combining two diverse and accurate sub-models with two
distinct learning objectives: alleviating data bias and learning bias. to
achieve this, we propose two dynamic loss weighting strategies, distdw
(distributionaware debiased weighting) and diffdw (difficulty-aware debiased
weighting), to guide the training of the two sub-models. distdw and diffdw
demonstrate complementary properties. thus, by incorporating multiple
perspectives and sources of information with distdw and diffdw, the overall
framework reduces over-fitting and enhances the generalization capability.
assume that the whole dataset consists of n l labeled samples {(x l i , y i )}
nl i=1 and n u unlabeled samples {x u i } nu i=1 , where x i ∈ r d×h×w is the
input volume and y i ∈ r k×d×h×w is the ground-truth annotation with k classes
(including background). the two sub-models of dhc complement each other by
minimizing the following objective functions with two diverse and accurate
weighting strategies:where pis the output probability map and ŷ(• y) is the
supervised cross entropy loss function to supervise the output of labeled data,
andis the unsupervised loss function to measure the prediction consistency of
two models by taking the same input volume x i . note that both labeled and
unlabeled data are used to compute the unsupervised loss. finally, we can obtain
the total loss: l total = l s + λl u , we empirically set λ as 0.1 and follow
[10] to use the epoch-dependent gaussian ramp-up strategy to gradually enlarge
the ratio of unsupervised loss. w dif f i and w dist i are the dynamic
class-wise loss weights obtained by the proposed weighting strategies, which
will be introduced next.
to mitigate the data distribution bias, we propose a simple yet efficient
reweighing strategy, distdw. distdw combines the benefits of the simis [3],
which eliminate the weight of the largest majority class, while preserving the
distinctive weights of the minority classes (fig. 4(c)). the proposed strategy
rebalances the learning process by forcing the model to focus more on the
minority classes. specifically, we utilize the class-wise distribution of the
unlabeled pseudo labels p u by counting the number of voxels for each category,
denoted as n k , k = 0, ..., k. we construct the weighting coeffcient for k th
category as follows:where β is the momentum parameter, set to 0.99
experimentally.
after analyzing the proposed distdw, we found that some classes with many
samples present significant learning difficulties. for instance, despite having
the second highest number of voxels, the stomach class has a much lower dice
score than the aorta class, which has only 20% of the voxels of the stomach
(fig. 1(b)). blindly forcing the model to prioritize minority classes may
further exacerbate the learning bias, as some challenging classes may not be
learned to an adequate extent. to alleviate this problem, we design diffdw to
force the model to focus on the most difficult classes (i.e. the classes learned
slower and with worse performances) rather than the minority classes. the
difficulty is modeled in two ways: learning speed and performance. we use
population stability index [6] to measure the learning speed of each class after
the t th iteration:where λ k denotes the dice score of k th class in t th
iteration and = λ k,t -λ k,t-1 . du k,t and dl k,t denote classes not learned
and learned after the t th iteration. i(•) is the indicator function. τ is the
number accumulation iterations and set to 50 empirically. then, we define the
difficulty of k th class after t th iteration as, where is a smoothing item with
minimal value. the classes learned faster have smaller d k,t , the corresponding
weights in the loss function will be smaller to slow down the learn speed. after
several iterations, the training process will be stable, and the difficulties of
all classes defined above will be similar. thus, we also accumulate 1λ k,t for τ
iterations to obtain the reversed dice weight w λ k,t and weight d k,t . in this
case, classes with lower dice scores will have larger weights in the loss
function, which forces the model to pay more attention to these classes. the
overall difficulty-aware weight of k th class is defined as:α is empirically set
to 1 5 in the experiments to alleviate outliers. the difficulty-aware weights
for all classes are
dataset and implementation details. we introduce two new benchmarks on the
synapse [9] and amos [7] datasets for class-imbalanced semi-supervised medical
image segmentation. the synapse dataset has 13 foreground classes, including
spleen (sp), right kidney (rk), left kidney (lk), gallbladder (ga), esophagus
(es), liver(li), stomach(st), aorta (ao), inferior vena cava (ivc), portal &
splenic veins (psv), pancreas (pa), right adrenal gland (rag), left adrenal
gland (lag) with one background and 30 axial contrast-enhanced abdominal ct
scans. we randomly split them as 20, 4 and 6 scans for training, validation, and
testing, respectively. compared with synapse, the amos dataset excludes psv but
adds three new classes: duodenum(du), bladder(bl) and prostate/uterus(p/u). 360
scans are divided into 216, 24 and 120 scans for training, validation, and
testing. we ran experiments on synapse three times with different seeds to
eliminate the effect of randomness due to the limited samples. more training
details are in the supplementary material.comparison with state-of-the-art
methods. we compare our method with several state-of-the-art semi-supervised
segmentation methods [4,11,19,21]. moreover, simply extending the
state-of-the-art semi-supervised classification methods [2,3,5,15,17], including
class-imbalanced designs [3,5,17] to segmentation, is a straightforward solution
to our task. therefore, we extend these methods to segmentation with cps as the
baseline. as shown in table 1 and2, the general semi-supervised methods which do
not consider the class imbalance problem fail to capture effective features of
the minority classes and lead to terrible performances (colored with red). the
methods considered the class imbalance problem have better results on some
smaller minority classes such as gallbladder, portal & splenic veins and etc.
however, they still fail in some minority classes (es, rag, and lag) since this
task is highly imbalanced. our proposed dhc outperforms these methods,
especially in those classes with very few samples. note that our method performs
better than the fully-supervised method for the rag segmentation. furthermore,
our method outperforms sota methods on synapse by larger margins than the amos
dataset, demonstrating the more prominent stability and effectiveness of the
proposed dhc framework in scenarios with a severe lack of data. visualization
results in fig. 5 show our method performs better on minority classes which are
pointed with green arrows. more results on datasets with different labeled
ratios can be found in the supplementary material. ablation study. to validate
the effectiveness of the proposed dhc framework and the two learning strategies,
distdw and diffdw, we conduct ablation experiments, as shown in table 3. distdw
('distdw-distdw') alleviates the bias of baseline on majority classes and thus
segments the minority classes (ra, la, es, etc.) very well. however, it has
unsatisfactory results on the spleen and stomach, which are difficult classes
but down-weighted due to the larger voxel numbers. diffdw ('diffdw-diffdw')
shows complementary results with distdw, it has better results on difficult
classes (e.g. , stomach since it is hollow inside). when combining these two
weighting strategies in a heterogeneous cotraining way ('diffdw-distdw', namely
dhc), the dice score has 5.12%, 5.6% and 13.78% increase compared with distdw,
diffdw, and the cps baseline. these results highlight the efficacy of
incorporating heterogeneous information in avoiding over-fitting and enhancing
the performance of the cps baseline.
this work proposes a novel dual-debiased heterogeneous co-training framework for
class-imbalanced semi-supervised segmentation. we are the first to state the
homogeneity issue of cps and solve it intuitively in a heterogeneous way. to
achieve it, we propose two diverse and accurate weighting strategies: distdw for
eliminating the data bias of majority classes and diffdw for eliminating the
learning bias of well-performed classes. by combining the complementary
properties of distdw and diffdw, the overall framework can learn both the
minority classes and the difficult classes well in a balanced way. extensive
experiments show that the proposed framework brings significant improvements
over the baseline and outperforms previous ssl methods considerably.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1 56.
primary liver cancer is one of the most common and deadly cancer diseases in the
world, and liver resection is a highly effective treatment [11,14]. the couinaud
segmentation [7] based on ct images divides the liver into eight functionally
independent regions, which intuitively display the positional relationship
between couinaud segments and intrahepatic lesions, and helps surgeons for make
surgical planning [3,13]. in clinics, couinaud segments obtained from manual
annotation are tedious and time-consuming, based on the vasculature used as
rough guide (fig. 1). thus, designing an automatic method to accurately segment
couinaud segments from ct images is greatly demanded and has attracted
tremendous research attention.however, automatic and accurate couinaud
segmentation from ct images is a challenging task. since it is defined based on
the anatomical structure of live vessels, even no intensity contrast (fig.
1.(b)) can be observed between different couinaud segments, and the uncertainty
of boundary (fig. 1.(d)) often greatly affect the segmentation performance.
previous works [4,8,15,19] mainly rely on handcrafted features or atlas-based
models, and often fail to robustly handle those regions with limited features,
such as the boundary between adjacent couinaud segments. recently, with the
advancement of deep learning [5,10,18], many cnn-based algorithms perform
supervised training through pixel-level couinaud annotations to automatically
obtain segmentation results [1,9,21]. unfortunately, the cnn models treat all
voxel-wise features in the ct image equally, cannot effectively capture key
anatomical regions useful for couinaud segmentation. in addition, all these
methods deal with the 3d voxels of the liver directly without considering the
spatial relationship of the different couinaud segments, even if this
relationship is very important in couinaud segmentation. it can supplement the
cnn-based method and improve the segmentation performance in regions without
intensity contrast.in this paper, to tackle the aforementioned challenges, we
propose a pointvoxel fusion framework that represents the liver ct in continuous
points to better learn the spatial structure, while performing the convolutions
in voxels to obtain the complementary semantic information of the couinaud
segments. specifically, the liver mask and vessel attention maps are first
extracted from the ct images, which allows us to randomly sample points embedded
with vessel structure prior in the liver space and voxelize them into a voxel
grid. subsequently, points and voxels pass through two branches to extract
features. the point-based branch extracts the fine-grained feature of
independent points and explores spatial topological relations. the voxel-based
branch is composed of a series of convolutions to learn semantic features,
followed by de-voxelization to convert them back to points. through the
operation of voxelization and devoxelization at different resolutions, the
features extracted by these two branches can achieve multi-scale fusion on
point-based representation, and finally output the couinaud segment category of
each point. extensive experiments on two publicly available datasets named
3dircadb [20] and lits [2] demonstrate that our proposed framework achieves
state-of-the-art (sota) performance, outperforming cutting-edge methods
quantitatively and qualitatively.
the overview of our framework to segment couinaud segments from ct images is
shown in fig. 2, including the liver segmentation, vessel attention map
generation, point data sampling and multi-scale point-voxel fusion network.
liver segmentation is a fundamental step in couinaud segmentation task.
considering that the liver is large and easy to identify in the abdominal
organs, we extracted the liver mask through a trained 3d unet [6]. different
from liver segmentation, since we aim to use the vessel structure as a rough
guide to improving the performance of the couinaud segmentation, we employ
another 3d unet [6] to generate the vessel attention map more easily.
specifically, given a 3d ct image containing only the area covered by the liver
mask (l), the 3d unet [6] output a binary vessel mask (m ). a morphological
dilation is then used to enclose more vessel pixels in the m -covered area,
generating a vessel attention map (m ). we employ the bce loss to supervise the
learning process.
based on the above work, we first use the m and the l to sample get point data,
which can convert into a voxel grid through re-voxelization. the converted voxel
grid embeds the vessel prior and also dilutes the liver parenchyma information.
inspired by [12], a novel multi-scale point-voxel fusion network then is
proposed to simultaneously process point and voxel data through point-based
branch and voxel-based branch, respectively, aiming to accurately perform
couinaud segmentation. the details of this part of our method are described
below.
map. in order to obtain the topological relationship between couinaud segments,
a direct strategy is to sample the coordinate point data with 3d spatial
information from liver ct and perform point-wise classification. hence, we first
convert the image coordinate points i = i 1 , i 2 , ..., i t , i t ∈ r 3 in
liver ct into the world coordinate points p = p 1 , p 2 , ..., p t , p t ∈ r 3
:where spacing represents the voxel spacing in the ct images, direction
represents the direction of the scan, and origin represents the world
coordinates of the image origin. based on equation(1), we obtain the world
coordinate p t = (x t , y t , z t ) corresponding to each point i t in the liver
space. however, directly feeding the transformed point data as input into the
point-based branch undoubtedly ignores the vessel structure, which is crucial
for couinaud segmentation. where r denotes the rounding integer function. based
on this, we achieve arbitrary resolution sampling in the continuous space
covered by the m .
where r denotes the voxel resolution, i [•] is the binary indicator of whether
the coordinate pt belongs to the voxel grid (u, v, w), f t,c denotes the cth
channel feature corresponding to pt , and n u,v,w is the number of points that
fall in that voxel grid. note that the re-voxelization in the model is used
three times (as shown in fig. 2), and the f t,c in the first operation is the
coordinate and intensity, with c = 4. moreover, due to the previously mentioned
point sampling strategy, the converted voxel grid also inherits the vessel
structure from the point data and dilutes the unimportant information in the ct
images.multi-scale point-voxel fusion network. intuitively, due to the image
intensity between different couinaud segments being similar, the voxel-based cnn
model is difficult to achieve good segmentation performance. we propose a
multi-scale point-voxel fusion network for accurate couinaud segmentation, take
advantage of the topological relationship of coordinate points in 3d space, and
leverage the semantic information of voxel grids. as shown in fig. 2, our method
has two branches: point-based and voxel-based. the features extracted by these
two branches on multiple scales are fused to provide more accurate and robust
couinaud segmentation performance. specifically, in the point-based branch, the
input point data {(p t , f t )} passes through an mlp, denoted as e p , which
aims to extract fine-grained features with topological relationships. at the
same time, the voxel grid {v u,v,w } passes the voxel branch based on
convolution, denoted as e v , which can aggregate the features of surrounding
points and learn the semantic information in the liver 3d space. we re-transform
the features extracted from the voxel-based branch to point representation
through trilinear interpolation, to combine them with fine-grained features
extracted from the point-based branch, which provide complementary
information:where the superscript 1 of (p t , f 1 t ) indicates that the fused
point data and corresponding features f 1 t are obtained after the first round
of point-voxel operation. then, the point data (p t , f 1 t ) is voxelized again
and extracted point features and voxel features through two branches. note that
the resolution of the voxel grid in this round is reduced to half of the
previous round. after three rounds of pointvoxel operations, we concatenate the
original point feature f t and the features f 1 t , f 2 t , f 3 t with multiple
scales, then send them into a point-wise decoder d, parameterized by a fully
connected network, to predict the corresponding couinaud segment category:where
{0, 1, ..., 7} denotes the couinaud segmentation category predicted by our model
for the point p t . we employ the bce loss and the dice loss to supervise the
learning process. more method details are shown in the supplementary materials.
we evaluated the proposed framework on two publicly available datasets, 3dircadb
[20] and lits [2]. the 3dircadb dataset [20] contains 20 ct images with spacing
ranging from 0.56 mm to 0.87 mm, and slice thickness ranging from 1 mm to 4 mm
with liver and liver vessel segmentation labels. the lits dataset [2] consists
of 200 ct images, with a spacing of 0.56 mm to 1.0 mm and slice thickness of
0.45 mm to 6.0 mm, and has liver and liver tumour labels, but without vessels.
we annotated the 20 subjects of the 3dircadb dataset [20] with the couinaud
segments and randomly divided 10 subjects for training and another 10 subjects
for testing. for lits dataset [2], we observed the vessel structure on ct
images, annotated the couinaud segments of 131 subjects, and randomly selected
66 subjects for training and 65 for testing.we have used three widely used
metrics, i.e., accuracy (acc, in %), dice similarity metric (dice, in %), and
average surface distance (asd, in mm) to evaluate the performance of the
couinaud segmentation.
the proposed framework was implemented on an rtx8000 gpu using pytorch. based on
the liver mask has been extracted, we train a 3d unet [6] on the 3diradb dataset
[20] to generate the vessel attention map of two datasets. then, we sample t =
20, 000 points in each epoch to train our proposed multi-scale point-voxel
fusion network each epoch. we perform scaling within the range of 0.9 to 1.1,
arbitrary axis flipping, and rotation in the range of 0 to 5 • c on the input
point data as an augmentation strategy. besides, we use the stochastic gradient
descent optimizer with a learning rate of 0.01, which is reduced to 0.9 times
for every 50 epochs of training. all our experiments were trained 400 epochs,
with a random seed was 2023, and then we used the model with the best
performance on the training set to testing.
we compare our framework with several sota approaches, including voxelbased 3d
unet [6], point-based pointnet2plus [16], and the methods of jia et al. [9]. the
method of jia et al. is a 2d unet [17] with dual attention to focus on the
boundary of the couinaud segments and is specifically used for the couinaud
segmentation task. we use pytorch to implement this model and maintain the same
implementation details as other methods. the quantitative and qualitative
comparisons are shown in table 1 and fig. 3, respectively.quantitative
comparison. table 1 summarizes the overall comparison results under three
metrics. by comparing the first two rows, we can see that point-net2plus [16]
and 3d unet [6] have achieved close performance in the lits dataset [2], which
demonstrates the potential of the point-based methods in the couinaud
segmentation task. in addition, the third row shows that jia et al.'s [9] 2d
unet [17] as the backbone method performs worst on all metrics, further
demonstrating the importance of spatial relationships. finally, our proposed
point-voxel fusion segmentation framework achieves the best performance.
especially on the 3diradb dataset [20] with only 10 training subjects, the acc
and dice achieved by our method exceed pointnet2plus [16] and 3d unet [6] by
nearly 10 points, and the asd is also greatly reduced, which demonstrates the
effectiveness of the combining point-based and voxel-based methods.qualitative
comparison. to further evaluate the effectiveness of our method, we also provide
qualitative results, as shown in fig. 3. the first two rows show that the vessel
structure is used as the boundary guidance for couinaud segmentation, but
voxel-based 3d unet [6] fails to accurately capture this key structural
relationship, resulting in inaccurate boundary segmentation. note that, it can
be seen that our method can learn the boundary guidance provided by the portal
vein (the last two rows), to deal with the uncertain boundary more
robustly.besides, compared with the 3d view, it is obvious that the voxel-based
cnn methods are easy to pay attention to the local area and produce a large area
of error segmentation, so the reconstructed surface is uneven. the point-based
method obtains smooth 3d visualization results, but it is more likely to cause
segmentation blur in boundary areas with high uncertainty. our method combines
the advantages of point-based and voxel-based methods, and remedies their
respective defects, resulting in smooth and accurate couinaud segmentation.
to further study the effectiveness of our proposed framework, we compared two
ablation experiments: 1) random sampling of t points in the liver space,
with-out considering the guidance of vascular structure, and 2) considering only
the voxel-based branch, where the couinaud segments mask is output by a cnn
decoder. figure 4 shows the ablation experimental results obtained on all the
couinaud segments of two datasets, under the dice and the asd metrics. it can be
seen that our full method is significantly better than the cnn branch joint
decoder method on both metrics of two datasets, which demonstrates the
performance gain by the combined point-based branch. in addition, compared with
the strategy of random sampling, our full-method reduces the average asd by more
than 2mm on eight couinaud segments. this is because to the vessel
structure-guided sampling strategy can increase the important data access
between the boundaries of the couinaud segments. besides, perturbations are
applied to the points in the coverage area of the vessel attention map, so that
our full method performs arbitrary point sampling in the continuous space near
the vessel, and is encouraged to implicitly learn the couinaud boundary in
countless points.
we propose a multi-scale point-voxel fusion framework for accurate couinaud
segmentation that takes advantage of the topological relationship of coordinate
points in 3d space, and leverages the semantic information of voxel grids.
besides, the point sampling strategy embedded with vascular prior increases the
access of our method to important regions, and also improves the segmentation
accuracy and robustness in uncertain boundaries. experimental results
demonstrate the effectiveness of our proposed method against other cutting-edge
methods, showing its potential to be applied in the preoperative application of
liver surgery.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1 45.
segmentation of the pulmonary vessels is the foundation for the clinical
diagnosis of pulmonary vascular diseases such as pulmonary embolism (pe),
pulmonary hypertension (ph) and lung cancer [9]. accurate vascular quantitative
analysis is crucial for physicians to study and apply in treatment planning, as
well as making surgical plans. although contrast-enhanced ct images have better
contrast for pulmonary vessels compared to non-contrast ct images, the
acquisition of contrast-enhanced ct images needs to inject a certain amount of
contrast agent to the patients. some patients have concerns about the possible
risk of contrast media [2]. at the same time, non-contrast ct is the most widely
used imaging modality for visualizing, diagnosing, and treating various lung
diseases.in the literature, several conventional methods [5,16] have been
proposed for the segmentation of pulmonary vessels in contrast-enhanced ct
images. most of these methods employed manual features to segment peripheral
intrapulmonary vessels. in recent years, deep learning-based methods have
emerged as promising approaches to solving challenging medical image analysis
problems and have demonstrated exciting performance in segmenting various
biological structures [10,11,15,17]. however, for vessel segmentation, the
widely used models, such as u-net and its variants, limit their segmentation
accuracy on low-contrast small vessels due to the loss of detailed information
caused by the multiple downsampling operations. accordingly, zhou et al. [17]
proposed a nested structure unet++ to redesign the skip connections for
aggregating multi-scale features and improve the segmentation quality of
varying-size objects. also, some recent methods combine convolutional neural
networks (cnns) with transformer or non-local block to address this issue
[3,6,13,18]. wang et al. [13] replaced the original skip connections with
transformer blocks to better merge the multi-scale contextual information. for
this task, cui et al. [1] also proposed an orthogonal fused u-net++ for
pulmonary peripheral vessel segmentation. however, all these methods ignored the
significant variability in hu values of pulmonary vessels at different
regions.to summarize, there exist several challenges for pulmonary vessel
segmentation in non-contrast ct images: (1) the contrast between pulmonary
vessels and background voxels is extremely low (fig. 1(c)); (2) pulmonary
vessels have a complex structure and significant variability in vessel
appearance, with different scales in different areas. the central extrapulmonary
vessels near the heart have a large irregular ball-like shape, while the shape
of the intrapulmonary vessels is delicate and tubular-like (fig. 1(a) and (b)).
vessels become thinner as they get closer to the peripheral lung; (3) hu values
of vessels in different regions vary significantly, ranging from -850 hu to 100
hu. normally, central extrapulmonary vessels have higher hu values than
peripheral intrapulmonary vessels. thus, we set different ranges of hu values to
better visualize the vessels in fig. 1(d) and(e).to address the above
challenges, we propose a h ierarchical e nhancement n etwork (henet) for
pulmonary vessel segmentation in non-contrast ct images by enhancing the
representation of vessels at both image-and feature-level. for the input ct
images, we propose an auto contrast enhancement (ace) module to automatically
adjust the range of hu values in different areas of ct images. it mimics the
radiologist in setting the window level (wl) and window width (ww) to better
enhance vessels from surrounding voxels, as shown in fig. 1(d) and (e). also, we
propose a cross-scale non-local block (csnb) to replace the skip connections in
vanilla u-net [11] structure for the aggregation of multi-scale feature maps. it
helps to form local-to-global information connections to enhance vessel
information at the feature-level, and address the complex scale variations of
pulmonary vessels.
the overview of the proposed method is illustrated in fig. 2. our proposed
hierarchical enhancement network (henet) consists of two main modules: (1) auto
contrast enhancement (ace) module, and (2) cross-scale non-local block (csnb) as
the skip connection bridge between encoders and decoders. in this section, we
present the design of these proposed modules. first, the ace module is developed
to enhance the contrast of vessels in the original ct images for the following
vessel segmentation network. after that, we introduce the csnb module to make
the network pay more attention to multi-scale vessel information in the latent
feature space.
in non-contrast ct images, the contrast between pulmonary vessels and the
surrounding voxels is pretty low. also, the hu values of vessels in different
regions vary significantly as ranging from -850 hu to 100 hu. normally,
radiologists have to manually set the suitable window level (wl) and window
width (ww) for different regions in images to enhance vessels according to the
hu value range of surrounding voxels, just as different settings to better
visualize the extrapulmonary and intrapulmonary vessels (fig. 1(d) and(e)).
instead of a fixed wl/ww as employed in existing methods, we address it by
adding an ace module to automatically enhance the contrast of vessels.the ace
module leverages convolution operations to generate dynamic wl and ww for the
input ct images according to the hu values covered by the kernel. here we set
the kernel size as 15 × 15 × 15. first, we perform min-max normalization to
linearly transform the hu values of the original image x to the range (-1, 1).
then, it passes through a convolution layer to be downsampled into half-size of
the original shape, which is utilized to derive the following shift map and
scale map. here, the learned shift map and scale map act as the window level and
window width settings of the "width/level" scaling in ct images. we let values
in the shift map be the wl, so the tanh activation function is used to limit
them within (-1, 1). the values in the scale map denote the half of ww, and we
perform the sigmoid activation function to get the range (0, 1). it matches the
requirement of the positive integer for ww. after that, the shift map and scale
map will be upsampled by the nearest neighbor interpolation into the original
size of the input x. this operation can generate identical shift and scale
values with each 2 × 2 × 2 window, avoiding sharp contrast changes in the
neighboring voxels. the upsampled shift map and scale map are denoted as m shif
t and m scale , respectively, and then the contrast enhancement image x ace can
be generated through:it can be observed that the intensity values of input x are
re-centered and re-scaled by m shif t and m scale (fig. 3(c)). the clip
operation (clip(•)) truncates the final output into the range [-1, 1], which
sets the intensity value above 1 to 1, and below -1 to -1. in our experiments,
we find that a large kernel size for learning of m shif t and m scale could
deliver better performance, which can capture more information on hu values from
the ct images.
there are studies [14,18] showing that non-local operations could capture
longrange dependency to improve network performance. to segment pulmonary
vessels with significant variability in scale and shape, we design a cross-scale
nonlocal block (csnb) to fuse the local features extracted by cnn backbone from
different scales, and to accentuate the cross-scale dependency to address the
complex scale variations of pulmonary vessels.inspired by [18], our csnb
incorporates 6 modified asymmetric non-local blocks (anbs), which integrate
pyramid sampling modules into the non-local blocks to largely reduce the
computation and memory consumption. as illustrated in fig. 2, the csnb works as
the information bridge between encoders and decoders while also ensuring the
feasibility of experiments involving large 3d data. specifically, the i 1 ∼ i 4
are the inputs of csnb, and o 1 ∼ o 4 are the outputs. within the csnb, there
are three levels of modified anbs, we denote them as anb-h (anb-head) and anb-p
(anb-post). for the two anbs in each level, the anb-h has two input feature
maps, and the lower-level feature maps (denoted as f l ) contain more
fine-grained information than the higherlevel feature maps (denoted as f h ). we
use f h to generate embedding q, while embeddings k and v are derived from f l .
by doing this, csnb can enhance the dependencies of cross-scale features. the
specific computation of anb proceeds as follows: first, three 1×1×1 convolutions
(denoted as conv(•)) are applied to transform f h and f l into different
embeddings q, k, and v ; then, spatial pyramid pooling operations (denoted as p
(•)) are implemented on k and v . the calculation can be expressed as:next,
these three embeddings are reshaped towhere n represents the total count of the
spatial locations, i.e., n = d × h × w and s is equivalent to the concatenated
output size after the spatial pyramid pooling, i.e., setting s = 648. the
similarity matrix between q and k p is obtained through matrix multiplication
and normalized by softmax function to get a unified similarity matrix. the
attention output is acquired by:where the output o ∈ r n × ĉ . the final output
of anb is given as:where the final convolution is used as a weighting parameter
to adjust the importance of this non-local operation and recover the channel
dimension to c h . anb-p has the same structure as anb-h, but the inputs f h and
f l here are the same, which is the output of anb-h at the same level. the anb-p
is developed to further enhance the intra-scale connection of the fused features
in different regions, which is equivalent to the self-attention mechanism. note
that, o 1 is directly skipped from i 1 . for the first level of csnb, the input
f l of anb-h is the i 1 , while for the other levels, the input f l is the
output of anb-p of the above level. that is, each level of csnb can fuse the
feature maps from its corresponding level with the fused feature maps from all
of the above lower levels. thereby, the response of multi-scale vessels can be
enhanced.
dataset and evaluation metrics. we use a total of 160 non-contrast ct images
with the inplane size of 512 × 512, where the slice number varies from 217 to
622. the axial slices have the same spacing ranging from 0.58 to 0.86 mm, and
the slice thickness varies from 0.7 to 1.0 mm. the annotations of pulmonary
vessels are semi-automatically segmented in 3d by two radiologists using the 3d
slicer software. this study is approved by the ethical committee of west china
hospital of sichuan university, china. these cases are randomly split into a
training set (120 scans) and a testing set (40 scans). the quantitative results
are reported by dice similarity coefficient (dice), mean intersection over union
(miou), false positive rate (fpr), average surface distance (asd), and hausdorff
distance (hd). for the significance test, we use the paired t-test.
implementation details. our experiments are implemented using pytorch framework
and trained using a single nvidia-a100 gpu. we pre-process the data by
truncating the hu value to the range of [-900, 900] and then linearly scaling it
to [-1, 1]. in the training stage, we randomly crop sub-volumes with the size of
192 × 192 × 64 near the lung field, and then the cropped sub-volumes are
augmented by random horizontal and vertical flipping with a probability of 0.5.
in the testing phase, we perform the sliding window average prediction with
strides of (64, 64, 32) to cover the entire ct images. for a fair comparison, we
use the same hyper-parameter settings and dice similarity coefficient loss
across all experiments. in particular, we use the same data augmentation, no
post-processing scheme, adam optimizer with an initial learning rate of 10 -4 ,
and train for 800 epochs with a batch size of 4. in our experiments, we use a
two-step optimization strategy: 1) first, train the ace module with the basic
u-net; 2) integrate the trained ace module and a new csnb module into the u-net,
and fix the parameters of ace module when training this network.ablation study.
we conduct ablation studies to validate the efficacy of the proposed modules in
our henet by combining them with the baseline u-net [11]. the quantitative
results are summarized in table 1. compared to the baseline, both ace module and
csnb lead to better performance. with the two components, our henet has
significant improvements over baseline on all the metrics. for regional measures
dice and miou, it improves by 3.02% and 2.32% respectively. for surface-aware
measures asd and hd, it improves by 35% and 53%, respectively. results
demonstrate effectiveness of the proposed ace module and csnb.to validate the
efficacy of ace module, we show the qualitative result in fig. 3. as shown in
fig. 3(c), the ace module effectively enhances the contrast of pulmonary vessels
at the image-level. we also visualize the summation of feature maps from the
final decoder in fig. 3(d) and (e). as can be seen, the baseline u-net can focus
on local features for certain intrapulmonary vessels, but it fails to activate
complete vascular regions of multiple scales. comparison with state-of-the-art
methods. since we adopt u-net as our baseline, we compare our method with
several state-of-the-art encoder-decoder cnns and the transformer-based method
vt-unet [8] within a considerable computational complexity. we also compare our
method with state-of-the-art deep learning-based vessel segmentation methods,
including cldice [12], cs 2 -net [7], and of-net [1]. the quantitative and
qualitative results are presented in table 2 and fig. 4, respectively. as shown
in table 2, our method outperforms the competing methods and achieves the best
dice, miou, asd, and hd. cs 2 -net performs best on fpr, but our method has
better dice and miou than cs 2 -net (increasing 0.56% and 0.43%, respectively),
indicating the under-segmentation of cs 2 -net and more vessels being correctly
segmented by our method. in the first row of qualitative results (fig. 4), the
competing methods can produce satisfactory results for the overall structure but
generate many false positives. furthermore, due to low contrast between small
intrapulmonary vessels and the surrounding voxels, results of competing methods
exist many discontinuities (the second row), while our method obtains more
connective segmentation for these small vessels. also, for the segmentation of
large extrapulmonary vessels (the last row), our method can produce more
accurate results. note that, although cldice can also yield connective results
for small vessels, their fpr is 0.16% higher than ours. this implies that cldice
tends to over-segment vessels, and it cannot obtain precise segmentation for the
large extrapulmonary vessels. results proved the superiority of our method.
in this paper, we have proposed a hierarchical enhancement network to enhance
the representation of vessels at both image-and feature-level for pulmonary
vessel segmentation first time in non-contrast ct images. in the proposed henet,
an auto contrast enhancement module is designed to enhance vessels in different
regions of the input ct. and the cross-scale non-local block is further designed
as the information bridge between encoders and decoders, to enhance the ability
to capture and integrate vascular features of multiple scales. experimental
results show that our method outperforms the competing methods and demonstrates
effectiveness of the proposed ace module and csnb. at the same time, it can be
observed that the learning of m shif t and m scale is only supervised by the
final segmentation loss. one of our future research directions is to develop
explicit constraints for the ace module to better re-center and re-scale the ct
images.
± 2.29 0.72 ± 0.41 9.43 ± 10.91 0.27 ± 0.14
. this work was supported in part by national key research and development
program of china (2021zd0111100), national natural science foundation of china
(62131015), and science and technology commission of shanghai municipality
(stcsm) (21010502600).
image segmentation is a fundamental task in medical image analysis. one of the
key design choices in many segmentation pipelines that are based on neural
networks lies in the selection of the loss function. in fact, the choice of loss
function goes hand in hand with the metrics chosen to assess the quality of the
predicted segmentation [46]. the intersection-over-union (iou) and the dice
score are commonly used metrics because they reflect both size and localization
agreement, and they are more in line with perceptual quality compared to, e.g.,
pixel-wise accuracy [9,27]. consequently, directly optimizing the iou or the
dice score using differentiable surrogates as (a part of) the loss function has
become prevalent in semantic segmentation [2,9,20,24,47]. in medical imaging in
particular, the dice score and the soft dice loss (sdl) [30,42] have become the
standard practice, and some reasons behind its superior functioning have been
uncovered and further optimizations have been explored [3,9,45].another
mechanism to further improve the predicted segmentation that has gained
significant interest in recent years, is the use of soft labels during training.
soft labels can be the result of data augmentation techniques such as label
smoothing (ls) [21,43] and are integral to regularization methods such as
knowledge distillation (kd) [17,36]. their role is to provide additional
regularization so as to make the model less prone to overfitting [17,43] and to
combat overconfidence [14], e.g., providing superior model calibration [31]. in
medical imaging, soft labels emerge not only from ls or kd, but are also present
inherently due to considerable intra-and inter-rater variability. for example,
multiple annotators often disagree on organ and lesion boundaries, and one can
average their annotations to obtain soft label maps [12,23,25,41].this work
investigates how the medical imaging community can combine the use of sdl with
soft labels to reach a state of synergy. while the original sdl surrogate was
posed as a relaxed form of the dice score, naively inputting soft labels to sdl
is possible (e.g. in open-source segmentation libraries [6,19,20,51]), but it
tends to push predictions towards 0-1 outputs rather than make them resemble the
soft labels [3,32,47]. consequently, the use of sdl when dealing with soft
labels might not align with a user's expectations, with potential adverse
effects on the dice score, model calibration and volume estimation [3].motivated
by this observation, we first (in sect. 2) propose two probabilistic extensions
of sdl, namely, dice semimetric losses (dmls). these losses satisfy the
conditions of a semimetric and are fully compatible with soft labels. in a
standard setting with hard labels, dmls are identical to sdl and can safely
replace sdl in existing implementations. secondly (in sect. 3), we perform
extensive experiments on the public qubiq, lits and kits benchmarks to
empirically confirm the potential synergy of dmls with soft labels (e.g.
averaging, ls, kd) over hard labels (e.g. majority voting, random selection).
we adopt the notation from [47]. in particular, we denote the predicted
segmentation as ẋ ∈ {1, ..., c} p and the ground-truth segmentation as ẏ ∈ {1,
..., c} p , where c is the number of classes and p the number of pixels. for a
class c, we define the set of predictions as x c = { ẋ = c}, the set of
ground-truth as y c = { ẏ = c}, the union as u c = x c ∪y c , the intersection
as v c = x c ∩y c , the symmetric difference (i.e., the set of mispredictions)
as x c i the cardinality of the relevant set. moreover, when the context is
clear, we will drop the superscript c.
if we want to optimize the dice score, hence, minimize the dice loss δ dice = 1
-dice in a continuous setting, we need to extend δ dice with δ dice such that it
can take any predicted segmentation x ∈ [0, 1] p as input. hereinafter, when
there is no ambiguity, we will use x and x interchangeably.the soft dice loss
(sdl) [42] extends δ dice by realizing that when x, y ∈ {0, 1} p , |v| = x, y ,
|x| = x 1 and |y| = y 1 . therefore, sdl replaces the set notation with vector
functions:the soft jaccard loss (sjl) [33,37] can be defined in a similar way:a
major limitation of loss functions based on l 1 relaxations, including sdl, sjl,
the soft tversky loss [39] and the focal tversky loss [1], as well as those
relying on the lovasz extension, such as the lovasz hinge loss [49], the
lovasz-softmax loss [2] and the pixiou loss [50], is that they cannot handle
soft labels [47]. that is, when y is also in [0, 1] p . in particular, both sdl
and sjl do not reach their minimum at x = y, but instead they drive x towards
the vertices {0, 1} p [3,32,47]. take for example y = 0.5; it is straightforward
to verify that sdl achieves its minimum at x = 1, which is clearly
erroneous.loss functions that utilize l 2 relaxations [9,30] do not exhibit this
problem [47], but they are less commonly employed in practice and are shown to
be inferior to their l 1 counterparts [9,47]. to address this, wang and blaschko
[47] proposed two variants of sjl termed as jaccard metric losses (jmls). these
two variants, δ jml,1 andjmls are shown to be a metric on [0, 1] p , according
to the definition below.note that reflexivity and positivity jointly imply x = y
⇔ f (x, y) = 0, hence, a loss function that satisfies these conditions will be
compatible with soft labels.
we focus here on the dice loss. for the derivation of the tversky loss and the
focal tversky loss, please refer to our full paper on arxiv. since dice
. there exist several alternatives to define δ iou , but not all of them are
feasible, e.g., sjl. generally, it is easy to verify the following
proposition:proposition 1. δ dice satisfies reflexivity and positivity iff δ iou
does.among the definitions of δ iou , wang and blaschko [47] found only two
candidates as defined in eq. ( 3) satisfy reflexivity and positivity. following
proposition 1, we transform these two iou losses and define dice semimetric
losses (dmls)δ dice that is defined over integers does not satisfy the triangle
inequality [11], which is shown to be helpful in kd [47]. nonetheless, we can
consider a weaker form of the triangle inequality:(functions that satisfy the
relaxed triangle inequality for some fixed scalar ρ and conditions (i)-(iii) of
a metric are called semimetrics. δ dice is a semimetric on {0, 1} p [11]. δ
dml,1 and δ dml,2 , which extend δ dice to [0, 1] p , remain semimetrics in the
continuous space:theorem 1. δ dml,1 and δ dml,2 are semimetrics on [0, 1] p .the
proof can be found in appendix a. moreover, dmls have properties that are
similar to jmls and they are presented as follows:the proofs are similar to
those given in [47]. importantly, theorem 2 indicates that we can safely
substitute the existing implementation of sdl with dmls and no change will be
incurred, as they are identical when only hard labels are presented.
in this section, we provide empirical evidence of the benefits of using soft
labels. in particular, using qubiq [29], which contains multi-rater information,
we show that models trained with averaged annotation maps can significantly
surpass those trained with majority votes and random selections. leveraging lits
[4] and kits [16], we illustrate the synergistic effects of integrating ls and
kd with dmls.
qubiq is a recent challenge held at miccai 2020 and 2021, specifically designed
to evaluate the inter-rater variability in medical imaging. following [23,41],
we use qubiq 2020, which contains 7 segmentation tasks in 4 different ct and mr
datasets: prostate (55 cases, 2 tasks, 6 raters), brain growth (39 cases, 1
task, 7 raters), brain tumor (32 cases, 3 tasks, 3 raters), and kidney (24
cases, 1 task, 3 raters). for each dataset, we calculate the average dice score
between each rater and the majority votes in table 1. in some datasets, such as
brain tumor t2, the inter-rater disagreement can be quite substantial. in line
with [23], we resize all images to 256 × 256. lits contains 201 high-quality ct
scans of liver tumors. out of these, 131 cases are designated for training and
70 for testing. as the ground-truth labels for the test set are not publicly
accessible, we only use the training set. following [36], all images are resized
to 512×512 and the hu values of ct images are windowed to the range of [-60,
140]. kits includes 210 annotated ct scans of kidney tumors from different
patients. in accordance with [36], all images are resized to 512 × 512 and the
hu values of ct images are windowed to the range of [-200, 300].
we adopt a variety of backbones including resnet50/18 [15], efficientnetb0 [44]
and mobilenetv2 [40]. all these models that have been pretrained on ima-genet
[7] are provided by timm library [48]. we consider both unet [38] and deeplabv3+
[5] as the segmentation method.we train the models using sgd with an initial
learning rate of 0.01, momentum of 0.9, and weight decay of 0.0005. the learning
rate is decayed in a poly policy with an exponent of 0.9. the batch size is set
to 8 and the number of epochs is 150 for qubiq, 60 for both lits and kits. we
leverage a mixture of ce and dmls weighted by 0.25 and 0.75, respectively.
unless otherwise specified, we use δ dml,1 by default.in this work, we are
mainly interested in how models can benefit from the use of soft labels. the
superiority of sdl over ce has been well established in the medical imaging
community [9,20], and our preliminary experiments also confirm this, as shown in
table 5 (appendix c). therefore, we do not include any further comparison with
ce in this paper.
we report both the dice score and the expected calibration error (ece) [14]. for
qubiq experiments, we additionally present the binarized dice score (bdice),
which is the official evaluation metrics used in the qubiq challenge. to compute
bdice, both predictions and soft labels are thresholded at different probability
levels (0.1, 0.2, ..., 0.8, 0.9). we then compute the dice score at each level
and average these scores with all thresholds.for all experiments, we conduct
5-fold cross validation, making sure that each case is presented in exactly one
validation set, and report the mean values in the aggregated validation set. we
perform statistical tests according to the procedure detailed in [9] and
highlight results that are significantly superior (with a significance level of
0.05) in red.
in table 2, we compare different training methods on qubiq using unet-resnet50.
this comparison includes both hard labels, obtained through (i) majority votes
[25] and (ii) random sampling each rater's annotation [22], as well as soft
labels derived from (i) averaging across all annotations [12,25,41] and (ii)
label smoothing [43].in the literature [12,25,41], annotations are usually
averaged with uniform weights. we additionally consider weighting each rater's
annotation by its dice score with respect to the majority votes, so that a rater
who deviates far from the majority votes receives a low weight. note that for
all methods, the dice score and ece are computed with respect to the majority
votes, while bdice is calculated as illustrated in sect. 3.3.generally, models
trained with soft labels exhibit improved accuracy and calibration. in
particular, averaging annotations with uniform weights obtains the highest
bdice, while a weighted average achieves the highest dice score. it is worth
noting that the weighted average significantly outperforms the majority votes in
terms of the dice score which is evaluated based on the majority votes
themselves. we hypothesize that this is because soft labels contain extra
interrater information, which can ease the network optimization at those
ambiguous regions. overall, we find the weighted average outperforms other
methods, with the exception of brain tumor t2, where there is a high degree of
disagreement among raters.we compare our method with state-of-the-art (sota)
methods using unet-resnet50 in table 3. in our method, we average annotations
with uniform weights for brain tumor t2 and with each rater's dice score for all
other datasets. our method, which simply averages annotations to produce soft
labels obtains superior results compared to methods that adopt complex
architectures or training techniques.
wang and blaschko [47] empirically found that a well-calibrated teacher can
distill a more accurate student. concurrently, menon et al. [28] argued that the
effectiveness of kd arises from the teacher providing an estimation of the bayes
class-probabilities p * (y|x) and this can lower the variance of the student's
empirical loss. that is, the bias of the estimation is bounded above by the
calibration error and this explains why the calibration of the teacher would be
important for the student. inspired by this, we apply a recent kernel density
estimator (kde) [35] that provides consistent estimation of e[y|f (x)]. we then
adopt it as a post-hoc calibration method to replace the temperature scaling to
calibrate the teacher in order to improve the performance of the student. for
more details of kde, please refer to our full paper on arxiv.in table 4, we
compare models trained with hard labels, ls [43] and kd [17] on lits and kits,
respectively. for all kd experiments, we use unet-resnet50 as the teacher.
again, we obtain noticeable improvements in both the dice score and ece. it is
worth noting that for unet-resnet18 and unet-efficientnetb0 on lits, the
student's dice score exceeds that of the teacher.
in table 6 (appendix c), we compare sdl with dmls. for qubiq, we train
unet-resnet50 with soft labels obtained from weighted average and report bdice.
for lits and kits, we train unet-resnet18 with kd and present the dice score.
for a fair comparison, we disable kde in all kd experiments.we find models
trained with sdl can still benefit from soft labels to a certain extent because
(i) models are trained with a mixture of ce and sdl, and ce is compatible with
soft labels; (ii) although sdl pushes predictions towards vertices, it can still
add some regularization effects in a binary segmentation setting. however, sdl
is notably outperformed by dmls. as for dmls, we find δ dml,1 is slightly
superior to δ dml,2 and recommend using δ dml,1 in practice.in table 7 (appendix
c), we ablate the contribution of each kd term on lits and kits with a
unet-resnet18 student. in the table, ce and dml represent adding the ce and dml
term between the teacher and the student, respectively. in table 8 (appendix c),
we illustrate the effect of bandwidth that controls the smoothness of kde.
results shown in the tables verify the effectiveness of the proposed loss and
the kde method.
in this study, our focus is on extending the dice loss within the realm of
medical image segmentation. it may be intriguing to apply dmls in the context of
longtailed classification [26]. additionally, while we employ dmls in the label
space, it holds potential for measuring the similarity of two feature vectors
[18], for instance, as an alternative to cosine similarity.
in this work, we introduce the dice semimetrics losses (dmls), which are
identical to the soft dice loss (sdl) in a standard setting with hard labels,
but are fully compatible with soft labels. our extensive experiments on the
public qubiq, lits and kits benchmarks validate that incorporating soft labels
leads to higher dice score and lower calibration error, indicating that these
losses can find wide application in diverse medical image segmentation problems.
hence, we suggest to replace the existing implementation of sdl with dmls.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43898-1_46.
despite their remarkable success in numerous tasks, deep learning models trained
on a source domain face the challenges to generalize to a new target domain,
especially for segmentation which requires dense pixel-level prediction. this is
attributed to a large semantic gap between these two domains. unsupervised
domain adaptation (uda) has lately been investigated to bridge this semantic gap
between labeled source domain, and unlabeled target domain [27], including
adversarial learning for aligning latent representations [23], image translation
networks [24], etc. however, these methods produce subpar performance because of
the lack of supervision from the target domain and a large semantic gap in style
and content information between the source and target domains. moreover, when an
image's content-specific information is entangled with its domain-specific style
information, traditional uda approaches fail to learn the correct representation
of the domain-agnostic content while being distracted by the domain-specific
styles. so, they cannot be generalized for multi-domain segmentation tasks
[4].compared to uda, obtaining annotation for a few target samples is worthwhile
if it can substantially improve the performance by providing crucial target
domain knowledge. driven by this speculation, and the recent success of
semisupervised learning (semisl), we investigate semi-supervised domain
adaptation (ssda) as a potential solution. recently, liu et al. [14] proposed an
asymmetric co-training strategy between a semisl and uda task, that complements
each other for cross-domain knowledge distillation. xia et al. [22] proposed a
co-training strategy through pseudo-label refinement. gu et al. [7] proposed a
new ssda paradigm using cross-domain contrastive learning (cl) and
selfensembling mean-teacher. however, these methods force the model to learn the
low-level nuisance variability, which is insignificant to the task at hand,
hence failing to generalize if similar variational semantics are absent in the
training set. fourier domain adaptation (fda) [26] was proposed to address these
challenges by an effective spectral transfer method. following [26], we design a
new gaussian fda to handle this cross-domain variability, without feature
alignment.contrastive learning (cl) is another prospective direction where we
enforce models to learn discriminative information from (dis)similarity learning
in a latent subspace [2,10]. liu et al. [15] proposed a margin-preserving
constraint along with a self-paced cl framework, gradually increasing the
training data difficulty. gomariz et al. [6] proposed a cl framework with an
unconventional channel-wise aggregated projection head for inter-slice
representation learning. however, traditional cl utilized for da on images with
entangled style and content leads to mixed representation learning, whereas
ideally, it should learn discriminative content features invariant to style
representation. besides, the instance-level feature alignment of cl is subpar
for segmentation, where dense pixel-wise predictions are indispensable [1].to
alleviate these three underlined shortcomings, we propose a novel contrastive
learning with pixel-level consistency constraint via disentangling the style and
content information from the joint distribution of source and target domain.
precisely, our contributions are as follows: (1) we propose to disentangle the
style and content information in their compact embedding space using a
joint-learning framework; (2) we propose encoder pre-training with two cl
strategies: style cl and content cl that learns the style and content
information respectively from the embedding space; (3) the proposed cl is
complemented with a pixel-level consistency constraint with dense feature
propagation module, where the former provides better categorization competence
whereas the later enforces effective spatial sensitivity; (4) we experimentally
validate that our ssda method can be extended in the uda setting easily,
achieving superior performance as compared to the sota methods on two
widely-used domain adaptive segmentation tasks, both in ssda and uda settings.
given the source domain image-label pairs {(x i s , y i s ) ns i=1 ∈ s}, a few
image-label pairs from target domain {(x i t1 , y i t1 ) nt1 i=1 ∈ t 1}, and a
large number of unlabeled target images {(x i t2 ) nt2 i=1 ∈ t 2}, our proposed
pre-training stage learns from images in {s ∪ t ; t = t 1 ∪ t 2} in a
self-supervised way, without requiring any labels. the following fine-tuning in
ssda considers image-label pairs in {s ∪t 1} for supervised learning alongside
unlabeled images t 2 in the target domain for unsupervised prediction
consistency. our workflow is shown in fig. 1.
manipulating the low-level amplitude spectrum of the frequency domain is the
easiest way for style transfer between domains [26], without notable alteration
in the visuals of high-level semantics. however, as observed in [26], the
generated images consist of incoherent dark patches, caused by abrupt changes in
amplitude around the rectangular mask. instead, we propose a gaussian mask for a
smoother transition in frequency. let, f a (•) and f p (•) be the amplitude and
phase spectrum in frequency space of an rgb image, and f -1 indicates inverse
fourier transform. we define a 2d gaussian mask g σ of the same size as f a ,
with σ being the standard deviation. given two randomly sampled images x s ∼ s
and x t ∼ t , our proposed gfda can be formulated as:where indicates
element-wise multiplication. it generates an image preserving the semantic
content from s but preserving the style from t . reciprocal pair x t→s is also
formulated using the same drill. the source and target images, and the
style-transferred versions {x s , x s→t , x t , x t→s } are then used for
contrastive pre-training below. visualization of gfda is shown in the
supplementary file.
we aim to learn discriminative content-specific features that are invariant of
the style of the source or target domain, for a better pre-training of the
network for the task at hand. hence, we propose to disentangle the style and
content information from the images and learn them jointly in a novel
disentangled cl paradigm: style cl (scl) and content cl (ccl). the proposed scl
imposes learning of domain-specific attributes, whereas ccl enforces the model
to identify the roi, irrespective of the spatial semantics and appearance. in
joint learning, they complement each other to render the model to learn
domain-agnostic and content-specific information, thereby mitigating the domain
dilemma. the set of images {x s , x s→t , x t , x t→s }, along with their
augmented versions are passed through encoder e, followed by two parallel
projection heads, namely style head (g s ) and content head (g c ) to obtain the
corresponding embeddings. two different losses: style contrastive loss l scl and
content contrastive loss l ccl , are derived below. assuming {x s , x t→s }
(along with their augmentations) having source-style representation (style a),
and {x t , x s→t } (and their augmentations) having target-style representation
(style b), in style cl, embeddings from the same domain (style) are grouped
together whereas embeddings from different domains are pushed apart in the
latent space. considering the i th anchor point x i t ∈ t in a minibatch and its
corresponding style embedding s i t ← g s (e(x i t )) (with style b), we define
the positive set consisting of the same target domain representations as λ + =
{s j+ t , s j+ s→t } ← g s (e({x j t , x j s→t })), ∀j ∈ minibatch, and negative
set having unalike source domain representation as λ -= {s j- s , s j- t→s } ← g
s (e({x j s , x j t→s })), ∀j ∈ minibatch. following simclr [5] our style
contrastive loss can be formulated as:where {s i , s j+ } ∈ style b; s j-∈ style
a, sim(•, •) defines cosine similarity, τ is the temperature parameter [5].
similarly, we define l ccl for content head as:where {c i , c j } ← g c (e({x i
, x j })). these contrastive losses, along with the consistency constraint below
enforce the encoder to extract domain-invariant and content-specific feature
embeddings.
the disentangled cl aims to learn global image-level representation, which is
useful for instance discrimination tasks. however, segmentation is attributed to
learning dense pixel-level representations. hence, we propose an additional
dense feature propagation module (dfpm) along with a momentum encoder e with
exponential moving average (ema) of parameters from e. given any pixel m of an
image x, we transform its feature f m e obtained from e by propagating other
pixel features from the same image:where k is a linear transformation layer, ⊗
denotes matmul operation. this spatial smoothing of learned representation is
useful for structural sensitivity, which is fundamental for dense segmentation
tasks. we enforce consistency between this smoothed feature fe from e and the
regular feature f e from e as:where d(•, •) indicates the spatial distance, t h
is a threshold. the overall pretraining objective can be summarized as:
the pre-training stage is followed by semi-supervised fine-tuning using a
studentteacher framework [18]. the pre-trained encoder e, along with a decoder d
are used as a student branch, whereas an identical encoder-decoder network (but
differently initialized) is used as a teacher network. we compute a supervised
loss on the labeled set {s ∪ t 1} along with a regularization loss between the
prediction of the student and teacher branches on the unlabeled set {t 2}
as:where ce indicates cross-entropy loss, e s , d s , e t , d t indicate the
student and teacher encoder and decoder networks. the student branch is updated
using a consolidated loss l = l sup + λ 3 l reg , whereas the teacher parameters
(θ t ) are updated using ema from the student parameters (θ s ):where t tracks
the step number, and α is the momentum coefficient [9]. in summary, the overall
ssda training process contains pre-training (subsect. 2.1-subsect. 2.3) and
fine-tuning (subsect. 2.4), whereas, we only use the student branch (e s , d s )
for inference.
datasets: we evaluate our work on two different da tasks to evaluate its
generalizability: (1) polyp segmentation from colonoscopy images in kvasir-seg
[11] and cvc-endoscene still [20], and (2) brain tumor segmentation in mri
images from brats2018 [16]. kvasir and cvc contain 1000 and 912 images
respectively and were split into 4 : 1 training-testing sets following [10].
brats consists of brain mris from 285 patients with t1, t2, t1ce, and flair
scans. the data was split into 4 : 1 train-test ratio, following [14].
source→target: we perform experiments on cv c → kvasir and kvasir → cv c for
polyp segmentation, and t 2 → {t 1, t 1ce, f lair} for tumor segmentation. the
ssda accesses 10 -50% and 1 -5 labels from the target domain for the two tasks,
respectively. for uda, only s is used for l sup , whereas t 1 ∪ t 2 is used for
l reg . implementation details: implementation is done in a pytorch environment
using a tesla v100 gpu with 32gb ram. we use u-net [17] backbone for the
encoder-decoder structure, and the projection heads g s and g c are shallow fc
layers. the model is trained for 300 epochs for pre-training and 500 epochs for
fine-tuning using an adam optimizer with a batch size of 4 and a learning rate
of 1e -4. λ1, λ2, λ3, and t h are set to 0.75, 0.75, 0.5, 0.6, respectively by
validation, τ, α are set to 0.07, 0.999 following [9]. augmentations include
random rotation and translation. metrics: segmentation performance is evaluated
using dice similarity score (dsc) and hausdorff distance (hd).
quantitative comparison of our proposed method with different ssda methods
[4,14,21,24] for both tasks are shown in table 1 andtable 2. act [14] simply
ignores the domain gap and only learns content semantics, resulting in
substandard performance on the brats dataset that has a significant domain gap.
fsm [24], on the other hand, is adaptable to learning explicit domain
information, but table 1. comparison with state-of-the-art uda and ssda methods
for polyp segmentation on kvasir and cvc. ssda results are shown for 10%-labeled
(10%l) and 50%-labeled (50%l) data in the target domain. the results of cited
methods are directly reported from the corresponding papers. no da: the
encoder-decoder model trained only using labeled data from the source domain is
applied to the target domain without adaptation. supervised: model is trained
using all labeled data from source and target domains. the best and second-best
results are highlighted in red and blue, respectively. lacks strong pixel-level
regularization on its prediction, resulting in subpar performance. we address
both of these shortcomings in our work, resulting in superior performance on
both tasks. other methods like [4,21], which are originally designed for natural
images, lack critical refining abilities even after fine-tuning for medical
image segmentation and hence are far behind our performance in both tasks. the
margins are even higher for less labeled data (1l) on the brats dataset, which
is promising considering the difficulty of the task. moreover, our method
produces performance close to its fully-supervised counterpart (last row in
table 1 and table 2), using only a few target labels.
unlike ssda methods, uda fully relies on unlabeled data for domain-invariant
representation learning. to analyze the effectiveness of da, we extend our model
to the uda setting (explained in sect. 3 [source → target]) and compare it with
sota methods [3,8,10,12,13,25,26,28] in table 1 and table 2. methods like
[10,19] rely on adversarial learning for aligning multi-level feature space,
which is not effective for small-sized medical data. other methods [12,25] rely
on an image-translation network but fail in effective style adaptation,
resulting in source domain-biased subpar performance. our method, although
relies on fda [26], outperforms it with a large margin of upto 12.5% dsc for
polyp segmentation, owing to its superior learning ability of disentangled style
and content semantics. similar results are observed for the brats dataset in
table 2, where our work achieved a margin of upto 2.4% dsc than its closest
performer.
we perform a detailed ablation experiment, as shown in table 3. the
effectiveness of disentangling and joint-learning of style and content
information is evident from the experiment (b)&(c) as compared to (a), where the
introduction of scl and ccl boosts overall performance significantly. moreover,
when combined together (experiment (d)), they provide a massive 9.54% and 8.52%
dsc gain over traditional cl (experiment (a)) for cv c → kvasir and kvasir → cv
c, respectively. this also points out a potential shortfall of traditional cl:
its inability to adapt to a complex domain in da. the proposed dfpm (experiment
(e)) provides local pixel-level regularization, complementary to the global
disentangled cl, resulting in a further boost in performance (∼ 1.5%). we have
similar ablation study observations on the brats2018 dataset, which is provided
in the supplementary file, along with some qualitative examples along with
available ground truth.
we propose a novel style-content disentangled contrastive learning, guided by a
pixel-level feature consistency constraint for semi-supervised domain adaptive
medical image segmentation. to the best of our knowledge, this is the first
attempt for ssda in medical image segmentation using cl, which is further
extended to the uda setting. our proposed work, upon evaluation on two different
domain adaptive segmentation tasks in ssda and uda settings, outperforms the
existing sota methods, justifying its effectiveness and generalizability.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8_25.
medical image segmentation plays a crucial role in enabling better diagnosis,
surgical planning, and image-guided surgery [8]. the inherent ambiguity and high
uncertainty of medical images pose significant challenges [5] for accurate
segmentation, attributed to factors such as unclear tumor boundaries in brain
magnetic resonance imaging (mri) images and multiple plausible annotations of
lung nodule in computed tomography (ct) images. existing medical image
segmentation methods typically provide a single, deterministic, most likely
hypothesis mask, which may lead to misdiagnosis or sub-optimal treatment.
therefore, providing accurate and diverse segmentation masks as valuable
references [17] for radiologists is crucial in clinical practice.recently,
diffusion models [10] have shown strong capacities in various visual generation
tasks [21,22]. however, how to better deal with discrete segmentation tasks
needs further consideration. although many researches [1,26] have combined
diffusion model with segmentation tasks, all these methods do not take full
account of the discrete characteristic of segmentation task and still use
gaussian noise as their diffusion kernel.to achieve accurate and diverse
segmentation masks, we propose a novel conditional bernoulli diff usion model
for medical image segmentation (berdiff). instead of using the gaussian noise,
we first propose to use the bernoulli noise as the diffusion kernel to enhance
the capacity of the diffusion model for segmentation, resulting in more accurate
segmentation masks. moreover, by leveraging the stochastic nature of the
diffusion model, our berdiff randomly samples the initial bernoulli noise and
intermediate latent variables multiple times to produce a range of diverse
segmentation masks, highlighting salient regions of interest (roi) that can
serve as a valuable reference for radiologists. in addition, our berdiff can
efficiently sample sub-sequences from the overall trajectory of the reverse
diffusion based on the rationale behind the denoising diffusion implicit models
(ddim) [25], thereby speeding up the segmentation process.the contributions of
this work are summarized as follows. 1) instead of using the gaussian noise, we
propose a novel conditional diffusion model based on the bernoulli noise for
discrete binary segmentation tasks, achieving accurate and diverse medical image
segmentation masks. 2) our berdiff can efficiently sample sub-sequences from the
overall trajectory of the reverse diffusion, thereby speeding up the
segmentation process. 3) experimental results on lidc-idri and brats 2021
datasets demonstrate that our berdiff outperforms other state-of-the-art
methods.
in this section, we first describe the problem definitions, and then demonstrate
the bernoulli forward and diverse reverse processes of our berdiff, as shown in
fig. 1. finally, we provide an overview of the training and sampling procedures.
let us assume that x ∈ r h×w×c denotes the input medical image with a spatial
resolution of h × w and c channels. the ground-truth mask is represented as y 0
∈ {0, 1}h×w , where 0 represents background while 1 roi. inspired by
diffusion-based models such as denoising diffusion probabilistic model (ddpm)
and ddim, we propose a novel conditional bernoulli diffusion model, which can be
represented as p θ (y 0 |x) := p θ (y 0:t |x)dy 1:t , where y 1 , . . . , y t
are latent variables of the same size as the mask y 0 . for medical binary
segmentation tasks, the diverse reverse process of our berdiff starts from the
initial bernoulli noise y t ∼ b(y t ; 1 2 •1) and progresses through
intermediate latent variables constrained by the input medical image x to
produce segmentation masks, where 1 denotes an all-ones matrix of the size h ×w
.
in previous generation-related diffusion models, gaussian noise is progressively
added with increasing timestep t. however, for segmentation tasks, the
groundtruth masks are represented by discrete values. to address this, our
berdiff gradually adds more bernoulli noise using a noise schedule β 1 , . . . ,
β t , as shown in fig. 1. the bernoulli forward process q(y 1:t |y 0 ) of our
berdiff is a markov chain, which can be represented as:where b denotes the
bernoulli distribution with the probability parameters (1 -using the notation α
t = 1β t and ᾱt = t τ =1 α τ , we can efficiently sample y t at an arbitrary
timestep t in closed form:(to ensure that the objective function described in
sect. 2.4 is tractable and easy to compute, we use the sampled bernoulli noise ∼
b( ; 1-ᾱt 2 •1) to reparameterize y t of eq. ( 3) as y 0 ⊕ , where ⊕ denotes the
logical operation of "exclusive or (xor)". additionally, let denote elementwise
product, and norm(•) denote normalizing the input data along the channel
dimension and then returning the second channel. the concrete bernoulli
posterior can be represented as:where]).
the diverse reverse process p θ (y 0:t ) can also be viewed as a markov chain
that starts from the bernoulli noise y t ∼ b(y t ; 1 2 • 1) and progresses
through intermediate latent variables constrained by the input medical image x
to produce diverse segmentation masks, as shown in fig. 1. the concrete diverse
reverse process of our berdiff can be represented as:specifically, we utilize
the estimated bernoulli noise ˆ (y t , t, x) of y t to parameterize μ(y t , t,
x) via a calibration function f c , as follows: (7) where | • | denotes the
absolute value operation. the calibration function aims to calibrate the latent
variable y t to a less noisy latent variable y t-1 in two steps: 1) estimating
the segmentation mask y 0 by computing the absolute deviation between y t and
the estimated noise ˆ ; and 2) estimating the distribution of y t-1 by
calculating the bernoulli posterior, p(y t-1 |y t , y 0 ), using eq. (4).
here, we provide an overview of the training and sampling procedure in
algorithms 1 and 2. during the training phase, given an image and mask data pair
{x, y 0 }, we sample a random timestep t from a uniform distribution {1, . . . ,
t }, which is used to sample the bernoulli noise .we then use to sample y t from
q(y t | y 0 ), which allows us to obtain the bernoulli posterior q(y t-1 | y t ,
y 0 ). we pass the estimated bernoulli noise ˆ (y t , t, x) through the
calibration function f c to parameterize p θ (y t-1 | y t , x). based on the
variational upper bound on the negative log-likelihood in previous diffusion
models [3], we adopt kullback-leibler (kl) divergence and binary cross-entropy
(bce) loss to optimize our berdiff as follows:finally, the overall objective
function is presented as:during the sampling phase, our berdiff first samples
the initial latent variable y t , followed by iterative calculation of the
probability parameters of y t-1 for different t. in algorithm 2, we present two
different sampling strategies from ddpm and ddim for the latent variable y t-1 .
finally, our berdiff is capable of producing diverse segmentation masks. by
taking the mean of these masks, we can further obtain a saliency segmentation
mask to highlight salient roi that can serve as a valuable reference for
radiologists. note that our berdiff has a novel parameterization technique, i.e.
calibration function, to estimate the bernoulli noise of y t , which is
different from previous works [3,11,24].
dataset and preprocessing. the data used in this experiment are obtained from
lidc-idri [2,7] and brats 2021 [4] datasets. lidc-idri contains 1,018 lung ct
scans with plausible segmentation masks annotated by four radiologists. we adopt
a standard preprocessing pipeline for lung ct scans and the trainvalidation-test
partition as in previous work [5,15,23]. brats 2021 consists of four different
sequence (t1, t2, flair, t1ce) mri images for each patient. all 3d scans are
sliced into axial slices and discarded the bottom 80 and top 26 slices. note
that we treat the original four types of brain tumors as one type following
previous work [25], converting the multi-target segmentation problem into
binary. our training set includes 55,174 2d images scanned from 1,126 patients,
and the test set comprises 3,991 2d images scanned from 125 patients. finally,
the sizes of images from lidc-idri and brast 2021 are resized to a resolution of
128 × 128 and 224 × 224, respectively. implementation details. we implement all
the methods with the pytorch library and train the models on nvidia v100 gpus.
all the networks are trained using the adamw [19] optimizer with a mini-batch
size of 32. the initial learning rate is set to 1 × 10 -4 for brats 2021 and 5 ×
10 -5 for lidc-idri. the bernoulli noise estimation u-net network in fig. 1 of
our berdiff is the same as previous diffusion-based models [20]. we employ a
linear noise schedule for t = 1000 timesteps for all the diffusion models. and
we use the sub-sequence sampling strategy of ddim to accelerate the segmentation
process. during minibatch training of lidc-idri, our berdiff learns diverse
expertise by randomly sampling one from four annotated segmentation masks for
each image. four metrics are used for performance evaluation, including
generalized energy distance (ged), hungarian-matched intersection over union
(hm-iou), soft-dice and dice coefficient. we compute ged using varying numbers
of segmentation samples (1, 4, 8, and 16), hm-iou and soft-dice using 16
samples.
we start by conducting ablation experiments to demonstrate the effectiveness of
different losses and estimation targets, as shown in table 1. all experiments
are trained for 21,000 training iterations on lidc-idri. we first explore the
selection of losses in the top three rows. we find that the combination of kl
divergence and bce loss can achieve the best performance. then, we explore the
selection of estimation targets in the bottom two rows. we observe that
estimating bernoulli noise, instead of directly estimating the ground-truth
mask, is the u-net has the same architecture as the noise estimation network in
our berdiff and previous diffusion-based models. fig. 2. diverse segmentation
masks and the corresponding saliency mask of two lung nodules randomly selected
in lidc-idri. y i 0 and y i gt refer to the i-th generated and ground-truth
segmentation masks, respectively. saliency mask is the mean of diverse
segmentation masks. more suitable for our binary segmentation task. all of these
findings are consistent with previous works [3,10].here, we conduct ablation
experiments on our berdiff with gaussian or bernoulli noise, and the results are
shown in table 2. for discrete segmentation tasks, we find that using bernoulli
noise can produce favorable results when training iterations are limited (e.g.
21,000 iterations) and even outperform using gaussian noise when training
iterations are sufficient (e.g. 86,500 iterations). we also provide a more
detailed performance comparison between bernoulliand gaussian-based diffusion
models over training iterations in fig. s3. 3, and find that our berdiff
performs well for discrete segmentation tasks. probabilistic u-net (prob.u-net),
hierarchical prob.u-net (hprob.u-net), and joint prob.u-net (jpro.u-net) use
conditional variational autoencoder (cvae) to accomplish segmentation tasks.
calibrated adversarial refinement (car) employs generative adversarial networks
(gan) to refine segmentation. pixelseg is based on autoregressive models, while
segdiff and medsegdiff are diffusion-based models. there are also methods that
attempt to model multi-annotators explicitly [13,18,27]. we have the following
three observations: 1) diffusion-based methods demonstrate significant
superiority over traditional approaches based on vae, gan, and autoregression
models for discrete segmentation tasks; 2) our berdiff outperforms other
diffusion-based models that use gaussian noise as the diffusion kernel; and 3)
our berdiff also outperforms the methods that explicitly model the annotator,
striking a good balance between diversity and accuracy. at the same time, we
present comparison segmentation results in fig. 2. compared to other models, our
berdiff can effectively learn diverse expertise, resulting in more diverse and
accurate segmentation masks. especially for small nodules that can create
ambiguity, such as the lung nodule on the left, our berdiff approach produces
segmentation masks that are more in line with the ground-truth masks.results on
brats 2021. here, we present the quantitative and qualitative results of brats
2021 in table 4 and fig. 3, respectively. we conducted a comparative analysis of
our berdiff with other models such as nnunet, transformer-based models like
transu-net and swin unetr, as well as diffusion-based methods like segdiff.
first, we find that diffusion-based methods show superior performance compared
to traditional u-net and transformer-based approaches. besides, the high
performance achieved by u-net, which shares the same architecture as our noise
estimation network, highlights the effectiveness of the backbone design in
diffusion-based models. moreover, our proposed berdiff surpasses other
diffusion-based models that use gaussian noise as the diffusion kernel. finally,
from fig. 3, we find that our berdiff segments more accurately on parts that are
difficult to recognize by the human eye, such as the tumor in the 3rd row. at
the same time, we can also generate diverse plausible segmentation masks to
produce a saliency segmentation mask. we note that some of these masks may be
false positives, as shown in the 1st row, but they can be filtered out due to
low saliency. please refer to figs. s1 ands2 for more examples of diverse
segmentation masks generated by our berdiff.
in this paper, we proposed to use the bernoulli noise as the diffusion kernel to
enhance the capacity of the diffusion model for binary segmentation tasks,
achieving accurate and diverse medical image segmentation results. our berdiff
only focuses on binary segmentation tasks and takes much time during the
iterative sampling process as other diffusion-based models; e.g. our berdiff
takes 0.4 s to segment one medical image, which is ten times of traditional
u-net. in the future, we will extend our berdiff to the multi-target
segmentation problem and implement additional strategies for speeding up the
segmentation process.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8_47.
medical image segmentation is a challenging task that requires accurate
delineation of structures and regions of interest in complex and noisy images.
multiple expert annotators are often employed to address this challenge, to
provide binary segmentation annotations for the same image. however, due to
differences in experience, expertise, and subjective judgments, annotations can
vary significantly, leading to inter-and intra-observer variability. in
addition, manual annotation is a time-consuming and costly process, which limits
the scalability and applicability of segmentation methods.to overcome these
limitations, automated methods for multi-annotator prediction have been
proposed, which aim to fuse the annotations from multiple annotators and
generate an accurate and consistent segmentation result. existing approaches for
multi-annotator prediction include majority voting [7], label fusion [3], and
label sampling [12].in recent years, diffusion models have emerged as a
promising approach for image segmentation, for example by using learned semantic
features [2]. by modeling the diffusion of image intensity values over the
iterations, diffusion models capture the underlying structure and texture of the
images and can separate regions of interest from the background. moreover,
diffusion models can handle noise and image artifacts, and adapt to different
image modalities.in this work, we propose a novel method for multi-annotator
prediction, using diffusion models for medical segmentation. the goal is to fuse
multiple annotations of the same image from different annotators and obtain a
more accurate and reliable segmentation result. in practice, we leverage the
diffusionbased approach to create one map for each level of consensus. to obtain
the final prediction, we average the obtained maps and obtain one soft map.we
evaluate the performance of the proposed method on a dataset of medical images
annotated by multiple annotators. our results demonstrate the effectiveness and
robustness of the proposed method in handling inter-and intra-observer
variability and achieving higher segmentation accuracy than the state-of-the-art
methods. the proposed method could improve the efficiency and quality of medical
image segmentation and facilitate the clinical decision-making process.
multi-annotator strategies. research attention has recently been directed
towards the issues of multi-annotator labels [7,12]. during training, jensen et
al. [12] randomly sampled different labels per image. this method produced a
more calibrated model. guan et al. [7] predicted the gradings of each annotator
individually and acquired the corresponding weights for the final prediction.
kohl et al. [15] used the same sampling strategy to train a probabilistic model,
based on a u-net combined with a conditional variational autoencoder. another
recent probabilistic approach [20] combines a diffusion model with kl divergence
to capture the variability between the different annotators. in our work, we use
consensus maps as the ground truth and compare them to other
strategies.diffusion probabilistic models (dpm). [23] are a class of generative
models based on a markov chain, which can transform a simple distribution (e.g.
gaussian) to data sampled from a complex distribution. diffusion models are
capable of generating high-quality images that can compete with and even
outperform the latest gan methods [5,9,19,23]. a variational framework for the
likelihood estimation of diffusion models was introduced by huang et al. [11].
subsequently, kingma et al. [14] proposed a variational diffusion model that
produces state-of-the-art results in likelihood estimation for image
density.conditional diffusion probabilistic models. in our work, we use
diffusion models to solve the image segmentation problem as conditional
generation, given the image. conditional generation with diffusion models
includes methods for class-conditioned generation, which is obtained by adding a
class embedding to the timestep embedding [19]. in [4], a method for guiding the
generative process in ddpm is present. this method allows the generation of
images based on a given reference image without any additional learning. in the
domain of super-resolution, the lower-resolution image is upsampled and then
concatenated, channelwise, to the generated image at each iteration [10,21]. a
similar approach passes the low-resolution images through a convolutional block
[16] prior to the concatenation.a previous study directly applied a diffusion
model to generate a segmentation mask based on a conditioned input image [1].
baranchuk et al. [2] extract features from a pretrained diffusion model for
training a segmentation network, while our diffusion model generates the output
mask. compared to the diffusionbased image segmentation method of wolleb et al.
[26], our architecture differs in two main aspects: (i) the concatenation method
of the condition signal, and (ii) an encoder that processes the conditioning
signal. we also use a lower value of t, which reduces the running time.
our approach for binary segmentation with multi-annotators employs a diffusion
model that is conditioned on the input image i ∈ r w ×h , the step estimation t,
and the consensus index c. the diffusion model updates its current estimate x t
iteratively, using the step estimation function θ . see fig. 1 for an
illustration.given a set of c annotations {a i k } c i=1 associated with input
sample i k , we define the ground truth consensus map at level c to beduring
training, our algorithm iteratively samples a random level of the consensus c ∼
u [1, 2, ..., c] and an input-output pair (i k , m c k ). the iteration number 1
≤ t ≤ t is sampled from a uniform distribution and x t is sampled from a normal
distribution.we then compute x t from x t , m c k and t according to:where ᾱ is
a constant that defines the schedule of added noise. the current step index t,
and the consensus index c are integers that are translated to z t ∈ r d and z c
∈ r d , respectively with a pair of lookup tables. the embeddings are passed to
the different networks f , d and e.in the next step, our algorithm encodes the
input signal x t with network f and encodes the condition image i k with network
g. we compute the conditioned signal u t = f (x t , z c , z t ) + g(i k ), and
apply it to the networks e and d, where the output is the estimation of x t-1
.the loss function being minimized is:the training procedure is depicted in
algorithm 1. the total number of diffusion steps t is set by the user, and c is
the number of different annotators in the dataset. our model is trained using
binary consensus maps (m c k ) as the ground truth, where k is the sample id,
and c is the consensus index.the inference process is described in algorithm 2.
we sample our model for each consensus index, and then calculate the mean of all
results to obtain our target, which is a soft-label map representing the
annotator agreement. mathematically, if the consensus maps are perfect, this is
equivalent to assigning each image location with the fraction of annotations
that consider this location to be part of the mask (if c annotators mark a
pixel, it would appear in levels 1..c). in sect. 4, we compare our method with
other variants and show that estimating the fraction map directly, using an
identical diffusion model, is far inferior to estimating each consensus level
separately and then averaging.
β 1 is significant variability between different runs of the inference method on
the same inputs, see fig. 2(b).in order to exploit this phenomenon, we run the
inference algorithm multiple times, then average the results. this way, we
stabilize the results of segmentation and improve performance, as demonstrated
in fig. 2(c). we use twenty-five generated instances in all experiments. in the
ablation study, we quantify the gain of this averaging procedure.architecture.
in this architecture, the u-net's decoder d is conventional and its encoder is
broken down into three networks: e, f , and g. the last encodes the input image,
while f encodes the segmentation map of the current step x t . the two processed
inputs have the same spatial dimensionality and number of channels. based on the
success of residual connections [8], we sum these signals f (x t , z t , z c ) +
g(i). this sum then passes to the rest of the u-net encoder e.the input image
encoder g is built from residual in residual dense blocks [24] (rrdbs), which
combine multi-level residual connections without batch normalization layers. g
has an input 2d-convolutional layer, an rrdb with a residual connection around
it, followed by another 2d-convolutional layer, leaky relu activation and a
final 2d-convolutional output layer. f is a 2dconvolutional layer with a
single-channel input and an output of l channels.the encoder-decoder part of θ ,
i.e., d and e, is based on u-net, similarly to [19]. each level is composed of
residual blocks, and at resolution 16 × 16 and 8 × 8 each residual block is
followed by an attention layer. the bottleneck contains two residual blocks with
an attention layer in between. each attention layer contains multiple attention
heads.the residual block is composed of two convolutional blocks, where each
convolutional block contains group-norm, silu activation, and a 2d-convolutional
layer. the residual block receives the time embedding through a linear layer,
silu activation, and another linear layer. the result is then added to the
output of the first 2d-convolutional block. additionally, the residual block has
a residual connection that passes all its content.on the encoder side (network
e), there is a downsample block after the residual blocks of the same depth,
which is a 2d-convolutional layer with a stride of two. on the decoder side
(network d), there is an upsample block after the residual blocks of the same
depth, which is composed of the nearest interpolation that doubles the spatial
size, followed by a 2d-convolutional layer. each layer in the encoder has a skip
connection to the decoder side.
we conducted a series of experiments to evaluate the performance of our proposed
method for multi-annotator prediction. our experiments were carried out on
datasets of the qubiq benchmark1 . we compared the performance of our proposed
method with several state-of-the-art methods. datasets. the quantification of
uncertainties in biomedical image quantification challenge (qubiq), is a
recently available challenge dataset specifically for the evaluation of
inter-rater variability. qubiq comprises four different segmentation datasets
with ct and mri modalities, including brain growth (one task, mri, seven raters,
34 cases for training and 5 cases for testing), brain tumor (one task, mri,
three raters, 28 cases for training and 4 cases for testing), prostate (two
subtasks, mri, six raters, 33 cases for training and 15 cases for testing), and
kidney (one task, ct, three raters, 20 cases for training and 4 cases for
testing). following [13], the evaluation is performed using the soft dice
coefficient with five threshold levels, set as (0.1, 0.3, 0.5, 0.7, 0.9).
implementation details. the number of diffusion steps in previous works was 1000
[9] and even 4000 [19]. the literature suggests that more is better [22]. in our
experiments, we employ 100 diffusion steps, to reduce inference time.the adamw
[18] optimizer is used in all our experiments. based on the intuition that the
more rrdb blocks, the better the results, we used as many blocks as we could fit
on the gpu without overly reducing batch size.following [13], for all datasets
of the qubiq benchmark the input image resolution, as well as the test image
resolution, was 256 × 256. the experiments were performed with a batch size of
four images and eight rrdb blocks. the network depth was seven, and the number
of channels in each depth was [l, l, l, 2l, 2l, 4l, 4l], with l = 128. the
augmentations used were: random scaling by a factor sampled uniformly in the
range [0.9, 1.1], a rotation between 0 and 15 • , translation between [0, 0.1]
in both axes, and horizontal and vertical flips, each applied with a probability
of 0.5.results. we compare our method with fcn [17], mcd [6], fpm [27], daf
[25], mv-unet [13], ls-unet [12], mh-unet [7], and mrnet [13].we also compare
with models that we train ourselves, using public code amis [20], and dmise
[26]. the first is trained in a scenario where each annotator is a different
sample ("no annotator" variant of our ablation results below), and the second is
trained on the consensus setting, similar to our method. as can be seen in table
1, our method outperforms all other methods across all datasets of qubiq
benchmark.ablation study. we evaluate alternative training variants as an
ablation study in table 2. the "annotator" variant, in which our model learns to
produce each annotator binary segmentation map and then averages all the results
to obtain the required soft-label map, achieves lower scores compared to the
"consensus" variant, which is our full method. the "no annotator" variant, where
images were paired with random annotators without utilizing the annotator ids,
achieves a slightly lower average score compared to the "annotator" variant. we
also note that our "no annotator" variant outperforms the analog amis model in
four out of five datasets, indicating that our architecture is somewhat
preferable. in a third variant, our model learns to predict the soft-label map
that denotes the fraction of annotators that mark each image location directly.
since this results in fewer generated images, we generate c times as many images
per test sample. the score of this variant is also much lower than that of our
method.next, we study the effect of the number of generated images on
performance. the results can be seen in fig. 3. in general, increasing the
number of generated instances tends to improve performance. however, the number
of runs required to reach optimal performance varies between classes. for
example, for the brain and the prostate 1 datasets, optimal performance is
achieved using 5 generated images, while on prostate 2 the optimal performance
is achieved using 25 gen- erated images. figure 4 depicts samples from multiple
datasets and presents the progression as the number of generated images
increases. as can be seen, as the number of generated images increases, the
outcome becomes more and more similar to the target segmentation.
in order to investigate the relationship between the annotator agreement and the
performance of our model, we conducted an analysis by calculating the average
dice score between each pair of annotators across the entire dataset. the
results of this pairwise dice analysis can be found in table 3, where higher
mean-scores indicate a greater consensus among the annotators. we observed that
our proposed method demonstrated improved performance on datasets with higher
agreement among annotators, specifically the kidney and prostate 1 datasets.
conversely, the performance of the other methods significantly deteriorated on
the kidney dataset, leading to a lower correlation between the dice score and
the overall performance. additionally, we examined the relationship between the
number of annotators and the performance of our model. surprisingly, we found no
significant correlation between the number of annotators and the performance of
our model.
shifting the level of consensus required to mark a region from very high to as
low as one annotator, can be seen as creating a dynamic shift from a very
conservative segmentation mask to a very liberal one. as it turns out, this
dynamic is wellcaptured by diffusion models, which can be readily conditioned on
the level of consensus. another interesting observation that we make is that the
mean (over the consensus level) of the obtained consensus masks is an effective
soft mask. applying these two elements together, we obtain state-of-the-art
results on multiple binary segmentation tasks.
semantic segmentation aims to segment objects in an image by classifying each
pixel into an object class. training a deep neural network (dnn) for such a task
is known to be data-hungry, as labeling dense pixel-level annotations requires
laborious and expensive human efforts in practice [23,32]. furthermore, semantic
segmentation in medical imaging suffers from privacy and data sharing issues
[13,35] and a lack of experts to secure accurate and clinically meaningful
regions of interest (rois). this data shortage problem causes overfitting for
training dnns, resulting in the networks being biased by outliers and ignorant
of unseen data.to alleviate the sample size and overfitting issues, diverse data
augmentations have been recently developed. for example, cutmix [31] and cutout
[4] augment images by dropping random-sized image patches or replacing the
removed regions with a patch from another image. random erase [33] extracts
noise from a uniform distribution and injects it into patches. geometric
transformations such as elastic transformation [26] warp images and deform the
original shape of objects. alternatively, feature perturbation methods augment
data by perturbing data in feature space [7,22] and logit space [9].although
these augmentation approaches have been successful for natural images, their
usage for medical image semantic segmentation is quite restricted as objects in
medical images contain non-rigid morphological characteristics that should be
sensitively preserved. for example, basalioma (e.g., pigmented basal cell
carcinoma) may look similar to malignant melanoma or mole in terms of color and
texture [6,20], and early-stage colon polyps are mostly small and
indistinguishable from background entrail surfaces [14]. in these cases, the
underlying clinical features of target rois (e.g., polyp, tumor and cancer) can
be distorted if regional colors and textures are modified with blur-based
augmentations or geometric transformations. also, cut-and-paste and crop-based
methods carry risks of dropping or distorting key objects such that expensive
pixel-level annotations could not be properly used. considering the rois are
usually small and underrepresented compared to the backgrounds, the loss of
information may cause a fatal class imbalance problem in semantic segmentation
tasks.in these regards, we tackle these issues with a novel augmentation method
without distorting the semantics of objects in image space. this can be achieved
by slightly but effectively perturbing target objects with adversarial noises at
the object level. we first augment hard samples with adversarial attacks [18]
that deceive a network and defend against such attacks with anti-adversaries.
specifically, multi-step adversarial noises are injected into rois to maximize
loss and induce false predictions. conversely, anti-adversaries are obtained
with antiadversarial perturbations that minimize a loss which eventually become
easier samples to predict. we impose consistency regularization between these
contrasting samples by evaluating their prediction ambiguities via supervised
losses with true labels. with this regularization, the easier samples provide
adaptive guidance to the misclassified data such that the difficult (but
object-relevant) pixels can be gradually integrated into the correct prediction.
from active learning perspective [12,19], as vague samples near the decision
boundary are augmented and trained, improvement on a downstream prediction task
is highly expected.we summarize our main contributions as follows: 1) we propose
a novel online data augmentation method for semantic segmentation by imposing
objectspecific consistency regularization between anti-adversarial and
adversarial data.
our method provides a flexible regularization between differently perturbed data
such that a vulnerable network is effectively trained on challenging samples
considering their ambiguities. 3) our method preserves underlying morphological
characteristics of medical images by augmenting data with quasiimperceptible
perturbation. as a result, our method significantly improves sensitivity and
dice scores over existing augmentation methods on kvasir-seg [11] and etis-larib
polyp db [25] benchmarks for medical image segmentation.
adversarial attack is an input perturbation method that adds quasiimperceptible
noises into images to deceive a dnn. given an image x, let μ be a noise bounded
by l ∞ -norm. while the difference between x and the perturbed sample x = x + μ
is hardly noticeable to human perception, a network f θ (•) can be easily fooled
(i.e., f θ (x) = f θ (x + μ)) as the μ pushes x across the decision boundary.to
fool a dnn, fast gradient sign method (fgsm) [8] perturbs x toward maximizing a
loss function l by defining a noise μ as the sign of loss derivative with
respect to x as follows: x = x + sign(∇ x l), where controls the magnitude of
perturbation. the authors in [18] proposed an extension of fgsm, i.e., projected
gradient descent (pgd), which is an iterative adversarial attack that also finds
x with a higher loss. given an iteratively perturbed sample x t at t-th
perturbation where x 0 = x, the x t of pgd is defined asrecently,
anti-adversarial methods were proposed for the benign purpose to defend against
such attacks. the work in [15] used an anti-adversarial class activation map to
identify objects and the authors in [1] proposed an anti-adversary layer to
handle adversaries. in contrast to adversarial attacks, these works find μ that
minimizes a loss to make easier samples to predict. figure 1a shows multistep
adversarial and anti-adversarial perturbations in the latent space. to increase
a classification score, the anti-adversarial noises move data away from the
decision boundary, which is the opposite direction of the adversarial
perturbations.
let {x i } n i=1 be an image set with n samples each paired with corresponding
ground truth pixel-level annotations y i . our proposed method aims to 1)
generate realistic images with adversarial attacks and 2) train a segmentation
model f θ (x i ) = y i for robust semantic segmentation with anti-adversarial
consistency regularization (aac). figure 2 shows the overall training scheme
with three phases: 1) online data augmentation, 2) computing adaptive aac
between differently perturbed samples, and 3) updating the segmentation model
using the loss from the augmented and original data. first, we generate
plausible images with iterative adversarial and anti-adversarial perturbations.
we separate the roles of perturbed data: adversaries are used as training
samples and anti-adversaries are used to provide guidance (i.e., pseudo-labels)
to learn the adversaries. specifically, consistency regularization is imposed
between these contrasting data by adaptively controlling the regularization
magnitude in the next phase. lastly, considering each sample's ambiguity, the
network parameters θ are updated for learning the adversaries along with the
given data so that discriminative regions are robustly expanded for challenging
samples.data augmentation with object-targeted adversarial attack. in many
medical applications, false negatives (i.e., failing to diagnose a critical
disease) are much more fatal than false positives. to deal with these false
negatives, we mainly focus on training a network to learn diverse features at
target rois (e.g., polyps) where disease-specific variations exist. to do so, we
first exclude the background and perturb only the objects in the given image.
given o as the target object class and (p, q) as a pixel coordinate, a masked
object is defined as xi = {(p, q)|x i (p,q) = o}. as in pgd [18], we perform
iterative perturbations on the xi for k steps. given xi,k as a perturbed sample
at k-th step (k = 1, ..., k), the adversarial and anti-adversarial perturbations
use the same initial image as x - i,0 = xi and x + i,0 = xi , respectively. with
this input, the iterative adversarial attack is defined aswhere), y i ) is a
quasi-imperceptible adversarial noise that fools f θ (•) and is a perturbation
magnitude that limits the noise (i.e., |μ (p,q) | ≤ , s.t. (p, q) ∈ xi ).
similarly, iterative anti-adversarial perturbation is defined asin contrast to
the adversarial attack in eq. 1, the anti-adversarial noise), y i ) manipulates
samples to increase the classification score.note that, generating noises and
images are online and training-free as the loss derivatives are calculated with
freezed network parameters. the adversaries x - i,1 , ..., x - i,k are used as
additional training samples so that the network includes the non-discriminative
yet object-relevant features for the prediction. on the other hand, as the
anti-adversaries are sufficiently discriminative, we do not use them as training
samples. instead, only the k-th anti-adversary x + i,k (i.e., the most perturbed
sample with the lowest loss) is used for downstream consistency regularization
to provide informative guidance to the adversaries.computing adaptive
consistency toward anti-adversary. let x i be either x i or x - i,k . as shown
in fig. 1b, consistency regularization is imposed between the anti-adversary x +
i,k and x i to reduce the gap between samples with different prediction
uncertainties. the weight of regularization between x i and x + i,k is
automatically determined by evaluating the gap in their prediction quality via
supervised losses with ground truth y i as w(x i , xwhere l(•) is dice loss [28]
and p i is the output of f θ (•) for x i . specifically, if x i is a harder
sample to predict than x + i,k , i.e., l(p i , y i )>l(p + i,k , y i ), the
weight gets larger, and thus consistency regularization is intensified between
the images.training a segmentation network. let ŷ + i,k be a segmentation
outcome, i.e., one-hot encoded pseudo-label from the network output p + i,k of
anti-adversary x + i,k . given x i and {x - i,k } k k=1 as training data, the
supervised segmentation loss l sup and the consistency regularization r con are
defined asusing the pseudo-label from anti-adversary as a perturbation of the
ground truth, the network is supervised by diverse and realistic labels that
contain auxiliary information that the originally given labels do not provide.
with a hyperparameter α, the whole training loss l = l sup +αr con is minimized
via backpropagation to optimize the network parameters for semantic
segmentation.
dataset. we conducted experiments on two representative public polyp
segmentation datasets: kvasir-seg [11] and etis-larib polyp db [25] (etis). both
are comprised of two classes: polyp and background. they provide 1000/196
(kvasir-seg/etis) input-label pairs in total and we split train/validation/test
sets into 80%/10%/10% as in [5,10,24,27,29]. the images of kvasir-seg were
resized to 512 × 608 (h × w ) and that of etis was set with 966 × 1255
resolution.implementation. we implemented our method on pytorch framework with 4
nvidia rtx a6000 gpus. adam optimizer with learning rates of 4e-3/1e-4
(kavsir-seg/etis) were used for 200 epochs with a batch size of 16. we set the
number of perturbation steps k as 10 and the magnitude of perturbation as 0.001.
the weight α for r con was set to 0.01.
along with conventional augmentation methods (i.e., random horizontal and
vertical flipping denoted as 'basic' in table 1), recent methods such as cutmix
[31], cutout [4], elastic transform [26], random erase [33], drop-block [7],
gaussian noise training (gnt) [22], logit uncertainty (lu) [9] and tumor
copy-paste (tumorcp) [30] were used as baselines. their hyperparameters were
adopted from the original papers. the basic augmentation was used in all methods
including ours by default. for the training, we used k augmented images with the
given images for all baselines as in ours for a fair comparison.
to verify the effectiveness of our method, evaluations are conducted using
various popular backbone architectures such as u-net [21], u-net++ [34], linknet
[2], and deeplabv3+ [3]. as the evaluation metric, mean intersection over union
(miou) and mean dice coefficient (mdice) are used for all experiments on test
sets. additionally, we provide recall and precision scores to offer a detailed
analysis of class-specific misclassification performance.
as shown in table 1, our method outperforms all baselines for all settings by at
most 10.06%p and 5.98%p miou margin on kvasir-seg and etis, respectively.
moreover, in fig. 3, our method with u-net on kvasir-seg surpasses the baselines
by ∼8.2%p and ∼7.2%p in precision and recall, respectively. note that, all
baselines showed improvements in most cases. however, our method performed
better even compared with the tumorcp which uses seven different augmentations
methods together for tumor segmentation. this is because our method preserves
the semantics of the key rois with small but effective noises unlike geometric
transformations [26,30], drop and cut-and-paste-based methods [4,7,30,31,33].
also, as we augment uncertain samples that deliberately deceive a network as in
active learning [12,16], our method is able to sensitively include the
challenging (but roi-relevant) features into prediction, unlike existing
noise-based methods that extract noises from known distributions [9,22,30].
in fig. 4, we visualize data augmentation results with (anti-) adversarial
perturbations on kvasir-seg dataset. the perturbed data (c and e) are the
addition of noise (d and f) to the given data (a), respectively. interestingly,
while adversaries (c) and anti-adversaries (e) are visually indistinguishable,
they induce totally opposite model decisions towards different classes. in fig.
5, we qualitatively and quantitatively compared their effects via visualizing
perturbation trajectories in the feature space projected with t-sne [17] and
comparing their supervision losses. in fig. 5a, the adversarial attacks send a
pixel embedding of a polyp class to the background class, anti-adversarial
perturbations push it towards the true class with a higher classification score.
also, loss comparisons in fig. 5b and 5c demonstrate that the anti-adversaries
(blue) are consistently easier to predict than the given data (grey) and
adversaries (red) during the training and their differences get larger as the
perturbations are iterated. these results confirm that the anti-adversaries send
their pseudo label ŷ + k closer to the ground truth with a slight change.
therefore, they can be regarded as a perturbation of the ground truth that
contain a potential to provide additional information to train a network on the
adversaries. we empirically show that ŷ + k is able to provide such auxiliary
information that the true labels do not provide, as our method performs better
with r con (i.e., l = l sup +αr con , 92.43% miou) than the case without r con
(i.e., l = l sup , 92.15% miou) using u-net on kvasir-seg. training samples in
fig. 6 show that the pseudo-labels ŷ + k can capture detailed abnormalities
(marked in red circles) which are not included in the ground truths. moreover,
as the aac considers sample-level ambiguity, the effect from ŷ + k is
sensitively controlled and a network can selectively learn the under-trained yet
object-relevant features from adversarial samples.
we present a novel data augmentation method for semantic segmentation using a
flexible anti-adversarial consistency regularization. in particular, our method
is tailored for medical images that contain small and underrepresented key
objects such as a polyp and tumor. with object-level perturbations, our method
effectively expands discriminative regions on challenging samples while
preserving the morphological characteristics of key objects. extensive
experiments with various backbones and datasets confirm the effectiveness of our
method.
head and neck (han) cancer is a prevalent type of cancer [3] with a yearly
incidence of above 1 million cases and prevalence of above 4 million cases
worldwide, accounting for around 5% of all cancer sites [17]. radiotherapy (rt)
is a standard treatment modality for han cancer, which aims to deliver high
doses of radiation to cancerous cells while sparing nearby healthy
organs-at-risk (oars) [21]. to optimize radiation dose distribution, accurate
three-dimensional (3d) segmentation of target volumes and oars is required.
computed tomography (ct) is the primary imaging modality used for rt planning
due to its ability to provide information about electron density, however, its
low image contrast for soft tissues, including tumors, makes accurate
segmentation of soft tissue oars challenging. therefore, the integration of
complementary imaging modalities, such as magnetic resonance (mr), has been
strongly recommended in clinical practice to enhance the segmentation of several
soft tissue oars in the han region [1]. this naturally poses a question of
whether automatic oar segmentation can benefit from the mr image modality. our
study therefore aims to evaluate the impact of mr integration on the quality and
robustness of automatic oar segmentation in the han region, therefore
contributing to the growing body of research on multimodal methods for medical
image analysis.
a literature review by zhang et al. [24] divides deep learning (dl)-based
multimodal segmentation methods into three fusion strategy groups: early, late
and hybrid (also named layer ) fusion. the first two groups of methods are most
commonly applied; early fusion comprises simple concatenation of modalities
along the channel dimension before feeding them into the deep neural network.
additionally, concatenating feature maps (fms) from separate modality encoders
can also be considered as early fusion [7]. late fusion, on the other hand,
employs separate branches for each input modality and then fuses the output
features by either plain concatenation or by weighing the contributions of
separate branches at the decision level. for example, zhang et al. [23] proposed
an attention mechanism to fuse fms from two separate u-nets that accepted
contrast-enhanced arterial and venous phase ct images. the third group, hybrid
fusion, aims to combine the strengths of early and late fusion [24] by employing
two or more separate encoders (i.e. one for each modality) and a single decoder,
where features from different resolution levels of the encoder are fused and fed
into the decoder that produces the final full-resolution segmentation. such
hybrid or multi-level fusion along with the adaptive fusion method represents
the current trend in computer vision [24], with the self-supervised model
adaptation method as a prime example [18]. one important aspect is also the
missing modality scenario, meaning that the multimodal model should produce
satisfactory results even if only one input modality is available. nevertheless,
the optimal fusion strategy remains an open question in need of further
exploration. similar conclusions were reached in a review of multimodal
segmentation methods in the medical imaging community by zhou et al. [25]. most
methods implement either early or late fusion, however, the layer fusion
strategy was identified as a better choice, since dense connections among layers
can exploit more complex and complementary information to enhance training. the
highlight is hyperdensenet, a dual-path 3d network proposed by dolz et al. [4]
that employs dense connections between two convolutional paths, and achieves
improvements compared to other fusion strategies and single modality variants.
however, other studies have shown that the best fusion strategy depends on the
specific nature of the problem, e.g. yan et al. [22] demonstrated that the late
fusion outperforms the other two approaches for the longitudinal detection of
diabetic retinopathy. relevant to the field of multimodal segmentation are also
developments on unpaired multimodal segmentation, where cross-modality learning
is employed to take advantage of different image modalities covering the same
anatomy, but without the constraint to collect images from the same patients
[5,10,19]. although the methodologies comprising cyclegans and/or multiple
segmentation networks [10,19] seem promising, they can be excessively complex
for the task of han oar segmentation where both ct and mr image modalities from
the same patient are often available. consequently, our primary focus is the
paired multimodal segmentation problem, including the missing modality
scenario.motivation. when segmenting oars in the han region for the purpose of
rt planning, a multimodal segmentation model that can leverage the information
from ct and mr images of the same patient might be beneficial compared to
separate single-modal models. firstly, as intuition suggests, such a model would
rely on the ct image for bone structures and on the mr image for soft tissues,
and therefore improve the overall segmentation quality by exploiting the
complementary information from both modalities. secondly, a multimodal model
would facilitate cross-modality learning by extracting knowledge from one and
applying that knowledge to the other modality, potentially improving the
segmentation accuracy. several studies indicated that such an approach is
feasible, for example, for improving video classification by training a model on
an auxiliary audio reconstruction task [12], or for audio-based detection by
using the multimodal knowledge distillation concept, where teacher networks
trained on rgb, depth and thermal images improve a student network trained only
on audio data [20]. finally, from the dl infrastructure maintenance perspective,
it is easier to maintain a single model that can handle both modalities than two
separate models for each modality. however, clinical practice differs
considerably from theory, meaning that a number of considerations must be taken
into account. firstly, although mr image acquisition is recommended, it is not
always feasible due to time constraints, scanner occupancy and financial
aspects. consequently, automatic oar multimodal segmentation is required to
handle the missing modality scenario, and provide a similar segmentation quality
as a single-modality system. secondly, because ct and mr images are not acquired
simultaneously and with the same acquisition parameters (e.g. resolution), there
is an inherent misalignment between both modalities. this can be mitigated with
image registration, but not completely, mainly due to different patient
positioning that especially affects the deformation of soft tissues, and various
modality-specific artifacts (e.g. motion, implants, partial volume effect,
etc.).
to tackle these considerations, we propose a mechanism named modality fusion
module (mfm) that can generally be applied to any network architecture that
learns features from multiple modalities, and shows promising performance also
in the missing modality scenario. the advantages of the proposed mfm are the
following: 1) it enables the spatial alignment of fms from one with fms from the
other modality to further reduce errors that persist after deformable
registration of input images, and enrich the fms to improve the final oar
segmentation, 2) it significantly improves the performance of the missing
modality scenario compared to other baseline fusion approaches, and 3) it
performs well also on single modality out-of-distribution data, therefore
facilitating cross-modality learning and contributing to better model
generalizability.
backbone architecture. our chosen backbone network is based on nnu-net, a
publicly available framework for dl-based segmentation [8] that builds on the
u-net architecture [16], adds self-configurable pre-processing, augmentation and
post-processing, and employs efficient training strategies. however, nnu-net,
which uses an early fusion strategy by concatenating input images or patches
before feeding them to the first network layer, may not be the optimal strategy
for multimodal segmentation. recent studies have shown that this approach does
not allow the network to learn meaningful high-level features from each modality
before their fusion, resulting in only simple relationships between intensities
from each input modality [4,23]. this is particularly problematic when fusing ct
and mr images, which differ in several aspects, such as the type and location of
artifacts, acquisition parameters, and visibility of soft tissues and bone
structures. while mr images can help to improve the delineations of oars that
are poorly visible in ct images, the primary delineation is always performed on
ct images with the help of registered mr images. an important repercussion is
that image registration errors propagate into oar delineations, which is
particularly salient in the han region. to address these challenges, we propose
an upgraded nnu-net network with two separate encoders, one for each modality,
and a common decoder that fuses fms using the proposed mfm that learns to infer
affine transformation parameters in a single forward pass. this approach
efficiently pseudo-registers fms from the mr encoder with those from the ct
encoder, mitigating the effects of registration errors caused by non-rigid
deformation of oars and imaging artifacts.
the proposed mfm draws inspiration from the work of jaderberg et al. [9], who
introduced a spatial transformer network (stn) that learns to infer
transformation parameters in a single forward pass, and then uses them to
transform images and/or fms. the fundamental idea is that stn can learn
meaningful features that are spatially invariant to characteristics of the input
data, without the need for extra supervision, thereby enhancing task
performance. while it was demonstrated that complete spatial invariance cannot
be achieved with stns [6], the work of jaderberg et al. is crucial in showing
that stns can be implemented as differentiable modules, enabling the loss to be
propagated through the sampling (interpolation) mechanism. the same underlying
principle of stns has also been leveraged in optical flow and its derivative
work semantic flow, where the flow alignment module was proposed to resample
low-resolution fms and align them with high-resolution fms [2]. we capitalize on
the same principle to register fms from mr images to those from ct images.
notably, mfm is different from semantic flow, as it takes two fms of the same
resolution but from different modalities, and aligns fms from the auxiliary
modality to fms of the primary modality. we propose to use mfm at each
resolution level of the nnu-net backbone, which is schematically presented in
fig. 1, and consists of three blocks: localization network, grid generator and
sampler. the localization network is a regressor network that accepts
concatenated fms from both encoders and applies four blocks of strided
convolutions followed by the relu activation to reduce their spatial dimensions.
the final fms are flattened and fed into a simple two-layer fully connected
network, which outputs 12 affine 3d transformation parameters that are then
passed to the grid generator. the generated sampling grid is used by the sampler
to resample fms from the second encoder, which are then concatenated with the
untouched fms from the first encoder and the decoder (right before the
bottleneck, only the first two are concatenated, as there are no decoder fms at
that level). both the grid generator and the sampler and readily implemented in
the pytorch library [9], and because they are both differentiable, no special
optimization is needed for the localization network, allowing localization
parameters to be optimized with the main (segmentation) loss function. since
there is no additional supervision that would assure perfect registration, we
refer to this process as pseudo-registration. the purpose of this architecture
is to align fms from both modalities and improve their fusion, leading to better
segmentation results.baseline comparison. we evaluate the performance of the
proposed mfm nnu-net against three baseline networks: 1) a single modality
nnu-net trained only on ct images, 2) a nnu-net trained on concatenated ct and
mr image pairs, and 3) a model with separate encoders for both modalities, but
with a simple concatenation along the channel axis instead of the proposed mfm.
in addition, we compare our model with the state-of-the-art modality-aware
mutual learning nnu-net (maml) that was presented at miccai 2021 [23].
image datasets. the proposed methodology was evaluated on two publicly available
datasets: our recently released han-seg dataset [14] and the pddca dataset [15].
the han-seg dataset comprises ct and t1-weighted mr images of 56 patients, which
were deformably registered with the simpleelastix registration tool, and
corresponding curated manual delineations of 30 oars (for details, please refer
to [14]). although only a subset of images is publicly available1 due to the
ongoing han-seg challenge2 , both the publicly available training as well as the
privately withheld test images were used in our 4-fold cross-validation
experiments. on the other hand, to evaluate the generalization ability of our
method, we also conducted experiments on the ct-only pddca dataset (for details,
please refer to [15]), from which we collected 15 images from the offand on-site
test sets of the corresponding challenge for our evaluation. as this dataset is
widely used for evaluating the performance of automatic han oar segmentation
methods, it serves as a valuable benchmark for comparison with other
state-of-the-art methods. note that none of the images from the ct-only pddca
dataset were used for training, and as our model expects two inputs, we
substituted the missing mr modality with an empty matrix (i.e.
zeros).implementation details. all models were trained for all oars using the 3d
fullres configuration of nnu-net, with the only modification that we reduced
rotation around the axial axis and disabled image flipping along the sagittal
plane, which eliminated segmentation errors that were previously observed for
the paired (left and right) oars. the same modification was also used with the
maml model. to ensure a fair model comparison, we set the number of filters in
the encoder of the single modality baseline model to match the number of filters
of the entry-level concatenation encoder. we also halved the number of filters
in networks that have separate encoders so that the overall number of parameters
in the proposed model and the baselines remains approximately the same
(excluding the parameters in the localization part of mfm block). note that the
maml model, which is composed of two u-nets, had a considerably higher number of
parameters. to address the challenge of a relatively small dataset, we adopted a
4-fold cross-validation strategy without using any external training images. all
models were trained until convergence, i.e. when the validation loss plateaued,
and we selected the model with the best validation loss for inference.results.
the quality of the obtained oar segmentation masks was evaluated by computing
the dice similarity coefficient (dsc) and the 95 th -percentile hausdorff
distance (hd 95 ) against reference manual delineations, and the results for all
oars are presented in figs. 2 and3, respectively. since not all images contain
all 30 oars (due to a different field-of-view), we first calculated the mean
metric for each oar and then the overall mean across all oars to ensure that the
contributions were equally weighted. we also performed analysis of statistical
significance by applying paired sample t-tests with the bonferroni correction,
presented with bars on top of the box plots (non-significant: ns (p > 0.05),
significant: * (0.01 < p < 0.05), * * (0.001 < p < 0.01), * * * (0.0001 < p <
0.001) and * * * * (p < 0.0001)).
in this study, we evaluated the impact on the quality and robustness of
automatic oar segmentation in the han region caused by the incorporation of the
mr modality into the segmentation framework. we devised a mechanism named mfm
and combined it with nnu-net as our backbone segmentation network. the choice of
using nnu-net as the backbone was based on the rationale that nnu-net already
incorporates numerous state-of-the-art dl innovations proposed in recent years,
and therefore validation of the proposed mfm is more challenging in comparison
to simply improving a vanilla u-net architecture, and consequently also more
valuable to the research community.segmentation results. the obtained results
demonstrate that our model performs best in terms of dsc (fig. 2). the resulting
gains are significant compared to separate encoders and ct-only models, and were
achieved with 4-fold crossvalidation, therefore reducing the chance of a
favorable initialization. however, dsc has been identified not to be the most
appropriate metric for evaluating the clinical adequacy of segmentations,
especially when the results are close to the interrater variability [13],
moreover, it is not appropriate for volumetrically small structures [11]. on the
other hand, distance-based metrics, such as hd 95 (fig. 3), are preferred as
they better measure the shape consistency between the reference and predicted
segmentations. although maml achieved the best results in terms of hd 95 ,
indicating that late fusion can efficiently merge the information from both
modalities, it should be noted that maml has a considerate advantage due to
having two decoders and an additional attention fusion block compared to the
baseline nnu-net with separate encoders and a single decoder. on the other hand,
our approach based on separate encoders with mfm is not far behind, with a mean
hd 95 of 4.06 mm, which is more than 15% better than the early concatenation
fusion. the comparison to the baseline nnu-net with separate encoders offers the
most direct evaluation of the proposed mfm. an approximate 10% improvement in hd
95 suggests that mfm allows the network to learn more informative fms that lead
to a better overall performance.missing modality scenario. the overall good
performance on the han-seg dataset suggests that all models are close to the
maximal performance, which is bounded by the quality of reference segmentations.
however, the performance on the pddca dataset that consists only of ct images
allows us to test how the models handle the missing modality scenario and
perform on an out-ofdistribution dataset, as images from this dataset were not
used for training. as expected, the ct-only model performed best in its regular
operating scenario, with a mean dsc of 74.7% (fig. 2) and hd 95 of 6.02 mm (fig.
3). however, significant differences can be observed between multimodal methods,
where the proposed model outperformed maml and other baselines by a large margin
in both metrics. the maml model with a mean dsc of less than 15% and hd 95 of
more than 190 mm was not able to handle the missing modality scenario, whereas
the mfm model performed almost as good as the ct-only model, with a mean dsc of
67.8% and hd 95 of 8.18 mm. it should be noted that we did not employ any
training strategies to improve handling of missing modalities, such as swapping
input images or intensity augmentations. a possible explanation is that the
proposed mfm facilitates cross-modality learning, enabling nnu-net to extract
better fms from ct images even in such extreme scenarios.
in this study, we introduced mfm, a fusion module that aligns fms from an
auxiliary modality (e.g. mr) to fms from the primary modality (e.g. ct). the
proposed mfm is versatile, as it can be applied to any multimodal segmentation
network. however, it has to be noted that it is not symmetrical, and therefore
requires the user to specify the primary modality, which is typically the same
as the primary modality used in manual delineation (i.e. in our case ct). we
evaluated the performance of mfm combined with the nnu-net backbone for
segmentation of oars in the han region, an important task in rt cancer treatment
planning. the obtained results indicate that the performance of mfm is similar
to other state-of-the-art methods, but it outperforms other multimodal methods
in scenarios with one missing modality.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8 71.
integrating multi-modality medical images for tumor segmentation is crucial for
comprehensive diagnosis and surgical planning. in the clinic, the consistent
information and complementary information in multi-modality medical images
provide the basis for tumor diagnosis. for instance, the consistent anatomical
structure information offers the location feature for tumor tracking [22], while
the complementary information such as differences in lesion area among
multimodality medical images provides the texture feature for tumor
characterization. multi-modality machine learning aims to process and relate
information from multiple modalities [4]. but it is still tricky to integrate
multi-modality medical images due to the complexity of medical images.existing
methods for multi-modality medical image integration can be categorized into
three groups: (1) input-based integration methods that concatenate
multi-modality images at the beginning of the framework to fuse them directly [
19,21], (2) feature-based fusion methods that incorporate a fusion module to
merge feature maps [16,23], and (3) decision-based fusion methods that use
weighted averaging to balance the weights of different modalities [11,15].
essentially, these methods differ in their approach to modifying the number of
channels, adding additional convolutional layers with a softmax layer for
attention, or incorporating fixed modality-specific weights. however, there is
no mechanism to evaluate the reliability of information from multi-modality
medical images. since the anatomical information of different modality medical
images varies, the reliability provided by different modalities may also differ.
therefore, it remains challenging to consider the reliability of different
modality medical images when combining multi-modality medical image
information.dempster-shafer theory (dst) [18], also known as evidence theory, is
a powerful tool for modeling information, combining evidence, and making
decisions by integrating uncertain information from various sources or knowledge
[10].some studies have attempted to apply dst to medical image processing [8].
however, using evidence theory alone does not enable us to weigh the different
anatomical information from multi-modality medical images. to explore the
reliability of different sources when using evidence theory, the work by mercier
et al. [13] proposed a contextual discounting mechanism to assign weights to
different sources. furthermore, for medical image segmentation, denoising
diffusion probabilistic models (ddpm [7]) have shown remarkable performance
[9,20]. inspired by these studies, if ddpm can parse the reliability of
multi-modality medical images to weigh the different anatomy information from
them, it will provide a significant approach for tumor segmentation.in this
paper, we propose an evidence-identified ddpm (ei-ddpm) with contextual
discounting for tumor segmentation via integrating multi-modality medical
images. our basic assumption is that we can learn the segmentation feature on
single modality medical images using ddpm and parse the reliability of different
modalities medical images by evidence theory with a contextual discounting
mechanism. specifically, the ei-ddpm first utilizes parallel conditional ddpm to
learn the segmentation feature from a single modality image. next, the
evidence-identified layer (eil) preliminarily integrates multi-modality images
by comprehensively using the multi-modality uncertain information. lastly, the
contextual discounting operator (cdo) performs the final integration of
multimodality images by parsing the reliability of information from
multi-modality medical images. the contributions of this work are:-our ei-ddpm
achieves tumor segmentation by using ddpm under the guidance of evidence theory.
it provides a solution to integrate multi-modality medical images when deploying
the ddpm algorithm. -the proposed eil and cdo apply contextual discounting
guided dst to parse the reliability of information from different modalities of
medical images. this allows for the integration of multi-modality medical images
with learned weights corresponding to their reliability. -we conducted extensive
experiments using the brats 2021 [12] dataset for brain tumor segmentation and a
liver mri dataset for liver tumor segmentation. experimental results demonstrate
the superiority of ei-ddpm over other state-of-the-art (sota) methods.
the ei-ddpm achieves tumor segmentation by parsing the reliability of
multimodality medical images. specifically, as shown in fig. 1, the ei-ddpm is
fed with multi-modality medical images into the parallel ddpm path and performs
the conditional sampling process to learn the segmentation feature from the
single modality image (sect. 2.1). next, the eil preliminary integrates
multimodality images by embedding the segmentation features from multi-modality
images into the combination rule of dst (sect. 2.2). lastly, the cdo integrates
multi-modality medical images for tumor segmentation by contextual discounting
mechanism (sect. 2.3).
background of ddpm: as an unconditional generative method, ddpm [7] has the form
of p θ (x 0 ) := p θ (x 0:t )dx 1:t , where x 1 , ..., x t represent latents
with the same dimensionality as the data x 0 ∼ q(x 0 ). it contains the forward
process of diffusion and the reverse process of denoising. the forward process
of diffusion that the approximate posterior q(x 1:t |x 0 ), it is a markov chain
by gradually adding gaussian noise for converting the noise distribution to the
data distribution according to the variance schedule β 1 , ..., β t :the reverse
process of denoising that the joint distribution p θ (x 0:t ), it can be defined
as a markov chain with learnt gaussian transitions starting from p(x t ) = n (x
t ; 0, i):where α t := 1β t , ᾱt := t s=1 α s , and multi-modality medical
images conditioned ddpm: in eq. 2 from ddpm [7], the unconditional prediction x
t-1 at each step is obtained by subtracting the predicted noise from the
previous x t , which can be defined as:as shown in fig. 2, to perform the
conditional sampling in ei-ddpm, the prediction x t-1 at each step t is on the
basis of the concatenation "⊕" of previous x t and the conditional image x c,t .
thus, the x t-1 here can be defined as:where the conditional image x c ∈ {t 1, t
2, f lair, t 1ce} corresponding to the four parallel conditional ddpm path. and
in each step t, the x c,t was also performed the operation of adding gaussian
noise to convert the distribution:where x c,0 presents the multi-modality
medical images.
as shown in fig. 1, the eil is followed at the end of the parallel ddpm path.the
hypothesis is to regard multi-modality images as independent and different
sources knowledge. and then embedding multi-phase features into the combination
rule of dst for evidence identification, which comprehensively parses the
multi-phase uncertainty information for confident decision-making. the basic
concepts of evidence identification come from dst [18]. it is assumed that θ =
{θ 1 , θ 2 , ..., θ n } is a finite domain called discriminant frame. mass
function is defined as m. so, the evidence about θ can be represented by m
as:where the m(a) denotes the whole belief and evidence allocated to a. the
associated belief (bel) and plausibility (pls) functions are defined as:and
using the contour function pls to restrict the plausibility function p ls of
singletons (i.e. pls(θ) = p ls({θ}) for all θ ∈ θ) [18]. according to the dst,
the mass function m of a subset always has lower and upper bound as bel(a) ≤
m(a) ≤ p ls(a). from the evidence combination rule of dst, for the mass
functions of two independent items m 1 and m 2 (i.e. two different modality
images), it can be calculated by a new mass function m 1 ⊕ m 2 (a), which is the
orthogonal sum of m 1 and m 2 as:where γ = b∩c=∅ m 1 (b)m 2 (c) is conflict
degree between m 1 and m 2 . and the combined contour function p ls 12
corresponding to m 1 ⊕ m 2 is:the specific evidence identification layer mainly
contains three sub-layers (i.e. activation layer, mass function layer, and
belief function layer). for activation layer, the activation of i unit can be
defined as:. the w i is the weight of i unit. λ i > 0 and a i are parameters.
the mass function layer calculates the mass of each k classes using m i ({θ k })
= u ik y i , where k k=1 u ik = 1. u ik means the degree of i unit to class θ k
. lastly, the third layer yields the final belief function about the class of
each pixel using the combination rule of dst (eq. 8). where m ? is a vacuous
mass function defined by m(θ) = 1, η m is a mixture of m and m ? . the
coefficient η plays the role of weighting mass function η m.the corresponding
contour function of η m is:advantage: eil and cdo parse the reliability of
different modality medical images in different contexts. for example, if we feed
two modality medical images like t 1 and t 2 into the ei-ddpm, with the discount
rate 1η t 1 and 1η t 2 , we will have two contextual discounted contour
functions ηt1 pls t 1 and ηt 2 pls t 2 .the combined contour function in eq. 9
is proportional to the ηt 1 pls t 1 ηt 2 pls t 2 . in this situation, the η t 1
and η t 2 can be trained to weight the two modality medical images by parsing
the reliability.
dataset. we used two mri datasets that brats 2021 [1,2,12] and a liver mri
dataset. brats 2021 1 contains 1251 subject with 4 aligned mri modalities: t1,
t2, flair, and contrast-enhanced t1 (t1ce). the segmentation labels consist of
implementation details. for the number of ddpm paths, brats 2021 dataset is
equal to 4 corresponding to the input 4 mri modalities and the liver mri dataset
is equal to 3 corresponding to the input 3 mri modalities. in the parallel ddpm
path, the noise schedule followed the improved-ddpm [14], and the u-net [17] was
utilized as the denoising model with 300 sampling steps.in eil, the initial
values of a i equal 0.5 and λ i is equal to 0.01. for cdo, the initial of
parameter η k is equal to 0.5. with the adam optimization algorithm, the
denoising process was optimized using l 1 , and the eil and cdo were optimized
using dice loss. the learning rate of ei-ddpm was set to 0.0001. the
quantitative and visual evaluation. the performance of ei-ddpm is evaluated by
comparing with three methods: a classical cnn-based method (u-net [17]), a
transformer-based method (transu-net [5]), and a ddpm-based method for
multi-modality medical image segmentation (segddpm [20]). the dice score is used
for evaluation criteria. figure 3 shows the visualized segmentation results of
ei-ddpm and compared methods. it shows some ambiguous area lost segmentation in
three compared methods but can be segmented by our ei-ddpm. discussion of learnt
reliability coefficients η. table 3 and table 4 show the learned reliability
coefficients η on brats 2021 dataset and liver mri dataset. the higher η value,
the higher reliability of the corresponding region segmentation. as shown in
table 3, the flair modality provides the highest reliability for ed
segmentation. and both the t2 modality and t1ce modality provide relatively high
reliability for et and ncr segmentation. as shown in table 4, the t1ce modality
provides the highest reliability for hemangioma and hcc segmentation. these
reliability values are the same as clinical experience [1,3].
in this paper, we proposed a novel ddpm-based framework for tumor segmentation
under the condition of multi-modality medical images. the eil and cdo enable our
ei-ddpm to capture the reliability of different modality medical images with
respect to different tumor regions. it provides a way of deploying contextual
discounted dst to parse the reliability of multi-modality medical images.
extensive experiments prove the superiority of ei-ddpm for tumor segmentation on
multi-modality medical images, which has great potential to aid in clinical
diagnosis. the weakness of ei-ddpm is that it takes around 13 s to predict one
segmentation image. in future work, we will focus on improving sampling steps in
parallel ddpm paths to speed up ei-ddpm.
breast cancer is the leading cause of cancer-related fatalities among women.
currently, it holds the highest incidence rate of cancer among women in the
u.s., and in 2022 it accounted for 31% of all newly diagnosed cancer cases [1].
due to the high incidence rate, early breast cancer detection is essential for
reducing mortality rates and expanding treatment options. bus imaging is an
effective screening option because it is cost-effective, nonradioactive, and
noninvasive. however, bus image analysis is also challenging due to the large
variations in tumor shape and appearance, speckle noise, low contrast, weak
boundaries, and occurrence of artifacts.in the past decade, deep learning-based
approaches achieved remarkable advancements in bus tumor classification [2,3].
the progress has been driven by the capability of cnn-based models to learn
hierarchies of structured image representations as semantics. to extract deep
context features, cnns apply a series of convolutional and downsampling layers,
frequently organized into blocks with residual connections. nevertheless, one
disadvantage of such architectural choice is that the feature representations in
the deeper layers become increasingly abstract, leading to a loss of spatial and
contextual information. the intrinsic locality of convolutional operations
hinders the ability of cnns to model longrange dependencies while preserving
spatial information in images effectively.vision transformer (vit) [5] and its
variants recently demonstrated superior performance in image classification
tasks. these models convert input images into smaller patches and utilize the
self-attention mechanism to model the relationships between the patches.
self-attention enables vits to capture long-range dependencies and model complex
relationships between different regions of the image. however, the effectiveness
of vit-based approaches heavily relies on access to large datasets for learning
meaningful representations of input images. this is primarily because the
architectural design of vits does not rely on the same inductive biases in
feature extraction which allow cnns to learn spatially invariant
features.accordingly, numerous prior studies introduced modifications to the
original vit network specifically designed for bus image classification
[13,14,23]. in addition, several works proposed network architectures that
combined transformers and cnns [4,15,16]. for instance, mo et al. [15] proposed
a hybrid cnn-transformer incorporating bus anatomical priors. qu et al. [16]
employed squeeze and excitation blocks to enhance the feature extraction
capacity in a hybrid cnn-based vgg16 network and vit. similarly, iqbal et al.
[4] designed two hybrid cnn-transformer networks intended either for
classification or segmentation of multi-modal breast cancer images. despite the
promising results of such hybrid approaches, effectively capturing the local
patterns and global long-range dependencies in bus images remains challenging
[4,5,24].multitask learning leverages shared information across related tasks by
jointly training the model. it constrains models to learn representations that
are relevant to all tasks rather than learning task-specific details. moreover,
multitask learning acts as a regularizer by introducing inductive bias and
prevents overfitting [25] (particularly with vits), and with that, can mitigate
the challenges posed by small bus dataset sizes. in [3], the authors
demonstrated that multitask learning outperforms single-task learning approaches
for bus classification.in this study, we introduce a hybrid multitask approach,
hybrid-mt-estan, which encompasses tumor classification as a primary task and
tumor segmentation as a secondary task. hybrid-mt-estan combines the advantages
of cnns and transformers in a framework incorporating anatomical tissue
information in bus images. specifically, we designed a novel attention block
named anatomy-aware attention (aaa), which modifies the attention block of swin
transformer by considering the breast anatomy. the anatomy of the human breast
is categorized into four primary layers: the skin, premammary (subcutaneous
fat), mammary, and retromammary layers, where each layer has a distinct texture
and generates different echo patterns. the primary layers in bus images are
arranged in a vertical stack, with similar echo patterns appearing horizontally
across the images. the kernels in the introduced aaa attention blocks are
organized in rows and columns to capture the anatomical structure of the breast
tissue. in the published literature, the closest approach to ours is the work by
iqbal et al. [4], in which the authors used hybrid single-task cnn-transformer
networks for either classification or segmentation of bus images. conversely,
hybrid-mt-estan employs a multitask approach and introduces novel architectural
design. the main contributions of this work are summarized as: [17], which
employs row-column-wise kernels to learn and fuse context information in bus
images at different context scales (see fig. 2). specifically, each mt-estan
block is composed of two parallel branches consisting of four square
convolutional kernels and two consecutive row-column-wise kernels. these
specialized convolutional kernels effectively extract contextual information of
small tumors in bus images. refer to [17,22], and [3] for the implementation
details of estan and mt-estan. the source codes of these works are available at
http://busbench.midalab.net. f l = w-msa(ln(f l-1 )) + f l-1 (1)where f l and f
l are the output features of the mlp module and the (s)w-msa module for block l,
respectively; in the proposed anatomy-aware attention (aaa) block, we redesigned
the swin blocks to enhance their ability to model both global and local features
by adding an attention block based on the breast anatomy (see fig. 3). the
additional layers are defined byconcretely, we first reconstruct the i-th
feature map (y i ) by merging (m ) all patches, and afterward, we applied
average pooling (avg-p) and max pooling (max-p) layers with size (2, 2). the
outputs of (avg-p) and (max-p) layers are concatenated and up-sampled (u ) with
size (2, 2) and stride (2, 2). rowcolumn-wise kernels (a) with size (9 , 1) and
(1 , 9) are then employed to adapt to the anatomy of the breast, and finally a
sigmoid function (σ) is applied to the output of (a) multiplied by the input
feature map (y i ).
the segmentation branch in fig. 1 outputs dense mask predictions of bus tumors.
it consists of four up blocks, each with three convolutional layers and one
upsampling layer (with size (2, 2) and stride (2, 2)). the settings of the
convolutional layers are adopted from [3]. in addition, the blocks receive four
skip connections from the mt-estan encoder, i.e., there is a skip connection
from each mt-estan block 1 to 4. the classification branch consists of three
dense layers, a dropout layer (50%), and the final dense layer that predicts the
tumor class into benign or malignant.
we applied a multitask loss function (l mt ) that aggregates two terms: a focal
loss l f ocal for the classification task and dice loss l dice for the
segmentation task. therefore, the composite loss function is l mt = w 1 • l f
ocal + l dice , where the weight coefficient w 1 is set to apply greater
importance to the classification task as the primary task. since in medical
image diagnosis achieving high sensitivity places emphasis on the detection of
malignant lesions, we employed the focal loss for the classification task to
trade off between sensitivity and specificity. because malignant tumors are more
challenging to detect due to greater differences in margin, shape, and
appearance in bus images, focal loss forces the model to focus more on difficult
predictions. specifically, focal loss adds a factor (1 -p i ) γ to the
cross-entropy loss where γ is a focusing parameter, resulting inin the
formulation, α is a weighting coefficient, n denotes the number of image
samples, t i is the target label of the i th training sample, and p i denotes
the prediction. the segmentation loss is calculated using the commonly-employed
dice loss (l dice ) function.
we evaluated the performance of hybrid-mt-estan using four public datasets, hmss
[9], busi [10], busis [20], and dataset b [6]. we combined all four datasets to
build a large and diverse dataset with a total of 3,320 b-mode bus images, of
which 1,664 contain benign tumors and 1,656 have malignant tumors. table 1 shows
the detailed information for each dataset. hmss dataset does not provide the
segmentation ground-truth masks, and for this study we arranged with a group of
experienced radiologists to prepare the masks for hmss. refer to the original
publications of the datasets for more details.
for performance evaluation of the classification task, we used the following
metrics: accuracy (acc), sensitivity (sens), specificity (spec), f1 score, area
under the curve of receiver operating characteristic (auc), false positive rate
(fpr), and false negative rate (fnr). to evaluate the segmentation performance,
we used dice similarity coefficient (dsc) and jaccard index (ji).
the proposed approach was implemented with keras and tensorflow libraries.all
experiments were performed on a machine with nvidia quadro rtx 8000 gpus and two
intel xeon silver 4210r cpus (2.40ghz) with 512 gb of ram. all bus images in the
dataset were zero-padded and reshaped to form square images. to avoid data
leakage and bias, we selected the train, test, and validation sets based on the
cases, i.e., the images from one case (patient) were assigned to only one of the
training, validation, and test sets. furthermore, we employed horizontal flip,
height shift (20%), width shift (20%), and rotation (20 • c) for data
augmentation. the proposed approach utilizes the building blocks of resnet50 and
swin-transformer-v2, pretrained on imagenet dataset. namely, mt-estan uses
pretrained resnet50 as a base model for the five encoder blocks (the
implementation details of mt-estan can be found in [3]). the encoder with aaa
blocks uses the swintransformer v2 base 256 pretrained model as a backbone. for
the composite loss function, we adopted a weight coefficient w 1 = 3, and in the
focal loss α = 0.5 and γ = 2. for model training we utilized adam optimizer with
a learning rate of 10 -5 and mini batch size of 4 images.
we compared the performance of hybrid-mt-estan for bus classification to nine
deep learning approaches commonly used for medical image analysis. the compared
models include cnn-based, vit-based, and hybrid approaches. cnnbased networks
are sha-mtl [8], mobilenet [19], densenet121 [7], and emt-net [12]. vit-based
approaches include the original vit [5], chowdery [10], and swin transformer
[18]. vgga-vit [16] is a hybrid cnn-transformer network. the values of the
performance metrics are shown in table 2, indicating that the proposed
hybrid-mt-estan outperformed all nine approaches by achieving the best accuracy,
sensitivity, f1 score, and auc with 82.8%, 86.4%, 86.0%, and 82.8%,
respectively. although sha-mtl [8] obtained the highest specificity of 90.8% and
fnr of 9.2%, the trade-off between sensitivity and specificity should be taken
into consideration, as that approach had sensitivity of 48.1%. the preferred
trade-off in medical image analysis typically is high sensitivity without
significant degradation in specificity.we evaluated the segmentation performance
of hybrid mt-estan and compared the results to five multitask approaches,
including sha-mtl [8], emt-net [12], chowdery [10], mt-estan [3], and vgga-vit
[16]. as shown in table 2,the proposed hybrid mt-estan achieved the highest
performance and increased dsc and ji by 5.9% and 6.4%, respectively compared to
mt-estan. note that results of single-task models in table 2 are not provided.
to verify the effectiveness of the anatomy-aware attention (aaa) block, we
conducted an ablation study that quantified the impact of the different
components in hybrid-mt-estan on the classification and segmentation
performance. table 3 presents the values of the performance metrics for mt-estan
(pure cnn-based approach), swin transformer (pure transformer network), a hybrid
architecture of mt-estan and swin transformer, and our proposed hybrid-mt-estan
with aaa block. according to the results in table 3, mt-estan achieved better
sensitivity and f1 score than swin transformer, with 83.7% and 83%,
respectively. the hybrid architectures of mt-estan with swin transformer
improved the classification performance and has higher accuracy, sensitivity, f1
score, and auc with 80.3%, 84.2%, 83%, and 80.2%, compared to mt-estan and swin
transformer individually. the proposed approach, hybrid-mt-estan with aaa block,
further improved accuracy, sensitivity, f1 score, and auc by 2.5%, 2.2%, 3%, and
2.6%, respectively, relative to the hybrid model without the aaa block.to
evaluate the segmentation performance, we compared the proposed approach with
and without the aaa block and swin transformer. as shown in table 3, mt-estan
combined with swin transformer improved dsc and ji by 4.1% and 4.3%,
respectively compared to mt-estan. employing the proposed aaa block further
improved dsc and ji by 1.8% and 2.1%, respectively.
in this paper, we introduced the hybrid-mt-estan, a multitask learning approach
for bus image analysis that alleviates the lack of global contextual
infor-mation in the low-level layers of cnn-based approaches. hybrid-mt-estan
concurrently performs bus tumor classification and segmentation, with a hybrid
architecture that employs cnn-based and swin transformer layers. the proposed
approach exploits multi-scale local patterns and global long-range dependencies
provided by mt-esta and aaa transformer blocks for learning feature
representations, resulting in improved generalization. experimental validation
demonstrated significant performance improvement by hybrid-mt-estan in
comparison to current state-of-the-art models for bus classification.
medical image segmentation is a core step for quantitative and precision
medicine. in the past decade, convolutional neural networks (cnns) became the
sota method to achieve accurate and fast medical image segmentation [10,12,21].
nn-unet [12], which is based on unet [21], has achieved top performances on over
20 medical segmentation challenges. parallel to manually created networks such
as nn-unet, dints [10], a cnn designed by automated neural network search, also
achieved top performances in medical segmentation decathlon (msd) [1]
challenges. the convolution operation in cnn provides a strong inductive bias
which is translational equivalent and efficient in capturing local features like
boundary and texture. however, this inductive bias limits the representation
power of cnn models which means a potentially lower performance ceiling on more
challenging tasks [7]. additionally, cnn has a local receptive field and are not
able to capture long-range dependencies unlike transformers. recently, vision
transformers have been proposed, which adopt the transformers in natural
language processing by splitting images into patches (tokens) [6], and use
self-attention to learn features. the self-attention mechanism enables learning
longrange dependencies between far-away tokens. this is intriguing and numerous
works have been proposed to incorporate transformer attentions into medical
image segmentation [2,3,9,23,24,30,32,35]. among them, swinunetr [23] has
achieved the new top performance in the msd challenge and beyond the cranial
vault (btcv) segmentation challenge by pretraining on large datasets. it has a
u-shaped structure where the encoder is a swin-transformer [16].although
transformers have achieved certain success in medical imaging, the lack of
inductive bias makes them harder to be trained and requires much more training
data to avoid overfitting. the self-attentions are good at learning complicated
relational interactions for high-level concepts [5] but are also observed to be
ignoring local feature details [5]. unlike natural image segmentation
benchmarks, e.g. ade20k [34], where the challenge is in learning complex
relationships and scene understanding from a large amount of labeled training
images, many medical image segmentation networks need to be extremely focused on
local boundary details while less in need of highlevel relationships. moreover,
the number of training data is also limited. hence in real clinical studies and
challenges, cnns can still achieve better results than transformers. for
example, the top solutions in the last year miccai challenges hector [19], flare
[11], instance [15,22] and amos [13] are all cnn based. besides lacking
inductive bias and enough training data, one extra reason could be that
transformers are computationally much expensive and harder to tune. more
improvements and empirical evidence are needed before we say transformers are
ready to replace cnns for medical image segmentation.in this paper, we try to
develop a new "to-go" transformer for 3d medical image segmentation, which is
expected to exhibit strong performance under different data situations and does
not require extensive hyperparameter tuning. swinunetr reaches top performances
on several large benchmarks, making itself the current sota, but without
effective pretraining and excessive tuning, its performance on new datasets and
challenges is not as high-performing as expected.a straightforward direction to
improve transformers is to combine the merits of both convolutions and
self-attentions. many methods have been proposed and most of them fall into two
directions: 1) a new self-attention scheme to have convolutionlike properties
[5,7,16,25,26,29]. swin-transformer [16] is a typical work in the first
direction. it uses a local window instead of the whole image to perform
self-attention. although the basic operation is still self-attention, the local
window and relative position embedding give self-attention a conv-like local
receptive field and less computation cost. another line in 1) is changing the
self-attention operation directly. coatnet [5] unifies convolution and
self-attention with relative attention, while convit [7] uses gated positional
self-attention which is equipped with a soft convolutional inductive bias. works
in the second direction 2) employs both convolution and self-attention in the
network [3,4,8,20,27,28,30,31,33,35]. for the works in this direction, we
sum-marize them into three major categories as shown in fig. 1: 2.a) dual branch
feature fusion. mobileformer [4], conformer [20], and transfuse [33] use a cnn
branch and a transformer branch in parallel to fuse the features, thus the local
details and global features are learned separately and fused altogether.
however, this doubles the computation cost. another line of works 2.b) focuses
on the bottleneck design. the low-level features are extracted by convolution
blocks and the bottleneck is the transformer, like the transunet [3], cotr [30]
and transbts [27]. the third direction 2.c) is a new block containing both
convolution and self-attention. moat [31] removes the mlp in self-attention and
uses a mobile convolution block at the front. the moat block is then used as the
basic block in building the network. cvt [28] uses convolution as the embedding
layer for key, value, and query. nnformer [35] replaces the patch merging with
convolution with stride. although those works showed strong performances, which
works best and can be the "to go" transformer for 3d medical image segmentation
is still unknown. for this purpose, we design the swinunetr-v2, which improves
the current sota swi-nunetr by introducing stage-wise convolutions into the
backbone. our network belongs to the second category, which employs convolution
and self-attention directly. at each resolution level, we add a residual
convolution (resconv) block at the beginning, and the output is then used as
input to the swin transformer blocks (contains a swin block and a shifted window
swin block). moat [31] and cvt [28] add convolution before self-attention as a
micro-level building block, and nnformer has a similar design that uses
convolution with stride to replace the patch merging layer for downsampling.
differently, our work only adds a resconv block at the beginning of each stage,
which is a macro-network level design. it is used to regularize the features for
the following transformers. although simple, we found it surprisingly effective
for 3d medical image segmentation. the network is evaluated extensively on a
variety of benchmarks and achieved top performances on the word [17], flare2021
[18], msd prostate, msd lung cancer, and msd pancreas cancer datasets [1].
compared to the original swin-unetr which needs extensive recipe tuning on a new
dataset, we utilized the same training recipe with minimum changes across all
benchmarks, showcasing the straightforward applicability of swinunetr-v2 to
reach state-of-the-art without extensive hyperparameter tuning or pretraining.
we also experimented with four design variations inspired by existing works to
justify the swinunetr-v2 design.
our swinunetr-v2 is based on the original swinunetr, and we focus on the
transformer encoder. the overall framework is shown in fig. 2.
swin-transformer. we briefly introduce the 3d swin-transformer as used in
swin-unetr [23]. a patch embedding layer of 3d convolution (stride = 2,2,2,
kernel size = 2,2,2) is used to embed the patch into tokens. four stages of swin
transformer block followed by patch merging are used to encode the input
patches. given an input tensor z i of size (b, c, h, w, d) at swin block i, the
swin transformer block splits the tensor into ( h/m , w/m , d/m windows. it
performs four operationsw-msa and sw-msa represent regular window and shifted
window multi-head selfattention, respectively. mlp and ln represent multilayer
perceptron and layernorm, respectively. a patch merging layer is applied after
every swin transformer block to reduce each spatial dimension by half.stage-wise
convolution. although swin-transformer uses local window attention to introduce
inductive bias like convolutions, self-attentions can still mess up with the
local details. we experimented with multiple designs as in fig. 3 and found that
interleaved stage-wise convolution is the most effective for swin: convolution
followed by swin blocks, then convolution goes on. at the beginning of each
resolution level (stage), the input tokens are reshaped back to the original 3d
volumes. a residual convolution (resconv) block with two sets of 3 × 3x3
convolution, instance normalization, and leaky relu are used. the output then
goes to a set of following swin transformer blocks (we use 2 in the paper).
there are in total 4 resconv blocks at 4 stages. we also tried inverted
convolution blocks with depth-wise convolution like moat [31] or with original
3d convolution, they improve the performance but are worse than the resconv
block.decoder. the decoder is the same as swinunetr [23], where convolution
blocks are used to extract outputs from those four swin blocks and the
bottleneck. the extracted features are upsampled by deconvolutional layers and
concatenated with features from a higher-resolution level(long-skip connection).
a final convolution with 1 × 1 × 1 kernel is used to map features to
segmentation maps.
we use extensive experiments to show its effectiveness and justify its design
for 3d medical image segmentation. to make fair comparisons with baselines, we
did not use any pre-trained weights.datasets. the network is validated on five
datasets of different sizes, targets and modalities:1) the word dataset [17] the
challenge comes from segmenting small tumors from large full 3d ct images. the
pancreas dataset contains 281 3d ct scans with annotated pancreas and tumors (or
cysts). the challenge is from the large label imbalances between the background,
pancreas, and tumor structures. for all three msd tasks, we perform 5-fold
crossvalidation with 70%/10%/20% train, validation, and test splits. these 20%
test data will not overlap with other folds and cover all data by 5 folds.
the training pipeline is based on the publicly available swinunetr codebase
(https://github.com/project-monai/research-contributions/tree/main/swinunetr/bt
cv, our training recipe is the same as that by swinunetr). we changed the
initial learning rate to 4e-4, and the training epoch is adapted to each task
such that the total training iteration is about 40k. random gaussian smooth,
gaussian noise, and random gamma correction are also added as additional data
augmentation. there are differences in data preprocessing across tasks. msd data
are resampled to 1 × 1x1 mm resolution and normalized to zero mean and standard
deviation (ct images are firstly clipped by .5% and 99.5% foreground intensity
percentile). for word and flare preprocessing, we use the default transforms in
swinunetr codebase (https://github.
com/project-monai/research-contributions/tree/main/swinunetr/btcv, our training
recipe is the same as that by swinunetr) and 3d uxnet codebase (see footnote 1).
besides these, all other training hyperparameters are the same. we only made
those minimal changes for different tasks and show surprisingly good
generalizability of the swinunetr-v2 and the pipeline across tasks.
word result. we follow the data split in [17] and report the test scores. all
the baseline scores are from [17] except nnformer and swinunetr. to make a fair
comparison, we didn't use any test-time augmentation or model ensemble. the test
set dice 1 and table 2. we don't have the original baseline results for
statistical testing (we reproduced some baseline results but the results are
lower than reported), so we report the standard deviation of our methods.
swinunetr has 62.5m parameters/295 gflops and swinunetr-v2 has 72.8m
parameters/320 gflops. the baseline parameters/flops can be found in [14].flare
2021 result. we use the 5-fold cross-validation data split and baseline scores
from [14]. following [14], the five trained models are evaluated on 20 held-out
test scans, and the average dice scores (not model ensemble) are shown in table
3. we can see our swinunetr-v2 surpasses all the baseline methods by a large
margin.
for msd datasets, we perform 5-fold cross-validation and ran the baseline
experiments with our codebase using exactly the same hyperparameters as
mentioned. nnunet2d/3d baseline experiments are performed using nnunet's
original codebase 2 since it has its own automatic hyperparameter selection. the
test dice score and standard deviation (averaged over 5 fold) are shown in table
4. we did not do any postprocessing or model ensembling, thus there can be a gap
between the test values and online msd leaderboard values. we didn't compare
with leaderboard results because the purpose of the experiments is to make fair
comparisons, while not resorting to additional training data/pretraining,
postprocessing, or model ensembling.
in this section, we investigate other variations of adding convolutions into
swin transformer. we follow fig. 1 [31] work. 4) swin-var-down: the patch
merging is replaced by convolution with stride 2 like nnformer [35]. we perform
the study on the word dataset, and the mean test dice and hd95 scores are shown
in table 5. we can see that adding convolution at different places does affect
the performances, and the swinunetr-v2 design is the optimal on word test set.
in this paper, we propose a new 3d medical image segmentation network
swinunetr-v2. for some tasks, we found the original swinunetr with pure
transformer backbones (or other vit-based models) may have inferior performance
and training stability than cnns. to improve this, our core intuition is to
combine convolution with window-based self-attention. although existing
window-based attention already has a convolution-like inductive bias, it is
still not good enough for learning local details as convolutions. we tried
multiple combination strategies as in table 5 and found our current design most
effective. by only adding one resconv block at the beginning of each resolution
level, the features can be well-regularized while not too constrained by the
convolution inductive bias, and the computation cost will not increase by a lot.
extensive experiments are performed on a variety of challenging datasets, and
swinunetr-v2 achieved promising improvements. the optimal combination of swin
transformer and convolution still lacks a clear principle and theory, and we can
only rely on trial and error in designing new architectures. we will apply the
network to active challenges for more evaluation.
breast cancer is the most common cause of cancer-related deaths among women all
around the world [8]. early diagnosis and treatment is beneficial to improve the
survival rate and prognosis of breast cancer patients. mammography,
ultrasonography, and magnetic resonance imaging (mri) are routine imaging
modalities for breast examinations [15]. recent clinical studies have proven
that dynamic contrast-enhanced (dce)-mri has the capability to reflect tumor
morphology, texture, and kinetic heterogeneity [14], and is with the highest
sensitivity for breast cancer screening and diagnosis among current clinical
imaging modalities [17]. the basis for dce-mri is a dynamic t1-weighted contrast
enhanced sequence (fig. 1). t1-weighted acquisition depicts enhancing
abnormalities after contrast material administration, that is, the cancer
screening is performed by using the post-contrast images. radiologists will
analyze features such as texture, morphology, and then make the treatment plan
or prognosis assessment. computer-aided feature quantification and diagnosis
algorithms have recently been exploited to facilitate radiologists analyze
breast dce-mri [12,22], in which automatic cancer segmentation is the very first
and important step.to better support the radiologists with breast cancer
diagnosis, various segmentation algorithms have been developed [20]. early
studies focused on image processing based approaches by conducting graph-cut
segmentation [29] or analyzing low-level hand-crafted features [1,11,19]. these
methods may encounter the issue of high computational complexity when analyzing
volumetric data, and most of them require manual interactions. recently,
deep-learning-based methods have been applied to analyze breast mri. zhang et
al. [28] proposed a mask-guided hierarchical learning framework for breast tumor
segmentation via convolutional neural networks (cnns), in which breast masks
were also required to train one of cnns. this framework achieved a mean dice
value of 72% on 48 testing t1-weighted scans. li et al. [16] developed a
multi-stream fusion mechanism to analyze t1/t2-weighted scans, and obtained a
dice result of 77% on 313 subjects. gao et al. [7] proposed a 2d cnn
architecture with designed attention modules, and got a dice result of 81% on 87
testing samples. zhou et al. [30] employed a 3d affinity learning based
multi-branch ensemble network for the segmentation refinement and generated 78%
dice on 90 testing subjects. wang et al. [24] integrated a combined 2d and 3d
cnn and a contextual pyramid into u-net to obtain a dice result of 76% on 90
subjects. wang et al. [25] proposed a tumor-sensitive synthesis module to reduce
false segmentation and obtained 78% dice value. to reduce the huge annotation
burden for the segmentation task, zeng et al. [27] presented a semi-supervised
strategy to segment the manually cropped dce-mri scans, and attained a dice
value of 78%.although [27] has been proposed to alleviate the annotation effort,
to acquire the voxel-level segmentation masks is still time-consuming and
laborious, see fig. 1(c). weakly-supervised learning strategies such as extreme
points [5,21], bounding box [6] and scribbles [4] can be promising solutions.
roth et al. [21] utilized extreme points to generate scribbles to supervise the
training of the segmentation network. based on [21], dorent et al. [5]
introduced a regularized loss [4] derived from a conditional random field (crf)
formulation to encourage the prediction consistency over homogeneous regions. du
et al. [6] employed bounding boxes to train the segmentation network for organs.
however, the geometric prior used in [6] can not be an appropriate strategy for
the segmentation of lesions with various shapes. to our knowledge, currently
only one weakly-supervised work [18] has been proposed for breast mass
segmentation in dce-mri. this method employed three partial annotation methods
including single-slice, orthogonal-slice (i.e., 3 slices) and interval-slice (∼6
slices) to alleviate the annotation cost, and then constrained segmentation by
estimated volume using the partial annotation. the method obtained a dice value
of 83% using the interval-slice annotation, on a testing dataset containing only
28 patients.in this study, we propose a simple yet effective weakly-supervised
strategy, by using extreme points as annotations (see fig. 1(d)) to segment
breast cancer. specifically, we attempt to optimize the segmentation network via
the conventional trainfine-tuneretrain process. the initial training is
supervised by a contrastive loss to pull close positive voxels in feature space.
the fine-tune is conducted by using a similarity-aware propagation learning
(simple) strategy to update the pseudo-masks for the subsequent retrain. we
evaluate our method on a collected dce-mri dataset containing 206 subjects.
experimental results show our method achieves competitive performance compared
with fully supervision, demonstrating the efficacy of the proposed simple
strategy.
the proposed simple strategy and the trainfine-tuneretrain procedure is
illustrated in fig. 2. the extreme points are defined as the left-, right-,
anterior-, posterior-, inferior-, and superior-most points of the cancerous
region in 3d. the initial pseudo-masks are generated according to the extreme
points by using the random walker algorithm. the segmentation network is firstly
trained based on the initial pseudo-masks. then simple is employed to fine-tune
the network and update the pseudo-masks. at last, the network is retrained from
random initialization using the updated pseudo-masks.
we use the extreme points to generate pseudo-masks based on random walker
algorithm [9]. to improve the performance of random walker, according to [21],
we first generate scribbles by searching the shortest path on gradient magnitude
map between each extreme point pair via the dijkstra algorithm [3]. after
generating the scribbles, we propose to dilate them to increase foreground seeds
for random walker. voxels outside the bounding box (note that once we have the
six extreme points, we have the 3d bounding box of the cancer) are expected to
be the background seeds. next, the random walker algorithm is used to produce a
foreground probability map y :where ω is the spatial domain. to further increase
the area of foreground, the voxel at location k is considered as new foreground
seed if y (k) is greater than 0.8 and new background seed if y (k) is less than
0.1. then we run the random walker algorithm repeatedly. after seven times
iterations, we set foreground in the same way via the last output probability
map. voxels outside the bounding box are considered as background. the rest of
voxels remain unlabeled. this is the way initial pseudo-masks y init : ω ⊂ r 3 →
{0, 1, 2} generated, where 0, 1 and 2 represent negative, positive and
unlabeled.
let x : ω ⊂ r 3 → r denotes a training volume. let f and θ be network and its
parameters, respectively. a simple training approach is to minimize the partial
cross entropy loss l pce , which is formulated as:(moreover, supervised
contrastive learning is employed to encourage voxels of the same label to gather
around in feature space. it ensures the network to learn discriminative features
for each category. specifically, features corresponding to n negative voxels and
n positive voxels are randomly sampled, then the contrastive loss l ctr is
minimized:where p(k) denotes the set of points with the same label as the voxel
k and n (k) denotes the set of points with the different label. z(k) denotes the
feature vector of the voxel at location k. sim(•, •) is the cosine similarity
function. σ denotes sigmoid function. τ is a temperature parameter.to summarize,
we employ the sum of the partial cross entropy loss l pce and the contrastive
loss l ctr to train the network with initial pseudo-masks:(3)
the performance of the network trained by the incomplete initial pseudo-masks is
still limited. we propose to fine-tune the entire network using the pre-trained
weights as initialization. the fine-tune follows the simple strategy which
evaluates the similarity between unlabeled voxels and positive voxels to
propagate labels to unlabeled voxels. specifically, n positive voxels are
randomly sampled as the referring voxel. for each unlabeled voxel k, we evaluate
its similarity with all referring voxels:where i(•) is the indicator function,
which is equal to 1 if the cosine similarity is greater than λ and 0 if less. if
s(k) is greater than αn , the voxel at location k is considered as positive.
then the network is fine-tuned using the partial cross entropy loss same as in
the initial train stage. the loss function l f inetune is formulated as:where w
is the weighting coefficient that controls the influence of the pseudo labels.
to reduce the influence of possible incorrect label propagation, pseudo labels
for unlabeled voxels are valid only for the current iteration when they are
generated.after the fine-tune completed, the network generates binary
pseudo-masks for every training data, which are expected to be similar to the
ground-truths provided by radiologists. finally the network is retrained from
random initialization by minimizing the cross entropy loss with the binary
pseudo-masks.
dataset. we evaluated our method on an in-house breast dce-mri dataset collected
from the cancer center of sun yat-sen university. in total, we collected 206
dce-mri scans with biopsy-proven breast cancers. all mri scans were examined
with 1.5t mri scanner. the dce-mri sequences (tr/te = 4.43 ms/1.50 ms, and flip
angle = 10 • ) using gadolinium-based contrast agent were performed with the
t1-weighted gradient echo technique, and injected 0.2 ml/kg intravenously at 2.0
ml/s followed by 20 ml saline. the dce-mri volumes have two kinds of resolution,
0.379×0.379×1.700 mm 3 and 0.511×0.511×1.000 mm 3 .all cancerous regions and
extreme points were manually annotated by an experienced radiologist via
itk-snap [26] and further confirmed by another radiologist. we randomly divided
the dataset into 21 scans for training and the remaining scans for testing 1 .
before training, we resampled all volumes into the same target spacing
0.600×0.600×1.000 mm 3 and normalized all volumes as zero mean and unit
variance.implementation details. the framework was implemented in pytorch, using
a nvidia geforce gtx 1080 ti with 11gb of memory. we employed 3d unet [2] as our
network backbone.• train: the network was trained by stochastic gradient descent
(sgd) for 200 epochs, with an initial learning rate η = 0.01. the ploy learning
policy was used to adjust the learning rate, (1epoch/200) 0.9 . the batch size
was 2, consisting of a random foreground patch and a random background patch
located via initial segmentation y init . such setting can help alleviate class
imbalance issue. the patch size was 128 × 128 × 96. for the contrastive loss, we
set n = 100, temperature parameter τ = 0.1. • fine-tune: we initialized the
network with the trained weights. we trained it by sgd for 100 iterations, with
η = 0.0001. the ploy learning policy was also used. for the simple strategy, we
set n = 100, λ = 0.96, α = 0.96, w = 0.1. quantitative and qualitative analysis.
we first verified the efficacy of our simple in the training stage. figure 3
illustrates the pseudo-masks at different training stages. it is obvious that
our simple effectively updated the pseudomasks to make them approaching the
ground-truths. therefore, such fune-tuned pseudo-masks could be used to retrain
the network for better performance. 1 reports the quantitative dice, jaccard,
average surface distance (asd), and hausdorff distance (95hd) results of
different methods. we compared our method with an end-to-end approach [4] that
proposed to optimize network via crf-regularized loss l crf . although our l ctr
supervised method outcompeted l crf [4], the networks trained only using the
initial pseudo-masks could not achieve enough high accuracy (dice values<70%).
in contrast, the proposed simple largely boosted the performance of the
basically trained networks, by +14.74% dice and +15.16% jaccard (v.s. l crf ),
+11.81% dice and +12.65% jaccard (v.s. l ctr ). table 1 also shows the
comparison results of three general weakly-supervised strategies, including
entropy minimization [10], mean teacher [23], and bounding box [13]. our method
consistently outperformed these strategies with respect to all evaluation
metrics. furthermore, our method achieved competitive dice results compared with
fully supervision, which again proves the efficacy of the proposed simple
strategy. note that the average annotation time for extreme points and full
masks were 31 s and 95 s per scan, respectively. figure 5 visualizes the 3d
distance map between the segmented surface and ground-truth. it can be observed
that our simple consistently enhanced the segmentation.
we introduce a simple yet effective weakly-supervised learning method for breast
cancer segmentation in dce-mri. the primary attribute is to fully exploit the
simple trainfine-tuneretrain process to optimize the segmentation network via
only extreme point annotations. this is achieved by employing a similarityaware
propagation learning (simple) strategy to update the pseudo-masks. experimental
results demonstrate the efficacy of the proposed simple strategy for
weakly-supervised segmentation.
accurate and robust classification and segmentation of the medical image are
powerful tools to inform diagnostic schemes. in clinical practice, the
image-level classification and pixel-wise segmentation tasks are not independent
[8,27]. joint classification and segmentation can not only provide clinicians
with results for both tasks simultaneously, but also extract valuable
information and improve performance. however, improving the reliability and
interpretability of medical image analysis is still reaching.considering the
close correlation between the classification and segmentation, many researchers
[6,8,20,22,24,27,28] proposed to collaboratively analyze the two tasks with the
help of sharing model parameters or task interacting. most of the methods are
based on sharing model parameters, which improves the performance by fully
utilizing the supervision from multiple tasks [8,27]. for example, thomas et al.
[20] combined whole image classification and segmentation of skin cancer using a
shared encoder. task interacting is also a widely used method [12,24,28] as it
can introduce the high-level features and results produced by one task to
benignly guide another. however, there has been relatively little research on
introducing reliability into joint classification and segmentation. the
reliability and interpretability of the model are particularly important for
clinical tasks, a single result of the most likely hypothesis without any clues
about how to make the decision might lead to misdiagnoses and sub-optimal
treatment [10,22]. one potential way of improving reliability is to introduce
uncertainty for the medical image analysis model.the current uncertainty
estimation method can roughly include the dropoutbased [11], ensemble-based
[4,18,19], deterministic-based methods [21] and evidential deep learning
[5,16,23,30,31]. all of these methods are widely utilized in classification and
segmentation applications for medical image analysis. abdar et al. [1] employed
three uncertainty quantification methods (monte carlo dropout, ensemble mc
dropout, and deep ensemble) simultaneously to deal with uncertainty estimation
during skin cancer image classification. zou et al. [31] proposed tbrats based
on evidential deep learning to generate robust segmentation results for brain
tumor and reliable uncertainty estimations. unlike the aforementioned methods,
which only focus on uncertainty in either medical image classification or
segmentation. furthermore, none of the existing methods have considered how
pixel-wise and image-level uncertainty can help improve performance and
reliability in mutual learning.based on the analysis presented above, we design
a novel uncertaintyinformed mutual learning (uml) network for medical image
analysis in this study. our uml not only enhances the image-level and pixel-wise
reliability of medical image classification and segmentation, but also leverages
mutual learning under uncertainty to improve performance. specifically, we adopt
evidential deep learning [16,31] to simultaneously estimate the uncertainty of
both to estimate image-level and pixel-wise uncertainty. we introduce an
uncertainty navigator for segmentation (un) to generate preliminary segmentation
results, taking into account the uncertainty of mutual learning features. we
also propose an uncer- tainty instructor for classification (ui) to screen
reliable masks for classification based on the preliminary segmentation results.
our uml represents pioneering work in introducing reliability and
interpretability to joint classification and segmentation, which has the
potential to the development of more trusted medical analysis tools1 .
the overall architecture of the proposed uml, which leverages mutual learning
under uncertainty, is illustrated in fig. 1. firstly, uncertainty estimation for
classification and segmentation adapts evidential deep learning to provide
image-level and pixel-wise uncertainty. then, trusted mutual learning not only
utilizes the proposed un to fully exploit pixel-wise uncertainty as the guidance
for segmentation but also introduces the ui to filter the feature flow between
task interaction.given an input medical image i, i ∈ r h,w , where h, w are the
height and width of the image, separately. to maximize the extraction of
specific information required for two different tasks while adequately mingling
the common feature which is helpful for both classification and segmentation, i
is firstly fed into the dual backbone network that outputs the classification
feature maps f c i , i ∈ 1, ..., 4 and segmentation feature maps f s i , i ∈ 1,
..., 4, where i denotes the i th layer of the backbone. then following [29], we
construct the feature mixer using pairwise channel map interaction to mix the
original feature and get the mutual feature maps f m i , i ∈ 1, ..., 4. finally,
we combine the last layer of mutual feature with the original feature.
classification uncertainty estimation. for the k classification problems, we
utilize subjective logic [7] to produce the belief mass of each class and the
uncertainty mass of the whole image based on evidence. accordingly, given a
classification result, its k + 1 mass values are all non-negative and their sum
is one:where b c k ≥ 0 and u c ≥ 0 denote the probability belonging to the k th
class and the overall uncertainty value, respectively. as shown in fig. 1
whererepresents the dirichlet strength. actually, eq. 2 describes such a
phenomenon that the higher the probability assigned to the k th class, the more
evidence observed for k th category should be.segmentation uncertainty
estimation. essentially, segmentation is the classification for each pixel of a
medical image. given a pixel-wise segmentation result, following [31] the seg
dirichlet distribution can be parameterized by α s(h,w) = [α s(h,w) 1
], (h, w) ∈ (h, w ). we can compute the belief mass and uncertainty mass of the
input image bywhere b s(h,w) q ≥ 0 and u s(h,w) ≥ 0 denote the probability of
the pixel at coordinate (h, w) for the q th class and the overall uncertainty
value respectively. we also define u s = {u s(h,w) , (h, w) ∈ (h, w )} as the
pixel-wise uncertainty of the segmentation result.
uncertainty navigator for segmentation. actually, we have already obtained an
initial segmentation mask m = α s , m ∈ (q, h, w ) through estimating
segmentation uncertainty, and achieved lots of valuable features such as lesion
location. in our method, appropriate uncertainty guided decoding on the feature
list can obtain more reliable information and improve the performance of
segmentation [3,9,26]. so we introduce uncertainty navigator for
segmentation(un) as a feature decoder, which incorporates the pixel-wise
uncertainty in u s and lesion location information in m with the segmentation
feature maps to generate the segmentation result and reliable features. having a
unet-like architecture [15], un computes segmentation s i , i ∈ 1, .., 4 at each
layer, as well as introduces the uncertainty in the bottom and top layer by the
same way. take the top layer as an example, as shown in fig. 2(a), un calculates
the reliable mask m r by:then, the reliable segmentation feature r s , which
combines the trusted information in m r with the original features, is generated
by:where f s 1 derives from jump connecting and f b 2 is the feature of the s 2
with one up-sample operation. conv(•) represents the convolutional operation,
cat(•, •) denotes the concatenation. especially, the u s is also used to guide
the bottom feature with the dot product. the r s is calculated from the
segmentation result s 1 and contains uncertainty navigated information not found
in s 1 .uncertainty instructor for classification. in order to mine the
complementary knowledge of segmentation as the instruction for the
classification and eliminate intrusive features, we devise an uncertainty
instructor for classification (ui) following [22]. and the rich information
(e.g., lesion location and boundary characteristic) in r s , which can be
expressed by:where d n (•) denotes that the frequency of down-sampling
operations is n. then the produced features are transformed into a semantic
feature vector by the global average pooling. the obtained vector is converted
into the final result (belief values) of classification with uncertainty
estimation.
in a word, to obtain the final results of classification and segmentation, we
construct an end-to-end mutual learning process, which is supervised by a joint
loss function. to obtain an initial segmentation result m and a pixel-wise
uncertainty estimation u s , following [31], a mutual loss is used as:where y s
is the ground truth (gt) of the segmentation. the hyperparameters λ m 1 and λ m
2 play a crucial role in controlling the kullback-leibler divergence (kl) and
dice score, as supported by [31]. similarly, in order to estimate the imagelevel
uncertainty and classification results. a classification loss is constructed
following [5], as:where y c is the true class of the input image. the
hyperparameter λ c serves as a crucial hyperparameter governing the kl, aligning
with previous work [5]. to obtain reliable segmentation results, we also adopt
deep supervision for the final segmentation result s = {s i , i = 1, ..., 4},
which can be denoted as:where υ n indicates the number of up-sampling is 2 n .
thus, the overall loss function of our uml can be given as:where w m , w c , w s
denote the weights and are set 0.1, 0.5, 0.4, separately.
dataset and implementation. we evaluate the our uml network on two datasets
refuge [14] and ispy-1 [13]. refuge contains two tasks, classification of
glaucoma and segmentation of optic disc/cup in fundus images. the overall 1200
images were equally divided for training, validation, and testing. all images
are uniformly adjusted to 256 × 256 px. the tasks of ispy-1 are the pcr
prediction and the breast tumor segmentation. a total of 157 patients who suffer
the breast cancer are considered -43 achieve pcr and 114 non-pcr.for each case,
we cut out the slices in the 3d image and totally got 1,570 2d images, which are
randomly divided into the train, validation, and test datasets with 1,230, 170,
and 170 slices, respectively. we implement the proposed method via pytorch and
train it on nvidia geforce rtx 2080ti. the adam optimizer is adopted to update
the overall parameters with an initial learning rate 0.0001 for 100 epochs. the
scale of the regularizer is set as 1 × 10 -5 . we choose vgg-16 and res2net as
the encoders for classification and segmentation, separately.compared methods
and metrics. we compared our method with singletask methods and multi-task
methods. (1) single-task methods: (a) ec [17], (b) tbrats [31] and (c) transunet
[2]. evidential deep learning for classification (ec) first proposed to
parameterize classification probabilities as dirichlet distributions to explain
evidence. tbrats then extended ec to medical image segmentation. meriting both
transformers and u-net, transunet is a strong model for medical image
segmentation. (2) multi-task methods: (d) bcs [25] and (e) dsi [28]. the
baseline of the joint classification and segmentation framework (bcs) is a
simple but useful way to share model parameters, which utilize two different
encoders and decoders for learning respectively. the deep synergistic
interaction network (dsi) has demonstrated superior performance in joint task.
we adopt overall accuracy (acc) and f1 score (f1) as the evaluation criteria for
the classification task. dice score (di) and average symmetric surface distance
(assd) are chosen for the segmentation task. comparison under noisy data. to
further valid the reliability of our model, we introduce gaussian noise with
various levels of standard deviations (σ) to the input medical images. the
comparison results are shown in table 2. as can be observed that, the accuracy
of classification and segmentation significantly decreases after adding noise to
the raw data. however, benefiting from the uncertainty-informed guiding, our uml
consistently deliver impressive results. in fig. 3, we show the output of our
model under the noise. it is obvious that both the image-level uncertainty and
the pixel-wise uncertainty respond reasonably well to noise. these experimental
results can verify the reliability and interpre of the uncertainty guided
interaction between the classification and segmentation in the proposed uml. the
results of more qualitative comparisons can be found in the supplementary
material.ablation study. as illustrated in table 3, both of the proposed un and
ui play important roles in trusted mutual learning. the baseline method is
bcs.md represents the mutual feature decoder. it is clear that the performance
of classification and segmentation is significantly improved when we introduce
supervision of mutual features. as we thought, the introduction of un and ui
takes the reliability of the model to a higher level.
in this paper, we propose a novel deep learning approach, uml, for joint
classification and segmentation of medical images. our approach is designed to
improve the reliability and interpretability of medical image classification and
segmentation, by enhancing image-level and pixel-wise reliability estimated by
evidential deep learning, and by leveraging mutual learning with the proposed un
and ui modules. our extensive experiments demonstrate that uml outperforms
baselines and introduces significant improvements in both classification and
segmentation. overall, our results highlight the potential of uml for enhancing
the performance and interpretability of medical image analysis.
. this work was supported by the national research foundation, singapore under
its ai singapore programme (aisg award no: aisg2-tc-2021-003), a*star ame
programmatic funding scheme under project a20h4b0141, a*star central research
fund, the science and technology department of sichuan province (grant no.
2022yfs0071 & 2023yfg0273), and the china scholarship council (no.
202206240082).
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8 4.
medical image segmentation is always a critical task as it can be used for
disease diagnosis, treatment planning, and anomaly monitoring. weakly supervised
semantic segmentation attracts significant attention from medical image
community since it greatly reduces the cost of dense pixel-wise labeling to get
segmentation mask. in wsss, the training labels are usually easier and faster to
obtain, like image-level tags, bounding boxes, scribbles, or point annotations.
this work only focuses on wsss with image-level tags, like whether a tumor
presents or not. in this field, previous wsss works [10,14] are dominated by
class activation map (cam) [25] and its variants [4,18,21], which was firstly
introduced as a tool to visualize saliency maps when making a class
prediction.meanwhile, denoising diffusion models [6,9] demonstrate superior
performance in image synthesis than other generative models. also, there are
several works exploring the application of diffusion models to semantic
segmentation in natural images [2,8]and medical images [15,[22][23][24]. to the
best of our knowledge, wolleb et al. [22] is the only work that introduces
diffusion models to pixel-wise anomaly detection with only classification
labels. they achieve this by utilizing an external classifier trained with
image-level annotations to guide the reverse markov chain. by passing the
gradient of the classifier, the diffusion model gradually removes the anomaly
areas during the denoising process and then obtains the anomaly map by comparing
the reconstructed and original images however, this approach is based on the
hypothesis that the classifier can accurately locate the target objects and that
the background is not changed when removing the noise. this assumption does not
always hold, especially when the distribution of positive images is diverse, and
the reconstruction error can also be accumulated after hundreds of steps. as
shown in fig. 1, the reconstructed images guided by the gradient of non-kidney
not only remove the kidney area but also change the content in the background.
another limitation of this method is the long inference time required for a
single image, as hundreds of iterations are needed to restore the image to its
original noise level. in contrast, cam approaches need only one inference to get
the saliency maps. therefore, there is ample room for improvement in using
diffusion models for wsss task. in this work, we propose a novel wsss framework
with conditional diffusion models (cdm) as we observe that the predicted noises
on different condition show difference. instead of completely removing the
noises from images, we calculate the derivative of the predicted noise after a
few stages with respect to conditions so that the related objects are
highlighted in the gradient map with less background misidentified. as the
output of diffusion model is not differentiable with respect to the discrete
condition input, we adopt the finite difference method, i.e., perturbing the
condition embedding by a small amplitude and logging the change of the output
with ddim [19] generative process. in addition, our method does not require the
full reverse denoising process for the noised images and may only need one or a
few iterations. thus the inference time of our method is comparable to that of
cam-based approaches. we evaluate our methods on two different tasks, brain
tumor segmentation and kidney segmentation, and provide the quantitative results
of both cam based and diffusion model based methods as comparison. our approach
achieves state-of-the-art performance on both datasets, demonstrating the
effectiveness of the proposed framework. we also conduct extensive ablation
studies to analyze the impact of various components in our framework and provide
reasoning for each design.
suppose that we have a sample x 0 from distribution d(x|y), and y is the
condition. the condition y can be various, like different modality [20],
inpainting [3] and low resolution images [17]. in this work, y ∈ {y 0 , y 1 }
indicates the binary classification label, like brain ct scans without tumor vs.
with tumor. we then gradually add guassian noise to the original image sample
with different level t ∈ {0, 1, ..., t } aswith fixed variance {β 1 , β 2 , ...,
β t }, x t can be explicitly expressed by x 0 ,where α t := 1β t , ᾱt := t s=1 α
s . then a conditional u-net [16] θ (x, t, y) is trained to approximate the
reverse denoising process,the variance μ σ can be learnable parameters or a
fixed set of scalars, and both settings achieve comparable results in [9]. as
for the mean, after reparameterization with x t = √ ᾱt x 0 + √ 1ᾱt for ∼ n (0,
i), the loss function can be simplified as:as for how to infuse binary condition
y in the u-net, we follow the strategy in [6], using a embedding projection
function e = f (y), f ∈ r → r n , with n being the embedding dimension. then the
condition embedding is added to feature maps in different blocks. after training
the denoising model, tashiro et al. [3] proved that the network can yield the
desired conditional distribution d(x|y) given condition y.algorithm 1.
generation of wsss prediction mask using differentiate conditional model with
ddim sampling input: input image x with label y 1 , noise level q, inference
stage r, noise predictor θ , τ output: prediction mask of label y 1 for all t
from 1 to q do) end for return a
inspired by the finding in [2] that the denoising model extracts semantic
information when performing reverse diffusion process, we aim to get
segmentation mask from the sample generated by single or just several reverse
markov steps with ddim [19]. the reason for using ddim is that one can generate
a sample x t-1 from x t deterministically if removing the random noise term
via:when given the same images at noise level q, but with different conditions,
the noises predicted by the network θ are supposed to reflect the localization
of target objects, that is equivalently x t-1 (x t , t, y 1 )x t-1 (x t , t, y 0
) . this idea is quite similar to [22] if we keep sampling x t-1 until x 0 .
however, it is not guaranteed that the condition y 0 does not change background
areas besides the object needed to be localized. therefore, in order to minimize
the error caused by the generation process, we propose to visualize the
sensitivity of x t-1 (x t , t, y 1 ) w.r.t condition y 1 , that is ∂xt-1 ∂y .
since the embedding projection function f (•) is not differentiable, we
approximate the gradient using the finite difference method:) in which, τ is the
moving weight from f (y 1 ) towards f (y 0 ). the weight τ can not be too close
to 1, otherwise there is no noticeable gap between x t-1 and x t-1 , and we find
τ = 0.95 gives the best performance. algorithm 1 shows the detailed workflow of
obtaining the segmentation mask of samples with label y 1 . notice that we
iterate the process (5) for r steps, and the default r in this work is set as
10, much smaller than q = 400. the purpose of r is to amplify the change of x
t-1 since the condition does not change the predicted noise a lot in one step.in
addition, we find that the guidance of additional classifier can further boost
the wsss task, by passing the gradient ˆ ← θ (x t )s √ 1ᾱt ∇ xt logp φ (y|x t ),
where p φ is the classifier and s is the gradient scale. then, in algorithm 1, ˆ
1 and ˆ 0 have additional terms guided by gradient of y 1 and y 0 ,
respectively. the ablation studies of related hyperparameters can be seen in
sect. 3.2.
brain tumor segmentation. brats (brain tumor segmentation challenge) [1]
contains 2,000 cases of 3d brain scans, each of which includes four different
mri modalities as well as tumor segmentation ground truth. in this work, we only
use the flair channel and treat all types of tumor as one single class. we
divide the official training set into 9:1 for training and validation purpose,
and evaluate our method on the official validation set. for preprocessing, we
slice each volume into 2d images, following the setting in [5]. then the total
number of slices in training set is 193,905, and we report metrics on the 5802
positive samples in the test set kidney segmentation. this task is conducted on
dataset from isbi 2019 chaos challenge [12], which contains 20 volumes of
t2-spir mr abdominal scans. chaos provides pixel-wise annotation for several
organs, but we focus on the kidney. we split the 20 volumes into four folds for
cross-validation, and then decompose 3d volumes to 2d slices in every fold. in
the test stage, we remove slices with area of interest taking up less than 5% of
the total area in the slice, in order to avoid the influence of extreme cases on
the average results.only classification labels are used during training the
diffusion models, and segmentation masks are used for evaluation in the test
stage. for both datasets, we repeat the evaluation protocols for four times and
report the average metrics and their standard deviation on test set.
as for model architecture, we use the same setting as in [8]. the diffusion
model is based on u-net with encoder and decoder consisting of resnet blocks. we
implement two different versions of the proposed method, one without classifier
guidance and one with it. to differentiate the two in the experiments, we
continue to call them former cdm, and the latter cg-cdm. the classifier used in
cg-cdm is the same as the encoder of the diffusion model. we stop training the
diffusion model after 50,000 iterations or 7 days, and the classifier is trained
for 20,000 iterations. we choose adamw as the optimizer with learning rate being
1e-5 and 5e-5 for diffusion model and classifier. the batch sizes for both
datasets are 2. the implementation of all methods in this work is based on
pytorch library, and all experiments are run on a single nvidia rtx 2080ti.
dice↑ miou↑ hd95↓ infer time cam gradcam [18] 0.235 ± 0.075 0.149 ± 0.051 44.4 ±
8.9 3.79 s gradcam++ [4] 0.281 ± 0.084 0.187 ± 0.059 32.6 ± 6.0 3.59 s scorecam
[21] 0.303 ± 0.053 0.202 ± 0.039 32.7 ± 2.1 27.0 s layercam [11] types methods
dice↑ miou↑ hd95↓cam gradcam [18] 0.105 ± 0.017 0.059 ± 0.010 33.9 ± 5.1
gradcam++ [4] 0.147 ± 0.016 0.085 ± 0.010 28.5 ± 4.5 scorecam [21] 0.135 ± 0.024
0.078 ± 0.015 32.1 ± 6.7 layercam [11] fsl n/a 0.847 ± 0.011 0.765 ± 0.023 3.6 ±
1.7
comparison with state of the arts. we benchmark our methods against previous
wsss works on two datasets in table 1 & 2, in terms of dice score, mean
intersection over union (miou), and hausdorff distance (hd95). for cam based
methods, we include the classical gradcam [18] and gradcam++ [4], as well as two
more recent methods, scorecam [21] and layercam [11]. the implementation of
these cam approaches is based on the repository [7]. for diffusion based
methods, we include the only diffusion model for medical image segmentation in
the wsss literature, namely cg-diff [22]. we follow the default setting in [22],
setting noise level q = 400 and gradient scale s = 100. we also present the
results under the fully supervised learning setting, which is the upper bond of
all wsss methods (fig. 2).from the results, we can make several key
observations. firstly, our proposed method, even without classifier guidance,
outperform all other wsss methods including the classifier guided diffusion
model cg-diff on both datasets for all three metrics. when classifier guidance
is provided, the improvement gets even bigger, and cg-cdm can beat other methods
regarding segmentation accuracy. secondly, all wsss methods have performance
drop on kidney dataset compared with brats dataset. this demonstrates that the
kidney segmentation task is a more challenging task for wsss than brain tumor
task, which may be caused by the small training size and diverse appearance
across slices in the chaos dataset. time efficiency. regarding inference time
for different methods, as shown in table 1, both cdm and cg-cdm are much faster
than cg-diff. the default noise level q is set as 400 for all diffusion model
approaches, and our methods run 10 iterations during the denoising steps. for
all cam-based approaches, we add augmentation smooth and eigen smooth suggested
in [7] to reduce noise in the prediction mask. this post-processing greatly
increases the inference time. without the two smooth methods, the inference time
for gradcam is 0.031 s, but the segmentation accuracy is significantly degraded.
therefore, considering both inference time and performance, our method is a
better option than cam for wsss.ablation studies. there are several important
hyperparameters in our framework, noise level q, number of iterations r, moving
weight τ , and gradient scale s. the default setting is cg-cdm on brats dataset
with q = 400, r = 10, τ = 0.95, and s = 10. we evaluate the influence of one
hyperparameter at a time by keeping other parameters at their default values. as
illustrated in fig. 3, a few observations can be made: (1) either too large or
too small noise level can negatively influence the performance. when q is small,
most spatial information is still kept in x t and the predicted noise by
diffusion model contains no semantic knowledge. when q is large, most of the
spatial information is lost and the predicted noise can be distracted from
original structure. meanwhile, larger number of iterations can lightly improve
the dice score at the beginning. when r gets too high, the error in the
background is also accumulated after too many iterations. (2) we try different τ
in the range (0, 1.0). small τ leads to more noises in the background when
calculating the difference in different conditions. on the other hand, as τ gets
close to 1, the difference between x t-1 and x t-1 becomes minor, and the
gradient map mainly comes from the guidance of the classifier, making
localization not so accurate. thus, τ = 0.95 becomes the optimal choice for this
task. (3) as for gradient scale, fig. 3 shows that before s = 100, larger
gradient scale can boost the cdm, because at this time, the gradient from the
classifier is at the same magnitude as the difference caused by the changed
condition embedding. when the guidance of the classifier becomes dominant, the
dice score gets lower as the background is distorted by too large gradients.
in this paper, we present a novel weakly supervised semantic segmentation
framework based on conditional diffusion models. fundamentally, the essence of
generative approaches on wsss is maximizing the change in class-related areas
while minimizing the noise in the background. our methods are designed around
this rule to enhance the state-of-the-art. first, existing work that utilizes a
trained classifier to remove target objects leads to unpredictable distortion in
other areas, thus we decide to iterate the reverse denoising process for as few
steps as possible. second, to amplify the difference caused by different
conditions, we extract the semantic information from gradient of the noise
predicted by the diffusion model. finally, this rule also applies to all other
designs and choice of hyperparameters in our framework. when compared with
latest wsss methods on two public medical image segmentation datasets, our
method shows superior performance regarding both segmentation accuracy and
inference efficiency.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8 72.
glioma is one of the most common malignant brain tumors with varying degrees of
invasiveness [1]. brain tumor semantic segmentation of gliomas based on 3d
spatially aligned magnetic resonance imaging (samm-bts) is crucial for accurate
diagnosis and treatment planning. unfortunately, radiologists suffer from
spending several hours manually performing the samm-bts task in clinical
practice, resulting in low diagnostic efficiency. in addition, manual
delineation requires doctors to have high professionalism. therefore, it is
necessary to design an efficient and accurate glioma lesion segmentation
algorithm to effectively alleviate this problem and relieve doctors' workload
and improve radiotherapy quality.with the rise of deep learning, researchers
have begun to study deep learning-based image analysis methods [2,37].
specifically, many convolutional neural network-based (cnn-based) models have
achieved promising results [3][4][5][6][7][8]. compared with natural images,
medical image segmentation often requires higher accuracy to make subsequent
treatment plans for patients. u-net reaches an outstanding performance on
medical image segmentation by combining the features from shallow and deep
layers using skipconnection [9][10][11]. based on u-net, brugger. et al. [12]
proposed a partially reversible u-net to reduce memory consumption while
maintaining acceptable segmentation results. pei et al. [13] explored the
efficiency of residual learning and designed a 3d resunet for multi-modal brain
tumor segmentation. however, due to the lack of global understanding of images
for convolution operation, cnn-based methods struggle to model the dependencies
between distant features and make full use of the contextual information [14].
but for semantic segmentation tasks whose results need to be predicted at
pixel-level or voxel-level, both local spatial details and global dependencies
are extremely important.in recent years, models based on the self-attention
mechanism, such as transformer, have received widespread attention due to their
excellent performance in natural language processing (nlp) [15]. compared with
convolution operation, the self-attention mechanism is not restricted by local
receptive fields and can capture long-range dependencies. many works
[16][17][18][19] have applied transformers to computer vision tasks and achieved
favorable results. for classification tasks, vision transformer (vit) [19] was a
groundbreaking innovation that first introduced pure transformer layers directly
across domains. and for semantic segmentation tasks, many methods, such as setr
[20] and segformer [21], use vit as the direct backbone network and combine it
with a taskspecific segmentation head for prediction results, reaching excellent
performance on some 2d natural image datasets. for 3d medical image
segmentation, vision transformer has also been preferred by researchers. a lot
of robust variants based on transformer have been designed to endow u-net with
the ability to capture contextual information in long-distance dependencies,
further improving the semantic segmentation results of medical images
[22][23][24][25][26][27]. wang et al. [25] proposed a novel framework named
transbts that embeds the transformer in the bottleneck part of a 3d u-net
structure. peiris et al. [26] introduced a 3d swin-transformer [28] to
segmentation tasks and first incorporated the attention mechanism into
skip-connection.while transformer-based models have shown effectiveness in
capturing long-range dependencies, designing a transformer architecture that
performs well on the samm-bts task remains challenging. first, modeling
relationships between 3d voxel sequences is much more difficult than 2d pixel
sequences. when applying 2d models, 3d images need to be sliced along one
dimension. however, the data in each slice is related to three views, discarding
any of them may lead to the loss of local information, which may cause the
degradation of performance [29]. second, most existing mri segmentation methods
still have difficulty capturing global interaction information while effectively
encoding local information. moreover, current methods just stack modalities and
pass them through a network, which treats each modality equally along the
channel dimension and may ignore the contribution of different modalities. to
address the above limitations, we propose a novel encoder-decoder model, namely
dbtrans, for multi-modal medical image segmentation. in the encoder, two types
of window-based attention mechanisms, i.e., shifted window-based multi-head self
attention (shifted-w-msa) and shuffle window-based multi-head cross attention
(shuffle-w-mca), are introduced and applied in parallel to dual-branch encoder
layers, while in the decoder, in addition to shifted-w-msa mechanism, shifted
window-based multi-head cross attention (shifted-w-mca) is designed for the
dual-branch decoder layers. these mechanisms in the dual-branch architecture
greatly enhance the ability of both local and global feature extraction.
notably, dbtrans is designed for 3d medical images, avoiding the information
loss caused by data slicing.the contributions of our proposed method can be
described as follows: 1) based on transformer, we construct dual-branch encoder
and decoder layers that assemble two attention mechanisms, being able to model
close-window and distant-window dependencies without any extra computational
cost. 2) in addition to the traditional skipconnection structure, in the
dual-branch decoder, we also establish an extra path to facilitate the decoding
process. we design a shifted-w-mca-based global branch to build a bridge between
the decoder and encoder, maintaining affluent information of the segmentation
target during the decoding process. 3) for the multi-modal data adopt in the
task of samm-bts, we improve the channel attention mechanism in se-net by
applying se-weights to features from both branches in the encoder and decoder
layers. by this means, we implicitly consider the importance of multiple mri
modalities and two window-based attention branches, thereby strengthening the
fusion effect of the multi-modal information from a global perspective.
figure 1 shows the overall structure of the proposed dbtrans. it is an
end-to-end framework that has a 3d patch embedding along with a u-shaped model
containing an encoder and a decoder. the model takes mri data of d×h ×w ×c with
four modalities stacked along channel dimensions as the input. the 3d patch
embedding converts the input data to feature embedding e 1 ∈ r d 1 ×h 1 ×w 1 ×c
1 which will be further processed by encoder layers. at the tail of the decoder,
the segmentation head takes the output of the last layer and generates the final
segmentation result of d × h × w × k.
as shown in fig. 1(b), the encoder consists of four dual-branch encoder layers
(one bottleneck included). each encoder layer contains two consecutive encoder
blocks and a 3d patch merging to down-sample the feature embedding. note that
there is only one encoder block in the bottleneck. the encoder block includes a
dual-branch architecture with the local feature extraction branch, the global
feature extraction branch, and the channel-attention-based dual-branch fusion
module. after acquiring the embedding 2] which are then separately fed into two
branches.
] is fed into the local branch in the encoder block. in the process of window
partition (denoted as wp), e i 1 is split into non-overlapping windows after a
layer normalization (ln) operation to obtain the window matrix m i 1 . since we
set m as 2, the input of 4×4×4 size is uniformly divided into 8 windows of 2 × 2
× 2. following 3d swin-transformer [27], we introduce msa based on the
shifted-window partition to the second block in an encoder layer. during window
partition of shifted-w-msa, the whole feature map is shifted by half of the
window size, i.e., m 2 , m 2 , m 2 . after the window partition, w -msa is
applied to calculate multi-head self-attention within each window. specifically,
for window matrix m i 1 , we first apply projection matrices w i q ,w i k , and
andv i 1 matrices (the process is denoted as proj). after projection, multi-head
selfattention calculation is performed on q i 1 ,k i 1 , and v i 1 to get the
attention score of every window. finally, we rebuild the feature map from the
windows, which serves as the inverse process of the window partition. after
calculating the attention score, other basic components in transformer are also
employed, that is, layer normalization (ln), as well as a multi-layer perceptron
(mlp) with two fully connected layers and a gaussian error linear unit (gelu).
residual connection is applied after each module. the whole process can be
expressed as follows:where ẑi 1 represents the attention score after w -msa
calculation, " shifted-" represents that we use the shifted-window partition and
restoration in the second block of every encoder layer. o i 1 is the final
output of the local branch. shuffle-w-mca-based global branch. through the local
branch, the network still cannot model the long-distance dependencies between
non-adjacent windows in the same layer. thus, shuffle-w-mca is designed to
complete the complementary task. after the window-partition process that
converts the embedding, inspired by shufflenet [35], instead of moving the
channels, we propose to conduct shuffle operations on the patches in different
windows. the patches at the same relative position in different windows are
rearranged together in a window, and their position is decided by the position
of the window they originally belong to. then, this branch takes the query from
the local branch, while generating keys and values from m i 2 to compute
cross-attention scores. such a design aims to enable the network to model the
relationship between a window and other distant windows. note that we adopt the
projection function proj from the local branch, indicating that the weights of
the two branches are shared. through the shuffle operation, the network can
generate both local and global feature representations without setting
additional weight parameters. the whole process can be formulated as follows:(in
the second block, we get the final output o i 2 of the layer through the same
process.
as shown in fig. 1(c), the decoder takes the output of the encoder as the input
and generates the final prediction through three dual-branch decoder layers as
well as a segmentation head. each decoder layer consists of two consecutive
decoder blocks and a 3d patch expanding layer. as for the j-th decoder layer (j
∈ [1, 3]), the embedding d j ∈ r d j ×h j ×w j ×c j is also divided into the
feature maps 2] . we further process d j 1 , d j 2 using a dual-branch
architecture similar to that of the encoder, but with an additional global
branch based on shifted-w-mca mechanism. finally, the segmentation head
generates the final segmentation result of d × h × w × k, where k represents the
number of classes. the local branch based on shifted-w-msa is the same as that
in the encoder and will not be introduced in this section.
apart from employing shifted-w-msa to form the local branch of the decoder
layer, we design a novel shifted-w-mca mechanism for the global branch to ease
the information loss during the decoding process and take full advantage of the
features from the encoder layers. the global branch receives the query matrix
from the split feature map d j 2 ∈ r d j ×h j ×w j ×[c j /2] , while receiving
key and value matrices from the encoder block in the corresponding stage,
denoted as q j 2 , k e i , and v e i . the process of shifted-w-mca can be
formulated as follows:where ẑj 2 denotes the attention score after mca
calculation, o j 2 denotes the final output of the global branch.
as shown in fig. 1(b) and fig. 1(c), the dual-branch fusion module is based on
the channel attention. for the block of the m -th (m ∈ [1,3]) encoder or decoder
layer, the dual-branch fusion module combines the features 2] from the two
extraction branches, obtaining a feature map filled with abundant multiscale
information among different modalities. subsequently, the dependencies between
the feature channels within the individual branches are implicitly modeled with
the se-weight assignment first proposed in se-net [30]. different from se-net,
we dynamically assign weights for both dual-branch fusion and multi-modal
fusion. the process of obtaining the attention weights can be represented as the
formula (5) below:whereis the attention weight of a single branch. then, the
weight vectors of the two branches are re-calibrated using a softmax function.
finally, the weighted channel attention is multiplied with the corresponding
scale feature map to obtain the refined output feature map with richer
multi-scale feature information:where " " represents the operation of
element-wise multiplication and "cat" represents the concatenation. the
concatenated output o, serving as the dual-branch output of a block in the
encoder or decoder, implicitly integrates attention interaction information
within individual branches across different channels/modalities, as well as
across different branches.
for the loss function, the widely used cross entropy (ce) loss and dice loss
[32] are introduced to train our dbtrans. here we use parameter γ to balance the
two loss parts. our network is implemented based on pytorch, and trained for 300
epochs using a single rtx 3090 with 24g memory. the weights of the network were
updated using the adam optimizer, the batch size was set to 1, and the initial
learning rate was set to 1 × 10 -4 . a cosine decay scheduler was used as the
adjustment strategy for the learning rate during training. we set the embedding
dimensions c 0 as 144. following previous segmentation methods, the parameter γ
is set to 0.5.
datasets. we use the multimodal brain tumor segmentation challenge (brats 2021
[33,34,38]) as the benchmark training set, validation set, and testing set. we
divide the 1251 scans provided into 834, 208, and 209 (in a ratio of 2:1:1),
respectively for training and testing. the ground truth labels of gbm
segmentation necrotic/active tumor and edema are used to train the model. the
brats 2021 dataset reflects real clinical diagnostic species and has four
spatially aligned mri modality data, namely t1, t1ce, t2, and flair, which are
obtained from different devices or according to different imaging protocols. the
dataset contains three distinct sub-regions of brain tumors, namely peritumoral
edema, enhancing tumor, and tumor core. the data augmentation includes random
flipping, intensity scaling and intensity shifting on each axis with
probabilities set to 0.5, 0.1 and 0.1, respectively.comparative experiments. to
evaluate the effectiveness of the proposed dbtrans, we compare it with the
state-of-the-art brain tumor segmentation methods including six
transformer-based networks swin-unet [27], transbts [25], unetr [22], nnformer
[23], vt-unet-b [26], nestedformer [36] as well as the most basic cnn network 3d
u-net [31] as the baseline. during inference, for any size of 3d images, we
utilize the overlapping sliding windows technique to generate multi-class
prediction results and take average values for the voxels in the overlapping
region. the evaluation strategy adopted in this work is consistent with that of
vt-unet [26]. for other methods, we used the corresponding hyperparameter
configuration mentioned in the original papers and reported the average metrics
over 3 runs. ablation study. to further verify the contribution of each module,
we establish the ablation models based on the modules introduced above. note
that, dp-e represents the dual-branch encoder layer, while db-d represents the
dual-branch decoder layer. when the dual-branch fusion is not included, we do
not split the input, and simply fuse the features from the two branches using a
convolution layer. in all, there are 5 models included in this ablation study:
2, after applying our proposed dual-branch encoder and decoder layers to the
baseline model, the average dice score notably increased by 2.13. subsequently,
applying the dual-branch fusion module also prominently contributes to the
performance of the model by an improvement of 0.83 on the dice score. notably,
our dual-branch designs achieve higher performance while also reducing the
number of parameters required. this is because we split the original feature
embedding into two parts, thus the channel dimensions of features in two
branches are halved.
in this paper, we innovatively proposed an end-to-end model named dbtrans for
multimodal medical image segmentation. in dbtrans, first, we well designed the
dual-branch structures in encoder and decoder layers with shifted-w-msa,
shuffle-w-mca, and shifted-w-mca mechanisms, facilitating feature extraction
from both local and global views. moreover, in the decoder, we establish a
bridge between the query of the decoder and the key/value of the encoder to
maintain the global context during the decoding process for the segmentation
target. finally, for the multi-modal superimposed data, we modify the channel
attention mechanism in se-net, focusing on exploring the contribution of
different modalities and branches to the effective information of feature maps.
experimental results demonstrate the superiority of dbtrans compared with the
state-of-the-art medical image segmentation methods.
convolutional neural networks (cnns) have been widely used for medical image
segmentation because of their speed and accuracy [11,18]. nevertheless, given
the local receptive fields of convolutional layers, long-range spatial
correlations are mainly captured through consecutive convolutions and pooling.
for computationally demanding 3d segmentation, the receptive fields and abstract
levels can be more limited than in 2d as fewer layers can be used. to balance
between computational complexity and network capability, input size reductions
by image downsampling and patch-wise training are common approaches. however,
cnns trained with downsampled images can be suboptimal when applied on the
original resolution, and the receptive field of patch-wise training can be
largely reduced depending on the patch size.with the introduction of
transformers [24] and their vision alternatives [6,19], the self-attention
mechanism for long-range dependencies has been adopted to medical image
segmentation with promising results [5,7,9,27]. these approaches form a sequence
of samples by either using the pixel values of lowresolution features [7,27] or
by dividing an image into smaller patches [5,9], and the multi-head attention is
used to learn the dependencies among samples. although these self-attention
approaches allow capturing of long-range dependencies, as the computational
requirements are proportional to sequence lengths and patch sizes which are
proportional to image sizes, size-reduction approaches are needed for large
images especially in 3d.as image size reduction is usually required for large
images, it is desirable to have a model that is robust to training image
resolution so that the trained model can be applied to higher-resolution images
with decent accuracy. furthermore, as self-attention of transformers allows
better expressiveness through high-order channel and sample mixing [15,23],
incorporating self-attention in an efficient way can be beneficial. to gain
these advantages, here we propose the hartleymha model which is a
resolution-robust and parameter-efficient network architecture with
frequency-domain self-attention for 3d image segmentation. this model is based
on the fourier neural operator (fno) [17], which is a deep learning model that
learns mappings between functions in partial differential equations (pdes) and
has the appealing properties of zero-shot super-resolution and global receptive
field. our contributions include: 1. to utilize the fno for computationally
expensive 3d segmentation, we modify it by using the hartley transform with
shared model parameters in the frequency domain. residual connections [10] and
deep supervision [14] are also introduced. these reduce the number of model
parameters by orders of magnitude and improve accuracy. we call it the hnoseg
model. 2. as only low-frequency components are required for decent segmentation
results, multi-head self-attention can be efficiently applied in the frequency
domain. this allows high-order combination of features to improve the
expressiveness of the model. we call it the hartleymha model. 3. we compare our
proposed models with other models on different training image resolutions to
study their robustness. this provides useful insights that are usually
unavailable in other studies.experimental results on the brats'19 dataset
[2,3,22] show that the proposed models have superior robustness to training
image resolution than other tested models with less than 1% of their model
parameters.
fno is a deep learning model for learning mappings between functions in pdes
without the pdes provided [17]. by formulating the solution in the continuous
space based on the green's function [16], fno can learn a single set of model
parameters for multiple resolutions. for computationally expensive 3d
segmentation, such zero-shot super-resolution capability is advantageous as a
model trained with lower-resolution images can be applied on higher-resolution
images with decent accuracy. the neural operator is formulated as iterative
updates:where u t (x) ∈ r du t is a function of x. w ∈ r du t+1 ×du t is a
learnable linear transformation and σ accounts for normalization and activation.
in our work, d ⊂ r 3 represents the 3d imaging space, and u t (x) are the
outputs of hidden layers with d ut channels. k is the kernel integral operator
with κ ∈ r du t+1 ×du t a learnable kernel function. as (ku t ) is a
convolution, it can be efficiently solved by the convolution theorem which
states that the fourier transform (f) of a convolution of two functions is the
pointwise product of their fourier transforms:therefore, each pointwise product
at k is realized as a matrix multiplication. when the fast fourier transform is
used in implementation, k ∈ n 3 are non-negative integer coordinates, and each k
has a learnable r(k). as mainly low-frequency components are required for image
segmentation, only k i ≤ k max,i corresponding to the lower frequencies in each
dimension i are used to reduce model parameters and computation time.
as the fno requires complex number operations in the frequency domain, the
computational requirements such as memory and floating point operations are
higher than with real numbers. therefore, we use the hartley transform instead,
which is an integral transform alternative to the fourier transform [8]. the
hartley transform (h) converts real-valued functions to real-valued functions,
which is related to the fourier transform as (hf ) = real(ff ) -imag(ff ). the
convolution theorem of discrete hartley transform is more complicated [4], and
the kernel integration in (1) becomes: 3 is the size of the frequency domain. r
and û are n -periodic in each dimension 1 . similar to using (2), the models
built using (3) have tens of million parameters even with small k max (e.g.,
(14,14,10)). therefore, instead of using a different r(k) at each k, we use the
same (shared) r for all k and (3) becomes:this is equivalent to applying a
convolution layer with the kernel size of one in the frequency domain. we find
that using (4) simplifies the computation and largely reduces the number of
parameters without affecting the accuracy.
as real instead of complex numbers are used in (4), multi-head attention in [24]
can be applied in the frequency domain for high-order feature combination.as k
max can be much smaller than the image size for image segmentation, the sequence
length (number of voxels) can be largely reduced. with (4), the query, key, and
value matrices (q, k, v ) of self-attention can be computed as:wherealthough n f
can be relatively small, the computation and memory requirements of computing qk
t can still be demanding. for example, k max = (14, 14, 10) corresponds to an
attention matrix with around 246m elements. to remedy this, for each q, k, and v
, we group the feature vectors with a patch size of 2 × 2 × 2 voxels in the
frequency domain and their matrix sizes becomethis reduces the number of
elements in qk t by 64 times. the self-attention can then be computed as:where
selu represents the scaled exponential linear unit [13]. similar to [21], we
find that using softmax in self-attention results in suboptimal segmentations,
thus the selu was chosen after testing with multiple activations. furthermore,
we find that position encoding is unnecessary. the result of ( 6) can be
rearranged back to the original shape in the frequency domain so that the
inverse hartley transform can be applied. the multi-head attention can be used
with (6).
figure 1 shows the network architecture. we call it hnoseg with the hno blocks
and hartleymha with the hartley mha blocks. different from the fno in [17],
residual connections [10] and deep supervision [14] are used to improve the
training stability, convergence, and accuracy. as the batch size is usually
small for memory demanding 3d segmentation, layer normalization (ln) is used
[1]. the selu [13] is used as the activation function, and the softmax function
is used to produce the final prediction scores. similar to the fourier
transform, the hartley transform provides a global receptive field as all voxels
are used to compute the value at each k, thus pooling is not required. as using
the original image resolution usually results in out-of-memory errors in 3d
segmentation, downsampling the inputs and then upsampling the predictions may be
required. instead of using traditional image resampling methods, we use a
convolutional layer with the kernel size and stride of two right after the input
layer, and replace the output convolutional layer by a transposed convolutional
layer with the kernel size and stride of two (red blocks in fig. 1). in this
way, the model can learn the optimal resampling approach. in the experiments, k
max = (14, 14, 10) so that it can be used with the lowest tested training
resolution of 60 × 60 × 39. other hyperparameters such as d ut+1 , n h , and n b
were obtained empirically for decent segmentations when training with the
original image resolution. as each hno block and hartley mha block can be
implemented as a deep-learning layer in commonly used libraries, they can be
easily adopted by other architectures.
the images of different modalities are stacked along the channel axis to provide
a multi-channel input. as the intensity ranges across modalities can be quite
different, intensity normalization is performed on each image of each modality.
image augmentation with rotation (axial, ±30 • ), shifting (±20%), and scaling
([0.8, 1.2]) is used and each image has an 80% chance to be transformed. the
adamax optimizer [12] is used with the cosine annealing learning rate scheduler
[20], with the maximum and minimum learning rates as 10 -2 and 10 -3 ,
respectively. the pearson's correlation coefficient loss is used as it is robust
to learning rate for image segmentation [25], and it consistently outperformed
the dice loss and weighted cross-entropy in our experiments. an nvidia tesla
p100 gpu with 16 gb memory is used with a batch size of one and 100 epochs, and
keras in tensorflow 2.6.2 is used for implementation. note that small batch
sizes are common in 3d segmentation given the high memory requirement.
the dataset of brats'19 with 335 cases of gliomas was used, each with four
modalities of t1, post-contrast t1, t2, and t2-flair images with 240 × 240 × 155
voxels [3]. there is also an official validation dataset of 125 cases in the
same format without given annotations. models were trained with images
downsampled by different factors (1, 2, 3, and 4) to study the robustness to
image resolution. in training, we split the training dataset (335 cases) into
90% for training and 10% for validation. in testing, each model was tested on
the official validation dataset (125 cases) with 240 × 240 × 155 voxels
regardless of the downsampling factor. the predictions were uploaded to the
cbica image processing portal3 for the results statistics of the "whole tumor"
(wt), "tumor core" (tc), and "enhancing tumor" (et) regions [3]. we compare our
proposed hnoseg and hartleymha models with three other models:1. v-net-ds [26]:
a v-net with deep supervision representing the commonlyused encoding-decoding
architectures. 2. utnet [7]: a u-net enhanced by the transformer's attention
mechanism. 3. fno [17]: original fno without shared parameters, residual
connections, and deep supervision. the same hyperparameters as hnoseg were used.
the learnable resampling approach in sect. 2.4 was applied to all models. note
that our goal is not competing for the best accuracy but studying the robustness
to image resolution. although only the results of a dataset are shown because of
the page limit, the characteristics of the proposed models can be demonstrated
through this challenging multi-modal brain tumor segmentation problem.
figure 2 and table 1 show comparisons of resolution robustness among tested
models, and table 2 shows the computational costs during inference. at the
original resolution, v-net-ds and utnet outperformed hnoseg and hartleymha by
less than 3% in the dice coefficient on average, but hnoseg and hartleymha only
had less than 50k model parameters which were less than 1% of v-net-ds and
utnet. fno performed worst with the most parameters (144.5m). as the resolution
decreased, the accuracies of v-net-ds and utnet decreased almost linearly with
the downsampling factor, while hnoseg and hartleymha were more robust. when the
downsampling factor changed from 1 to 3, the average dice coefficients of
v-net-ds and utnet decreased by more than 14.8%, while those of hnoseg and
hartleymha only decreased by less than 2.5%. similar trends can be observed for
the 95% hausdorff distance, except that fno performed surprisingly well in this
aspect. hartleymha performed better overall than hnoseg. note that we fixed k
max for the consistency among models in the experiments, which can be adjusted
for better results in other situations.for computation cost, table 2 shows that
v-net-ds and utnet had shorter inference times than hnoseg and hartleymha,
though all models used less figure 3 shows the visual comparisons of the
segmentation results on an unseen case. consistent to fig. 2 with such superior
robustness to image resolution, hnoseg and hart-leymha can be trained with
lower-resolution images using fewer computational resources to provide decent
segmentation results on the original resolution during inference. while
hartleymha performed better than hnoseg in general, their similar performance is
consistent with the findings in [15,23] that self-attention is sufficient for
good performance but is not crucial. on the other hand, as the use of efficient
self-attention improves the expressiveness of the hartley mha block, fewer
layers can be used to reduce the overall computational costs.
in this paper, based on the idea of fno which has the properties of zeroshot
super-resolution and global receptive field, we propose the hnoseg and
hartleymha models for resolution-robust and parameter-efficient 3d image
segmentation. hnoseg is fno improved by the hartley transform, residual
connections, deep supervision, and shared parameters in the frequency domain. we
further extend this concept for efficient multi-head attention in the frequency
domain as hartleymha. experimental results show that hnoseg and hartleymha had
similar accuracies as other tested segmentation models when trained with the
original image resolution, but had superior performance when trained with images
of much lower resolutions. hartleymha performed slightly better than hnoseg and
ran faster with less memory. with these advantages, hartleymha can be a
promising alternative for 3d image segmentation especially when computational
resources are limited.
medical image segmentation (mis) is a crucial component in medical image
analysis, which aims to partition an image into distinct regions (or segments)
that are semantically related and/or visually similar. this process is essential
for clinicians to, among others, perform qualitative and quantitative
assessments of various anatomical structures or pathological conditions and
perform imageguided treatments or treatment planning [2]. vision transformers
(vits), with their inherent ability to model long-range dependencies, have
recently been considered a promising technique to tackle mis. they process
images as sequences of patches, with each patch having a global view of the
entire image. this enables a vit to achieve improved segmentation performance
compared to traditional convolutional neural networks (cnns) on plenty of
segmentation tasks [16]. however, due to the lack of inductive biases, such as
weight sharing and locality, vits are more data-hungry than cnns, i.e., require
more data to train [31]. meanwhile, it is common to have access to multiple,
diverse, yet small-sized datasets (100 s to 1000 ss of images per dataset) for
the same mis task, e.g., ph2 [25] and isic 2018 [11] in dermatology, lits [6]
and chaos [18] in liver ct, or oasis [24] and adni [17] in brain mri. as each
dataset alone is too small to properly train a vit, the challenge becomes how to
effectively leverage the different datasets. various strategies have been
proposed to address vits' data-hunger (table 1), mainly: adding inductive bias
by constructing a hybrid network that fuses a cnn with a vit [39], imitating
cnns' shifted filters and convolutional operations [7], or enhancing spatial
information learning [22]; sharing knowledge by transferring knowledge from a
cnn [31] or pertaining vits on multiple related tasks and then fine-tuning on a
down-stream task [37]; increasing data via augmentation [34]; and non-supervised
pre-training [8]. nevertheless, one notable limitation in these approaches is
that they are not universal, i.e., they rely on separate training for each
dataset rather than incorporate valuable knowledge from related domains. as a
result, they can incur additional training, inference, and memory costs, which
is especially challenging when dealing with multiple small datasets in the
context of mis tasks. multi-domain learning, which trains a single universal
model to tackle all the datasets simultaneously, has been found promising for
reducing computational demands while still leveraging information from multiple
domains [1,21]. to the best of our knowledge, multi-domain universal models have
not yet been investigated for alleviating vits' data-hunger.given the
inter-domain heterogeneity resulting from variations in imaging protocols,
scanner manufacturers, etc. [4,21], directly mixing all the datasets for
training, i.e., joint training, may improve a model's performance on one dataset
while degrading performance on other datasets with non-negligible unrelated
domain-specific information, a phenomenon referred to as negative knowledge
transfer (nkt) [1,38]. a common strategy to mitigate nkt in computer vision is
to introduce adapters aiding the model to adapt to different domains, i.e.,
multi-domain adaptive training (mat), such as domain-specific mechanisms
[21,26,32], and squeeze-excitation layers [28,35] (table 1). however, those mat
techniques are built based on cnn rather than vit or are scalable, i.e., the
models' size at the inference time increases linearly with the number of
domains.to address vits' data-hunger, in this work, we propose mdvit, a novel
fixedsize multi-domain vit trained to adaptively aggregate valuable knowledge
from multiple datasets (domains) for improved segmentation. in particular, we
introduce a domain adapter that adapts the model to different domains to
mitigate negative knowledge transfer caused by inter-domain heterogeneity.
besides, for better representation learning across domains, we propose a novel
mutual knowledge distillation approach that transfers knowledge between a
universal network (spanning all the domains) and additional domain-specific
network branches.we summarize our contributions as follows: (1) to the best of
our knowledge, we are the first to introduce multi-domain learning to alleviate
vits' data-hunger when facing limited samples per dataset. (2) we propose a
multi-domain vit, mdvit, for medical image segmentation with a novel domain
adapter to counteract negative knowledge transfer and with mutual knowledge
distillation to enhance representation learning. (3) the experiments on 4 skin
lesion segmentation datasets show that our multi-domain adaptive training
outperforms separate and joint training (st and jt), especially a 10.16%
improvement in iou on the skin cancer detection dataset compared to st and that
mdvit outperforms state-of-the-art data-efficient vits and multi-domain learning
strategies.
let x ∈ r h×w ×3 be an input rgb image and y ∈ {0, 1} h×w be its groundtruth
segmentation mask. training samples {(x, y )} come from m datasets, each
representing a domain. we aim to build and train a single vit that performs well
on all domain data and addresses the insufficiency of samples in any of the
datasets. we first introduce our baseline (base), a vit with hierarchical
transformer blocks (fig. 1-a). our proposed mdvit extends base with 1) a domain
adapter (da) module inside the factorized multi-head self-attention (mhsa) to
adapt the model to different domains (fig. 1-b,c), and 2) a mutual knowledge
distillation (mkd) strategy to extract more robust representations across
domains (fig. 1-d). we present the details of mdvit in sect. 2.1. base is a
u-shaped vit based on the architecture of u-net [27] and pyramid vits [7,19]. it
contains encoding (the first four) and decoding (the last four) transformer
blocks, a two-layer cnn bridge, and skip connections. as described in [19], the
ith transformer block involves a convolutional patch embedding layer with a
patch size of 3 × 3 and l i transformer layers with factorized mhsa in linear
complexity, the former of which converts a feature map x i-1 into a sequence of
patch embeddings z i ∈ r ni×ci , whereis the number of patches and c i is the
channel dimension. we use the same position embedding as [19] and skip
connections as [27]. to reduce computational complexity, following [19], we add
two and one cnn layer before and after transformer blocks, respectively,
enabling the 1st transformer block to process features starting from a lower
resolution: h 4 × w 4 . we do not employ integrated and hierarchical cnn
backbones, e.g., resnet, in base as data-efficient hybrid vits [33,39], to
clearly evaluate the efficacy of multi-domain learning in mitigating vits'
data-hunger.
mdvit consists of a universal network (spanning m domains) and m auxiliary
network branches, i.e., peers, each associated with one of the m domains. the
universal network is the same as base, except we insert a domain adapter (da) in
each factorized mhsa to tackle negative knowledge transfer. further, we employ a
mutual knowledge distillation (mkd) strategy to transfer domain-specific and
shared knowledge between peers and the universal network to enhance
representation learning. next, we will introduce da and mkd in detail.
in multi-domain adaptive training, some methods build domain-specific layers in
parallel with the main network [21,26,32]. without adding domain-specific
layers, we utilize the existing parallel structure in vits, i.e., mhsa, for
domain adaptation. the h parallel heads of mhsa mimic how humans examine the
same object from different perspectives [10]. similarly, our intuition of
inserting the da into mhsa is to enable the different heads to have varied
perspectives across domains. rather than manually designate each head to one of
the domains, guided by a domain label, mdvit learns to focus on the
corresponding features from different heads when encountering a domain. da
contains two steps: attention generation and information selection (fig.
1-c).attention generation generates attention for each head. we first pass a
domain label vector m (we adopt one-hot encoding m ∈ r m but other encodings are
possible) through one linear layer with a relu activation function to acquire a
domain-aware vector d ∈ r k r . k is the channel dimension of features from the
heads. we set the reduction ratio r to 2. after that, similar to [20], we
calculate attention for each head:where ψ is a softmax operation across heads
andinformation selection adaptively selects information from different heads.
after getting the featurefrom the hth head, we utilize a h to calibrate the
information along the channel dimension: ũh
distilling knowledge from domainspecific networks has been found beneficial for
universal networks to learn more robust representations [21,40]. moreover,
mutual learning that transfers knowledge between teachers and students enables
both to be optimized simultaneously [15]. to realize these benefits, we propose
mkd that mutually transfers knowledge between auxiliary peers and the universal
network. in fig. 1-d, the mth auxiliary peer is only trained on the mth domain,
producing output ŷ m , whereas the universal network's output is ŷ . similar to
[21], we utilize a symmetric dice loss l am mkd = dice( ŷ , ŷ m ) as the
knowledge distillation loss. each peer is an expert in a certain domain, guiding
the universal network to learn domain-specific information. the universal
network experiences all the domains and grasps the domain-shared knowledge,
which is beneficial for peer learning.each auxiliary peer is trained on a small,
individual dataset specific to that peer (fig. 1-d). to achieve a rapid training
process and prevent overfitting, particularly when working with numerous
training datasets, we adapt a lightweight multilayer perception (mlp) decoder
designed for vit encoders [36] to our peers' architecture. specifically,
multi-level features from the encoding transformer blocks (fig. 1-a) go through
an mlp layer and an up-sample operation to unify the channel dimension and
resolution to h 4 × w 4 , which are then concatenated with the feature involving
domain-shared information from the universal network's last transformer block.
finally, we pass the fused feature to an mlp layer and do an up-sample to obtain
a segmentation map.
similar to combo loss [29], base's segmentation loss combines dice and binary
cross entropy loss: l seg = l dice +l bce . in mdvit, we use the same
segmentation loss for the universal network and auxiliary peers, denoted as l u
seg and l a seg , respectively. the overall loss is calculated as follows.we set
both α and β to 0.5. l am seg does not optimize da to avoid interfering with the
domain adaptation learning. after training, we discard the auxiliary peers and
only utilize the universal network for inference.
we study 4 skin lesion segmentation datasets collected from varied sources: isic
2018 (isic) [11], dermofit image library (dmf) [3], skin cancer detection (scd)
[14], and ph2 [25], which contain 2594, 1300, 206, and 200 samples,
respectively. to facilitate a fairer performance comparison across datasets, as
in [4], we only use the 1212 images from dmf that exhibited similar lesion
conditions as those in other datasets. we perform 5-fold cross-validation and
utilize dice and iou metrics for evaluation as [33].implementation details: we
conduct 3 training paradigms: separate (st), joint (jt), and multi-domain
adaptive training (mat), described in sect. 1, to train all the models from
scratch on the skin datasets. images are resized to 256 × 256 and then augmented
through random scaling, shifting, rotation, flipping, gaussian noise, and
brightness and contrast changes. the encoding transformer blocks' channel
dimensions are [64, 128, 320, 512] (fig. 1-a). we use two transformer layers in
each transformer block and set the number of heads in mhsa to 8. the hidden
dimensions of the cnn bridge and auxiliary peers are 1024 and 512. we deploy
models on a single titan v gpu and train them for 200 epochs with the adamw [23]
optimizer, a batch size of 16, ensuring 4 samples from each dataset, and an
initial learning rate of 1×10 -4 , which changes through a linear decay
scheduler whose step size is 50 and decay factor γ = 0.5.
in table 2-a,b, compared with base in st, base in jt improves the segmentation
performance on small datasets (ph2 and scd) but at the expense of diminished
performance on larger datasets (isic and dmf). this is expected given the
non-negligible inter-domain heterogeneity between skin lesion datasets, as found
by bayasi et al. [5]. the above results demonstrate that shared knowledge in
related domains facilitates training a vit on small datasets while, without a
well-designed multi-domain algorithm, causing negative knowledge transfer (nkt)
due to inter-domain heterogeneity, i.e., the model's performance decreases on
other datasets. meanwhile, mdvit fits all the domains without nkt and
outperforms base in st by a large margin; significantly increasing dice and iou
on scd by 6.4% and 10.16%, showing that mdvit smartly selects valuable knowledge
when given data from a certain domain. additionally, mdvit outperforms base in
jt across all the domains, with average improvements of 0.82% on dice and 1.4%
on iou. [7], utnet [13], bat [33], transfuse [39], and swin unetr [30]. we
implement resnet-34 as the backbone of bat for fair comparison (similar model
size). as illustrated in table 2-a,b,c, these sota models are superior to base
in st. this is expected since they are designed to reduce data requirements.
nevertheless, in jt, these models also suffer from nkt: they perform better than
models in st on some datasets, like scd, and worse on others, like isic.
finally, mdvit achieves the best segmentation performance in average dice and
iou without nkt and has the best results on scd and ph2. figure 2 shows mdvit's
excellent performance on isic and dmf and that it achieves the closest results
to ground truth on scd and ph2. more segmentation results are presented in the
supplementary material. though bat and transfuse in st have better results on
some datasets like isic, they require extra compute resources to train m models
as well as an m -fold increase in memory requirements. the above results
indicate that domain-shared knowledge is especially beneficial for training
relatively small datasets such as scd.we employ the two fixed-size (i.e.,
independent of m ) multi-domain algorithms proposed by rundo et al. [28] and
wang et al. [35] on base. we set the number of parallel se adapters in [35] to
4. in table 2-b,d, mdvit outperforms both of them on all the domains, showing
the efficacy of mdvit and that multi-domain methods built on vits might not
perform as well as on cnns. we also apply the domain-specific normalization [21]
to base and mdvit to get base † and mdvit † , respectively. in table 2-d, base †
confronts nkt, which lowers the performance on dmf compared with base in st,
whereas mdvit † not only addresses nkt but also outperforms base † on average
dice and iou.
we conduct ablation studies to demonstrate the efficacy of da, mkd, and
auxiliary peers. table 3b reveals that using one-direction knowledge
distillation (kd) or either of the critical components in mdvit, i.e., da or
mkd, but not together, could not achieve the best results. table 3-c exemplifies
that, for building the auxiliary peers, our proposed mlp architecture is more
effective and has fewer parameters (1.6m) than deeplabv3's decoder [9] (4.7m) or
base's decoding layers (10.8m). finally, we incorporate da into two vits:
transfuse and dosvit (the latter includes the earliest vit encoder [12] and a
deeplabv3's decoder). as shown in table 3-a,b, da can be used in various vits
but is more advantageous in mdvit with more transformer blocks in the encoding
and decoding process.
we propose a new algorithm to alleviate vision transformers (vits)' datahunger
in small datasets by aggregating valuable knowledge from multiple related
domains. we constructed mdvit, a robust multi-domain vit leveraging novel domain
adapters (das) for negative knowledge transfer mitigation and mutual knowledge
distillation (mkd) for better representation learning. mdvit is nonscalable,
i.e., has a fixed model size at inference time even as more domains are added.
the experiments on 4 skin lesion segmentation datasets show that mdvit
outperformed sota data-efficient medical image segmentation vits and
multi-domain learning methods. our ablation studies and application of da on
other vits show the effectiveness of da and mkd and da's plug-in capability.
† 28.6(.02×) mat 90.24 90.71 93.38 95.90 92.56 ± 0.52 82.97 83.31 88.06 92.19
86.64 ± 0.76
comparing against state-of-the-art (sota) methods: we conduct experiments on
sota data-efficient mis vits and multi-domain learning methods. previous mis
vits mitigated the data-hunger in one dataset by adding inductive bias, e.g.,
swinunet
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8 43.
deep learning methods have demonstrated their tremendous potential when it comes
to medical image segmentation. however, the success of most existing
architectures relies on the availability of pixel-level annotations, which are
difficult to produce [1]. furthermore, these methods are known to be
inadequately equipped for distribution shifts. therefore, cross-modality
generalization is needed when one imaging modality has insufficient training
data. for instance, conditions such as vestibular schwannoma, where new hrt2
sequences are set to replace cet1 for diagnosis to mitigate the use of contrast
agents, is a sample use case [2]. recently billot et al. [3] proposed a domain
randomisation strategy to segment images from a wide range of target contrasts
without any fine-tuning. the method demonstrated great generalization capability
for brain parcellation, but the model performance when exposed to tumors and
pathologies was not quantified. this challenge could also be addressed through
unsupervised domain-adaptive approaches, which transfer the knowledge available
in the "source" modality s from pixel-level labels to the "target" imaging
modality t lacking annotations [4].several generative models attempt to
generalize to a target modality by performing unsupervised domain adaptation
through image-to-image translation and image reconstruction. in [5], by learning
to translate between ct and mr cardiac images, the proposed method jointly
disentangles the domain specific and domain invariant features between each
modality and trains a segmenter from the domain invariant features. other
methods [6][7][8][9][10][11][12] also integrate this translation approach, but
the segmenter is trained in an end-to-end manner on the synthetic target images
generated from the source modality using a cycle-gan [13] model. these methods
perform well but do not explicitly use the unannotated target modality data to
further improve the segmentation.in this paper, we propose m-genseg, a novel
training strategy for crossmodality domain adaptation, as illustrated in fig. 1.
this work leverages and extends genseg [14], a generative method that uses
image-level "diseased" or "healthy" labels for semi-supervised segmentation.
given these labels, the model imposes an image-to-image translation objective
between the image domain presenting tumor lesions and the domain corresponding
to an absence of lesions. therefore, like in low-rank atlas based methods
[15][16][17] the model is taught to find and remove a lesion, which acts as a
guide for the segmentation. we incorporate cross-modality image segmentation
with an image-to-image translation objective between source and target
modalities. we hypothesize both objectives are complementary since genseg helps
localizing the tumors on unannotated target images, while modality translation
enables fine-tuning the segmenter on the target modality by displaying annotated
pseudo-target images. we evaluate m-genseg on a modified version of the brats
2020 dataset, in which each type of sequence (t1, t2, t1ce and flair) is
considered as a distinct modality. we demonstrate that our model can better
generalize than other state-of-the-art methods to the target modality.
healthy-diseased translation. we propose to integrate image-level supervision to
the cross-modality segmentation task with genseg, a model that introduces
translation between domains with a presence (p) or absence (a) of tumor lesions.
leveraging this framework has a two-fold advantage here. indeed, (i) training a
genseg module on the source modality makes the model aware of the tumor
appearances in the source images even with limited source pixel-level
annotations. this helps to preserve tumor structures during the generation of
pseudo-target samples (see sect. 2.1). furthermore, (ii) training a second
genseg module on the target modality allows to further close the domain gap by
extending the segmentation objective to unannotated target data.
in order to disentangle the information common to a and p, and the information
specific to p, we split the latent representation of each image into a common
code c and a unique code u. essentially, the common code contains information
inherent to both domains, which represents organs and other structures, while
the unique code stores features like tumor shapes and location. in the two
fol-lowing paragraphs, we explain p→a and a→p translations for source images.
the same process is applied for target images by replacing s notation with t
.presence to absence translation. given an image s p of modality s in the
presence domain p, we use an encoder e s to compute the latent representation [c
s p , u s p ]. a common decoder g s com takes as input the common code c s p and
generates a healthy version s pa of that image by removing the apparent tumor
region. simultaneously, both common and unique codes are used by a residual
decoder g s res to output a residual image δ s pp , which corresponds to the
additive change necessary to shift the generated healthy image back to the
presence domain. in other words, the residual is the disentangled tumor that can
be added to the generated healthy image to create a reconstruction s pp of the
initial diseased image: like approaches in [18][19][20] we therefore generate
diseased samples from healthy ones for data augmentation. however, m-genseg aims
primarily at tackling cross-modality lesion segmentation tasks, which is not
addressed in these studies. furthermore, note that these methods are limited to
data augmentation and do not incorporate any unannotated diseased samples when
training the segmentation network, as achieved by our model with the p→a
translation.modality translation. our objective is to learn to segment tumor
lesions in a target modality by reusing potentially scarce image annotations in
a source modality. note that for each modality m ∈ {s, t }, m-genseg holds a
segmentation decoder g m seg that shares most of its weights with the residual
decoder g m res , but has its own set of normalization parameters and a
supplementary classifying layer. thus, through the absence and presence
translations, these segmenters have already learned how to disentangle the tumor
from the background. however, supervised training on a few example annotations
is still required to learn how to transform the resulting residual
representation into appropriate segmentation maps. while this is a fairly
straightforward task for the source modality using pixel-level annotations,
achieving this for the target modality is more complex, justifying the second
unsupervised translation objective between source and target modalities. based
on the cyclegan [13] approach, modality translations are performed via two
distinct generators that share their encoder with the genseg task. more
precisely, combined with the encoder e s a decoder g t enables performing s→t
modality translation, while the encoder e t and a second decoder g s perform the
t→s modality translation. to maintain the anatomical information, we ensure
cycle-consistency by reconstructing the initial images after mapping them back
to their original modality. we note for the t→s→t cycle. note that to perform
the domain adaptation, training the model to segment only the pseudo-target
images generated by the s→t modality generator would suffice (in addition to the
diseased/healthy target translation). however, training the segmentation on
diseased source images also imposes additional constraints on encoder e s ,
ensuring the preservation of tumor structures. this constraint proves beneficial
for the translation decoder g t as it generates pseudo-target tumoral samples
that are more reliable. segmentation is therefore trained on both diseased
source images s p and their corresponding synthetic target images s t p , when
provided with annotations y s . to such an extent, two segmentation masks are
predicted ŷs = g s seg • e s (s p ) and ŷst = g t seg • e t (s t p ).
segmentation loss. for the segmentation objective, we compute a soft dice loss
[21] on the predictions for both labelled source images and their
translations:reconstruction losses. l mod cyc and l gen rec respectively impose
pixel-level image reconstruction constraints on modality translation and genseg
tasks. note that l 1 refers to the standard l1 norm:moreover, like in [14] we
compute a loss l gen lat that ensures that the translation task holds the
information relative to the initial image, by reconstructing their latent codes
with the l1 norm. it also enforces the distribution of unique codes to match the
prior n (0, i) by making u ap match u, where u ap is obtained by encoding the
fake diseased sample x ap produced with random sample u.adversarial loss. for
the healthy-diseased translation adversarial objective, we compute a hinge loss
l gen adv as in genseg, learning to discriminate between pairs of real/synthetic
images of the same output domain and always in the same imaging modality, e.g. s
a vs s pa . in the modality translation task, the l mod adv loss is computed
between pairs of images of the same modality without distinction between domains
a and p , e.g. {s a , s p } vs {t s a , t s p }.overall loss. the overall loss
for m-genseg is a weighted sum of the aforementioned losses. these are tuned
separately. all weights sum to 1. first, λ gen adv , λ gen rec , and λ gen lat
weights are tuned for successful translation between diseased and healthy
images. then, λ mod adv and λ mod cyc are tuned for successful modality
translation. finally, λ seg is tuned for segmentation performance.
training and hyper-parameters. all models are implemented using pytorch and are
trained on one nvidia a100 gpu with 40 gb memory. we used a batch size of 15, an
amsgrad optimizer (β 1 = 0.5 and β 2 = 0.999) and a learning rate of 10 -4 . our
models were trained for 300 epochs and weights of the segmentation model with
the highest validation dice score were saved for evaluation. the same on-the-fly
data augmentation as in [14] was applied for all runs. each training experiment
was repeated three times with a different random seed for weight initialization.
the performance reported is the mean of all test dice scores, with standard
deviation, across the three runs. the following parameters yielded both great
modality and absence/presence translations: λ mod adv = 3, λ mod cyc = 20, λ gen
adv = 6, λ gen rec = 20 and λ gen lat = 2. note that optimal λ seg varies
depending on the fraction of pixel-level annotations provided to the network for
training.architecture. one distinct encoder, common decoder,
residual/segmentation decoder, and modality translation decoder are used for
each modality. the architecture used for encoders, decoders and discriminators
is the same as in [14]. however, in order to give insight on the model's
behaviour and properly choose the semantic information relevant for each
objective, we introduced attention gates [22] in the skip connections. figure 2a
shows the attention maps generated for each type of decoder. as expected,
residual decoders focus towards tumor areas. more interestingly, in order not to
disturb the process of healthy image generation, common decoders avoid lesion
locations. finally, modality translators tend to focus on salient details of the
brain tissue, which facilitates contrast redefinition needed for accurate
translation.
experiments were performed on the brats 2020 challenge dataset [23][24][25],
adapted for the cross-modality tumor segmentation problem where images are known
to be diseased or healthy. amongst the 369 brain volumes available in brats, 37
were allocated each for validation and test steps, while the 295 left were used
for training. we split the 3d brain volumes into 2 hemispheres and extracted 2d
axial slices. any slices with at least 1% tumor by brain surface area were
considered diseased. those that didn't show any tumor lesion were labelled as
healthy images. datasets were then assembled from each distinct pair of the four
mri contrasts available (t1, t2, t1ce and flair). to constitute unpaired
training data, we used only one modality (source or target) per training volume.
all the images are provided with healthy/diseased weak labels, distinct from the
pixel-level annotations that we provide only to a subset of the data. note that
the interest for cross-sequence segmentation is limited if multi-parametric
acquisitions are performed as is the case in brats. however, this modified
version of the dataset provides an excellent study case for the evaluation of
any modality adaptation method for tumor segmentation.
domain adaptation. we compared m-genseg with accsegnet [10] and attent [6], two
high performance models for domain-adaptative medical image segmentation. to
that extent, we performed domain-adaptation experiments with source and target
modalities drawn from t1, t2, flair and t1ce. we used available github code for
the two baselines and performed fine-tuning on our data. for each possible
source/target pair, pixel-level annotations were only retained for the source
modality. we show in fig. 2b several presence to absence translations and
segmentation examples on different target modality images. although no
pixel-level annotations were provided for the target modality, tumors were well
disentangled from the brain, resulting in a successful presence to absence
translation, as well as segmentation. note that for hypo-intense lesions (t1 and
t1ce), m-genseg still manages to convert complex residuals into consistent
segmentation maps. we plot in fig. 3 the dice performance on the target modality
for (i) supervised segmentation on source data without domain adaptation, (ii)
domain adaptation methods and (iii) uagan [26], a model designed for unpaired
multi-modal datasets, trained on all source and target data. over all modality
pairs our model shows an absolute dice score increase of 0.04 and 0.08,
respectively, compared to accsegnet and attent. annotation deficit. m-genseg
introduces the ability to train with limited pixel-level annotations available
in the source modality. we show in fig. 4 the dice scores for models trained
when only 1%, 10%, 40%, or 70% of the source t1 modality and 0% of the t2 target
modality annotations were available. while performance is severely dropping at
1% of annotations for the baselines, our model shows in comparison only a slight
decrease. we thus claim that m-genseg can yield robust performance even when a
small fraction of the source images is annotated.reaching supervised
performance. we report that, when the target modality is completely unannotated,
m-genseg reaches 90% of uagan's performance (vs 81% and 85% for attent and
accsegnet). further experiments showed that with a fully annotated source
modality, it is sufficient to annotate 25% of the target modality to reach 99%
of the performance of fully-supervised uagan (e.g. m-genseg: 0.861 ± 0.004 vs
uagan: 0.872 ± 0.003 for t1 → t2 experiment). thus, the annotation burden could
be reduced with m-genseg.
we conducted ablation tests to validate our methodological choices. we report in
table 1 the relative loss in dice scores on target modality as compared to the
proposed model. we assessed the value of doing image-level supervision by
setting all the λ gen loss weights to 0 . also, we showed that training modality
translation only on diseased data is sufficient . however, doing it for healthy
data as well provides additional training examples for this task. likewise,
performing translation from absence to presence domain is not necessary but
makes more efficient use of the data. finally, we evaluated m-genseg with
separate latent spaces for the image-level supervision and modality translation,
and we contend that m-genseg efficiently combines both tasks when the latent
representations share model updates.
we propose m-genseg, a new framework for unpaired cross-modality tumor
segmentation. we show that m-genseg is an annotation-efficient framework that
greatly reduces the performance gap due to domain shift in cross-modality tumor
segmentation. we claim that healthy tissues, if adequately incorporated to the
training process of neural networks like in m-genseg, can help to better
delineate tumor lesions in segmentation tasks. however, top performing methods
on brats are 3d models. thus, future work will explore the use of full 3d images
rather than 2d slices, along with more optimal architectures. our code is
available: https://github.com/maloadba/mgenseg_2d.
p ) and s pp = s pa + δ s pp(1) absence to presence translation. concomitantly,
a similar path is implemented for images in the healthy domain. given an image s
a of modality s in domain a, we generate a translated version in domain p. to do
so, a synthetic tumor δ s ap is generated by sampling a code from the normal
distribution n (0, i) a , u s a ] as follows:
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8_14.
simultaneous multi-index quantification (i.e., max diameter (md), center point
coordinates (x o , y o ), and area), segmentation, and uncertainty prediction of
liver tumor have essential significance for the prognosis and treatment of
patients [6,16]. in clinical settings, segmentation and quantitation are
manually performed by the clinicians through visually analyzing the
contrast-enhanced mri images (cemri) [9,10,18]. however, as shown in fig. 1(b),
contrast-enhanced fig. 1. our method integrates segmentation and quantification
of liver tumor using multi-modality ncmri, which has the advantages of avoiding
contrast agent injection, mutual promotion of multi-task, and reliability and
stability. mri (cemri) has the drawbacks of being toxic, expensive, and
time-consuming due to the need for contrast agents (ca) to be injected [2,4].
moreover, manually annotating medical images is a laborious and tedious process
that requires human expertise, making it manpower-intensive, subjective, and
prone to variation [14]. therefore, it is desirable to provide a reliable and
stable tool for simultaneous segmentation, quantification, and uncertainty
analysis, without requiring the use of contrast agents, as shown in fig.
1(a).recently, an increasing number of works have been attempted on liver tumor
segmentation or quantification [25,26,28,30]. as shown in fig. 1(c), the work
[26] attempted to use the t2fs for liver tumor segmentation, while it ignored
the complementary information between multi-modality ncmri of t2fs and dwi. in
particular, there is evidence that diffusion-weighted imaging (dwi) helps to
improve the detection sensitivity of focal lesions as these lesions typically
have higher cell density and microstructure heterogeneity [20]. the study in
[25,30] attempted to quantify the multi-index of liver tumor, however, the
approach is limited to using multi-phase cemri that requires the injection of
ca. in addition, all these works are limited to a single task and ignore the
constraints and mutual promotion between multi-tasks. available evidence
suggests that uncertainty information regarding segmentation results is
important as it guides clinical decisions and helps understand the reliability
of the provided segmentation. however, current research on liver tumors tends to
overlook this vital task.to the best of our knowledge, although many works focus
on the simultaneous quantization, segmentation, and uncertainty in medical
images (i.e., heart [3,5,11,27], kidney [17], polyp [13]). no attempt has been
made to automatically liver tumor multi-task via integrating multi-modality
ncmri due to the following challenges: (1) the lack of an effective
multi-modality mri fusion mechanism. because the imaging characteristics between
t2fs and dwi have significant differences (i.e., t2fs is good at anatomy
structure information while dwi is good at location information of lesions
[29]). (2) the lack of strategy for capturing the accurate boundary information
of liver tumors. due to the lack of contrast agent injection, the boundary of
the lesion may appear blurred or even invisible in a single ncmri, making it
challenging to accurately capture tumor boundaries [29]. (3) the lack of an
associated multi-task framework. because segmentation and uncertainty involve
pixel-level classification, whereas quantification tasks involve image-level
regression [11]. this makes it challenging to integrate and optimize the
complementary information between multi-tasks.in this study, we propose an
edge-aware multi-task network (eamtnet) that integrates the multi-index
quantification (i.e., center point, max-diameter (md), and area), segmentation,
and uncertainty. our basic assumption is that the model should capture the
long-range dependency of features between multimodality and enhance the boundary
information for quantification, segmentation, and uncertainty of liver tumors.
the two parallel cnn encoders first extract local feature maps of multi-modality
ncmri. meanwhile, to enhance the weight of tumor boundary information, the sobel
filters are employed to extract edge maps that are fed into edge-aware feature
aggregation (eafa) as prior knowledge. then, the eafa module is designed to
select and fuse the information of multi-modality, making our eamtnet edge-aware
by capturing the long-range dependency of features maps and edge maps. lastly,
the proposed method estimates segmentation, uncertainty prediction, and
multi-index quantification simultaneously by combining multi-task and cross-task
joint loss.the contributions of this work mainly include: (1) for the first
time, multiindex quantification, segmentation, and uncertainty of the liver
tumor on multimodality ncmri are achieved simultaneously, providing a
time-saving, reliable, and stable clinical tool. (2) the edge information
extracted by the sobel filter enhances the weight of the tumor boundary by
connecting the local feature as prior knowledge. (3) the novel eafa module makes
our eamtnet edge-aware by capturing the long-range dependency of features maps
and edge maps for feature fusion. the source code will be available on the
author's website.
the eamtnet employs an innovative approach for simultaneous tumor multiindex
quantification, segmentation, and uncertainty prediction on multimodality ncmri.
as shown in fig. 2, the eamtnet inputs multi-modality ncmri of t2fs and dwi for
capturing the feature and outputs the multiindex quantification, segmentation,
and uncertainty. specifically, the proposed approach mainly consists of three
steps: 1) the cnn encoders for capturing feature maps and the sobel filters for
extracting edge maps (sect. 2.1); 2) the edge-aware feature aggregation (eafa)
for multi-modality feature selection and fusion via capturing the long-distance
dependence (sect. 2.2); and 3) multi-task prediction module (sect. 2.3).
in step 1 of fig. 2, the multi-modality ncmri (i.e., χ i t 2 ∈ r h×w , χ i dw i
∈ r h×w ) are fed into two parallel encoders and the sobel filter to extract the
feature maps (i.e., g i t 2 ∈ r h×w ×n , g i dw i ∈ r h×w ×n ) and the
corresponding edge maps (i.e., edge i t 2 ∈ r h×w , edge i dw i ∈ r h×w )
respectively. specifically, eamtnet employs unet as the backbone for
segmentation because the cnn encoder has excellent capabilities in low-range
semantic information extraction [15]. the two parallel cnn encoders have the
same architecture where each encoder contains three shallow convolutional
network blocks to capture features of adjacent slices. each conv block consists
of a convolutional layer, batch normalization, relu, and non-overlapping
subsampling. at the same time, eamt-net utilizes the boundary information
extracted by the sobel filter [19] as prior knowledge to enhance the weight of
tumor edge information to increase the awareness of the boundary.
in step 2 of the proposed model, the feature maps (i.e., g i t 2 , g i dw i )
and the edge maps (i.e., edge i t 2 , edge i dw i ) are fed into eafa for
multi-modality feature fusion with edge-aware. in particular, the eafa makes the
eamtnet edge-aware by using the transformer to capture the long-range dependency
of feature maps and edge maps. specifically, the feature maps and edge maps are
first flattened to the 1d sequence corresponding to x 1d ∈ r n ×p 2 and e 1d ∈ r
2×q 2 , respectively. where n = 2 × c means the channel number c of the last
convolutional layer from the two parallel encoders. (p, p ) and (q, q) represent
the resolution of each feature map and each edge map, respectively. on the basis
of the 1d sequence, to make the feature fusion with edge awareness, the
operation of position encoding is performed not only on feature maps but also on
edge maps. the yielded embeddings z ∈ r n ×p 2 +2×q 2 can serve as the input
sequence length for the multi-head attention layer in transformer. the following
operations in our eafa are similar to the traditional transformer [22]. after
the three cascade transformer layers, the eafa yields the fusion feature vector
f for multi-task prediction. the specific computation of the self-attention
matrix and multi-head attention are defined below [22]:where query q, key k, and
value v are all vectors of the flattened 1d sequences of x 1d and e 1d . w o i
is the projection matrix, and 1is the scaling factor.
in step 3 of fig. 2, the eamtnet outputs the multi-modality quantification ŷq
(i.e., md, x o , y o and area), segmentation result ŷs and uncertainty map ûi .
specifically, for the quantification path, ŷq is directly obtained by performing
a linear layer to the feature f from eafa. for the segmentation and uncertainty
path, the output feature f from eafa is first reshaped into a 2d feature map f
out . then, to scale up to higher-resolution images, a 1 × 1 convolution layer
is employed to change the channel number of f out for feeding into the decoder.
after upsampling by the cnn decoder, eamtnet predicts the segmentation result ŷs
with h × w and uncertainty map ûi with h × w . the cnn decoder contains three
shallow deconv blocks, which consist of deconv layer, batch normalization, and
relu. inspired by [24], we select the entropy map as our uncertainty measure.
given the prediction probability after softmax, the entropy map is computed as
follows:where z i is the probability of pixel x belonging to category i. when a
pixel has high entropy, it means that the network is uncertain about its
classification. therefore, pixels with high entropy are more likely to be
misclassified. in other words, its entropy will decrease when the network is
confident in a pixel's label.under the constraints of uncertainty, the eamtnet
can effectively rectify the errors in tumor segmentation because the uncertainty
estimation can avoid overconfidence and erroneous quantification [23]. moreover,
the eamtnet novelly make represent different tasks in a unified framework,
leading to beneficial interactions. thus, the quantification performance is
improved through backpropagation by the joint loss function l multi-task . the
function comprises segmentation loss l seg and quantification loss l qua , where
the loss function l seg is utilized for optimizing tumor segmentation, and l qua
is utilized for optimization of multi-index quantification. it can be defined
as:where ŷs represents the prediction, and y i represents the ground truth
label. the sum is performed on s pixels, ŷi task represents the predicted
multi-index value, and y i task represents the ground truth of multi-index
value, task ∈ {md, x, y , area}.
for the first time, eamtnet has achieved high performance with the dice
similarity coefficient (dsc) up to 90.01 ± 1.23%, and the mean absolute error
(mae) of the md, x o , y o and area are down to 2.72 ± 0.58 mm,1.87±0.76 mm,
2.14 ± 0.93 mm and 15.76 ± 8.02 cm 2 , respectively.dataset and configuration.
an axial dataset includes 250 distinct subjects, each underwent initial standard
clinical liver mri protocol examinations with corresponding pre-contrast images
(t2fs [4mm]) and dwi [4mm]) was collected. the ground truth was reviewed by two
abdominal radiologists with 10 and 22 years of experience in liver imaging,
respectively. if any interpretations demonstrated discrepancies between the
reviewers, they would re-evaluate the examinations together and reach a
consensus. to align the paired images of t2 and dwi produced at different times.
we set the t2 as the target image and the dwi as the source image to perform the
pre-processing of non-rigid registration between t2 and dwi by using the demons
non-rigid registration method. it has been widely used in the field of medical
image registration since it was proposed by thirion [21]. we perform the demons
non-rigid registration on an open-source toolbox dirart using matlab
2017b.inspired by the work [22], we set the scaling factor d k to 64 in equation
(1). all experiments were assessed with a 5-fold cross-validation test. to
quantitatively evaluate the segmentation results, we calculated the dice
coefficient scores (dsc) metric that measures the overlapping between the
segmentation prediction and ground truth [12]. to quantitatively evaluate the
quantification results, we calculated the mean absolute error (mae). our eamtnet
was implemented using ubuntu 18.04 platform, python v3.6, pytorch v0.4.0, and
running on two nvidia gtx 3090ti gpus.accurate segmentation. the segmentation
performance of eamtnet has been validated and compared with three
state-of-the-art (sota) segmentation methods (transunet [1], unet [15], and
unet++ [31]). furthermore, to ensure consistency in input modality, the channel
number of the first convolution layer in the three comparison methods is set to
2. the visual examples of liver tumors are shown in fig. 3, it is evident that
our proposed eamtnet outperforms the three sota methods. some quantitative
analysis results are shown in table 1 and table 2, our network achieves high
performance with the dsc of 90.01 ± 1.23% (5.39% higher than the second-best).
the results demonstrate that edge-aware, multi-modality fusion, and uncertainty
prediction are essential for segmentation.ablation study. to verify the
contributions of edge-aware feature aggregation (eafa) and uncertainty, we
performed ablation study and compared and performance of different networks.
first, we removed the eafa and used concatenate, meaning we removed fusion
multi-modality (no-eafa). then, we removed the uncertainty task
(no-uncertainty). the quantitative analysis results of these ablation studies
are shown in table 1. our method exhibits high performance in both segmentation
and quantification, indicating that each component of the eamtnet plays a vital
role in liver tumor segmentation and quantification.performance comparison with
state-of-the-art. the eamtnet has been validated and compared with three sota
segmentation methods and two sota quantification methods (i.e., resnet-50 [7]
and densenet [8]). furthermore, the channel number of the first convolution
layer in the two quantification comparison methods is set to 2 to ensure the
consistency of input modalities. the visual segmentation results are shown in
fig. 3. moreover, the quantitative results (as shown in table 2) corresponding
to the visualization results (i.e., fig. 3) obtained from the existing
experiments further demonstrate that our method outperforms the three sota
methods. specifically, compared with the second-best approach, the dsc is
boosted from 84.62 ± 1.45% to 90.01 ± 1.23%. the quantitative analysis results
are shown in table 3. it is evident that our method outperforms the two sota
methods with a large margin in all metrics, owing to the proposed multi-modality
fusing and multi-task association.
in this paper, we have proposed an eamtnet for the simultaneous segmentation and
multi-index quantification of liver tumors on multi-modality ncmri. the new eafa
enhances edge awareness by utilizing boundary information as prior knowledge
while capturing the long-range dependency of features to improve feature
selection and fusion. additionally, multi-task leverages the prediction
discrepancy to estimate uncertainty, thereby improving segmentation and
quantification performance. extensive experiments have demonstrated the proposed
model outperforms the sota methods in terms of dsc and mae, with great potential
to be a diagnostic tool for doctors.
automatic detection of brain tumors from magnetic resonance imaging (mri) is
complex, tedious, and time-consuming because there are a lot of missed,
misinterpreted, and misleading tumor-like lesions in the images of the brain
tumors [8]. most of the current work focuses on brain tumor classification and
segmentation from mri and detection tasks are less explored [1,13,22]. while
existing studies showed that various convolutional neural networks (cnns) are
efficient for brain tumor detection, the performance of using you only look once
(yolo) networks is scarcely investigated [12,20,[23][24][25]27].with the rapid
development of cnns, the accuracies of different visual tasks are constantly
improved. however, the increasingly complex network architecture in cnn-based
models, such as resnet [6], densenet [9], inception [28], etc. renders the
inference speed slower. though many advanced cnns deliver higher accuracy, the
complicated multi-branch designs (e.g., residual-addition in resnet and
branch-concatenation in inception) make the models difficult to implement and
customize, slowing down the inference and reducing memory utilization. the
depth-wise separable convolutions used in mobilenets [7] also reduce the upper
limit of the gpu inference speed. in addition, 3 × 3 regular convolution is
highly optimized by some modern computing libraries. consequently, vgg [26] is
still heavily used for real-world applications in both research and
industries.repvgg [2] is an extension of vgg via reparametrization to accelerate
inference time. repvgg uses a multi-branch topological architecture during the
training phase, which is then reparameterized to a simplified single-branch
architecture during the inference phase. in terms of the optimization strategy
of network training, reparameterization was introduced in yolov6 [16], yolov7
[31], and yolov6 v3.0 [17]. yolov6 and yolov6 v3.0 employ reparameterization
from repvgg. repconv, a repvgg without an identity connection, is converted from
repvgg during inference time in yolov6, yolov6 v3.0, and yolov7 (named repconvn
in yolov7). due to the removal of identity connections in repconv, direct access
to resnet or the concatenation in densenet can provide more diversity of
gradients for different feature maps. grouped convolutions, which use a group of
convolutions with multiple kernels per layer, like repvgg, can also
significantly reduce the computational complexity of the model, but there is no
information communication between groups, which limits the ability of feature
extraction of the convolution operator. in order to overcome the disadvantage of
grouped convolutions, shufflenet v1 [34] and v2 [21] introduced the channel
shuffle operation to facilitate information flows across different feature
channels. in addition, when comparing spatial pyramid pooling & cross stage
partial network plus convbnsilu (sppcspc) in yolov7 with spatial pyramid pooling
fast (sppf) in yolov5 [10] and yolov8 [11], it is found that more convolution
layers in sppcspc architecture slow down the computation of the network.
nevertheless, spp [4,5]
the architecture of the proposed rcs-yolo network is shown in fig. 1. it
incorporates a new module-rcs-osa in the backbone and neck of the yolobased
object detector.
inspired by shufflenet, we design a structural reparameterized convolution based
on channel shuffle. figure 2 shows the structural schematic diagram of rcs.given
that the feature dimensions of an input tensor are c × h × w , after the channel
split operator, it is divided into two different channel-wise tensors with equal
dimensions of c×h ×w . for one of the tensors, we use the identity branch, 1 × 1
convolution, and 3 × 3 convolution to construct the training-time rcs. at the
inference stage, the identity branch, 1 × 1 convolution, and 3 × 3 convolution
are transformed to 3 × 3 repconv by using structural reparameterization. the
multi-branch topology architecture can learn abundant information about features
during the training time, simplified single-branch architecture can save memory
consumption during the inference time to achieve fast inference. after the
multi-branch training of one of the tensors, it is concatenated to the other
tensor in a channel-wise manner. the channel shuffle operator is also applied to
enhance information fusion between two tensors so that the depth measurement
between different channel features of the input can be realized with low
computational complexity. when there is no channel shuffle, the output feature
of each group only relates to the input feature within a group of grouped
convolutions, and outputs from a certain group only relate to the input within
the group. this blocks information flow between channel groups and weakens the
ability of feature extraction. when channel shuffle is used, input and output
features are fully related where one convolution group takes data from other
groups, enabling more efficient feature information communication between
different groups. the channel shuffle operates on stacked grouped convolutions
and allows more informative feature representation. moreover, assuming that the
number of groups is g, for the same input feature, the computational complexity
of channel shuffle is 1 g times that of a generic convolution.compared with the
popular 3 × 3 convolution, during the inference stage, rcs uses the operators
including channel split and channel shuffle to reduce the computational
complexity by a factor of 2, while keeping the inter-channel information
exchange. moreover, using structural reparameterization enables deep
representation learning from input features during the training stage, and
reduction of inference-time memory consumption to achieve fast inference.
the one-shot aggregation (osa) module has been proposed to overcome the
inefficiency of dense connections in densenet, by representing diversified
features with multi-receptive fields and aggregating all features only once in
the last feature maps. vovnet v1 [14] and v2 [15] used the osa module within its
architecture to construct both lightweight and large-scale object detectors,
which outperform the widely-used resnet backbone with faster speed and better
energy efficiency.we develop an rcs-osa module by incorporating rcs developed in
sect. 2.1 for osa, as shown in fig. 3. the rcs modules are stacked repeatedly to
ensure the reuse of features and to enhance the information flow among different
channels between features of adjacent layers. at different locations of the
network, we set a different number of stacked modules. to reduce the level of
network fragmentation, only three feature cascades are maintained on the
one-shot aggregate path, which can mitigate the amount of network calculation
burden and reduce the memory footprint. in terms of multi-scale feature fusion,
inspired by the idea of path aggregation network (panet) [19], rcs-osa +
upsampling and rcs-osa + repvgg/repconv undersampling carry out the alignment of
feature maps of different sizes to allow information exchange between the two
prediction feature layers. this enables high-accuracy fast inference in object
detection. moreover, rcs-osa maintains the same number of input channels and
minimum output channels, thus reducing the memory access cost (mac). for network
building, we perpetuate max-pooling undersampling 32 times of yolov7 to
construct a backbone network and adopt repvgg/repconv with a step of 2 to
achieve undersampling. due to the diversified feature representation of the
rcs-osa module and low-cost memory consumption, we use a different number of
stacked rcs in rcs-osa modules to achieve semantic information extraction during
different stages of both backbone and neck networks. the common evaluation
metric of computation efficiency (or time complexity) is floating-point
operations (flops). flops are only the indirect indicator to measure the speed
of inference. however, the object detector with a densenet backbone shows rather
slow speed and low energy efficiency because the linearly increasing number of
channels by dense connection leads to heavy mac, which causes considerable
computation overhead. given input features of dimension m × m , the convolution
kernel of size k × k, number of input channels c 1 , and the number of output
channels c 2 , flops and mac can be calculated as:assuming n to be 4, flops of
the proposed rcs-osa and efficient layer aggregation networks (elan) [31,33] are
20.25c 2 m 2 and 40c 2 m 2 respectively. compared with elan, flops of rcs-osa
are reduced by nearly 50%. the mac of rcs-osa (i.e., 6cm 2 + 20.25c 2 ) is also
reduced compared to that of elan (i.e., 17cm 2 + 40c 2 ).
to further reduce inference time, we decrease the number of detection heads
comprised of repvgg and idetect from 3 to 2. the yolov5, yolov6, yolov7, and
yolov8 have three detection heads. however, we use only two feature layers for
prediction, reducing the number of original nine anchors with different scales
to four and using the k-means unsupervised clustering method to regenerate
anchors with different scales. the corresponding scales are (87, 90), (127,
139), (154, 171), (191,240). this not only reduces the number of convolution
layers and computational complexity of rcs-yolo but also reduces the overall
computational requirements of the network during the inference stage and the
computational time of postprocessing non-maximum suppression.
to evaluate the proposed rcs-yolo model, we used the brain tumor detection 2020
dataset (br35h) [3], with a total of 701 images in the 'train' and 'val' two
folders, 500 images of which are the 'train' folder were selected as the
training set, while the other 201 images in the 'val' folder as the testing set.
for the input size of 640×640 image, the actual corresponding size is 44×32. the
small object is defined as the object whose pixel size is less than 32 × 32
defined by the ms coco dataset [18], so there are no small objects in the brain
tumor medical image data sets, and the scale change of the target boxes is
smooth, almost square. the label boxes of the brain images were normalized (see
supplementary material sect. 1).
for model training and inference, we used ubuntu 18.04 lts, intel r xeon r gold
5218 cpu processor, cuda 12.0, and cudnn 8.2. gpu is geforce rtx 3090 with 24g
memory size. the networking development framework is pytorch 1.9.1. the
integrated development environment (ide) is pycharm. we uniformly set epoch 150,
the batch size as 8, image size as 640 × 640. stochastic gradient descent (sgd)
optimizer was used with an initial learning rate of 0.01 and weight decay of
0.0005.
in this paper, we choose precision, recall, ap 50 , ap 50:95 , flops, and frames
per second (fps) as comparative metrics of detection effect to determine the
advantages and disadvantages of the model. taking iou = 0.5 as the standard,
precision, and recall can be calculated by the following equations:
(3)where t p represents the number of positive samples correctly identified as
positive samples, f p represents the number of negative samples incorrectly
identified as positive samples and f n represents the number of positive samples
incorrectly identified as negative samples. ap 50 is the area under the
precision-recall (pr) curve formed by precision and recall. for ap 50:95 ,
divide 10 iou threshold of 0.5:0.05:0.95 to acquire the area under the pr curve,
then average the results. fps represents the number of images detected by the
model per second.
to highlight the accuracy and rapidity of the proposed model for the detection
of brain tumor medical image data set, table 1 shows the performance comparison
between our proposed detector and other state-of-the-art object detectors. the
time duration of fps includes data preprocessing, forward model inference, and
post-processing. the long border of the input images is set as 640 pixels. the
short border adaptively scales without distortion, whilst keeping the grey
filling with 32 times the pixels of the short border.it can be seen that
rcs-yolo with the advantages of incorporating the rcs-osa module performs well.
compared with yolov7, the flops of the object detectors of this paper decrease
by 8.8g, and the inference speed improves by 43.4 fps. in terms of detection
rate, precision improves by 0.024; ap 50 increases by 0.01; ap 50:95 by 0.006.
also, rcs-yolo is faster and more accurate than yolov6-l v3.0 and yolov8l.
although the ap 50:95 of rcs-yolo equals that of yolov8l, it doesn't obscure the
essential advantage of rcs-yolo. the results clearly show the superior
performance and efficiency of our method, compared to the state-of-the-art for
brain tumor detection. as shown in supplementary material fig. 2, brain tumor
regions are accurately detected from mri by using the proposed method.
we demonstrate the effectiveness of the proposed rcs-osa module in yolobased
object detectors. the results of repvgg-csp in table 2, where rcs-osa in the
rcs-yolo is replaced with the cross stage partial network (csp-net) [32] used in
existing yolov4-csp [30] architecture, are decreased than rcs-yolo except
gflops. because the parameters of repvgg-csp (22.2m) are less than half those of
rcs-yolo (45.7m), the computation amount (i.e., gflops) of repvgg-csp is
accordingly smaller than rcs-yolo. nevertheless, rcs-yolo still performs better
in actual inference speed measured by fps.
we developed an rcs-yolo network for fast and accurate medical object detection,
by leveraging the reparameterized convolution operator rcs based on channel
shuffle in the yolo architecture. we designed an efficient one-shot aggregation
module rcs-osa based on rcs, which serves as a computational unit in the
backbone and neck of a new yolo network. evaluation of the brain mri dataset
shows superior performance for brain tumor detection in terms of both speed and
precision, as compared to yolov6, yolov7, and yolov8 models.
yolo-based model for fast brain tumor detection. evaluation on a publicly
available brain tumor detection annotated dataset shows superior detection
accuracy and speed compared to other state-of-the-art yolo architectures.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8_57.
for the past decade, deep neural networks have dominated the computer vision
community and provided near human performance on many different tasks, including
classification [18], segmentation [24], and image generation [16]. given these
impressive results, convolutional neural networks are now used on a daily basis
in fields like healthcare, self-driving cars, and robotics, to cite a few. in
medical imaging, convolutional neural networks are particularly used to segment
organs or regions of interest on different modalities such as x-rays, ct scans,
mris, or ultrasound [36]. indeed, segmentation techniques and variations of 2d
and 3d u-nets are currently the state-of-the-art to identify and isolate tumors,
blood vessels, organs, or other structures within an image and provide crucial
help to physicians for medical diagnosis, screening, and prognosis
[32].nowadays, segmentation models are gaining widespread adoption in modern
clinical practice and are being used with increasing frequency, making the
results of these models critical for many patients. however, it is now commonly
known that neural networks can be vulnerable to adversarial attacks [17,34],
i.e., small input perturbations invisible to humans crafted specifically such
that the network performs errors. over the past few years, a large body of work
has devised empirical defenses against adversarial attacks for classification
tasks [3,17,25], as well as segmentation tasks [37], including applications on
medical imaging [27]. although state-of-the-art empirical defenses provide
significant robustness, these defenses do not guarantee theoretical robustness
and stronger attacks can be crafted to break them [5]. recently, certified
defenses, for classification [2,11,26] and segmentation [15,23], have been
proposed to guarantee the accuracy and reliability of neural networks. however,
certified defenses for segmentation in the context of medical imaging are still
lacking, even if models are getting market approvals (e.g., fda, ce) and are
already adopted in clinical practice.in this paper, we provide the first method
for certified robustness in the context of segmentation for medical imaging. we
leverage the randomized smoothing strategy [11,15], and the recent work on
diffusion models [7] to achieve stateof-the-art certified robustness for
segmentation models. randomized smoothing consists in convolving the neural
network with a gaussian distribution (i.e., by adding noise to the input) in
order to obtain a smooth segmentation model. from the smoothness properties of
the segmentation model, we can derive a robustness guarantee and compute a
certified dice score. we go even further by using diffusion models to first
denoise the perturbed input and boost the certified robustness. by extension, we
show that current diffusion models, trained on 'classical images' generalize
well to medical datasets for denoising tasks. extensive experiments on five
public medical datasets of chest x-rays [21,31], skin lesions [10], and
colonoscopies [6], and different popular segmentation models, prove the
potential of our method. we hope that this study will provide the first step
towards robustness guarantees for medical image segmentation.
since the discovery of adversarial attacks [17,34], numerous defenses [8,17,25]
and attacks have been devised [8,25], demonstrating that neural networks are
sensitive to small input perturbation and vulnerable to attacks. adversarial
training, which has been acknowledged as one of the most successful empirical
defenses, consists in training a network directly on adversarial examples [25].
however, it is now known that even strong defenses can be bypassed by adaptive
attacks [12]. paschali et al. [27] were among the first to study adversarial
attacks in the context of medical imaging. they conducted experiments using
several neural network architectures [20,33] (i.e., inception v3, v4, mobilenet)
and several attacks [17,25] to demonstrate that the vulnerability of neural
networks is extended to medical images.more specifically, in the context of
classification, a previous work [4] has analyzed the robustness of neural
networks for chest x-ray images and showed that gradient-based attacks were
successful in fooling both machines and humans. in a similar line of work, yao
et al. [38] proposed an add-on to known attacks that bypasses state-of-the-art
adversarial detectors making current defenses even less robust. on the other
hand, several works have been focused on crafting defense strategies
specifically in the context of medical imaging. for example, almalik et al. [1]
proposed a self-ensembling method to enhance the robustness of vision
transformers in the presence of adversarial attacks. in the context of
segmentation in medical imaging, [30] introduced a vector quantization approach
by learning a discrete representation in a low dimensional embedding space and
improving the robustness of a segmentation model. finally, daza et al. [13]
proposed a lattice architecture that segments organs and lesions on mri and ct
scans and leveraged an efficient approach of adversarial training to defend
against adversarial examples.although a large body of work has focused on
constructing defenses for classification and segmentation tasks in the context
of medical imaging, certified defenses are under-studied by the medical
community. in this paper, we propose to leverage randomized smoothing and
diffusion models for certified segmentation on medical datasets, setting the
first baseline for this challenging problem and certifying popular segmentation
architectures.
randomized smoothing is a model agnostic technique, proposed by cohen et al.
[11], used to improve and certify the robustness of neural networks against
adversarial attacks. this method consists in adding random noise (e.g., noise
generated from a gaussian distribution) to the input data and then classifying
the perturbed data using the neural network. let d = x × y denote the data
distribution where x ⊂ r d and y = {1, . . . , k} represent the input space and
target space respectively and k is the number of classes. let f : x → y be a
neural network such that for (x, y) ∈ d, the classifier correctly classifies
ifrandomized smoothing is a procedure to construct a new smooth classifier g
given any base classifier f . let n (0, σ 2 i) be a gaussian distribution of
mean 0 and variance σ, then, the smooth classifier g is defined as follows:cohen
et al. [11] have shown that if r = σφ -1 (g(x)) where φ is the cumulative
distribution function of the standard gaussian distribution and r can be
considered the certified radius, then, g(x + δ) = y for all δ satisfying δ 2 ≤
r.however, since it is not possible to compute g at x exactly, they proposed
using monte carlo algorithms as an alternative approach for estimating g(x)
using random sampling. in order to obtain a reliable estimate of the probability
g(x), they also suggested a method that involves generating n samples of η from
a normal distribution n (0, σ 2 i) and evaluating f (x + η) for each sample. the
resulting counts for each class in y are then used to estimate probability p y
and the radius r with confidence 1 -α (where α is a value between 0 and 1). if
the confidence level cannot be achieved (for example, due to insufficient
samples), the method will abstain from providing an estimate. more recently,
fischer et al. [15] built upon the work of [11] by introducing segcertify, the
first certified approach for image segmentation. the segmentation process
involves assigning a segmentation class to every pixel in the image, which can
be viewed as a form of classification at the pixel level. in the segmentation
settings, the output space consists of regions or categories to be segmented,
such as cars, roads, pedestrians, etc. the classifier function f : x → y d
determines the class for each pixel and categorizes each component
independently. in this context, the certification algorithm proposed by cohen et
al. [11] can be extended to accommodate the segmentation task.to obtain a smooth
classifier, it is necessary to add random noise to the input of the classifier.
however, this creates a trade-off between accuracy and robustness. if low
variance noise is added, accuracy won't be impacted significantly, but the
certified radius will remain low. conversely, adding high variance noise can
improve certificates but at the expense of accuracy. to address this issue,
cohen et al. proposed a simple trick of training the network with noise
injection during the training phase. while this method may reduce accuracy when
evaluating the classifier with noise during the certification process, it can
also help mitigate the trade-off between accuracy and robustness. one can note
that during training, the network's objective is to learn to ignore the noise
and classify at the same time. to improve the natural as well as the certified
accuracy, salman et al. [29] proposed to separate the two tasks with two
networks trained separately. first, a network, h : x → x , is trained to denoise
the data such that for η ∼ n (0, σ 2 i), we have h(x + η) ≈ x, then, the output
of the denoiser is given to the classifier.in this paper, we leverage randomized
smoothing and diffusion probabilistic models to obtain state-of-the-art results
on certified segmentation for medical imaging. to the best of our knowledge, we
are the first to propose a comprehensive study on certified segmentation for
medical imaging.
the training of a denoising diffusion probabilistic model (ddpm) is an iterative
process that involves adding a small amount of noise at every step of the
diffusion process until random noise is reached. the reverse process then starts
from random noise and generates a new image that conforms to the data
distribution. since ddpms are inherently iterative denoising models, we can
leverage this property for randomized smoothing. the idea would be to start the
reverse process with a noisy image, rather than gaussian noise, enabling the
ddpm to output an image that resembles the original image.similar to carlini et
al. [7], our proposed pipeline is composed of two main steps: we denoise, then
we certify. in order to complete the denoising, we need to first map between the
noise model utilized in diffusion models and the one used in randomized
smoothing. randomized smoothing needs a data point that is enhanced with
gaussian noise added to it, given by x rs = x + δ with δ ∼ n (x, σ 2 i). on the
other hand, diffusion models suppose the noise model for since randomized
smoothing is applied to each pixel separately with a probability of 1 -α,
considering the entire segmentation region would imply considering a union bound
with significantly reduced confidence. similar to fischer et al. [15], we
leverage the holm-bonferroni method [19] and perform multipletesting
corrections. for each image, we repeat this process n = 100 times, identifying
pixels on which the model abstains, and computing the certified scores. we
extend the work of fischer et al. [15] to also compute a certified dice score
that is calculated ignoring the abstain class ( ). our approach has a
significant advantage compared to segcertify since it leverages off-the-shelf
and stateof-the-art pre-trained denoisers and segmentation models. segcertify,
on the other hand, relies on models trained with gaussian noise.
datasets: we perform experiments on 5 different publicly available datasets. all
datasets were divided to 70% for training, 10% for validation, and 20% for
testing. the testing set is the one used to compute certified results.chest
x-rays datasets: jsrt dataset [31] with annotations of lung, heart, and
clavicles provided by [35] is used. this dataset contains 247 images. for lung
segmentation only, we use both the montgomery and shenzen datasets [21].
montgomery consists of 138 and shenzen of 662 annotated images.skin lesion: skin
images with their annotations provided by the isic 2018 boundary segmentation
challenge [10] were used. this dataset consists of 2694 rgb dermatoscopy images.
colonoscopy images: cvc-clinicdb dataset [6] containing 612 colonoscopy images
in rgb together with their annotations were utilized.implementation details: we
train three different segmentation models namely, a unet [28], a resunet++ [22],
and a deeplabv2 [9] with and without noise. the models trained without noise are
used exclusively with our method. the models trained with a gaussian noise of
0.25 are used to compute segcertify scores. all 6 models use an image input size
of 512 × 512 for x-ray images, 384×512 for skin lesions, and 288×384 for
colonoscopy. as a denoiser, we use an off-the-shelf denoising diffusion
probabilistic model provided by [14]. we perform our experiments with the 256 ×
256 class unconditional denoiser with a linear scheduler and without timestep
respacing. for each noise level, our method follows the steps described in the
previous section and uses n 0 = 10, n=100 for each image, and α = 0.001, and τ =
0.75. our code is made publicly available at:
https://github.com/othmanela/medical_cert_seg.results and discussion: for all
five datasets, we compute a certified dice score and certified mean intersection
over union (iou). we also report the percentage of abstentions (% ) representing
the mean number of pixels on which the model's prediction confidence was
insufficient with respect to the radius r.the lower the percentage of
abstentions the better the segmentation model is.in table 1, we compare our
method using 3 different and popular architectures (unet, resunet++, and
deeplabv2) on the chest x-rays datasets. we s2 of the supplementary material.
overall, for both methods, resunet++ is the most robust architecture followed by
unet and then deeplabv2 for all σ and r combinations. moreover, certified
metrics for lungs and heart remain high for our method, even with high levels of
noise. however, the increasing level of noise affects the clavicles since these
are smaller structures. a comparison of our method and segcertify using the
resunet++ architecture is presented in table 2 for the three chest x-ray
datasets. we observe that we outperform segcertify, especially for high sigma
values. for σ = 0.25, segcertify performs slightly better. this is due to the
fact that the model used with segcertify is trained with a noise level of 0.25.
the main drawback however is that its dice on unperturbed images drops
considerably (e.g., from 0.96 to 0.91 on lung segmentation). on the other hand,
our pipeline does not require training a segmentation model with noise or even a
denoising model. our methodology relies only on off-the-shelf models. for the
highest noise level of σ = 1.0, we notice that the certified dice and iou with
segcertify both drop to 0 whereas our proposed method is able to maintain high
certified scores.qualitative results are provided in fig. 1 for our proposed
method and segcertify for the different datasets and different levels of noise.
regarding the structures to segment, we notice that the abstentions around the
clavicles (the smallest benchmarked region of interest on chest x-rays) get
bigger. we also notice that the fine segmentation boundaries (e.g., area around
the skin lesion) may not be as sharp after denoising. as we increase the noise,
the decision boundary is harder to find for all models. this may be due to the
fact that fine details on the image are lost after the denoising step. however,
our method is still able to segment the large majority of pixels properly on the
image, contrary to its competitor, especially for high noise levels (third row
on chest x-rays).table 3 reports certified segmentation results for skin lesions
and colonoscopy on both techniques. we notice that our method is still
performing better than segcertify. this supports our claim that ddpms generalize
quite well to medical images and that harnessing their potential boosts the
state-of-the-art.regarding the denoiser, we used a single-step denoising
strategy, i.e., we perform a single call to the ddpm to compute the denoised
image from t * to t = 0. another strategy could be to iteratively denoise from t
* , t * -1, ... until t = 0. however, this implies predicting a denoised image
multiple times and in the end, may result in images with unwanted artifacts. we
perform multi-step denoising experiments and report results in table s1 of the
supplementary material. we note that the single-step denoising performs best
since it relies more on the denoising power of ddpms rather than their
generative capabilities, and is also faster than the multi-step approach.
finally, we perform a comparison with another denoiser architecture. we train
three unet models (one for each noise level) on the jsrt dataset. we report
results in table s3 and notice that even with custom-trained denoisers, the ddpm
outperforms the unet denoising architecture. a comparison of denoised images is
provided in figure s1. we notice that the ddpm is able to keep high-fidelity
images compared to the unet and is therefore more relevant for certified medical
image segmentation.
in this paper, we present the first work on certified segmentation for medical
imaging, and extensively evaluate it on five different datasets and three deep
learning segmentation models. our technique leverages off-the-shelf denoising
and segmentation models and provides the highest certified dice and miou on
multi-class and binary segmentation of five different datasets. with that, we
are able to remove the overhead of having to train and fine-tune models
specifically for robustness. this paradigm shift alleviates the dilemma of
having to choose between highly accurate segmentation models or models robust to
attacks. we hope that this work serves as a baseline for the unexplored yet
critical topic of certified segmentation in medical imaging. future work will
involve extending our approach to 3d medical imaging modalities as well as
exploring the realm of certified classification.
* (x + δ), δ ∼ n (0, σ 2 i). * to t = 0. a multi-step denoising strategy implies
iteratively predicting all images at t * , t * -1, . . . until t = 0. both
techniques are explored in the next section and supplementary material.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8_58.
the unlabeled image pool can be quickly enriched via the support from partner
clinical centers with low barriers of entry (only unlabeled images are required)
data heterogeneity due to different scanners, scanning protocols and subject
groups, which violate the typical ssl assumption of i.
prostate segmentation from magnetic resonance imaging (mri) is a crucial step
for diagnosis and treatment planning of prostate cancer. recently, deep
learningbased approaches have greatly improved the accuracy and efficiency of
automatic prostate mri segmentation [7,8]. yet, their success usually requires a
large amount of labeled medical data, which is expensive and expertise-demanding
in practice. in this regard, semi-supervised learning (ssl) has emerged as an
attractive option as it can leverage both limited labeled data and abundant
unlabeled data [3,[9][10][11]15,16,[21][22][23][24][25][26]28]. nevertheless,
the effectiveness of ssl is heavily dependent on the quantity and quality of the
unlabeled data.regarding quantity , the abundance of unlabeled data serves as a
way to regularize the model and alleviate overfitting to the limited labeled
data. unfortunately, such "abundance" may be unobtainable in practice, i.e., the
local unlabeled pool is also limited due to restricted image collection
capabilities or scarce patient samples. as a specific case shown in table 1,
there are only limited prostate scans available per center. taking c1 as a case
study, if the amount of local unlabeled data is limited, existing ssl methods
may still suffer from inferior performance when generalizing to unseen test data
(fig. 1). to efficiently enrich the unlabeled pool, seeking support from other
centers is a viable solution, as illustrated in fig. 1. yet, due to differences
in imaging protocols and variations in patient demographics, this solution
usually introduces data heterogeneity, lead-ing to a quality problem. such
heterogeneity may impede the performance of ssl which typically assumes that the
distributions of labeled data and unlabeled data are independent and identically
distributed (i.i.d.) [16]. thus, proper mechanisms are called for this practical
but challenging ssl scenario.here, we define this new ssl scenario as multi-site
semi-supervised learning (ms-ssl), allowing to enrich the unlabeled pool with
multi-site heterogeneous images. being an under-explored scenario, few efforts
have been made. to our best knowledge, the most relevant work is ahdc [2].
however, it only deals with additional unlabeled data from a specific source
rather than multiple arbitrary sources. thus, it intuitively utilizes
image-level mapping to minimize dual-distribution discrepancy. yet, their
adversarial min-max optimization often leads to instability and it is difficult
to align multiple external sources with the local source using a single image
mapping network.in this work, we propose a more generalized framework called
categorylevel regularized unlabeled-to-labeled (cu2l) learning, as depicted in
fig. 2, to achieve robust ms-ssl for prostate mri segmentation. specifically,
cu2l is built upon the teacher-student architecture with customized learning
strategies for local and external unlabeled data: (i) recognizing the importance
of supervised learning in data distribution fitting (which leads to the failure
of cps [3] in ms-ssl as elaborated in sec. 3), the local unlabeled data is
involved into pseudolabel supervised-like learning to reinforce fitting of the
local data distribution; (ii) considering that intra-class variance hinders
effective ms-ssl, we introduce a non-parametric unlabeled-to-labeled learning
scheme, which takes advantage of the scarce expert labels to explicitly
constrain the prototype-propagated predictions, to help the model exploit
discriminative and domain-insensitive features from heterogeneous multi-site
data to support the local center. yet, observing that such scheme is challenging
when significant shifts and various distributions are present, we further
propose category-level regularization, which advocates prototype alignment, to
regularize the distribution of intra-class features from arbitrary external data
to be closer to the local distribution; (iii) based on the fact that
perturbations (e.g., gaussian noises [15]) can be regarded as a simulation of
heterogeneity, perturbed stability learning is incorporated to enhance the
robustness of the model. our method is evaluated on prostate mri data from six
different clinical centers and shows promising performance on tackling ms-ssl
compared to other semi-supervised methods.
in our scenario of ms-ssl, we have access to a local target dataset d local
(consisted of a labeled sub-set d l local and an unlabeled sub-set d u local )
and the external unlabeled support datasets d u e = m j=1 d u,j e , where m is
the number of support centers. specifically,with n u unlabeled scans. with n j
unlabeled samples. considering the large variance on slice thickness among
different centers [7,8], our experiments are performed in 2d. thus, we refer to
pixels in the subsequent content. as shown in fig. 2, our framework is built
upon the popular teacher-student framework. specifically, the student f s θ is
an in-training model optimized by loss back-propagation as usual while the
teacher model f t θ is slowly updated with a momentum term that averages
previous weights with the current weights, where θ denotes the student's weights
and θ the teacher's weights. θ is updated by θt = α θt-1 + (1α)θ t at iteration
t, where α is the exponential moving average (ema) coefficient and empirically
set to 0.99 [26]. compared to the student, the teacher performs self-ensembling
by nature which helps smooth out the noise and avoid sudden changes of
predictions [15]. thus, the teacher model is suitable for handling the
heterogeneous external images and producing relatively stable pseudo labels
(will be used later). as such, our task of ms-ssl can be formulated as
optimizing the following loss:where l l sup is the supervised guidance from
local labeled data and l u denotes the additional guidance from the unlabeled
data. λ is a trade-off weight scheduled by the time-dependent ramp-up gaussian
function [15] where w max and t max are the maximal weight and iteration,
respectively. the key challenge of ms-ssl is the proper design of l u for
robustly exploiting multi-site unlabeled data {d u local , d u e } to support
the local center.
as mentioned above, supervised-like learning is advocated for local unlabeled
data to help the model fit local distribution better. owning the self-ensembling
property, the teacher model provides relatively stable pseudo labels for the
student model. given the predicted probability map p u,t local of x u local from
the teacher model, the pseudo label ŷ u,t local corresponds to the class with
the maximal posterior probability. yet, with limited local labeled data for
training, it is difficult to generate high-quality pseudo labels. thus, for each
pixel, if max c (p u,t local ) ≥ δ, where c denotes the c-th class and δ is a
ramp-up threshold ranging from 0.75 to 0.9 as training goes, this pixel will be
included in loss calculation. considering that the cross-entropy loss has been
found very sensitive to label noises [18], we adopt the partial dice loss l dice
[27] to perform pseudo label learning, formulated as:, where p u,s local denotes
the prediction of x u local from the student model. the dice loss is calculated
for each of the k equally-sized regions of the image, and the final loss is
obtained by taking their mean. such a regional form [6] can help the model
better perceive the local discrepancies for fine-grained learning.
unlabeled-to-labeled learning. inherently, the challenge of ms-ssl stems from
intra-class variation, which results from different imaging protocols, disease
progress and patient demographics. inspired by prototypical networks [13,19,25]
that compare class prototypes with pixel features to perform segmentation, here,
we introduce a non-parametric unlabeled-to-labeled (u2l) learning scheme that
utilizes expert labels to explicitly constrain the prototype-propagated
predictions. such design is based on two considerations: (i) a good
prototypepropagated prediction requires both compact feature and discriminative
prototypes, thus enhancing this prediction can encourage the model to learn in a
variation-insensitive manner and focus on the most informative clues; (ii) using
expert labels as final guidance can prevent error propagation from pseudo
labels. specifically, we denote the feature map of the external unlabeled image
x u e before the penultimate convolution in the teacher model as f u,t e . note
that f u,t e has been upsampled to the same size of x u e via bilinear
interpolation but with l channels. with the argmax pseudo label ŷ u,t e and the
predicted probability map p u,t e , the object prototype from the external
unlabeled data can be computed via confidence-weighted masked average pooling:
c.likewise, the background prototype c u(bg) e can also be obtained. considering
the possible unbalanced sampling of prostate-containing slices, ema strategy
across training steps (with a decay rate of 0.9) is applied for prototype
update. then, as shown in fig. 2 , where we use cosine similarity for sim(•, •)
and empirically set the temperature t to 0.05 [19]. note that a similar
procedure can also be applied to the local unlabeled data x u local , and thus
we can obtain another prototype-propagated unlabeledto-labeled prediction p u2l
local for x l local . as such, given the accurate expert label y l local , the
unlabeled-to-labeled supervision can be computed as:category-level
regularization. being a challenging scheme itself, the above u2l learning can
only handle minor intra-class variation. thus, proper mechanisms are needed to
alleviate the negative impact of significant shift and multiple distributions.
specifically, we introduce category-level regularization, which advocates class
prototype alignment between local and external data, to regularize the
distribution of intra-class features from arbitrary external data to be closer
to the local one, thus reducing the difficulty of u2l learning. in u2l, we have
obtained prototypes from local unlabeled data {c where mean squared error is
adopted as the distance function d(•, •). the weight of background prototype
alignment is smaller due to less relevant contexts.
although originally designed for typical ssl, encouraging stability under
perturbations [26] can also benefit ms-ssl, considering that the perturbations
can be regarded as a simulation of heterogeneity and enforcing such perturbed
stability can regularize the model behavior for better generalizability.
specifically, for the same unlabeled input x u ∈ {d u local ∪ d u e } with
different perturbations ξ and ξ (using the same gaussian noises as in [26]), we
encourage consistent pre-softmax predictions between the teacher and student
models, formulated aswhere mean squared error is also adopted as the distance
function d(•, •).overall, the final loss for the multi-site unlabeled data is
summarized as:
materials. we utilize prostate t2-weighted mr images from six different clinical
centers (c1-6) [1,4,5] to perform a retrospective evaluation. rizes the
characteristics of the six data sources, following [7,8], where [7,8] also
reveal the severity of inter-center heterogeneity here through extensive
experiments. the heterogeneity comes from the differences in scanners, field
strengths, coil types, disease and in-plane/through-plane resolution. compared
to c1 and c2, scans from c3 to c6 are taken from patients with prostate cancer,
either for detection or staging purposes, which can cause inherent semantic
differences in the prostate region to further aggravate heterogeneity. following
[7,8], we crop each scan to preserve the slices with the prostate region only
and then resize and normalize it to 384 × 384 px in the axial plane with zero
mean and unit variance. we take c1 or c2 as the local target center and randomly
divide their 30 scans into 18, 3, and 9 samples as training, validation, and
test sets, respectively.implementation and evaluation metrics. the framework is
implemented on pytorch using an nvidia geforce rtx 3090 gpu. considering the
large variance in slice thickness among different centers, we adopt the 2d
architecture. specifically, 2d u-net [12] is adopted as our backbone. consists
of the cross-entropy loss and the k-regional dice loss [6]. the maximum
consistency weight w max is set to 0.1 [20,26]. t max is set to 20,000. k is
empirically set to 2. the network is trained using the sgd optimizer and the
learning rate is initialized as 0.01 and decayed by multiplication with (1.0t/t
max ) 0.9 . data augmentation is applied, including random flip and rotation. we
adopt the dice similarity coefficient (dsc) and jaccard as the evaluation
metrics and the results are the average over three runs with different
seeds.comparison study. table 2 presents the quantitative results with either c1
or c2 as the local target center, wherein only 6 or 8 local scans are annotated.
besides the supervised-only baselines, we include recent top-performing ssl
methods [2,3,11,14,15,17,20,25,26] for comparison. all methods are implemented
with the same backbone and training protocols to ensure fairness. as observed,
compared to the supervised-only baselines, our cu2l with {6, 8} local labeled
scans achieves {19.15%, 17.42%} and {9.1%, 6.44%} dsc improvements in {c1, c2},
showing its effectiveness in leveraging multi-site unlabeled data. despite the
violation of the assumption of i.i.d. data, existing ssl methods can still
benefit from the external unlabeled data to some extent compared to the results
using local data only as shown in fig. 1, revealing that the quantity of
unlabeled data has a significant impact. however, due to the lack of proper
mechanisms for learning from heterogeneous data, limited improvement can be
achieved by them, especially for cps [3] and fixmatch [14] in c2. particularly,
cps relies on cross-modal pseudo labeling which exploits all the unlabeled data
in a supervised-like fashion. we attribute its degradation to the fact that
supervised learning is crucial for distribution fitting, which supports our
motivation of performing pseudo-label learning on local unlabeled data only. as
a result, its models struggle to determine which distribution to prioritize.
meanwhile, the most relevant ahdc [2] is mediocre in ms-ssl, mainly due to the
instability of adversarial training and the difficulty of aligning multiple
distributions to the local distribution via a single image-mapping network. in
contrast, with specialized mechanisms for simultaneously learning informative
representations from multi-site data and handling heterogeneity, our cu2l
obtains the best performance over the recent ssl methods. figure 3(a) further
shows that the predictions of our method fit more accurately with the ground
truth.ablation study. to evaluate the effectiveness of each component, we
conduct an ablation study under the setting with 6 local labeled scans, as shown
in fig. 2(b). firstly, when we remove l u p l (cu2l-1), the performance drops by
{5.69% (c1), 3.05%(c2)} in dsc, showing that reinforcing confirmation on local
distribution is critical. cu2l-2 represents the removal of both l u2l and l cr ,
and it can be observed that such an unlabeled-to-labeled learning approach
combined with class-level regularization is crucial for exploring multi-site
data. if we remove l cr which accompanies with l u2l (cu2l-3), the performance
degrades, which justifies the necessity of this regularization to reduce the
difficulty of unlabeled-to-labeled learning process. cu2l-4 denotes the removal
of l u sta . as observed, such a typical stability loss [15] can further improve
the performance by introducing hand-crafted noises to enhance the robustness to
real-world heterogeneity.
in this work, we presented a novel category-level regularized
unlabeled-to-labeled (cu2l) learning framework for semi-supervised prostate
segmentation with multi-site unlabeled mri data. cu2l robustly exploits
multi-site unlabeled data via three tailored schemes: local pseudo-label
learning for better local distribution fitting, category-level regularized
unlabeled-to-labeled learning for exploiting the external data in a
distribution-insensitive manner and stability learning for further enhancing
robustness to heterogeneity. we evaluated our method on prostate mri data from
six different clinical centers and demonstrated its superior performance
compared to other semi-supervised methods.
center source#scans field strength (t) resolution (in-plane/through-plane in mm)
coil
summa-
segmenting the prostate anatomy and detecting tumors is essential for both
diagnostic and treatment planning purposes. hence, the task of developing domain
generalisable prostate mri segmentation models is essential for the safe
translation of these models into clinical practice. deep learning models are
susceptible to textural shifts and artefacts which is often seen in mri due to
variations in the complex acquisition protocols across multiple sites [12].the
most common approach to tackle domain shifts is with data augmentation
[16,33,35] and adversarial training [11,30]. however, this increases training
time and we propose to tackle the problem head on by learning shape only
embedding features to build a shape dictionary using vector quantisation [31]
which can be sampled to compose the segmentation output. we therefore
hypothesise by limiting the search space to a set of shape components, we can
improve generalisability of a segmentation model. we also propose to correctly
sample and compose shape components with local and global topological
constraints by tracking topological features as we compose the shape components
in an ordered manner. this is achieved using a branch of algebraic topology
called cellular sheaf theory [8,19]. we hypothesise this approach will produce
more anatomically meaningful segmentation maps and improve tumour
localisation.the contributions of this paper are summarized as follows: 1. this
work considers shape compositionality to enhance the generalisability of deep
learning models to segment the prostate on mri. 2. we use cellular sheaves to
aid compositionality for segmentation as well as improve tumour localisation.
topological data analysis is a field which extracts topological features from
complex data structures embedded in a topological space. one can describe a
topological space through its connectivity which can be captured in many forms.
one such form is the cubical complex. the cubical complex c is naturally
equipped to deal with topological spaces represented as volumetric grid
structured data such as images [32]. in a 3d image, a cubical complex consists
of individual voxels serving as vertices, with information regarding their
connections to neighboring voxels captured through edges, squares, and cubes.
matrix reduction algorithms enable us to represent the connectivity of c in
terms of a series of mathematical groups, known as the homology groups. each
homology group encompasses a specific dimension, d of topological features, such
as connected components (d = 0), holes (d = 1), and voids (d = 2). the number of
topological features present in each group is quantified by the corresponding
betti number. betti numbers provide useful topological descriptors of the binary
label maps as it is a single scale topological descriptor. however, the output,
y from a segmentation model is continuous. thus, the betti number for a cubical
complex where vertices are continuous will be a noisy topological descriptor. we
therefore use persistent homology which tracks changes in the topological
features at multiple scales [17]. a cubical complex can be constructed at some
threshold, τ over the output defined as: c τ = {y ∈ y|y ≥ τ }. we can now create
q cubical complexes over q ordered thresholds. this leads to a sequence of
nested cubical complexes shown in eq. 1 known as a sublevel set filtration. the
persistent homology defines d dimensional topological features such as connected
components which are born at τ i and dies at τ j where τ j > τ i . this creates
tuples (τ i , τ j ) which are stored as points in a persistence diagram (fig.
2b).
the homology of segmentation maps provides a useful tool for analysing global
topology but does not describe how local topology is related to construct global
topological features. sheaf theory provides a way of composing or 'gluing' local
data together to build a global object (new data) that is consistent with the
local information [8]. this lends well to modelling compositionality. formally,
a sheaf is a mathematical object which attaches to each open subset or subspace,
u in a topological space, y an algebraic object like a vector space or set
(local data) such that it is well-behaved under restriction to smaller open sets
[8].we can consider a topological space, y such as a segmentation output divided
into a finite number of subspaces, {∅, y 1 , y 2 ...y n } which are the base
spaces for y or equivalently the patches in a segmentation map. if we
sequentially glue base spaces together in a certain order to form increasingly
larger subspaces of y starting with the ∅, one can construct a filtration of y
such that;we neatly formalise the subspaces and how subspaces are glued together
with a poset. a poset (p ) is a partially ordered set defined by a relation, ≤
between elements in p which is reflexive, antisymmetric, and transitive [19]. in
our work, we define a poset by the inclusion relation; p i ≤ p j implies p i ⊆ p
j for p i , p j ∈ p . hence, we can map each element in p with a subspace in x
which satisfies the inclusion relations in p like in x.a cellular sheaf, f over
a poset is constructed by mapping, each element, p ∈ p to a vector space f(p)
over a fixed field which preserves the ordering in p by linear transformations,
ρ .,. which are inclusion maps in this case [19]. in our work each element in p
maps to the vector space, r 2 which preserves the inclusion relations in p .
specifically, we compute a persistence diagram, d for the subspace in x
associated (homeomorphic) with p ∈ p whereby (τ i , τ j ) in the persistence
diagram are a set of vectors in the vector space, r 2 . a cellular sheaf
naturally arises in modelling the connectivity of a segmentation map and
provides a mathematically precise justification for using cellular sheaves in
our method. we show by approaching the composition of segmentation maps through
this lens, one can significantly improved the robustness of segmentation models.
there have been various deep learning based architectures developed for prostate
tumour segmentation [3,15,18]. there is however no work looking at developing
models which generalise well to target domains after training on one source
domain known as single domain generalisation (sdg). effective data augmentation
techniques, such as cutout [16], mixup [34] and bigaug [35] offer a
straightforward approach to enhance the generalisability of segmentation models
across different domains. recent methods have utilized adversarial techniques,
such as advbias [11], which trains the model to generate bias field deformations
and enhance its robustness.randconv [33] incorporates a randomized convolution
layer to learn textural invariant features. self-supervised strategies such as
jigen [9] can also improve generalisability. the principle of compositionality
has been integrated into neural networks for tasks such as image classification
[23], generation [2] and more recently, segmentation [26,28] to improve
generalisability. the utilization of persistent homology in deep learning-based
segmentation is restricted to either generating topologically accurate
segmentations in the output space [21] or as a subsequent processing step [14].
the novel approach of topological auto-encoders [27] marks the first instance of
incorporating persistent homology to maintain the topological structure of the
data manifold within the latent representation. cellular sheaves were used to
provide a topological insight into the poor performance of graph neural networks
in the heterophilic setting [7]. recently, cellular sheaves were used as a
method of detecting patch based merging relations in binary images [20].
finally, [4] recently proposed using sheaf theory to construct a shape space
which allows one to precisely define how to glue shapes together in this shape
space.
given spatial, t s and textural, t i transformations of the input space, x , the
goal is to learn an encoder, φ e to map x to lower dimensional embedding
features, e which are shape equivariant and texture invariant as shown in eq.
2.we assume t2 and adc mri images share the same spatial information and only
have textural differences. we exploit this idea in fig. 1, where firstly an adc
image under spatial transformation, t s is mapped with an encoder, φ e to z 2
and the t 2 image is mapped with the same encoder to z 1 . shape equivariance
and texture invariance is enforced with the contrastive loss, l contr = t s (z 1
)z 2 2 2 . specifically, we apply transformations from the dihedral group (d4)
which consists of 90 • rotations in the z plane and 180 • rotations in the y
plane. note, a contrastive only learns equivariance as opposed to constraining
the convolutional kernels to be equivariant. z 1 containing 128 channels is
spatially quantised before passing into the composer. in the test phase, the adc
image is not required and only the t2 image is used as input. t2 segmentations
are used as the label.
we posit that there is limited shape variation in the low dimensional embedding
space across subjects which can be fully captured in n discrete shapes. n
discrete shapes form a shape dictionary, d shown in fig. 1 which is learnt with
vector quantisation [31]. given we enforce a texture invariant continuous
embedding space and hence only contains shape information, quantisation converts
this continuous embedding space to discrete shape features, ẑ. the quantisation
process involves minimising the euclidean distance between the embedding space,
z 1 divided into m features, z 1 i and its nearest shape component, e k ∈ d
shown in eq. 3 where k = argmin j z 1ie j 2 and m = 3048. next, sampling d such
that z 1 i is replaced by e k produces the spatially quantized embedding space
ẑ. straight-through gradient approximation is applied for backpropagation
through the sampling process to update z 1 and d [31]. gradient updates are
applied to only the appropriate operands using stop gradients (sg) during
optimization.
shapes in d sampled with a uniform prior can lead to anatomically implausible
segmentations after composition which we tackle through the language of cellular
sheaves to model the connectivity of patches in an image which provides a
connectivity-based loss function.
the quantised embedding space, ẑ, is split into c groups. the composition of
each group in ẑ to form each class segmentation, y c in the output, y involves
two steps. initially, a decoder with grouped convolutions equal to the number of
classes followed by the softmax function maps, ẑ ∈ r 128×16×16×12 to c ∈ r
p×c×256×256×24 where p is the number of patches for each class c. the second
step of the composition uses a cellular sheaf to model the composition of y c by
gluing the patches together in an ordered manner defined by a poset while
tracking its topology using persistent homology. this in turn enforces d to be
sampled in a topological preserving manner as input into the decoder/composer to
improve both the local and global topological correctness of each class
segmentation output, y c after composition.
we illustrate our methodology of using cellular sheaves with a simple example in
fig. 2. here, we show y as perfectly matching the ground truth label (not
one-hot encoded) divided into 2 patches. y is a topological space with the
subspaces,each element in p is associated with a subspace in v such that the
inclusion relationship is satisfied. therefore, in fig. 2a, p defines that y 1
and y 2 associated with (1, 0) and (0, 1) respectively are glued together to
form y = y 1 ∪ y 2 which maps with (1, 1). a cellular sheaf f over p is created
by assigning a vector space to p ∈ p by deriving a persistence diagram, d for
each element in v associated with p as shown in fig. 2b. the arrows in fig. 2b
are inclusion maps defined as ρ .,. . persistence diagrams are computed from the
sequence of nested cubical complexes of each subspace in v. the persistence
diagrams in fig. 2b are formed by overlapping the persistence diagrams for each
class segmentation. note, persistence diagrams contain infinite points in the
form (τ, τ ) (diagonal line in persistence diagrams) which always allows a
bijection between two persistence diagrams. the main advantage of our approach
is that in addition to ensuring correct local topology (patch level) and global
topology (image level), we also force our network to produce topologically
accurate patches correctly merged together in a topology preserving manner which
matches the ground truth. for example in fig. 2b, y 2 contains 3 connected
components glued onto y 1 containing 2 connected components to form y , which
also has 3 connected components. this means an extra connected component is
added by y 2 due to tumour which therefore improves patch-wise tumour
localisation. it also indicates the other 2 connected components in y 2 are
merged into the 2 connected components in y 1 to form 2 larger connected
components (peripheral and transitional zone) in y . hence, the same 2 vectors
present in both f(1, 0) and f(0, 1) representing the peripheral and transitional
zone are also in f (1,1). this is also known as a local section in f.
we construct cellular sheaves, f over p c and p c and minimise the distance
between these cellular sheaves.we firstly plot persistence diagrams, d from the
set of vectors (τ i , τ j ) in f(p c i ) and f( p c i ). next, we minimise the
total p th wasserstein distance (topological loss) between the persistence
diagrams d(f(p c i )) and d(f( p c i )) shown in eq. 4 where η : d(f(p c i )) →
d(f( p c i )) is a bijection between the persistence diagrams [27] and p = 2.
this loss function is proven to be stable to noise [29] and differentiable [10].
we add a dice loss between y and ŷ . the total loss to train our entire
framework is:
in the task of anatomical segmentation, the first two columns of table 1 show
the results for the domain shift from runmc in the decathlon dataset to
bmc.here, we demonstrate that our method improves segmentation performance in
all evaluation metrics compared to the baseline, nn-unet and the other sdg
methods. similar findings are noted for the domain shift from the internal
dataset to the runmc data in the prostatex2 dataset (second two columns of table
1).in table 2, we note our method significantly improves tumour segmentation and
localisation performance. we visualise our findings with an example in fig. 3,
where there is improved localisation of the tumour and the correct number of
tumour components enforced by our topological loss. this significantly reduces
the false positive rate highlighted in table 2. also, note the more anatomically
plausible zonal segmentations. however, our method is restricted by the number
of low dimensional shape components in the shape dictionary used to compose the
high dimensional segmentation output. therefore, our approach can fail to
segment the finer details of prostate tumours due to its high shape variability
which leads to coarser but better localised tumour segmentations.
in conclusion, we propose shape compositionality as a way to improve the
generalisability of segmentation models for prostate mri. we devise a method to
learn texture invariant and shape equivariant features used to create a
dictionary of shape components. we use cellular sheaf theory to help model the
composition of sampled shape components from this dictionary in order to produce
more anatomically meaningful segmentations and improve tumour localisation.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8 24. pre-processing: all images are
resampled to 0.5 × 0.5 × 3 mm, centre cropped to 256 × 256 × 24 and normalised
between 0 and 1.
in order to address the anisotropic characteristics of prostate mri images, we
have chosen a hybrid 2d/3d unet as our baseline model. we use the same encoder
and decoder architecture as the baseline model in our method. see supplementary
material for further details.
we compare our method with the nnunet [22] and several approaches to tackle sdg
segmentation namely, randconv [33], advbias [11], jigen [9] and bigaug [35]
applied to the baseline model. we also compare to a compositionality driven
segmentation method called the vmfnet [26].training: in all our experiments, the
models were trained using adam optimization with a learning rate of 0.0001 and
weight decay of 0.05. training was run for up to 500 epochs on three nvidia rtx
2080 gpus. the performance of the models was evaluated using the dice score,
betti error [21] and hausdorff distance. we evaluate tumour localisation by
determining a true positive if the tumour segmentation overlaps by a minimum of
one pixel with the ground truth.in our ablation studies, the minimum number of
shape components required in d for the zonal and zonal + tumour segmentation
experiments was 64 and 192 respectively before segmentation performance dropped.
see supplementary material for ablation experiments analysing each component of
our framework.
transformers [7,21,30] have seen wide-scale adoption in medical image
segmentation as either components of hybrid architectures [2,3,8,9,31,33] or
standalone techniques [15,25,34] for state-of-the-art performance. the ability
to learn longrange spatial dependencies is one of the major advantages of the
transformer architecture in visual tasks. however, transformers are plagued by
the necessity of large annotated datasets to maximize performance benefits owing
to their limited inductive bias. while such datasets are common to natural
images (imagenet-1k [6], imagenet-21k [26]), medical image datasets usually
suffer from the lack of abundant high quality annotations [19]. to retain the
inherent inductive bias of convolutions while taking advantage of architectural
improvements of transformers, the convnext [22] was recently introduced to
re-establish the competitive performance of convolutional networks for natural
images. the con-vnext architecture uses an inverted bottleneck mirroring that of
transformers, composed of a depthwise layer, an expansion layer and a
contraction layer (sect. 2.1), in addition to large depthwise kernels to
replicate their scalability and long-range representation learning. the authors
paired large kernel con-vnext networks with enormous datasets to outperform
erstwhile state-of-the-art transformer-based networks. in contrast, the vggnet
[28] approach of stacking small kernels continues to be the predominant
technique for designing convnets in medical image segmentation. out-of-the-box
data-efficient solutions such as nnunet [13], using variants of a standard unet
[5], have still remained effective across a wide range of tasks.the convnext
architecture marries the scalability and long-range spatial representation
learning capabilities of vision [7] and swin transformers [21] with the inherent
inductive bias of convnets. additionally, the inverted bottleneck design allows
us to scale width (increase channels) while not being affected by kernel sizes.
effective usage in medical image segmentation would allow benefits from -1)
learning long-range spatial dependencies via large kernels, 2) less intuitively,
simultaneously scaling multiple network levels. to achieve this would require
techniques to combat the tendency of large networks to overfit on limited
training data. despite this, there have been recent attempts to introduce large
kernel techniques to the medical vision domain. in [18], a large kernel 3d-unet
[5] was used by decomposing the kernel into depthwise and depthwise dilated
kernels for improved performance in organ and brain tumor segmentationexploring
kernel scaling, while using constant number of layers and channels. the convnext
architecture itself was utilized in 3d-ux-net [17], where the transformer of
swinunetr [8] was replaced with convnext blocks for high performance on multiple
segmentation tasks. however, 3d-ux-net only uses these blocks partially in a
standard convolutional encoder, limiting their possible benefits.in this work,
we maximize the potential of a convnext design while uniquely addressing
challenges of limited datasets in medical image segmentation. we present the
first fully convnext 3d segmentation network, mednext, which is a scalable
encoder-decoder network, and make the following contributions: mednext achieves
state-of-the-art performance against baselines consisting of transformer-based,
convolutional and large kernel networks. we show performance benefits on 4 tasks
of varying modality (ct, mri) and sizes (ranging from 30 to 1251 samples),
encompassing segmentation of organs and tumors. we propose mednext as a strong
and modernized alternative to standard convnets for building deep networks for
medical image segmentation.
in prior work, convnext [22] distilled architectural insights from vision
transformers [7] and swin transformers [21] into a convolutional architecture.
the convnext block inherited a number of significant design choices from
transformers, designed to limit computation costs while scaling the network,
which demonstrated performance improvements over standard resnets [10]. in this
work, we leverage these strengths by adopting the general design of convnext as
the building block in a 3d-unet-like [5] macro architecture to obtain the
mednext. we extend these blocks to up and downsampling layers as well (sect.
2.2), resulting in the first fully convnext architecture for medical image
segmentation. the macro architecture is illustrated in fig. 1a. mednext blocks
(similar to convnext blocks) have 3-layers mirroring a transformer block and are
described for a c-channel input as follows:1. depthwise convolution layer: this
layer contains a depthwise convolution with kernel size k ×k ×k, followed by
normalization, with c output channels. we use channel-wise groupnorm [32] for
stability with small batches [27], instead of the original layernorm. the
depthwise nature of convolutions allow large kernels in this layer to replicate
a large attention window of swin-transformers, while simultaneously limiting
compute and thus delegating the "heavy lifting" to the expansion layer.
corresponding to a similar design in transformers, this layer contains an
overcomplete convolution layer with cr output channels, where r is the expansion
ratio, followed by a gelu [12] activation. large values of r allow the network
to scale width-wise while 1 × 1 × 1 kernel limits compute. it is important to
note that this layer effectively decouples width scaling from receptive field
(kernel size) scaling in the previous layer. 3. compression layer: convolution
layer with 1 × 1 × 1 kernel and c output channels performing channel-wise
compression of the feature maps.mednext is convolutional and retains the
inductive bias inherent to conv-nets that allows easier training on sparse
medical datasets. our fully convnext architecture also enables width (more
channels) and receptive field (larger kernels) scaling at both standard and
up/downsampling layers. alongside depth scaling (more layers), we explore these
3 orthogonal types of scaling to design a compound scalable mednext for
effective medical image segmentation (sect. 2.4).
the original convnext design utilizes separate downsampling layers which consist
of standard strided convolutions. an equivalent upsampling block would be
standard strided transposed convolutions. however, this design does not
implicitly take advantage of width or kernel-based convnext scaling while
resampling. we improve upon this by extending the inverted bottleneck to
resampling blocks in mednext. this is done by inserting the strided convolution
or transposed convolution in the first depthwise layer for downsampling and
upsampling mednext blocks respectively. the corresponding channel reduction or
increase is inserted in the last compression layer of our mednext 2× up or down
block design as in fig. 1a. additionally, to enable easier gradient flow, we add
a residual connection with 1 × 1 × 1 convolution or transposed convolution with
stride of 2.in doing so, mednext fully leverages the benefits from
transformer-like inverted bottlenecks to preserve rich semantic information in
lower spatial resolutions in all its components, which should benefit dense
medical image segmentation tasks.
large convolution kernels approximate the large attention windows in
transformers, but remain prone to performance saturation. convnext architectures
in classification of natural images, despite the benefit of large datasets such
as imagenet-1k and imagenet-21k, are seen to saturate at kernels of size 7 × 7 ×
7 [22]. medical image segmentation tasks have significantly less data and
performance saturation can be a problem in large kernel networks. to propose a
solution, we borrow inspiration from swin transformer v2 [20] where a
largeattention-window network is initialized with another network trained with a
smaller attention window. specifically, swin transformers use a bias matrix b ∈
r (2m -1)×(2m -1) to store learnt relative positional embeddings, where m is the
number of patches in an attention window. on increasing the window size, m
increases and necessitates a larger b. the authors proposed spatially
interpolating an existing bias matrix to the larger size as a pretraining step,
instead of training from scratch, which demonstrated improved performance. we
propose a similar approach but customized to convolutions kernels, as seen in
fig. 1b, to overcome performance saturation. upkern allows us to iteratively
increase kernel size by initializing a large kernel network with a compatible
pretrained small kernel network by trilinearly upsampling convolutional kernels
(represented as tensors) of incompatible size. all other layers with identical
tensor sizes (including normalization layers) are initialized by copying the
unchanged pretrained weights. this leads to a simple but effective
initialization technique for med-next which helps large kernel networks overcome
performance saturation in the comparatively limited data scenarios common to
medical image segmentation.
compound scaling [29] is the idea that simultaneous scaling on multiple levels
(depth, width, receptive field, resolution etc.) offers benefits beyond that of
scaling at one single level. the computational requirements of indefinitely
scaling kernel sizes in 3d networks quickly becomes prohibitive and leads us to
investigate simultaneous scaling at different levels. keeping with fig. 1a, our
scaling is tested for block count (b), expansion ratio (r) and kernel size (k)
-corresponding to depth, width and receptive field size. we use 4 model
configurations of the mednext to do so, as detailed in table 1 (left). the basic
functional design (mednext-s) uses number of channels (c) as 32, r = 2 and b =
2. further variants increase on just r (mednext-b) or both r and b
(mednext-m).the largest 70-mednext-block architecture uses high values of both r
and b (mednext-l) and is used to demonstrate the ability of mednext to be
significantly scaled depthwise (even at standard kernel sizes). we further
explore large kernel sizes and experiment with k = {3, 5} for each
configuration, to maximize performance via compound scaling of the mednext
architecture.3 experimental design
we use pytorch [24] for implementing our framework. we experiment with 4
configurations of the mednext with 2 kernel sizes as detailed in sect. 2.4. the
gpu memory requirements of scaling are limited via -1) mixed precision training
with pytorch amp, 2) gradient checkpointing. [4]. our experimental framework
uses the nnunet [13] as a backbone -where the training schedule (epochs = 1000,
batches per epoch = 250), inference (50% patch overlap) and data augmentation
remain unchanged. all networks, except nnunet, are trained with adamw [23] as
optimizer. the data is resampled to 1.0 mm isotropic spacing during training and
inference (with results on original spacing), using input patch size of 128 ×
128 × 128 and 512 × 512, and batch size 2 and 14, for 3d and 2d networks
respectively. the learning rate for all mednext models is 0.001, except kernel:5
in kits19, which uses 0.0001 for stability. for baselines, all swin models and
3d-ux-net use 0.0025, while vit models use 0.0001. we use dice similarity
coefficient (dsc) and surface dice similarity (sdc) at 1.0 mm tolerance for
volumetric and surface accuracy. 5-fold cross-validation (cv) mean performance
for supervised training using 80:20 splits for all models are reported. we also
provide test set dsc scores for a 5-fold ensemble of mednext-l (kernel: 5 × 5 ×
5) without postprocessing. our extensive baselines consist of a high-performing
convolutional network (nnunet [13]), 4 convolution-transformer hybrid networks
with transformers in the encoder (unetr [9], swinunetr [8]) and in intermediate
layers (transbts [31], transunet [3]), a fully transformer network (nnformer
[34]) as well as a partially convnext network (3d-ux-net [17]). transunet is a
2d network while the rest are 3d networks. the uniform framework provides a
common testbed for all networks, without incentivizing one over the other on
aspects of patch size, spacing, augmentations, training and evaluation.
we use 4 popular tasks, encompassing organ as well as tumor segmentation tasks,
to comprehensively demonstrate the benefits of the mednext architecture -1)
beyond-the-cranial-vault (btcv) abdominal ct organ segmentation [16], 2) amos22
abdominal ct organ segmentation [14] 3) kidney tumor segmentation challenge 2019
dataset (kits19) [11], 4) brain tumor segmentation challenge 2021 (brats21) [1].
btcv, amos22 and kits19 datasets contain 30, 200 and 210 ct volumes with 13, 15
and 2 classes respectively, while the brats21 dataset contains 1251 mri volumes
with 3 classes. this diversity shows the effectiveness of our methods across
imaging modalities and training set sizes.
we ablate the mednext-b configuration on amos22 and btcv datasets to highlight
the efficacy of our improvements and demonstrate that a vanilla convnext is
unable to compete with existing segmentation baselines such as nnunet. the
following are observed in ablation tests in 3. the performance boost in large
kernels is seen to be due to the combination of upkern with a larger kernel and
not merely a longer effective training schedule (upkern vs trained 2×), as a
trained mednext-b with kernel 3×3×3 retrained again is unable to match its large
kernel counterpart.this highlights that the mednext modifications successfully
translate the convnext architecture to medical image segmentation. we further
establish the performance of the mednext architecture against our baselines
-comprising of convolutional, transformer-based and large kernel baselines -on
all 4 datasets. we discuss the effectiveness of the mednext on multiple levels.
there are 2 levels at which mednext successfully overcomes existing baselines -5
fold cv and public testset performance. in 5-fold cv scores in table 2,
med-next, with 3 × 3 × 3 kernels, takes advantage of depth and width scaling to
provide state-of-the-art segmentation performance against every baseline on all
4 datasets with no additional training data. mednext-l outperforms or is
competitive with smaller variants despite task heterogeneity (brain and kidney
tumors, organs), modality (ct, mri) and training set size (btcv: 18 samples vs
brats21: 1000 samples), establishing itself as a powerful alternative to
established methods such as nnunet. with upkern and 5 × 5 × 5 kernels, mednext
takes advantage of full compound scaling to improve further on its own small
kernel networks, comprehensively on organ segmentation (btcv, amos22) and in a
more limited fashion on tumor segmentation (kits19, brats21).furthermore, in
leaderboard scores on official testsets (fig. 1c), 5-fold ensembles for
mednext-l (kernel: 5 × 5 × 5) and nnunet, its strongest competitor are compared
-1) btcv: mednext beats nnunet and, to the best of our knowledge, is one of the
leading methods with only supervised training and no extra training data (dsc:
88.76, hd95: 15.34), 2) amos22: mednext not only surpasses nnunet, but is also
rank 1 (date: 09.03.23) currently on the leaderboard (dsc: 91.77, nsd: 84.00),
3) kits19: mednext exceeds nnunet performance (dsc: 91.02), 4) brats21: mednext
surpasses nnunet in both volumetric and surface accuracy (dsc: 88.01, hd95:
10.69). mednext attributes its performance solely to its architecture without
leveraging techniques like transfer learning (3d-ux-net) or repeated 5-fold
ensembling (unetr, swinunetr), thus establishing itself as the state-of-the-art
for medical image segmentation.
in comparison to natural image analysis, medical image segmentation lacks
architectures that benefit from scaling networks due to inherent domain
challenges such as limited training data. in this work, mednext is presented as
a scalable transformer-inspired fully-convnext 3d segmentation architecture
customized for high performance on limited medical image datasets. we
demonstrate med-next's state-of-the-art performance across 4 challenging tasks
against 7 strong baselines. additionally, similar to convnext for natural images
[22], we offer the compound scalable mednext design as an effective
modernization of standard convolution blocks for building deep networks for
medical image segmentation.
fluorodeoxyglucose positron emission tomography (pet) is widely recognized as an
essential tool in oncology [10], playing an important role in the stag-ing,
monitoring, and follow-up radiotherapy (rt) planning [2,19]. delineation of
region of interest (roi) is a crucial step in rt planning. it enables the
extraction of semi-quantitative metrics such as standardized uptake values
(suvs), which normalize pixel intensities based on patient weight and
radiotracer dose [20]. manual delineation is a time-consuming and laborious task
that is prone to poor reproducibility in medical imaging, and this is
particularly true for pet, due to its low signal-to-noise ratio and limited
spatial resolution [10]. in addition, manual delineation depends heavily on the
expert's prior knowledge, which often leads to large inter-observer and
intra-observer variations [8]. therefore, there is an urgent need for developing
accurate automatic segmentation algorithms in pet images which will reduce
expert workload, speed up rt planning while reducing intra-observer
variability.in the last decade, cnns have demonstrated remarkable achievements
in medical image segmentation tasks. this is primarily due to their ability to
learn informative hierarchical features directly from data. however, as
illustrated in [9,23], it is rather difficult for cnns to recognize the object
boundary precisely due to the information loss in the successive downsampling
layers. despite the headway made in using cnns, their applications have been
restricted to the generation of pixel-wise segmentation maps instead of smooth
contour. although cnns may yield satisfactory segmentation results, low values
of the loss function may not always indicate a meaningful segmentation. for
instance, a noisy result can create incorrect background contours and blurry
object boundaries near the edge pixels [6]. to address this, a kernel
smoothing-based probability contour (kspc) approach was proposed in our previous
work [22]. instead of a pixel-wise analysis, we assume that the true suvs come
from a smooth underlying spatial process that can be modelled by kernel
estimates. the kspc provides a surface over images that naturally produces
contour-based results rather than pixel-wise results, thus mimicking experts'
hand segmentation. however, the performance of kspc depends heavily on the
tuning parameters of bandwidth and threshold in the model, and it lacks
information from other patients.beyond tumour delineation, another important use
of functional images, such as pet images is their use for designing imrt dose
painting (dp). in particular, dose painting uses functional images to paint
optimised dose prescriptions based on the spatially varying radiation
sensitivities of tumours, thus enhancing the efficacy of tumour control [14,18].
one of the popular dp strategies is dose painting by contours (dpbc), which
assigns a homogeneous boost dose to the subregions defined by suv thresholds.
however, there is an urgent need to develop image segmentation approaches that
reproducibly and accurately identify the high recurrent-risk contours [18]. our
previously proposed kspc provides a clear framework to calculate the probability
contours of the suv values and can readily be used to define an objective
strategy for segmenting tumours into subregions based on metabolic activities,
which in turn can be used to design the imrt dp strategy.to address both tumour
delineation and corresponding dose painting challenges, we propose to combine
the expressiveness of deep cnns with the versa-tility of kspc in a unified
framework, which we call kspc-net. in the proposed kspc-net, a cnn is employed
to learn directly from the data to produce the pixel-wise bandwidth feature map
and initial segmentation map, which are used to define the tuning parameters in
the kspc module. our framework is completely automatic and differentiable. more
specifically, we use the classic unet [17] as the cnn backbone and evaluate our
kspc-net on the publicly available miccai hecktor (head and neck tumor
segmentation) challenge 2021 dataset. our proposed kspc-net yields superior
results in terms of both dice similarity scores and hausdorff distance compared
to state-of-art models. moreover, it can produce contour-based segmentation
results which provide a more accurate delineation of object edges and provide
probability contours as a byproduct, which can readily be used for dp planning.
kernel-based method and follow up approach of modal clustering [13,16] have been
used to cluster high-dimensional random variables and natural-scene image
segmentation. in this work, we propose to model the pixel-specific suv as a
discretized version of the underlying unknown smooth process of "metabolic
activity". the smooth process can then be estimated as kernel smoothed surface
of the suvs over the domain of the entire slice. in particular, let y = (y 1 , y
2 , ..., y n ) denote n pixel's suv in a 2d pet image sequentially, and x i = (x
i1 , x i2 ), i = 1, ..., n denote position vector with x i1 and x i2 being the
position in 2d respectively. note that x i ∈ r d and d = 2 in our case. we
assume that for each position vector x, the suv represents the frequency of x
appearing in the corresponding grid. the suv surface can therefore be modelled
as kernel density estimate (kde) [3,15] of an estimated point x, which is
defined generally aswhere k is a kernel function and h is a symmetric, positive
definite, d×d matrix of smoothing tuning parameters, called bandwidth which
controls the orientation and amount of smoothing via the scaled kernelon the
other hand, since x t is counted y i times at the same position, eq. 1 can be
further simplified asa scaled kernel is positioned so that its mode coincides
with each data point x i which is expressed mathematically as k h (x-x i ). in
this paper, we have used a guassian kernel which is denoted as:which is a normal
distribution with mean x i and variance-covariance matrix h. therefore, we can
interpret f in eq. ( 2) as the probability mass of the data point x which is
estimated by smoothing the suvs of the local neighbourhood using the gaussian
kernel. the resulting surface built by the kde process can be visualized in fig.
1(c). by placing a threshold plane, a contour-based segmentation map can
naturally be obtained. note that one can obtain a pixel-based segmentation map,
by thresholding the surface at the observed grid points. after delineating the
gross tumour volume, a follow-up application of the kernel smoothed surface is
to construct probability contours. mathematically, a 100 ω% region of a density
f is defined as the level set l(f ω ) = {f (x) ≥ f ω } with its corresponding
contour level f ω such that p(x∈ l(f ω ) = 1ω, where x is a random variable and
l(f ω ) has a minimal hypervolume [11]. in other words, for any ω ∈ (0, 1), the
100 ω% contour refers to the region with the smallest area which encompasses 100
ω% of the probability mass of the density function [11]. in practice, f ω can be
estimated using the following result.result. the estimated probability contour
level f ω can be computed as the ω-th quantile of fω of f (x 1 ; h), ..., f (x n
; h) (proof in supplementary materials).the primary advantage of utilizing
probability contours is their ability to assign a clear probabilistic
interpretation on the defined contours, which are scale-invariant [5]. this
provides a robust definition of probability under the perturbation of the input
data. in addition, these contours can be mapped to the imrt dose painting
contours, thus providing an alternative prescription strategy for imrt. examples
of the application of probability contours will be demonstrated and explained in
sect. 4.2.
in the kspc module, the model performance heavily depends on the bandwidth
matrix h and it is often assumed that each kernel shares the same scalar
bandwidth parameter. however, one may want to use different amounts of smoothing
in the kernel at different grid positions. the commonly used approach for
bandwidth selection is cross-validation [4], which is rather time-consuming even
in the simpler scalar situation. in this paper, we instead use the classic
2d-unet [17] as our cnn backbone to compute the pixel-level bandwidth feature
map, which informs the kspc bandwidth. additionally, we obtain the optimal
threshold for constructing the kspc contour from the initial segmentation map.
as shown in fig. 2 the proposed kspc-net integrates the kspc approach with a cnn
backbone (unet) in an end-to-end differentiable manner. first, the initial
segmentation map and pixel-level bandwidth parameter map h(x i1 , x i2 ) of kspc
are learned from data by the cnn backbone. then the kspc module obtains the
quantile threshold value for each image by identifying the quantile
corresponding to the minimum suv of the tumour class in the initial segmentation
map. the next step involves transmitting the bandwidth map, quantile threshold,
and raw image to kspc module to generate the segmentation map and its
corresponding probability contours. the resulting output from kspc is then
compared to experts' labels using a dice similarity loss function, referred to
kspc loss. additionally, the initial unet segmentation can produce another loss
function, called cnn loss, which serves as an auxiliary supervision for the cnn
backbone. the final loss can then be constructed as the weighted sum of cnn loss
and kspc loss. by minimizing the final loss, the error can be backpropagated
through the entire kspc architecture to guide the weights updating the cnn
backbone.
the dice similarity coefficient is widely employed to evaluate segmentation
models. we utilize the dice loss function to optimize the model performance
during training, which is defined as:, where y i is the label from experts and
ŷi is the predicted label of i-th pixel. n is the total number of pixels and is
a small constant in case of zero division.as shown in fig. 2, we construct the
weighted dice loss to train the model as follows:where l f inal denotes the
weighed dice loss while l cnn and l ksp c denotes the cnn loss and kspc loss,
respectively. in addition, α is a balancing parameter and is set to be 0.01 in
this work.
the dataset is from the hecktor challenge in miccai 2021 (head and neck tumor
segmentation challenge). the hecktor training dataset consists of 224 patients
diagnosed with oropharyngeal cancer [1]. for each patient, fdg-pet input images
and corresponding labels in binary description (0 s and 1 s) for the primary
gross tumour volume are provided and co-registered to a size of 144 × 144 × 144
using bounding box information encompassing the tumour. five-fold
cross-validation is used to generalize the performance of models.
we used python and a trained network on a nvidia dual quadro rtx machine with 64
gb ram using the pytorch package. we applied a batch size of 12 and the adam
algorithm [12] with default parameters to minimize the dice loss function. all
models were trained for 300 epochs. each convolutional layer is followed by relu
activation and batch normalization.
to evaluate the performance of our kspc-net, we compared it with the results of
5-fold cross-validation against three widely-used models namely, the standard 2d
unet, the 2d residual unet and the 3d unet. additionally, we compare our
performance against newly developed approaches msa-net [7] and ccut-net [21]
which were reported in the hecktor 2021 challenges [1]. to quantify the
performance, we report several metrics including dice similarity scores,
precision, recall, and hausdorff distance. table 1 shows the quantitative
comparison of different approaches on hecktor dataset. it is worth mentioning
that since our kspc-net is in a 2d unet structure, the hausdorff distance here
was calculated on slice averages to use a uniform metric across all 2d and 3d
segmentation models. however, the results of 2d hausdorff distances of msa-net
and ccut-net are not available and therefore they are omitted in the table of
comparison. the results clearly demonstrate that the proposed kspc-net is
effective in segmenting h&n tumours, achieving a mean dice score of 0.768. this
represents a substantial improvement over alternative approaches, including
2d-unet (0.740), 3d u-net (0.764), residual-unet (0.680), msa-net (0.757) and
ccut-net (0.750). while we acknowledge that there was no statistically
significant improvement compared to other sota models, it is important to note
that our main goal is to showcase the ability to obtain probability contours as
a natural byproduct while preserving state-of-the-art accuracy levels. on the
other hand, in comparison to the baseline 2d-unet model, kspc-net yields a
higher recall (0.911) with a significant improvement (4.35%), indicating that
kspc-net generates fewer false negatives (fn). although the precision of
kspc-net is slightly lower than the best-performing method (3d unet), it
achieves a relatively high value of 0.793. in addition, the proposed kspc-net
achieves the best performance on hausdorff distance among the three commonly
used unet models (2d-unet, res-unet and 3d-unet), which indicates that kspc-net
exhibits a stronger capacity for accurately localizing the boundaries of
objects. this is consistent with the mechanisms of kspc, which leverages
neighbouring weights to yield outputs with enhanced smoothness.
one of the byproducts of using the kernel-smoothed densities to model the suvs
is the associated probability contours, which can be readily used to develop a
comprehensive inferential framework and uncertainty quantification. for example,
fig. 3 provides two examples of pet image segmentation maps by kspc-net and
their corresponding probability contours in the last column. there are 5
contours in each case which is linear in probability space, in the sense that
each contour encloses 10%, 30%, 50%, 70% and 90% probability mass respectively
(from inner to outer), thus dividing the density surface into subregions with
attached probability mass. these probability contours can provide a rigorous
framework for designing the number and magnitude of suv thresholds for designing
optimal dp strategies. since the suvs are smoothed by the kernel density
heights, the inner 10% probability contour corresponds to the subregion with
relatively higher suvs. in other words, there is an inverse mapping between the
probability contours and the amount of dose boost assigned to subvolumes.
in this paper, we present a novel network, kspc-net, for the segmentation in 2d
pet images, which integrates kspc into the unet architecture in an end-toend
differential manner. the kspc-net utilizes the benefits of kspc to deliver both
contour-based and grid-based segmentation outcomes, leading to improved
precision in the segmentation of contours. promising performance was achieved by
our proposed kspc-net compared to the state-of-the-art approaches on the miccai
2021 challenge dataset (hecktor). it is worth mentioning that the architecture
of our kspc-net is not limited to head & neck cancer type and can be broadcast
to different cancer types. additionally, a byproduct application of our kspc-net
is to construct probability contours, which enables probabilistic interpretation
of contours. the subregions created by probability contours allow for a strategy
planning for the assigned dose boosts, which is a necessity for the treatment
planning of radiation therapy for cancers.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8 51.
extracting brain tumors from medical image scans plays an important role in
further analysis and clinical diagnosis. typically, a brain tumor includes
peritumoral edema, enhancing tumor, and non-enhancing tumor core. since
different modalities present different clarity of brain tumor components, we
often use multi-modal image scans, such as t1, t1c, t2, and flair, in the task
of brain tumor segmentation [12]. works have been done to handle brain tumor
segmentation using image scans collected from all four modalities [11,15].
however, in practice, we face the challenge of collecting all modalities at the
same time, with often one or more missing. therefore, in this paper, we consider
the problem of segmenting brain tumors with missing image modalities.current
image segmentation methods for handling missing modalities can be divided into
three categories, including: 1) brute-force methods: designing individual
segmentation networks for each possible modality combination [18], 2) completion
methods: synthesizing the missing modalities to complete all modalities required
for conventional image segmentation methods [16], and 3) fusionbased methods:
mapping images from different modalities into the same feature space for fusion
and then segmenting brain tumors based on the fused features [10]. methods in
the first category have good segmentation performance; however, they are
resource intensive and often require more training time. the performance of
methods in the second category is limited by the synthesis quality of the
missing modality. the third category often has one single network to take care
of different scenarios of missing modalities, which is the most commonly used
one in practice.to handle various numbers of modal inputs, hemis [5] projects
the image features of different modalities into the same feature space, by
computing the mean and variance of the feature maps extracted from different
modalities as the fused features. to improve the representation of feature
fusion, hved [3] treats the input of each modality as a gaussian distribution,
and fuses feature maps from different modalities through a gaussian mixture
model. robustseg [1], on the other hand, decomposes the modality features into
modality-invariant content code and modality-specific appearance code, for more
accurate fusion and segmentation. considering the different clarity of brain
tumor regions observed in different modalities, rfnet [2] introduces an
attention mechanism to model the relations of modalities and tumor regions
adaptively. based on graph structure and attention mechanism, mfi [21] is
proposed to learn adaptive complementary information between modalities in
different missing situations.due to the complexity of current models, we tend to
develop a simple model, which adopts a simple average fusion and attention
mechanism. these two techniques are demonstrated to be effective in handling
missing modalities and multimodal fusion [17]. inspired by maml [20], we propose
a model called a2fseg (average and adaptive fusion segmentation network, see
fig. 1), which has two fusion steps, i.e., an average fusion and an
attention-based adaptive fusion, to integrate features from different modalities
for segmentation. although our fusion idea is quite simple, a2fseg achieves
state-of-the-art (sota) performance in the incomplete multimodal brain tumor
image segmentation task on the brats2020 dataset. our contributions in this
paper are summarized below:-we propose a simple multi-modal fusion network,
a2fseg, for brain tumor segmentation, which is general and can be extended to
any number of modalities for incomplete image segmentation. -we conduct
experiments on the brats 2020 dataset and achieve the sota segmentation
performance, having a mean dice core of 89.79% for the whole tumor, 82.72% for
the tumor core, and 66.71% for the enhancing tumor.
figure 1 presents the network architecture of our a2fseg. it consists of four
modality-specific sub-networks to extract features from each modality, an
average fusion module to simply fuse features from available modalities at the
first stage, and an adaptive fusion module based on an attention mechanism to
adaptively fuse those features again at the second stage.modality-specific
feature extraction (msfe) module. before fusion, we first extract features for
every single modality, using the nnunet model [7] as shown in fig. 1. in
particular, this msfe model takes a 3d image scan from a specific modality m,
i.e., i m ∈ r h×w ×d and m ∈ {t1, t2, t1c, flair}, and outputs the corresponding
image featureshere, the number of channels is c = 32; h f , w f , and d f are
the height, width, and depth of feature maps f m , which share the same size as
the input image. for every single modality, each msfe module is supervised by
the image segmentation mask to fasten its convergence and provide a good feature
extraction for fusion later. all four msfes have the same architecture but with
different weights.average fusion module. to aggregate image features from
different modalities and handle the possibility of missing one or more
modalities, we use the average of the available features from different
modalities as the first fusion result. that is, we obtain a fused average
featurehere, n m is the number of available modalities. for example, as shown in
fig. 1, if only the first two modalities are available at an iteration, then n m
= 2, and we will take the average of these two modalities, ignoring those
missing ones.adaptive fusion module. since each modality contributes differently
to the final tumor segmentation, similar to maml [20], we adopt the attention
mechanism to measure the voxel-level contributions of each modality to the final
segmentation. as shown in fig. 1, to generate the attention map for a specific
modality m, we take the concatenation of its feature extracted by the msfe
module f m and the mean feature after the average fusion f, which is passed
through a convolutional layer to generate the initial attention weights:here, f
m is a convolutional layer for this specific modality m, and θ m represents the
parameters of this layer, and σ is a sigmoid function. that is, we have an
individual convolution layer f m for each modality to generate different
weights.due to the possibility of missing modalities, we will have different
numbers of feature maps for fusion. to address this issue, we normalize the
different attention weights by using a softmax function:that is, we only
consider feature maps from those available modalities but normalize their
contribution to the final fusion result, so that, the fused one has a consistent
value range, no matter how many modalities are missing. then, we perform
voxel-wise multiplication of the attention weight with the corresponding modal
feature maps. as a result, the adaptively fused feature maps f is calculated by
the weighted sum of each modal feature:here, ⊗ indicates the voxel-wise
multiplication.loss function. we have multiple segmentation heads, which are
distributed in each module of a2fseg. for each segmentation head, we use the
combination of the cross-entropy and the soft dice score as the basic loss
function, which is defined aswhere ŷ and y represent the segmentation prediction
and the ground truth, respectively. based on this basic one, we have the overall
loss function defined aswhere the first term is the basic segmentation loss for
each modality m after feature extraction; the second term is the loss for the
segmentation output of the average fusion module; and the last term is the
segmentation loss for the final output from the adaptive fusion module. 3
experiments
our experiments are conducted on brats2020, which contains 369 multicontrast mri
scans with four modalities: t1, t1c, t2, and flair. these images went through a
sequence of preprocessing steps, including co-registration to the same
anatomical template, resampling to the same resolution (1 mm 3 ), and
skullstripping. the segmentation masks have three labels, including the whole
tumor (abbreviated as complete), tumor core (abbreviated as core), and enhancing
tumor (abbreviated as enhancing). these annotations are manually provided by one
to four radiologists according to the same annotation protocol.
we implement our model with pytorch [13] and perform experiments on an nvidia
rtx3090 gpu. we use the adam optimizer [8], with an initial learning rate of
0.01. since we use the method of exponential decay of learning rate, the initial
learning rate is then multiplied by (1 -#epoch #max_epoch ) 0.9 . due to the
limitation of gpu memory, each volume is randomly cropped into multiple patches
with the size of 128 × 128 × 128 for training. the network is trained for 400
epochs. in the inference stage, we use a sliding window to produce the final
segmentation prediction of the input image.
to evaluate the performance of our model, we compare it with four recent models,
hemis [5], u-hved [3], mmformer [19], and mfi [21]. the dataset is randomly
split into 70% for training, 10% for validation, and 20% for testing, and all
methods are evaluated on the same dataset and data splitting. we use the dice
score as the metric. as shown in table 1, our method achieves the best result.
for example, our method outperforms the current sota method mfi [21] in most
missing-modality cases, including all cases for the whole/complete tumor, 8 out
of 15 cases for the tumor core, 12 out of 15 cases for the enhancing tumor.
compared to mfi, for the whole tumor, tumor core, and enhancing tumor regions,
we improve the average dice scores by 0.99%, 0.41%, and 0.77%, respectively.
although the design of our model is quite simple, these results demonstrate its
effectiveness for the incomplete multimodel segmentation task of brain tumors.
figure 2 visualizes the segmentation results of samples from the brats2020
dataset. with only one flair image available, the segmentation results of the
tumor core and enhancing tumor are poor, because little information on these two
regions is observed in the flair image. with an additional t1c image, the
segmentation results of these two regions are significantly improved and quite
close to the ground truth. although adding t1 and t2 images does not greatly
improve the segmentation of the tumor core and the enhancing tumor, the boundary
of the whole tumor is refined with their help.figure 3 visualizes the
contribution to each tumor region from each modality. the numbers are the mean
values of the attention maps computed for images in the test set. overall, in
our model, each modality has its contribution to the final segmentation, and no
one dominates the result. this is because we have supervision on the
segmentation branch of each modality, so that, each modality has the ability to
segment each region to some extent. however, we still observe that flair and t2
modalities have relatively larger contributions to the segmentation of all tumor
regions, followed by t1c and then t1. this is probably because the whole tumor
area is much clear in flair and t2 compared to the other two modalities. each
modality shows its preference when segmenting different regions. flair and t2
are more useful for extracting the peritumoral edema (ed) than the enhancing
tumor (et) and the non-enhancing tumor and necrosis (ncr/net); while t1c and t1
are on the opposite and more helpful for extracting et and ncr/net.
in this part, we investigate the effectiveness of the average fusion module and
the adaptive fusion module, which are two important components of our method.
firstly, we set a baseline model without any modal interaction, that is, with
the average fusion module only. then, we add the adaptive fusion module to the
baseline model. table 2 reports this ablation study. with only adding the
average fusion module, our method already obtains comparable performance with
the current sota method mfi. by adding the adaptive fusion module, the dice
scores of the three regions further increase by 0.50%, 0.72%, and 0.71%,
respectively. this shows that both the average fusion module and the adaptive
fusion module are effective in this brain tumor segmentation task.
in this paper, we propose an average and adaptive fusion segmentation network
(a2fseg) for the incomplete multi-model brain tumor segmentation task. the
essential components of our a2fseg network are the two stages of feature fusion,
including an average fusion and an adaptive fusion. compare to existing
complicated models, our model is much simpler and more effective, which is
demonstrated by the best performance on the brats 2020 brain tumor segmentation
task. the experimental results demonstrate the effectiveness of two techniques,
i.e., the average fusion and the attention-based adaptive one, for incomplete
modal segmentation tasks.our study brings up the question of whether having
complicated models is necessary. if there is no huge gap between different
modalities, like in our case where all four modalities are images, the image
feature maps are similar and a simple fusion like ours can work. otherwise, we
perhaps need an adaptor or an alignment strategy to fuse different types of
features, such as images and audio.also, we observe that a good feature
extractor is essential for improving the segmentation results. in this paper, we
only explore a reduced unet for feature extraction. in future work, we will
explore other feature extractors, such as vision transformer (vit) or other
pre-trained visual foundation models [4,6,14]. recently, the segment anything
model (sam) [9] demonstrates its general ability to extract different regions of
interest, which is promising to be adopted as a good starting point for brain
tumor segmentation. besides, our model is general for multi-modal segmentation
and we will apply it to other multi-model segmentation tasks to evaluate its
generalization on other applications.
diffuse glioma is a common malignant tumor with highly variable prognosis across
individuals. to improve survival outcomes, many pre-operative survival
prediction methods have been proposed with success. based on the prediction
results, personalized treatment can be achieved. for instance, isensee et al.
[1] proposed a random forest model [2], which adopts the radiomics features [3]
of the brain tumor images, to predict the overall survival (os) time of diffuse
glioma patients. nie et al. [4] developed a multi-channel 3d convolutional
neural network (cnn) [5] to learn features from multimodal mr brain images and
classify os time as long or short using a support vector machine (svm) model
[6]. in [7], an end-to-end cnn-based method was presented that uses multimodal
mr brain images and clinical features such as karnofsky performance score [8] to
predict os time. in [9], an imaging phenotype and genotype based survival
prediction method (pgsp) was proposed, which integrates tumor genotype
information to enhance prediction accuracy.despite the promising results of
existing pre-operative survival prediction methods, they often overlook clinical
knowledge that could aid in improving the prediction accuracy. notably, tumor
types have been found to be strongly correlated with the prognosis of diffuse
glioma [10]. unfortunately, tumor type information is unavailable before
craniotomy. to address this limitation, we propose a new pre-operative survival
prediction method that integrates a tumor subtyping network into the survival
prediction backbone. the subtyping network is responsible for learning
tumor-type-related features from pre-operative multimodal mr brain images.
concerning the inherent issue of imbalanced tumor types in the training data
collected in clinic, a novel ordinal manifold mixup based feature augmentation
is presented and applied in the training stage of the tumor subtyping network.
unlike the original manifold mixup [11], which ignores the feature distribution
of different classes, in the proposed ordinal manifold mixup, feature
distribution of different tumor types is encouraged to be in the order of risk
grade, and the augmented features are produced between neighboring risk grades.
in this way, inconsistency between the augmented features and the corresponding
labels can be effectively reduced.our method is evaluated using pre-operative
multimodal mr brain images of 1726 diffuse glioma patients collected from
cooperation hospitals and a public dataset brats2019 [12] containing multimodal
mr brain images of 210 patients. our method achieves the highest prediction
accuracy of all state-of-the-art methods under evaluation. in addition, ablation
study further confirms the effectiveness of the proposed tumor subtyping network
and the ordinal manifold mixup.
diffuse glioma can be classified into three histological types: the
oligodendroglioma, the astrocytoma, and the glioblastoma [10]. the median
survival times (in months) are 119 (oligodendroglioma), 36 (astrocytoma), and 8
(glioblastoma) [13]. so the tumor types have strong correlation with the
prognosis of diffused glioma. based on this observation, we propose a new
pre-operative survival prediction method (see fig. 1). our network is composed
of two parts: the survival prediction backbone and the tumor subtyping network.
the survival prediction backbone is a deep cox proportional hazard model [14]
which takes the multimodal mr brain images of diffuse glioma patients as inputs
and predicts the corresponding risks. the tumor subtyping network is a
classification network, which classifies the patient tumor types and feeds the
learned tumor-type-related features to the backbone to enhance the survival
prediction performance.the tumor subtyping network is trained independently
before being integrated into the backbone. to solve the inherent issue of
imbalanced tumor type in the training data collected in clinic, a novel ordinal
manifold mixup based feature augmentation is applied in the training of the
tumor subtyping network. it is worth noting that the ground truth of tumor
types, which is determined after craniotomy, is available in the training data,
while for the testing data, tumor types are not required, because
tumor-type-related features can be learned from the pre-operative multimodal mr
brain images.
the architecture of the survival prediction backbone, depicted in fig. 1 top,
consists of an encoder e cox with four resblocks [15], a global average pooling
layer (gap), and three fully connected (fc) layers. assume that d = {x 1 , ...,
x n } is the dataset containing pre-operative multimodal mr brain images of
diffuse glioma patients, and n is the number of patients. the backbone is
responsible for deriving features from x i to predict the risk of the patient.
moreover, after the gap of the backbone, the learned featurefrom the tumor
subtyping network (discussed later), and m is the vector dimension which is set
to 128. as f type i has strong correlation with prognosis, the performance of
the backbone can be improved. in addition, information of patient age and tumor
position is also used. to encode the tumor position, the brain is divided into 3
× 3 × 3 blocks, and the tumor position is represented by 27 binary values (0 or
1) with each value for one block. if a block contains tumors, then the
corresponding binary value is 1, otherwise is 0. the backbone is based on the
deep cox proportional hazard model, and the loss function is defined as:where h
θ (x i ) represents the risk of the i-th patient predicted by the backbone, θ
stands for the parameters of the backbone, x i is the input multimodal mr brain
images of the i-th patient, r(t i ) is the risk group at time t i , which
contains all patients who are still alive before time t i , t i is the observed
time (time of death happened) of x i , and δ i = 0/1 for censored/non-censored
patient.
the tumor subtyping network has almost the same structure as the backbone.it is
responsible for learning tumor-type-related features from each input
preoperative multimodal mr brain image x i and classifying the tumor into
oligodendroglioma, astrocytoma, or glioblastoma. the cross entropy is adopted as
the loss function of the tumor subtyping network, which is defined as:where y k
i and p k i are the ground truth (0 or 1) and the prediction (probability) of
the k-th tumor type (k = 1, 2, 3) of the i-th patient, respectively. the learned
tumor-type-related feature f type i ∈ r m is fed to the survival prediction
backbone and concatenated with f cox i learned in the backbone to predict the
risk. in the in-house dataset, the proportions of the three tumor types are
20.9% (oligodendroglioma), 28.7% (astrocytoma), and 50.4% (glioblastoma), which
is consistent with the statistical report in [13]. to solve the imbalance issue
of tumor types in the training of the tumor subtyping network, a novel ordinal
manifold mixup based feature augmentation is presented.
in the original manifold mixup [11], features and the corresponding labels are
augmented using linear interpolation on two randomly selected features (e.g., f
i and f j ). specifically, the augmented feature fi∼j and label ȳi∼j is defined
as:where y k i and y k j stand for the labels of the k-th tumor type of the i-th
and j-th patients, respectively, and λ ∈ [0, 1] is a weighting factor. for
binary classification, the original manifold mixup can effectively enhance the
network performance, however, for the classification of more than two classes,
e.g., tumor types, there exists a big issue.as shown in fig. 2 left, assume that
f i and f j are features of oligodendroglioma (green) and astrocytoma (yellow)
learned in the tumor subtyping network, respectively. the augmented feature fi∼j
(red) produced from the linear interpolation between f i and f j has the
corresponding label ȳi∼j with high probabilities for the tumor types of
oligodendroglioma and astrocytoma. however, since these is no constraint imposed
on the feature distribution of different tumor types, fi∼j could fall into the
distribution of glioblastoma (blue) as shown in fig. 2 left. in this case, fi∼j
and ȳi∼j are inconsistent, which could influence the training and degrade the
performance of the tumor subtyping network. as aforementioned, the survival time
of patients with different tumor types varies largely (oligodendroglioma >
astrocytoma > glioblastoma), so the tumor types can be regarded as risk grade,
which are ordered rather than categorical. based on this assertion and inspired
by [16], we impose an ordinal constraint on the tumor-type-related features to
make the feature distribution of different tumor types in the order of risk
grade. in this way, the manifold mixup strategy can be applied between each two
neighboring tumor types to produce augmented features with consistent labels
that reflect reasonable risk (see fig. 2
normally, the feature distribution of each tumor type is assumed to be
independent normal distribution, so their joint distribution is given by:where f
k , k = 1, 2, 3 represents the feature set of oligodendroglioma, astrocytoma,
and glioblastoma, respectively, μ k and σ 2 k are mean and variance of f k . to
impose the ordinal constraint, we define the desired feature distribution of
each tumor type as n (μ 1 , σ21 ) for k = 1, and n (μ k-1 + δ k , σ2 k ) for k =
2 and 3. in this way, the feature distribution of each tumor type depends on its
predecessor, and the mean feature of each tumor type μk (except μ1 ) is equal to
the mean feature of its predecessor μk-1 shifted by δ k . note that δ k is set
to be larger than 3 × σk to ensure the desired ordering [16]. in this way, the
conditional distribution under the ordinal constraint is defined as:which can be
represented as:where μ1 and σk , k = 1, 2, 3 can be learned by the tumor
subtyping network. finally, the ordinal loss, which is in the form of kl
divergence, is defined as:in our method, μ k and σ 2 k are calculated bywhere φ
θ and g are the encoder and gap of the tumor subtyping network, respectively, θ
is the parameter set of the encoder,stands for the subset containing the
pre-operative multimodal mr brain images of the patients with the k-th tumor
type, n k is the patient number in d k . so we impose the ordinal loss l kl to
the features after the gap of the tumor subtyping network as shown in fig. 1.
since the ordinal constraint, feature distribution of different tumor types
learned in the subtyping network is encouraged to be in the ordered of risk
grade, and features can be augmented between neighboring tumor type. in this
way, inconsistency between the resulting augmented features and labels can be
effectively reduced.the tumor subtyping network is first trained before being
integrated into the survival prediction backbone. in the training stage of the
tumor subtyping network, each input batch contains pre-operative multimodal mr
brain images of n patients and can be divided into k = 3 subsets according to
their corresponding tumor types, i.e., d k , k = 1, 2, 3. with the ordinal
constrained feature distribution, high consistent features can be augmented
between neighboring tumor types. based on the original and augmented features,
the performance of the tumor subtyping network can be enhanced.once the tumor
subtyping network has been trained, it is then integrated into the survival
prediction backbone, which is trained under the constraint of the cox
proportional hazard loss l cox .
in our experiment, both in-house and public datasets are used to evaluate our
method. specifically, the in-house dataset collected in cooperation hospitals
contains pre-operative multimodal mr images, including t1, t1 contrast enhanced
(t1c), t2, and flair, of 1726 patients (age 49.7 ± 13.1) with confirmed diffuse
glioma types. the patient number of each tumor type is 361 (oligodendroglioma),
495 (astrocytoma), and 870 (glioblastoma), respectively. in the 1726 patients,
743 have the corresponding overall survival time (dead, non-censored), and 983
patients have the last visiting time (alive, censored). besides the inhouse
dataset, a public dataset brats2019, including pre-operative multimodal mr
images of 210 non-censored patients (age 61.4 ± 12.2), is adopted as the
external independent testing dataset. all images of the in-house and brats2019
datasets go through the same pre-processing stage, including image normalization
and affine transformation to mni152 [17]. based on the tumor mask of each image,
tumor bounding boxes can be calculated. according to the bounding boxes of all
1936 patients, the size of input 3d image patch is set to 96 × 96 × 64 voxels,
which can cover the entire tumor of every patient.besides our method, four
state-of-the-art methods, including random forest based method (rf) [18], deep
convolutional survival model (deepconvsurv) [19], multi-channel survival
prediction method (mcsp) [20], and imaging phenotype and genotype based survival
prediction method (pgsp) [9], are evaluated. it is worth noting that in the rf
method, 100 decision trees and 390 handcrafted radiomics features are used. the
output of rf, mcsp, and pgsp is the overall survival (os) times in days, while
for deepconvsurv and our method, the output is the risk (deep cox proportional
hazard models). concordance index (c-index) is adopted to quantify the
prediction accuracy:where d = {x 1 , ..., x n } is the dataset containing all
patients, t i and t j are ground truth of survival times of the i-th and j-th
patients, r i and r j are the days predicted by rf, mcsp, and pgsp or risks
predicted by the deep cox proportional hazard models (i.e., deepconvsurv and our
method), 1 x<y = 1 if x < y, else 0, and δ i = 0 or 1 when the i-th patient is
censored or non-censored. as rf, mcsp, and pgsp cannot use the censored data in
the in-house dataset, 80% of the non-censored data (594 patients) are randomly
selected as the training data, and the rest 20% non-censored data (149 patients)
are for testing. while deepconvsurv and our method are deep cox models, both
censored and non-censored patients can be utilized. so besides the 80%
non-censored patients, all censored data (983 patients) are also included in the
training data.table 1 shows the evaluation results of the in-house and the
external independent (brats2019) testing datasets using all methods under
evaluation. our method achieves the highest c-index of all the methods under
evaluation. moreover, comparing with deepconvsurv, our method can improve the
prediction accuracy up to 10% (in-house) and 8% (brats2019).
to show the effect of the tumor subtyping network and the ordinal manifold mixup
in survival prediction, our method without the tumor subtyping network
(baseline-1) and our method with the tumor subtyping network (using original
manifold mixup instead, baseline-2) are evaluated. for the in-house dataset, the
resulting c-indices are 0.744 (baseline-1) and 0.735 (baseline-2). so our method
make the improvement of c-index more than 8% comparing with baseline-2. for the
external independent testing dataset brats2019, the resulting c-indices are
0.738 (baseline-1) and 0.714 (baseline-2), and our method still has more than 6%
improvement comparing with baseline-2.figure 3 shows the distributions of
tumor-type-related features (after the gap) of the in-house testing data in
baseline-2, and our method. principal component analysis [21] is used to project
features to a 2d plane. since the ordinal constraint, the feature distribution
of different tumor types is in the order of risk grade using our method, which
cannot be observed in baseline-2.
we proposed a new method for pre-operative survival prediction of diffuse glioma
patients, where a tumor subtyping network is integrated into the prediction
backbone. based on the tumor subtyping network, tumor type information, which
are only available after craniotomy, can be derived from the pre-operative
multimodal mr images to boost the survival prediction performance. moreover, a
novel ordinal manifold mixup was presented, where ordinal constraint is imposed
to make feature distribution of different tumor types in the order of risk
grade, and feature augmentation only takes place between neighboring tumor
types. in this way, inconsistency between the augmented features and
corresponding labels can be effectively reduced. both in-house and public
datasets containing 1936 patients were used in the experiment. our method
outperformed the state-of-the-art methods in terms of the concordance-index.
segmentation of skin lesions from dermoscopy images is a critical task in
disease diagnosis and treatment planning of skin cancers [17]. manual lesion
segmentation is time-consuming and prone to inter-and intra-observer
variability. to improve the efficiency and accuracy of clinical workflows,
numerous automated skin lesion segmentation models have been developed over the
years [1,2,7,10,18,19,21]. these models have focused on enhancing feature
representations using various techniques such as multi-scale feature fusion
[10], attention mechanisms [1,7], self-attention mechanisms [18,19], and
boundary-aware attention [2,18,19], resulting in significant improvements in
skin lesion segmentation performance. despite these advances, the segmentation
of skin lesions with ambiguous boundaries, particularly at extremely challenging
scales, remains a bottleneck issue that needs to be addressed. in such cases,
even state-of-the-art segmentation models struggle to achieve accurate and
consistent results.
small large fig. 1. the boundary evolution process. it could be seen that
various lesions can be accurately segmented by splitting the segmentation into
sequential timesteps (t), named as boundary evolution in this work.two
representative boundaries are visualized in fig. 1, where one extremely small
lesion and one particularly large lesion are presented. the small one covers
1.03% in the image space and the large one covers 72.96%. as studied prior,
solving the segmentation problems of such two types of lesions have different
strategies. (1) for the small lesions, translating the features at a lower depth
to the convolutional layers at a higher depth can avoid losing local contexts
[10]. (2) for the large lesions, enlarging the receptive field by dilated
convolution [1], and even global attention [18] can capture the long-range
dependencies to improve the boundary decision. besides the challenge of how to
yield stable representations for various scales, multi-scale lesions will cause
training fluctuation, that is, small lesions usually lead to large dice loss.
feeding more boundary-aware supervision can reduce these negative effects to
some degree [2,19]. the latest transformer, xbound-former, comprehensively
addresses the multi-scale boundary problem through cross-scale boundary learning
and exactly reaches higher performance on whatever small or large
lesions.however, current models for skin lesion segmentation are still
struggling with extremely challenging cases, which are often encountered in
clinical practice. while some approaches aim to optimize the model architecture
by incorporating local and global contexts and multi-task supervision, and
others seek to improve performance by collecting more labeled data and building
larger models, both strategies are costly and can be limited by the inherent
complexity of skin lesion boundaries. therefore, we propose a novel approach
that shifts the focus from merely segmenting lesion boundaries to predicting
their evolution. our approach is inspired by recent advances in image synthesis
achieved by diffusion probabilistic models [6,9,14,15], which generate synthetic
samples from a randomly sampled gaussian distribution in a series of finite
steps. we adapt this process to model the evolution of skin lesion boundaries as
a parameterized chain process, starting from gaussian noise and progressing
through a series of denoising steps to yield a clear segmentation map with
well-defined lesion boundaries. by predicting the next step in the chain process
rather than the final segmentation map, our approach enables the more accurate
segmentation of challenging lesions than previous models. we illustrate the
process of boundary evolution in fig. 1, where each row corresponds to a
different step in the evolution process, culminating in a clear segmentation map
with well-defined boundaries.in this paper, we propose a medical boundary diff
usion model (mb-diff ) to improve the skin lesion segmentation, particularly in
cases where the lesion boundaries are ambiguous and have extremely large or
small sizes. the mb-diff model follows the basic design of the plain diffusion
model, using a sequential denoising process to generate the lesion mask.
however, it also includes two key innovations: firstly, we have developed an
efficient multi-scale image guidance module, which uses a pretrained transformer
encoder to extract multi-scale features from prior images. these features are
then fused with the evolution features to constrain the direction of evolution.
secondly, we have implemented an evolution uncertainty-based fusion strategy,
which takes into account the uncertainty of different initializations to refine
the evolution results and obtain more precise lesion boundaries. we evaluate our
model on two popular skin lesion segmentation datasets, isic-2016 and ph 2
datasets, and find that it performs significantly better than existing models.
the key objective of mb-diff is to improve the representation of ambiguous
boundaries by learning boundary evolution through a cascaded series of steps,
rather than a single step. in this section, we present the details of our
cascaded boundary evolution learning process and the parameterized architecture
of the evolution process. we also introduce our evolution-based uncertainty
estimation and boundary ensemble techniques, which have significant potential
for enhancing the precision and reliability of the evolved boundaries.
we adopt a step-by-step denoising process to model boundary evolution in
mb-diff, drawing inspiration from recent diffusion probabilistic models (dpms).
specifically, given the image and boundary mask distributions as (x , y),
assuming that the evolution consists of t steps in total, the boundary at t -th
step (y t ) is the randomly initialized noise and the boundary at 0-th (y 0 )
step denotes the accurate result. we formulate the boundary evolution process as
follows:where p(y t ) = n (y t ; 0, i) is the initialized gaussian distribution
and p θ (y t-1 |y t ) is each learnable evolution step, formulated as the
gaussian transition, denoted as:note that the prediction function takes the
input image as a condition, enabling the evolving boundary to fit the
corresponding lesion accurately. by modeling boundary evolution as a
step-by-step denoising process, mb-diff can effectively capture the complex
structures of skin lesions with ambiguous boundaries, leading to superior
performance in lesion segmentation.to optimize the model parameters θ, we use
the evolution target as an approximation of the posterior at each evolution
step. given the segmentation label y as y 0 , the label is gradually added by a
gaussian noise as:where {β t } t t=1 is a set of constants ranging from 0 to 1.
after that, we compute the posterior q(y t-1 |y t , y 0 ) using bayes' rule. the
mse loss function is utilized to measure the distance between the predicted mean
and covariance of the gaussian transition distribution and the evolution target
q(y t-1 |y t , y 0 ).
the proposed model is a parameterized chain process that predicts the μ * t-1
and * t-1 at each evolution step t under the prior conditions of the image x and
the prior evolution y * t . to capture the deep semantics of these conditions
and perform efficient fusion, we adopt a basic u-net [16] architecture inspired
by the plain dpm and introduce novel designs for condition fusion, that is the
efficient multi-scale image guidance module.the architecture consists of a
multi-level convolutional encoder and a symmetric decoder with short connection
layers between them. to incorporate the variable t into the model, we first
embed it into the latent space. then, the prior evolution y * t is added to the
latent t before each convolution. at the bottleneck layer, we fuse the evolution
features with the image guidance to constrain the evolution and ensure that the
final boundary suits the conditional image.to achieve this, priors train a
segmentation model concurrently with the evolution model and use an
attention-based parser to translate the image features in the segmentation
branch into the evolution branch [22]. since the segmentation model is trained
much faster than the evolution model, we adopt a pretrained pyramid vision
transformer (pvt) [20] as the image feature extractor to obtain the multi-scale
image features. let {f l } 4 l=1 denote the extracted features at four levels,
with a 2x, 4x, 8x, 16x, smaller size of the original input. each feature at the
three lower levels is resized to match the scale of f 4 using adaptive averaging
pooling layers. after that, the four features are concatenated and fed into a
fullconnection layer to map the image feature space into the evolution space. we
then perform a simple yet effective addition of the mapped image feature and the
encoded prior evolution feature, similar to the fusion of time embeddings, to
avoid redundant computation.
similar to typical evolutionary algorithms, the final results of boundary
evolution are heavily influenced by the initialized population. as a stochastic
chain process, the boundary evolution process may result in different endpoints
due to the random gaussian samples at each evolution step. this difference is
particularly evident when dealing with larger ambiguity in boundary regions. the
reason is that the image features in such ambiguous regions may not provide
discriminative guidance for the evolution, resulting in significant variations
in different evolution times. instead of reducing the differences, we
surprisingly find that these differences can represent segmentation uncertainty.
based on the evolution-based uncertainty estimation, the segmentation results
become more accurate and trustworthy in practice [4,5,12].uncertainty
estimation: to estimate uncertainty, the model parameters θ are fixed, and the
evolution starts with a randomly sampled gaussian noise y * t ∼ n (0, i). let {y
* ,i t } n i=1 denote a total of n initializations. once the evolution is
complete, the obtained {μ * ,i } n i=1 , { * ,i } n i=1 are used to the sample
final lesion maps as: y * ,i = μ * ,i + exp( 1 2 * ,i )n (0, i). unlike
traditional segmentation models that typically scale the prediction into the
range of 0 to 1, the evolved maps generated by mb-diff have unfixed
distributions due to random sampling. since the final result is primarily
determined by the mean value μ, and the predicted has a limited range [6], we
calculate the uncertainty as:evolution ensemble: instead of training multiple
networks or parameters to make the ensemble, mb-diff allows running the
inference multiple times and fusing the obtained evolutions. however, simply
averaging the predicted identities from multiple evolutions is not effective, as
the used mse loss without activation constrains the predicted identities to be
around 0 or 1, unlike the sigmoid function which would limit the identities to a
range between 0 and 1. therefore, we employ the max vote algorithm to obtain the
final segmentation map. in this algorithm, each pixel is classified as a lesion
only if its identity sum across all n evolutions is greater than a threshold
value τ . finally, the segmentation map is generated as3 experiment
datasets: we use two publicly available skin lesion segmentation datasets from
different institutions in our experiments: the isic-2016 dataset and the ph 2
dataset. the isic-2016 dataset [8] is provided by the international skin imaging
collaboration (isic) archive and consists of 900 samples in the public training
set and 379 samples in the public validation set. as the annotation for its
public test set is not currently available, we additionally collect the ph 2
dataset [13], which contains 200 labeled samples and is used to evaluate the
generalization performance of our methods.evaluation metrics: to comprehensively
compare the segmentation results, particularly the boundary delineations, we
employ four commonly used metrics to quantitatively evaluate the performance of
our segmentation methods. these metrics include the dice score, the iou score,
average symmetric surface distance (assd), and hausdorff distance of boundaries
(95-th percentile; hd95).to ensure fair comparison, all labels and predictions
are resized to (512×512) before computing these scores, following the approach
of a previous study [18].
for the diffusion model hyper-parameters, we use the default settings of the
plain diffusion model, which can be found in the supplementary materials.
regarding dataset. we highlight the small lesions using dotted boxes in the
third row.the training parameters, we resize all images to (256 × 256) for
efficient memory utilization and computation. we use a set of random
augmentations, including vertical flipping, horizontal flipping, and random
scale change (limited to 0.9 ∼ 1.1), to augment the training data. we set the
batch size to 4 and train our model for a total of 200,000 iterations. during
training, we use the adamw optimizer with an initial learning rate of 1e-4. for
the inference, we set n = 4 and τ = 2 considering the speeds.
we majorly compare our method to the latest skin lesion segmentation models,
including the cnn-based and transformer-based models, i.e., u-net++ [24], ca-net
[7], transfuse [23], transunet [3], and especially the boundary-enhanced method,
x-boundformer [18]. additionally, we evaluate our method against medsegdiff
[22], a recently released diffusion-based model, which we re-trained for 200,000
steps to ensure a fair comparison. the quantitative results are shown in table
1, which reports four evaluation scores for two datasets. though the parameters
of cnns and transformers are selected with the best performance on isic-2016
validation set and the parameters of our method are selected by completing the
200,000 iterations, mb-diff still achieves the 1.18% iou improvement and 0.7%
dice improvement. additionally, our predicted boundaries are closer to the
annotations, as evidenced by the assd and hd95 metrics, which reduce by 1.02 and
1.93 pixels, respectively. when compared to medsegdiff, mb-diff significantly
outperforms it in all metrics. moreover, our method shows a larger improvement
in generalization performance on the ph 2 dataset, indicating its better ability
to handle new data. we present a visual comparison of challenging samples in
fig. 2, including three samples from the isic-2016 validation set and three from
the ph 2 dataset. these samples represent edge cases that are currently being
studied in the community, including size variation, boundary ambiguity, and
neighbor confusion. our visual comparison reveals several key findings: (1)
mb-diff consistently achieves better segmentation performance on small and large
lesions due to its thorough learning of boundary evolution, as seen in rows 3,
5, and 6. (2) mb-diff is able to produce correct boundaries even in cases where
they are nearly indistinguishable in human perception, eliminating the need for
further manual adjustments and demonstrating significant practical value. (3)
mb-diff generates fewer false positive segmentation, resulting in cleaner
predictions that enhance the user experience.furthermore, we provide a
visualization of evolution uncertainties in fig. 2, where deeper oranges
indicate larger uncertainties. it is evident that most regions with high
uncertainties correspond to false predictions. this information can be used to
guide human refinement of the segmentation in practical applications, ultimately
increasing the ai's trustworthiness.
in this subsection, we make a comprehensive analysis to investigate the
performance of each component in our method and compare it to the
diffusion-based model, medsegdiff. the results of our ablation study are
presented in fig. 3(a), where "w/o evo" refers to using image features to
directly train a segmentation model with fpn [11] architecture and "w/o fusion"
means no evolution fusion is used. to ensure a fair comparison, we average the
scores of multiple evolutions to represent the performance of "w/o fusion". the
results demonstrate that our evolutionary approach can significantly improve
performance, and the evolution uncertainty-based fusion strategy further
enhances performance. comparing our method to medsegdiff, the training loss
curve in fig. 3(b) shows that our method converges faster and achieves smaller
losses, indicating that our multi-scale image guidance is more effective than
that of medsegdiff. furthermore, we evaluate our method's performance using
parameters saved at different iterations, as shown in fig. 3(c). our results
demonstrate that our method has competitive performance at 50k iterations versus
medsegdiff at 200k iterations and our method at 100k iterations has already
outperformed well-trained medsegdiff.
in this paper, we introduced the medical boundary diffusion (mb-diff) model,
which is a novel approach to segment skin lesions. our proposed method
formulates lesion segmentation as a boundary evolution process with finite
timesteps, which allows for efficient and accurate segmentation of skin lesions.
to guide the boundary evolution towards the lesions, we introduce an efficient
multi-scale image guidance module. additionally, we propose an evolution
uncertainty-based fusion strategy to yield more accurate segmentation. our
method is evaluated on two well-known skin lesion segmentation datasets, and the
results demonstrate superior performance and generalization ability in unseen
domains. through a detailed analysis of our training program, we find that our
model has faster convergence and better performance compared to other
diffusion-based models. overall, our proposed mb-diff model offers a promising
solution to accurately segment skin lesions, and has the potential to be applied
in a clinical setting.
accurate tumor segmentation from medical images is essential for quantitative
assessment of cancer progression and preoperative treatment planning [3]. tumor
tissues usually present different features in different imaging modalities. for
example, computed tomography (ct) and positron emission tomography (pet) are
beneficial to represent morphological and metabolic information of tumors,
respectively. in clinical practice, multimodal registered images, such as pet-ct
images and magnetic resonance (mr) images with different sequences, are often
utilized to delineate tumors to improve accuracy. however, manual delineation is
time-consuming and error-prone, with a low inter-professional agreement [12].
these have prompted the demand for intelligent applications that can
automatically segment tumors from multimodal images to optimize clinical
procedures.recently, multimodal tumor segmentation has attracted the interest of
many researchers. with the emergence of multimodal datasets (e.g., brats [25]
and hecktor [1]), various deep-learning-based multimodal image segmentation
methods have been proposed [3,10,13,27,29,31]. overall, large efforts have been
made on effectively fusing image features of different modalities to improve
segmentation accuracy. according to the way of feature fusion, the existing
methods can be roughly divided into three categories [15,36]: input-level
fusion, decisionlevel fusion, and layer-level fusion. as a typical approach,
input-level fusion [8,20,26,31,34] refers to concatenating multimodal images in
the channel dimension as network input during the data processing or
augmentation stage. this approach is suitable for most existing end-to-end
models [6,32], such as u-net [28] and u-net++ [37]. however, the shallow fusion
entangles the low-level features from different modalities, preventing the
effective extraction of high-level semantics and resulting in limited
performance gains. in contrast, [35] and [21] propose a solution based on
decision-level fusion. the core idea is to train an independent segmentation
network for each data modality and fuse the results in a specific way. these
approaches can bring much extra computation at the same time, as the number of
networks is positively correlated with the number of modalities. as a compromise
alternative, layer-level fusion methods such as hyperdense-net [10] advocate the
cross-fusion of the multimodal features in the middle layer of the network.in
addition to the progress on the fusion of multimodal features, improving the
model representation ability is also an effective way to boost segmentation
performance. in the past few years, transformer structure [11,24,30], centered
on the multi-head attention mechanism, has been introduced to multimodal image
segmentation tasks. extensive studies [2,4,14,16] have shown that the
transformer can effectively model global context to enhance semantic
representations and facilitate pixel-level prediction. wang et al. [31] proposed
transbts, a form of input-level fusion with a u-like structure, to segment brain
tumors from multimodal mr images. transbts employs the transformer as a
bottleneck layer to wrap the features generated by the encoder, outperforming
the traditional end-to-end models. saeed et al. [29] adopted a similar structure
in which the transformer serves as the encoder rather than a wrapper, also
achieving promising performance. other works like [9] and [33], which combine
the transformer with the multimodal feature fusion approaches mentioned above,
further demonstrate the potential of this idea for multimodal tumor
segmentation.although remarkable performance has been accomplished with these
efforts, there still exist several challenges to be resolved. most existing
methods are either limited to specific modality numbers due to the design of
asymmetric connections or suffer from large computational complexity because of
the huge amount of model parameters. therefore, how to improve model ability
while ensuring computational efficiency is the main focus of this paper.to this
end, we propose an efficient multimodal tumor segmentation solution named hybrid
densely connected network (h-denseformer). first, our method leverages
transformer to enhance the global contextual information of different
modalities. second, h-denseformer integrates a transformer-based multi-path
parallel embedding (mpe) module, which can extract and fuse multimodal image
features as a complement to naive input-level fusion structure. specifically,
mpe assigns an independent encoding path to each modality, then merges the
semantic features of all paths and feeds them to the encoder of the segmentation
network. this decouples the feature representations of different modalities
while relaxing the input constraint on the specific number of modalities.
finally, we design a lightweight, densely connected transformer (dct) module to
replace the standard transformer to ensure performance and computational
efficiency. extensive experimental results on two publicly available datasets
demonstrate the effectiveness of our proposed method. as the auxiliary extractor
of multimodal fusion features, while the latter is used to generate predictions.
specifically, given a multimodal image input x 3d ∈ r c×h×w ×d or x 2d ∈ r c×h×w
with a spatial resolution of h × w , the depth dimension of d (number of slices)
and c channels (number of modalities), we first utilize mpe to extract and fuse
multimodal image features. then, the obtained features are progressively
upsampled and delivered to the encoder of the segmentation network to enhance
the semantic representation. finally, the segmentation network generates
multi-scale outputs, which are used to calculate deep supervision loss as the
optimization target.
many methods [5,10,15] have proved that decoupling the feature representation of
different modalities facilitates the extraction of high-quality multimodal
features. inspired by this, we design a multip-path parallel embedding (mpe)
module to enhance the representational ability of the network. as shown in fig.
1, each modality has an independent encoding path consisting of a patch
embedding module, stacked densely connected transformer (dct) modules, and a
reshape operation. the independence of the different paths allows mpe to handle
an arbitrary number of input modalities. besides, the introduction of the
transformer provides the ability to model global contextual information. given
the input x 3d , after convolutional embedding and tokenization, the obtained
feature of the i-th path is, where i ∈ [1, 2, ..., c], p = 16 and l = 128 denote
the path size and embedding feature length respectively. first, we concatenate
the features of all modalities and entangle them using a convolution operation.
then, interpolation upsampling is performed to obtain the multimodal fusion
featurewhere k = 128 refers to the channel dimension. finally, f out is
progressively upsampled to multiple scales and delivered to different encoder
stages to enhance the learned representation.
standard transformer structures [11] typically consist of dense linear layers
with a computational complexity proportional to the feature dimension.
therefore, integrating the transformer could lead to a mass of additional
computation and memory requirements. shortening the feature length can
effectively reduce computation, but it also weakens the representation
capability meanwhile. to address this problem, we propose the densely connected
transformer (dct) module inspired by densenet [17] to balance computational cost
and representation capability. figure 1 details the dct module, which consists
of four transformer layers and a feedforward layer. each transformer layer has a
linear projection layer that reduces the input feature dimension to g = 32 to
save computation. different transformer layers are connected densely to preserve
representational power with lower feature dimensions. the feedforward layer at
the end generates the fusion features of the different layers. specifically, the
output z j of the j-th (j ∈ [1,2,3,4]) transformer layer can be calculated
by:where z 0 represents the original input, cat(•) and p(•) denote the
concatenation operator and the linear layer, respectively. the norm(•), att(•),
f (•) are the regular layer normalization, multi-head self-attention mechanism,
and feedforward layer. the output of dct is z out = f (cat([z 0 ; z 1 ; ...; z 4
])). table 1 shows that the stacked dct has lower parameters and computational
complexity than a standard transformer structure with the same number of layers.
the h-denseformer adopts a u-shaped encoder-decoder structure as its backbone.
as shown in fig. 1, the encoder extracts features and reduces their resolution
progressively. to preserve more details, we set the maximum downsampling factor
to 8. the multi-level multimodal features from mpe are fused in a bitwise
addition way to enrich the semantic information. the decoder is used to restore
the resolution of the features, consisting of deconvolutional and convolutional
layers with skip connections to the encoder. in particular, we employ deep
supervision (ds) loss to improve convergence, which means that the multiscale
output of the decoder is involved in the final loss computation. deep
supervision loss. during training, the decoder has four outputs; for example,
the i-th output of 2d h-denseformer is, where i ∈ [0, 1, 2, 3], and c = 2 (tumor
and background) represents the number of segmentation classes. to mitigate the
pixel imbalance problem, we use a combined loss of focal loss [23] and dice loss
as the optimization target, defined as follows:where n refers to the total
number of pixels, p t and q t denote the predicted probability and ground truth
of the t-th pixel, respectively, and r = 2 is the modulation factor. thus, ds
loss can be calculated as follows:where g i represents the ground truth after
resizing and has the same size as o i . α is a weighting factor to control the
proportion of loss corresponding to the output at different scales. this
approach can improve the convergence speed and performance of the network.
to validate the effectiveness of our proposed method, we performed extensive
experiments on hecktor21 [1] and pi-cai221 . hecktor21 is a dualmodality dataset
for head and neck tumor segmentation, containing 224 pet-ct image pairs. each
pet-ct pair is registered and cropped to a fixed size of (144,144,144). pi-cai22
provides multimodal mr images of 220 patients with prostate cancer, including
t2-weighted imaging (t2w), high b-value diffusion-weighted imaging (dwi), and
apparent diffusion coefficient (adc) maps. after standard resampling and center
cropping, all images have a size of (24,384,384). we randomly select 180 samples
for each dataset as the training set and the rest as the independent test set
(44 cases for hecktor21 and 40 cases for pi-cai22). specifically, the training
set is further randomly divided into five folds for cross-validation. for
quantitative analysis, we use the dice similarity coefficient (dsc), the jaccard
index (ji), and the 95% hausdorff distance (hd95) as evaluation metrics for
segmentation performance. a better segmentation will have a smaller hd95 and
larger values for dsc and ji. we also conduct holistic t-tests of the overall
performance for our method and all baseline models with the two-tailed p < 0.05.
we use pytorch to implement our proposed method and the baselines. for a fair
comparison, all models are trained from scratch using two nvidia a100 gpus and
all comparison methods are implemented with open-source codes, following their
original configurations. in particular, we evaluate the 3d and 2d h-denseformer
on hecktor21 and pi-cai22, respectively. during the training phase, the adam
optimizer is employed to minimize the loss with an initial learning rate of 10
-3 and a weight decay of 10 -4 . we use the polylr strategy [19] to control the
learning rate change. we also use an early stopping strategy with a tolerance of
30 epochs to find the best model within 100 epochs. online data augmentation,
including random rotation and flipping, is performed to alleviate the
overfitting problem. table 2 compares the performance and computational
complexity of our proposed method with the existing state-of-the-art methods on
the independent test sets. for hecktor21, 3d h-denseformer achieves a dsc of
73.9%, hd95 of 8.1mm, and ji of 62.5%, which is a significant improvement (p <
0.01) over 3d u-net [7], unetr [16], and transbts [31]. it is worth noting that
the performance of hybrid models such as unetr is not as good as expected, even
worse than 3d u-net, perhaps due to the small size of the dataset. moreover,
compared to the champion solution of hecktor20 proposed by iantsen et al. [18],
our method has higher accuracy and about 10 and 5 times lower amount of network
parameters and computational cost, respectively. for pi-cai22, the 2d variant of
h-denseformer also outperforms existing methods (p < 0.05), achieving a dsc of
49.9%, hd95 of 35.9 mm, and ji of 37.1%. overall, h-denseformer reaches an
effective balance of performance and computational cost compared to existing
cnns and hybrid structures. for qualitative analysis, we show a visual
comparison of the different methods. it is evident from fig. 2 that our approach
can describe tumor contours more accurately while providing better segmentation
accuracy for small-volume targets. these results further demonstrate the
effectiveness of our proposed method in multimodal tumor segmentation tasks.
impact of dct depth. as illustrated in table 3, the network performance varies
with the change in dct depth. h-denseformer achieves the best performance at the
dct depth of 6. an interesting finding is that although the depth of the dct has
increased from 3 to 9, the performance does not improve or even worsen. we
suspect that the reason is over-fitting due to over-parameterization. therefore,
choosing a proper dct depth is crucial to improve accuracy. impact of different
modules. the above results demonstrate the superiority of our method, but it is
unclear which module plays a more critical role in performance improvement.
therefore, we perform ablation experiments on mpe, dct and ds loss.
specifically, w/o mpe refers to keeping one embedding path, w/o dct signifies
using a standard 12-layer transformer, and w/o ds loss denotes removing the deep
supervision mechanism. as shown in table 4, the performance decreases with
varying degrees when removing them separately, which means all the modules are
critical for h-denseformer. we can observe that dct has a greater impact on
overall performance than the others, further demonstrating its effectiveness. in
particular, the degradation after removing the mpe also con- firms that
decoupling the feature expression of different modalities helps obtain
higher-quality multimodal features and improve segmentation performance.
in this paper, we proposed an efficient hybrid model (h-denseformer) that
combines transformer and cnn for multimodal tumor segmentation. concretely, a
multi-path parallel embedding module and a densely connected transformer block
were developed and integrated to balance accuracy and computational complexity.
extensive experimental results demonstrated the effectiveness and superiority of
our proposed h-denseformer. in future work, we will extend our method to more
tasks and explore more efficient multimodal feature fusion methods to further
improve computational efficiency and segmentation performance.
dct depth params↓ gflops↓ dsc (%) ↑ hd95 (mm) ↓ ji (%) ↑ 7 ± 1.2 8.7 ± 0.6 61.2
± 1.3
3d h-denseformer w/o ds loss 72.2 ± 0.9 10.2 ± 1.0 60.1 ± 1.2 3d h-denseformer
73.9 ± 0.5 8.1 ± 0.6 62.5 ± 0.5
accurate cancer diagnosis, grading, and treatment decisions from medical images
heavily rely on the analysis of underlying complex nuclei structures [7]. yet,
due to the numerous nuclei contained in a digitized whole-slide image (wsi), or
even in an image patch of deep learning input, dense annotation of nuclei
contouring is extremely time-consuming and labor-expensive [11]. consequently,
automated nuclei segmentation approaches have emerged to satisfy a broad range
of computer-aided diagnostic systems, where the deep learning methods,
particularly the convolutional neural networks [5,12,14,19,21] have received
notable attention due to their simplicity and generalization ability.in the
literature work, the sole-decoder design in these unet variants (fig. 1(a)) is
susceptible to failures in splitting densely clustered nuclei when precise edge
information is absent. hence, deep contour-aware neural network (dcan) [3] with
bi-decoder structure achieves improved instance segmentation performance by
adopting multi-task learning, in which one decoder learns to segment the nuclei
and the other recognizes edges as described in fig. 1(b). similarly, cia-net
[20] extends dcan with an extra information aggregator to fuse the features from
two decoders for more precise segmentation. much recently, ca 2.5 -net [6] shows
identifying the clustered edges in a multiple-task learning manner can achieve
higher performance, and thereby proposes an extra output path to learn the
segmentation of clustered edges explicitly. a significant drawback of the
aforementioned multi-decoder networks is the ignorance of the prediction
consistency between branches, resulting in sub-optimal performance and missing
correlations between the learned branches. specifically, a prediction mismatch
between the nuclei and edge branches is observed in previous work [8], implying
a direction for performance improvement. to narrow this gap, we propose a
consistency distillation between the branches, as shown by the dashed line in
fig. 1(c). furthermore, to resolve the cost of involving more decoders, we
propose an attention sharing scheme, along with an efficient token mlp
bottleneck [16], which can both reduce the number of parameters.additionally,
existing methods are cnn-based, and their intrinsic convolution operation fails
to capture global spatial information or the correlation amongst nuclei [18],
which domain experts rely heavily on for accurate nuclei allocation. it suggests
the presence of long-range correlation in practical nuclei segmentation tasks.
inspired by the capability in long-range global context capturing by
transformers [17], we make the first attempt to construct a tri-decoder based
transformer model to segment nuclei. in short, our major contributions are
three-fold: (1) we propose a novel multi-task framework for nuclei segmentation,
namely transnuseg, as the first attempt at a fully swin-transformer driven
architecture for nuclei segmentation. (2) to alleviate the prediction
inconsistency between branches, we propose a novel self distillation loss that
regulates the consistency between the nuclei decoder and normal edge decoder.
(3) we propose an innovative attention sharing scheme that shares attention
heads amongst all decoders. by leveraging the high correlation between tasks, it
can communicate the learned features efficiently across decoders and sharply
reduce the number of parameters. furthermore, the incorporation of a
light-weighted mlp bottleneck leads to a sharp reduction of parameters at no
cost of performance decline. fig. 2. the overall framework of the proposed
transnuseg of three output branches to separate the nuclei, normal edges, and
cluster edges, respectively. in the novel design, a pre-defined proportion of
the attention heads are shared between the decoders via the proposed sharing
scheme, which considerably reduces the number of parameters and enables more
efficient information communication.
network architecture overview. figure 2 illustrates the overall architecture of
the proposed multi-task tri-decoder transformer network, named transnuseg. both
the encoder and decoders utilize the swin transformer [13] as the building
blocks to capture the long-range feature correlations in the nuclei segmentation
context. our network consists of three individual output decoder paths for
nuclei segmentation, normal edges segmentation, and clustered edges
segmentation. given the high dependency between edge and clustered edge, we are
inspired to propose a novel attention sharing scheme, which can communicate the
information and share learned features across decoders while also reducing the
number of parameters. additionally, a token mlp bottleneck is incorporated to
further increase the model efficiency.attention sharing scheme. to capture the
strong correlation between nuclei segmentation and contour segmentation between
multiple decoders [15], we introduce a novel attention sharing scheme that is
designed as an enhancement to the multi-headed self-attention (msa) module in
the plain transformer [17]. based on the attention sharing scheme, we design a
shared msa module, which is similar in structure to vanilla msa. specifically,
it consists of a layernorm layer [1], residual connection, and feed-forward
layer. innovatively, it differs from the vanilla msa by sharing a proportion of
globally-shared self-attention (sa) heads amongst all the parallel transformer
blocks in decoders, while keeping the remaining sa heads unshared i. e. learn
the weights separately. a schematic illustration of the shared msa module in the
swin transformer block is demonstrated in fig. 3, as is formally formulated as
follows:[•] writes for the concatenation, sa(•) denotes the self-attention head
whose output dimension is d h , and u u msa ∈ r (m+n)•d h ×d is a learnable
matrix. the superscript s and u refer to the globally-shared and unshared
weights across all decoders, respectively.token mlp bottleneck. to reduce the
complexity of the model, we leverage a token mlp bottleneck as a light-weight
alternative for the swin transformer bottleneck. specifically, this approach
involves shifting the latent features extracted by the encoder via two mlp
blocks across the width and height channels, respectively [16]. the objective of
this process is to attend to specific areas, which mimics the shifted window
attention mechanism in swin transformer [13]. the shifted features are then
projected by a learnable mlp and normalized through a layernorm [1] before being
fed to a reprojection mlp layer.consistency self distillation. to alleviate the
inconsistency between the contour generated from the nuclei segmentation
prediction and the predicted edge, we propose a novel consistency self
distillation loss, denoted as l sd . formally, this regularization is defined as
the dice loss between the contour generated from the nuclei branch prediction (y
n ) using the sobel operation (sobel(y n )) and the predicted edges y e from the
normal edge decoder. specifically, the self distillation loss l d is formulated
by l sd = dice(sobel(y n ), y e ).multi-task learning objective. we employ a
multi-task learning paradigm to train the tri-decoder network, aiming to improve
model performance by leveraging the additional supervision signal from edges.
particularly, the nuclei semantic segmentation is considered the primary task,
while the normal edge and clustered edge semantic segmentation are viewed as
auxiliary tasks. all decoder branches follow a uniform scheme that combines the
cross-entropy loss and the dice loss, with the balancing coefficients set to
0.60 and 0.40 respectively, as previous work [6]. subsequently, the overall loss
l is calculated as a weighted summation of semantic nuclei mask loss (l n ),
normal edge loss (l e ), and clustered edge loss (l c ), and the self
distillation loss (l sd ) i. e., where coefficients γ n , γ e and γ c are set to
0.30, 0.35, 0.35 respectively, and γ sd is initially set to 1 with a 0.3
decrease for every 10 epochs until it reaches 0.4.
dataset. we evaluated the applicability of our approach across multiple
modalities by conducting evaluations on microscopy and histology datasets. the
private dataset contains 300 images sized at 512 × 512 tessellated from 50 wsis
scanned at 20×, and meticulously labeled by five pathologists according to the
labeling guidelines of the monuseg [10]. for both datasets, we randomly split
80% of the samples on the patient level as the training set and the remaining
20% as the test set. transnuseg demonstrates superior segmentation performance
compared to its counterparts, which can successfully distinguish severely
clustered nuclei from normal edges.implementations. all experiments are
performed on one nvidia rtx 3090 gpu with 24 gb memory. we use adam optimizer
with an initial learning rate of 1 × 10 -4 . we compare transnuseg with unet
[14], unet++ [21], tran-sunet [4], swinunet [2], and ca 2.5 -net [6]. we
evaluate the results by using dice score (dsc), intersection over union (iou),
pixel-level accuracy (acc), and f1-score(f1) as metrics, and ercnt [8]. to
ensure statistical significance, we run all methods five times with different
fixed seeds and report the results as mean ± standard deviation.results. table 1
shows the quantitative comparisons for the nuclei segmentation. the large margin
between the swinunet and the other cnn-based or hybrid networks also confirms
the superiority of the transformer in fine-grained nuclei segmentation. more
importantly, our method can outperform swinunet and the previous methods on both
datasets. for example, in the histology image dataset, transnuseg improves the
dice score, f1 score, accuracy, and iou by 2.08%, 3.41%, 1.25%, and 2.70%
respectively, over the second-best models. similarly, in the fluorescence
microscopy image dataset, our proposed model improves dsc by 0.96%, while also
leading to 1.65%, 1.03% and 1.91% increment in f1 score, accuracy, and iou to
the second-best performance. for better visualization, representative samples
and their segmentation results using different methods are demonstrated in fig.
4. furthermore, table 2 compares the model complexity in terms of the number of
parameters, floating point operations per second (flops), and the training
computational cost, where our approach can significantly reduce around 28% of
the training time compared to the state-ofthe-art cnn multi-task method ca 2.5
-net, while also boosting performance. ablation. our ablation study yields that
token mlp bottleneck and attention sharing schemes can complementarily reduce
the training cost while increasing efficiency, as shown in table 2 (the last 4
rows). to further show the effectiveness of these schemes, as well as
consistency self distillation, we conduct a comprehensive ablation study on both
datasets. as described in table 3, each component proportionally contributes to
the improvement to reach the overall performance boost. moreover, self
distillation can enhance the intrinsic consistency between two branches, as
visualized in fig. 5.
in this paper, we make the first attempt at an efficient but effective
multi-task transformer framework for modality-agnostic nuclei segmentation.
specifically, our tri-decoder framework transnuseg leverages an innovative self
distillation regularization to impose consistency between the different
branches. experimental results on two datasets demonstrate the excellence of our
transnuseg against state-of-the-art counterparts for potential real-world
clinical deployment. additionally, our work opens a new architecture to perform
nuclei segmentation tasks with swin transformer, where further investigations
can be performed to explore the generalizability to the top of our methods with
different modalities.
multi-modal learning has become a popular research area in computer vision and
medical image analysis, with modalities spanning across various media types,
including texts, audio, images, videos and multiple sensor data. this approach
has been utilised in robot control [15,17], visual question answering [12] and
audio-visual speech recognition [10], as well as in the medical field to improve
diagnostic system performance [7,18]. for instance, magnetic resonance imaging
(mri) is a common tool for brain tumour detection that relies on multiple
modalities (flair, t1, t1 contrast-enhanced known as t1c, and t2) rather than a
single type of mri images. however, most existing multi-modal methods require
complete modalities during training and testing, which limits their
applicability in real-world scenarios, where subsets of modalities may be
missing during training and testing.the missing modality issue is a significant
challenge in the multi-modal domain, and it has motivated the community to
develop approaches that attempt to address this problem. havaei et al. [8]
developed hemis, a model that handles missing modalities using statistical
features as embeddings for the model decoding process. taking one step ahead,
dorent et al. [6] proposed an extension to hemis via a multi-modal variational
auto-encoder (mvae) to make predictions based on learned statistical features.
in fact, variational auto-encoder (vae) has been adopted to generate data from
other modalities in the image or feature domains [3,11]. yin et al. [20] aimed
to learn a unified subspace for incomplete and unlabelled multi-view data. chen
et al. [4] proposed a feature disentanglement and gated fusion framework to
separate modality-robust and modalitysensitive features. ding et al. [5]
proposed an rfm module to fuse the modal features based on the sensitivity of
each modality to different tumor regions and a segmentation-based regularizer to
address the imbalanced training problem. zhang et al. [22] proposed an ma module
to ensure that modality-specific models are interconnected and calibrated with
attention weights for adaptive information exchange. recently, zhang et al. [21]
introduced a vision transformer architecture, mmformer, that fuses features from
all modalities into a set of comprehensive features. there are several existing
works [9,16,19] proposed to approximate the features from full modalities when
one or more modalities are absent. but none work performs cross-modal knowledge
distillation. from an other point of view, wang et al. [19] introduced a
dedicated training strategy that separately trains a series of models
specifically for each missing situation, which requires significantly more
computation resources compared with a nondedicated training strategy. an
interesting fact about multi-modal problems is that there is always one modality
that contributes much more than other modalities for a certain task. for
instance, for brain tumour segmentation, it is known from domain knowledge that
t1c scans clearly display the enhanced tumour, but not edema [4]. if the
knowledge of these modalities can be successfully preserved, the model can
produce promising results even when these best performing modalities are not
available. however, the aforementioned methods neglect the contribution biases
of different modalities and failed to consider keeping that knowledge.aiming at
this issue, we propose the non-dedicated training model1 learnable cross-modal
knowledge distillation (lckd) for tackling the missing modality issue. lckd is
able to handle missing modalities in both training and testing by automatically
identifying important modalities and distilling knowledge from them to learn the
parameters that are beneficial for all tasks while training for other modalities
(e.g., there are four modalities and three tasks for the three types of tumours
in brats2018). our main contributions are:-we propose the learnable cross-modal
knowledge distillation (lckd) model to address missing modality problem in
multi-modal learning. it is a simple yet effective model designed from the
viewpoint of distilling crossmodal knowledge to maximise the performance for all
tasks; -the lckd approach is designed to automatically identify the important
modalities per task, which helps the cross-modal knowledge distillation process.
it also can handle missing modality during both training and testing.the
experiments are conducted on the brain tumour segmentation benchmark brats2018
[1,14], showing that our lckd model achieves state-of-theart performance. in
comparison to recently proposed competing methods on brats2018, our model
demonstrates better performance in segmentation dice score by 3.61% for
enhancing tumour, 5.99% for tumour core, and 3.76% for whole tumour, on average.
let us represent the n -modality data with m l = {x∈ x denotes the l th data
sample and the superscript (i) indexes the modality. to simplify the notation,
we omit the subscript l when that information is clear from the context. the
label for each set m is represented by y ∈ y, where y represents the
ground-truth annotation space. the framework of lckd is shown in fig.
1.multi-modal segmentation is composed not only of multiple modalities, but also
of multiple tasks, such as the three types of tumours in brats2018 dataset that
represent the three tasks. take one of the tasks for example. our model
undergoes an external teacher election procedure prior to processing all
modalities {x (i) } n i=1 ∈ m in order to select the modalities that exhibit
promising performance as teachers. this is illustrated in fig. 1, where one of
the modalities, x (2) , is selected as a teacher, {x (1) , x (3) , ..., x (n ) }
are the students, and x (n) (with n = 2) is assumed to be absent. subsequently,
the modalities are encoded to output features {f (i) } n i=1 , individually. for
the modalities that are available, namely x (1) , ..., x (n-1) , x (n+1) , ...,
x (n ) , knowledge distillation is carried out between each pair of teacher and
student modalities. however, for the absent i=1 } are processed by the encoder
to produce the features {f (i) n i=1 }, which are concatenated and used by the
decoder to produce the segmentation. the teacher is elected using a validation
process that selects the top-performing modalities as teachers. cross-modal
distillation is performed by approximating the available students' features to
the available teachers' features. features from missing modalities are generated
by averaging the other modalities' features. modality x (n) , its features f (n)
are produced through a missing modality feature generation process from the
available features f (1) , ..., f (n-1) , f (n+1) , ..., f (n ) .in the next
sections, we explain each module of the proposed learnable crossmodal knowledge
distillation model training and testing with full and missing modalities.
usually, one of the modalities is more useful than others for a certain task,
e.g. for brain tumour segmentation, t1c scan clearly displays the enhanced
tumour, but it does not clearly show edema [4]. following knowledge distillation
(kd) [9], we propose to transfer the knowledge from modalities with promising
performance (known as teachers) to other modalities (known as students). the
teacher election procedure is further introduced to automatically elect proper
teachers for different tasks.more specifically, in the teacher election
procedure, a validation process is applied: for each task k (for k ∈ {1, ...,
k}), the modality with the best performance is selected as the teacher t (k) .
formally, we have:where i indexes different modalities, f (•; θ) is the lckd
segmentation model parameterised by θ, including the encoder and decoder
parameters {θ enc , θ dec } ∈ θ, and d(•, •) is the function to calculate the
dice score. based on the elected teachers for different tasks, a list of unique
teachers (i.e., repetitions are not allowed in the list, so for brats, {t1c,
t1c, flair} would be reduced to {t1c, flair}) are generated with: t = φ(t (1) ,
t (2) , ..., t (k) , ..., t (k) ), (2) where φ is the function that returns the
unique elements from a given list, and t ⊆ {1, ..., n } is the teacher set.
as shown in fig. 1, after each modality x (i) is inputted into the encoder
parameterised by θ enc , the features f (i) for each modality is fetched, as
in:(3)the cross-modal knowledge distillation (ckd) is defined by a loss function
that approximates all available modalities' features to the available teacher
modalities in a pairwise manner for all tasks, as follows:where • p presents the
p-norm operation, and here we expended the notation of missing modalities to
make it more general by assuming a set of modalities m is missing. the
minimisation of this loss pushes the model parameter values to a point in the
parameter space that can maximise the performance of all tasks for all
modalities.
because of the knowledge distillation between each pair of teachers and
students, the features of modalities in the feature space ought to be close to
the "genuine" features that can uniformly perform well for different tasks.
still assuming that modality set m is missing, the missing features f (n) can
thus be generated from the available features:where |m| denotes the number of
missing modalities.
all features encoded from eq. 3 or generated from eq. 5 are then concatenated to
be fed into the decoder parameterised by θ dec for predictingwhere ỹ ∈ y is the
prediction of the task.the training of the whole model is achieved by minimising
the following objective function: tot (d, θ) = task (d, θ enc , θ dec ) + α ckd
(d; θ enc ), (7) where task (d, θ enc , θ dec ) is the objective function for
the whole task (e.g., cross-entropy and dice losses are adopted for brain tumour
segmentation), and α is the trade-off factor between the task objective and
cross-modal kd objective.testing is based on taking all image modalities
available in the input to produce the features from eq. 3, and generating the
features from the missing modalities with eq. 5, which are then provided to the
decoder to predict the segmentation with eq. 6.
our model and competing methods are evaluated on the brats2018 segmentation
challenge dataset [1,14]. the task involves segmentation of three subregions of
brain tumours, namely enhancing tumour (et), tumour core (tc), and whole tumour
(wt). the dataset consists of 3d multi-modal brain mris, including flair, t1, t1
contrast-enhanced (t1c), and t2, with ground-truth annotations. the dataset
comprises 285 cases for training, and 66 cases for evaluation. the ground-truth
annotations for the training set are publicly available, while the validation
set annotations are hidden 2 .3d unet architecture (with 3d convolution and
normalisation) is adopted as our backbone network, where the ckd process occurs
at the bottom stage of the unet structure. to optimise our model, we adopt a
stochastic gradient descent optimiser with nesterov momentum [2] set to 0.99. l1
loss is adopted for ckd (.) in eq. 4. batch-size is set to 2. the learning rate
is initially set to 10 -2 and gradually decreased via the cosine annealing [13]
strategy. we trained the lckd model for 115,000 iterations and use 20% of the
training data as the validation task for teacher election. to simulate
modality-missing situations with non-dedicated training of models, we randomly
dropped 0 to 3 modalities for each iteration. our training time is 70.12 h and
testing time is 6.43 s per case on one nvidia 3090 gpu. 19795 mib gpu memory is
used for model training with batch-size 2 and 3789 mib gpu memory is consumed
for model testing with batch-size 1.
table 1 shows the overall performance on all 15 possible combinations of missing
modalities for three sub-regions of brain tumours. our models are compared with
several strong baseline models: u-hemis (abbreviated as hmis in the figure) [8],
u-hved (hved) [6], robust-mseg (rseg) [4] and mmformer (mmfm) [21]. we can
clearly observe that with t1c, the model performs considerably better than other
modalities for et. similarly, t1c for tc and flair for wt contribute the most,
which confirm our motivation.the lckd model significantly outperforms (as shown
by the one-tailed paired t-test for each task between models in the last row of
table 1) u-hemis, u-hved, robust-mseg and mmformer in terms of the segmentation
dice for enhancing tumour and whole tumour on all 15 combinations and the tumour
core on 14 out of 15. it is observed that, on average, the proposed lckd model
improves the state-of-the-art performance by 3.61% for enhancing tumour, 5.99%
for tumour core, and 3.76% for whole tumour in terms of the segmentation dice
score. especially in some combinations without the best modality, e.g. et/tc
without t1c and wt without flair, lckd has a 6.15% improvement with only flair
and 10.69% with only t1 over the second best model for et segmentation; 10.8%
and 10.03% improvement with only flair and t1 for tc; 8.96% and 5.01%
improvement with only t1 and t1c for wt, respectively. these results demonstrate
that useful knowledge of the best modality has been successfully distilled into
the model by lckd for multimodal learning with missing modalities.
single teacher vs. multi-teacher. to analyse the effectiveness of knowledge
distillation from multiple teachers of all tasks in the proposed lckd model, we
perform a study to compare the model performance of adopting single teacher and
multi-teachers for knowledge distillation. we enable multi-teachers for lckd by
default to encourage the model parameters to move to a point that can perform
well for all tasks. however, for single teacher, we modify the 2, compared with
multi-teacher model lckd-m, we found that the single teacher model lckd-s
receives comparable results for et and tc segmentation (it even has better
average performance on et), but it cannot outperform lckd-m on wt. this
phenomenon, also shown in fig. 2, demonstrates that lckd-m has better overall
segmentation performance. this resonates with our expectations because there are
3 tasks in brats, and the best teachers for et and tc are the same, which is
t1c, but for wt, flair is the best one. therefore, for lckd-s, the knowledge of
the best teacher for et and tc can be distilled into the model, but not for wt.
the lckd-m model can overcome this issue since it attempts to find a point in
the parameter space that is beneficial for all tasks. empirically, we observed
that both models found the correct teacher(s) quickly: the best teacher of the
single teacher model alternated between t1c and flair for a few validation
rounds and stabilised at t1c; while the multi-teacher model found the best
teachers (t1c and flair) from the first validation round. role of α and ckd loss
function. as shown in fig. 3, we set α in eq. 7 to {0, 0.1, 0.5, 1} using t1
input only and l1 loss for ckd (.) in (4). if α = 0, the model performance drops
greatly, but when α > 0, results improve, where α = 0.1 produces the best
result. this shows the importance of the cross-modal knowledge distillation loss
in (7). to study the effect of a different ckd loss, we show dice score with l2
loss for ckd (.) in ( 4), with α = 0.1. compared with the l1 loss, we note that
dice decreases slightly with the l2 loss, especially for tc and wt.
in this paper we introduced the learnable cross-modal knowledge distillation
(lckd), which is the first method that can handle missing modality during
training and testing by distilling knowledge from automatically selected
important modalities for all training tasks to train other modalities.
experiments on brats2018 [1,14] show that lckd reaches state-of-the-art
performance in missing modality segmentation problems. we believe that our
proposed lckd has the potential to allow the use of multimodal data for training
and missingmodality data per testing. one point to improve about lckd is the
greedy teacher selection per task. we plan to improve this point by transforming
this problem into a meta-learning strategy, where the meta parameter is the
weight for each modality, which will be optimised per task.
gliomas are the most commonly seen central nervous system malignancies with
aggressive growth and low survival rates [19]. accurate multi-class segmentation
of gliomas in multimodal magnetic resonance imaging (mri) plays an indispensable
role in quantitative analysis, treatment planning, and monitoring of progression
and treatment. although deep learning-based methods have achieved
state-of-the-art performance in automated brain tumor segmentation [6][7][8]14],
their performance often drops when tasked with segmenting out-of-distribution
samples and poor-quality artifactual images. however, segmentations of desired
quality are required to reliably drive treatment decisions and facilitate
clinical management of gliomas. therefore, tools for automated quality control
(qc) are essential for the clinical translation of automated segmentation
methods. such tools can enable a streamlined clinical workflow by identifying
catastrophic segmentation failures, informing clinical experts where the
segmentations need to be refined, and providing a quantitative measure of
quality that can be taken into account in downstream analyses.most previous
studies of segmentation qc only provide subject-level quality assessment by
either directly predicting segmentation-quality metrics or their surrogates.
specifically, wang et al. [18] leveraged a variational autoencoder to learn the
latent representation of good-quality image-segmentation pairs in the context of
cardiac mri segmentation. during the inference, an iterative search scheme was
performed in the latent space to find a surrogate segmentation. this
segmentation is assumed to be a good proxy of the (unknown) ground-truth
segmentation of the query image, and can thus be compared to the at-hand
predicted segmentation to estimate its quality. another approach that takes
advantage of the pairs of images and ground-truth segmentation is the reverse
classification accuracy (rca) framework [13,17]. in this framework, the test
image is registered to a preselected reference dataset with known ground-truth
segmentation. the quality of a query segmentation is assessed by warping the
query image to the reference dataset. however, these methods primarily targeted
qc of cardiac mri segmentation, which involves a single imaging modality and a
single tissue type with a welch-characterized location and appearance. in
contrast, brain tumor segmentation involves the delineation of heterogeneous
tumor regions, which are manifested through intensity changes relative to the
surrounding healthy tissue across multiple modalities. importantly, there is
significant variability in brain tumor appearances, including multifocal masses
and complex shapes with heterogeneous textures. consequently, adapting
approaches for automated qc of cardiac segmentation to brain tumor segmentation
is challenging. additionally, iterative search or registration during inference
makes the existing methods computationally expensive and time-consuming, which
limits their applicability in large-scale segmentation qc.multiple studies have
also explored regression-based methods to directly predict segmentation-quality
metrics, e.g., dice similarity coefficient (dsc). for example, kohlberger et al.
[10] used support vector machine (svm) with handcrafted features to detect
cardiac mri segmentation failures. robinson et al. [12] proposed a convolutional
neural network (cnn) to automatically extract features from segmentations
generated by a series of random forest segmenters to predict dsc for cardiac mri
segmentation. kofler et al. [9] proposed a cnn to predict holistic ratings of
segmentations, which were annotated by neuroradiologists, with the goal of
better emulating how human experts. though these regression-based methods are
advantageous for fast inference, they do not provide voxel-level localization of
segmentation failures, which can be crucial for both auditing purposes and
guiding manual refinements.in summary, while numerous efforts have been devoted
to segmentation qc, most works were in the context of cardiac mri segmentation
with few works tackling segmentation qc of brain tumors, which have more complex
and heterogeneous appearances than the heart. furthermore, most of the existing
methods do not localize segmentation errors, which is meaningful for both
auditing purposes and guiding manual refinement. to address these challenges, we
propose a novel framework for joint subject-level and voxel-level prediction of
segmentation quality from multimodal mri. the contribution of this work is
four-fold. first, we proposed a predictive model (qcresunet) that simultaneously
predicts dsc and localizes segmentation errors at the voxel level. second, we
devised a datageneration approach, called seggen, that generates a wide range of
segmentations of varying quality, ensuring unbiased model training and testing.
third, our end-to-end predictive model yields fast inference. fourth, the
proposed method achieved a good performance in predicting subject-level
segmentation quality and identifying voxel-level segmentation failures.
given four imaging modalities denoted as [x 1 , x 2 , x 3 , x 4 ] and a
predicted multiclass brain tumor segmentation mask (s pred ), the goal of our
approach is to automatically assess the tumor segmentation quality by
simultaneously predicting dsc and identifying segmentation errors as a binary
mask (s err ). toward this end, we proposed a 3d encoder-decoder architecture
termed qcresunet (see fig. 1(a)) for simultaneously predicting dsc and
localizing segmentation errors. qcresunet has two parts trained in an end-to-end
fashion: i) a resnet-34 [4] encoder for dsc prediction; and ii) a decoder
architecture for segmentation error map prediction (i.e., the difference between
predicted segmentation and ground-truth segmentation).the resnet-34 encoder
enables the extraction of semantically rich features that are useful for
characterizing the quality of the segmentation. we maintained the main structure
of the vanilla 2d resnet-34 [4] but made the following modifications, which were
necessary to account for the 3d nature of the input data (see fig. 1(b)). first,
all the 2d convolutional layers and pooling layers in the vanilla resnet were
changed to 3d. second, the batch normalization [5] was replaced by instance
normalization [16] to accommodate the small batch size in 3d model training.
third, spatial dropout [15] with a probability of 0.3 was added to each residual
block to prevent overfitting.the building block of the decoder consisted of an
upsampling by a factor of two, which was implemented by a nearest neighbor
interpolation in the feature map, followed by two convolutional blocks that
halve the number of feature maps. each convolutional block comprised a 3 × 3 × 3
convolutional layer followed by an instance normalization layer and a leaky relu
activation [11] (see fig. 1(c)). the output of each decoder block was
concatenated with features from the corresponding encoder level to facilitate
information flow from the encoder to the decoder. compared to the encoder, we
used a shallower decoder with fewer parameters to prevent overfitting and reduce
computational complexity.the objective function for training qcresunet consists
of two parts. the first part corresponds to the dsc regression task. it consists
of a mean absolute error (mae) loss (l mae ) term that penalizes differences
between ground truth (dsc gt ) and predicted dsc (dsc pred ):where n denotes the
number of samples in a batch. the second part of the objective function
corresponds to the segmentation error prediction. it consists of a dice loss [3]
and a binary cross-entropy loss, given by:where s errgt , s err pred denote the
binary ground-truth segmentation error map and the predicted error segmentation
map from the sigmoid output of the decoder, respectively. the dice loss and
cross-entropy loss were averaged across the number of pixels i in a batch. the
two parts are combined using a weight parameter λ to balance the different loss
components:3 experimentsfor this study, pre-operative multimodal mri scans of
varying grades of glioma were obtained from the 2021 brain tumor segmentation
(brats) challenge [1] training dataset (n = 1251). for each subject, four
modalities viz. pre-contrast t1-weighted (t1), t2-weighted (t2), post-contrast
t1-weighted (t1c), and fluid attenuated inversion recovery (flair) are included
in the dataset. it also included expert-annotated multi-class tumor segmentation
masks comprising enhancing tumor (et), necrotic tumor core (ncr), and edema (ed)
classes. all data were already registered to a standard anatomical atlas and
skull-stripped. the skull-stripped scans were then z-scored to zero mean and
unit variance. all the data was first cropped to non-zero value regions, and
then zero-padded to a size of 160 × 192 × 160 to be fed into the network.
the initial dataset was expanded by producing segmentation results at different
levels of quality to provide an unbiased estimation of segmentation quality. to
this end, we adopted a three-step approach. first, a nnunet framework [6] was
adopted and trained five times separately using different modalities as input
(i.e., t1-only, t1c-only, t2-only, flair-only, and all four modalities). as only
certain tissue-types are captured in each modality (e.g., enhancing tumor is
captured well in t1c but not in flair), this allowed us to generate
segmentations of a wide range of qualities. nnunet was selected for this purpose
due to its wide success in brain tumor segmentation tasks. second, to further
enrich our dataset with segmentations of diverse quality, we sampled
segmentations along the training routines at different iterations. a small
learning rate (1 × 10 -6 ) was chosen in training all the models to slower their
convergence in order to sample segmentations gradually sweeping from poor
quality to high quality. third, we devised a method called seggen that applied
image transformations, including random rotation (angle = [-15 • , 15 • ]),
random scaling (scale = [0.85, 1.25]), random translation (moves = [-20, 20]),
and random elastic deformation (displacement = [0, 20]), to the ground-truth
segmentations with a probability of 0.5, resulting in three segmentations for
each subject.the original brats 2021 training dataset was split into training (n
= 800), validation (n = 200), and testing (n = 251) sets. after applying the
three-step approach, it resulted in 48000, 12000, and 15060 samples for the
three sets, respectively. however, this generated dataset suffered from
imbalance (fig. 2(a), (b), and(c)) because the cnn models could segment most of
the cases correctly. training using such an imbalanced dataset is prone to
producing biased models that do not generalize well. to mitigate this issue, we
proposed a resampling strategy during the training to make the dsc more
uniformly distributed. specifically, we used the quantile transform to map the
distribution of a variable to a target distribution by randomly smoothing out
the samples unrelated to the target distribution. using the quantile transform,
the data generator first transformed the distribution of the generated dsc to a
uniform distribution. next, the generated samples closest to the transformed
uniform distribution in terms of euclidean distance were chosen to form the
resampled dataset. after applying our proposed resampling strategy, the dsc in
the training and validation set approached a uniform distribution (fig. 2(a),
(b), and (c)). the total number of samples before and after resampling remained
the same with repeating samples. we kept the resampling stochastic at each
iteration during training to make all the generated samples seen by the model.
the generated testing set was also resampled to perform an unbiased estimation
of the quality at different levels resulting in 4895 samples.in addition to the
segmentations generated by the nnunet framework and the seggen method, we also
generated out-of-distribution segmentation samples for the testing set to
validate the generalizability of our proposed model. for this purpose, five
models were trained on the training set using the deepmedic framework [8] with
different input modalities (i.e., t1-only, t1c-only, t2-only, flair-only, and
all four modalities). this resulted in 251 × 5 = 1255 out-ofdistribution samples
in the testing set.
baseline methods: in this study, we compared the performance of the proposed
model to three baseline models: (i) a unet model [14], (ii) a resnet-34 [4], and
(iii) the renet-50 model used by robinson et al. [12]. for a fair comparison,
the residual blocks in the resnet-34 and resnet-50 were the same as that in the
qcresunet. we added an average pooling followed by a fully-connected layer to
the last feature map of the unet to predict a single dsc value. the evaluation
was conducted on in-sample (nnunet and seggen) and out-of-sample segmentations
generated by deepmedic.table 1. the qc performance of three baseline methods and
the proposed method was evaluated on in-sample (nnunet and seggen) and
out-of-sample (deepmedic) segmentations. the best metrics in each column are
highlighted in bold. dscerr denotes the median dsc between serr gt and serr pred
across all samples. training procedure: all models were trained for 150 epochs
using an adam optimizer with a l 2 weight decay of 5 × 10 -4 . the batch size
was set to 4. data augmentation, including random rotation, random scaling,
random mirroring, random gaussian noise, and gamma intensity correction, was
applied to prevent overfitting during training. we performed a random search [2]
to determine the optimal hyperparameters (i.e., initial learning rate and loss
weight balance parameter λ) on the training and validation set. the
hyperparameters that yielded the best results were λ = 1 and an initial learning
rate of 1 × 10 -4 . the learning rate was exponentially decayed by a factor of
0.9 at each epoch until 1 × 10 -6 . model training was performed on four nvidia
tesla a100 and v100s gpus. the proposed method was implemented in pytorch
v1.12.1.
we assessed the performance of the subject-level segmentation quality prediction
in terms of pearson coefficient r and mae between the predicted dsc and the
ground-truth dsc. the performance of the segmentation error localization was
assessed by the dsc err between the predicted segmenta- tion error map and the
ground-truth segmentation error map. p-values were computed using a paired
t-test between dsc predicted by qcresunet versus ones predicted by corresponding
baselines.
the proposed qcresunet achieved good performance in predicting subjectlevel
segmentation quality for in-sample (mae = 0.0570, r = 0.964) and out-ofsample
(mae = 0.0606, r = 0.966) segmentations. the proposed method also showed
statistically significant improvement against all three baselines (table 1 and
fig. 3). we found that the dsc prediction error (mae) of the proposed method was
distributed more evenly across different levels of quality than all baselines
(see fig. 3) with a smaller standard deviation of 0.050 for in-sample
segmentations and 0.049 for out-of-sample segmentations. a possible explanation
is that the joint training of predicting subject-level and voxel-level quality
enabled the qcresunet to learn deep features that better characterize the
segmentation quality. for the voxel-level segmentation error localization task,
the model achieved a median dsc of 0.834 for in-sample segmentations and 0.867
for out-of-sample segmentations. this error localization is not provided by any
of the baselines and enables qcresunet to track segmentation failures at
different levels of segmentation quality (fig. 4).
in this work, we proposed a novel cnn architecture called qcresunet to perform
automatic brain tumor segmentation qc in multimodal mri scans. qcre-sunet
simultaneously provides subject-level segmentation-quality prediction and
localizes segmentation failures at the voxel level. it achieved superior dsc
prediction performance compared to all baselines. in addition, the ability to
localize segmentation errors has the potential to guide the refinement of
predicted segmentations in a clinical setting. this can significantly expedite
clinical workflows, thus improving the overall clinical management of gliomas.
malignant melanoma is one of the most rapidly growing cancers in the world. as
estimated by the american cancer society, there were approximately 100,350 new
cases and over 6,500 deaths in 2020 [14]. thus, an automated skin lesion
segmentation system is imperative, as it can assist medical professionals in
swiftly identifying lesion areas and facilitating subsequent treatment
processes. to enhance the segmentation performance, recent studies tend to
employ modules with larger parameter and computational complexity, such as
incorporating self-attention mechanisms of vision transformer (vit) [7]. for
example, swin-unet [4], based on the swin transformer [11], leverages the
feature extraction ability of self-attention mechanisms to improve segmentation
performance. b) respectively show the visualization of comparative experimental
results on the isic2017 and isic2018 datasets. the x-axis represents the number
of parameters (lower is better), while y-axis represents miou (higher is
better). the color depth represents computational complexity (gflops, lighter is
better). (color figure online)transunet [5] has pioneered a serial fusion of cnn
and vit for medical image segmentation. transfuse [26] employs a dual-path
structure, utilizing cnn and vit to capture local and global information,
respectively. utnetv2 [8] utilizes a hybrid hierarchical architecture, efficient
bidirectional attention, and semantic maps to achieve global multi-scale feature
fusion, combining the strengths of cnn and vit. transbts [23] introduces
self-attention into brain tumor segmentation tasks and uses it to aggregate
high-level information.prior works have enhanced performance by introducing
intricate modules, but neglected the constraint of computational resources in
real medical settings. hence, there is an urgent need to design a low-parameter
and low-computational load model for segmentation tasks in mobile healthcare.
recently, unext [22] has combined unet [18] and mlp [21] to develop a
lightweight model that attains superior performance, while diminishing parameter
and computation. furthermore, malunet [19] has reduced the model size by
declining the number of model channels and introducing multiple attention
modules, resulting in better performance for skin lesion segmentation than
unext. however, while malunet greatly reduces the number of parameter and
computation, its segmentation performance is still lower than some large models,
such as trans-fuse. therefore, in this study, we propose ege-unet, a lightweight
skin lesion segmentation model that achieves state-of-the-art while
significantly reducing parameter and computation costs. additionally, to our
best knowledge, this is the first work to reduce parameter to approximately
50kb.to be specific, ege-unet leverages two key modules: the group multi-axis
hadamard product attention module (ghpa) and group aggregation bridge module
(gab). on the one hand, recent models based on vit [7] have shown promise, owing
to the multi-head self-attention mechanism (mhsa). mhsa divides the input into
multiple heads and calculates self-attention in each head, which allows the
model to obtain information from diverse perspectives, integrate different
knowledge, and improve performance. nonetheless, the quadratic complexity of
mhsa enormously increases the model's size. therefore, we present the hadamard
product attention mechanism (hpa) with linear complexity. hpa employs a
learnable weight and performs a hadamard product operation with the input to
obtain the output. subsequently, inspired by the multi-head mode in mhsa, we
propose ghpa, which divides the input into different groups and performs hpa in
each group. however, it is worth noting that we perform hpa on different axes in
different groups, which helps to further obtain information from diverse
perspectives. on the other hand, for gab, since the size and shape of
segmentation targets in medical images are inconsistent, it is essential to
obtain multi-scale information [19]. therefore, gab integrates high-level and
low-level features with different sizes based on group aggregation, and
additionally introduce mask information to assist feature fusion. via combining
the above two modules with unet, we propose ege-unet, which achieves excellent
segmentation performance with extremely low parameter and computation. unlike
previous approaches that focus solely on improving performance, our model also
prioritizes usability in real-world environments. a clear comparison of ege-unet
with others is shown in fig. 1.in summary, our contributions are threefold: (1)
ghpa and gab are proposed, with the former efficiently acquiring and integrating
multi-perspective information and the latter accepting features at different
scales, along with an auxiliary mask for efficient multi-scale feature fusion.
(2) we propose ege-unet, an extremely lightweight model designed for skin lesion
segmentation.(3) we conduct extensive experiments, which demonstrate the
effectiveness of our methods in achieving state-of-the-art performance with
significantly lower resource requirements.
the overall architecture. ege-unet is illustrated in fig. 2, which is built upon
the u-shape architecture consisting of symmetric encoder-decoder parts. we take
encoder part as an example. the encoder is composed of six stages, each with
channel numbers of {8, 16, 24, 32, 48, 64}. while the first three stages employ
plain convolutions with a kernel size of 3, the last three stages utilize the
proposed ghpa to extract representation information from diverse perspectives.
in contrast to the simple skip connections in unet, ege-unet incorporates gab
for each stage between the encoder and decoder. furthermore, our model leverages
deep supervision [27] to generate mask predictions of varying scales, which are
utilized for loss function and serve as one of the inputs to gab. via the
integration of these advanced modules, ege-unet significantly reduces the
parameter and computational load while enhancing the segmentation performance
compared to prior approaches. group multi-axis hadamard product attention
module. to overcome the quadratic complexity issue posed by mhsa, we propose hpa
with linear complexity. given an input x and a randomly initialized learnable
tensor p, bilinear interpolation is first utilized to resize p to match the size
of x. then, we employ depth-wise separable convolution (dw) [10,20] on p,
followed by a hadamard product operation between x and p to obtain the output.
however, utilizing simple hpa alone is insufficient to extract information from
multiple perspectives, resulting in unsatisfactory results. motivated by the
multi-head mode in mhsa, we introduce ghpa based on hpa, as illustrated in
algorithm 1. we divide the input into four groups equally along the channel
dimension and perform hpa on the height-width, channel-height, and channel-width
axes for the first three groups, respectively. for the last group, we only use
dw on the feature map. finally, we concatenate the four groups along the channel
dimension and apply another dw to integrate the information from different
perspectives. note that all kernel size employed in dw are 3.group aggregation
bridge module. the acquisition of multi-scale information is deemed pivotal for
dense prediction tasks, such as medical image segmentation. hence, as shown in
fig. 3, we introduce gab, which takes three inputs: low-level features,
high-level features, and a mask. firstly, depthwise separable convolution (dw)
and bilinear interpolation are employed to adjust the size of high-level
features, so as to match the size of low-level features. secondly, we partition
both feature maps into four groups along the channel dimension, and concatenate
one group from the low-level features with one from the high-level features to
obtain four groups of fused features. for each group of fused features, the mask
is concatenated. next, dilated convolutions [25] with kernel size of 3 and
different dilated rates of {1, 2, 5, 7} are applied to the different groups, in
order to extract information at different scales. finally, the four groups are
concatenated along the channel dimension, followed by the application of a plain
convolution with the kernel size of 1 to enable interaction among features at
different scales.loss function. in this study, since different gab require
different scales of mask information, deep supervision [27] is employed to
calculate the loss function for different stages, in order to generate more
accurate mask information. our loss function can be expressed as eqs. (1) and
(2). l i = bce(y, ŷ) + dice(y, ŷ)(1)where bce and dice represent binary cross
entropy and dice loss. λ i is the weight for different stage. in this paper, we
set λ i to 1, 0.5, 0.4, 0.3, 0.2, 0.1 from i = 0 to i = 5 by default.
datasets and implementation details. to assess the efficacy of our model, we
select two public skin lesion segmentation datasets, namely isic2017 [1,3] and
isic2018 [2,6], containing 2150 and 2694 dermoscopy images, respectively.
consistent with prior research [19], we randomly partition the datasets into
training and testing sets at a 7:3 ratio. ege-unet is developed by pytorch [17]
framework. all experiments are performed on a single nvidia rtx a6000 gpu. the
images are normalized and resized to 256 × 256. we apply various data
augmentation, including horizontal flipping, vertical flipping, and random
rotation. adamw [13] is utilized as the optimizer, initialized with a learning
rate of 0.001 and the cosineannealinglr [12] is employed as the scheduler with a
maximum number of iterations of 50 and a minimum learning rate of 1e-5. a total
of 300 epochs are trained with a batch size of 8. to evaluate our method, we
employ mean intersection over union (miou), dice similarity score (dsc) as
metrics, and we conduct 5 times and report the mean and standard deviation of
the results for each dataset.
table 1 reveal that our ege-unet exhibits a comprehensive state-of-the-art
performance on the isic2017 dataset. specifically, in contrast to larger models,
such as transfuse, our model not only demonstrates superior performance, but
also significantly curtails the number of parameter and computation by 494x and
160x, respectively. in comparison to other lightweight models, ege-unet
surpasses unext-s with a miou improvement of 1.55% and a dsc improvement of
0.97%, while exhibiting parameter and computation reductions of 17% and 72% of
unext-s. furthermore, ege-unet outperforms malunet with a miou improvement of
1.03% and a dsc improvement of 0.64%, while reducing parameter and computation
to 30% and 85% of malunet. for the isic2018 dataset, the performance of our
model also outperforms that of the best-performing model. besides, it is
noteworthy that ege-unet is the first lightweight model reducing parameter to
about 50kb with excellent segmentation performance.figure 1 presents a more
clear visualization of the experimental findings and fig. 4 shows some
segmentation results.ablation results. we conduct extensive ablation experiments
to demonstrate the effectiveness of our proposed modules. the baseline utilized
in our work is referenced from malunet [19], which employs a six-stage u-shaped
architecture with symmetric encoder and decoder components. each stage includes
a plain convolution operation with a kernel size of 3, and the number of
channels at each stage is set to {8, 16, 24, 32, 48, 64}. in table 2(a), we
conduct macro ablations on ghpa and gab. firstly, we replace the plain
convolutions in the last three layers of baseline with ghpa. due to the
efficient multi-perspective feature acquisition of ghpa, it not only outperforms
the baseline, but also greatly reduces the parameter and computation. secondly,
we substitute the skip-connection operation in baseline with gab, resulting in
further improved performance. table 2(b) presents the ablations for ghpa. we
replace the multiaxis grouping with single-branch and initialize the learnable
tensors with only random values. it is evident that the removal of these two key
designs leads to a marked drop. table 2(c) illustrates the ablations for gab.
initially, we omit the mask information, and miou metric even drops below 79%,
thereby confirming once again the critical role of mask information in guiding
feature fusion. furthermore, we substitute the dilated convolutions in gab with
plain convolutions, which also leads to a reduction in performance.
in this paper, we propose two advanced modules. our ghpa uses a novel hpa
mechanism to simplify the quadratic complexity of the self-attention to linear
complexity. it also leverages grouping to fully capture information from
different perspectives. our gab fuses low-level and high-level features and
introduces a mask to integrate multi-scale information. based on these modules,
we propose ege-unet for skin lesion segmentation tasks. experimental results
demonstrate the effectiveness of our approach in achieving state-of-the-art
performance with significantly lower resource requirements. we hope that our
work can inspire further research on lightweight models for the medical image
community.regarding limitations and future works, on the one hand, we mainly
focus on how to greatly reduce the parameter and computation complexity while
improving performance in this paper. thus, we plan to deploy ege-unet in a
real-world environment in the future work. on the other hand, ege-unet is
currently designed only for the skin lesion segmentation task. therefore, we
will extend our lightweight design to other tasks.
dynamic contrast-enhanced magnetic resonance imaging (dce-mri) revealing tumor
hemodynamics information is often applied to early diagnosis and treatment of
breast cancer [1]. in particular, automatically and accurately segmenting tumor
regions in dce-mri is vital for computer-aided diagnosis (cad) and various
clinical tasks such as surgical planning. for the sake of promoting segmentation
performance, recent methods utilize the dynamic mr sequence and exploit its
temporal correlations to acquire powerful representations [2][3][4]. more
recently, a handful of approaches take advantage of hemodynamic knowledge and
time intensity curve (tic) to improve segmentation accuracy [5,6]. however, the
aforementioned methods require the complete dce-mri sequences and overlook the
difficulty in assessing complete temporal sequences and the missing time point
problem, especially post-contrast phase, due to the privacy protection and
patient conditions. hence, these breast cancer segmentation models cannot be
deployed directly in clinical practice.recently, denoising diffusion
probabilistic model (ddpm) [7,8] has produced a tremendous impact on image
generation field due to its impressive performance. diffusion model is composed
of a forward diffusion process that add noise to images, along with a reverse
generation process that generates realistic images from the noisy input [8].
based on this, several methods investigate the potential of ddpm for natural
image segmentation [9] and medical image segmentation [10][11][12].
specifically, baranchuk et al. [9] explores the intermediate activations from
the networks that perform the markov step of the reverse diffusion process and
find these activations can capture semantic information for segmentation.
however, the applicability of ddpm to medical image segmentation are still
limited. in addition, existing ddpm-based segmentation networks are generic and
are not optimized for specific applications. in particular, a core question for
dce-mri segmentation is how to optimally exploit hemodynamic priors.based on the
above observations, we innovatively consider the underlying relation between
hemodynamic response function (hrf) and denoising diffusion process (ddp). as
shown in fig. 1, during hrf process, only tumor lesions are enhanced and other
non-tumor regions remain unchanged. by designing a network architecture to
effectively transmute pre-contrast images into post-contrast images, the network
should acquire hemodynamic inherent in hrf that can be used to improve
segmentation performance. inspired by the fact that ddpm generates images from
noisy input provided by the parameterized gaussian process, this work aims to
exploit implicit hemodynamic information by a diffusion process that predict
post-contrast images from noisy pre-contrast images. specifically, given the
pre-contrast and post-contrast images, the latent kinetic code is learned using
a score function of ddpm, which contains sufficient hemodynamic characteristics
to facilitate segmentation performance.once the diffusion module is pretrained,
the latent kinetic code can be easily generated with only pre-contrast images,
which is fed into a segmentation module to annotate cancers. to verify the
effectiveness of the latent kinetic code, the sm adopts a simple u-net-like
structure, with an encoder to simultaneously conduct semantic feature encoding
and kinetic code fusion, along with a decoder to obtain voxel-level
classification. in this manner, our latent kinetic code can be interpreted to
provide tic information and hemodynamic characteristics for accurate cancer
segmentation.we verify the effectiveness of our proposed diffusion kinetic model
(dkm) on dce-mri-based breast cancer segmentation using breast-mri-nact-pilot
dataset [13]. compared to the existing state-of-the-art approaches with complete
sequences, our method yields higher segmentation performance even with
precontrast images. in summary, the main contributions of this work are listed
as follows:• we propose a diffusion kinetic model that implicitly exploits
hemodynamic priors in dce-mri and effectively generates high-quality
segmentation maps only requiring pre-contrast images. • we first consider the
underlying relation between hemodynamic response function and denoising
diffusion process and provide a ddpm-based solution to capture a latent kinetic
code for hemodynamic knowledge. • compared to the existing approaches with
complete sequences, the proposed method yields higher cancer segmentation
performance even with pre-contrast images.
the overall framework of the proposed diffusion kinetic model is illustrated in
fig. 2. it can be observed that the devised model consists of a diffusion module
(dm) and a segmentation module (sm). let {x k , k = 0, 1, ..., k} be a sequence
of images representing the dce-mri protocol, in which x 0 represents the
precontrast image and x k represents the late post-contrast image. the dm takes
a noisy pre-contrast image x t as input and generates post-contrast images to
estimate the latent kinetic code. once the dm is trained, the learned kinetic
code is incorporated into the sm as hemodynamic priors to guide the segmentation
process. model details are shown as follows.
the diffusion module is following the denoising diffusion probabilistic model
[8,14]. based on the consideration from nonequilibrium thermodynamics, ddpm
approximates the data distribution by learning a markov chain process which
originates from the gaussian distribution. the forward diffusion process
gradually adds gaussian noise to the data x 0 according to a variance schedule β
1 , ..., β t [8]:particularly, a noisy image x t can be directly obtained from
the data x 0 :where α t := 1β t and ᾱt := t s=1 α s . afterwards, ddpm
approximates the reverse diffusion process by the following parameterized
gaussian transitions:where μ θ (x t , t) is the learned posterior mean and θ (x
t ; t) is a fixed set of scalar covariances. in particular, we employ a noise
predictor network ( θ (x t , t)) to predict the noise component at the step t
(as shown in fig. 2(a)).inspired by the property of ddpm [8], we devise the
diffusion module by considering the pre-contrast images x 0 as source and
regarding the post-contrast images x k as target. formally, a noisy sample can
be acquired by:where α t := 1-β t and ᾱt := t s=1 α s . next, we employ the
reverse diffusion process to transform the noisy sample x t to the post-contrast
data x k . as thus, the dm gradually exploits the latent kinetic code by
comparing the pre-contrast and post-contrast images, which contains hemodynamic
knowledge for segmentation.
once pretrained, the dm outputs multi-scale latent kinetic code f dm from
intermediate layers, which is fed into the sm to guide cancer segmentation. as
shown in fig. 2(b), the sm consists of four kinetic blocks and four up blocks.
each kinetic block is composed of a fusion layer, two convolutional layers, two
batch normalization layers, two relu activation functions, a max pooling layer
and a residual addition. specifically, to obtain sufficient expressive power to
transform the learned kinetic code into higher-level features, at least one
learnable linear transformation is required. to this end, a linear
transformation, parametrized by a weight matrix w , is applied to the latent
code f dm , followed by a batch normalization, relu activation layer and
concatenation, which can be represented as follows:where * represents 1 × 1
based convolution operation, w is the weight matrix, bn represents batch
normalization, φ represents relu activation function and c is concatenation
operation. in this way, the hemodynamic knowledge can be incorporated into the
sm to capture more expressive representations to improve segmentation
performance.
to maintain training stability, the proposed dkm adopts a two-step training
procedure for cancer annotation. in the first step, the dm is trained to
transform pre-contrast images into post-contrast images for a latent space where
hemodynamic priors are exploited. in particular, the diffusion loss for the
reverse diffusion process can be formulated as follows:where θ represents the
denoising model that employs an u-net structure, x 0 and x k are the
pre-contrast and post-contrast images, respectively, is gaussian distribution
data ∼ n (0, i), and t is a timestep. for a second step, we train the sm that
integrates the previously learned latent kinetic code to provide tumor
hemodynamic information for voxel-level prediction. considering the varying
sizes, shapes and appearances of tumors that results from intratumor
heterogeneity and results in difficulties of accurate cancer annotation, we
design the segmentation loss as follows:where l ssim is used to evaluate tumor
structural characteristics, s and g represents segmentation map and ground
truth, respectively; μ s is the mean of s and μ g is the mean of g; ϕ s
represents the variance of s and ϕ g represents the variance of g; c 1 and c 2
denote the constant to hold training stable [15], and ϕ sg is the covariance
between s and g. the λ is set as 0.5 empirically. following [16],where k 1 is
set as 0.01, k 2 is set as 0.03 and l is set as the range of voxel values.
dataset: to demonstrate the effectiveness of our proposed dkm, we evaluate our
method on 4d dce-mri breast cancer segmentation using the breast-mri-nact-pilot
dataset [13], which contains a total of 64 patients with the contrastenhanced
mri protocol: a pre-contrast scan, followed by 2 consecutive postcontrast time
points (as shown in fig. 3). each mr volume consists of 60 slices and the size
of each slice is 256 × 256. regarding preprocessing, we conduct zeromean
unit-variance intensity normalization for the whole volume. we divided the
original dataset into training (70%) and test set (30%) based on the scans.
ground truth segmentations of the data are provided in the dataset for tumor
annotation. no data augmentation techniques are used to ensure fairness.
to comprehensively evaluate the proposed method, we compare it with 3d
segmentation methods, including dual attention net (danet) [17], multiresunet
[18] and multi-task learning network (mtln) [19], and 4d segmentation methods,
including lnet [20], 3d patch u-net [21], and hybridnet [5]. all approaches are
evaluated using 1) dice similarity coefficient (dsc) and 2) jaccard index
(ji).implementation details: we implement our proposed framework with pytorch
using two nvidia rtx 2080ti gpus to accelerate model training.following ddpm
[8], we set 128, 256, 256, 256 channels for each stage in the dm and set the
noise level from 10 -4 to 10 -2 using a linear schedule with t = 1000.once the
dm is trained, we extract intermediate feature maps from four resolutions for
further segmentation task. similar to dm, the sm also consists of four
resolution blocks. however, unlike channel settings of dm, we set 128, 256, 512,
1024 channels for each stage in the sm to capture expressive and sufficient
semantic information. the sm is optimized by adam with a learning rate 2 × 10 -5
and a weight decay 10 -6 . the model is trained for 500 epochs with the batch
size to 1. no data augmentation techniques are used to ensure
fairness.comparison with sota methods: the quantitative comparison of the
proposed method to recent state-of-the-art methdos is reported in table 1.
experimental results demonstrate that the proposed method comprehensively other
models with less scans (i.e., pre-contrast) in testing. we attribute it to the
ability of diffusion module to exploit hemodynamic priors to guide the
segmentation task. specifically, in comparison with 3d segmentation models (e.g.
mtln), our method yields higher segmentation scores. the possible reason is that
our method is able to exploit the time intensity curve, which contains richer
information compared to post-contrast scan. besides, we can observe that our
method achieves improvements when compared to 4d segmentation models using
complete sequence. our method outperform the hybridnet by 7.1% and 7.0% in dsc
and ji, respectively. it probably due to two aspects: 1) the hemodynamic
knowledge is implicitly exploited by diffusion module from pre-contrast images,
which is useful for cancer segmentation.2) the intermediate activations from
diffusion models effectively capture the semantic information and are excellent
pixel-level representations for the segmentation problem [9]. thus, combining
the intermediate features can further promote the segmentation performance. in a
word, the proposed framework can produce accurate prediction masks only
requiring pre-contrast images. this is useful when post-contrast data is
limited.ablation study: to explore the effectiveness of the latent kinetic code,
we first conduct ablation studies to select the optimal setting. we denote the
intermediate features extracted from each stage in the dm as f 1 , f 2 , f 3 ,
and f 4 , respectively, where f i represents the feature map of i-th stage.
table 2 reports the segmentation performance with different incorporations of
intermediate kinetic codes. it can be observed that the latent kinetic code is
able to guide the network training for better segmentation results.
specifically, we note that the incorporation of f 3 and f 4 achieves the highest
scores among these combinations, and outperforms the integration of all features
by 2.0% and 2.6% in dsc and ji, respectively. we attribute it to the denoising
diffusion model that receives the noisy input, leading to the noise of shallow
features. in contrast, the deep features capture essential characteristics to
reveal the structural information and hemodynamic changes of tumors. figure 4
shows visual comparison of segmentation performance. the above results reveal
that incorporation of kinetic code comfortably outperform the baseline without
hemodynamic information.
we propose a diffusion kinetic model by exploiting hemodynamic priors in dce-mri
to effectively generate high-quality segmentation results only requiring
precontrast images. our models learns the hemodynamic response function based on
the denoising diffusion process and estimates the latent kinetic code to guide
the segmentation task. experiments demonstrate that our proposed framework has
the potential to be a promising tool in clinical applications to annotate
cancers.
f1 f2 f3 f4 dice (%) ↑ ji (%) ↑ 67.9 ± 2.4 54.4 ± 2.4 68.6 ± 2.3 55.0 ± 2.2 68.3
± 2.4 54.8 ± 2.5 69.3 ± 2.0 55.5 ± 2.1
accurate gland segmentation from whole slide images (wsis) plays a crucial role
in the diagnosis and prognosis of cancer, as the morphological features of
glands can provide valuable information regarding tumor aggressiveness [11].
with the prior uss methods in medical image research [2] and natural image
research [6] vs. our mssg. green and orange regions denote the glands and the
background respectively. (color figure online) emergence of deep learning (dl),
there has been a growing interest in developing dl-based methods for
semantic-level [9,12,36] and instance-level [5,13,25,27,32,35] gland
segmentation. however, such methods typically rely on large-scale annotated
image datasets, which usually require significant effort and expertise from
pathologists and can be prohibitively expensive [28].to reduce the annotation
cost, developing annotation-efficient methods for semantic-level gland
segmentation has attracted much attention [10,18,23,37]. recently, some
researchers have explored weakly supervised semantic segmentation methods which
use weak annotations (e.g., bound box [37] and patch tag [18]) instead of
pixel-level annotations to train a gland segmentation network. however, these
weak annotations are still laborious and require expert knowledge [37]. to
address this issue, previous works have exploited conventional clustering
[8,22,23] and metric learning [10,29] to design annotation-free methods for
gland segmentation. however, the performance of these methods can vary widely,
especially in cases of malignancy. this paper focuses on unsupervised gland
segmentation, where no annotations are required during training and
inference.one potential solution is to adopt unsupervised semantic segmentation
(uss) methods which have been successfully applied to medical image research and
natural image research. on the one hand, existing uss methods have shown
promising results in various medical modalities, e.g., magnetic resonance images
[19],x-ray images [1,15] and dermoscopic images [2]. however, directly utilizing
these methods to segment glands could lead to over-segment results where a gland
is segmented into many fractions rather than being considered as one target (see
fig. 1(b)). this is because these methods are usually designed to be extremely
sensitive to color [2], while gland images present a unique challenge due to
their highly dense and complex tissues with intricate color distribution [18].
on the other hand, prior uss methods for natural images can be broadly
categorized into coarse-to-fine-grained [4,14,16,21,31] and end-to-end (e2e)
cluster-ing [3,6,17]. the former ones typically rely on pre-generated coarse
masks (e.g., super-pixel proposals [16], salience masks [31], and self-attention
maps [4,14,21]) as prior, which is not always feasible on gland images. the e2e
clustering methods, however, produce under-segment results on gland images by
confusing many gland regions with the background; see fig. 1(b). this is due to
the fact that e2e clustering relies on the inherent connections between pixels
of the same class [33], and essentially, grouping similar pixels and separate
dissimilar ones. nevertheless, the glands are composed of different parts (gland
border and interior epithelial tissues, see fig. 1(a)) with significant
variations in appearance. gland borders typically consist of dark-colored cells,
whereas the interior epithelial tissues contain cells with various color
distributions that may closely resemble those non-glandular tissues in the
background. as such, the e2e clustering methods tend to blindly cluster pixels
with similar properties and confuse many gland regions with the background,
leading to under-segment results.to tackle the above challenges, our solution is
to incorporate an empirical cue about gland morphology as additional knowledge
to guide gland segmentation. the cue can be described as: each gland is
comprised of a border region with high gray levels that surrounds the interior
epithelial tissues. to this end, we propose a novel morphology-inspired method
via selective semantic grouping, abbreviated as mssg. to begin, we leverage the
empirical cue to selectively mine out proposals for the two gland sub-regions
with variant appearances. then, considering that our segmentation target is the
gland, we employ a morphology-aware semantic grouping module to summarize the
semantic information about glands by explicitly grouping the semantics of the
sub-region proposals. in this way, we not only prioritize and dedicate extra
attention to the target gland regions, thus avoiding under-segmentation; but
also exploit the valuable morphology information hidden in the empirical cue,
and force the segmentation network to recognize entire glands despite the
excessive variance among the sub-regions, thus preventing over-segmentation.
ultimately, our method produces well-delineated and complete predictions; see
fig. 1(b).our contributions are as follows: (1) we identify the major challenge
encountered by prior unsupervised semantic segmentation (uss) methods when
dealing with gland images, and propose a novel mssg for unsupervised gland
segmentation. (2) we propose to leverage an empirical cue to select gland
sub-regions and explicitly group their semantics into a complete gland region,
thus avoiding over-segmentation and under-segmentation in the segmentation
results. (3) we validate the efficacy of our mssg on two public glandular
datasets (i.e., the glas dataset [27] and the crag dataset [13]), and the
experiment results demonstrate the effectiveness of our mssg in unsupervised
gland segmentation.
the overall pipeline of mssg is illustrated in fig. 2. the proposed method
begins with a selective proposal mining (spm) module which generates a proposal
map that highlights the gland sub-regions. the proposal map is then used to
train a segmentation network. meantime, a morphology-aware semantic grouping
(msg) module is used to summarize the overall information about glands from
their sub-region proposals. more details follow in the subsequent sections.
instead of generating pseudo-labels for the gland region directly from all the
pixels of the gland images as previous works typically do, which could lead to
over-segmentation and under-segmentation results, we propose using the empirical
cue as extra hints to guide the proposal generation process.specifically, let
the i th input image be denoted as x i ∈ r c×h×w , where h, w , and c refer to
the height, width, and number of channels respectively. we first obtain a
normalized feature map f i for x i from a shallow encoder f with 3 convolutional
layers, which can be expressed as f i = f (x i ) 2 . we train the encoder in a
self-supervised manner, and the loss function l consists of a typical
self-supervised loss l ss , which is the cross-entropy loss between the feature
map f i and the one-hot cluster label c i = arg max (f i ), and a spatial
continuity loss l sc , which regularizes the vertical and horizontal variance
among pixels within a certain area s to assure the continuity and completeness
of the gland border regions (see fig. 1 in the supplementary material). the
expressions for l ss and l sc are given below:(1)(2)then we employ k-means [24]
to cluster the feature map f i into 5 candidate regions, denoted as y i = y i,1
∈ r d×n0 , y i,2 ∈ r d×n2 , ..., y i,5 ∈ r d×n5 , where n 1 + n 2 + ... + n 5
equals the total number of pixels in the input image (h × w ). sub-region
proposal selection via the empirical cue. the aforementioned empirical cue is
used to select proposals for the gland border and interior epithelial tissues
from the candidate regions y i . particularly, we select the region with the
highest average gray level as the proposal for the gland border. then, we fill
the areas surrounded by the gland border proposal and consider them as the
proposal for the interior epithelial tissues, while the rest areas of the gland
image are regarded as the background (i.e., non-glandular region). finally, we
obtain the proposal map p i ∈ r 3×h×w , which contains the two proposals for two
gland sub-regions and one background proposal.
a direct merge of the two sub-region proposals to train a fully-supervised
segmentation network may not be optimal for our case. firstly, the two gland
subregions exhibit significant variation in appearance, which can impede the
segmentation network's ability to recognize them as integral parts of the same
object. secondly, the spm module may produce proposals with inadequate
highlighting of many gland regions, particularly the interior epithelial
tissues, as shown in fig. 2 where regions marked with × are omitted.
consequently, applying pixel-level cross-entropy loss between the gland image
and the merged proposal map could introduce undesired noise into the
segmentation network, thus leading to under-segment predictions with confusion
between the glands and the background. as such, we propose two types of
morphology-aware semantic grouping (msg) modules (i.e., msg for variation and
msg for omission) to respectively reduce the confusion caused by the two
challenges mentioned above and improve the overall accuracy and
comprehensiveness of the segmentation results. the details of the two msg
modules are described as follows.here, we first slice the gland image and its
proposal map into patches as inputs. let the input patch and its corresponding
sliced proposal map be denoted as x ∈ r c× ĥ× ŵ and p ∈ r 3× ĥ× ŵ . we can
obtain the feature embedding map f which is derived as f = f feat ( x) and the
prediction map x as x = f cls ( f ), where f feat and f cls refers to the
feature extractor and pixel-wise classifier of the segmentation network
respectively.msg for variation is designed to mitigate the adverse impact of
appearance variation between the gland sub-regions. it regulates the pixel-level
feature embeddings of the two sub-regions by explicitly reducing the distance
between them in the embedding space. specifically, according to the proposal map
p , we divide the pixel embeddings in f ∈ r d× ĥ× ŵ into gland border set g = g
0 , g 1 , ..., g kg , interior epithelial tissue set i = {i 0 , i 1 , ..., i ki
} and non-glandular (i.e., background) set n = {n 0 , n 1 , ..., n kn }, where k
g + k i + k n = ĥ × ŵ . then, we use the average of the pixel embeddings in
gland border set g as the alignment anchor and pull all pixels of i towards the
anchor:(msg for omission is designed to overcome the problem of partial omission
in the proposals. it identifies and relabels the overlooked gland regions in the
proposal map and groups them back into the gland semantic category. to achieve
this, for each pixel n in the non-glandular (i.e., background) set n , two
similarities are computed with the gland sub-regions g and i respectively:s g n
(or s i n ) represents the similarity between the background pixel n and gland
borders (or interior epithelial tissues). if either of them is higher than a
preset threshold β (set to 0.7), we consider n as an overlooked pixel of gland
borders (or interior epithelial tissues), and relabel n to g (or i). in this
way, we could obtain a refined proposal map rp . finally, we impose a
pixel-level cross-entropy loss on the prediction and refined proposal rp to
train the segmentation network:the total objective function l for training the
segmentation network can be summarized as follows: (6) where λ v (set to 1) is
the coefficient.
we evaluate our mssg on the gland segmentation challenge (glas) dataset [27] and
the colorectal adenocarcinoma gland (crag) dataset [13]. the glas dataset
contains 165 h&e-stained histopathology patches extracted from 16 wsis. the crag
dataset owns 213 h&e-stained histopathology patches extracted from 38 wsis. the
crag dataset has more irregular malignant glands, which makes it more difficult
than glas, and we would like to emphasize that the results on crag are from the
model trained on glas without retraining.
the experiments are conducted on four rtx 3090 gpus. for the spm, a 3layer
encoder is trained for each training sample. each convolutional layer uses a 3 ×
3 convolution with a stride of 1 and a padding size of 1. the encoder is trained
for 50 iterations using an sgd optimizer with a polynomial decay policy and an
initial learning rate of 1e-2. for the msg, mmsegmentation [7] is used to
construct a pspnet [38] with a resnet-50 backbone as the segmentation network.
the network is trained for 20 epochs with an sgd optimizer, a learning rate of
5e-3, and a batch size of 16. for a fair comparison, the results of all
unsupervised methods in table 1 are obtained using the same backbone trained
with the corresponding pseudo-labels. the code is available at https://github.
com/xmed-lab/mssg.
we compare our mssg with multiple approaches with different supervision settings
in table 1. on the glas dataset, the end-to-end clustering methods (denoted by "
* ") end up with limited improvement over a randomly initialized network. our
mssg, on the contrary, achieves significant advances. moreover, mssg surpasses
all other unsupervised counterparts, with a huge margin of 10.56% at miou,
compared with the second-best unsupervised counterpart. on crag dataset, even in
the absence of any hints, mssg still outperforms all unsupervised methods and
even some of the fully-supervised methods. additionally, we visualize the
segmentation results of mssg and its counterpart (i.e., sgscn [2]) in fig. 3. on
both datasets, mssg obtains more accurate and complete results.
table 2 presents the ablation test results of the two msg modules. it can be
observed that the segmentation performance without the msg modules is not
satisfactory due to the significant sub-region variation and gland omission.
with the gradual inclusion of the msg for variation and omission, the miou is
improved by 6.42% and 2.57%, respectively. moreover, with both msg modules
incorporated, the performance significantly improves to 62.72% (+14.30%). we
also visualize the results with and without msg modules in fig. 4. it is
apparent that the model without msg ignores most of the interior epithelial
tissues.with the incorporation of msg for variation, the latent distance between
gland borders and interior epithelial tissues is becoming closer, while both of
these two sub-regions are further away from the background. as a result, the
model can highlight most of the gland borders and interior epithelial tissues.
this paper explores a dl method for unsupervised gland segmentation, which aims
to address the issues of over/under segmentation commonly observed in previous
uss methods. the proposed method, termed mssg, takes advantage of an empirical
cue to select gland sub-region proposals with varying appearances. then, a
morphology-aware semantic grouping is deployed to integrate the gland
information by explicitly grouping the semantics of the selected proposals. by
doing so, the final network is able to obtain comprehensive knowledge about
glands and produce well-delineated and complete predictions. experimental
results prove the superiority of our method qualitatively and quantitatively.
accurate segmentation of brain tumors from mri images is of great significance
as it enables more accurate assessment of tumor morphology, size, location, and
distribution range, thereby providing clinicians with a reliable basis for
diagnosis and treatment [16]. physicians manually delineate the tumor regions
based on the varying signal intensities between diseased and normal tissues.
this signal disparity constitutes the edge information in the images, making it
essential for accurate tumor segmentation.cnn-based networks, such as unet [2],
segresnet [15], and nnunet [8], have made significant progress in the field of
medical image segmentation, including brain tumor segmentation. with the
emergence of transformer [19], which is capable of modeling long-range
dependencies that cnns struggle with, a number of cnn-transformer hybrid
networks have been proposed, such as transbts [21], unetr [7], swin-unetr [6]
and nestedformer [23], leading to further improvements in brain tumor
segmentation. however, the performance of existing brain tumor segmentation
methods are still unsatisfactory, especially for the segmentation of edges
between tumor lesion and normal tissues.considerable advancement has been
achieved in the field of natural image segmentation by focusing on the edge
information [3,11,18,25], and this idea is also being applied to medical image
segmentation. some methods utilize the distance-dependent objective functions to
generate more accurate edge predictions. karimi et al. [9] design a
hausdorff-based metric loss function to minimize hausdorff distance (hd), which
is used to measure the edge distance between two point sets. other methods
[1,12,20,22] involve post-processing uncertain regions to more accurately
segment pixels near edges. for example, bat [20] considers global context to
coarsely locate lesion area and paying special attention to the ambiguous area
to specify the exact edges of the skin cancers. similarly, xie et al. [22] use
the confidence map to evaluate the uncertainty of each pixel to enhance the
segmentation of the ambiguous edges of ultrasound images. however, the methods
mentioned above are not suitable for brain tumor segmentation for two main
reasons. (1) efficiency. for instance, karimi et al. [9] require the calculation
of the hd at each iteration, which is both time-consuming and computationally
demanding. moreover, processing every slice of large volumes of mri images at
the pixel-level is impractical. (2) task complexity. unlike many other medical
image segmentation tasks that involve the segmentation of a single roi, brain
tumor segmentation requires the simultaneous segmentation of three regions: the
whole tumor (wt), the tumor core (tc), and the enhancing tumor (et) regions.
therefore, in addition to focusing on the edge between the tumor lesion and
normal tissue to segment the wt, it is also necessary to consider the edges
within the tumor in order to segment the tc and et regions.in this paper, we
propose an edge-oriented transformer (eoformer), for efficient and accurate
brain tumor segmentation. we design a cnn-transformer based encoder for more
effective feature representation, called efficient hybrid encoder (ehe).
specifically, the input image is first processed by the cnn blocks to extract
low-level local features. then, the extracted features are fed into the
transformer blocks to create long-range dependencies, resulting in the formation
of high-level semantic features. in addition, to provide more accurate edge
predictions, we design two edge sharpening modules in the decoder, called
edgeoriented sobel (eos) and laplacian (eol) modules. by implicitly embedding
sobel and laplacian filters into the convolution layers to extract 1st-order and
2nd-order differential features, the two modules could enhance the edge
information contained in the feature maps. in order to reduce the computational
and memory complexity of the model, we replace the vanilla attention module with
our extended efficient attention module [17]. to simplify the model architecture
and reduce inference time, we also introduce the re-parameterization technique
[4,5]. our model has been evaluated on both the publicly brats 2020 dataset and
a private medulloblastoma segmentation dataset. the results demonstrate that
eoformer clearly outperforms the state-of-the-art methods with limited model
parameters and lower flops (see more in supplementary material). (2) a decoder
which incorporates edge-oriented modules to enhance the edge information in
features.
the ehe, shown in fig. 1(a), comprises four stages, each of which consists of a
feature extraction module and a downsampling module. all four feature extraction
modules follow the same paradigm of the general transformer architecture (see
fig. 1(b)), which regards the attention module in the transformer as a token
mixer [24]. in the first two stages of ehe, we use depth-wise convolution
(dwconv) to instantiate the token mixer, called the convformer block. in the
third stage and bottleneck, we use the multi-head self-attention (msa) to
instantiate the token mixer, which is the typical transformer block. for each
stage i, given an input feature map x, the output of the i th block x is
computed as follows:where the tokenmixer i (•) corresponds to dwconv (i ∈ {0,
1}) and msa (i ∈ {2, 3}), norm( • ) represents layer normalization, and mlp(•)
denotes the multilayer perceptron. our approach combines the strengths of cnn
and transformer to create a more powerful encoder that can extract both local
and global information from input data.we address the computational and memory
complexity issues that arise from 3d input by replacing the vanilla attention
with our extended 3d efficient attention. assuming the size of the input feature
is n and the dimensionality is d, the input feature x ∈ r n×d pass through three
linear layers to generate the queriesand the efficient attention e(•) are
computed as follows:where ρ(•) is the softmax activation function, t represents
the matrix transpose operator. the efficient attention reduces the memory
complexity and computational complexity of vanilla attention from o(n 2 ) and
o(dn 2 ) to o(dn + d 2 ) and o(nd 2 ), where
we design the eoformer block (see fig. 1(c)) in the decoder, which instantiates
the token mixer with our proposed edge-oriented sobel module (eos) and
edgeoriented laplacian module (eol). each edge-oriented module includes a normal
3 × 3 × 3 convolution and an edge detection path to extract the 1st-order or the
2nd-order spatial derivatives from intermediate features. this design allows the
edge-oriented module to efficiently extract the edges and textures of the
features. moreover, to boost the segmentation performance without sacrificing
efficiency, we incorporate the re-parameterization technique in the
decoder.edge-oriented sobel module. we use a dual-branch structure, where the
input feature x is simultaneously processed by two different branches. the first
branch contains a 3 × 3 × 3 convolution that extracts basic features from the
input. the second branch, which is responsible for edge extraction, first uses a
c × c × 1 × 1 × 1 convolution to enhance the interaction between channel
features of x, then utilizes a learnable scaled sobel filter to extract the
1storder differentiation edge information from x. this filter is capable of
detecting edges in three directions (i.e. horizontal, vertical, and orthogonal
directions), so it comprises three filters m x , m y , and m z , each of which
is represented by a 3 × 3 × 3 array. take m x as an example, which is described
as:we then apply a learnable scaling matrix s ∈ r c×1×1×1 to m x , which allows
for dynamic adjustment of the scaling factor in each channel. the resulting
feature extracted from the scaled sobel-x filter is denoted as:where the '•'
denotes channel-wise multiplication; the dwconv s•mx indicates that dwconv(•)
applies a s • m x learnable scaled filter as its kernel weight. similarly, f y
and f z are processed in the same way. the final output of the eos module,
denoted as f sob , is given by:edge-oriented laplacian module. different from
the sobel filter that only extracts edges in the horizontal, vertical, and
orthogonal directions, the laplacian filter can extract edges in all directions.
after extracting the 1st-order differentiation edge information, the
intermediate features are then fed into the eol module for extracting the
2nd-order differentiation edge information. similarly, the feature f , obtained
from the learnable scaling laplacian filter, and the final output of the eol
module, denoted as f lap , are defined as:re-parameterization of the
edge-oriented modules. we introduce the re-parameterization [4,5] into the
edge-oriented modules to boost the segmentation performance while maintaining
high efficiency. specifically, we explain the re-parameterization of the eol
module as follows:where ' * ' represents the convolution operation, w conv means
the weights of the convolution and b conv denotes the bias, and up(•) is the
spatial broadcasting operation ,which upgrades the bias b ∈ r 1×c×1×1×1 into
up(b) ∈ r 1×c×3×3×3 . in the inference stage, the output feature f is produced
by a normal 3 × 3 × 3 convolution as follows:
in order to validate the performance of eoformer, we conduct extensive
experiments on both the publicly available brats 2020 dataset and a private
medulloblastoma segmentation dataset (medseg). the brats 2020 dataset [14]
consists of mri image data from 369 patients, with each patient having four
modalities (t1, t1ce, t2 and t2-flair) of skull-striped mri, which are aligned
to a standard brain template. the training/validation/test split follows
315/16/37 according to recent works [10,23].the medseg dataset includes mri
images of t1, t1ce, t2, and t2 flair modalities from 255 patients with
medulloblastoma. the dataset includes manual annotations of the wt and et
regions. these annotated masks are reviewed by two experienced physicians to
ensure the accuracy of the annotated results. the images are registered to the
size of 24 × 256 × 256. the training/validation/test split ratio is 3:1:1.
four-fold cross-validation is performed on this dataset.
we implement eoformer in pytorch 1.11. our model is trained from scratch for 300
epochs using two nvidia gtx 3090 gpus. we select a combination of soft dice loss
and cross-entropy as the loss function and utilize the adamw optimizer [13] with
a weight decay of 1×10 -5 . the initial learning rate is 2×10 -4 . for data
augmentation, we apply image croping, flipping, identity scaling and shifting.
we compare eoformer with seven methods, including cnn-based methods (3d-unet
[2], segresnet [15] and nnunet [8]) and transformer-based methods (transbts
[21], unetr [7], swin-unetr [6], nestedformer [23]). the results are reproduced
on our data split.table 1 displays the performance comparison of eoformer
against other methods on the brats 2020 dataset. eoformer achieves the highest
dice scores on tc, et, and the average. in addition, eoformer attains the best
hd95 scores on tc, et, and the average. hd95 measures the edge distance between
prediction and annotation, which is more sensitive to boundaries. table 2
illustrates the performance of eoformer and other methods on medseg. eoformer
outperforms the second-ranked nestedformer by an average of 1.59% on dice and
achieves the top performance for both wt and et on hd95. furthermore, compared
to the second-ranked segresnet, eoformer demonstrates an
we evaluate the effectiveness of our proposed eoformer framework by conducting
ablation experiments on the brats 2020. in
in this paper, we propose the eoformer, a novel approach for brain tumor
segmentation. our method comprises the efficient hybrid encoder and the
edgeoriented transformer decoder. the encoder effectively extracts features from
images by striking a balance between cnn and transformer architectures. the
decoder integrates the sobel and laplacian edge detection filters into our
edgeoriented modules that enhance the extraction capability of edge and texture
information. besides, we introduce the efficient attention mechanism and the
re-parameterization technology to improve the model efficiency. our eoformer
outperforms other state-of-the-art methods on both brats 2020 and medseg. our
model is computationally efficient and can be readily applied to other 3d
medical image segmentation tasks.
medical image analysis has greatly benefited from advances in ai [1] yet some
improvements still remain to be addressed, importantly in areas that allow both
algorithmic performance and fairness [2], and in certain medical applications
that promise to significantly lessen morbidity and mortality. early detection of
skin lesions is such an endeavor as it can aid in identifying infectious
diseases with cutaneous manifestations. lyme disease is an example of that with
a potentially diagnostic skin lesion [3]-which is caused by the bacterium
borrelia burgdorferi and leads to nearly 476,000 cases per annum during
2010-2018 [4]. the earliest and most treatable phase of lyme disease is
manifested via a red concentric lesion at the site of a tick bite, called
erythema migrans (em) [5]. while the em pattern may appear simple to recognize,
its diagnosis can be challenging for those with or without a medical background
alike, as only 20% of united states patients have the stereotypical bull's eye
lesion [6]. when skin lesions are atypical they can be mistaken for other
diseases such as tinea corporis (tc) or herpes zoster (hz), two other diseases
acting as confusers for lyme, considered herein. this has increased interest in
medical applications of deep learning (dl), and using deep convolutional neural
networks (cnns), to assist clinicians in timely and accurate diagnosis of
conditions including lyme disease, tc and hz [7][8][9].one important diagnosis
task is to segment lyme lesion, particularly the em pattern, from benign skins.
such dl-assisted segmentation not only helps clinicians in pre-screening
patients but also improves downstream tasks such as lesion classification.
however, while lyme disease lesion segmentation is intuitively simple, it is
challenging due to the following reasons. first, there lacks of a well-segmented
dataset with manual labels on lyme disease. on one hand, some datasets-such as
ham10000 [10] and isbi challenges [11]-have manual annotated segmentations for
diseases like melanoma, but they do not have lyme disease lesions. on the other
hand, some datasets-such as groh et al. [12]-have lyme disease and skin tone and
classification labels, but not segmentation.second, the segmentation of lyme
lesion is itself challenging due to the nature of em pattern. specifically, a
typical lyme lesion exhibits a bull's eye pattern with one central redness and
one outer circle, which is different from darkness lesion in cancer-related skin
disease like melanoma. furthermore, clinical data collected for training is
usually imbalanced in some properties, e.g., more samples with light skins
compared with dark skins. therefore, existing skin disease segmentation [13] as
well as existing general segmentation works, such as u-net [14], polar training
[15], vit-adapter [16], and mfsnet [17], usually suffer from relatively low
performance and reduced fairness [2,18,19].in this paper, we present the first
lyme disease dataset that contains labeled segmentation and skin tones. our lyme
disease dataset contains two parts: (i) a classification dataset, composed of
more than 3,000 diseased skin images that are either obtained from public
resources or clinicians with patient-informed consent, and (ii) a segmentation
dataset containing 185 samples that are manually annotated for three
regions-i.e., background, skin (light vs. dark), and lesionconducted under
clinician supervision and institutional review boards (irb) approval. our
dataset with manual labels is available at this url [20].secondly, we design a
simple yet novel data preprocessing and alternation method, called edgemixup, to
improve lyme disease segmentation and diagnosis fairness on samples with
different skin-tones. the key insight is to alter a skin image with a linear
combination of the source image and a detected lesion boundary so that the
lesion structure is preserved while minimizing skin tone information. such an
improvement is an iterative process that gradually improves lesion edge
detection and segmentation fairness until convergence. then, the detected,
converged edge in the first step also helps classification of lyme diseases via
mixup with improved fairness. our source code is available at this url [20].we
evaluate edgemixup for skin disease segmentation and classification tasks. our
results show that edgemixup is able to increase segmentation utility and improve
fairness. we also show that the improved segmentation further improves
classification fairness as well as joint fairness-utility metrics compared to
existing debiasing methods, e.g., ad [21] and st-debias [22].
in this section, we motivate the design of edgemixup by showing that added
lesion boundary helps a dl model focus more on the lesion part instead of other
features such as skin or background. note that not all skin disease datasets are
carefully processed either due to the large amount of work required or the
scarcity of data samples collected, e.g., sd-198 [23] contains samples that are
taken under variant environments. specifically, we train two resnet-34 models
using the same dataset with and without edgemixup for a classification task of
skin disease. we keep all hyper-parameters exactly the same for two models, and
only augment the same image with and without mixing lesion boundary up with the
original image. we generate initial lesion edges using edgemixup, which we will
elaborate in following sections. figure 1 shows the original image (fig. 1a) as
well as two models' attention as heat-maps where red color represents the
highest attention, yellow a higher attention, and purple the least attention.
edgemixup helps the model to focus more on the lesion area comparing fig. 1b
and1c. the reason is that a legacy diagnosis has no information about lesion and
does not know where to locate its focus, thus easily gets distracted by fingers
instead of the lesion pattern.
in this section, we first give the definition for model fairness, and we then
describe the design of edgemixup for the purpose of de-biasing in fig. 2
edgemixup improves model fairness on light and dark skin samples in both
segmentation and classification tasks, and it has two major components: (i) edge
detection using mixup, and (ii) data preprocessing and alteration for downstream
tasks. more specifically, our proposed edge detection has two parts: initial
edge detection and iterative improvement.
the purpose of initial detection, which is documented in the
initial_edge_detection function of algorithm 1, is to provide a starting point,
i.e., a rough boundary, for the next step of iterative improvement. the
high-level idea is that edgemixup detects several edge candidates using the
color range of ground-truth lesions in both red-green-blue (rgb) and
hue-saturation-value (hsv) color space and then selects the target edge using a
learning model based on the output confidence score. first, edgemixup trains a
classification model based on a mixup of the ground-truth segmentation under
clinician supervision and the original image (line 7). second, edgemixup
generates many edge candidates. for example, edgemixup collects the mean range
of lesion color from the training set and use the range as threshold to filter
out any given sample for a candidate mask (line 9). lastly, edgemixup selects an
edge candidate with the highest confidence score output by the learning model
(line 11) and returns it as the edge for this given sample. note that the
initial edge detection is irrelevant to the sample size of a particular
subpopulation, thus improving the fairness. that is, even if the original
dataset is imbalanced, as long as one sample from a subpopulation exists, the
color range of the sample's lesion is considered in the initial detection.
get all edge candidates {edge1, edge2, .., edgen} for each sample x 10:mixup
each edge candidate with x 11:query mclass using all mixed-up {xedge 1 ,
...xedge n } and choose the optimal edge edge opt
generate edged sample xedge = mixup(x, edge opt , α)
while current_jaccard > best_jaccard do
best_jaccard = current_jaccard
predict lesion masks using miter, convert them to lesion edge edge
generate new training set for next model mixup(dtrain, edge, α)
train a model for next iteration miter+1
evaluate miter+1 using edged d test edge and get current_jaccard
iter += 1 affected area, further detection will refine and constrain the
detected boundary. besides, edgemixup calculates a linear combination of
original image and lesion boundary, i.e., by assigning the weight of original
image as α and lesion boundary as 1α. figure 3 shows the edge-mixed-up images
for different iterations. edgemixup removes more skin areas after each iteration
and gradually gets close to the real lesion at the third iteration.
we present two datasets: (i) a dataset collected and annotated by us (called
skin), and (ii) a subset of sd-198 [23] with our annotation (called sd-sub).
first, we collect and annotate a dataset with 3,027 images containing three
types of disease/lesions, i.e., tinea corporis (tc), herpes zoster (hz), and
erythema migrans (em). all skin images are either collected from publicly
available sources or from clinicians with patient informed consent. then, a
medical technician and a clinician in our team manually annotate each image. for
the segmentation task, we annotate skin images into three classes: background,
skin, and lesion; then, for the classification task, we annotate skin images by
classifying them into four classes: no disease (no), tc, hz, and em. we name it
as skin-class for later reference. second, we select five classes from sd-198
[23], a benchmark dataset for skin disease classification, as another dataset
for both segmentation and classification tasks. note that due to the amount of
manual work involved in annotation, we select those classes based on the number
of samples in each class. the selected classes are dermatofibroma (df),
keratoacanthoma (ka), pyogenic granuloma (pg), tinea corporis (tc), and tinea
faciale (tf). we choose 30 samples in each class for segmentation task, and we
split them into 0.7, 0.1, and 0.2 ratio for training, validation, and testing,
respectively.table 1 show the characteristics of these two datasets for both
classification and segmentation tasks broken down by the disease type and skin
tone, as calculated by the individual typology angle (ita) [24]. specifically,
we consider tan2, tan1, and dark as dark skin (ds) and others as light skin
(ls). compared to other skin tone classification schemas such as fitzpartick
scale [25], we divide ita scores into more detailed categories (eight). one
prominent observation is that ls images are more abundant than ds images due to
a disparity in the availability of ds imagery found from either public sources
or from clinicians with patient consent.
we implement edgemixup using python 3.8 and pytorch, and all experiments are
performed using one geforce rtx 3090 graphics card (nvidia).segmentation
evaluation. our segmentation evaluation adopts four baselines, (i) a u-net
trained to segment skin lesions, (ii) a polar training [15] transforming images
from cartesian coordinates to polar coordinates, (iii) vit-adapter [16], a
state-of-the-art semantic segmentation using a fine-tuned vit model, (iv) mfsnet
[17], a segmentation model with differently scaled feature maps to compute the
final segmentation mask. we follow the default setting from each paper for
evaluation. our evaluation metrics include (i) jaccard index (iou score), which
measures the similarity between a predicted mask and the manually annotated
ground truth, and (ii) the gap between jaccard values (j gap ) to measure
fairness. table 2 shows the performance and fairness of edgemixup and different
baselines. we compare predicted masks with the manually-annotated ground truth
by calculating the jaccard index, and computing the gap for subpopulations with
ls and ds (based on ita). edgemixup, a data preprocessing method, improves the
utility of lesion segmentation in terms of jaccard index compared with all
existing baselines. one reason is that edgemixup preserves skin lesion
information, thus improving the segmentation quality, while attenuating markers
for protected factors. note that edgemixup iteratively improves the segmentation
results. take our skin-seg dataset for example. we trained our baseline unet
model for three iterations, and the model utility is increased by 0.0468 on
jaccard index while the j gap between subpopulations is reduced by 0.0193.
classification evaluation. our classification evaluation involves: (i)
adversarial debiasing (ad) [21], (ii) dexined-avg, the average version of
dexined [26] as an boundary detector used by edgemixup, and (iii) st-debias
[22], a debiasing method augmenting data with conflicting shape and texture
information. our evaluation metrics include accuracy gap, the (rawlsian) minimum
accuracy across subpopulations, area under the receiver operating characteristic
curve (auc), and joint metrics (cai α and cauci α ).table 3 shows utility
performance (acc and auc) and fairness results (gaps of acc and auc between ls
and ds subpopulations). we here list two variants of edgemixup, and one of
which, "unet", uses the lesion edge generated by
skin disease classification and segmentation: previous researches mainly work on
improving model utility for both medical image [27] and skin lesion [28]
classification. as for skin lesion segmentation tasks, few works has been
proposed due to the lack of datasets with ground-truth segmentation masks.
international skin imaging collaboration (isic) hosts challenges of
international symposium on biomedical imaging (isbi) [11] to encourage
researches studying lesion segmentation, feature detection, and image
classification. however, official datasets released, e.g., ham10000 [10] only
contains melanoma samples and all of the samples are with light skins according
to our inspection using ita scores.bias mitigation: researchers have addressed
bias and heterogeneity in deep learning models [18,29]. first, masking sensitive
factors in imagery is shown to improve fairness in object detection and action
recognition [30]. second, adversarial debiasing operates on the principle of
simultaneously training two networks with different objectives [31]. the
competing two-player optimization paradigm is applied to maximizing equality of
opportunity [32]. as a comparison, edgemixup is an effective preprocessing
approach to debiasing when applied to skin disease particularly for lyme-focused
classification and segmentation tasks.
we present a simple yet novel approach to segment lyme disease lesion, which can
be further used for disease classification. the key insight is a novel data
preprocessing method that utilizes edge detection and mixup to isolate and
highlight skin lesions and reduce bias. edgemixup outperforms sotas in terms of
jaccord index for segmentation and cai α and cauci α for disease classification.
medical hyperspectral imaging (mhsi) is an emerging imaging modality which
acquires two-dimensional medical images across a wide range of electromagnetic
spectrum. it brings opportunities for disease diagnosis, and computational
pathology [16]. typically, an mhsi is presented as a hypercube, with hundreds of
narrow and contiguous spectral bands in spectral dimension, and thousands of
pixels in spatial dimension (fig. 1(a)). due to the success of 2-dimensional
(2d) deep neural network in natural images, the simplest way to classify/segment
an mhsi is to treat its two spatial dimensions as input spatial dimension, and
treat its spectral dimension as input channel dimension [25] (fig. 1(c)).
dimensionality reduction [12] and recurrent approaches [1] are usually adopted
to aggregate spectral information before feeding the hsi into 2d networks (fig.
1(d)). these methods are not suitable for high spatial resolution mhsi, and they
may bring noises in spatial features while reducing spectral dimension. the 2d
networks are computationally efficient, usually much faster than 3d networks.
but, they mix spectral information after the first convolutional layer, making
the interband correlations of mhsis underutilized. building a 3d network usually
suffers from high computational complexity, but it is the most straightforward
way to learn interpixel and interband correlations of mhsis [23] (fig. 1(e)).
since spatiospectral orientations are not equally likely, there is no need to
treat space and spectrum symmetrically, as is implicit in 3d networks. we might
instead design a dual-stream strategy to "factor" the architecture. a few hsi
classification backbones try to design dual-stream architectures that treat
spatial structures and spectral intensities separately [2,20,30] (fig. 1(f)).
but, these methods simply adopt convolutional or mlp layers to extract spectral
features. spectr [28], learning spectral and spatial features alternatively,
utilizes transformer to capture the global spectral feature. they overlook the
low rankness in the spectral domain, which contains discriminative information
for differentiating targets from the background.high spatiospectral dimensions
make it difficult to perform a thorough analysis of mhsi. in mhsis, there exist
two types of correlation. one is a spectral correlation in adjacent pixels. as
shown in fig. 1(b), the intensity values vs. spectral bands for the local
positive (cancer) area and negative (normal) area are highly correlated. the
other is spatial correlation between adjacent bands. figure 1(b) plots the
spatial similarity among all bands, and shows large cosine similarity scores
among nearby bands (error band of line chart in the light color area) and small
scores between bands in a long distance. the correlation implies spectral
redundancy when representing spatial features, and spatial redundancy when
learning spectral features. the low-rank structure in mhsis holds significant
discriminatory and characterizing information [11]. exploring mhsi's low-rank
prior can promote the segmentation performance.in this paper, we consider
treating spatiospectral dimensions separately and propose an effective and
efficient dual-stream strategy to "factor" the architecture, by exploiting the
correlation information of mhsis. our dual-stream strategy is designed based on
2d cnns with u-shaped [16] architecture. for the spatial feature extraction
stream, inspired from spatial redundancy between adjacent bands, we group
adjacent bands into a spectral agent. different spectral agents are fed into a
2d cnn backbone as a batch. for the spectral feature extraction stream, inspired
by the low-rank prior on the spectral space, we propose a matrix
factorization-based method to capture global spectral information. to remove the
redundancy in the spatiospectral features and promote the capability of
representing the low-rank prior of mhsi, we further design lowrank decomposition
modules, and employ the canonical-polyadic decomposition method [9,32]. our
space and spectrum factorization strategy is plug-and-play. the effectiveness of
the proposed strategy is compared and verified by plugging in different 2d
architectures. we also show that with our proposed strategy, u-net model using
resnet-34 can achieve state-of-the-art mhsi segmentation with 3-13 faster than
other 3d architectures.
mathematically, let z ∈ r s×h×w denote the 3d volume of a pathology mhsi, where
h × w is the spatial resolution, and s is the number of spectral bands. the goal
of mhsi segmentation is to predict the per-pixel annotation mask, where y i
denotes the per-pixel groundtruth for mhsi z i .the overall architecture of our
proposed method is shown in fig. 2, where the 2d cnn in the figure is a proxy
which may represent all widely-used 2d architectures. it represents a spatial
stream, which focuses on extracting spatial features from spectral agents (sect.
2.1). the lightweight spectral stream learns multi-granular spectral features,
and it consists of three key modules: depthwise convolution (dwconv), spectral
matrix decomposition (smd) and feed forward network, where smd module
effectively leverages low-rank prior from spectral features (sect. 2.1).
besides, the low-rank decomposition module (ld) represents high-level low-rank
spatiospectral features (sect. 2.2). the input mhsi z is decomposed into a
spatial input z spa ∈ r g×(s/g)×h×w and a spectral input z spe ∈ r s×c spe 0
×h×w , where g indicates evenly dividing spectral bands into g groups, i.e.,
spectral agents. s/g and c spe 0 = 1 are the input feature dimensions for two
streams respectively.
as mentioned above, for the spatial stream, we first reshape mhsi z ∈ r s×h×w
into z spa ∈ r g×(s/g)×h×w , which has g spectral agents. each spectral agent is
treated as one sample. one sample contains highly correlated spectral bands, so
that the spatial stream can focus on spatial feature extraction. for the
spectral stream, to deal with problems of spatiospectral redundancy and the
inefficiency of global spectral feature representation, we propose a novel and
concise hierarchical structure shown in fig. 2. we employ a basic transformer
paradigm [21] but design it tailored for capturing global low-rank spectral
features. our spectral encoder block can be formulated by:where z in ∈ r
s×cspe×h×w indicates the input spectral token tensor, and c spe is the spectral
feature dimension. we introduce depth-wise conv (dwconv) for dynamically
integrating redundant spatial information into spectral features to reduce
spatial redundant noises, achieved by setting different strides of the
convolutional kernel. then, we represent long-distance dependencies among
spectral inter-bands as a low-rank completion problem. sm d(•) indicates the
spectral matrix decomposition operation. concretely, we flatten feature map x to
spectral sequence tokens x spe ∈ r h•w ×s×cspe , which has s spectral tokens. we
map x spe to a feature space using a linear transform w l ∈ r cspe×c spe . we
then apply a matrix decomposition method nmf (non-negative matrix factorization)
[10], denoted by m(•), to identify and solve for a low-rank signal subspace and
use iterative optimization algorithms backpropagate gradients [4]: sm d(x spe )
= m(w l x spe ). finally, to enhance the individual component of spectral
tokens, we utilize a feedforward neural network (ffn) in transformer consisting
of two linear layers and an activation layer.in the framework shown in fig. 2,
spectral information is integrated from channel dimensions by performing
concatenation, after the second and fourth encoder blocks, to aggregate the
spatiospectral features. the reason for this design is that spectral features
are simpler and lack hierarchical structures compared to spatial features, we
will discuss more in the experimental section.
the mhsi has low-rank priority due to redundancy, so we propose a lowrank
decomposition module using canonical-polyadic (cp) decomposition [9] to set
constraints on the latent representation. for a three-order tensor u ∈ r c ×h ×w
, where h × w is the spatial resolution and c is the channel number. it can be
decomposed into a linear combination of n rank-1 tensors. the mathematical
formulation of cp decomposition can be expressed asr is the tensor rank and λ i
is a scaling factor. recent research [3,32] has proposed new methods based on
dnns to address this problem of representing mhsis as low-rank tensors. as shown
in fig. 2, rank-1 generators are used to create rank-1 tensors in different
directions, which are then aggregated by kronecker product to synthesize a
sub-attention map a 1 . the residual part between the input of features and the
generated rank-1 tensor is used to generate second rank-1 tensors a 2 . it can
obtain r rank-1 tensors by repeating r times. mathematically, this process can
be expressed as:where g c (•), g h (•) and g w (•) are the channel, height and
width generators. finally, we aggregate all rank-1 tensors (from a 1 to a r )
into the attention map along the channel dimension, followed by a linear layer
used to reduce the feature dimension to obtain the low-rank feature u low :where
is the element-wise product, and u low ∈ r c ×h ×w . we employ a straightforward
non-parametric ensemble approach for grouping spectral agents. this approach
involves multiple agents combining their features by averaging the vote. the
encoders in the spatial stream produce 2d feature maps with g spectral agents,
defined as f i ∈ r g×ci×h/2 i ×w/2 i for the ith encoder, where g, c i , h/2 i ,
and w/2 i represent the spectral, channel, and two spatial dimensions,
respectively. the ensemble is computed byrepresents the 2d feature map of the
gth agent. the ensemble operation aggregates spectral agents to produce a 2d
feature map table 1. ablation study (in "mean (std)") on mdc dataset using
regnetx40 [26] as the backbone. sa denote the spectral agent. l1 to l4 represent
the locations where output spectral features from the spectral flow module are
inserted into the spatial flow. tr and conv mean we replace the smd module in
the spectral stream with self-attention and convolutional blocks. best results
are highlighted. with enhanced information interactions learned from the
multi-spectral agents.
the feature maps obtained from the ensemble can be decoded using lightweight 2d
decoders to generate segmentation masks.3 experimental results
we conducted experiments on the public multi-dimensional choledoch (mdc) dataset
[31] with 538 scenes and hyperspectral gastric carcinoma (hgc) dataset [33]
(data provided by the author) with 414 scenes, both with highquality labels for
binary mhsi segmentation tasks. these mhsis are collected by hyperspectral
system with an objective lens of 20x, and wavelengths from 550 nm to 1000 nm for
mdc and 450 nm to 750 nm for hgc, resulting in 60 and 40 spectral bands for each
scene. the size of a single band image in mdc and hgc are both resized to 256 ×
320. following [23,27], we partition the datasets into training, validation, and
test sets using a patient-centric hard split approach with a ratio of 3:1:1.
specifically, each patient's data is allocated entirely to one of the three
sets, ensuring that the same patient's data do not appear in multiple sets.we
use data augmentation techniques such as rotation and flipping, and train with
an adam optimizer using a combination of dice loss and cross-entropy loss for 8
batch size and 100 epochs. the segmentation performance is evaluated using
dice-sørensen coefficient (dsc), intersection of union (iou), and hausdorff
distance (hd) metrics, and throughput (images/s) is reported for comparison.
pytorch framework and four nvidia geforce rtx 3090 are used for implementation.
ablation study. our dual-stream strategy is plug-and-play. we first conduct an
ablation study to show the effectiveness of each component. we use a dualstream
strategy with regnetx40 and u-net architecture. as shown in table 1, our
ablation study shows that spectral agent strategy improves segmentation
performance by more than 2.5% (63.11 vs. 66.05). if we utilize spectral
information from the spectral stream to assist in the spatial stream, we find
that inserting spectral information at l2 and l4 yields a significant
improvement of 3.7% (69.73 vs. 66.05), while inserting at l4 alone also results
in a significant increase of 1.9% in dsc (67.95 vs. 66.05). a slight improvement
is observed when inserting at l2, possibly due to the coarse features of shallow
spectral information. inserting spectral information at all spatial layers
(i.e., l1 to l4) and only at l2 and l4 produce similar results, indicating that
spectral features do not possess complex multilevel characteristics relative to
spatial features. therefore, we adopt a simple and efficient two-layer spectral
flow design. replacing the spectral stream with transformer layers results in a
0.96% (70.88 vs. 69.89) lower dsc, possibly because transformers are difficult
to optimize on small datasets. our proposed ld module is crucial, resulting in a
1.12% performance drop in terms of dsc without it.it is known that high feature
redundancy limits the generalization of neural networks [29]. here we show our
low-rank representation effectively reduces the redundancy of features. our
quantitative and qualitative analysis demonstrated that the proposed mdc and ld
modules effectively reduces the redundancy of output features. following [8], we
define the dominant features for the feature embedding of i-th mhsi h i ∈ r c d
as l i = j : h ij > μ + σ, where μ is mean of h i and σ is stand deviation of h
i . as shown in the left part of fig. 3, our designed modules effectively reduce
the number of dominant features and maintain sparsity in the entire
spatiospectral feature space. inspired by [24], we evaluate the degree of
feature redundancy by computing the pearson correlation coefficient between
different feature channels. as shown in the right part of fig. 3, the
comparisons with state-of-the-art mhsi segmentation methods. table 3 shows
comparisons on mdc and hgc datasets. we use a lightweight and efficient resnet34
as the backbone of our dual-stream method. experimental results show that 2d
methods are generally faster than 3d methods in inference speed, but 3d methods
have an advantage in segmentation performance (dsc & hd). however, our approach
outperforms other methods in both inference speed and segmentation accuracy. it
is also plug-and-play, with the potential to achieve better segmentation
performance by selecting more powerful backbones. the complete table (including
iou and variance) and qualitative results are shown in the supplementary
material.
in this paper, we present to factor space and spectrum for accurate and fast
medical hyperspectral image segmentation. our dual-stream strategy, leveraging
low-rank prior of mhsis, is computationally efficient and plug-and-play, which
can be easily plugged into any 2d architecture. we evaluate our approach on two
mhsi datasets. experiments show significant performance improvements on
different evaluation metrics, e.g., with our proposed strategy, we can obtain
over 7.7% improvement in dsc compared with its 2d counterpart. after plugging
our strategy into resnet-34 backbone, we can achieve state-of-the-art mhsi
segmentation accuracy with 3-13 times faster in terms of inference speed than
existing 3d networks.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43901-8_15.
the periodic acquisition and analysis of volumetric ct and mri scans of oncology
patients is essential for the evaluation of the disease status, the selection of
the treatment, and the response to treatment. currently, scans are acquired
every 2-12 months according to the patient's characteristics, disease stage, and
treatment regime. the scan interpretation consists of identifying lesions
(primary tumors, metastases) in the affected organs and characterizing their
changes over time. lesion changes include changes in the size of existing
lesions, the appearance of new lesions, the disappearance of existing lesions,
and complex lesion changes, e.g., the formation of conglomerate lesions. as
treatments improve and patients live longer, the number of scans in longitudinal
studies increases and their interpretation is more challenging and
time-consuming.radiological follow-up requires the quantitative analysis of
lesions and patterns of lesion changes in subsequent scans. it differs from
diagnostic reading since the goal is to find and quantify the differences
between the scans, rather than to find abnormalities in a single scan. in
current practice, quantification of lesion changes is partial and approximate.
the recist 1.1 guidelines call for finding new lesions (if any), identifying up
to the five largest lesions in each scan in the ct slice where they appear
largest, manually measuring their diameters, and comparing their difference [1].
while volumetric measures of individual lesions and of all lesions (tumor
burden) have long been established as more accurate and reliable than partial
linear measurements, they are not used clinically because they require manual
lesion delineation and lesion matching in unregistered scans, which is usually
time-consuming and subject to variability [2].in a previous paper, we presented
an automatic pipeline for the detection and quantification of lesion changes in
pairs of ct liver scans [3]. this paper describes a graph-based lesion tracking
method for the comprehensive analysis of lesion changes and their patterns at
the lesion level. the tasks are formalized as graph-theoretic problems (fig. 1).
complex lesion changes include merged lesions, which occurs when at least two
lesions grow and merge into one (possible disease progression), split lesions,
which occurs when a lesion shrinks and cleaves into several parts (possible
response to treatment) and conglomeration of lesions, which occurs when clusters
of lesions coalesce. while some of these lesion changes have been observed [4],
they have been poorly studied. comprehensive quantitative analysis of lesion
changes and patterns is of clinical importance, since response to treatment may
vary among lesions, so the analysis of a few lesions may not be
representative.the novelties of this paper are: 1) identification and
formalization of longitudinal lesion matching and patterns of lesion changes in
ct in a graph-theoretic framework; 2) new classification and detection of
changes of individual lesions and lesion patterns based on the properties of the
lesion changes graph and its connected components; 3) a simultaneous lesion
matching method with more than two scans; 4) graph-based methods for the
detection of changes in individual lesions and patterns of lesion changes.
experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18
patients) datasets show that our method yields high classification accuracy.to
the best of our knowledge, ours is the first method to perform longitudinal
lesion matching and lesion changes pattern detection. only a few papers address
lesion matching in pairs of ct/mri scans [5][6][7][8][9][10][11][12][13] -none
performs simultaneous matching of all lesions in more than two scans. also, very
few methods [3,14] handle matching of split/merged lesions. although many
methods exist for object tracking in optical images and videos [15][16][17],
they are unsuited for analyzing lesion changes since they assume many
consecutive 2d images where objects have very similar appearance and undergo
small changes between images. overlap-based methods pair two lesions in
registered scans when their segmentations overlap, with a reported accuracy of
66-98% [3,[5][6][7][8][9][10][11]18]. these methods assume that organs and
lesions undergo minor changes, are very sensitive to registration errors, and
cannot handle complex lesion changes. similarity-based methods pair two lesions
with similar features, e.g., intensity, shape, location [13][14][15][16] with an
84-96% accuracy on the deeplesion dataset [14]. they are susceptible to major
changes in the lesion appearance and do not handle complex lesion changes.
split-andmerge matching methods are used for cell tracking in fluorescence
microscopy [19]. they are limited to 2d images, assume registration between
images, and do not handle conglomerate changes.
we present a new generic model-based method for the automatic detection and
classification of changes in individual lesions and patterns of lesion changes
in consecutive ct scans. the tasks are formalized in a graph-theoretic framework
in which nodes represent lesions, edges represent lesion matchings, and paths
and connected components represent patterns of lesion changes. lesion matchings
are computed with an overlap-based lesion pairing method after establishing a
common reference frame by deformable registration of the scans and organ
segmentations. changes in individual lesions and patterns of lesion changes are
computed from the graph's properties and its connected components. we define
seven individual lesion change classes and five lesion change patterns that
fully describe the evolution of lesions over time.the method inputs the scans
and the organ and lesion segmentations in each scan. its outputs are the lesion
matchings, the labels of the changes in individual lesions, and the patterns of
the lesion changes. the method is a pipeline of four steps: 1) pairwise
deformable registration of each prior scan, organ and lesion segmentations, with
the most recent (current) scan as in [3]; 2) overlap-based lesion matching; 3)
construction of the lesion change graph from the individual lesion segmentations
and lesion matches; 4) detection of changes in individual lesions and patterns
of lesion changes from the graph properties and from analysis of its connected
components.
let s = s 1 , . . . , s n be a series of n ≥ 2 consecutive patient scans
acquired at timesis a set of vertices v i j corresponding to the lesions
associated with the lesion segmentation masks l i = l i 1 , l i 2 , . . . , l i
n i , where n i ≥ 0 is the number of lesions in scan s i at time t i . by
definition, any two lesionsj,l indicates that the lesions corresponding to
vertices v i j , v k l are the same lesion, i.e., that the lesion appears in
scans s i , s k in the same location. edges of consecutive scans s i , s i+1 are
called consecutive edges; edges of non-consecutive scans, s i , s k , i < k -1,
are called non-consecutive edges. the in-and out-degree of a vertex v i j , d in
(v i j ) and d out (v i j ), are the number of incoming and outcoming edges,
respectively.let cc = {cc m } m m=1 be the set of connected components of the
undirected graph version of g, where m is the number of connected components
andby definition, for each 1 ≤ m ≤ m , the sets v m , e m are mutually disjoint
and their unions are v , e, respectively. in a connected component cc m , there
is an undirected path between any two vertices v i j , v k l consisting of a
sequence of undirected edges in e m . . in this setup, connected components
correspond to matched lesions and their pattern of evolution over time (fig.
1d).we define seven mutually exclusive individual lesion change labels for
lesion v i j in scan s i based on the vertex in-and out-degrees (fig. 2). in the
following definitions we refer to the indices: 1 ≤ k < i < l ≤ n ; 1) lone: a
lesion present in scan s i and absent in all previous scans s k and subsequent
scans s l ; 2) new: a lesion present in scan s i and absent in all previous
scans s k ; 3) disappeared: a lesion present in scan s i and absent in all
subsequent scans s l ; 4) unique: a lesion present in scan s i and present as a
single lesion in a previous scan s k and/or in a subsequent scan s l ; 5)
merged: a lesion present in scan s i and present as two or more lesions in a
previous scan s k ; 6) split: a lesion present in scan s i and present as two or
more lesions in a subsequent scan s l ; 7) complex: a lesion present as two or
more lesions in at least one previous scan s k and at least one subsequent scan
s l . we also define as existing a lesion present in scan s i and present in at
least one previous scan s k and one subsequent scan s l , (d in (v i j ) ≥ 1, d
out (v i j ) ≥ 1). for the first and current scans s 1 and s n , we set d in (v
1 j ) = 1, d out (v n j ) = 1, i.e., the lesion existed before the first scan or
remains after the last scan. thus, lesions in the first (last) scan can only be
unique, disappeared or split (unique, new or merged). finally, when lesion v i j
is merged and d out (v i j ) = 0, i < n , it is also labeled disappeared; when
it is split and d in (v i j ) = 0, i > 1, it is also labeled new. we define five
patterns of lesion changes based on the properties of the connected components
cc m of g and on the labels of lesion changes: 1) single_p: a connected
component cc m = v i j consisting of a single lesion labeled as lone, new,
disappeared; 2) linear_p: a connected component consisting of a single earliest
vertex v the changes in individual lesions and the detection and classification
of patterns of lesion changes consist of constructing a graph whose vertices are
the corresponding lesion in the scans, computing the graph consecutive and
non-consecutive edges that correspond to lesion matchings, computing the
connected components of the resulting graph, and assigning an individual lesion
change label to each vertex and a lesion change pattern label to each connected
component according to the categories above.
the changes in individual lesions are directly computed for each lesion from the
resulting graph with the in-and out-degree of each vertex (fig. 2a). the
connected components cc = {cc m } of g are computed by graph depth first search
(dfs). the patterns of lesion changes (fig. 2b) are computed with path and tree
graph algorithms.the changes in individual lesions, patterns of lesion changes,
and lesion changes graph serve as the basis for individual lesion tracking,
which consists of following the path from the lesion in the most recent scan
backwards to its origins in earlier scans and recording the merged, split and
complex lesion changes labels. summarizing longitudinal studies and queries can
also be performed with graph-based algorithms.
we evaluated our method with two studies on retrospectively collected patient
datasets that were manually annotated by an expert radiologist.dataset: lung and
liver ct studies were retrospectively obtained from two medical centers
(hadassah univ hosp jerusalem israel) during the routine clinical examination of
patients with metastatic disease. each patient study consists of at least 3
scans.dlung consists of 83 chest ct scans from 19 patients with a mean 4.4 ± 2.0
scans/patient, a mean time interval between consecutive scans of 125.9 ± 81.3
days, and voxel sizes of 0.6-1.0 × 0.6-1.0 × 1.0-3.0 mm 3 . dliver consists of
77 abdominal cect scans from 18 patients with a mean 4.3 ± 2.0 scans/patient, a
mean time interval between consecutive scans of 109.7 ± 93.5 days, and voxel
sizes of 0.6-1.0 × 0.6-1.0 × 0.8-5.0 mm 3 .lesions in both datasets were
annotated by an expert radiologist, yielding a total of 1,178 lung and 800 liver
lesions, with a mean of 14.2 ± 19.1 and 10.4 ± 7.9 lesions/scan (lesions with
<20 voxels were excluded). ground-truth lesion matching graphs and lesion
changes labeling were produced by running the method on the datasets and then
having the radiologist review and correct the resulting node labels and
edges.study 1: lesion changes labeling, lesion matching, evaluation of patterns
of lesion changes. we ran our method on the dlungs and dliver lesion
segmentations. the settings of the parameters were: dilation distance d = 1 mm,
overlap percentage p = 10%, number of iterations r = 5 and 7, and centroid
maximum distance δ = 17 and 23 mm for the lungs and liver lesions,
respectively.we compared the computed and ground-truth lesion changes graphs
with two metrics: 1) lesion changes classification accuracy, which is the % of
correct computed labels from the ground truth labels; 2) lesion matching
precision and recall based on the presence/absence of computed vs. ground truth
edges. the precision and recall definitions were adapted so that wrong or missed
non-consecutive edges are counted as true positive when there is a path between
their vertices in either the ground-truth or the computed graph. table 1
summarizes the results. the distribution of lesion changes labels for dlungs
(1,178 lesions) is unique 785 (67%), new 215 (18%), lone 109 (9%), disappeared
51 (4%), merged 12 (1%), split 6 (1%), complex 0 (0%) with class accuracy ≥ 96%
for all except split (66%). for dliver (800 lesions) it is unique 450 (56%), new
185 (23%), lone 45 (6%), disappeared 77 (10%), merged 27 (3%), split 18 (2%),
complex 1 (0.05%) with class accuracy ≥ 81% for all except disappeared (71%) and
split (67%).for the patterns of lesion changes, we compared the computed and
ground truth patterns of lesion changes. the accuracy is the % of identical
connected components in each category. table 1 summarizes the results. note that
the split_p, merged_p and complex_p patterns jointly account for 3% and 8% of
the cases. these patterns are hard to detect manually but their correct
classification and tracking are crucial for the proper application of the recist
1.1 follow-up protocol [1]. study 2: detection of missed lesions in the ground
truth. the expert radiologist was asked to examine non-consecutive edges and
lesions labeled as lone in the lesion changes graph and determine if lesions
were unseen or undetected (actual or presumed false negative) in the skipped or
contiguous scans (fig. 1d). for each non-consecutive edge connecting lesions v i
j , v k l , he analyzed the corresponding region in the skipped scans s j at t j
∈ ]t i , t k [ for possible missed lesions. for the dlungs dataset, 25 visible
and 5 faintly visible or surmised to be present unmarked lesions were found for
27 nonconsecutive edges. for the dliver dataset, 20 visible and 21 faintly
visible or surmised to be present unmarked lesions were found for 25
non-consecutive edges.after reviewing the 42 and 37 lesions labeled as lone in
dlungs and dliver with > 5mm diameter, the radiologist determined that 1 and 8
of them had been wrongly identified as a cancerous lesion. moreover, he found
that 14 and 16 lesions initially labeled as lone, had been wrongly classified:
for these lesions he found 15 and 21 previously unmarked matching lesions in the
next or previous scans. in total, 45 and 62 missing lesions were added to the
ground truth dlungs and dliver datasets, respectively. these hard-to-find
ground-truth false negatives (3.7%, 7.2% of all lesions) may change the
radiological interpretation and the disease status. see the supplemental
material for examples of these scenarios.
the use of graph-based methods for lesion tracking and detection of patterns of
lesion changes was shown to achieve high accuracy in classifying changes in
individual lesion and identifying patterns of lesion changes in liver and lung
longitudinal ct studies of patients with metastatic disease. this approach has
proven to be useful in detecting missed, faint, and surmised to be present
lesions, otherwise hardly detectable by examining the scans separately or in
pairs, leveraging the added information provided by evaluating all patient's
scans simultaneously using the labels from the lesion changes graph and
non-consecutive edges.
lesion matchings are determined by the location and relative proximity of the
lesions in two or more registered scans. the lesion matching rule is lesion
voxel overlap: when the lesion segmentation voxels l i j , l k l of vertices v i
j , v k l overlap, 1 ≤ i < k ≤ n , they are matched and the edge e i,k j,l = v i
j , v k l is added to e. lesion matchings are computed first on consecutive
pairs and then on non-consecutive pairs of scans.consecutive lesion matching on
scans (s i , s i+1 ) is performed with an iterative greedy strategy whose aim is
to compensate for registration errors: 1) the lesion segmentations in l i and l
i+1 are isotropically dilated in 3d by d millimeters; 2) for all pairs of
lesions v i j , v i+1 l , compute the intersection % of their corresponding
lesion a segmentationsl is added to e c ; ; 4) remove the lesion segmentations l
i j , l i+1 l from l i , l i+1 , respectively. steps 1-4 are repeated r times.
this yields the consecutive edges graph). the values of d , r are pre-defined
empirically. lesion matching on non-consecutive scans searches for lesion
matchings that were not found previously due to missing lesions (unseen or
undetected). it is performed by examining the pairs of connected components of g
c and finding possible edges (lesion pairings) between them. formally, let cc =
{cc m } m m=1 be the set of undirected connected components of g c . let τ m = t
first m , t last m be the time interval between the first and last scans of cc m
, and let centroid (cc m ) be the center of mass of all lesions in cc m at all
times. let g cc = (v cc , e cc ) be the graph of connected components of g c
such that each vertex cc m of v cc corresponds to a connected component cc m and
edges e cc i,j = cc i , cc j of e cc satisfy three conditions: 1) the time
interval of cc i is disjoint and precedes by at least one time point that of cc
j , i.e., t first i > t last i + 1; 2) the connected components are not too far
from each other, i.e., the distance between the connected components centroids
is smaller than a fixed distance δ, centroid (cc i )centroid cc j ≤ δ; 3) there
is at least one pairwise matching between a lesion in t last i and a lesion in t
first j computed with the consecutive lesion matching method described above.
when the consecutive lesion matching between the lesions in t last i in cc i and
the lesions in t first j in cc j yields a nonempty set of edges, these edges are
added as non-consecutive edges to e c . . iterating over all non-ordered pairs
(cc i , cc j ) yields the set of consecutive and non-consecutive edges of e.we
illustrate this process with the graph of fig. 1. first, the consecutive edges
(straight lines) of the graph are computed by consecutive lesion matching. this
yields a graph with four connected components:
neuropsychiatric systemic lupus erythematosus (npsle) refers to a complex
autoimmune disease that damages the brain nervous system of patients. the
clinical symptoms of npsle include cognitive disorder, epilepsy, mental illness,
etc., and patients with npsle have a nine-fold increased mortality compared to
the general population [11]. since the pathogenesis and mature treatment of
npsle have not yet been found, it is extremely important to detect npsle at its
early stage and put better clinical interventions and treatments to prevent its
progression. however, the high overlap of clinical symptoms with other
psychiatric disorders and the absence of early non-invasive biomarkers make
accurate diagnosis difficult and time-consuming [3].although conventional
magnetic resonance imaging (mri) tools are widely used to detect brain injuries
and neuronal lesions, around 50% of patients with npsle present no brain
abnormalities in structural mri [17]. in fact, metabolic changes in many brain
diseases precede pathomorphological changes, which indicates proton magnetic
resonance spectroscopy ( 1 h-mrs) to be a more effective way to reflect the
early appearance of npsle. 1 h-mrs is a non-invasive neuroimaging technology
that can quantitatively analyze the concentration of metabolites and detect
abnormal metabolism of the nervous system to reveal brain lesions. however, the
complex noise caused by overlapping metabolite peaks, incomplete information on
background components, and low signal-tonoise ratio (snr) disturb the analysis
results of this spectroscopic method [15]. meanwhile, the individual differences
in metabolism and the interaction between metabolites under low sample size make
it difficult for traditional learning methods to distinguish npsle. figure 1
shows spectra images of four participants including healthy controls (hc) and
patients with npsle. it can be seen that the visual differences between patients
with npsle and hcs in the spectra of the volumes are subtle. therefore, it is
crucial to develop effective learning algorithms to discover metabolic
biomarkers and accurately diagnose npsle. the machine learning application for
biomarker analysis and early diagnosis of npsle is at a nascent stage [4]. most
studies focus on the analysis of mr images using statistical or machine learning
algorithms, such as mann-whitney u test [8], support vector machine (svm)
[7,24], ensemble model [16,22], etc. generally, machine learning algorithms
based on the minimum mean square error (mmse) criterion heavily rely on the
assumption that noise is of gaussian distribution. however, measurement-induced
non-gaussian noise in 1 h-mrs data undoubtedly limits the performance of
mmse-based machine learning methods.on the other hand, for the discovery task of
potential biomarkers, sparse codingbased methods (e.g., 2,1 norm, 2,0 norm,
etc.) force row elements to zero that remove some valuable features [12,21].
more importantly, different brain regions have different functions and
metabolite concentrations, which implies that the metabolic features for each
brain region have different sparsity levels. therefore, applying the same
sparsity constraint to the metabolic features of all brain regions may not
contribute to the improvement of the diagnostic performance of npsle.in light of
this, we propose a robust exclusive adaptive sparse feature selection (reasfs)
algorithm to jointly address the aforementioned problems in biomarker discovery
and early diagnosis of npsle. specifically, we first extend our feature learning
through generalized correntropic loss to handle data with complex non-gaussian
noise and outliers. we also present the mathematical analysis of the adaptive
weighting mechanism of generalized correntropy. then, we propose a novel
regularization called generalized correntropy-induced exclusive 2,1 to
adaptively accommodate various sparsity levels and preserve informative
features. the experimental results on a benchmark npsle dataset demonstrate the
proposed method outperforms comparing methods in terms of early noninvasive
biomarker discovery and early diagnosis.
dataset and preprocessing: the t2-weighted mr images of 39 participants
including 23 patients with npsle and 16 hcs were gathered from our affiliated
hospital. all images were acquired at an average age of 30.6 years on a signa
3.0t scanner with an eight-channel standard head coil. then, the mr images were
transformed into spectroscopy by multi-voxel 1 h-mrs based on a point-resolved
spectral sequence (press) with a two-dimensional multi-voxel technique. the
collected spectroscopy data were preprocessed by a sage software package to
correct the phase and frequency. an lcmodel software was used to fit the
spectra, correct the baseline, relaxation, and partial-volume effects, and
quantify the concentration of metabolites. finally, we used the absolute naa
concentration in single-voxel mrs as the standard to gain the absolute
concentration of metabolites, and the naa concentration of the corresponding
voxel of multi-voxel 1 h-mrs was collected consistently. the spectra would be
accepted if the snr is greater than or equal to 10 and the metabolite
concentration with standard deviations (sd) is less than or equal to 20%. the
absolute metabolic concentrations, the corresponding ratio, and the linear
combination of the spectra were extracted from different brain regions: rpcg,
lpcg, rdt, ldt, rln, lln, ri, rpwm, and lpwm. a total of 117 metabolic features
were extracted, and each brain region contained 13 metabolic features: cr,
phosphocreatine (pcr), cr+pcr, naa, naag, naa+naag, naa+naag/cr+pcr, mi,
mi/cr+pcr, cho+phosphocholine (pch), cho+pch/cr+pcr, glu+gln, and
glu+gln/cr+pcr.
given a data matrix x = [x 1 ; • • • ; x n ] ∈ r n×d with n sample, the i-th row
is represented by x i , and the corresponding label matrix is denoted aswhere y
i is one-hot vector. the frobenius norm offor sparse codingbased methods, the
general problem can be formulated aswhere l(w), r(w), and λ are the loss
function, the regularization term, and the hyperparameter, respectively. for
least absolute shrinkage and selection operator (lasso) [20],for multi-task
feature learning [6], 2,1 norm is the most widely used regularization to select
classshared features via row sparsity, which is defined as. due to the row
sparsity of 2,1 norm, the features selected for all different classes are
enforced to be exactly the same. thus, the inflexibility of 2,1 norm may lead to
the deletion of meaningful features.
originating from information theoretic learning (itl), the correntropy [2] is a
local similarity measure between two random variables a and b, given bywhere e,
k σ (•, •), and f ab (a, b) denote the mathematical expectation, the gaussian
kernel, and the joint probability density function of (a, b), respectively. when
applying correntropy to the error criterion, the boundedness of the gaussian
kernel limits the disturbance of large errors caused by outliers on estimated
parameters. however, the kernelized second-order statistic of correntropy is not
suitable for all situations. therefore, the generalized correntropy [1], a more
flexible and powerful form of correntropy, was proposed by substituting the
generalized gaussian density (ggd) function for the gaussian kernel in
correntropy, and the ggd function is defined aswhere γ (•) is the gamma
function, α, β > 0 are the shape and bandwidth parameters, respectively, s = 1/β
α is the kernel parameter, γ = α/(2βγ (1/α)) is the normalization factor.
specifically, when α = 2 and α = 1, ggd degenerates to gaussian distribution and
laplacian distribution, respectively. as an adaptive similarity measure,
generalized correntropy can be applied to machine learning and adaptive systems
[14]. the generalized correntropic loss function between a and b can be defined
asto analyze the adaptive weighting mechanism of generalized correntropy, we
consider an alternative problem of (1), wherethe optimal projection matrix w
should satisfy (∂j(w)/∂w) = 0, and we havewhere λ is a diagonal matrix with
error-based diagonal elementsfor adaptive sample weight.
to overcome the drawback of 2,1 norm and achieve adaptive sparsity
regularization on metabolic features of different brain regions, we propose a
novel gcie 2,1 . a flexible feature learning algorithm exclusive 2,1 [9] is
defined aswherebased on the exclusive 2,1 , we can not only removes the
redundant features shared by all classes through row sparsity of 2,1 norm but
also selects different discriminative features for each class through exclusive
sparsity of 1,2 norm. then we propose to introduce generalized correntropy to
measure the sparsity penalty in the feature learning algorithm. we apply
generalized correntropy to the row vector w i of w to achieve the adaptive
weighted sparse constraint, and the problem (6) can be rewritten aswheresince
minimizing w i 2 is equivalent to maximizing exp(-s w i 2 ), we add a negative
sign to this term. through the gcie 2,1 regularization term in (7), each feature
is expected to be enforced with a sparsity constraint of a different weight
according to its sparsity level of metabolic information in different brain
regions.optimization: since the gcie 2,1 is a non-smooth regularization term,
the final problem (7) can be optimized by the stochastic gradient method with
appropriate initialization of w. to this end, we use the closed-form solution
(5) to initialize w reasonably, and the error-driven sample weight has reached
the optimum based on half-quadratic analysis [13]. once we obtained the solution
to the problem (7), the importance of feature i is proportional to w i 2 . we
then rank the importance of features according to ( w i 2 , • • • , w d 2 ), and
select the top m ranked features from the sorted order for further
classification.
experimental settings: the parameters α and λ 1 are are set to 1, while β and λ
2 are searched form {0.5, 1, 2, 5} and {0.1, 0.5, 1}, respectively. we use adam
optimizer and the learning rate is 0.001. to evaluate the performance of
classification, we employ a support vector machine as the basic classifier,
where the kernel is set as the radial basis function (rbf) and parameter c is
set to 1. we average the 3-fold cross-validation results.results and discussion:
we compare the classification accuracy of the proposed reasfs with several sota
baselines, including two filter methods: maximal information coefficient (mic)
[5], gini [23], and four sparse coding-based methods: multi-task feature
learning via 2,1 norm [6,12], discriminative feature selection via 2,0 norm
[21], feature selection via 1,2 norm [10] and exclusive 2,1 [9]. the proposed
reasfs is expected to have better robustness and flexibility. it can be seen
from fig. 2 that the sparse coding-based methods achieve better performance than
filter methods under most conditions, where "0%" represents no noise
contamination. the highest accuracy of our reasfs demonstrates the effectiveness
and flexibility of the proposed gcie 2,1 . generally speaking, the probability
of samples being contaminated by random noise is equal. therefore, we randomly
select features from the training set and replace the selected features with
pulse noise. the number of noisy attributes is denoted by the ratio between the
numbers of selected features and total features, such as 15% and 30%. the
classification performances of the npsle dataset contaminated by attribute noise
are shown in fig. 3(a) and fig. 3(b), where one clearly perceives that our
reasfs achieves the highest accuracy under all conditions. besides, it is
unreasonable to apply the same level of sparse regularization to noise features
and uncontaminated features, and our gcie 2,1 can adaptively increase the sparse
level of noise features to remove redundant information, and vice versa. for
label noise, we randomly select samples from the training set and replace
classification labels of the selected samples with opposite values, i.e., 0 → 1
and 1 → 0. the results are shown in fig. 3(c) and fig. 3(d), where the proposed
reasfs is superior to other baselines. it can be seen from fig. 3 that our
reasfs achieves the highest accuracy in different noisy environments, which
demonstrates the robustness of generalized correntropic loss. for non-invasive
biomarkers, our method shows that some metabolic features contribute greatly to
the early diagnosis of npsle, i.e., naag, mi/cr+pcr, and glu+gln/cr+pcr in rpcg;
cr+pcr, naa+naag, naa+naag/cr+pcr, mi/cr+pcr and glu+gln in lpcg; naa, naag, and
cho+pch in ldt; pcr, cr+pcr, cho+pch, cho+pch/cr+pcr and glu+gln/cr+pcr in rln;
mi/cr+pcr, cho+pch and cho+pch/cr+pcr in lln; naa+naag/cr+pcr and cho+pch in ri;
cho+pch/cr+pcr and glu+gln/cr+pcr in rpwm; and pcr, naag and naa+naag/cr+pcr in
lpwm. moreover, we use isometric feature mapping (isomap) [19] to analyze these
metabolic features and find that this feature subset is essentially a
low-dimensional manifold. meanwhile, by combining the proposed reasfs and
isomap, we can achieve 99% accuracy in the early diagnosis of npsle. in
metabolite analysis, some studies have shown that the decrease in naa
concentration is related to chronic inflammation, damage, and tumors in the
brain [18]. in the normal white matter area, different degrees of npsle disease
is accompanied by different degrees of naa decline, but structural mri is not
abnormal, suggesting that naa may indicate the progress of npsle. we also found
that glu+gln/cr+pcr in ri decreased, which indicates that the excitatory
neurotransmitter glu in the brain of patients with npsle may have lower
activity. to sum up, the proposed method provides a shortcut for revealing the
pathological mechanism of npsle and early detection.
in this paper, we develop reasfs, a robust flexible feature selection that can
identify metabolic biomarkers and detect npsle at its early stage from noisy 1
h-mrs data. the main advantage of our approach is its robust generalized
correntropic loss and a novel gcie 2,1 regularization, which jointly utilizes
the row sparsity and exclusive sparsity to adaptively accommodate various
sparsity levels and preserve informative features. the experimental results show
that compared with previous methods, reasfs plays a very important role in the
biomarker discovery and early diagnosis of npsle. finally, we analyze metabolic
features and point out their clinical significance.
lung cancer is currently the foremost cause of cancer-related mortalities
globally, with non-small cell lung cancer (nsclc) being responsible for 85% of
reported cases [25]. within nsclc, squamous cell carcinoma (scc) and
adenocarcinoma (adc) are recognized as the two principal histological subtypes.
since scc and adc differ in the effectiveness of chemotherapy and the risk of
complications, accurate identification of different subtypes is crucial for
clinical treatment options [15]. although pathological diagnosis via lung biopsy
can provide a reliable result of subtype identification, it is highly invasive
with potential clinical implications [19]. therefore, non-invasive methods
utilizing computed tomography (ct) images have garnered significant attention
over the last decade [15,16].recently, several deep-learning methods have been
put forward to differentiate between the nsclc histological subtypes using ct
images [4,11,13,22]. chaunzwa et al. [4] and marentakis et al. [13] both employ
a convolutional neural network (cnn) model with axial view ct images to classify
the tumor histology into scc and adc. albeit the good performance, the above 2d
cnn-based models only take ct images from a single view as the input, limiting
their ability to describe rich spatial properties of ct volumes [20]. multi-view
deep learning, a 2.5d method, represents a promising solution to this issue, as
it focuses on obtaining a unified joint representation from different views of
lung nodules to capture abundant spatial information [16,20]. for example, wu et
al. [22] aggregate features from axial, coronal, and sagittal view ct images via
a multi-view fusion model. similarly, li et al. [11] also extract patches from
three orthogonal views of a lung nodule and present a multi-view resnet for
feature fusion and classification. by integrating multi-view representations,
these methods efficiently preserve the spatial information of ct volumes while
significantly reducing the required computational resource compared to 3d cnns
[9,20,23].despite the promising results of previous multi-view methods, they
still confront a severe challenge for accurate nsclc histological subtype
prediction. in fact, due to the limitation of scan time and hardware capacity in
clinical practice, different views of ct volumes are anisotropic in terms of
in-plane and inter-plane resolution [21]. additionally, images from certain
views may inevitably contain some unique background information, e.g., the spine
in the sagittal view [17]. such anisotropy and background dissimilarity both
reveal the existence of significant variations between different views, which
lead to markedly various representations in feature space. consequently, the
discrepancies of distinct views will hamper the fusion of multi-view
information, limiting further improvements in the classification performance.to
overcome the challenge mentioned above, we propose a novel cross-aligned
representation learning (carl) method for the multi-view histologic subtype
classification of nsclc. carl offers a holistic and disentangled perspective of
multi-view ct images by generating both view-invariant and -specific
representations. specifically, carl incorporates a cross-view representation
alignment learning network which targets the reduction of multi-view
discrepancies by obtaining discriminative view-invariant representations. a
shared encoder with a novel discriminability-enforcing similarity constraint is
utilized to map all representations learned from multi-view ct images to a
common subspace, enabling cross-view representation alignment. such aligned
projections help to capture view-invariant features of cross-view ct images and
meanwhile make full use of the discriminative information obtained from each
view. additionally, carl learns view-specific representations as well which
complement the view-invariant ones, providing a comprehensive picture of the ct
volume data for histological subtype prediction. we validate our approach by
using a publicly available nsclc dataset from the cancer imaging archive (tcia).
detailed experimental results demonstrate the effectiveness of carl in reducing
multi-view discrepancies and improving nsclc histological subtype classification
performance. our contributions can be summarized as follows:-a novel
cross-aligned representation learning method called carl is proposed for nsclc
histological subtype classification. to reduce the discrepancies of multiview ct
images, carl incorporates a cross-view representation alignment learning network
for discriminative view-invariant representations. -we employ a view-specific
representation learning network to learn view-specific representations as a
complement to the view-invariant representations. -we conduct experiments on a
publicly available dataset and achieve superior performance compared to the most
advanced methods currently available.
figure 1 shows the overall architecture of carl. the cross-view representation
alignment learning network includes a shared encoder which projects patches of
axial, coronal, and sagittal views into a common subspace with a
discriminability-enforcing similarity constraint to obtain discriminative
view-invariant representations for multi-view discrepancy reduction. in
addition, carl introduces a view-specific representation learning network
consisting of three unique encoders which focus on learning view-specific
representations in respective private subspaces to yield complementary
information to view-invariant representations. finally, we introduce a
histological subtype classification module to fuse the view-invariant and
-specific representations and make accurate nsclc histological subtype
classification.
since the discrepancies of different views may result in divergent statistical
properties in feature space, e.g., huge distributional disparities, aligning
representations of different views is essential for multi-view fusion. with the
aim to reduce multi-view discrepancies, carl introduces a cross-view
representation alignment learning network for mapping the representations from
distinct views into a common subspace, where view-invariant representations can
be obtained by cross-view alignment. specifically, inspired by [12,14,24], we
exert a discriminability-enforcing similarity constraint to align all sub-view
representations with those of the main view, significantly mitigating the
distributional disparities of multi-view representations.technically speaking,
given the axial view image i av , coronal view image i cv , and sagittal view
image i sv , the cross-view representation alignment learning network tries to
generate view-invariant representations h c v , v ∈ {av, cv, sv} via a shared
encoder based on a residual neural network [10]. this can be formulated as
below:where e c (•) indicates the shared encoder, and av, cv, sv represent
axial, coronal, and sagittal views, respectively. in the common subspace, we
hope that through optimizing the shared encoder e c (•), the view-invariant
representations can be matched to some extent. however, the distributions of h c
av , h c cv and h c sv are very complex due to the significant variations
between different views, which puts a burden on obtaining well-aligned
view-invariant representations with merely an encoder.to address this issue, we
design a discriminability-enforcing similarity loss l dsim to further enhance
the alignment of cross-view representations in the common subspace. importantly,
considering that the axial view has a higher resolution than other views and are
commonly used in clinical diagnosis, we choose axial view as the main view and
force the sub-views (e.g., the coronal and sagittal views) to seek
distributional similarity with the main view. mathematically, we introduce a
cross-view similarity loss l sim which calculates the central moment discrepancy
(cmd) metric [24] between all sub-views and the main view as shown below:where
cmd(•) denotes the distance metric which measures the distribution disparities
between the representations of i-th sub-view h sub i and the main view h main .
n is the number of sub-views. despite the fact that minimizing the l sim can
efficiently mitigate the issue of distributional disparities, it may not
guarantee that the alignment network will learn informative and discriminative
representations. inspired by recent work on multimodal feature extraction
[12,14], we impose a direct supervision by inputting h main into a classifier f
(•) to obtain the prediction of histological subtype, and use a crossentropy
loss to enforce the discriminability of the main-view representations. finally,
the discriminability-enforcing similarity loss l dsim is as follows:where y
denotes the ground-truth subtype labels, λ controls the weight of l ce . we
observed that l ce is hundred times smaller than l sim , so this study uses an
empirical value of λ = 110 to balance the magnitude of two terms. by minimizing
l dsim , the cross-view representation alignment learning network pushes the
representations of each sub-view to align with those of the main view in a
discriminability-enforcing manner.notably, the benefits of such cross-alignment
are twofold. firstly, it greatly reduces the discrepancies between the sub-views
and the main view, leading to consistent viewinvariant representations.
secondly, since the alignment between distinct views compels the representation
distribution of the sub-views to match that of the discriminative main view, it
can also enhance the discriminative power of the sub-view representations.in
other words, the cross-alignment procedure spontaneously promotes the transfer
of discriminative information learned by the representations of the main view to
those of the sub-views. as a result, the introduced cross-view representation
alignment learning network is able to generate consistent and discriminative
view-invariant representations cross all views to effectively narrow the
multi-view discrepancies.
on the basis of learning view-invariant representations, carl additionally
learns view-specific representations in respective private subspaces, which
provides supplementary information for the view-invariant representations and
contribute to subtype classification as well. to be specific, a view-specific
representation learning network containing three unique encoders is proposed to
learn view-specific representations h p v , v ∈ {av, cv, sv}, enabling effective
exploitation of the specific information from each view. we formulate the unique
encoders as follows:whereis the encoder function dedicated to capture
single-view characteristics. to induce the view-invariant and -specific
representations to learn unique characteristics of each view, we draw
inspiration from [14] and adopt an orthogonality loss l orth with the squared
frobenius norm between the representations in the common and private subspaces
of each view, which is denoted bya reconstruction module is also employed to
calculate a reconstruction loss l rec between original image i v and
reconstructed image i r v using the l 1 -norm, which ensures the hidden
representations to capture details of the respective view.
after obtaining view-invariant and -specific representations from each view, we
integrate them together to perform nsclc subtype classification. specifically,
we apply a residual block [10] to fuse view-invariant and -specific
representations into a unified multi-view representation h. then, h is sent to a
multilayer perceptron neural network (mlp) to make the precise nsclc subtype
prediction. the nsclc histological subtype classification loss l cls can be
calculated by using cross-entropy loss.
the optimization of carl is achieved through a linear combination of several
loss terms, including discriminability-enforcing similarity loss l dsim ,
orthogonality loss l orth , reconstruction loss l rec and the classification
loss l cls . accordingly, the total loss function can be formulated as a
weighted sum of these separate loss terms:where α, β and γ denote the weights of
l dsim , l rec and l orth . to normalize the scale of l dsim which is much
larger than the other terms, we introduce a scaling factor s = 0.001, and
perform a grid search for α, β and γ in the range of 0.1s-s, 0.1-1, and 0.1-1,
respectively.throughout the experiments, we set the values of α, β and γ to
0.6s, 0.4 and 0.6, respectively.
our dataset nsclc-tcia for lung cancer histological subtype classification is
sourced from two online resources of the cancer imaging archive (tcia) [5]:
nsclc radiomics [1] and nsclc radiogenomics [2]. exclusion criteria involves
patients diagnosed with large cell carcinoma or not otherwise specified, along
with cases that have contouring inaccuracies or lacked tumor delineation [9,13].
finally, a total of 325 available cases (146 adc cases and 179 scc cases) are
used for our study. we evaluate the performance of nsclc classification in
five-fold cross validation on the nsclc-tcia dataset, and measure accuracy
(acc), sensitivity (sen), specificity (spe), and the area under the receiver
operating characteristic (roc) curve (auc) as evaluation metrics. we also
conduct analysis including standard deviations and 95% ci, and delong
statistical test for further auc comparison.for preprocessing, given that the ct
data from nsclc-tcia has an in-plane resolution of 1 mm × 1 mm and a slice
thickness of 0.7-3.0 mm, we resample the ct images using trilinear interpolation
to a common resolution of 1mm × 1mm × 1mm. then one 128 × 128 pixel slice is
cropped from each view as input based on the center of the tumor. finally
following [7], we clip the intensities of the input patches to the interval
(-1000, 400 hounsfield unit) and normalize them to the range of [0, 1].
the implementation of carl is carried out using pytorch and run on a workstation
equipped with nvidia geforce rtx 2080ti gpus and intel xeon cpu 4110 @ 2.10ghz.
adam optimizer is used with an initial learning rate of 0.00002, and the batch
size is set to 8.
comparison with existing methods. several subtype classification methods have
been employed for comparison including: two conventional methods, four
single-view and 3d deep learning methods, and four representative multi-view
methods. we use publicly available codes of these comparison methods and
implement models for methods without code. the experimental results are reported
in table 1. the multi-view methods are generally superior to the single-view and
3d deep learning methods. it illustrates that the multi-view methods can exploit
richer spatial properties of ct volumes than the single-view methods while
greatly reducing the model parameters to avoid overfitting compared to the 3d
methods. the floating point operations (flops) comparison between carl (0.9
gflops) and the 3d method [9] (48.4 gflops) also proves the computational
efficiency of our multi-view method. among all multi-view methods, our proposed
carl achieves the best results, outperforming wu et al. by 3.2%, 3.2%, 1.5% and
4.1% in terms of auc, acc, sen and spe, respectively. not surprisingly, the roc
curve of carl in fig. 2(a) is also closer to the upper-left corner, further
indicating its superior performance. these results demonstrate that carl can
effectively narrow the discrepancies of different views by obtaining
view-invariant representations in a discriminative way, thus leading to
excellent classification accuracy compared to other methods. ablation analysis.
we evaluate the efficacy of different losses in our method. the results are
reported in table 2, where carl-b0 refers to carl only using the classification
loss, +l * indicates the loss superimposed on carl-b0 and l all denotes that we
utilize all the losses in eq. 5. we can observe that carl-b2 performs better
than carl-b0 by employing the discriminability-enforcing similarity loss to
align crossview representations. besides, carl-b3 and carl-b4 show better
performance than carl-b0, illustrating view-specific representations as a
complement which can also contribute to subtype classification. though single
loss already contributes to performance improvement, carl-b5 to carl-b7
demonstrate that the combinations of different losses can further enhance
classification results. more importantly, carl with all losses achieves the best
performance among all methods, demonstrating that our proposed method
effectively reduces multi-view discrepancies and significantly improves the
performance of histological subtype classification by providing a holistic and
disentangled perspective of the multi-view ct images. the roc curve of carl in
fig. 2(b) is generally above its variants, which is also consistent with the
quantitative results.
in summary, we propose a novel multi-view method called cross-aligned
representation learning (carl) for accurately distinguishing between adc and scc
using multi-view ct images of nsclc patients. it is designed with a cross-view
representation alignment learning network which effectively generates
discriminative view-invariant representations in the common subspace to reduce
the discrepancies among multi-view images. in addition, we leverage a
view-specific representation learning network to acquire viewspecific
representations as a necessary complement. the generated view-invariant and
-specific representations together offer a holistic and disentangled perspective
of the multi-view ct images for histological subtype classification of nsclc.
the experimental result on nsclc-tcia demonstrates that carl reaches 0.817 auc,
76.8% acc, 73.2% sen, and 79.7% spe and surpasses other relative approaches,
confirming the effectiveness of the proposed carl method.
gastroscopic lesion detection (gld) plays a key role in computer-assisted
diagnostic procedures. although deep neural network-based object detectors
achieve tremendous success within the domain of natural images, directly
training generic object detectors on gld datasets performs below expectations
for two reasons: 1) the scale of labeled data in gld datasets is limited in
comparison to natural images due to the annotation costs. though gastroscopic
images are abundant, those containing lesions are rare, which necessitates
extensive image review for lesion annotation. 2) the characteristic of
gastroscopic images exhibits distinct differences from the natural images
[18,19,21] and is often of high similarity in global but high diversity in
local. specifically, each type of lesion may have diverse appearances though
gastroscopic images look quite similar. some appearances of lesions are quite
rare and can only be observed in a few patients. generic self-supervised
backbone pre-training or semi-supervised detector training methods can solve the
first challenge for natural images but its effectiveness is undermined for
gastroscopic images due to the second challenge.self-supervised backbone
pre-training methods enhance object detection performance by learning
high-quality feature representations from massive unlabelled data for the
backbone. the mainstream self-supervised backbone pretraining methods adopt
self-supervised contrast learning [3,4,7,9,10] or masked fig. 1. pipeline of
self-and semi-supervised learning (ssl) for gld. ssl consists of a hybrid
self-supervised learning (hsl) method and a prototype-based pseudo-label
generation (ppg) method. hsl combines patch reconstruction with dense
contrastive learning. ppg generates pseudo-labels for potential lesions based on
the similarity to the prototype feature vectors.image modeling [8,15].
self-supervised contrastive learning methods [3,4,7,9] can learn discriminative
global feature representations, and [10] can further learn discriminative local
feature representations by extending contrastive learning to dense paradigm.
however, these methods usually cannot grasp enough local detailed information.
on the other hand, masked image modeling is expert in extracting local detailed
information but is weak in preserving the discriminability of feature
representation. therefore, both types of methods have their own weakness for gld
tasks.semi-supervised object detection methods [12,14,16,17,20,22,23] first use
detectors trained with labeled data to generate pseudo-labels for unlabeled data
and then enhance object detection performance by regarding these unlabeled data
with pseudo-labels as labeled data to train the detector. current pseudolabel
generation methods rely on the objectiveness score threshold to generate
pseudo-labels, which makes them perform below expectations on gld, because the
characteristic of gastroscopic lesions makes it difficult to set a suitable
threshold to discover potential lesions meanwhile avoiding introducing much
noise.the motivation of this paper is to explore how to enhance gld performance
using massive unlabeled gastroscopic images to overcome the labeled data
shortage problem. the main challenge for this goal is the characteristic of
gastroscopic lesions. intuitively, such a challenge requires local feature
representations to contain enough detailed information, meanwhile preserving
discriminability. enlightened by this, we propose the self-and semi-supervised
learning (ssl) framework tailored to address challenges in daily clinical
practice and use massive unlabeled data to enhance gld performance. ssl
overcomes the challenges of gld by leveraging a large volume of unlabeled
gastroscopic images using self-supervised learning for improved feature
representations and semi-supervised learning to discover and utilize potential
lesions to enhance performance. specifically, it consists of a hybrid
self-supervised learning (hsl) method for self-supervised backbone pre-training
and a prototype-based pseudo-label generation (ppg) method for semi-supervised
detector training. the hsl combines the dense contrastive learning [10] with the
patch reconstruction to inherit the advantages of discriminative feature
learning and grasp the detailed information that is important for gld tasks. the
ppg generates pseudo-labels based on the similarity to the prototype feature
vectors (formulated from the feature vectors in its memory module) to discover
potential lesions from unlabeled data, and avoid introducing much noise at the
same time. moreover, we propose the first large-scale gld datasets (lgldd),
which contains 10,083 gastroscopic images with 12,292 well-annotated lesion
bounding boxes of four categories of lesions (polyp, ulcer, cancer, and
sub-mucosal tumor). we evaluate ssl with multiple detectors on lgldd and ssl
brings significant improvement compared with baseline methods (centernet [6]:
+2.7ap, faster rcnn [13]: +2.0ap). in summary, our contributions include:-a
self-and semi-supervise learning (ssl) framework to leverage massive unlabeled
data to enhance gld performance. -a large-scale gastroscopic lesion detection
datasets (lgldd) -experiments on lgldd demonstrate that ssl can bring
significant enhancement compared with baseline methods.
in this section, we introduce the main ideas of the proposed ssl for gld. the
proposed approach includes 2 main components and is illustrated in fig. 1.
the motivation of hybrid self-supervised learning (hsl) is to learn the local
feature representations of high discriminability meanwhile contain detailed
information for the backbone from massive unlabeled gastroscopic images. among
existing backbone pre-training methods, dense contrastive learning can preserve
local discriminability and masked image modeling can grasp local detailed
information. therefore, to leverage the advantages of both types of methods, we
propose hybrid self-supervised learning (hsl), which combines patch
reconstruction with dense contrastive learning to achieve the goal.structure.
hsl heritages the structure of the densecl [10] but adds an extra reconstruction
projection head to reconstruct patches. specifically, hsl consists of a backbone
network and 3 parallel sub-heads. the global projection head and the dense
projection head heritages from the densecl [10], and the proposed reconstruction
projection head is inspired by the masked image modeling.enlightened by the
simmim [15], we adopt a lightweight design for the reconstruction projection
head, which only contains 2 convolution layers.learning pipeline. like other
self-supervised contrastive learning methods, hsl randomly generates 2 different
"views" of the input image, uses the backbone to extract the dense feature maps
f 1 , f 2 ∈ r h×w ×c , and then feeds them to the following projection heads.
the global projection head of hsl uses f 1 , f 2 to obtain the global feature
vector f g1 , f g2 like moco [9]. the dense projection head and the
reconstruction projection head crop the dense feature maps f 1 , f 2 into s×s
patches and obtain the local feature vector sets f 1 and f 2 of each viewthe
dense projection head use f 1 and f 2 to obtain local feature vector sets f l1
and f l2 (f l = {f l1 , f l2 , ..., f ls 2 }) like densecl [10]. the
reconstruction projection head uses each feature vector in f 1 , f 2 to
reconstruct corresponding patches and obtains the patch set p 1 , p 2 (p = {p i1
, p i2 , ..., p is 2 }.training objective. the hsl formulates the two
contrastive learning as dictionary look-up tasks like densecl [10] while the
reconstruction learning as a regression task. the global contrastive learning
uses the global feature vector f g of an image as query q and feature vectors
from the alternate view of the query image and the other images within the batch
as keys k = {k 1 , k 2 , ..., }. for each query q, the only positive key k + is
the different views of the same images and the others are all negative keys (k
-) like moco [9]. we adopt the infonce loss function for it:the dense
contrastive learning uses the local feature vector in f li as query r and keys t
l = {t 1 , t 2 , ..., }. the negative keys t -here are the feature vectors of
different images while the positive key t + is the correspondence feature vector
of r in another view of the images. specifically, we adopt the correspondence
methods in densecl [10] to obtain the positive key t + , which first conducts
the matching process based on vector-wise cosine similarity between r and
feature vectors in t and then selects the t j of highest similarity as the t + .
the loss function is also the infonce loss but in a dense paradigm:the
reconstruction task uses the feature vector in f to reconstruct each patch and
obtain p i . the ground truth is the corresponding patchesof the input view. we
adopt the mse loss function for it:the overall loss function is the weighted sum
of these losses:where λ d and λ r are the weights of l d and l r and are set to
1 and 2.
we propose the prototype-based pseudo-label generation method (ppg) to discover
potential lesions from unlabeled gastroscopic data meanwhile avoid introducing
much noise to further enhance gld performance. specifically, ppg adopts a memory
module to remember feature vectors of the representative lesions as memory and
generates prototype feature vectors for each class based on the memories stored.
to preserve the representativeness of the memory and further the prototype
feature vectors, ppg designs a novel memory update strategy. in semi-supervised
learning, ppg generates pseudo-labels for unlabeled data relying on the
similarity to the prototype feature vectors, which achieves a better balance
between lesion discovery and noise avoidance.memory module. memory module stores
a set of lesion feature vectors as memory. for a c-class gld task, the memory
module stores c × n feature vectors as memory. specifically, for each lesion, we
denote the feature vector used to classify the lesion in the detector as f c .
ppg stores n feature vectors for each class c to formulate the class memory m c
= {f c1 , f c2 , ..., f cn }, and the memory m of ppg can be expressed as m = {m
1 , m 2 , ..., m c }. then, ppg obtains the prototype feature vector p c by
calculating the center of each class memory m c , and the prototype feature
vector set can be expressed as p t = {p 1 , p 2 , ..., p c }. moreover, the
prototype feature vectors further serve as supervision for detector training
under a contrastive clustering formulation and adopt a contrastive loss:if the
detector training loss is l det , the overall loss l can be expressed as:where
the λ cc is the weight of the contrastive learning loss and is set to 0.5.memory
update strategy. memory update strategy directly influences the
representativeness of the class memory m c and further the prototype feature
vector p c . therefore, ppg adopts a novel memory update strategy, which follows
the idea that "the memory module should preserve the more representative feature
vector among similar feature vectors". the pipeline of the strategy is as
follows: 1) acquisition the lesion feature vector f c . 2) identification of the
most similar f s to f c from corresponding class memory m c based on
similarity:3) updating the memory by selecting more unique features f s of f =
{f s , f c } compared to the class prototype feature vector p c based upon
similarity:the similarity function sim(u, v) can be expressed as sim(u, v) = u t
v/ u v .to initialize the memories, we empirically select 50 lesions randomly
for each class. to maintain stability, we start updating the memory and
calculating its loss after fixed epochs, and only the positive sample feature
vector can be selected to update the memory.pseudo-label generation. ppg
proposes to generate pseudo-labels based on the similarity between the prototype
feature vectors and the feature vector of potential lesions. to be specific, ppg
first detects a large number of potential lesions with a low objectiveness score
threshold τ u and then matches them with all the prototype feature vectors p to
find the most similar one:ppg assigns the pseudo-label c for similarity value
sim(p c , f u ) greater than the similarity threshold τ s otherwise omits it. we
set τ u = 0.5 and τ s = 0.5
we contribute the first large-scale gstroscopic lesion detection datasets
(lgldd) in the literature. collection : lgmdd collects about 1m+ gastroscopic
images from 2 hospitals of about 500 patients and their diagnosis reports. after
consulting some senior doctors and surveying gastroscopic diagnosis papers [1],
we select to annotate 4-category lesions: polyp(pol), ulcer(ulc), cancer(can)
and sub-mucosal tumor(smt). we invite 10 senior doctors to annotate them from
the unlabeled endoscopic images. to preserve the annotation quality, doctors can
refer to the diagnosis reports, and each lesion is annotated by a doctor and
checked by another. finally, they annotates 12,292 lesion boxes in 10,083 images
after going through about 120,000 images. the polyp, ulcer, cancer, and
sub-mucosal tumor numbers are 7,779, 2,171, 1,164and 1,178, respectively. the
train/val split of lgmdd is 8,076/2,007. the other data serves as unlabeled
data.evaluation metrics : we use standard object detection metrics to evaluate
the gld performance, which computes the average precision (ap) under multiple
intersection-of-union (iou) thresholds and then evaluate the performance using
the mean of aps (map) and the ap of some specific iou threshold. for map, we
follow the popular object detection datasets coco [11] and calculate the mean of
11 aps of iou from 0.5 to 0.95 with stepsize 0.05 (map @[.5:.05:.95]).we also
report ap under some specific iou threshold (ap 50 for .5, ap 75 for .75) and ap
of different scale lesions (ap s , ap m , ap l ) like coco [11].
please kindly refer to the supplemental materials for implementation details and
training setups. the objectiveness score threshold controls the quality of
pseudo-labels. a) a low threshold generates noisy pseudo-labels, leading to
reduced performance (-0.6/-0.2 ap at thresholds 0.5/0.6). b) a high threshold
produces high-quality pseudo-labels but may miss potential lesions, resulting in
only slight performance improvement (+0.3 ap at threshold 0.7). c) ppg approach
uses a low threshold (0.5) to identify potential lesions, which are then
filtered using prototype feature vectors, resulting in the most significant
performance enhancement (+0.9 ap).3) memory update strategy influences the
representativeness of memory and the prototype feature vectors. we compare our
memory update strategy with a queue-like ('q-like') memory update strategy
(first in & first out). experiment results (table 2.c) show our memory update
strategy performs better. 4) endo21: to further evaluate the effectiveness of
ssl, we conduct experiments on endo21 [2] sub-task 2 (endo21 challenge consists
of 4 sub-tasks and only the sub-task 2 train/test split is available according
to the [2]). experimental results in table 2.d show that ssl can bring
significant improvements to publicly available datasets. moreover, ssl
overperforms current sota (yolo v5 [2]).
in this work, we propose self-and semi-supervised learning (ssl) for gld
tailored for using massive unlabeled gastroscopic to enhance gld performance.the
key novelties of the proposed method include a hybrid contrastive learning
method for backbone pre-training and a prototype-based pseudo-label generation
method for semi-supervised learning. moreover, we contribute the first
large-scale gld datasets (lgldd). experiments on lgldd prove that ssl can bring
significant improvements to gld performance. since annotation cost always limits
of datasets scale of such tasks, we hope ssl and lgldd could fully realize its
potential, as well as kindle further research in this direction.
2) objectiveness score threshold τ u : we compare ppg with objectiveness
score-based pseudo-label generation methods with different τ u (table 2.b).
colorectal cancer is a life-threatening disease that results in the loss of
millions of lives each year. in order to improve survival rates, it is essential
to identify colorectal polyps early. hence, regular bowel screenings are
recommended, where endoscopy is the gold standard. however, the accuracy of
endoscopic screening can heavily rely on the individual skill and expertise of
the domain experts involved, which are prone to incorrect diagnoses and missed
cases. to reduce the workload on physicians and enhance diagnostic accuracy,
computer vision technologies, such as deep neural networks, are involved to
assist in the presegmentation of endoscopic images. the unet-like model uses
skip connections that transmit only single-stage features. in contrast, our
approach utilizes fpe to propagate features from all stages, incorporating a
gate mechanism to regulate the flow of valuable information.deep learning-based
image segmentation methods have gained popularity in recent years, dominated by
unet [11] in the field of medical image segmentation. unet's success has led to
the development of several other methods that use a similar encoder-decoder
architecture to tackle polyp segmentation, including resunet++ [7], pranet [3],
caranet [10] and uacanet [8]. however, these methods are prone to inefficient
feature fusion at the decoder due to the transmission of multi-stage features
without filtering out irrelevant information.to address these limitations, we
propose a novel feature enhancement network for polyp segmentation that employs
feature propagation enhancement (fpe) modules to transmit multi-scale features
from all stages to the decoder. figure 1 illustrates a semantic comparison of
our feature propagation scheme with the unet-like model. while the existing
unet-like models use skip connections to propagate a single-scale feature, our
method utilizes fpe to propagate multi-scale features from all stages in
encoder. more importantly, this research highlights the usage of fpe can
effectively replace skip connections by providing more comprehensive multi-scale
characteristics from full stages in encoder. to further address the issue of
high-level semantics being overwhelmed in the progressive feature fusion
process, we also integrate a feature aggregation enhancement (fae) module that
aggregates the outputs of fpe from previous stages at decoder. moreover, we
introduce gate mechanisms in both fpe and fae to filter out redundant
information, prioritizing informative features for efficient feature fusion.
finally, we propose a multi-scale aggregation (msa) module appended to the
output of the encoder to capture multi-scale features and provide the decoder
with rich multi-scale semantic information. the msa incorporates a cross-stage
multi-scale feature aggregation scheme to facilitate the aggregation of
multi-scale features. overall, our proposed method improves upon existing
unet-like encoder-decoder architectures by addressing the limitations in feature
propagation and feature aggregation, leading to improved polyp segmentation
performance.our major contributions to accurate polyp segmentation are
summarized as follows.(1) the method addresses the limitations of the unet-like
encoder-decoder architecture by introducing three modules: feature propagation
enhancement (fpe), feature aggregation enhancement (fae), and multi-scale
aggregation (msa). ( 2) fpe transmits all encoder-extracted feature maps to the
decoder, and fae combines the output of the last stage at the decoder and
multiple outputs from fpe. msa aggregates multi-scale high-level features from
fpes to provide rich multi-scale information. (3) the proposed method achieves
state-of-the-art results in five polyp segmentation datasets and outperforms the
previous cutting-edge approach by a large margin (3%) on cvc-colondb and etis
datasets.
overview. our proposed feature enhancement network illustrated in fig. 2(a), is
also a standard encoder-decoder architecture. following polyp-pvt [2], we adopt
pvt [16] pretrained on imagenet as the encoder. the decoder consists of three
feature aggregation enhancement modules (fae) and a multi-scale aggregation
module (msa). given an input image i, we first extract the pyramidal features
using the encoder, which is defined as follows,where, {p 1 , p 2 , p 3 , p 4 }
is the set of pyramidal features from four stages with the spatial size of 1/4,
1/8, 1/16, 1/32 of the input respectively. features with lower spatial
resolution usually contain richer high-level semantics. then, these features are
transmitted by the feature propagation enhancement module (fpe) to yield the
feature set {c 1 , c 2 , c 3 , c 4 }, which provides multi-scale information
from all the stages. this is different from the skip connection which only
transmits the single-scale features at the present stage. referring to [3,10],
three highest-level features generated by fpes are subsequently fed into a msa
module for aggregating rich multi-scale information. afterwards, feature fusion
is performed by fae in the decoder, whereby it progressively integrates the
outputs from fpe and previous stages. the higher-level semantic features of the
fpe output are capable of effectively compensating for the semantics that may
have been overwhelmed during the upsampling process. this process is formulated
as,a noteworthy observation is that the gating mechanism has been widely
utilized in both fpe and fae to modulate the transmission and integration of
features. by selectively controlling the flow of relevant information, this
technique has shown promise in enhancing the overall quality of feature
representations [4,9]. the final features o 1 are passed through the classifier
(i.e., a 1 × 1 convolutional layer) to get the final prediction result in o.
further details on fpe, fae, and msa will be provided in the following
sections.feature propagation enhancement module. in contrast to the traditional
encoder-decoder architecture with skip connections, the fpe aims to transmit
multi-scale information from full stage at the encoder to the decoder, rather
than single-scale features at the current stage. the fpe architecture is
illustrated in fig. 2(b). the input of the fpe includes the features from the
other three stages, in addition to the feature of the current stage, which
delivers richer spatial and semantic information to the decoder. however, these
multi-stage inputs need to be downsampled or upsampled to match the spatial
resolution of the features at the present stage. to achieve this, fpe employs a
stepwise downsampling strategy, with each step only downsampling by a factor of
2, and each downsampling step is followed by a convolutional unit (cu) to
perform feature transformation. the number of downsamplings is denoted as n =
log t 2 , where t stands for the scale factor for downsampling. the cu consists
of a 3 × 3 convolutional layer, a batch normalization layer, and an activation
layer (i.e., relu). this strategy can be a highly effective means of mitigating
the potential loss of intricate details during the process of large-scale
interpolation. similarly, this same strategy is employed in fae.the features
from the other three stages, denoted as p 1 , p 2 , and p 3 , are downsampled or
upsampled to generate p 1 , p 2 , and p 3 . instead of directly combining the
four inputs, fpe applies gate mechanisms to emphasize informative features. the
gate mechanism takes the form of y = g(x ) * y, where g (in this work, sigmoid
is used) measures the importance of each feature vector in the reference feature
x ∈ r h×w . by selectively enhancing useful information and filtering out
irrelevant information, the reference features x assist in identifying optimal
features y at the current level. the output of g is in [0, 1] h×w , which
controls the transmission of informative features from y or helps filter useless
information. notably, x can be y itself, serving as a reference feature. fpe
leverages such gate mechanism to obtain informative features in p and passes
them through a cu respectively. after that, fpe concatenates features from the
four branches to accomplish feature aggregation. a cu is followed to boost the
feature fusion.feature aggregation enhancement module. the fae is a novel
approach that integrates the outputs of the last stages at the decoder with the
fpe's outputs at both the current and deeper stages to compensate for the
high-level semantics that may be lost in the process of progressive feature
fusion. in contrast to the traditional encoder-decoder architecture with skip
connections, the fae assimilates the output of the present and higher-stage
fpes, delivering richer spatial and semantic information to the decoder.the fae,
depicted in fig. 2(c), integrates the outputs of the current and deeper fpe
stages with high-level semantics. as an example, the last fae takes as inputs o
2 (output of the penultimate fae), c 1 (output of the current fpe stage), and c
2 and c 3 (outputs of fpe from deeper stages). multiple outputs from deeper fpe
stages are introduced to compensate for high-level semantics. furthermore, gate
mechanisms are utilized to filter out valueless features for fusion, and the
resulting enhanced feature is generated by a cu after concatenating the filtered
features. finally, o 2 is merged with the output feature through element-wise
summation, followed by a cu to produce the final output feature o 1 .multi-scale
aggregation module. the msa module in our proposed framework, inspired by the
parallel partial decoder in pranet [3], integrates three highest-level features
c 2 , c 3 , and c 4 from the fpe output. this provides rich multi-scale
information for subsequent feature aggregation in the fae and also helps to form
a coarse localization of polyps under supervision. it benefits from an
additional supervision signal, as observed in pranet [3], caranet [8], and etc.
by aiding in forming a coarse location of the polyp and contributing to improved
accuracy and performance. as depicted in fig. 2(d), the msa module first
processes these three features separately. c 2 , which has the highest feature
resolution, is processed with multiple dilated convolutions to capture its
multiscale information while keeping its spatial resolution unchanged. c 3 is
processed with only one dilated convolution due to its higher spatial
resolution, while c 4 is not processed since it already contains the richest
contextual information. the output features of the three branches are then
upsampled to the size of c 2 . to better integrate these three multi-scale
high-level features for subsequent fusion, additional cross-feature fusion
operations (i.e., 2 cu layer) are performed in the msa module.
datasets. we conduct extensive experiments on five polyp segmentation datasets,
including kvasir [6], cvc-clinicdb [1], cvc-colondb [13], etis [12] and cvc-t
[14]. following the setting in [2,3,5,10,10,17,19], the model is trained using a
fraction of the images from cvc-clinicdb and kvasir, and its performance is
evaluated by the remaining images as well as those from cvc-t, cvc-colondb, and
etis. in particular, there are 1450 images in the training set, of which 900 are
from kvasir and 550 from cvc-clinicdb. the test set contains all of the images
from cvc-t, cvc-colondb, and etis, which have 60, 380, and 196 images,
respectively, along with the remaining 100 images from kvasir and the remaining
62 images from cvc-clinicdb.implementations. we utilize pytorch 1.10 to run
experiments on an nvidia rtx3090 gpu. we set an initial learning rate to 1e-4
and halve it after 80 epochs. we train the model for 120 epochs. the same
multi-scale input and gradient clip strategies used in [3,8,10] are employed in
the training phase. the batch size is 16 by default. adamw is selected as the
optimizer with a weight decay of 1e-4. we adopt the same data augmentation
techniques as uacanet [8], including random flip, random rotation, and color
jittering. in evaluation phase, we mainly focus on mdice, miou, the two most
common metrics in medical image segmentation, to evaluate the performance of the
model. referring to [3,8,10], we use a combination loss consisting of weighted
dice loss and weighted iou loss to supervise network optimization. comparison
with state-of-the-art methods. we compared our proposed method with previous
state-of-the-art methods. according to the experimental settings, the results on
cvc-clinicdb and kvasir demonstrate the learning ability of the proposed model,
while the results on cvc-t, cvc-colondb, and etis demonstrate the model's
ability for cross-dataset generalization. the experimental results are listed in
table .1. it can be seen that our model is slightly inferior to polyp-pvt on the
cvc-colondb, but the gap is quite small, e.g., 0.6% in mdice and 0.4% in miou.
on kvasir, we are ahead of the previous best model by 1.1% in mdice and 1.6% in
miou. this shows that our model is second to none in terms of learning ability,
which demonstrates the effectiveness of our model. furthermore, our proposed
method demonstrates strong cross-dataset generalization capability on cvc-t,
cvc-colondb, and etis datasets, with particularly good performance on the latter
two due to their larger and more representative datasets. our model outperforms
state-of-the-art models by 2.9% mdice and 3.2% miou on cvc-colondb and 3.5%
mdice and 4.0% miou on etis. these results validate the effectiveness of
feature-level enhancement and highlight the superior performance of our method.
we also provide visual results in fig. 3, where our predictions are shown to be
closer to the ground truth.ablation study. we carried out ablation experiments
to verify the effectiveness of the proposed fpe, fae, and msa. for our baseline,
we use the simple encoder-decoder structure with skip connections for feature
fusion and perform element-wise summation at the decoder. table 2 presents the
results of our ablation experiments. following the ablation study conducted on
our proposed approach, it is with confidence that we assert the significant
contribution of each module to the overall performance enhancement compared to
the baseline. our results indicate that the impact of each module on the final
performance is considerable, and their combination yields the optimal overall
performance. specifically, across the five datasets, our proposed model improves
the mdice score by at least 1.4% and up to 3.4% on cvc-t, compared to the
baseline. for miou, the improvements are 1.5% and 3.5% on the corresponding
datasets. in summary, our ablation study underscores the crucial role played by
each component of our approach, and establishes its potential as a promising
framework for future research in this domain.
we introduce a new approach to polyp segmentation that addresses inefficient
feature propagation in existing unet-like encoder-decoder networks.
specifically, a feature propagation enhancement module is introduced to
propagate multiscale information over full stages in the encoder, while a
feature aggregation enhancement module is attended at the decoder side to
prevent the loss of high-level semantics during progressive feature fusion.
furthermore, a multiscale aggregation module is used to aggregate multi-scale
features to provide rich information for the decoder. experimental results on
five popular polyp datasets demonstrate the effectiveness and superiority of our
proposed method. specifically, it outperforms the previous cutting-edge approach
by a large margin (3%) on cvc-colondb and etis datasets. to extend our work, our
future direction focuses on exploring more effective approaches to feature
utilization, such that efficient feature integration and propagation can be
achieved even on lightweight networks.
90.2/83.5 93.0/88.4 92.1/87.2 81.5/73.4 80.9/72.9 90.0/83.6 92.7/88.0 92.6/87.8
81.2/73.1 81.6/73.9 90.5/83.9 93.1/88.5 92.8/88.0 83.7/75.9 82.2/74.6
in the uk, approximately 11,500 patients are diagnosed with rectal cancer each
year [19]. a common form of treatment for such patients is neoadjuvant therapy,
including chemotherapy and radiotherapy, which can be given to patients with
locally advanced rectal cancer to shrink the tumour prior to surgery. recent
evidence suggests that 10-20% of patients will have a complete pathological
response to neoadjuvant therapy and can therefore avoid surgery altogether
[2,5]. however, one third of patients do not benefit from radiotherapy treatment
prior to surgery [8], hence it is important to determine how a patient will
respond to radiotherapy with a personalized approach in order to avoid
overtreatment.histology-based digital biomarkers enable the possibility to
predict a patient's response to therapy. the consensus molecular subtypes (cms)
classification system derived from gene expressions [9] has been developed to
provide biological insight into metastatic colorectal cancer. it has been shown
that these four cms classes can be predicted directly from the standard
haematoxylin and eosin (h&e) stained slide images using deep learning [18].
various studies have investigated the link between cms and patient outcomes,
suggesting that patients with tumour classified as cms4, which features stromal
invasion [9] and shows significantly higher stroma content [15], have worse
survival rates compared to the other cms classes [5]. increased stromal content
has independently been shown to be a predictor for increased risk of recurrence
in early rectal cancer [10], and tumour immune infiltrate evaluated with
immunoscore is a useful prognostic marker [3]. the spatial organisation of the
cancerous tissue has been identified as a biomarker for aggressiveness or
recurrence [12], and qi et al. [15] found that the features they developed
representing spatial organisation reflected characteristics of the four cms
classes. interactions between the epithelial tissue (cellular tissue lining) and
other prevalent tissue types in the tumour microenvironment are also indicators
of prognosis [15], since progression of colorectal cancer is dependent on both
the epithelial and stromal tissues [20]. other work has looked at predicting
chemoradiotherapy response in rectal cancer patients from h&e images using
different approaches, but without providing contextual interpretations
[19,22].as opposed to predicting response to radiotherapy alone, we aim to
analyse this prediction in the context of the overall tissue architecture and
the tumour biology as captured by cms. input to our model is a standard h&e
whole slide image (wsi) which is split into smaller patches to overcome the
memory limitations of existing gpus. to achieve our goal we need to capture the
heterogeneity at the slide level, which is why applying full or semi-supervised
approaches on individual tiles followed by a slide aggregation method is not
suitable. instead, we build on recent graph neural network (gnn) approaches that
allow us to model the entire wsi as a graph. as local cell communities form the
nodes of such a graph it can effectively model the micro-anatomy of the tissue.
at the same time it is possible to make predictions at the node-, graph-, and
slide-level. related work. to predict the grading of colorectal cancer (crc),
both cellbased and patch-based graphs have been used in separate works [16,23],
setting the nodes of the graph as either cell nuclei or square patches, defining
the node features as either handcrafted or learned features, and then applying a
gnn for outcome prediction. another patch-based gnn approach to predicting
genetic mutations in crc from h&e slides found their model trained on colon
cancer generalised well to rectal cancer. for other cancers, the slidegraph
pipeline clusters nuclei for the graph nodes, and provides node-level
predictions to make their model more interpretable [13]. other approaches to
setting the graph nodes include using subgraphs to represent regions [14], and
creating superpatches by combining patches [11]. edges between the nodes are
usually defined by a spatial distance metric, which helps model the spatial
organisation of the tissue. common choices for gnns include a graph isomorphism
network (gin) with jumping connectivity [7,13,14], as we use in this
research.our methodology proposes a novel and disease relevant approach to a
more interpretable model that effectively supports a diagnostic task.
pathologists and oncologists can use this information to inspect the validity of
the prediction result and interrogate key aspects of the spatial biology that is
critical for patient management. ultimately, this type of information that is
not available today will help to characterise interactions between the tumour
and the host tissue and therefore help to support choice of therapy. the
developed framework combines self-supervised training of a vision transformer
(vit) to extract morphological features, a superpixel algorithm for determining
nodes of a graph, and a gnn for predictions. we achieve 0.82 auc predicting
complete response to radiotherapy using deep learning on wsis for crc patients,
whilst providing novel interpretability of the results.
in this section we present the patch-level feature extraction, provide the
detail of the superpixel segmentation of the wsi, and illustrate the resulting
graph presentation. a gnn with three branches for our output predictions is used
to simultaneously make the three different predictions as shown in fig.
1.pipeline. for computational reasons, all images are split into patches of size
256 × 256 pixels. in order to have a common feature set all the way up to the
last layer of the gnn, individual patches should be represented by morphological
features that are label-agnostic. this last layer of the gnn then splits into
three branches to predict response to radiotherapy, the cms4 subtype
classification for crc, and epithelial tissue regions. this way we can guarantee
the common latent features and derivation across branches, maintaining the
contextual importance of each branch. the dino framework [4] uses a
self-distillation training approach, using data augmentation to locally crop the
patches and train with a local-global student-teacher approach. we use the dino
framework to train a vit in a self-supervised manner on our h&e slides [6],
representing each patch with 384 features. we use only the training set to train
this model, and use the image patches at 20x magnification. we extract
patch-level features from each wsi using selfsupervised dino training with a vit
model [4]. the slic superpixel algorithm segments the entire slide into smaller
regions [1]. we calculate the mean patch features for these superpixel regions,
and use the superpixel features and centers as our graph nodes, applying
delaunay triangulation to generate the edges of the graph. a gnn consisting of
ginconv layers is trained on these fixed graphs, and the final layer splits into
three separate mlp branches to provide predictions of three different outcomes,
complete response (cr) to radiotherapy (rt), cms4 classification, and epithelial
tissue. an example output is visualized in fig. 2.to find the nodes of the wsi
graphs, we apply the slic superpixel algorithm [1] on the wsis at 5x
magnification to segment the tissue to capture cellular neighbourhoods that are
roughly between 80-100 µm 2 /pixels in size. it can be seen that the superpixel
boundaries consistently align with the boundaries of tissue compartments.the
superpixels centers are used as the nodes of the graph, and the node features
are the weighted mean of the corresponding patch features which overlap with the
superpixel region. the edges of the graph are determined by nearest neighbours
from delaunay triangulation, as in slidegraph [13].building on the ideas
introduced by slidegraph [13] we use ginconv layers [21], adding tempering to
avoid overfitting, and replace their logistic regression scaler with a simple
sigmoid function. we add three branches to the final layer of the gnn, in the
form of three separate multilayer perceptrons (mlps). two of these mlps return a
graph-level prediction, for the response to rt and cms4 predictions, and the
final branch returns node-level predictions, predicting whether each node is
epithelial tissue or not. our loss function is defined aswhere bce is the binary
cross entropy loss, ŷrt ∈ r is the slide-level prediction of response to
radiotherapy, ŷcms4 ∈ r is the slide-level prediction of cms4, ŷepi ∈ r ni are
the node-level predictions of epithelial tissue and n i is the number of nodes
in the i th wsi graph.for each prediction branch, we can visualize the
individual node predictions from the wsi graph, overlaid on the wsi itself, to
get an idea of how the node predictions vary across the different tissue
regions. each graph-level prediction is derived from the corresponding branch
node predictions, by applying pooling and dropout.data. we train and validate
our methods on two retrospective rectal cancer datasets, grampian and aristotle.
both cohorts received standard chemoradiotherapy of pelvic irradiation
(45-50.4gy in 25 fractions over 5 weeks) with capecitabine 900mg/m 2 . the
pre-treatment biopsy slides were all sectioned and stained in the same
laboratory, and scanned at 20x magnification (0.5 µm 2 /pixel) on an aperio
scanner. pathological complete response, which we use as a target outcome here,
was derived from histopathological assessment from posttreatment resections.the
cms labels for this data are derived from three different transcriptomic
versions (single cohort, combined cohort correcting batch effects and combined
cohort including 2036 cases run with the same platform), in order to generate
robust classifications. in all cases the cms call was calculated using the
cmsclassifier random forest and single sample predictor [9]. final cms calls are
based on matching calls between the three transcriptomic versions. despite our
efforts to minimise the noise from rna sequencing, we still expect a certain
level of noise in our ground truth data, which we discuss in the results
section.the epithelial labels for each graph node are calculated from epithelial
masks for each wsi. these epithelial segmentation masks were generated at 10x
magnification (1 µm 2 /pixel) with a u-net [17] which was trained and validated
on 666 full tissue sections belonging to 362 patients from the focus cohort
[18]. the ground truth annotations for the training of this model were generated
by vk.for consistency the tumour regions were marked up by an expert
pathologist. we use these masks in our analysis to filter out background and
irrelevant tissue from the images. grampian and aristotle are used in both
training and validation, with a 70/30% training-validation split, keeping any
wsis from a single patient in the same dataset. we predict complete response to
radiotherapy against all other responses, such as partial response and no
response. the datasets are unbalanced, since in grampian only 61/244 slides have
complete response, and in aristotle only 24/121 slides have complete response.
they are even more unbalanced for cms4, since only 28/244 slides in grampian and
17/121 slides in aristotle are labelled with cms4. we address this imbalance in
the supplementary materials. there are 365 slides total in our dataset, from 249
patients.
implementation. we use the default dino parameters, but train for 20 epochs with
5 warmup epochs. we apply the slic algorithm [1] with compactness of 20, setting
the number of segments for each wsi as half the mean size of the wsi. prior to
fitting the graph model we normalize the node features relative to the whole
dataset. we train our graph model for 30 epochs using adam optimizer with
learning rate 0.001 and weight decay 0.0001. our graph model has three ginconv
layers [21] with dimensions 64, 32 and 16 respectively. we apply dropout of 0.5
in-between graph layers, use minimum aggregation for message passing between
nodes and use maximum pooling for concatenating the node activations. we apply
tempering to the outcome of the graph model, dividing the output by 1.5. we
evaluate the best validation epoch by finding the best mean auc across the three
prediction branches. we run the whole pipeline on four folds with different
random data splits for training and validation. the code for this research will
be made available upon request.table 1. for each fold, we take the mean metrics
for the three branch predictions from the best model on our validation data,
with the best epoch chosen based on mean auc for the three predictions. the
standard deviation of the metrics across the four folds is provided in brackets.
each prediction uses an optimised threshold value determined from the validation
set in order to round the output probabilities to a binary prediction. we use
weighted metrics due to the class imbalance in our dataset.
response results. despite the noise in our reference data used for training, our
model achieves good performance in terms of mean auc scores on all three
prediction branches of our model, predicting complete response to radiotherapy
(rt) with 0.819 auc, cms4 with 0.819 auc and epithelial tissue at the node level
with 0.760 auc across folds. further metrics are provided in table 1. the
prediction performance of the model could be improved by utilising a larger
training dataset and performing more exhaustive parameter searches, however the
current performance of the model is sufficient to demonstrate the impact of this
approach.the predicted response to radiotherapy can now be viewed in the context
of disease biology as captured by cms4. for example, the model demonstrates that
cms4 patients are less likely to respond to radiotherapy. in addition, it is now
possible to view the spatial distribution of cms4 active regions in the tissue
architecture context as shown in fig. 2. additional samples are presented in the
supplementary materials. an example of our proposed prediction maps on two
slides can be seen in fig. 2, with further slides in the supplementary
materials. a pathologist reviewing these maps assesses that the observed
patterns fit the known interplay of response to therapy, cms4 activation, and
the spatial localisation of these signals. in the top slide, we observe high
cms4 activation in stromal rich regions, and interestingly also high cms4
activation in the bottom center, dissociating from the response to rt activation
map. this could be explained by the lymphocyte content, supported by the higher
epithelial map activations in the same location. expert pathologists highlight a
similar pattern in certain regions of the maps for the bottom slide. different
from the slide above, the cms4 and response to rt maps have some overlap with
moderate activations here, encouraging discovery into tumour-host interactions.
ultimately, a pathologist confirmed that these maps support an interpretable and
trustworthy prediction in the context of response to radiotherapy. while we
cannot present a more extensive interpretation of these results due to space
limitations, these examples already indicate that the proposed approach enables
a level of analysis that has not been possible before. we find that predicting
these outcomes individually in a single branch model, particularly with response
to radiotherapy, can result in slightly higher auc scores, but we consciously
make this trade-off in order to provide better interpretability of the model
predictions. the focus of this research is not to achieve the best possible
metrics, but to develop robust methods which can add context and explanation to
clinical black box deep learning model predictions, with the view to ease
clinical translation of such models.to explore the effects of the noisy cms4
ground truth labels, we remove from our dataset any wsis classified as
'unmatched' for the cms call, which for the main results of this paper we
defined as 'not cms4'. removing this data and rerunning our analysis improved
our predictions for cms4 by +0.06 auc, and reduced our response to radiotherapy
and epithelial predictions by -0.02 and -0.01 respectively. the results can be
found in the supplementary materials. these small changes indicate that the
noise in our data does not degrade the performance of our classifier,
reinforcing it as a robust and accurate model.
by setting the prediction of response to therapy in context with disease biology
and spatial organisation of the tissue we are providing a novel approach for
enhancing the interpretablity of complex prediction tasks. these results do not
only enhance the interpretability, they also provide new ways to utilise large
retrospective clinical trial cohorts for which no additional molecular data is
available. extending the amount of training data and improving model training
will improve model performance, which is already impressive.we argue that this
work also advances the state of the art in feature representation and analysis.
our prediction maps derive from the same graph model, and hence they share
underlying graph features. the prediction branches only diverge at the final
stage of translating these graph features into outcome predictions for our three
clinically relevant outcomes. importantly, this level of visualisation is not
only accessible to pathologists, this joint prediction model also enhances the
communication between pathologists and oncologists which is critical for patient
management. by cross-referencing these prediction maps with our prior
understanding of cancer biology, this approach can help to establish trust in
the prediction model and also help to identify potential failure cases.this work
relies on access to well annotated clinical trial samples which will limit our
ability to include more data for training and testing. in future, we plan to use
these methods to help better characterise tumour-stromal interactions of the
tissue. we also plan to use a denser graph with less connectivity to be able to
better predict the heterogeneous epithelial tissue.the aristotle trial was
funded by cancer research uk (cruk/08/032). the funders played no role in the
analyses performed or the results presented. financial support: rw -epsrc center
for doctoral training in health data science (ep/s02428x/1), oxford cruk cancer
centre; vhk -promedica foundation (f-87701-41-01) and swiss national science
foundation (p2skp3_168322/1, p2skp3_168322/2); tsm -s:cort (see above); jr, ks
-oxford nihr national oxford biomedical research centre and the pathlake
consortium (innovateuk). the computational aspects of this research were funded
from the nihr oxford brc with additional support from the wellcome trust core
award grant number 203141/z/16/z. the views expressed are those of the author(s)
and not necessarily those of the nhs, the nihr or the department of health.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9_73.
esophagogastric varices are one of the common manifestations in patients with
liver cirrhosis and portal hypertension and occur in about 50 percent of
patients with liver cirrhosis [3,6]. the occurrence of esophagogastric variceal
bleeding is the most serious adverse event in patients with cirrhosis, with a
6-week acute bleeding mortality rate as high as 15%-20% percent [14]. it is
crucial to identify high-risk patients and offer prophylactic treatment at the
appropriate time. regular endoscopy examinations have been proven an effective
clinical approach to promptly detect esophagogastric varices with a high risk of
bleeding [7]. different from the grading of esophageal varices (ev) that is
relatively complete [1], the bleeding risk grading of gastric varices (gv)
involves complex variables including the diameter, shapes, colors, and
locations. several rating systems have been proposed to describe gv based on the
anatomical area. sarin et al. [16] described and divided gv into 2 groups
according to their locations and extensions. hashizume et al. [10] published a
more detailed examination describing the form, location, and color. although the
existing rating systems tried to identify the risk from different perspectives,
they still lack clear quantification standard and heavily rely on the
endoscopists' subjective judgment. this may cause inconsistency or even
misdiagnosis due to the variant experience of endoscopists in different
hospitals. therefore, we aim to build an automatic gv bleeding risk rating
method that can learn a stable and robust standard from multiple experienced
endoscopists.recent works have proven the effectiveness and superiority of deep
learning (dl) technologies in handling esophagogastroduodenoscopy (egd) tasks,
such as the detection of gastric cancer and neoplasia [4]. it is even
demonstrated that ai can detect neoplasia in barrett's esophagus at a higher
accuracy than endoscopists [8]. intuitively we may regard the gv bleeding risk
rating as an image classification task and apply typical classification
architectures (e.g., resnet [12]) or state-of-the-art gastric lesion
classification methods to it. however, they may raise poor performance due to
the large intra-class variation between gv with the same bleeding risk and small
inter-class variation between gv and normal tissue or gv with different bleeding
risks. first, the gv area may look like regular stomach rugae as it is caused by
the blood vessels bulging and crumpling up the stomach (see fig. 1). also, since
the gv images are taken from different distances and angles, the number of
pixels of the gv area may not reflect its actual size. consequently, the model
may fail to focus on the important gv areas for prediction as shown in fig. 3.
to encourage the model to learn more robust representations, we constructively
introduce segmentation into the classification framework. with the segmentation
information, we further propose a region-constraint module (rcm) and a
cross-region attention module (cram) for better feature localization and
utilization. specifically, in rcm, we utilize the segmentation results to
constrain the cam heatmaps of the feature maps extracted by the classification
backbone, avoiding the model making predictions based on incorrect areas. in
cram, the varices features are extracted using the segmentation results and
combined with an attention mechanism to learn the intra-class correlation and
cross-region correlation between the target area and the context.to learn from
experienced endoscopists, gv datasets with bleeding risks annotation is needed.
while most works and public datasets focus on colonoscopy [13,15] and esophagus
[5,9], with a lack of study on gastroscopy images. in the public dataset of
endocv challenge [2], the majority are colonoscopies while only few are
gastroscopy images. in this work, we collect a gv bleeding risks rating dataset
(gvbleed) that contains 1678 gastroscopy images from 411 patients with different
levels of gv bleeding risks. three senior clinical endoscopists are invited to
grade the bleeding risk of the retrospective data in three levels and annotated
the corresponding segmentation masks of gv areas.in sum, the contributions of
this paper are: 1) a novel gv bleeding risk rating framework that constructively
introduces segmentation to enhance the robustness of representation learning; 2)
a region-constraint module for better feature localization and a cross-region
attention module to learn the correlation of target gv with its context; 3) a gv
bleeding risk rating dataset (gvbleed) with high-quality annotation from
multiple experienced endoscopists. baseline methods have been evaluated on the
newly collected gvbleed dataset. experimental results demonstrate the
effectiveness of our proposed framework and modules, where we improve the
accuracy by nearly 5% compared to the baseline model.
the architecture of the proposed framework is depicted in fig. 2, which consists
of a segmentation module (sm), a region constraint module (rcm), and a
crossregion attention module (cram). given a gastroscopy image, the sm is first
applied to generate the varices mask of the image. then, the image together with
the mask are fed into the cram to extract the cross-region attentive feature
map, and a class activation map (cam) is calculated to represent the
concentrated regions through rcm. finally, a simple classifier is used to
predict the bleeding risk using the extracted feature map.
due to the large intra-class variation between gv with the same bleeding risk
and small inter-class variation between gv and normal tissue or gv with
different bleeding risks, existing classification models exhibit poor perform
and tend to lose focus on the gv areas. to solve this issue, we first embed a
segmentation network into the classification framework. the predict the varices
mask is then used to assist the gv feature to obtain the final bleeding risk
rate. specifically, we use swinunet [11] as the segmentation network,
considering its great performance, and calculate the diceloss between the
segmentaion result m p and ground truth mask of vaices region m gt for
optimizing the network:where is a smooth constant equals to 10 -5 .a
straightforward strategy to utilize the segmentation mask is directly using it
as an input of the classification model, such as concatenating the image with
the mask as the input. although such strategy can improve the classification
performance, it may still lose focus in some hard cases where the gv area can
hardly be distinguished. to further regularize the attention and fully utilize
the context information around the gv area, on top of the segmentation framework
we proposed the cross-region attention module and the region-constraint module.
inspired by the self-attention mechanism [17,18], we propose a cross-region
attention module (cram) to learn the correlation of context information. the
cram consists of an image encoder f im , a varices local encoder f vl and a
varices global encoder f ve . given the image i and the predicted varices mask m
p , a feature extraction step is first performed to generate the image feature v
m , the local varices feature v vl and global varices feature v vg :then,
through similarity measuring, we can compute the attention withwhich composes of
two correlations: self-attention over varices regions and crossregion attention
between varices and background regions. finally, the output feature is
calculated as:where γ is a learnable parameter. then the cross-region attentive
feature v is fed into a classifier to predict the bleeding risk.
to improve the focus ability of the model, we propose the region constraint
module (rcm) to add a constraint on the class activation map (cam) of the
classification model. specifically, we use the feature map after the last
convolutional layer to calculate the cam [19], which computes the weighted sum
of feature maps from the convolutional layer using the weights of the fc
layer.after getting the cam, we regularize cam by calculating the dice loss
between the cam and ground truth mask of varices region l co .
in our framework, we use the cross entropy loss as the classification loss:
where p is the prediction of the classifier and y is the ground-truth label. and
the total loss of our framework can be summarized as:where n is the total number
of samples, ω s , ω co and ω cl are weights of the three losses,
respectively.the training process of the proposed network consists of three
steps: 1) the segmentation network is trained first; 2) the ground-truth
segmentation masks and images are used as the inputs of the cram, the
classification network, including cram and rcm, are jointly trained; 3) the
whole framework is jointly fine-tuned.
data collection and annotation. the gvbleed dataset contains 1678 endoscopic
images with gastric varices from 527 cases. all of these cases are collected
from 411 patients in a grade-iii class-a hospital during the period from 2017 to
2022. in the current version, images from patients with ages elder than 18 are
retained 1 . the images are selected from the raw endoscopic videos and frames.
to maximize the variations, non-consecutive frames with larger angle differences
are selected. to ensure the quality of our dataset, senior endoscopists are
invited to remove duplicates, blurs, active bleeding, chromoendoscopy, and nbi
pictures.criterion of gv bleeding risk level rating. based on the clinical
experience in practice, the gv bleeding risks in our dataset are rated into
three levels, i.e., mild, moderate, and severe. the detailed rating standard is
as follows: 1) mild: low risk of bleeding, and regular follow-up is sufficient
(usually with a diameter less than or equal to 5 mm). 2) moderate: moderate risk
of bleeding, and endoscopic treatment is necessary, with relatively low
endoscopic treatment difficulty (usually with a diameter between 5 mm and 10
mm). 3) severe: high risk of bleeding and endoscopic treatment is necessary,
with high endoscopic treatment difficulty. the varices are thicker (usually with
a diameter greater than 10 mm) or less than 10mm but with positive red signs.
note that the diameter is only one reference for the final risk rating since the
gv is with 1 please refer to the supplementary material for more detailed
information about our dataset. various 3d shapes and locations. the other facts
are more subjectively evaluated based on the experience of endoscopists. to
ensure the accuracy of our annotation, three senior endoscopists with more than
10 years of clinical experience are invited to jointly label each sample in our
dataset. if three endoscopists have inconsistent ratings for a sample, the final
decision is judged by voting. a sample is selected and labeled with a specific
bleeding risk level only when two or more endoscopists reach a consensus on it.
the gvbleed dataset is partitioned into training and testing sets for
evaluation, where the training set contains 1337 images and the testing set has
341 images. the detailed statistics of the three levels of gv bleeding risk in
each set are shown in table 1. the dataset is planned to be released in the
future.
in experiments, the weights ω s , ω co , and ω cl of the segmentation loss,
region constraint loss, and classification loss are set to 0.2, 1, and 1,
respectively. the details of the three-step training are as follows: 1)
segmentation module: we trained the segmentation network for 600 epochs, using
adam as the optimizer, and the learning rate is initialized as 1e-3 and drops to
1e-4 after 300 epochs. 2) cross-region attention module and region constraint
module: we used the ground-truth varices masks and images as the inputs of the
cram, and jointly trained the cram and rcm for 100 epochs. adam is used as the
optimizer, the learning rate is set to 1e-3; 3) jointly fine-tuning: the whole
framework is jointly fine-tuned for 100 epochs with adam as optimizer and the
learning rate set to 1e-3. in addition, common data augmentation techniques such
as rotation and flipping were adopted here.
table 2 reports the quantitative results of different models and fig. 3 shows
the cam visualizations. we tested several baseline models, including both simple
cnn models and state-of-the-art transformer-based models. however, the
transformer-based models achieves much worse performances since they always
require more training data, which is not available in our task. thus, we
selected the simple cnn models as baselines since they achieve better
performances. as shown in the figure, the baseline model cannot focus on the
varices regions due to the complexity of gv structures with large intra-class
variations and small inter-class variations. by introducing the segmentation of
gv into the framework, concatenating the image with its segmentation mask as the
inputs of the classifier can improve the classification accuracy by 1.2%. and
the focus ability of the classifier is stronger than the baseline model. with
the help of cram, the performance of the model can be further improved. although
the model can extract more important context information at the varices regions,
the performance improvement is not very large since the focus ability is not the
best and the model may still make predictions based on the incorrect regions for
some hard images. by adding the rcm to the cram, the focus ability of the model
can be further improved, and thus the model has a significant improvement in
performance by 5% compared to the baseline model, this proves the effectiveness
of our proposed modules. note that, the baseline model tends to predict the
images as severe, thus the f1-score of severe is high but the f1-scores of mild
and moderate are significantly lower than other models. more quantitative and
visualization results are shown in supplementary material. in addition, given
the input image with resolution 512 × 512, the parameters and computational cost
of our framework are 40.2m, and 52.4g macs, and 29 ms inference time for a
single image on gpu rtx2080. for comparison, a single resnet152 model has 60.19m
parameters with 60.62 g macs.
in this paper, we propose a novel bleeding risk rating framework for gastric
varices. due to the large intra-class variation between gv with the same
bleeding risk and small inter-class variation between gv and normal tissue or gv
with different bleeding risks, existing classification models cannot correctly
focus on the varices regions and always raise poor performance. to solve this
issue, we constructively introduce segmentation to enhance the robustness of
representation learning. besides, we further design a region-constraint module
for better feature localization and a cross-region attention module to learn the
correlation of target gv with its context. in addition, we collected the gvbleed
dataset with high-quality annotation of three-level of gv bleeding risks. the
experiments on our dataset demonstrated the effectiveness and superiority of our
framework.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9_1.
bias in medicine has demonstrated a notable challenge for providing
comprehensive and equitable care. implicit biases can negatively affect patient
care, particularly for marginalized populations with lower socioeconomic status
[30]. evidence has demonstrated that implicit biases in healthcare providers
could contribute to exacerbating these healthcare inequalities and create a more
unfair system for people of lower socioeconomic status [30]. based on the data
with racial bias, the unfairness presents in developing evaluative algorithms.
in an algorithm used to predict healthcare costs, black patients who received
the same health risk scores as white patients were consistently sicker [21].
using biased data for ai models reinforces racial inequities, worsening
disparities among minorities in healthcare decision-making [22].within the
radiology arm of ai research, there have been significant advances in
diagnostics and decision making [19]. along these advancements, bias in
healthcare and ai are exposing poignant gaps in the field's understanding of
model implementation and their utility [25,26]. ai model quality relies on input
data and addressing bias is a crucial research area. systemic bias poses a
greater threat to ai model's applications, as these biases can be baked right
into the model's decision process [22].pulmonary embolism (pe) is an example of
health disparities related to race. black patients exhibit a 50% higher
age-standardized pe fatality rate and a twofold risk for pe hospitalization than
white patients [18,24]. hospitalized black patients with pe were younger than
whites. in terms of pe severity, blacks received fewer surgical interventions
for intermediate pe but more for highseverity pe [24]. racial disparities exist
in pe and demonstrate the inequities that affect black patients. the pulmonary
embolism severity index (pesi) is a well-validated clinical tool based on 11
clinical variables and used for outcome prediction measurement [2]. survival
analysis is often used in pe to assess how survival is affected by different
variables, using a statistical method like kaplan-meier method and cox
proportional-hazards regression model [7,12,14].however, one issue with
traditional survival analysis is bias from single modal data that gets
compounded when curating multimodal datasets, as different combinations of modes
and datasets create with a unified structure. multimodal data sets are useful
for fair ai model development as the bias complementary from different sources
can make de-biased decisions and assessments. in that process, the biases of
each individual data set will get pooled together, creating a multimodal data
set that inherits multiple biases, such as racial bias [1,15,23]. in addition,
it has been found that creating multimodal datasets without any debiasing
techniques does not improve performance significantly and does increase bias and
reduce fairness [5]. overall, a holistic approach to model development would be
beneficial in reducing bias aggregation in multimodal datasets. in recent years,
disentangled representation learning (drl) [4] for bias disentanglement improves
model generalization for fairness [3,6,27].we developed a pe outcome model that
predicted mortality and detected bias in the output. we then implemented methods
to remove racial bias in our dataset and model and output unbiased pe outcomes
as a result. our contributions are as follows: (1) we identified bias diversity
in multimodal information using a survival prediction fusion framework. (2) we
proposed a de-biased survival prediction framework with demographic bias
disentanglement. (3) the multimodal cph learning models improve fairness with
unbiased features.
this section describes the detail of how we identify the varying degrees of bias
in multimodal information and illustrates bias using the relative difference in
survival outcomes. we will first introduce our pulmonary embolism multimodal
datasets, including survival and race labels. then, we evaluate the baseline
survival learning framework without de-biasing in the various racial
groups.dataset. the pulmonary embolism dataset used in this study from 918
patients (163 deceased, median age 64 years, range 13-99 years, 52% female),
including 3978 ctpa images and 918 clinical reports, which were identified via
retrospective review across three institutions. the clinical reports from
physicians that provided crucial information are anonymized and divided into
four parts: medical history, clinical diagnosis, observations and radiologist's
opinion. for each patient, the race labels, survival time-to-event labels and
pesi variables are collected from clinical data, and the 11 pesi variables are
used to calculate the pesi scores, which include age, sex, comorbid illnesses
(cancer, heart failure, chronic lung disease), pulse, systolic blood pressure,
respiratory rate, temperature, altered mental status, and arterial oxygen
saturation at the time of diagnosis [2].diverse bias of multimodal survival
prediction model. we designed a deep survival prediction (sp) baseline framework
for multimodal data as shown in fig. 1, which compares the impact of different
population distributions. the frameworks without de-basing are evaluated for
risk prediction in the test set by performing survival prediction on ctpa
images, clinical reports, and clinical variables, respectively. first, we use
two large-scale data-trained models as backbones to respectively extract
features from preprocessed images and cleaned clinical reports. a
state-of-the-art pe detecting model, penet [11] is used as the backbone model
for analyzing imaging risk and extracting information from multiple slices of
volumetric ctpa scans to locate the pe. the feature with the highest pe
probability from a patient's multiple ctpas is considered as the most pe-related
visual representation. next, the gatortron [29] model is employed to recognize
clinical concepts and identify medical relations for getting accurate patient
information from pe clinical reports. the extracted features from the backbones
and pesi variables are represented as f m , m ∈ [img, text, var]. the survival
prediction baseline framework, built upon the backbones, consists of three
multi-layer perceptron (mlp) modules named imaging-based, text-based and
variable-based sp modules. to encode survival features z m sur from image, text
and pesi variables, these modules are trained to distinguish critical disease
from non-critical disease with cox partial log-likelihood loss (coxphloss) [13].
the framework also consists of a cox proportional hazard (coxph) model [7] that
is trained to predict patient ranking using a multimodal combination of risk
predictions from the above three sp modules. these coxph models calculate the
corresponding time-to-event evaluation and predict the fusion of patients' risk
as the survival outcome. we evaluate the performance of each module with
concordance probability (c-index), which measures the accuracy of prediction in
terms of ranking the order of survival times [8]. for reference, the c-index of
pesi scores is additionally provided for comparative analysis.in table 1
(baseline), we computed the c-index between the predicted risk of each model and
time-to-event labels. when debiasing is not performed, significant differences
exist among the different modalities, with the image modality exhibiting the
most pronounced deviation, followed by text and pesi variables. the biased
performance of the imaging-based module is likely caused by the richness of
redundant information in images, which includes implicit features such as body
structure and posture that reflect the distribution of different races. this
redundancy leads to model overfitting on race, compromising the fairness of risk
prediction across different races. besides, clinical data in the form of text
reports and pesi variables objectively reflect the patient's physiological
information and the physician's diagnosis, exhibiting smaller race biases in
correlation with survival across different races. moreover, the multimodal
fusion strategy is found to be effective, yielding more relevant survival
outcomes than the clinical gold standard pesi scores.
based on our sp baseline framework and multimodal findings from sect. 2, we
present a feature-level de-biased sp module that enhances fairness in survival
outcomes by decoupling race attributes, as shown in the lower right of fig. 1.
in the de-biased sp module, firstly, two separate encoders e m i and e m c are
formulated to embed features f m into disentangled latent vectors for
race-intrinsic attributes z id or race-conflicting attributes z sur implied
survival information [16]. then, the linear classifiers c m i and c m c
constructed to predict the race label y id with concatenated vector z = [z id ;
z sur ]. to disentangle survival features from the race identification, we use
the generalized cross-entropy (gce) loss [31] to train e m c and c m c to
overfit to race label while training e m i and c m i with crossentropy (ce)
loss. the relative difficulty scores w as defined in eq. 1 reweight and enhance
the learning of the race-intrinsic attributes [20]. the objective function for
disentanglement shown in eq. 2, but the parameters of id or survival branch are
only updated by their respective losses:to promote race-intrinsic learning in e
m i and c m i , we apply diversify with latent vectors swapping. the randomly
permuted zsur in each mini-batch concatenate with z id to obtain z sw = [z id ;
zsur ]. the two neural networks are trained to predict y id or ỹid with ce loss
or gce loss. as the random combination are generated from different samples, the
swapping decreases the correlation of these feature vectors, thereby enhancing
the race-intrinsic attributes. the loss functions of swapping augmentation added
to train two neural networks is defined as:the survival prediction head c m sur
predicts the risk on the survival feature z sur . coxph loss function [13],
which optimizes the cox partial likelihood, is used to maximize concordance
differentiable and update model weights of the survival branch. thus, coxph loss
and overall loss function are formulated as:where y t and y e are survival
labels including the survival time and the event, respectively. the weights λ sw
and λ sur are assigned as 0.5 and 0.8, respectively, to balance the feature
disentanglement and survival prediction.
we validate the proposed de-biased survival prediction frameworks on the
collected multi-modality pe data. the data from 3 institutions are randomly
split the lung region of cpta images is extracted with a slice thickness of 1.25
mm and scaled to n × 512 × 512 pixels [10]. hounsfield units (hu) of all slices
are clipped to the range of [-1000, 900] and applied with zero-centered
normalization. the penet-based imaging backbone consists of a 77-layer 3d
convolutional neural network and linear regression layers. it takes in a sliding
window of 24 slices at a time, resulting in a window-level prediction that
represents the probability of pe for the current slices [11]. the penet is
pre-trained on large-scale ctpa studies and shows excellent pe detection
performance with an auroc of 0.85 on our entire dataset. the 2048 dimensional
features from the last convolution with the highest probability of pe, are
designated as the imaging features.the gatortron [29] uses a transformer-based
architecture to extract features from the clinical text, which was pre-trained
on over 82 billion words of de-identified clinical text. we used the huggingface
library [28] to deploy the 345m-parameter cased model as the clinical report
feature extractor. the outputs from each patient's medical history, clinical
diagnosis, observations, and radiologist impression are separately generated and
concatenated to form the 1024 × 4 features.we build the encoders of the baseline
sp modules and de-biased sp modules with multi-layer perceptron (mlp) neural
networks and relu activation. the mlps with 3 hidden layers are used to encode
image and text features, and another mlps with 2 layers encodes the features of
pesi variables. a fully connected layer with sigmoid activation acts as a risk
classifier c m sur (z m sur ) for survival prediction, where z m sur is the
feature encoded from single modal data. for training the biased and de-biased sp
modules, we collect data from one modality as a batch with synchronized batch
normalization. the sp modules are optimized using the adamw [17] optimizer with
a momentum of 0.9, a weight decay of 0.0005, and a learning rate of 0.001. we
apply early stopping when validation loss doesn't decrease for 600 epochs.
experiments are conducted on an nvidia gv100 gpu.
table 1 shows the quantitative comparisons of the baseline and de-biased
frameworks with the c-indexes of the multimodal survival predictions. in
general, our framework including de-biased sp modules shows significantly better
predictions in testing set than the pesi-based outcome estimation with c-indexes
of 0.669, 0.654, 0.697, 0.043 for the overall testset, white testset, color
testset and race bias. the de-biased results outperform the baseline in overall
survival c-index and show a lower race bias, especially in imaging-and
fusion-based predictions. the results indicate the effectiveness of the proposed
de-biasing in mitigating race inequity. the results also prove the observations
for the different biases present in different modalities, especially in the ctpa
images containing more abundant race-related information. it also explains the
limited effectiveness of de-biasing the clinical results, which contain less
racial identification. the pre- diction performance based on multiply modalities
is significantly better than the pesi-based outcome estimation. the disentangled
representations, transformed from latent space to a 2d plane via tsne and
color-coded by race [9], are shown in fig. 2. we observe the disentanglement in
the visualization of the id features z id , while the survival features z sur
eliminate the race bias. the lack of apparent race bias observed in both the
original features and those encoded in the baseline can be attributed to the
subordinate role that id features play in the multimodal information. the
kaplan-meier (k-m) survival curve [14], as shown in fig. 3, is used to compare
the survival prediction between high-risk and lowrisk patient groups. the
p-values in the hypothesis test were found to be less than 0.001, which is
considered statistically significant difference. in addition, the predictions of
the de-biased framework show favorable performance, and our multimodal fusion
demonstrates a more pronounced discriminative ability in the k-m survival
analysis compared to the single-modal results.we conducted ablation studies to
examine the effect of the two key components, including swapping feature
augmentation and race-balance resampling. as shown in table 2, the different
training settings show significant differences in survival prediction
performance across modalities. the swapping augmentation provides a strong bias
correction effect for image data with obvious bias. for clinical data, the
resampling generally improves performance in most cases. overall, multimodal
fusion approaches are effective in all training settings, and the coxph model
can actively learn the optimal combination of multimodal features to predict
survival outcomes.
in this work, we developed a de-biased survival prediction framework based on
the race-disentangled representation. the proposed de-biased sp framework, based
on the sota pe detection backbone and large-scale clinical language model, can
predict the pe outcome with a higher survival correlation ahead of the clinical
evaluation index. we detected indications of racial bias in our dataset and
conducted an analysis of the multimodal diversity. experimental results
illustrate that our approach is effective for eliminating racial bias while
resulting in an overall improved model performance. the proposed technique is
clinically relevant as it can address the pervasive presence of racial bias in
healthcare systems and offer a solution for minimizing or eliminating bias
without pausing to evaluate their affection for the models and tools. our study
is significant as it highlights and evaluates the negative impact of racial bias
on deep learning models. the proposed de-biased method has already shown the
capacity to relieve them, which is vital when serving patients with an accurate
analysis. the research in our paper demonstrates and proves that eliminating
racial biases from data improves performance, and yields a more precise and
robust survival prediction tool. in the future, these de-biased sp modules can
be plugged into other models, offering a fairer method to predict survival
outcomes.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9_50.
breast cancer impacts women globally [15] and mammographic screening for women
over a certain age has been shown to reduce mortality [7,10,23]. however,
studies suggest that mammography alone has limited sensitivity [22]. to mitigate
this, supplemental screening like mri or a tailored screening interval have been
explored to add to the screening protocol [1,13]. however, these imaging
techniques are expensive and add additional burdens for the patient. recently,
several studies [8,32,33] revealed the potential of artificial intelligence (ai)
to develop a better risk assessment model to identify women who may benefit from
supplemental screening or a personalized screening interval and these may lead
to improved screening outcomes.in clinical practice, breast density and
traditional statistical methods for predicting breast cancer risks such as the
gail [14] and the tyrer-cuzick models [27] have been used to estimate an
individual's risk of developing breast cancer. however, these models do not
perform well enough to be utilized in practical screening settings [3] and
require the collection of data that is not always available. recently, deep
neural network based models that predict a patient's risk score directly from
mammograms have shown promising results [3,8,9,20,33]. these models do not
require additional patient information and have been shown to outperform
traditional statistical models.when prior mammograms are available, radiologists
compare prior exams to the current mammogram to aid in the detection of breast
cancer. several studies have shown that utilizing past mammograms can improve
the classification performance of radiologists in the classification of benign
and malignant masses [11,25,26,29], especially for the detection of subtle
abnormalities [25]. more recently, deep learning models trained on both prior
and current mammograms have shown improved performance in breast cancer
classification tasks [24]. integrating prior mammograms into deep learning
models for breast cancer risk prediction can provide a more comprehensive
evaluation of a patient's breast health.in this paper, we introduce a deep
neural network that makes use of prior mammograms, to assess a patient's risk of
developing breast cancer, dubbed prime+ (prior mammogram enabled risk
prediction). we hypothesize that mammographic parenchymal pattern changes
between current and prior allow the model to better assess a patient's risk. our
method is based on a transformer model that uses attention [30], similar to how
radiologists would compare current and prior mammograms.the method is trained
and evaluated on a large and diverse dataset of over 9,000 patients and shown to
outperform a model based on state-of-the art risk prediction techniques for
mammography [33]. although previous models such as lrp-net and radifusion [5,34]
have utilized prior mammograms, prime+ sets itself apart by employing an
attention mechanism to extract information about the prior scan.
survival analysis is done to predict whether events will occur sometime in the
future. the data comprises three main elements: features x, time of the event t,
and the occurrence of the event e [18]. for medical applications, x typically
represents patient information like age, family history, genetic makeup, and
diagnostic test results (e.g., a mammogram). if the event has not yet occurred
by the end of the study or observation period, the data is referred to as
right-censored (fig. 1).we typically want to estimate the hazard function h(t),
which measures the rate at which patients experience the event of interest at
time t, given that they have survived up to that point. the hazard function can
be expressed as the limit of the conditional probability of an event t occurring
within a small time interval [t, t + δt), given that the event has not yet
occurred by time t: the cumulative hazard function h(t) is another commonly used
function in survival analysis, which gives the accumulated probability of
experiencing the event of interest up to time t. this function is obtained by
integrating the hazard function over time from 0 to t: h(t) = t 0 h(s)ds.
we build on the current state-of-the art mirai [33] architecture, which is
trained to predict the cumulative hazard function. we use an imagenet pretrained
resnet-34 [12] as the image feature backbone. the backbone network extracts
features from the mammograms, and the fully connected layer produces the final
feature vector x. we make use of two additional fully connected layers to
calculate base hazard θ b and time-dependent hazard θ u , respectively. the
predicted cumulative hazard is obtained by adding the base hazard and
time-dependent hazard, according to:when dealing with right-censored data, we
use an indicator function δ i (t) to determine whether the information for
sample i at time t should be included in the loss calculation or not. this helps
us exclude unknown periods and only use the available information. it is defined
as follows:here, e i is a binary variable indicating whether the event of
interest occurs for sample i (i.e., e i = 1) or not (i.e., e i = 0), and c i is
the censoring time for sample i, which is the last known time when the sample
was cancer-free.we define the ground-truth h is a binary vector of length t max
, where t max is the maximum observation period. specifically, h(t) is 1 if the
patient is diagnosed with cancer within t years and 0 otherwise. we use binary
cross entropy to calculate the loss at time t for sample i:. the total loss is
defined as:here, n is the number of exams in the training set. the goal of
training the model is to minimize this loss function, which encourages the model
to make accurate predictions of the risk of developing breast cancer over time.
to improve the performance of the breast cancer risk prediction model, we
incorporate information from prior mammograms taken with the same view, using a
transformer decoder structure [30]. this structure allows the current and prior
mammogram features to interact with each other, similar to how radiologists
check for changes between current and prior mammograms.during training, we
randomly select one prior mammogram, regardless of when they were taken. this
allows the model to generalize to varying time intervals. to pair each current
mammogram during inference with the most relevant prior mammogram, we first
select the prior mammogram taken at the time closest to the current time. this
approach is based on research showing that radiologists often use the closest
prior mammogram to aid in the detection of breast cancer [26].next, a shared
backbone network is used to output the current feature x curr and the prior
feature x prior . these features are then flattened and fed as input to the
transformer decoder, where multi-head attention is used to find information
related to the current feature in the prior feature. the resulting output is
concatenated and passed through a linear layer to produce the current-prior
comparison feature x cp c . the current-prior comparison feature and current
feature are concatenated to produce the final feature x * = x cp c ⊕ x curr ,
which is then used by the base hazard network and time-dependent hazard network
to predict the cumulative hazard function ĥ.
we compiled an in-house mammography dataset comprising 16,113 exams (64,452
images) from 9,113 patients across institutions from the united states, gathered
between 2010 and 2021. each mammogram includes at least one prior mammogram. the
dataset has 3,625 biopsy-proven cancer exams, 5,394 biopsyproven benign exams,
and 7,094 normal exams. mammograms were captured using hologic (72.3%) and
siemens (27.7%) devices. we partitioned the dataset by patient to create
training, validation, and test sets. the validation set contains 800 exams (198
cancer, 210 benign, 392 normal) from 400 patients, and the test set contains
1,200 exams (302 cancer, 290 benign, 608 normal) from 600 patients. all data was
de-identified according to the u.s hhs safe harbor method. therefore, the data
has no phi (protected health information) and irb (institutional review board)
approval is not required.
we make use of uno's c-index [28] and the time-dependent auc [16]. the cindex
measures the performance of a model by evaluating how well it correctly predicts
the relative order of survival times for pairs of individuals in the dataset.
the c-index ranges from 0 to 1, with a value of 0.5 indicating random
predictions and a value of 1 indicating that the model is perfect.
time-dependent roc analysis generates an roc curve and the area under the curve
(auc) for each specific time point in the follow-up period, enabling evaluation
of the model's performance over time. to compare the c-index of two models, we
employ the comparec [17] test, and make use of the delong test [6] to compare
the timedependent auc values. confidence bounds are generated using
bootstrapping with 1,000 bootstrap samples.we evaluate the effectiveness of
prime+ by comparing it with two other models: (1) baseline based on mirai, a
state-of-the art risk prediction method from [33], and (2) prime, a model that
uses prior images by simply summing x curr and x prior without the use of the
transformer decoder.
our model is implemented in pytorch and trained on four v100 gpus. we trained
the model using stochastic gradient descent (sgd) for 20k iterations with a
learning rate of 0.005, weight decay of 0.0001, and momentum of 0.9. we use a
cosine annealing learning rate scheduling strategy [21].we resize the images to
960 × 640 pixels and use a batch size of 96. to augment the training data, we
apply geometric transformations such as vertical flipping, rotation and
photometric transformations such as brightness/contrast adjustment, gaussian
noise, sharpen, clahe, and solarize. empirically, we find that strong
photometric augmentations improved the risk prediction model's performance,
while strong geometric transformations had a negative impact. this is consistent
with prior work [20] showing that risk prediction models focus on overall
parenchymal pattern.
ablation study. to better understand the merit of the transformer decoder, we
first performed an ablation study on the architecture. our findings, summarized
in table 1, include two sets of results: one for all exams in the test set and
the other by excluding cancer exams within 180 days of cancer diagnosis which
are likely to have visible symptoms of cancer, by following a previous study
[33]. this latter set of results is particularly relevant as risk prediction
aims to predict unseen risks beyond visible cancer patterns. we also compare our
method to two other models, the state-of-the-art baseline and prime models.as
shown in the top rows in table 1, the baseline obtained a c-index of 0.68 (0.65
to 0.71). by using the transformer decoder to jointly model prior images, we
observed improved c-index from 0.70 (0.67 to 0.73) to 0.73 (0.70 to 0.76). the
c-index as well as all auc differences between the baseline and the prime+ are
all statistically significant (p < 0.05) except the 4-year auc where we had a
limited number of test cases.we observe similar performance improvements when
evaluating cases with at least 180 days to cancer diagnosis. interestingly, the
c-index as well as timedependent aucs of all three methods decreased compared to
when evaluating using all cases. the intuition behind this result is that
mammograms taken near the cancer diagnosis (<180 days) likely contain visible
signs of cancer and thus the task of risk prediction is easier. the model must
learn patterns of risk, not lastly, we empirically confirm that a transformer
decoder effectively models spatial relations between prior and current
mammograms by demonstrating consistent performance improvements of prime+ across
both short-term and longterm risk prediction settings. our results suggest that
incorporating changes in patients using prior mammograms and a transformer
decoder improves the performance of breast cancer risk prediction
models.analysis based on density. to better understand why adding prior images
improves performance, we divided our test set into subgroups to examine the
performance of the baseline model and the prime+ model on each of these groups.
mammographic breast density is one of the most important risk factor to predict
breast cancer [19,31]. women with dense breasts have a four-to six-fold higher
risk of breast cancer [2]. the addition of mammographic breast density has
improved the performance traditional breast cancer risk models [4] and can
therefore help us understand why the addition of prior images works.mammographic
breast density was determined using the breast imaging reporting and data system
(bi-rads) composition classification. bi-rads category a, b are defined as fatty
breasts and bi-rads category c, d are classified as dense breasts. to determine
the density category, we employed an internally developed density prediction
model, as most exams lack bi-rads ground truth. this model achieved an accuracy
of 0.81 on the internal density validation set.we categorized the exams into two
groups based on changes in density: "change" and "no change". density change was
defined according to whether the bi-rads category changed in the current image
as compared to the prior image. as shown in table 2, the baseline model performs
poorly for "change", with a c-index of 0.63 (0.49 to 0.77), especially for
long-term risk prediction, with 3-year auc of 0.56 (0.40 to 0.72). this suggests
that the baseline model has limitations in accurately predicting long-term risk
when there is a density change from the prior exam. however, prime+ is able to
predict long-term risk accurately even when a density change has occurred
(3-year auc = 0.74 (0.60 to 0.88)), by learning to refer previous exams
properly. this demonstrates the potential usefulness of incorporating past
mammogram information into breast cancer risk prediction models. thus, we
believe that incorporating prior exams is important to identify changes in
texture which are important for long term risk prediction (table 3). lastly, we
divided the exams based on the level of breast density, with a fatty group
consisting of density a and b, and a dense group consisting of density c and d.
both the baseline and prime+ performs better in fatty group than dense group. we
suspect this is because deep neural networks generally work better on low
density images given that visual cues of cancer in images with lower breast
density are more clearly visible.
in this paper, we introduce a novel breast cancer risk prediction method,
prime+, which incorporates prior mammograms with a transformer decoder to
capture changes in breast tissue over time. by doing so, we achieve high
performance for both short-term and long-term risk prediction. our extensive
experiments on a dataset of 16,113 exams show that prime+ outperformed a model
based on the state-of-the-art for breast cancer risk prediction [33]. our method
performed particularly well in cases where there was a change in breast density
from the previous exam. we believe that our method has the potential to improve
breast cancer risk prediction and ultimately contribute to earlier detection of
the disease.
lung cancer screening has a significant impact on the rate of mortality
associated with lung cancer. studies have proven that regular lung cancer
screening with low-dose computed tomography (ldct) can lessen the rate of lung
cancer mortality by up to 20% [1,14]. as most (e.g., 95% [13]) of the detected
nodules are benign, it is critical to accurately assess their malignancy on ct
to achieve a timely diagnosis of malignant nodules and avoid unnecessary
procedures such as biopsy for benign ones. particularly, the evaluation of
nodule (i.e., 8-30mm) malignancy is recommended in the guidelines [13].fig. 1.
in pare, a nodule is diagnosed from two levels: first parsing the contextual
information contained in the nodule itself, and then recalling the previously
learned nodules to look for related clues.one of the major challenges of lung
nodule malignancy prediction is the quality of datasets [6]. it is characterized
by a lack of standard-oftruth of labels for malignancy [16,27], and due to this
limitation, many studies use radiologists' subjective judgment on ct as labels,
such as lidc-idri [3]. recent works have focused on collecting pathologically
labeled data to develop reliable malignancy prediction models [16,17,19]. for
example, shao et al. [16] collated a pathological gold standard dataset of 990
ct scans. another issue is most of the studies focus on ldct for malignancy
prediction [10]. however, the majority of lung nodules are incidentally detected
by routine imaging other than ldct [4,15], such as noncontrast chest ct (ncct,
the most frequently performed ct exam, nearly 40% [18]).technically, current
studies on lung nodule malignancy prediction mainly focus on deep learning-based
techniques [10,12,17,23,24]. liao et al. [10] trained a 3d region proposal
network to detect suspicious nodules and then selected the top five to predict
the probability of lung cancer for the whole ct scan, instead of each nodule. to
achieve the nodule-level prediction, xie et al. [24] introduced a
knowledge-based collaborative model that hierarchically ensembles multi-view
predictions at the decision level for each nodule. liu et al. [12] extracted
both nodules' and contextual features and fused them for malignancy prediction.
shi et al. [17] effectively improved the malignancy prediction accuracy by using
a transfer learning and semi-supervised strategy. despite their advantages in
representation learning, these methods do not take into account expert
diagnostic knowledge and experience, which may lead to a bad consequence of poor
generalization. we believe a robust algorithm should be closely related to the
diagnosis experience of professionals, working like a radiologist rather than a
black box.in this paper, we suggest mimicking radiologists' diagnostic
procedures from intra-context parsing and inter-nodule recalling (see
illustrations in fig. 1),
at the intra-level, the contextual information of the nodules provides clues
about their shape, size, and surroundings, and the integration of this
information can facilitate a more reliable diagnosis of whether they are benign
or malignant. motivated by this, we first segment the context structure, i.e.,
nodule and its surroundings, and then aggregate the context information to the
nodule representation via the attention-based dependency modeling, allowing for
a more comprehensive understanding of the nodule itself. at the inter-level, we
hypothesize that the diagnosis process does not have to rely solely on the
current nodule itself, but can also find clues from past learned cases. this is
similar to how radiologists rely on their accumulated experience in clinical
practice. thus, the model is expected to have the ability to store and recall
knowledge, i.e., the knowledge learned can be recorded in time and then recalled
as a reference for comparative analysis. to achieve this, we condense the
learned nodule knowledge in the form of prototypes, and recall them to explore
potential inter-level clues as an additional discriminant criterion for the new
case. to fulfill both ldct and ncct screening needs, we curate a large-scale
lung nodule dataset with pathology-or follow-up-confirmed benign/malignant
labels. for the ldct, we annotate more than 12,852 nodules from 8,271 patients
from the nlst dataset [14]. for the ncct, we annotate over 4,029 nodules from
over 2,565 patients from our collaborating hospital. experimental results on
several datasets demonstrate that our method achieves outstanding performance on
both ldct and ncct screening scenarios.our contributions are summarized as
follows: (1) we propose context parsing to extract and aggregate rich contextual
information for each nodule. (2) we condense the diagnostic knowledge from the
learned nodules into the prototypes and use them as a reference to assist in
diagnosing new nodules. (3) we curate the largest-scale lung nodule dataset with
high-quality benign/malignant labels to fulfill both ldct and ncct screening
needs. (4) our method achieves advanced malignancy prediction performance in
both screening scenarios (0.931 auc), and exhibits strong generalization in
external validation, setting a new state of the art on lungx (0.801 auc).
figure 2 illustrates the overall architecture of pare, which consists of three
stages: context segmentation, intra context parsing, and inter prototype
recalling. we now delve into different stages in detail in the following
subsections.
the nodule context information has an important effect on the benign and
malignant diagnosis. for example, a nodule associated with vessel feeding is
more likely to be malignant than a solitary one [22]. therefore, we use a u-like
network (unet) to parse the semantic mask m for the input image patch x, thus
allowing subsequent context modeling of both the nodule and its surrounding
structures. specifically, each voxel of m belongs to {0 : background, 1 : lung,
2 : nodule, 3 : vessel, 4 : trachea}. this segmentation process allows pare to
gather comprehensive context information that is crucial for an accurate
diagnosis. for the diagnosis purpose, we extract the global feature from the
bottleneck of unet as the nodule embedding q, which will be used in later
diagnostic stages.
in this stage, we attempt to enhance the discriminative representations of
nodules by aggregating contextual information produced by the segmentation
model. specifically, the context mask is tokenized into a set of sequences via
the overlapped patch embedding. the input image is also split into patches and
then embedded into the context tokens to keep the original image information.
besides, positional encoding is added in a learnable manner to retain location
information. similar to the class token in vit [7], we prepend the nodule
embedding token to the context sequences, denoted by [q; t 1 , ..., t g ] ∈ r
(g+1)×d . here g is the number of context tokens, and d represents the embedding
dimension. then we perform the self-attention modeling on these tokens
simultaneously, called self context attention (sca), to aggregate context
information into the nodule embedding. the nodule embedding token at the output
of the last sca block serves as the updated nodule representation. we believe
that explicitly modeling the dependency between nodule embedding and its
contextual structure can lead to the evolution of more discriminative
representations, thereby improving discrimination between benign and malignant
nodules.
definition of the prototype: to retain previously acquired knowledge, a more
efficient approach is needed instead of storing all learned nodules in memory,
which leads to a waste of storage and computing resources. to simplify this
process, we suggest condensing these pertinent nodules into a form of
prototypes. as for a group of nodules, we cluster them into n groups {c 1 , ...,
c n } by minimizing the objective function n i=1 p∈ci d(p, p i ) where d is the
euclidean distance function and p represents the nodule embedding, and refer the
center of each cluster, p i = 1 |ci| p∈ci p, as its prototype. considering the
differences between benign and malignant nodules, we deliberately divide the
prototypes into benign and malignant groups, denoted by p b ∈ r n/2×d and p m ∈
r n/2×d . cross prototype attention: in addition to parsing intra context, we
also encourage the model to capture inter-level dependencies between nodules and
external prototypes. this enables pare to explore relevant identification basis
beyond individual nodules. to accomplish this, we develop a cross-prototype
attention (cpa) module that utilizes nodule embedding as the query and the
prototypes as the key and value. it allows the nodule embedding to selectively
attend to the most relevant parts of prototype sequences. the state of query at
the output of the last cpa module servers as the final nodule representation to
predict its malignancy label, either "benign" (y = 0) or "malignant" (y = 1).
the prototypes are updated in an online manner, thereby allowing them to adjust
quickly to changes in the nodule representations. as for the nodule embedding q
of the data (x, y), its nearest prototype is singled out and then updated by the
following momentum rules,where λ is the momentum factor, set to 0.95 by default.
the momentum updating can help accelerate the convergence and improve the
generalization ability.
the algorithm 1 outlines the training process of our pare model which is based
on two objectives: segmentation and classification. the dice and crossentropy
loss are combined for segmentation, while cross-entropy loss is used for
classification. additionally, deep classification supervision is utilized to
enhance the representation of nodule embedding in shallow layers like the output
of the unet and sca modules.
data collection and curation: nlst is the first large-scale ldct dataset for
low-dose ct lung cancer screening purpose [14]. there are 8,271 patients
enrolled in this study. an experienced radiologist chose the last ct scan of
each for l = 1, ..., l do 12:cross prototype attention 13:end for 15:
update prototype according to eq. 1
j ← seg loss(m, s) + 3 i=1 cls loss(y, p i ) update loss18: end for patient, and
localized and labeled the nodules in the scan as benign or malignant based on
the rough candidate nodule location and whether the patient develops lung cancer
provided by nlst metadata. the nodules with a diameter smaller than 4mm were
excluded. the in-house cohort was retrospectively collected from 2,565 patients
at our collaborating hospital between 2019 and 2022. unlike nlst, this dataset
is noncontrast chest ct, which is used for routine clinical care. segmentation
annotation: we provide the segmentation mask for our in-house data, but not for
the nlst data considering its high cost of pixel-level labeling. the nodule mask
of each in-house data was manually annotated with the assistance of ct labeler
[20] by our radiologists, while other contextual masks such as lung, vessel, and
trachea were generated using the totalsegmentator [21].
the training set contains 9,910 (9,413 benign and 497 malignant) nodules from
6,366 patients at nlst, and 2,592 (843 benign and 1,749 malignant) nodules from
2,113 patients at the in-house cohort. the validation set contains 1,499 (1,426
benign and 73 malignant) nodules from 964 patients at nlst. the nlst test set
has 1,443 (1,370 benign and 73 malignant) nodules from 941 patients. the
in-house test set has 1,437 (1,298 benign and 139 malignant) nodules from 452
patients. we additionally evaluate our method on the lungx [2] challenge
dataset, which is usually used for external validation in previous work
[6,11,24]. lungx contains 83 (42 benign and 41 malignant) nodules, part of which
(13 scans) were contrast-enhanced. segmentation: we also evaluate the
segmentation performance of our method on the public nodule segmentation dataset
lidc-idri [3], which has 2,630 nodules with nodule segmentation mask. evaluation
metrics: the area under the receiver operating characteristic curve (auc) is
used to evaluate the malignancy prediction performance.implementation: all
experiments in this work were implemented based on the nnunet framework [8],
with the input size of 32 × 48 × 48, batch size of 64, and total training
iterations of 10k. in the context patch embedding, each patch token is generated
from a window of 8 × 8 × 8. the hyper-parameters of pare are empirically set
based on the ablation experiments on the validation set. for example, the
transformer layer is set to 4 in both sca and cpa modules, and the number of
prototypes is fixed to 40 by default. more details can be found in the ablation.
due to the lack of manual annotation of nodule masks for the nlst dataset, we
can only optimize the segmentation task using our in-house dataset, which has
manual nodule masks.
ablation study: in table 1, we investigate the impact of different
configurations on the performance of pare on the validation set, including
transformer layers, number of prototypes, embedding dimension, and deep
supervision. we observe that a higher auc score can be obtained by increasing
the number of transformer layers, increasing the number of prototypes, doubling
the channel dimension of token embeddings, or using deep classification
supervision. based on the highest auc score of 0.931, we empirically set l=4,
n=40, d=256, and ds=true in the following experiments. in table 2, we
investigate the ablation study of different methods/modules on the validation
set and observe the following results: (1) the pure segmentation method performs
better than the pure classification method, primarily because it enables greater
supervision at the pixel level, (2) the joint segmentation and classification is
superior to any single method, indicating the complementary effect of both
tasks, (3) both context parsing and prototype comparing contribute to improved
performance on the strong baseline, demonstrating the effectiveness of both
modules, and (4) segmenting more contextual structures such as vessels, lungs,
and trachea provide a slight improvement, compared to solely segmenting nodules.
external evaluation on lungx: we used lungx [2] as an external test to evaluate
the generalization of pare. it is worth noting that these compared methods have
never been trained on lungx. table 4 shows that our pare model achieves the
highest auc of 0.801, which is 2% higher than the best method dar [11]. we also
conducted a reader study to compare pare with two experienced radiologists, who
have 8 and 13 years of lung nodule diagnosis experience respectively. results in
fig. 3 reveal that our method achieves performance comparable to that of
radiologists.
our model is trained on a mix of ldct and ncct datasets, which can perform
robustly across low-dose and regular-dose applications. we compare the
generalization performance of the models obtained under three training data
configurations (ldct, ncct, and a combination of them). we find that the models
trained on either ldct or ncct dataset alone cannot generalize well to other
modalities, with at least a 6% auc drop. however, our mixed training approach
performs best on both ldct and ncct with almost no performance degradation.
method auc nlnl [9] 0.683 d2cnn [26] 0.746 kbc [24] 0.768 dar [11] 0.781 pare
(ours) 0.801 fig. 3. reader study compared with ai.
in this work, we propose the pare model to mimic the radiologists' diagnostic
procedures for accurate lung nodule malignancy prediction. concretely, we
achieve this purpose by parsing the contextual information from the nodule
itself and recalling the previous diagnostic knowledge to explore related benign
or malignancy clues. besides, we curate a large-scale pathological-confirmed
dataset with up to 13,000 nodules to fulfill the needs of both ldct and ncct
screening scenarios. with the support of a high-quality dataset, our pare
achieves outstanding malignancy prediction performance in both scenarios and
demonstrates a strong generalization ability on the external validation.
screening mammography helps detect breast cancer earlier and has reduced the
breast cancer mortality rate significantly [4]. computer-aided diagnosis (cad)
software was developed to aid radiologists, but its effectiveness has been
questioned following recent large-scale clinical studies [6]. in particular, the
high [9]) to few negative cases (ddsm [8], inbreast [17]). to illustrate the
distribution shift, we train four popular dense detectors using a standard setup
that includes only annotated malignant and benign cases [1,13,16]. we utilize
optimam [7], a large dataset with a significant proportion of negatives (table
1), for training and evaluation. across all dense models, there is a large
performance drop in the clinically representative setting that includes negative
images. this means that the dense models are producing too many fps on negative
images. our model, m&m, successfully tackles this performance gap.rate of false
positive (fp) predictions of cad can cause a significant reduction in
radiologists' specificity [6]. surprisingly, recent deep learning literature
[3,5,13,16,20,21,32] focuses on improving recall without considering the need to
operate at low fp rates. as shown in fig. 1a, most works focus on reporting
recalls outside the clinically relevant region of less than 1 fp/image. to
tackle the high rate of false positives in mammography, we identify three
challenges: (1) a malignant mammogram typically contains only one malignant
finding. this is different from natural images: for example, an image in coco
contains on average 7.7 objects [11]. this calls into question the usage of
dense detectors for mammography; (2) a standard screening exam consists of two
views per breast. both views are essential in making a clinical decision because
a finding may appear suspicious in one view but not the other; (3) most
mammograms are negative: they do not contain any findings. however, excluding
negative images from training and evaluation leads to a distribution shift since
negative images are abundant in clinical practice. concretely, the false
positive rate is low for a typical evaluation data distribution but much higher
for a clinicallyrepresentative data distribution, as shown in fig. 1b.in this
work, we tackle these challenges and propose a multi-view and multiinstance
learning system, m&m. m&m is an end-to-end system that detects malignant
findings and provides breast-level classification. to achieve these goals, m&m
leverages three components: (1) sparse r-cnn to replace dense anchors with a set
of sparse proposals; (2) multi-view cross-attention to synthesize information
from two views and iteratively refine the predictions, and (3) multiinstance
learning (mil) to include negative images during training. ultimately, each
component contributes to our goal of reducing false positives.we validate m&m
through evaluation on five datasets: two in-house datasets, two public datasets
-ddsm [8] and cbis-ddsm [9], and optimam [7]. we perform ablation studies to
verify the contribution of each component of m&m. to summarize, our
contributions are:1. we show that sparsity of proposals is beneficial to the
analysis of mammograms, which have low disease prevalence (sec. with mil, m&m
improves the recall at 0.1 fp/image by 12.6% (fig. 4). furthermore, m&m can
provide breast-level classification predictions, achieving aucs of more than
0.88 on four different datasets (table 3).
the sparsity of malignant findings calls into question the use of dense
detectors.as shown in fig. 1b, dense detectors generalize poorly to negative
images as they produce too many false positives. thus, we propose to use sparse
r-cnn [24]. sparse r-cnn utilizes a sparse set of n learnable proposals
consisting of b 0 ∈ r n ×4 coordinates and h 0 ∈ r n ×d features. the
architecture uses 6 cascading heads to iteratively refine the proposals. within
the i th head, the proposals h i-1 first interact with themselves via
self-attention, and then generate dynamicconv (fig. 4, [24]) to interact with
roi features cropped by b i-1 . the resulting outputs h i ∈ r n ×d are features
for the (i+1) th head. in addition, a regression module is applied to h i to
generate boxes b i ∈ r n ×4 , and a classification module generates scores p i ∈
r n ×c , with c being the number of classes.we modify sparse r-cnn to include
dual classification modules (fig. 2). first, an objectness module produces
objectness logits o i ∈ r n to distinguish all findings -malignant and benign
-from the background. by utilizing all findings, the objectness head increases
the training sample size [1,13,16], but also increases fps because it flags
benign findings. to mitigate this side effect, we include a dedicated malignancy
module [w i , b i ] to generate malignancy logits m i ∈ r n that is trained to
distinguish malignant from benign findings:(1) the strictly positive function
softplus(x) = log(1 + e x ) is chosen to enforce consistency: a high objectness
logit o i is required to generate a high malignancy logit m i . thus, at the
finding level, we obtain the following losswhere l giou and l l1 are regression
losses as in sparse r-cnn. l objectness and l malignancy are focal losses
applied to the predicted objectness o i and the predicted malignancy m i across
all cascading heads 1 ≤ i ≤ 6, respectively.
a standard screening exam includes two standard views of each breast. the
craniocaudal (cc) view is taken from the top down, while the mediolateral
oblique (mlo) view is captured from the side at an oblique angle. radiologists
examine both views when making a clinical decision as a finding may look
innocuous in one view but suspicious in the other.to enable multi-view
reasoning, m&m incorporates a cross-attention module [28] into every cascading
head. recall that within the i th cascading head, selfattention is first applied
to proposal features h i-1 to reason about the relations between objects. after
this self-attention module, we introduce a cross-attention module (fig. 2,
appendix algo. 1) to reason about the relations between cc view feature h cc i-1
and mlo view feature h mlo i-1 :the enhanced embeddings hcc i-1 , hmlo i-1 then
generate dynamicconv to interact with roi features and produce new features h cc
i , h mlo i for the (i + 1) th head. thus, with the proposed cross-attention
module, the cc view's proposal features are refined iteratively using the mlo
view's proposal features and vice versa.
mammogram annotation is costly to obtain due to a dependency on radiologists.
this high cost means that bounding boxes are often unavailable. further, most
mammograms are negative: they do not contain any findings. yet, a model
generalizes poorly if these negative images are dropped during training (fig.
1b).since image-and breast-level labels are available, we adopt an mil module to
include images without bounding boxes during training. to compute imageand
breast-level scores, we leverage the proposal malignancy logits m i (eq. ( 1)).
since an image is malignant if it contains a malignant lesion, we obtain
imagelevel scores by applying the noisyor function fnext, as cc and mlo views
offer complimentary information on a breast, we obtain breast-level malignancy
score by averaging the image-level scores across these views.we apply
cross-entropy losses l image and l breast at the image and breast level for all
training samples. the lesion loss l lesion (eq. ( 2)) is only applied for
annotated lesions. we thus obtain the following total training loss for m&m: l =
1 annotated lesion l lesion + 0.5l image + 0.5l breast .(5)
implementation details. we use pytorch 1.10. the training settings follow sparse
r-cnn [24]. we apply random horizontal flipping and random rotation. we resize
the images' shorter edges to 2560 with the larger edges no longer than 3328. we
utilize a coco-pretrained pvt-b2-li backbone [30]. we use adamw optimizer with 5
× 10 -5 learning rate and 0.0001 weight decay. the model is trained for 9000
iterations, and the learning rate is scaled by 0.1 at the 6750 and 8250
iterations. each batch contains 16 breasts (32 images). we employ a 1:1 sampling
ratio between unannotated and annotated images.datasets. we utilize three 2d
digital mammography datasets: (1) optimam : a development dataset derived from
the optimam database [7], which is funded by cancer research uk. we split the
data into train/val/test with an 80:10:10 ratio at the patient level; (2)
inhouse-a: an evaluation dataset collected from a u.s. multi-site mammography
operator; (3) inhouse-b : an evaluation dataset collected from a u.s. academic
hospital (see [18], sec. 2.2 for more details on the inhouse datasets). we also
utilize two film mammography datasets: (4) ddsm: a dataset maintained at the
university of south florida [8]. we followed the methods by [3,5,13,16] to split
the test set; (5) cbis-ddsm: a curated subset of ddsm [9]. we only include
breasts that have one cc view and one mlo view. dataset statistics are reported
in table 1.metrics. we report average precision with intersection over union
from 0.25 to 0.75. ap mb denotes average precision on the set of annotated
malignant and benign images. ap denotes average precision when all data is
included. we report free response operating characteristic (froc) curves and
recalls at various fp/image (r@t). following [3,5,16,29], a proposal is
considered true positive if its center lies within the ground truth box. for
classification, we report the area under the receiver operating characteristic
curve (auc).detection results. gmic [23] 0.911 0.896 0.814 0.815 0.796 hct [25]
0.923 0.912 0.816 0.817 0.793 m&m (ours) 0.960 0.942 0.920 0.910 0.898resnet50
[14] 0.724 shared resnet [31] 0.735 phresnet50 [14] 0.739 cross-view transformer
[27] 0.803 * m&m (ours) 0.88323 points (pt) between excluding and including
negative images. large δ means the models are producing too many fps on negative
images. sparse r-cnn [24] generalizes significantly better with a gap of 17pt.
this shows the importance of sparsity for reducing fp. by adding both multi-view
and mil, m&m successfully reduces the δ gap to 3.5pt. with this performance gap
closed, m&m is able to achieve a high recall of 87.7% at just 0.1 fp/image.
figure 1a compares m&m with recent literature evaluated on ddsm. m&m adopts the
same ddsm splits used by [3,12,13,16,33], while [5,21,32] use other splits. m&m
(87% r@0.5) outperforms all recent sota with the same test split, including 2022
sota [33] (83% r@0.5), by at least 4%.classification results. table 3a reports
m&m's breast-level and exam-level classification results on optimam and the two
inhouse datasets. we use gmic [23] and hct [25] as baselines since they are
open-sourced classifiers developed for mammography. all three models were
trained only on optimam. for all models, the breast-level score is the average
of the cc score and mlo score, while the exam-level score is the max of the left
breast score and right breast score. both baseline models suffer large
generalization drops of approximately 3b compares m&m with recent literature
reporting on the public cbis-ddsm dataset. in particular, m&m outperforms the
cross-view transformer [27] and phresnet50 [14] by 0.08 and 0.14 breast auc,
respectively. qualitative evaluation. figure 3 presents a qualitative evaluation
of the multi-view module. with multi-view, m&m produces a tighter box on the cc
view and recovers a missed finding on the mlo view. ablation studies. figure 4
presents ablation results using the optimam validation split. on the left, we
demonstrate how each component of m&m contributes to closing the gap δ between
evaluating with and without negative images. notably, without using any extra
training samples, multi-view reasoning reduces δ to only -5.9pt (row 3). mil
allows the model to train with significantly more negative images, reducing δ to
-3.6pt (row 4). on the right of fig. 4, the froc curves show how each component
of m&m improves recall significantly at low fp/image. in particular, m&m's
recall at 0.1fp/image is 86.3%, +21.2% over vanilla sparse r-cnn.further
studies. in the appendix, we present more qualitative evaluation as well as
further ablation studies on (1) number of learnable proposals, (2) different mil
schemes, (3) backbone choices and (4) positional encoding.
we present m&m, an end-to-end model leveraging multi-view reasoning and
multi-instance learning for mammography detection and classification.as a
detector, m&m offers significant improvement in recall at low fp/image (fig. 1a,
table 2). this success comes from three points of advancement. first, unlike
previous works that do not consider the impact of sparsity [13,16,33], we show
that sparsity of proposals is beneficial for false positive reduction (table
2).second, m&m incorporates multi-view reasoning through iterative application
of cross-attention and proposal refinement in the cascading heads. m&m's
multiview module is effective (fig. 4) yet simple, requiring neither positional
encoding [13,16,32] nor extra proposal correspondence annotations [33]. finally,
our mil formulation allows for training with representative data distribution in
an endto-end one stage pipeline. this is more advantageous than previous
pipelines that require additional stages or classifiers to reduce false
positives [15,22,29].as a classifier, m&m establishes strong performance on
several datasets (table 3). m&m offers two advantages over image classifiers:
(1) image classifiers are often pre-trained as patch classifiers with patches
cropped from bounding box annotations [14,23,25]. in comparison, m&m utilizes
these bounding boxes to learn localization and can be trained directly in a
single stage from coco/imagenet weights; (2) image classifiers offer limited
explainability, while m&m's breast-level prediction is more interpretable
through its localization ability.
presents quantitative detection evaluation on optimam. all dense
detectors[2,10,19,26] suffer a large δ gap of more than
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9_75.
the early detection of lesions in medical images is critical for the diagnosis
and treatment of various conditions, including neurological disorders. stroke is
a leading cause of death and disability, where early detection and treatment can
significantly improve patient outcomes. however, the quantification of lesion
burden is challenging and can be time-consuming and subjective when performed
manually by medical professionals [14]. while supervised learning methods
[10,11] have proven to be effective in lesion segmentation, they rely heavily on
large fig. 1. overview of phanes (see fig. 2). our method can use both expert
annotatedor unsupervised generated masks to reverse and segment anomalies
annotated datasets for training and tend to generalize poorly beyond the learned
labels [21]. on the other hand, unsupervised methods focus on detecting patterns
that significantly deviate from the norm by training only on normal data.one
widely used category of unsupervised methods is latent restoration methods. they
involve autoencoders (aes) that learn low-dimensional representations of data
and detect anomalies through inaccurate reconstructions of abnormal samples
[17]. however, developing compact and comprehensive representations of the
healthy distribution is challenging [1], as recent studies suggest aes perform
better reconstructions on out-of-distribution (ood) samples than on training
samples [23]. various techniques have been introduced to enhance representation
learning, including discretizing the latent space [15], disentangling
compounding factors [2], and variational autoencoders (vaes) that introduce a
prior into the latent distribution [26,29]. however, methods that can enforce
the reconstruction of healthy generally tend to produce blurry
reconstructions.in contrast, generative adversarial networks (gans) [8,18,24]
are capable of producing high-resolution images. new adversarial aes combine
vaes' latent representations with gans' generative abilities, achieving sota
results in image generation and outlier detection [1,5,6,19]. nevertheless,
latent methods still face difficulties in accurately reconstructing data from
their low-dimensional representations, causing false positive detections on
healthy tissues.several techniques have been proposed that make use of the
inherent spatial information in the data rather than relying on constrained
latent representations [12,25,30]. these methods are often trained on a pretext
task, such as recovering masked input content [30]. de-noising aes [12] are
trained to eliminate synthetic noise patterns, utilizing skip connections to
preserve the spatial information and achieve sota brain tumor segmentation.
however, they heavily rely on a learned noise model and may miss anomalies that
deviate from the noise distribution [1]. more recently, diffusion models [9]
apply a more complex de-noising process to detect anomalies [25]. however, the
choice and granularity of the applied noise is crucial for breaking the
structure of anomalies [25]. adapting the noise distribution to the diversity
and heterogeneity of pathology is inherently difficult, and even if achieved,
the noising process disrupts the structure of both healthy and anomalous regions
throughout the entire image.in related computer vision areas, such as industrial
inspection [3], the topperforming methods do not focus on reversing anomalies,
but rather on detecting them by using large nominal banks [7,20], or pre-trained
features from large natural imaging datasets like imagenet [4,22]. salehi et al.
[22] have employed multi-scale knowledge distillation to detect anomalies in
industrial and medical imaging. however, the application of these networks in
medical anomaly segmentation, particularly in brain mri, is limited by various
challenges specific to the medical imaging domain. they include the variability
and complexity of normal data, subtlety of anomalies, limited size of datasets,
and domain shifts.this work aims to combine the advantages of constrained latent
restoration for understanding healthy data distribution with generative
in-painting networks. unlike previous methods, our approach does not rely on a
learned noise model, but instead creates masks of probable anomalies using
latent restoration. these guide generative in-painting networks to reverse
anomalies, i.e., preserve healthy tissues and produce pseudo-healthy in-painting
in anomalous regions. we believe that our proposed method will open new avenues
for interpretable, fast, and accurate anomaly segmentation and support various
clinical-oriented downstream tasks, such as investigating progression of
disease, patient stratification and treatment planning. in summary our main
contributions are:• we investigate and measure the ability of sota methods to
reverse synthetic anomalies on real brain t1w mri data. • we propose a novel
unsupervised segmentation framework, that we call phanes, that is able to
preserve healthy regions and utilize them to generate pseudo-healthy
reconstructions on anomalous regions. • we demonstrate a significant advancement
in the challenging task of unsupervised ischemic stroke lesion segmentation.
latent restoration methods use neural networks to estimate the parameters θ, φ
of an encoder e θ and a decoder d φ . the aim is to restore the input from its
lower-dimensional latent representation with minimal loss. the standard
objective is to minimize the residual, e.g., using mean squared error (mse)
loss:in the context of variational inference [13], the goal is to optimize the
parameters θ of a latent variable model p θ (x) by maximizing the log-likelihood
of the observed samples x: log p θ (x). the term is intractable, but the true
posterior p θ (z|x) can be approximated by q φ (z|x):(kl is the kullback-leibler
divergence; q φ (z|x) and p θ (x|z) are usually known as the encoder e φ and
decoder d θ ; the prior p(z) is usually the normal distribution n (μ 0 , σ 0 );
and the elbo denotes the evidence lower bound. in unsupervised anomaly
detection, the networks are trained only on normal samples x ∈ x ⊂ r n . given
an anomalous input x / ∈ x , it is assumed that the reconstruction x ph = (d φ
(e θ (x))) ∈ x represents its pseudo-healthy version. the aim of the
pseudo-healthy reconstructions is to accurately reverse abnormalities present in
the input images. this is achieved by preserving the healthy regions while
generating healthy-like tissues in anomalous regions. thus, anomalies can
ideally be directly localized by computing the difference between the anomalous
input and the pseudo-healthy reconstructions: s(x) = |x -x ph |.
figure 2 shows an overview of our proposed method. we introduce an innovative
approach by utilizing masks produced by latent generative networks to condition
generative inpainting networks only on healthy tissues. our framework is
modular, which allows for the flexibility of choosing a preferred generative
network, such as adversarial, or diffusion-based models for predicting the
pseudo-healthy reconstructions. in the following we describe each component in
detail. latent generative network. the first step involves generating masks to
cover potential anomalous regions in the input image. the goal of this step is
to achieve unbiased detection of various pathologies and minimize false
positives. it is therefore important to use a method that is restricted to
in-distribution samples, particularly healthy samples, while also accurately
reconstructing inputs. here, we have adopted our previous work [1] that augments
a soft introspective variational auto-encoder with a reversed embedding
similarity loss with the aim to enforcing more accurate pseudo-healthy
reconstructions. the training process encourages the encoder to distinguish
between real and generated samples by minimizing the kullback-leibler (kl)
divergence of the latent distribution of real samples and the prior, and
maximizing the kl divergence of generated samples. on the other hand, the
decoder is trained to deceive the encoder by reconstructing real data samples
using the standard elbo and minimizing the kl divergence of generated samples
compressed by the encoder:where e l φ is the l-th embedding of the l encoder
layers,, and l sim is the cosine similarity. mask generation. simple residual
errors have a strong dependence on the underlying intensities [16]. as it is
important to assign higher values to (subtle) pathological structures, we
compute anomaly masks as proposed in [1] by applying adaptive histogram
equalization (eq), normalizing with the 95th percentile, and augmenting the
errors with perceptual differences for robustness:with s lpips being the learned
perceptual image patch similarity [28]. finally, we binarize the masks using the
99th percentile value on the healthy validation set.inpainting generative
network. the objective of the refined ph generative network is to complete the
masked image by utilizing the remaining healthy tissues to generate a full ph
version of the input. considering computational efficiency, we have employed the
recent in-painting aot-gan [27]. the method uses a generator (g) and
discriminator neural network to optimize losses based on residual and perceptual
differences, resulting in accurate and visually precise inpainted images.
additionally, the discriminator predicts the input mask from the inpainted image
to improve the synthesis of fine textures. anomaly maps. the ph reconstruction
is computed as follows:) m, with being the pixel-wise multiplication. we compute
the final anomaly maps based on residual and perceptual differences:
datasets. we trained our model using two publicly available brain t1w mri
datasets, including fastmri+ (131 train, 15 val, 30 test) and ixi (581 train
samples), to capture the healthy distribution. performance evaluation was done
on a large stroke t1-weighted mri dataset, atlas v2.0 [14], containing 655
images with manually segmented lesion masks for training and 355 test images
with hidden lesion masks. we evaluated the model using the 655 training images
with public annotations. the mid axial slices were normalized to the 98 th
percentile, padded, and resized to 128 × 128 resolution. during training, we
performed random rotations up to 10 • , translations up to 0.1, scaling from 0.9
to 1.1, and horizontal flips with a 0.5 probability. we trained for 1500 epochs,
with a batch size of 8, lr of 5e -5 , and early stopping.
in this section, we test whether reconstruction-based methods can generate
pseudo-healthy images and reverse synthetic anomalies. results are evaluated in
table 1 and fig. 3 using 30 test images and synthetic masks as reference.vaes
produce blurry results that lead to poor reconstruction of both healthy and
anomalous regions (lpips) and thus poor segmentation performance. while daes
preserve the healthy tissues well with an lpips of 3.94, they do not gen- erate
pseudo-healthy reconstructions in anomalous regions (lpips ≈ 20). however, they
change the intensity of some structures, e.g., hypo-intensities, allowing for
improved detection accuracy (see auprc and dice). simplex noise in [25] is
designed to detect large hypo-intense lesions, leaving small anomalies
undetected by anoddpm. si-vaes and ra produce pseudo healthy versions of the
abnormal inputs, with the latter achieving the best results among the baselines.
our proposed method, phanes, successfully reverses the synthetic anomalies, with
its reconstructions being the most similar to ground truth healthy samples, as
can be seen in fig. 3. it achieved an improvement of 77% and 47% in generating
pseudo-healthy samples in healthy and anomalous regions, respectfully. this
enables the precised localization of anomalies (see bottom row in fig. 3).
in this section, we evaluate the performance of our approach in segmenting
stroke lesions and show the results in table 2 and fig. 4. for completeness, we
compare our approach to teacher-student methods that use multi-scale knowledge
distillation (mkd) for anomaly segmentation. the unsupervised detection of
(subtle) stroke lesions is challenging. the lack of healthy data from the same
scanner and the limited size of the healthy datasets limit the successful
application of such methods, with a maximum achievable dice score of just under
6%. on the other hand, patchcore, which is currently the sota method in
industrial anomaly detection, has demonstrated comparable performance to the
top-performing baselines. vaes yield many false positive detections due to the
blurry reconstructions and achieve poor localization results. daes can identify
anomalies that resemble the learned noise distribution and improve segmentation
results (auprc of 9.22), despite not producing pseudo-healthy reconstructions of
abnormal samples (see subsect. 4.1). the best performing latent restoration
method is ra, achieving a remarkable 79% improvement over si-vae. unlike
experiments in subsect. 4.1, the simplex noise aligns more closely with the
hypointense pathology distribution of stroke in t1w brain mri. as a result,
anod-dpm achieves the highest detection accuracy among the baselines. compared
to anoddpm, phanes increases the detection results by 22% auprc. figure 4 shows
a visual comparison between the two approaches. diffusion models tend to be more
susceptible to domain shifts (first three rows) and yield more false positives.
in contrast, phanes demonstrates more precise localization, especially for
subtle anomalies (bottom rows). generally, unsupervised methods tend to have
lower dice scores partly due to unlabeled artefacts in the dataset. these
include non-pathological (rows 1,2) as well as other pathological effects, such
as changes in ventricle structure (rows 3,4). phanes correctly identifies these
as anomalous, but their lack of annotation limits numerical evaluations.
this paper presents a novel unsupervised anomaly segmentation framework, called
phanes. it possesses the ability to reverse anomalies in medical images by
preserving healthy tissues and substituting anomalous regions with pseudohealthy
reconstructions. by generating pseudo-healthy versions of images containing
anomalies, phanes can be a useful tool in supporting clinical studies. while we
are encouraged by these achievements, we also recognize certain limitations and
areas for improvement. for example, the current binarization of anomaly maps
does not account for the inherent uncertainty in the maps, which we aim to
explore in future research. additionally, our method relies on accurate initial
estimates of the latent restoration and anomaly maps. nevertheless, our proposed
concept is independent of specific approaches and can leverage advancements in
both domains. our method is not optimized for detecting a certain anomaly
distribution but rather demonstrates robustness in handling various small
synthetic anomalies and diverse stroke lesions. we look forward to generalizing
our method to other anatomies and imaging modalities, paving the way for
exciting future research in the field of anomaly detection.in conclusion, we
demonstrated exceptional performance in reversing synthetic anomalies and
segmenting stroke lesions on brain t1w mris. we believe that deliberate masking
of (possible) abnormal regions will pave new ways for novel anomaly segmentation
methods and empower further clinical applications.
breast cancer is one of the high-mortality cancers among women in the 21st
century. every year, 1.2 million women around the world suffer from breast
cancer and about 0.5 million die of it [3]. accurate identification of cancer
types will make a correct assessment of the patient's risk and improve the
chances of survival. however, the traditional analysis method is time-consuming,
as it mainly depends on the experience and skills of the doctors. therefore, it
is essential to develop computer-aided diagnosis (cadx) for assisting doctors to
realize the rapid detection and classification.due to being collected by various
devices, the resolution of histopathological images extracted may not always be
high. low-resolution (lr) images lack of lots of details, which will have an
important impact on doctors' diagnosis. considering the improvement of
histopathological images' acquisition equipment will cost lots of money while
significantly increasing patients' expense of detection. the super-resolution
(sr) algorithms that improve the resolution of lr images at a small cost can be
a practical solution to assist doctors in diagnosis. at present, most single
super-resolution methods only have fixed receptive fields [7,10,11,18]. these
models cannot capture multi-scale features and do not solve the problems caused
by lr in various magnification factors well. mrc-net [6] adopted lstm [9] and
multi-scale refined context to improve the effect of reconstructing
histopathological images. it considered the problem of multi-scale, but only
fused two scales features. this limits its performance in the scenarios with
various magnification factors. therefore, designing an appropriate feature
extraction block for sr of the histopathological images is still a challenging
task.in recent years, a series of deep learning methods have been proposed to
solve the breast cancer histopathological image classification issue by the
highresolution (hr) histopathological images. [12,21,22] improved the specific
model structure to classify breast histopathology images, which showed a
significant improvement in recognition accuracy compared with the previous works
[1,20]. ssca [24] considered the problem of multi-scale feature extraction which
utilized feature pyramid network (fpn) [15] and attention mechanism to extract
discriminative features from complex backgrounds. however, it only concatenates
multi-scale features and does not consider the problem of feature fusion. so it
is still worth to explore the potential of extraction and fusion of multi-scale
features for breast images classification.to tackle the problem of lr breast
cancer histopathological images reconstruction and diagnosis, we propose the
single histopathological image super-resolution classification network
(shisrcnet) integrating super-resolution (sr) and classification (cf) modules.
the main contributions of this paper can be described as follows:(1) in the sr
module, we design a new block called multi-features extraction block (mfeblock)
as the backbone. mfeblock adopts multi-scale receptive fields to obtain
multi-scale features. in order to better fuse multi-scale features, a new fusion
method named multi-scale selective fusion (msf) is used for multi-scale
features. these make mfeblock reconstruct lr images into sr images well.(2) the
cf module completes the task of image classification by utilizing the sr images.
like sr module, it also needs to extract multi-scale features. the difference is
that the cf module can use the method of downsampling to capture multi-scale
features. so we combine the multi-scale receptive fields (sknet) [13] with the
feature pyramid network (fpn) to achieve the feature extraction of this module.
in fpn, we design a cross-scale selective fusion block (csfblock) to fuse
features of different scales.(3) through the joint training of these two
designed modules, the superresolution and classification of low-resolution
histopathological images are integrated into our model. for improving the
performance of cf module and reducing the error caused by the reconstructed sr
images, we introduce hr images to cf module in the training stage. the
experimental results demonstrate that the effects of our method are close to
those of sota methods that take hr breast cancer histopathological images as
inputs.
this section describes the proposed shisrcnet. the overall pipeline of the
proposed network is shown in fig. 1(a). it composes two modules: sr module and
cf module. the sr module reconstructs the lr image into the sr image. the cf
module utilize the reconstructed sr images to diagnose histopathological images.
in the training stage, we introduce hr images to improve the performance of cf
module and alleviate the error caused by sr images.
to better extract and fuse multi-scale features for super-resolution, we propose
a new sr network, called srmfenet. like srresnet [11], srmfenet takes a single
low-resolution image as input and uses the pixelshuffle layer to get the
restructured image. the difference between srmfenet and srresnet is that a
multi-features extraction block (mfeblock) is proposed to extract and fuse
multi-scale histopathological images' features. the structure of the mfeblock is
shown in fig. 1(b). the input features x capture multi-scale features y i
through four 3×3 atrous convolutions [4] with different rates:where n is the
number of atrous convolutions and is set to 4 by the experiments. this design
not only preserves the depth of the network, but also increases the width of the
network. it is beneficial for the network to extract shallow local texture
information and global semantic information. after the feature extraction phase,
a new fusion method named msf fuses all of different scale features y i . in the
end, the input features x are added with the fused features. the details of msf
show in the fig. 1(c). firstly, we conduct global average pooling (gap) [14] on
the multi-scale features to obtain their average channel-wise weights. then
using sigmoid activation function to map weight to 0 to 1. next softmax
operation normalizes the same position of the obtained multi-scale average
channel-wise weights. finally, the features are multiplied by the corresponding
normalized weights and the processed features are added together to generate the
new multi-scale features. mfeblock is very applicable to process
histopathological images of different magnification factors, as it employs
convolution and attention operations to capture local and global image context
information and fuse them well.
the task of the cf module is to classify the reconstructed sr images. it can use
the method of downsampling to capture multi-scale features. so we combine the
multi-scale receptive fields (sknet as backbone network) with the fpn (a
downsampling method) to achieve the feature extraction of this module. in fig.
1(a), the multi-sacle features extracted by sknet are the input of fpn. we
propose a new fusion method, called cross-scale selective fusion block
(csfblock) to effectively fuse high-resolution and low-resolution features in
fpn. after the fused features are processed by gap, they are aggregated into a
new multi-scale feature by concatenate operation. finally, the aggregated
multi-scale features are classified through the fully connected (fc) layer. the
structure of csfblock is shown in fig. 1(d). the inputs of csfblock are two-way
inputs which are the high-resolution features x h ∈ w ×h×c and the
low-resolution features x l ∈ w 1 ×h 1 ×c. in csfblock, the upsampling
operations are performed on the low-resolution features x l to realize
consistency with x h dimension. x h and restructured x l are fused via an
element-wise summation:then, using gap along the channel dimension to get the
global information s. a fc layer generates a compact feature vector z which
guides the feature selection procedure. and z is reconstructed into two weight
vectors a, b of the same dimension as s through two fc layers, which can be
defined as:where δ denotes relu and w a , w b , w c , means the weight of the fc
layers. specifically, a softmax operator is applied on a and b 's channel-wise
digits:the fused feature map f is obtained through the attention weights on
multi-scale features:
the sr module and the cf module exploit different loss functions for training.in
the sr module, l1 loss is used for super-resolution. in the cf module, we
introduce hr images to cf module in the training stage for improving the
performance of cf module and reducing the error caused by the reconstructed sr
images. we use f ocal loss [16] to alleviate the class imbalanced data problem
of the hr and sr images' classification. inspired by the contrastive learning
algorithm simclr [5], the hr and sr of the same images are similar to two
different views. so the nt -xent loss [19] is adopted to calculate similarity
between sr multi-scale features and hr multi-scale features for cf module's
robustness. the total loss function can be expressed as:where λ 1 , λ 2 and λ 3
are the weights of l1 loss, f ocal loss and nt -xent loss, respectively. in the
inference stage, only sr images are taken as inputs by cf module. in our
experiment, λ 1 , λ 2 and λ 3 are set to 0.6, 0.3 and 0.1, respectively. and the
temperature parameter in nt -xent loss is set to 0.5.
dataset: this work uses the breast cancer histopathological image database
(breakhis)1 [20]. the images in the dataset have four magnification factors the
model is trained using the adam optimizer [25] with the learning rate set to
1x10 -3 . the learning rate is multiplied by 0.9 for every two epochs. we use
sknet-26 [13] as the backbone network in the cf module. the total training
epochs are 100.
table 1 shows the results of the super-resolution phase. we adopt peak signal to
noise ratio (psnr) and structural similarity index (ssim) [6] to evaluate the
performance of the sr model. mrc-net and our proposed srmfenet (sr module)
achieves better metrics than the other algorithms. this proves the effectiveness
of multi-scale features extraction. compared with mrc-net, our mfeblocks can
extract and fuse multi-scale features well. and the joint training of srmfenet
and cf module improves the performance of super-resolution. figure 2
demonstrates that our model can recover more details with less blurring. we
compare our introduced cf module with five state-of-the-art breast cancer
histopathological image models and diagnosis network with mrc-net [6], as shown
in table 2. the results illustrate that the cf module reaches the best
fig. 2. qualitative comparison with sr methods on breast cancer
histopathological images x8 and x4.
deep learning for medical imaging has achieved remarkable progress, leading to a
growing body of parameter-tuning strategies [1][2][3]. those approaches are
often designed to address disease-specific problems with limitations in their
generalizability. in parallel, foundation models [4] have surged in computer
vision [5,6] and natural language processing [7,8] with growing model capacity
and data size, opening up perspectives in utilizing foundation models and
large-scale clinical data for diagnostic tasks. however, pure imaging data can
be insufficient to adapt foundation models with large model capacity to the
medical field. given the complex tissue characteristics of pathological whole
slide images (wsi), it is crucial to develop adaptation strategies allowing (1)
training data efficiency, and (2) data fusion flexibility for pathological image
analysis. although foundation models promise a strong generalization ability
[4], there is an inherent domain shift between medical and natural concepts in
both vision and language modalities. pre-trained biomedical language models are
increasingly applied to medical context understanding [9][10][11]. language
models prove to be effective in capturing semantic characteristics with a lower
data acquisition and annotation cost in medical areas [12]. such property is
desired to address the dilemma of medical imaging cohorts, where well-annotated,
high-quality medical imaging cohorts are expensive to collect and curate
compared with text inputs [13]. in addition, vision-language models demonstrate
the importance of joining multi-modal information for learning strong encoders
[5,6,14]. thus, connecting visual representations with text information from
biomedical language models becomes increasingly critical to adapting foundation
models for medical image classification, particularly in the challenging setting
of data deficiency.in this study, we propose cite, a data-efficient adaptation
framework that connects image and text embeddings from foundation models to
perform pathological image classification with limited training samples (see
fig. 1). to enable language comprehension, cite makes use of large language
models pretrained on biomedical text datasets [10,11] with rich and professional
biomedical knowledge. meanwhile, for visual understanding, cite only introduces
a small number of trainable parameters to a pre-trained foundation model, for
example, clip [5] and intern [6], in order to capture domain-specific knowledge
without modifying the backbone parameters. in this framework, we emphasize the
utility of text information to play a substitutive role as traditional
classification heads, guiding the adaptation of the vision encoder. a favorable
contribution of our approach is to retain the completeness of both pre-trained
models, enabling a low-cost adaptation given the large capacity of foundation
models. overall, our contributions are summarized as follows:1. we demonstrate
the usefulness of injecting biomedical text knowledge into foundation model
adaptation for improved pathological image classification.
medical image classification. deep learning for medical image classification has
long relied on training large models from scratch [1,15]. also, fine-tuning or
linear-probing the pre-trained models obtained from natural images [16][17][18]
is reasonable. however, those methods are supported by sufficient high-quality
data expensive to collect and curate [19]. in addition, task-specific models do
not generalize well with different image modalities [2]. to tackle this issue,
we emphasize the adaptation of foundation models in a data-efficient
manner.vision-language pre-training. recent work has made efforts in
pre-training vision-language models. clip [5] collects 400 million image-text
pairs from the internet and trains aligned vision and text encoders from
scratch. lit [20] trains a text encoder aligned with a fixed pre-trained vision
encoder. blip-2 [14] trains a query transformer by bootstrapping from
pre-trained encoders. react [21] fixes both pre-trained encoders and tunes extra
gated self-attention modules. however, those methods establish vision-language
alignment by pre-training on large-scale image-text pairs. instead, we combine
pre-trained unimodal models on downstream tasks and build a multi-modal
classifier with only a few data.model adaptation via prompt tuning. prompt
tuning proves to be an efficient adaptation method for both vision and language
models [22,23]. originating from natural language processing, "prompting" refers
to adding (manual) text instructions to model inputs, whose goal is to help the
pre-trained model better understand the current task. for instance, coop [22]
introduces learnable prompt parameters to the text branch of vision-language
models. vpt [23] demonstrates the effectiveness of prompt tuning with
pre-trained vision encoders. in this study, we adopt prompt tuning for
adaptation because it is lightweight and only modifies the input while keeping
the whole pre-trained model unchanged. however, existing prompt tuning methods
lack expert knowledge and understanding of downstream medical tasks. to address
this challenge, we leverage large language models pre-trained with biomedical
text to inject medical domain knowledge.biomedical language model utilization.
biomedical text mining promises to offer the necessary knowledge base in
medicine [9][10][11]. leveraging language models pre-trained with biomedical
text for medical language tasks is a common application. for instance, alsentzer
et al. [9] pre-train a clinical text model with biobert [10] initialization and
show a significant improvement on five clinical language tasks. however, the
potential of biomedical text information in medical imaging applications has not
been explicitly addressed. in our efforts, we emphasize the importance of
utilizing biomedical language models for adapting foundational vision models
into cancer pathological analysis.
figure 2 depicts an overview of our approach cite for data-efficient
pathological image classification. cite jointly understands the image features
extracted by vision encoders pre-trained with natural imaging, and text insights
encoded in large language models pre-trained with biomedical text (e.g.,
biolinkbert [11] which captures rich text insights spanning across biomedical
papers via citations). we connect text and imaging by a projection and classify
the images by comparing the cosine similarity between image and text embeddings.
importantly, we introduce two low-cost sets of trainable parameters to the
vision encoder in order to adapt the model with the guidance of text
information. they are (1) prompt tokens in the input space to model
task-specific information, and (2) a projection layer in the latent space to
align image and text embeddings. during model adaptation, we freeze the
pre-trained encoders and only tune the introduced parameters, which not only
saves remarkable training data and computational resources but also makes our
approach favorable with various foundation model architectures.
an image i to be classified is processed through a pre-trained vision encoder to
generate the image embedding x v with dimension d v , where v stands for
"vision":for the label information, we encode the class names t c (c ∈ [1, c])
with a pre-trained biomedical language model instead of training a
classification head (see fig. 2(e)). we tokenize and process t c through the
language encoder to generate the text embedding x c l with dimension d l , where
l stands for "language":vision-language models like clip [5] contain both a
vision encoder and a language encoder, which provide well-aligned embeddings in
the same feature space. in this case, prediction ŷ is obtained by applying
softmax on scaled cosine similarities between the image and text embeddings (see
fig. 2(d)):where sim(•, •) refers to cosine similarity and τ is the temperature
parameter.for irrelevant vision and language encoders, we introduce an extra
projection layer to the end of the vision encoder to map the image embeddings to
the same latent space as the text embeddings. we replace x v in eq. ( 3) with x
v :during adaptation, the extra parameters are updated by minimizing the
cross-entropy of the predictions from eq. ( 3) and the ground truth labels.
medical concepts exhibit a great visual distribution shift from natural images,
which becomes impractical for a fixed vision encoder to capture task-specific
information in few-shot scenarios. visual prompt tuning (vpt [23]) is a
lightweight adaptation method that can alleviate such an inherent difference by
only tuning prompt tokens added to the visual inputs of a fixed vision
transformer [24], showing impressive performance especially under data
deficiency. thus, we adopt vpt to adapt the vision encoder in our approach.a
vision transformer first cuts the image into a sequence of n patches and
projects them to patch embeddings e 0 ∈ r n×dv , where d v represents the visual
embedding dimension. a cls token c 0 ∈ r dv is prepended to the embeddings,
together passing through k transformer layers {l k v } k=1,2,...,k . cls
embedding of the last layer output is the image feature x v . following the
setting of shallow vpt, we concatenate the learnable prompt tokens p = [p1 , . .
. , p p ] ∈ r p×dv , where p is the prompt length, with cls token c 0 and patch
embeddings e 0 before they are processed through the first transformer
layer:where [•, •] refers to concatenation along the sequence length dimension,
and z k ∈ r p×dv represents the output embeddings of the k-th transformer layer
at the position of the prompts (see fig. 2(a-c)). the prompt parameters are
updated together with the projection layer introduced in sect. 3.1.
dataset. we adopt the patchgastric [25] dataset, which includes
histopathological image patches extracted from h&e stained whole slide images
(wsi) of stomach adenocarcinoma endoscopic biopsy specimens. there are 262,777
patches of size 300 × 300 extracted from 991 wsis at x20 magnification. the
dataset contains 9 subtypes of gastric adenocarcinoma. we choose 3 major
subtypes including "well differentiated tubular adenocarcinoma", "moderately
differentiated tubular adenocarcinoma", and "poorly differentiated
adenocarcinoma" to form a 3-class grading-like classification task with 179,285
patches from 693 wsis. we randomly split the wsis into train (20%) and
validation (80%) subsets for measuring the model performance. to extend our
evaluation into the real-world setting with insufficient data, we additionally
choose 1, 2, 4, 8, or 16 wsis with the largest numbers of patches from each
class as the training set.the evaluation metric is patient-wise accuracy, where
the prediction of a wsi is obtained by a soft vote over the patches, and
accuracy is averaged class-wise.implementation. we use clip vit-b/16 [5] as the
visual backbone, with input image size 224 × 224, patch size 16 × 16, and
embedding dimension d v = 512. we adopt biolinkbert-large [11] as the biomedical
language model, with embedding dimension d l = 1, 024. to show the extensibility
of our approach, we additionally test on vision encoders including imagenet-21k
vit-b/16 [24,26] and intern vit-b/16 [6], and biomedical language model
biobert-large [10].our implementation is based on clip 1 , huggingface2 and
mmclassification3 .training details. prompt length p is set to 1. we resize the
images to 224×224 to fit the model and follow the original data pipeline in
patchgastric [25]. a class-balanced sampling strategy is adopted by choosing one
image from each class in turn. training is done with 1,000 iterations of
stochastic gradient descent (sgd), and the mini-batch size is 128, requiring
11.6 gb of gpu memory and 11 min on two nvidia geforce rtx 2080 ti gpus. all our
experiment results are averaged on 3 random seeds unless otherwise specified.
cite consistently outperforms all baselines under all data scales. figure 3
shows the classification accuracy on the patchgastric dataset of our approach
compared with baseline methods and related works, including (1) r50-21k:
fine-tune the whole resnet50 [27] backbone pre-trained on imagenet-21k [26].(2)
linear probe: train a classification head while freezing the backbone
encoder.(3) fine-tune: train a classification head together with the backbone
encoder. (4) clam [18]: apply an attention network on image features to predict
pseudo labels and cluster the images. ( 5) zero-shot [5]: classify images to the
nearest text embeddings obtained by class names, without training. (6) few-shot
[28]: cluster image features of the training data and classify images to the
nearest class center. ( 7) vpt [23]: train a classification head together with
visual prompts. note that clip vit-b/16 vision encoder is adopted as the
backbone for ( 2)- (7) table 2. cite fits in with various pre-trained encoders.
we include clip vit-b/16 [5], imagenet-21k vit-b/16 [26] and intern vit-b/16 [6]
visual encoders, combined with clip textual encoder [5], biobert (bb) [10] and
biolinkbert (blb) [11] language models. the highest performance of each visual
encoder is bolded.for each combination, cite consistently outperforms linear and
fine-tune baselines. cite shows model extensibility. we evaluate our approach
with additional backbones and biomedical language models to assess its potential
extensibility. table 2 displays the findings of our approach compared with
linear probe and fine-tune baselines. the results demonstrate that cite is
compatible with a variety of pre-trained models, making it immune to upstream
model modifications. the text information encoded in biomedical language models
allows vision models pre-trained with natural imaging to bridge the domain gap
without taskspecific pre-training on medical imaging. importantly, when using
both the vision and language encoders of clip vit-b/16, our approach still
outperforms the baselines by a remarkable margin (47.7% to 60.1%), demonstrating
the importance of multi-modal information. while clip gains such modality
matching through pre-training, our cite shows an appealing trait that irrelevant
vision and language models can be combined to exhibit similar multi-modal
insights on pathological tasks without a need of joint pre-training.
adapting powerful foundation models into medical imaging constantly faces
data-limited challenges. in this study, we propose cite, a data-efficient and
model-agnostic approach to adapt foundation models for pathological image
classification. our key contribution is to inject meaningful medical domain
knowledge to advance pathological image embedding and classification. by tuning
only a small number of parameters guided by biomedical text information, our
approach effectively learns task-specific information with only limited training
samples, while showing strong compatibility with various foundation models. to
augment the current pipeline, the use of synthetic pathological images is
promising [29]. also, foundation training on multi-modal medical images is of
substantial interest to enhance model robustness under data-limited conditions
[30].
magnetic resonance imaging (mri) has been extensively applied to clinical
diagnosis [16]. compared with computed tomography (ct), a brain mri is more
sensitive for multiple stroke types [3], therefore considered as the gold
standard for stroke diagnosis. nevertheless, the long acquisition time for a
brain mri (20 to 30 min) imposes challenges, especially in cases of acute stroke
where rapid diagnosis is essential and patient movement during this distressing
period of time commonly limits evaluation. as a result, mri acceleration
techniques have been developed to achieve more rapid diagnosis, increasing
resource availability while reducing costs [14,18]. a k-space sub-sampling (ks)
approach serves as a simple mri acceleration solution [20], compared with other
hardware-based acceleration methods. however, the signal loss by ks leads to
blurry reconstructed mr images that are less than ideal for a reliable clinical
diagnosis.artificial intelligence (ai) plays an increasingly important role in
mri-based diagnosis, for both mr image reconstruction and clinical decision
making. deep neural networks (dnn) were trained to reconstruct the mr images
from the sub-sampled k-space [10,13], which provides a better reconstruction
than the inverse fast fourier transform (ifft). nevertheless, detailed
information in the brain may still be lost in the reconstructed mr images due to
the signal sparsity in the k-space. on the other hand, traditional convolutional
neural network (cnn) [15] and the latest vision transformer (vit)-based [6]
predictive models have shown impressive prediction accuracy on stroke diagnosis
tasks, such as slice classification and lesion segmentation [7,11]. however,
these dnns trained on clean images through empirical risk minimization (erm) are
vulnerable to perturbations in the input images [2]. whatever the reconstruction
method used, even the slightest perturbation in accelerated mr images can lead
to a wrong stroke prediction from the ai models. therefore, building robust dnn
models to handle the perturbed mr image input is important for mri
acceleration.in this paper, we introduce a distributionally robust learning
(drl)-based approach [4] into the deep mr image classifier training, in order to
improve the model robustness to the image perturbation resulting from the signal
sparsity in accelerated mri. compared with erm, drl is an optimization method
minimizing the worst-case loss over an ambiguity set, therefore, can tolerate
outliers in the data [5]. we implemented drl to different linear layers in deep
cnn/vit classifiers, and applied a randomized training approach to improve the
training efficiency. our results show that on a real-world dataset, drl can
significantly improve the stroke classification performance of erm and other
baseline defensive training methods, when the signal sparsity and noise in
accelerated mri are generated by the cartesian undersampling (cu) method [20]
and white gaussian noise (wgn). we further show that in highly perturbed mr
images where the erm model and even clinicians cannot give a reliable diagnosis,
our drl model can still correctly recognize stroke, which establishes that our
method can assist accelerated mri diagnosis.
we will use the drl framework under a multi-class classification setting
developed by [4] in a dnn-based stroke diagnosis application. we provide a brief
overview of the drl model. assume that there are k classes, and our goal is to
classify an example with an input feature x ∈ r d to one of the k classes with a
one-hot class label y ∈ {0, 1} k . logistic regression solves this problem by
minimizing the following expected true riskwhereis the coefficient matrix, p *
is the true distribution of the data (x, y), h b (x, y) log 1 e b x -y b x is
the loss function to be minimized, and e p * denotes the expectation under the
distribution p * . since p * is usually unknown, problem (1) cannot be solved
directly. the erm approach tackles this by replacing the expected true risk by a
sample averaged risk. given n realizations of (x, y), erm minimizes the
following empirical riskerm can produce unreliable solutions when the samples
are contaminated by noise or drawn from an outlying distribution. to obtain
robust estimators that can hedge against noise in the training data and
generalize well out-of-sample, [4] proposed the drl framework under the
wasserstein metric. specifically, it minimizes the worst-case expected loss over
a set of probability distributionswhere ω contains a set of probability
distributions that are close to the empirical distribution pn measured by the
wasserstein metric, ω {q ∈ p(z) : w 1 (q, pn ) ≤ }, where z is the set of
possible values for (x, y), p(z) is the space of all probability distributions
supported on z, is a pre-specified radius of the ambiguity set ω, pn is the
empirical distribution that assigns an equal probability 1/n to each observed
sample (x i , y i ), and w 1 (q, pn ) is the order-1 wasserstein distance
between q and pn defined aswhere π is the joint distribution of z 1 (x 1 , y 1 )
and z 2 (x 2 , y 2 ) with marginals q and pn , respectively, and l is a distance
metric on the data space.an equivalent reformulation (4) of (3) was developed by
[4] when, where w is a positive semidefinite weight matrix to account for any
transformation on the input feature x and can be estimated from data using
metric learning (see sec. 2.2) and with m → ∞:where
we apply drl to vit and cnn-based mr image stroke classification models, in
order to enhance their robustness against image perturbations in accelerated
mri. we apply the drl reformulation (4) to the last layer b, and certain
intermediate linear layers in a deep mr image classifier. for a vit model (cf.
fig. 1), we apply drl to the patch projection layer p and the final linear
classification layer b, in order to predict if an mr image slice is normal
(label 0) or depicts a stroke lesion (label 1). to speed up the training
process, during each epoch, we randomly pick one layer l to train while keeping
all other layers frozen. a validation set v is used to tune the hyperparameters
(e.g., regularization coefficients). to account for the non-linear
transformation of the raw image resulted from all layers before l, we solve the
following linear matrix inequality (lmi) problem [4] to estimate a weight matrix
w:where xi is the perturbed version of an mr image slice x i , d the training
set, s {(i, j)|x i , x j ∈ d, y i = y j }, |s| denotes the cardinality of the
set s, φ l is the input to l and φ (t) l is the t-th hidden state (i.e., the
vector representation for each instance in the sequence, output by and fed into
different layers in vit) in the sequence φ l of length t , and c is a fixed
parameter. t = 1 if l refers to the b layer. the intuition of ( 5) is that in
the transformed feature space, distance between the clean and perturbed version
of a slice will be minimized, while slices from different classes (normal and
stroke) are sufficiently far away. for a cnn model, we only applied drl to the
final linear layer b.we chose two approaches to generate the perturbation in
accelerated mri. cartesian undersampling (cu) perturbation [20] keeps only the
central and a few randomly-sampled parts of the k-space; the corresponding
reconstructed mr image only keeps the main structural information in the brain
and introduces misalignments. a smaller central fraction f used in k-space
indicates a larger perturbation. noise might be introduced during the signal
transmission, so we add white gaussian noise (wgn) as another type of
perturbation, where the standard deviation σ is regarded as the perturbation
intensity. to show the strength of drl, in addition to erm, we also apply
brute-force adversarial training (bat) [2] and projected gradient descent (pgd)
[12] as baseline methods. among all of the current defensive training methods
which improve the model robustness against perturbations, bat and pgd are
representative. bat adds noisy samples into the training set, therefore is
simple and widely used. pgd is known to be robust to a wide range of image
perturbations, and is considered a state-of-the-art method.
our dataset included mri brain scans from 226 patients performed at an urban
tertiary referral academic medical center that is a comprehensive stroke center.
clinical scans of adult patients aged 18-89 years with recent (acute or
subacute) strokes were identified between 1/1/2013 and 1/1/2021 for inclusion in
this study via a search of the philips performance bridge. scans meeting this
criteria were downloaded and simultaneously anonymized to preserve patient
anonymity and prevent disclosure of protected health information as part of this
irb exempt study. no patient demographic information was retained for the scans,
as it was considered to represent an unnecessary risk for accidental release of
protected health information. the diffusion weighted images with a gradient of
b=1000 were utilized for the analysis (see the supplement1 for information about
the mri scanner and parameter settings). each mr image contains multiple slices,
and every slice was annotated as normal or stroke by a board-certified
neuroradiologist with a subspecialty certification. annotation of the strokes
was performed on the diffusion weighted images using itk-snap (ver. 3.80) [19],
and all included mri examinations were reviewed by the neuroradiologist during
the annotation process to ensure that the images were of diagnostic quality
without significant motion degradation or other artifacts. to avoid the
dependency among the slices from the same subject, we applied a 2-d acquisition
during the mr imaging, and implemented a slice-level mr image preprocessing.
while the whole dataset includes 4,883 (74.7%) normal slices and 1,650 (25.3%)
stroke slices, we further randomly split them into training/validation/test sets
using the ratio 80%/10%/10%. for the training set, we implemented data
augmentation strategies by rotating or flipping each slice. finally, the
training/validation/test set contains 31,356/653/654 slices, correspondingly. we
implemented drl to both cnn and vit models. for the cnn model, we used a
resnet-18 [9] architecture, while for the vit model, we first pre-trained a
4-layer vit using a self-supervised pre-training method called masked
autoencoder (mae) [8], using the t1/t2-weighted brain mr images in the ixi
dataset [1]. mae pre-training first randomly masks 75% of the image patches in
an mri slice input, and then uses a vit encoder-decoder architecture to
reconstruct the masked mri patches, in order to learn the dependency among
different locations in the brain. after 400 pre-training epochs, an overall
satisfying reconstruction result can be observed in fig. 2.to evaluate the
binary classification performance of different models, we use the area under the
receiver operating characteristic (auroc) curve as our main metric. as our
dataset is unbalanced, we also considered the area under precision-recall curve
(auprc). we ran the experiments 3 times using different random seeds. the
training of our dnns were implemented on 3 nvidia rtx a6000 (48gb vram) gpus,
and each drl training epoch can be completed within 0.03 gpu hours. we used a
learning rate of 1 × 10 -5 and batchsize of 128 for drl training, while no
weight decay was applied. to solve the lmi problem in (5), we used sdpt3 v4.0
[17] as the solver. we set the cu perturbation with the acceleration factor of
4, 6, 8, 12 with the central fraction of 8%, 6%, 4% and 2% in k-space
respectively, and the remaining parts were chosen randomly in the peripheral
region accordingly.
we show the stroke classification auroc in fig. 3. when the k-space subsampling
fraction decreased and the signal became sparser, the performance of both vit
and cnn models trained under erm dropped significantly, from around 95% to below
80%. drl significantly improved the auroc of the ermbased vit model from 74.5%
to 83.1% when the mr images were under extreme cu perturbation, while only
slightly influenced model performance on the clean test set. for wgn, the
largest improvement brought to erm-based vit model was 16.9%. although we only
applied drl to the last layer of the cnn model, the improvement against erm was
still remarkable, up to 11.9%/4.9% for cu/wgn. with bat and pgd adversarial
training, the corresponding vit or cnn models were also improved, though when
drl was combined with bat and pgd, the model robustness can be further enhanced.
table 1 shows the maximum auroc and auprc improvement that drl can bring to
different baseline methods. for vit and cnn models, the auroc improvement w.r.t
bat/pgd defensive methods is up to 23.9%/12.2%, respectively. note that the
perturbed mri samples used to implement bat were the same as those used by drl,
which shows that drl is a more effective way to exploit the information in
adversarial samples, compared to simply adding the blurry images into the
training set. for cu perturbation, our best combined model using drl improved
the auroc/auprc of the erm model by up to 15%/12.5%, while this improvement
under wgn perturbation was up to 18.8%/36.2%. under cu perturbation, we further
show that our drl model can recognize stroke while clinicians may fail to. in
fig. 4, the stroke mri slices from the test sets are under different levels of
cu perturbations. for both erm-and drlbased vit models, we maximized the f1
score of the stroke class on the training set to calculate the optimal decision
threshold for stroke prediction, in order to balance the precision and recall.
when the k-space signal becomes more sparse, the reconstructed mri slices get
more blurry and the lesion areas become less recognizable, even for human eyes.
as a result, the erm model fails to detect stroke under high perturbation
levels. nevertheless, the drl model can tolerate more intense cu perturbation
and recognize stroke slices that may even be misclassified by clinicians, which
reveals its value in improving the diagnosis in an accelerated mri mode. we
verified the effectiveness of our approach on the actual clinical scans acquired
for clinical care and not just for research purposes, suggesting that the
methods and findings in the current study should be generalizable to routine
clinical practice conditions and potentially other types of clinical image-based
diagnosis (e.g., brain tumor) as well. in addition, our drl framework does not
necessarily need to be used in isolation, rather it can also be combined with
other performance boosting methods in accelerated mri to further improve them,
just like for bat and pgd.
in this study, we implemented a drl-based robust learning approach to improve
the robustness of deep image classifiers, in order to achieve more accurate
stroke classification from brain mr images reconstructed from a sub-sampled
k-space.our work can potentially be applied to accelerate and improve
time-critical stroke diagnosis. future work can apply drl to more mri diagnosis
tasks (e.g., lesion area segmentation), justifying its effectiveness on more
types of subsampling methods in mri acceleration.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9_74.
computer-aided diagnosis utilizes machine learning techniques to conduct a
pathological diagnosis concerning biomedical imaging data collected from various
pathological modalities, such as computed tomography [19], magnetic resonance
imaging [11], ultrasound [23], and angiography [9]. with the assistance of cad
techniques, the clinicians merely need to check the possible pathological
regions narrowed down by computer-aided diagnosis method, significantly reducing
the entire diagnosis time. with the recent success of deep learning, researchers
are able to raise the reliability of cad methods and assist clinicians in
diagnosing more complex clinical tasks. however, a reliable machine
learning-based cad method usually relies on the supervision of abundant
annotated training data. yet diseased pathological data are rare and diverse,
and acquiring reliable pathological annotations are labor-intensive and
expertise-required. as a result, the difficulty of data collection restricts the
development of the supervised cad.due to the difficulty of acquiring the
abundant annotated training data, the current sota method, i.e., csm [14],
proposes a mil-based wvad manner to specifically tackle one specific disease
detection task, i.e., colorectal cancer diagnosis via colonoscopy. considering
the case of colonoscopy, the csm's anomaly detection setting is used to handle
the rare and diverse diseased pathological data by commonly assuming that only
video-level annotations are available for training. furthermore, its video
setting concerns the temporal correlation within data. the setting of such
mil-based weakly supervision prevents the need for abundant annotated training
data by assuming that merely the video-level annotations, including normal and
diseased ones, are available for training.similar to the previous mil-based wvad
methods [4,13,14], our model assumes all training snippets (consecutive video
frames) within a non-diseased video are all normal snippets, yet each diseased
video has at least one abnormal snippet. furthermore, the proposed contrastive
feature decoupling network treats disease detection as an out-of-distribution
task. precisely, our cfd learns a memory bank to learn normal features. a
snippet that failed to be well reconstructed with these normal features is
considered diseased. on the other hand, the residual of a snippet and its
reconstructed one reveals the snippet's abnormal ingredients. consequently, we
are able to decouple each snippet as normal ingredients (reconstructed parts)
and abnormal ingredients (residual parts) by leveraging the memory bank. with
the decoupled snippet-level feature ingredients, our cfd employs both the normal
and abnormal feature ingredients via a contrastive learning paradigm to
concurrently optimize video-level and snippetlevel disease scores for pursuing
more accurate detection.to assess the proposed contrastive feature decoupling
network, we conduct experiments on two datasets, i.e., polyp and panda-mil. the
main contributions are summarized as follows.-our contrastive feature decoupling
network learns a memory bank to learn normal atoms for decoupling each snippet
as normal and diseased feature ingredients as opposite contrastive learning
samples. such a feature decoupling intrinsically fits the contrastive learning
paradigm for optimizing mil objectives on bags and instances. 2 related work
with the evolution of artificial intelligence techniques in the past decades,
deep learning has shown its potential for computer-aided diagnosis of various
symptoms [6,8,14,17,22]. for example, li et al. [8] established a large-scale
attentionbased database and designed a specialized model using retinal fundus
images for detecting glaucoma. windsor et al. [17] constructed a
transformer-based model to detect spinal cancer for mri scans. more recently,
tian et al. [14] formulated polyp detection in a wvad scheme while tackling
polyp detection using colonoscopy videos to search colon polyps in the temporal
sequence. unlike previous methods of handling one specific pathological
modality, we simultaneously address disease detection across pathological
modalities of colonoscopy videos and prostate tissue biopsies using our
contrastive feature decoupling network.
the characteristics of self-supervised learning are defining the proxy objective
or addressing pretext tasks using pseudo labels for the unlabeled instances. one
popular branch is contrastive learning which shows a remarkable ability to
obtain the desired semantic representation from various perspectives. for
example, cola [21] tackled action localization by proposing snippet contrast
loss to refine the feature representations of hard snippets according to the
easily discriminative snippets. csm [14] borrowed the concept from cola and
defined the hard/easy snippets for representing normal/abnormal from colonoscopy
videos. they empirically selected hard snippets based on the transitional edge
and missed disease snippets, such as an occlusive polyp. in this work, we employ
a similar contrastive learning strategy as csm while preventing their rule-based
contrastive training samples selection by intrinsically leveraging the decoupled
features derived from our feature decoupling process via memory bank.
we aim to design a mil-based wvad model for tackling disease detection across
different pathological modalities. our model contains an offline trained memory
bank to store feature atoms before the cfd training procedure, which associates
a contrastive loss to boost the model performance using decoupled features per
instance. winin the mil scenario, our model employs two classifiers to enable
reasoning of the disease scores at instance and bag levels. figure 1 overviews
the working flow of the proposed contrastive feature decoupling network.
given dataset d comprising normal sub-dataset d 0 and abnormal sub-dataset d 1 ,
we first encode all instances per bag b ∈ d into instance-level feature set f =
{f t } t t=1 ∈ r t ×c via a pre-trained feature extractor e. that is, f = e(b),
t is the number of instances, and c represents the instance-level feature
dimension. we then collect all normal instance-level features f ∈ r 1×c from d 0
to learn the memory bank m by using the dictionary learning technique [7] where
d 0 is the normal sub-dataset collected from the training split, w t is the
learned weights within the memory bank learning process, and λ is a
hyperparameter to constrain the memory bank sparsity.
with the learned normal instance features stored in the memory bank m, we are
able to reconstruct a normal version for any given bag-level feature f. such a
normal version is denoted as a normal-like feature f h . to this end, we
reconstruct a given f concerning m via the following equationwhere σ stands for
the softmax function; φ q , φ k , and φ v respectively represent the query, key,
and value linear projections, as introduced in the self-attention framework
[15]. to make a robust learning process by mining the hard and easy instances
for the subsequent contrastive loss, csm [14] designed a rule-based instances
selection concerning the transitional edge and missed disease instances, such as
occlusion or invisibility from the polyp. different from the csm model, we
generate the disease-like feature f d referring to f h as follows:where ω ∈ r
1×c is the weight for reweighting f by channel-wise multiplication.for depicting
the degree of disease/abnormal to estimate the channel-wise weight ω, we
consider attending to the distant features with respect to the normal ones,
i.e., f h . in practice, we estimate the degree of disease/abnormal based on the
difference between f and f h bywhere φ d , g, and ψ are linear projections,
global average pooling, and the multiscale temporal network [13], respectively.
with the decoupled features f h and f d , our mil-based instance-level
classifier aims to carry out the discriminating decision, and the bag-level
classifier seeks the prediction as fitting annotation y as possible. following
recent milbased methods, we select top-k instances of normal and abnormal in
each bag to form the separation loss aswhere δ is the hyperparameter for
constrained margin and {•} k is the operator that selects top-k instances. the
other common loss used in the recent milbased works is the classification loss
building upon the binary cross entropywhere s = φ i (f d ) represents the
instance-level prediction inferred by an instance-level classifier φ i and s = φ
b (g((1ω) f h )) means the bag-level prediction resulting from a bag-level
classifier φ b .
motivated by the recent wvad methods [4,13,14,18], which adopt auxiliary losses
to regularize the learning procedure, we consider the conventional
regularization losses, such as temporal smoothness and sparsity, as followsby
using the decoupled features f h and f d , we are ready to regularize the
opposite decoupled features across bags with the aid of a contrastive loss. an
expected contrastive loss aims to make our model attract features from the same
category while distracting the features from distinct classes. in practice, we
formulate such a contrastive loss bywhere τ denotes the temperature parameter in
the normalized temperaturescaled loss. notice that ( 8) is simplified for the
sake of clarity. a complete objective should consider the symmetric form by
switching d 1 and d 0 in (8).
we evaluate our model against sotas on the existing polyp [14] dataset and the
panda-mil dataset introduced in this work. we employ the same evaluation
criteria as the previous work for a fair comparison. please refer to the
supplementary material for the statistics of the two datasets.polyp. this
dataset collects colonoscopy videos from hyper-kvasir [1] and ldpolypvideo [10].
its training split contains 163 videos of video-level annotations, and the
testing split includes 90 videos of frame-level annotations.
the prostate cancer grade assessment (panda) challenge [2] comprises over 10k
whole-slide images (wsis) of digitized hematoxylin and eosin-stained biopsies
originating from radboud university medical center and karolinska institute.
panda-mil collects the eosin-stained biopsies with region-based masks indicating
the benign (normal) and cancerous (abnormal) tissue, combined by stroma and
epithelium. to fit the mil-based wvad task, we non-overlapped partition each wsi
(bag) into patches (instances) and only keep those patches comprising tissue
over the 50% patch size. each kept patch gets its patch-level annotations from
panda, and a wsi comprising any abnormal patch is treated as an abnormal wsi. in
sum, panda-mil's training split contains 3,925 bags of bag-level annotations,
and the testing split includes 975 bags of instance-level annotations.metric. we
follow the previous methods [4,13,14] to employ the instant-level area under
curve (auc) and the average precision (ap) for a fair comparison. the larger
values of both metrics mean better disease detection performance.
all the evaluated methods in the experiment used the same feature encoder, i.e.,
i3d [3] pre-trained on kinetics-400 [5], for a fair comparison. our method is
trained using adam optimizer with the learning rate of 0.001, batch size 32, and
200 epochs. each bag/video is encoded into t = 32 snippets among both datasets
via linear interpolation.
table 1 shows the compared results of our cfd model against recent wvad methods
[4, 13,14,18] for tackling the disease detection task. the results in table 1
demonstrate that our cfd consistently outperforms all the other methods on two
datasets. precisely, our model achieves the new sota by 1.1% auc and 1.5% ap
improvements on the polyp dataset and 1.09% auc and 2.45% ap improvements on the
panda-mil dataset. please refer to the supplementary material for the completed
results, including more wvad methods [12,16,20,24]. figure 2 visualizes one
disease detection result of our cfd model on the panda-mil dataset. the disease
score per instance/patch predicted by our method is close to the ground-truth
annotations, in which the clear margin between cancerous and benign validates
the robust prediction of the proposed cfd model.
we analysis on why the cfd network performs better than other methods listed in
table 1 by ablating the contributed components in cfd. the ablation study in
table 2 is conducted on the panda-mil dataset to evaluate the effectiveness of
the memory bank and loss functions in our model. row one in table 2 indicates
our cfd model without regularization, yet it has shown better auc values than
other methods besides the s3r. while employing the three loss functions for
regularization, as described in sect. 3.3, each loss function shows its
improvement in our model performance. the contrastive loss contributes the most
to auc improvement, enabling our model to achieve the sota performance.
this paper casts disease detection as a mil-based wvad task and introduces
contrastive feature decoupling (cfd) network to learn a memory bank boosted with
contrastive learning. with the learned feature atoms stored in the memory bank,
our contrastive feature decoupling is able to decouple each snippet as normal
and abnormal proxies. further, the decoupled abnormal proxies highlight the
abnormal feature ingredients for better reasoning the disease score. our feature
decoupling intrinsically fits the contrastive learning paradigm to define
opposite training samples for model optimization. besides, we introduce a new
dataset of prostate cancer detection, i.e., panda-mil, to provide a biomedical
imaging dataset concerning a different pathological modality. experiments
demonstrate that our cfd network achieves new sota performance on the polyp and
panda-mil datasets, indicating that our method effectively addresses the disease
detection task across different pathological modalities.
medical lesion detection plays an important role in assisting doctors with the
interpretation of medical images for disease diagnosing, cancer staging, etc.,
which can improve efficiency and reduce human errors [9,19]. current object
detection approaches are mainly based on supervised learning with abundant
well-paired image-level annotations, which heavily rely on expert-level
knowledge. as such, these supervised approaches may not be suitable for medical
lesion detection due to the laborious labeling.recently, large-scale pre-trained
vision-language models (vlms), by learning the visual concepts in the images
through the weak labels from text, have prevailed in natural object detection or
visual grounding and shown extraordinary performance. these models, such as glip
[11], x-vlm [10], and vinvl [24], can perform well in detection tasks without
supervised annotations. therefore, substituting conventional object detection
with vlms is possible and necessary. the vlms are first pre-trained to learn
universal representations via large-scale unlabelled data and can be effectively
transferred to downstream tasks. for example, a recent study [15] has
demonstrated that the pre-trained vlms can be used for zero-shot medical lesion
detection with the help of well-designed prompts.however, current existing vlms
are mostly based on a single prompt to establish textual and visual alignment.
this prompt needs refining to cover all the features of the target as much as
possible. apparently, even a well-designed prompt is not always able to combine
all expressive attributes into one sentence without semantic and syntactic
ambiguity, e.g., the prompt design for melanoma detection should include
numerous kinds of information describing attributes complementing each other,
such as shape, color, size, etc [8]. in addition, each keyword in a single
lengthy prompt cannot take effect equally as we expect, where the essential
information can be ignored. this problem motivates us to study alternative
approaches with multiple prompt fusion.in this work, instead of striving to
design a single satisfying prompt, we aim to take advantage of pre-trained vlms
in a more flexible way with the form of multiple prompts, where each prompt can
elicit respective knowledge from the model which can then be fused for better
lesion detection performance. to achieve this, we propose an ensemble guided
fusion approach derived from clustering ensemble learning [3], where we design a
step-wise clustering mechanism to gradually screen out the implausible
intermediate candidates during the grounding process, and an integration module
to obtain the final results by uniting the mutually independent candidates from
each prompt. in addition, we also examine the language syntax based prompt
fusion approach as a comparison, and explore several fusion strategies by first
grouping the prompts either with described attributes or categories and then
repeating the fusion process.we evaluate the proposed approach on a broad range
of public medical datasets across different modalities including photography
images for skin lesion detection isic 2016 [2], endoscopy images for polyp
detection cvc-300 [21], and cytology images for blood cell detection bccd. the
proposed approach exhibits extraordinary superiority compared to those with
single prompt and other common ensemble learning based methods for zero-shot
medical lesion detection. considering the practical need of lesion detection, we
further provide significantly improved fine-tuning results with a few labeled
examples.
object detection and vision-language models. in the vision-language field,
phrase grounding can be regarded as another solution for object detection apart
from conventional r-cnns [5,6,18]. recently, vision-language models have
achieved exciting performance in the zero-shot and few-shot visual recognition
[4,16]. glip [11] unifies phrase grounding and object detection tasks,
demonstrating outstanding transfer capability. in addition, vild [7] is proposed
for open-vocabulary object detection taking advantage of the rich knowledge
learned from clip [4] and text input.ensemble learning. as pointed out by a
review [3], ensemble learning methods achieve better performance by producing
predictions based on extracted features and fusing via various voting
mechanisms. for example, a selective ensemble of classifier chains [13] is
proposed to reduce the computational cost and the storage cost arose in
multi-label learning [12] by decreasing the ensemble size. undeed [23], a
semi-supervised classification method, is presented to increase the classifier
accuracy on labeled data and diversity on unlabeled data simultaneously. and a
hybrid semi-supervised clustering ensemble algorithm [22] is also proposed to
generate basic clustering partitions with prior knowledge.
in this section, we first briefly introduce the vision-language model for
unifying object detection as phrase grounding, e.g., glip [11] (sect. 3.1). then
we present a simple language syntax based prompt fusion approach in sect. 3.2.
finally, the proposed ensemble-guided fusion approach and several fusion
strategies are detailed in sect. 3.3 to improve the zero-shot lesion detection.
phrase grounding is the task of identifying the fine-grained correspondence
between phrases in a sentence and objects in an image. the glip model takes as
input an image i and a text prompt p that describes all the m candidate
categories for the target objects. both inputs will go through specific encoders
enc i and enc t to obtain unaligned representations. then, glip uses a grounding
module to align image boxes with corresponding phrases in the text prompt. the
whole process can be formulated as follows:where o ∈ r n ×d , p ∈ r m ×d denote
the image and text features respectively for n candidate region proposals and m
target objects, s ground ∈ r n ×m represents the cross-modal alignment scores,
and t ∈ {0, 1} n ×m is the target matrix.
as mentioned above, it is difficult for a single prompt input structure such as
glip to cover all necessary descriptions even through careful designation of the
prompt. therefore, we propose to use multiple prompts instead of a single prompt
for thorough and improved grounding. however, it is challenging to combine the
grounding results from multiple prompts since manual integration is subjective,
ineffective, and lacks uniform standards. here, we take the first step to fuse
the multiple prompts at the prompt level. we achieve this by extracting and
fusing the prefixes and suffixes of each prompt based on language conventions
and grammar rules. as shown in fig. 1 (a), given serials of multiple prompts p 1
, p 2 , . . . , p k , the final fused prompt p fuse from k single prompts is
given by:(2)
although the syntax based fusion approach is simple and sufficient, it is
restricted by the form of text descriptions which may cause ambiguity in the
fused prompt during processing. moreover, the fused prompts are normally too
long that the model could lose proper attention to the key information,
resulting in extremely unstable performance (results shown in sect.
4.2).therefore, in this subsection, we further explore fusion approaches based
on ensemble learning. more specifically, the vlm outputs a set of candidate
region proposals c i for each prompt p i , and these candidates carry more
multidimensional information than prompts. we find in our preliminary
experiments that direct concatenation of the candidates is not satisfactory and
effective, since simply integration hardly screens out the bad predictions. in
addition, the candidate, e.g., c ij ∈ c i , carries richer information that can
be further utilized, such as central coordinate x j and y j , region size w j
and h j , category label, and prediction confidence score. therefore, we
consider step-wise clustering mechanisms using the above information to screen
out the implausible candidates based on clustering ensemble learning [3].another
observation in our preliminary experiments is that most of the candidates
distribute near the target if the prompt description matches better with the
object. moreover, the candidate regions of inappropriate size containing too
much background or only part of the object should be abandoned directly. as
such, we consider clustering the center coordinate (x j , y j ) and region size
(w j , h j ) respectively to filter out those candidates with the wrong location
and size.this step-wise clustering with the aid of different features embodies a
typical ensemble learning idea. therefore, we propose a method called ensemble
guided fusion based on semi-clustering ensemble, as detailed in fig. 1 (b).
there are four sub-modules in our approach, where the location cluster f loc and
size cluster f size discard the candidates with large deviations and abnormal
sizes. then, in the prediction corrector f correct , we utilize the voting
mechanism to select the remaining candidates with appropriate category tags and
relatively high prediction confidence. after the first three steps of
processing, the remaining candidates c originated from each prompt can be
written as:the remaining candidates are then transferred to the integration
module for being integrated into the final fused result c fuse that is mutually
independent:besides, we also propose three fusion strategies to recluster
candidates in different ways before executing ensemble guided fusion, i.e.,
fusing the multiple prompts equally, by category, and by attribute. compared to
the first strategy, fusing by category and by attribute both have an additional
step of reorgnization. candidates whose prompts belong to the same category or
have identical attributes will share the similar distribution. accordingly, we
rearrange these candidates c i into a new set c for the subsequent fusion
process.
we collect three public medical image datasets across various modalities
including skin lesion detection dataset isic 2016 [2], polyp detection dataset
cvc-300 [21], and blood cell detection dataset bccd to validate our proposed
approach for zero-shot medical lesion detection. for the experiments, we use the
glip-t variant [11] as our base pre-trained model and adopt two metrics for the
grounding evaluation, including average precision (ap) and ap50. more details on
the dataset and implementation are described in the appendix. multiple nms [14]
12.0 20.6 27.9 37.9 11.9 21.4 soft-nms [1] 18.8 30. 3
this section demonstrates that our proposed ensemble guided fusion approach can
effectively benefit the model's performance.the proposed approach achieves the
best performance in zero-shot lesion detection compared to baselines. to confirm
the validity of our method, we conduct extensive experiments under the zero-shot
setting and include a series of fusion baselines: concatenation, non-maximum
suppression (nms) [14], soft-nms [1] and weighted boxes fusion (wbf) [20] for
comparisons. as illustrated in table 1, our ensemble guided fusion rivals the
glip [11] with single prompt and other fusion baselines across all datasets. the
first three rows in table 1 represent the results of single prompt by only
providing shape, color, and location information, respectively. furthermore, we
conduct a comparison between yolov5 [17] and our method on cvc-300 under 10-shot
settings. table 2 shows that our method outperforms yolov5, which indicates
fullysupervised models such as yolo may not be suitable for medical scenarios
where a large labeled dataset is often not available. in addition, we utilize
the automatic prompt engineering (ape) [25] method to generate prompts. these
prompts give comparable performance to our single prompt and can be still be
improved by our fusion method. and the details are described in the appendix.
here we present part of the single prompts used in the experiments for
illustration. the misclassification problem in some of the single prompts is
corrected (i.e., malignant to benign) on the first dataset. for all datasets,
the candidate boxes are more precise and associated with higher confidence
scores.fine-tuned models can further improve the detection performance. we
conduct 10-shot fine-tuning experiments as a complement, and find the
performance greatly improved. as shown in table 3 and fig. 2, with the same
group of multiple prompts, the accuracy of fine-tuned model has increased almost
twice as much as that of zero-shot, further demonstrating the effectiveness of
our method in both settings. therefore, we can conclude that the pre-trained
glip model has the ability to learn a reasonable alignment between textual and
visual modalities in medical domains.visualizations. figure 3 shows the
visualization of the zero-shot results across three datasets. syntax based
fusion sometimes fails to filter out unreasonable predictions because these
regions are generated directly by the vlm without further processing and
eventually resulting in unstable detection performance. on the contrary, our
approach consistently gives a better prediction that defeats all single prompts
with a relatively proper description, yet syntax based fusion relies too much on
the format and content of inputs, which results in great variance and
uninterpretability. the step-wise clustering mechanism based on ensemble
learning enables our method to exploit multi-dimensional information besides
visual features. in addition, the key components in our proposed approach are
unsupervised, which also enhances stability and generalization.comparison among
fusion strategies. in this work, we not only provide various solutions to fuse
multiple prompts but also propose three fusion strategies to validate the
generalization of ensemble-guided fusion. as shown in table 4, we present the
results obtained with three different fusion strategies: equally, by category,
and by attribute. the first strategy is to process each prompt equally, which is
the most convenient and suitable in any situation. fusing prompts by category is
specifically for multi-category datasets to first gather the prompts belonging
to the same category and make further fusion. similarly, fusing by attribute is
to fuse the candidates, whose prompts are describing the same attribute. the
strategy of fusing by attribute, outperforms the other ones, due to the fact
that candidates with the same attribute share a similar distribution, which is
prone to obtain a more reasonable cluster. on the contrary, it is possible to
neglect this distribution when fusing each prompt equally.ablation study. as
shown in table 5, we perform ablation studies on three datasets. our approach
has three key components, i.e., location cluster, size cluster and prediction
corrector. the location cluster filters out the candidates with severe deviation
from the target. the size cluster removes those abnormal ones. finally, the
prediction corrector further eliminates the candidates that cause low accuracy.
the results show that when combining the above three components, the proposed
approach gives the best lesion detection performance, suggesting that all
components are necessary and effective in the proposed approach.
in this paper, we propose an ensemble guided fusion approach to leverage
multiple text descriptions when tackling the zero-shot medical lesion detection
based on vision-language models and conduct extensive experiments to demonstrate
the effectiveness of our approach. compared to a single prompt that typically
requires exhaustive engineering and designation, the multiple medical prompts
provide a flexible way of covering all key information that help with lesion
detection. we also present several fusion strategies for better exploiting the
relationship among multiple prompts. one limitation of our method is that it
requires diverse prompts for effective clustering of the candidates. however,
with the help of other prompt engineering methods, the limitation can be
relatively alleviated.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9_28.
weakly supervised anomaly detection holds significant potential in real-world
clinical applications [27], particularly for new pandemic diseases where
obtaining pixel-wise annotations from human experts is challenging or even
impossible [16]. however, dominant anomaly detection methods based on the
one-class classification paradigm [8,11] often overlook the binary labels of
healthy and disease samples available in clinical centers, limiting their
detection granularity. traditional clustering techniques like z-score
thresholding [15], pca [14], and svm [26] have limited clinical value as they
mainly generate image-level anomaly results. to address this paradox, deep
generative models leverage additional pixel-wise domain knowledge captured
through adversarial training [1,12,24] or latent representation encoding
[3,4,6,32]. while these approaches allow obtaining pixelwise anomaly maps
through out-of-distribution (ood) detection, they often fail to generate
high-fidelity segmentation maps due to inadequate utilization of image
conditions. thus, the utilization of domain knowledge from weakly supervised
data becomes a crucial factor in achieving high-quality anomaly detection.
to enhance information transfer, we make an assumption that the forward states
and reverse states at the same time step t follow different distributions. based
on this assumption, we introduce conditions in the forward process, where all
previous states are used as conditions for the current state. this enables
efficient information transfer between states. and the final state xt
incorporates comprehensive information from the forward states. similarly, the
reverse process incorporates information from previous reverse states.moreover,
traditional generative models, such as gans [1,12] and normalizing flows [11],
commonly used for pixel-wise anomaly detection, are constrained by one-step data
projection in handling complex data distributions. this dilemma can be overcome
by employing probabilistic diffusion models [13,25] that capture data knowledge
through a series of step-by-step markovian processes [5]. the high-fidelity
generation proficiency, flexible network architecture, and stable training
scheme of existing diffusion-based anomaly detection approaches have
demonstrated promising performance in detecting anomaly regions
[23,[27][28][29]. however, diffusionnv-based approaches have high computational
costs due to iterative evaluations. [20] explores the diffusion on the latent
space with smaller sizes instead of pixel space to reduce the computations.
moreover, all these methods still face challenges including the lack of
fine-grained guidance and gradual loss of anatomical information in markovian
chains. to address these limitations, we propose a novel non-markovian
hybridconditioned diffusion model with fast samplers. our approach utilizes
strong hybrid image conditions that provide powerful sampling guidance by
integrating coarse segmentation maps and original instance information based on
the non-markovian assumption (as shown in fig. 1). additionally, we modify the
forward and reverse process as a higher-order deterministic ordinary
differential equation (ode) sampler to accelerate inference. we validate our
framework on two brain medical datasets, demonstrating the effectiveness of the
framework components and showing more accurate detection results of anomaly
regions.
in this section, we present a fast non-markovian diffusion model that utilizes
pixel-wise strong conditions and encoding/sampling accelerator for anomaly
segmentation to enhance generation fidelity and sampling speed. section 2.1
introduces the non-markonvian model and hybrid conditions for guided sampling.
section 2.2 proposes the acceleration approach for encoding and sampling.
learning deterministic mappings between diseased and healthy samples sharing the
same anatomical structures is essential to enhance inaccurate and timeconsuming
diffusion-based approaches, which require strong guidance during sampling.
however, current diffusion-based models only provide insufficient conditions
(such as binary classification results), leading to vague anomaly distributions.
to achieve consistent and stable generation, we propose a hybrid conditional
diffusion model dependent on the non-markovian assumption. it injects noise into
the original distribution sequentially using the gaussian distribution and then
reconstructs the original distribution by reverse sampling. following the
expression of [13], the markovian-based diffusion framework is defined as:where
the discrete states {x t } t t=0 are from step 0 to t , forward step q and
trained reverse step p θ have one-to-one mapping. denoting {α t } t t=0 and {σ t
} t t=0 as variance scales for noise perturbation, the gaussian transition
kernels are:to keep the anatomical information across states, the proposed
non-markovian anatomy structure mappings are built by adding previous-state
information into forward and reverse states, which preserves distribution prior
for high-quality reconstruction. denoting all accumulated states from the
forward process as c and the state in the backward step t as xt , we formulate
the generalized non-markovian diffusion framework (gndf) as:similar to vanilla
ddpm [13], our conditional noise prediction network is trained according to the
negative log-likelihood (nll) lower bound minimization of generated
distributions. it is further transformed into the l2 loss between the estimated
conditional noise and the ground-truth gaussian noise as:to enable the diffusion
model to effectively differentiate between anatomical and anomaly information
from previous states, we introduce a hybrid condition that includes the input
state x 0 , coarse segmentation maps, and classifier gradients derived from
healthy labels. in order to simplify the computational complexity and leverage
the rich information contained in x 0 , we replace the forward state conditions
c with the original state x 0 .regarding hybrid condition implementation, we
train a binary classifier with healthy and diseased images to provide further
guidance on anomaly regions independently, following class-conditional methods
[10,27]. a memory bank [21] is applied to store representative features of
healthy samples, which enables quick generation of coarse segmentation maps x
seg during the testing phase, addressing the issue of knowledge forgetting in
original diffusion models. to keep the active segmentation map as a condition in
the diffusion model training, a health image x 0 is transformed into a diseased
image x n 0 based on a random x seg . overall, we train our non-markovian
diffusion model depending on the current states, the coarse segmentation maps,
image labels, and the original data during the diffusion process with healthy
and diseased images. the training objective is given by:where x t is the
concatenation of xt , x n 0 , and x seg . y denotes the corresponding binary
label for each data. ω(t) is a weighted function for providing dynamic weights
to explore density regions. in this work, we simply set ω(t) as 1. the full
framework of our model is shown in fig. 2.
diffusion acceleration, which is equivalent to reconstruction error minimization
[7], is implemented by balancing the trade-off between sample quality and
computation cost. existing diffusion-based anomaly detectors [27] require a
significant number of evaluation steps for encoding and sampling, which makes
them impractical for clinical applications. to address this paradox, we adapt an
ordinary differential equation solver [17] for fast and stable encoding &
sampling independent of the complex trade-off.following the setting of
continuous-time ito stochastic differential equation, where the drift
coefficient, the diffusion coefficient, and the wiener process are denoted as f
(x, t), g(t), w, we have our forward process and probability flow ode sampling
[25] as:by decomposing the conditional sampling scheme according to bayes
theorem, the guided diffusion process can be achieved by mixture guidance
composed of conditional noise prediction model and classifier gradient:denote
the classifier as c, the binary label as y, the signal-to-noise coefficient as
λ, and the conditional noise prediction network as θ x , λ, y , we further have
the hybrid conditional diffusion network ˆ θ as: ˆ θ (x , λ, y) := θ (x , λ, y)
+ s • c(x t , t, y).(8) the semi-linear probability flow ode is solved reversely
with second-order multi-step numerical methods [18]. then, we apply hybrid
conditional sampling to noisy data xt to reproduce a healthy one with the same
anatomy structure by conditional data prediction network and the binary
classifier [10]. following the symbol of [17,18], the conditional sampling
is:finally, we post-process the reconstructed samples by subtracting original
inputs and performing otsu's threshold to obtain the anomaly segmentation map.
brats 2020 [2] is a brain tumor segmentation dataset containing the mr sequences
of t1, t1gd, t2, and flair. following the preprocessing approach of [27], we
concatenate all image modalities along the channel dimension, prune the upper
and lower axial slices, and pad each slice into 256 × 256. all tumor classes are
merged into a single class in the segmentation mask. the training set contains
10,410 slices with tumors and 5,809 healthy slices. the testing set includes
1,316 images with tumors. isles 2022 [19] is an mr image dataset for stroke
lesions segmentation. it contains ischemic strokes of various sizes and from
different disease stages. we extract the axial slices from dwi sequence and
resize them into 256 × 256. the training set includes 2,707 healthy slices and
1,483 slices with lesions. the testing set contains 282 slices with lesions.
note that only the image-level binary labels are used in our training. the
evaluation metrics include dice, volumetric similarity, and hausdorff distance,
which are calculated in slice level and volume level for brats and isles,
respectively.
to fairly compare with previous state-of-the-art diffusion models, we use the
same network architectures as diffano [27]. we set the batch sizes of diffusion
model and classifier as 3 and 10, respectively. the adam optimizer with a
learning rate of 0.0001 is used to train the diffusion model for 130,000
iterations and train the classifier for 150,000 iterations. the training
diffusion step is 1000. the forward encoding and sampling steps are both set to
50 in the inference. we randomly generate the lesion masks as [30] and corrupt
the corresponding regions on the conditioning images in the training phase.
to compare the performance, we choose the anomaly detection methods including
memory-based methods (such as patchcore [8] and padimcore [8]), normalizing flow
based methods (such as csflow [22] and fastflow [31]), distillationbased methods
(such as reverse distillation [9] and stfpm [33]), and a diffusion-based method
(diffano [27]) which also utilizes image-level binary labels. we train them on
brats2020 and isles datasets. table 1 shows the segmentation results. our fndm
outperforms the existing methods in all metrics on both datasets. diffusion
methods have a more powerful generation ability than non-diffusion methods, and
our method outperforms the best non-diffusion methods over 20% dice. fndm also
outperforms the previous state-of-theart diffusion method, diffano, by a large
gap of +9.56% dice, -0.98% hdis, and +0.54% vsim on brats dataset, and +19.98%
dice, -1.39% hdis, and +26.73% vsim on isles dataset, revealing that our fndm is
effective to reconstruct the healthy image from diseased to detect the anomaly
regions in brain mr images. thanks to non-markovian procedure and pixel-wise
hybrid guidance, the performance improvement of our method is larger on the
isles dataset where stroke lesions are more challenging due to smaller sizes and
irregular shapes.
we conduct ablation studies for hybrid conditions and the steps of encoding and
sampling procedures. from table 2, we decompose the overall pixel-wise hybrid
condition into classifier gradient (cg), non-markovian (nm), and memory bank
(mb), comparing all possible combinations on brats2020 dataset. we observe that
combinations with more components achieve better performance, and nm module
achieves a higher increase than mb module. in fig. 4, we further evaluate the
segmentation performance of diffano [27] and ours across diverse steps on
brats2020 dataset. diffano performs best at 300 steps, concluding that it is the
optima where anomaly information vanishes and anatomy information preserves
partly. our method only needs 50 steps which achieve 6-time acceleration
compared to diffano when both approaches reach the best dice scores. besides,
our method performs stably as the step amount exceeds 50 since non-markovain
strong guidance ensures high-quality information transition along the timeline,
independent of the loss of anatomy structure.
we propose a fast non-markovian diffusion model (fndm) for weakly supervised
anomaly detection. fndm first encodes the images into noisy ones, then applies
hybrid conditional generation to reconstructed original images without
anomalies. fndm achieves high-fidelity generation on weak labels by leveraging
non-markovian modeling and pixel-wise hybrid conditions. besides, fndm conducts
ode fast solver for encoding and sampling to reach 6-time acceleration.
extensive experiments on two brain datasets reveal the effectiveness and
superiority of our approach for anomaly detection. the limitation of our method
is that, as a diffusion-based method, it still needs more evaluation steps than
gans. in the future, we could investigate the knowledge distillation techniques
to further reduce the sampling steps and apply fndm in other modalities.
renal cancer is the most lethal malignant tumor of the urinary system, and the
incidence is steadily rising [13]. conventional b-mode ultrasound (us) is a good
screening tool but can be limited in its ability to characterize complicated
renal lesions. contrast-enhanced ultrasound (ceus) can provide information on
microcirculatory perfusion. compared with ct and mri, ceus is radiation-free,
cost-effective, and safe in patients with renal dysfunction. due to these
benefits, ceus is becoming increasingly popular in diagnosing renal lesions.
however, recognizing important diagnostic features from ceus videos to diagnose
lesions as benign or malignant is non-trivial and requires lots of experience.to
improve diagnostic efficiency and accuracy, many computational methods were
proposed to analyze renal us images and could assist radiologists in making
clinical decisions [6]. however, most of these methods only focused on
conventional b-mode images. in recent years, there has been increasing interest
in multi-modal medical image fusion [1]. directly concatenation and addition
were the most common methods, such as [3,4,12]. these simple operations might
not highlight essential information from different modalities. weight-based
fusion methods generally used an importance prediction module to learn the
weight of each modality and then performed sum, replacement, or exchange based
on the weights [7,16,17,19]. although effective, these methods did not allow
direct interaction between multi-modal information. to address this,
attention-based methods were proposed. they utilized cross-attention to
establish the feature correlation of different modalities and self-attention to
focus on global feature modeling [9,18]. nevertheless, we prove in our
experiments that these attentionbased methods may have the potential risks of
entangling features of different modalities.in practice, experienced
radiologists usually utilize dynamic information on tumors' blood supply in ceus
videos to make diagnoses [8]. previous researches have proved that temporal
information is effective in improving the performance of deep learning models.
lin et al. [11] proposed a network for breast lesion detection in us videos by
aggregating temporal features, which outperformed other image-based methods.
chen et al. [2] showed that ceus videos can provide more detailed blood supply
information of tumors allowing a more accurate breast lesion diagnosis than
static us images.in this work, we propose a novel multi-modal us video fusion
network (muvf-yolox) based on ceus videos for renal tumor diagnosis. our main
contributions are fourfold. (1) to the best of our knowledge, this is the first
deep learning-based multi-modal framework that integrates both b-mode and
ceusmode information for renal tumor diagnosis using us videos. (2) we propose
an attention-based multi-modal fusion (amf) module consisting of cross-attention
and self-attention blocks to capture modality-invariant and modality-specific
features in parallel. (3) we design an object-level temporal aggregation (ota)
module to make video-based diagnostic decisions based on the information from
multi-frames. (4) we build the first multi-modal us video datatset containing
b-mode and ceus-mode videos for renal tumor diagnosis. experimental results show
that the proposed framework outperforms single-modal, single-frame, and other
state-of-the-art methods in renal tumor diagnosis.
the proposed muvf-yolox framework is shown in fig. 1. it can be divided into two
stages: single-frame detection stage and video-based diagnosis stage. (1) in the
single-frame detection stage, the network predicts the tumor bounding box and
category on each frame in the multi-modal ceus video clips. dual-branch backbone
is adopted to extract the features from two modalities and followed by the amf
module to fuse these features. during the diagnostic process, experienced
radiologists usually take the global features of us images into consideration
[20]. therefore, we modify the backbone of yolox from csp-darknet to
swin-transformer-tiny, which is a more suitable choice by the virtue of its
global modeling capabilities [15]. (2) in the video-based diagnosis stage, the
network automatically chooses high-confidence region features of each frame
according to the single-frame detection results and performs temporal
aggregation to output a more accurate diagnosis. the above two stages are
trained successively. we first perform a strong data augmentation to train the
network for tumor detection and classification on individual frames. after that,
the first stage model is switched to the evaluation mode and predicts the label
of each frame in the video clip. finally, we train the ota module to aggregate
the temporal information for precise diagnosis.
using complementary information between multi-modal data can greatly improve the
precision of detection. therefore, we propose a novel amf module to fuse the
features of different modalities. as shown in fig. 1, the features of each
modality will be input into cross-attention and self-attention blocks in
parallel to extract modality-invariant features and modality-specific features
simultaneously.taking the b-mode as an example, we first map the b-mode features
f b and the ceus-mode features) using linear projection. then cross-attention
uses scaled dot-product to calculate the similarity between q b and k c . the
similarity is used to weight v c . crossattention extracts modality-invariant
features through correlation calculation but ignores modality-specific features
in individual modalities. therefore, we apply self-attention in parallel to
highlight these features. the self-attention calculates the similarity between q
b and k b and then uses the similarity to weight v b . similarly, the features
of the ceus modality go through the same process in parallel. finally, we merge
the two cross-attention outputs by addition since they are both invariant
features of two modalities and concatenate the obtained sum and the outputs of
the two self-attention blocks. the process mentioned above can be formulated as
follows:where, f invar represents the modality-invariant features. f b-spec and
f c-spec represent the modal-specific features of b-mode and ceus-mode
respectively. f am f is the output of the amf module.
in clinical practice, the dynamic changes in us videos provide useful
information for radiologists to make diagnoses. therefore, we design an ota
module that aggregates single-frame renal tumor detection results in temporal
dimension for diagnosing tumors as benign and malignant. first, we utilize a
feature selection module [14] to select high-quality features of each frame from
the cls_conv and reg_conv layers. specifically, we select the top 750 grid cells
on the prediction grid according to the confidence score. then, 30 of the top
750 grid cells are chosen by the non-maximum suppression algorithm for reducing
redundancy. the features are finally picked out from the cls_conv and reg_conv
layers guided by the positions of the top 30 grid cells. let f cls = {cls 1 ,
cls 2 , ...cls l } and f reg = {reg 1 , reg 2 , ...reg l } denote the above
obtained high-quality features from l frames. after feature selection, we
aggregate the features in the temporal dimension by time attention. f cls and f
reg are mapped into (q cls , k cls , v cls ) and (q reg , k reg ) via linear
projection. then, we utilize scaled dot-product to compute the attention weights
of v cls as:after temporal feature aggregation, f temp is fed into a multilayer
perceptron head to predict the class of tumor.3 experimental results
we collect a renal tumor us dataset of 179 cases from two medical centers, which
is split into the training and validation sets. we further collect 36 cases from
the two medical centers mentioned above (14 benign cases) and another center
(fujian provincial hospital, 22 malignant cases) to form the test set. each case
has a video with simultaneous imaging of b-mode and ceus-mode. some examples of
the images are shown in fig. 2. there is an obvious visual difference between
the images from the fujian provincial hospital (last column in fig. 2) and the
other two centers, which raises the complexity of the task but can better verify
our method's generalization ability. more than two radiologists with ten years
of experience manually annotate the tumor bounding box and class label at the
frame level using the pair annotation software package (https://www.aipair.
com.cn/en/, version 2.7, rayshape, shenzhen, china) [10]. each case has 40-50
labeled frames, and these frames cover the complete contrast-enhanced imaging
cycle. the number of cases and annotated frames is summarized in table 1.weights
pre-trained from imagenet are used to initialize the swin-transformer backbone.
data augmentation strategies are applied synchronously to b-mode and ceus-mode
images for all experiments, including random rotation, mosaic, mixup, and so on.
all models are trained for 150 epochs. the batch size is set to 2. we use the
sgd optimizer with a learning rate of 0.0025. the weight decay is set to 0.0005
and the momentum is set to 0.9. in the test phase, we use the weights of the
best model in validation to make predictions. all experiments are implemented in
pytorch with an nvidia rtx a6000 gpu. ap 50 and ap 75 are used to assess the
performance of single-frame detection. accuracy and f1-score are used to
evaluate the video-based tumor diagnosis.
single-frame detection. we explore the impact of different backbones in yolox
and different ways of multi-modal fusion. as shown in table 2, using
swin-transformer as the backbone in yolox achieves better performance than the
original backbone while reducing half of the parameters. the improvement may
stem from the fact that swin-transformer has a better ability to characterize
global features, which is critical in us image diagnosis. in addition, we
explore the role of cross-attention and self-attention blocks in multi-modal
tasks, as well as the optimal strategy for combining their outputs. comparing
row 5 with row 7 and row 8 in table 2, the dual-attention mechanism outperforms
the single crossattention. it indicates that we need to pay attention to both
modality-invariant and modality-specific features in our multi-modal task
through cross-attention and self-attention blocks. however, "ca+sa" (row 6 in
table 2) obtains inferior performance than "ca" (row 5 in table 2). we
conjecture that connecting the two attention modules in series leads to the
entanglement of modality-specific and modality-invariant information, which
would disrupt the model training. on the contrary, the "ca//sa" method,
combining two attention modules in parallel, enables the model to capture and
digest modality-specific and modality-invariant features independently. for the
same reason, we concatenate the outputs of the attention blocks rather than
summing, which further avoids confusing modality-specific and modality-invariant
information. therefore, the proposed method achieves the best performance.table
2. the results of ablation study. "ca" and "sa" denote cross-attention and
selfattention respectively. "//" and "+" mean parallel connection and series
connection. video-based diagnosis. we investigate the performance of the ota
module for renal tumor diagnosis in multi-modal videos. we generate a video clip
with l frames from annotated frames at a fixed interval forward. as shown in
table 3, gradually increasing the clip length can effectively improve the
accuracy. this suggests that the multi-frame model can provide a more
comprehensive characterization of the tumor and thus achieves better
performance. meanwhile, increasing the sampling interval tends to decrease the
performance (row 4 and row 5 in table 3). it indicates that continuous
inter-frame information is beneficial for renal tumor diagnosis.
the comparison results are shown in table 4. compared to the single-modal
models, directly concatenating multi-modal features (row 3 in table 4) improves
ap 50 and ap 75 by more than 15%. this proves that complementary information
exists among different modalities. for a fair comparison with other fusion
methods, we embed their fusion modules into our framework so that different
approaches can be validated in the same environment. cmml [19] and cen [17]
merge the multi-modal features or pick one of them by automatically generating
channel-wise weights for each modality. they score higher ap in the validation
set but lower one in the test set than "concatenate". this may be because the
generated weights are biased to make similar decisions to the source domain,
thereby reducing model generalization in the external data. moreover, cmf only
highlights similar features between two modalities, ignoring that each modality
contains some unique features. tmm focuses on both modality-specific and
modality-invariant information, but the chaotic confusion of the two types of
information deteriorates the model performance. therefore, both cmf [17] and tmm
[9] fail to outperform weight-based models. on the contrary, our amf module
prevents information entanglement by conducting cross-attention and
self-attention blocks in parallel. it achieves ap 50 = 82.8, ap 75 = 60.6 in the
validation set and ap 50 = 79.5, ap 75 = 39.2 in the test set, outperforming all
competing methods while demonstrating superior generalization ability.
meanwhile, the improvement of the detection performance is beneficial to our ota
module to obtain lesion features from more precise locations, thereby improving
the accuracy of benign and malignant renal tumor diagnosis.
in this paper, we create the first multi-modal ceus video dataset and propose a
novel attention-based multi-modal video fusion framework for renal tumor
diagnosis using b-mode and ceus-mode us videos. it encourages interactions
between different modalities via a weight-sharing dual-branch backbone and
automatically captures the modality-invariant and modality-specific information
by the amf module. it also utilizes a portable ota module to aggregate
information in the temporal dimension of videos, making video-level decisions.
the design of the amf module and ota module is plug-and-play and could be
applied to other multi-modal video tasks. the experimental results show that the
proposed method outperforms single-modal, single-frame, and other
stateof-the-art multi-modal approaches.
pathological image analysis is a vital area of research within medical image
analysis, focused on utilizing computer technology to aid doctors in diagnosing
and treating diseases by analyzing pathological tissue slide images [5].
advancements in pathological image analysis have been made in early cancer
diagnosis, tumor localization, and grading, and treatment planning [3,10].
multi-instance learning [2] is the primary analysis method used, which involves
analyzing tasks based on slide labels and patches. despite this, the clinical
pathological analysis presents certain challenges and complexities, with the
ultimate diagnosis relying on patients rather than slides.specifically, in
clinical problems of pathological image analysis, doctors usually summarize
patient-level labels based on slide labels as the diagnostic results [1,6]. for
example, for the pathological discrimination diagnosis task of intestinal
tuberculosis(itb) and crohn's desease(cd), the categories of postoperative
slides are divided into three types (normal, cd, itb), and doctors will
summarize the binary results of patients (itb or cd) based on slide-level labels
[6]. similar situations exist in other tasks, such as the classification of
breast cancer metastases in lymph nodes, where slide categories may have
different classifications, and the corresponding diagnosis of the same patient
is whether the cancer has spread to the regional lymph nodes (n-stage) [1].
therefore, as shown in fig. 1, actual pathological image analysis involves the
relationships of patches, slides, and patients, which is called a multi-level
multi-instance learning (ml-mil) problem. among them, for patients and slides,
patients are bags while slides are instances, and for slides and patches, slides
are bags while patches are instances.there are generally two methods to solve
the ml-mil problem. the first method is to directly average the prediction
values of slides or take the maximum prediction value [9]. this method is
relatively simple, but the information exchange between slides is not fully
utilized, which may lead to errors in the summary result. the second method is
to treat slide-patient as a new mil problem according to the traditional mil
thinking, where slides are regarded as instances and patient labels as bags.
although this method seems reasonable, the number of patients is usually
relatively small, and deep learning models usually require a large amount of
data for training. therefore, the insufficient number of samples at the
slide-patient level may make it difficult for the model to learn enough
information.to address the multi-level multi-instance learning (ml-mil) problem
in medical field, we propose a novel framework called patients and slides are
equal (p&sre). inspired by the iterative labeling process in medical diagnosis,
this framework treats patients and slides as instances at the same level and
uses transformers and attention mechanisms to build connections between them.
this simple yet effective method allows for interaction between patient-level
and slidelevel information to correct their respective features and improve
classification performance. our framework consists of two steps: first, at the
patch-slide level, a common mil framework is used to train a mil neural network
and obtain slide-level feature vectors; then, at the slide-patient level, we use
self-attention mechanisms to combine the slides of the same patient into
patient-level feature vectors, and treat these patient-level feature vectors
together with all slide-level feature vectors of the same patient as instances
at the same level, which are inputted into transformers for feature interaction
and prediction of patient-and slide-level labels. our method can effectively
solve the problem of difficult training due to the scarcity of samples at the
highest level in ml-mil, and can be integrated into two state-of-the-art methods
to further improve performance. we conducted rigorous experiments on two
datasets and demonstrated the effectiveness of our method. our contributions
include:1) proposing a novel general framework to address the unique
"patch-slidepatient" ml-mil problem in the medical field. before this, no other
framework had directly tackled this specific problem, making our proposal a
ground-breaking step in the application of ml-mil in healthcare; 2) proposing a
simple yet highly effective method that leverages self-attention mechanisms and
transformer models to enhance the interaction between slide and patient
information. this innovative approach not only improves the classification
performance at the patient level but also at the slide level, showcasing its
effectiveness and versatility; 3) conducting extensive experiments on two
separate datasets. our method was seamlessly integrated with two prior
state-of-the-art methods, demonstrating its compatibility and adaptability. the
experiments resulted in improved performance, indicating that our method
enhances the efficacy of these existing approaches.
our proposed method p&sre is illustrated in fig. 2. specifically, the framework
consists of two parts. the first part is the slide-patch level mil based on a
state-of-the-art mil method. the second part is the patient-slide level mil,
which generates patient-level features using attention mechanism and interacts
the features with transformer. to enhance readability, we first provide the
following symbolization for ml-mil: for a patient x i , it has a patient-level
classification label y i . for patient x i , there may exist n i slides s i ={s
j |j=1 to n i }, where the classification label for each slide s j is denoted as
z j . for each slide s j , it may be divided into m j patches p j ={p k |k=1 to
m j }. here, i,j, and k are indices for patient, slide, and patch levels,
respectively. our proposed framework has strong scalability as it can be based
on any attention-based mil method. therefore, we directly use the
state-of-the-art (sota) mil methods, abmil [8] and dsmil [9] for the slide-patch
stage. these two methods differ in their attention computing approach for each
patch.for abmil, the attention of each patch is computed by an mlp.
specifically, for m j patches p k , an encoder is applied to obtain the patch
feature matrix f i , where,f i ∈ r mj ×1024 . then, f i is passed through an fc
layer followed by a tanh activation and another fc layer followed by a sigmoid
activation to obtain two feature matrices, f i and f i , both ∈ r mj ×128 .
these matrices are elementwise multiplied and then passed through an fc layer to
obtain the weight of each patch, ω k .for dsmil, the attention of each patch is
based on the cosine distance between instances and key instances. first, an fc
layer is applied to the patch feature matrix f i to obtain the importance score
θ k for each patch. the patch with the highest score is selected as the key
instance. then, the feature matrix f i is mapped to a matrix q i ∈ r mj ×128 and
the cosine similarity between all instances and the key instance is computed as
the weight of each patch, ω k .although abmil and dsmil compute attention
differently, both methods compute the attention-weighted sum of patch instances
features as the bag representation of the slide. therefore, the slide feature
output by both methods can be generalized as:finally, we obtain the feature
vector set h i ={h j |j=1 to n i } for all slides {s j } of patientx i through
patch-slide mil.
after performing patch-slide level mil, we move on to patient-slide level mil.
in general mil algorithms, the patient is regarded as the bag and the slide as
the instance. however, considering the diagnostic process in clinical practice,
we propose to treat both patients and slides as instances at the same level.
specifically, our p&sre framework for patient-slide level consists of two parts:
patient-level feature generation based on self-attention and patient-slide
feature interaction based on transformer [11].patient-level feature generation
based on self-attention. doctors usually select certain key slides for careful
observation and information aggregation during diagnosis, similar to the
self-attention mechanism. therefore, we directly use a fully connected (fc)
layer to integrate the feature-level features into patient-level features v i
through attention mechanism, serving as patient instances. specifically, given
the feature vector collection {h j } from multiple slides in the previous step,
we input it to the fc layer and apply the sigmoid activation function to output
the weight α j for each h j . then, we perform a weighted average of the vectors
based on this weight to obtain the patient feature v i :patient-slide feature
interaction based on transformer. this process is where our framework shines.
after doctors summarize the patient-level results, they typically review the
slides to double-check the diagnosis results. this patient-slide feature
interaction (psfi) naturally lends itself to the construction of a transformer,
and information exchange and integration between slides and patient level are
bidirectional. thus, self-attention is more ideal for this purpose than other
kinds of attention (such as cross-attention or doctors' attention). by using the
self-attention-based transformer structure, each input token is treated equally
(i.e., viewed as the same instance level), and tokens can interact extensively
with each other, enabling mutual correction between patients and slides and even
between slides. specifically, we merge the slide feature set {h j } and the
patient feature v i into the input tokensand then input them into a multi-layer
transformer through self-attention and feed-forward neural network layers to
obtain the interaction information between slides and output tokens t out i
:where d is the dimension of the token, and t k and t l come from t in i . β k,l
is multi-head attention matrix, and w q , w k , and w v are weight matrices of
query, key, and value, respectively. w r and w o are transformation matrices. b
1 and b 2 are bias vectors. this update procedure is repeated for l layers,
where the t k are fed to the successive transformer layer. finally, we obtain
the output tokensthen, all output tokens are input into a shared fc layer, and
the patient's predicted logits y i and the predicted classification logits {z j
|j = 1 to n i } for each slide are output.training progress and loss function.
during training, we sampled one patient at a time and pre-extracted their
batch-level features for all slides, in order to save gpu memory. due to the
issue of class imbalance in both slide level and patient level, we use the lade
[7] loss function.
cd-itb dataset. cd-itb is a private dataset consisting of 853 slides from 163
patients, with binary patient-level labels of cd or itb in a ratio of 103:60 and
tri-class slide-level labels of cd, itb, and normal slides in a ratio of
436:121:296, respectively. on average, there were 5 slides per patient. the
slides were scanned at a magnification of 40× (0.25 µm/px), and annotations were
curated by experienced pathologists. we adopted a patient-level stratification
approach for 5-fold cross-validation, with 20% of the training set randomly
assigned as the validation set for each fold. the dataset comprises an average
of 2.3k instances per bag, with the largest bag containing over 16k
instances.camelyon17 dataset. camelyon17 [1] is a publicly dataset, and its
training set comprises 500 slides from 100 breast cancer patients with lymph
node metastases. the slides are classified into four distinct categories, namely
negative, itc, micro, and macro, in proportions of 318:36:59:87, respectively.
there were 5 slides per patient on average. the patients are divided into two
groups based on their pn stage, namely lymph node positive and lymph node
negative, in proportions of 24:76, respectively. the data folding method is the
same as the cd-itb dataset. the average number of instances per bag is
approximately 6.1k, and the largest bag contains over 23k instances.metrics. we
report class-wise weighted accuracy (acc), precision(pre), recall, and f1-score
(f1). to avoid randomness, we run all experiments five times and report the
averaged metrics.
we utilized resnet50, which was pre-trained on imagenet1k, to extract features
from patches. each patch was of size 512 × 512 pixels. for both abmil and dsmil
networks, we kept the original parameters for the number of channels at each
layer. following the reference [4], we employed a transformer with 8 heads and 8
layers in the patient-slide feature interactions. all networks are implemented
using pytorch and trained on a nvidia rtx titan gpu with 24 gb memory. we
employed two adam optimizers with a maximum learning rate of 1e-4 and a cosine
annealing update strategy that gradually decreased the learning rate to 1e-12
over 300 epochs.
we compared our strategy with two state-of-the-art mil methods to evaluate its
performance. to investigate the impact of self-attention and transformers on
slide-level and case-level results, we conducted ablation experiments: "abmil +
p&sre (with/without psfi)" and "dsmil + p&sre (with/without psfi)",
respectively. for slide-level classification, we used mean pooling and max
pooling to pool feature vectors of patches into a representative vector for the
slide, which was then fed into a fully connected layer for classification. at
the patient level, we used two approaches for prediction: maxs, where the
feature of the instance that achieves the maximum positive probability from the
slide-level mil model is selected to patient-level model, and maxmins, where the
mean value of features of the maximum and minimum positive probability from the
slide-level mil model is selected to patient-level model.the results of 5-fold
cv at the slide and patient levels are reported in table 1 and table 2,
respectively. our p&sre framework improves both abmil and dsmil methods at both
levels. abmil with p&sre improves the f1 score from 0.565 to 0.579 for the
cd-itb dataset and from 0.529 to 0.571 for the camelyon17 dataset at the
slide-level, and improves the f1 score from 0.522 to 0.599 for the cd-itb
dataset and from 0.842 to 0.861 for the camelyon17 dataset at the patient-level.
therefore, the ablation experiments demonstrate the effectiveness of p&sre in
enhancing the classification performance at both the slide and patient levels.
our study has some limitations that should be addressed. for instance, we did
not explore the possibility of treating patches as an equivalent level to slides
and patients. the primary reason is that the vast number of patches required for
analysis is significantly larger than that of slides and patients, which
presents a computational challenge for training. as a result, we have not yet
explored this avenue. in the future, we plan to leverage clustering and active
learning methods to reduce the number of patches and enable the interaction of
all three levels with the transformer, which would further enhance the accuracy
and efficiency of our proposed method.
this study proposes a highly scalable and versatile framework to address m-mil
problems. we first classify the process from patch to slide to the patient in
medical pathology diagnosis as a multi-level mil problem. based on existing
state-of-the-art mil methods, we then extend the framework to p&sre, which
conducts feature extraction and interaction at the slide-patient level. by
introducing a transformer, the framework enables iterative interaction and
correction of information between patients and slides, resulting in better
performance at both the patient level and slide level compared to existing
state-of-the-art algorithms on two validation datasets.
deep neural networks have recently shown impressive performance on lesion
quantification in positron emission tomography (pet) images [6]; however, they
usually rely on a large amount of well-annotated, diverse data for model
training. this is difficult or even infeasible for some applications such as
lesion identification in neuroendocrine tumor (net) images, because nets are
rare tumors and lesion annotation in low-resolution, noisy pet images is
expensive. to address the data shortage issue, we propose to train a deep model
for lesion detection with synthesized pet images generated from list mode pet
data, which is low-cost and does not require human effort for manual data
annotation.synthesized pet images may exhibit a different data distribution from
real clinical images (see fig. 1), i.e., a domain shift, which can pose
significant challenges to model generalization. to address domain shifts, domain
adaptation requires access to target data for model training [5,29], while
domain generalization (dg) trains a model with only source data [39] and has
recently attracted increasing attention in medical imaging [1,13,15,18]. most of
current dg methods rely on multiple sources of data to learn a generalizable
model, i.e., multisource dg (mdg); however, multi-source data collection is
often difficult in real practice due to privacy concerns or budget deficits.
although single-source dg (sdg) using only one source dataset has been applied
to medical images [12,14,32], very few studies focus on sdg with pet imaging and
the current sdg methods may not be suitable for lesion identification on pet
data. for instance, many existing methods use a complicated, multi-stage model
design pipeline [10,23,30], which introduces an additional layer of algorithm
variability. this situation will become worse for pet images, which typically
have a poor signal-to-noise ratio and low spatial resolution. several other sdg
approaches [26,31,34] leverage unique characteristics of the imaging modalities,
e.g., color spectrum of histological stained images, which are not applicable to
pet data.in this paper, we propose a novel single-stage sdg framework, which
learns with human annotation-free, list mode-synthesized pet images for
generalizable lesion detection in real clinical data. compared with domain
adaptation and mdg, the proposed method, while more challenging, is quite
practical for real applications due to the relatively cheaper net data
collection and annotation. specifically, we design a new data augmentation
module, which generates out-of-domain samples from single-source data with
multi-scale random convolutions. we integrate this module into a deep lesion
detection neural network and introduce a cross-domain consistency constraint for
feature encoding between original synthesized and augmented images. furthermore,
we incorporate a novel patch-based gradient reversal mechanism into the network
and accomplish a pretext task of domain classification, which explicitly
promotes domain-invariant, generalizable representation learning. trained with a
single-source synthesized dataset, the proposed method provides superior
performance of hepatic lesion detection in multiple cross-scanner real clinical
pet image datasets, compared with the reference baseline and recent
state-of-the-art sdg methods.
figure 2 presents the proposed sdg framework. given a source-domain dataset of
list mode-synthesized 3d pet images and corresponding lesion labels (x s , y s
), the goal of the framework is to learn a lesion detection model h , composed
of e and d, which generalizes to real clinical pet image data. the framework
first feeds synthesized images x s into a random-convolution data augmentation
module a and generates out-of-domain samples x a = a(x s ). then, it provides
both original and augmented images, x s and x a , to a feature encoder e , which
is followed by a decoder d for lesion detection. the framework imposes a
crossdomain consistency constraint on the encoder e to promote consistent
feature learning between x s and x a . meanwhile, it uses a patch gradient
reversalbased domain classifier c to differentiate x a from x s and further
encourages the encoder e to learn domain-agnostic representations for h .
in the synthesized pet image dataset, each subject have multiple simulated
lesions of varying size with known boundaries [11], and thus no human annotation
is required. however, this synthesized dataset presents a significant domain
shift from real clinical data, as they have markedly different image textures
and voxel intensity values (see fig. 1). inspired by previous domain
generalization work [39], we introduce a specific data augmentation module to
generate out-of-domain samples from this single-source synthesized dataset for
generalizable model learning (see fig. 2). specifically, we tailor a random
convolution technique [33] for synthesized pet image augmentation with the
following substantial improvement: 1) extend it from single value-prediction
image classification to a more challenging dense prediction task of lesion
detection; 2) refine it to produce realistic augmented images where the organ
regions are brighter than image background, instead of randomly switching the
foreground and background intensity values; 3) place a cross-domain consistency
constraint on the encoding features, rather than output predictions, of original
synthesized and augmented images, so as to directly encourage consistent
representation learning between the source and other domains. this module can
preserve global shapes or the structure of objects (e.g., lesions and livers) in
images but distorts local textures, so that the lesion detection model learned
with these augmented images can generalize to unseen real-world pet image data,
which typically have high lesion heterogeneity and divergent texture
styles.given a synthesized input image x s ∈ x s , our data augmentation module
a first performs a random convolution operation r(x s ) with a k × k kernel r,
where the kernel size k and the convolutional weights are randomly sampled from
a multi-scale set k = {1, 3, 5, 7} and a normal distribution n (0, 1/k 2 ),
respectively. then, inspired by [7,35,36], we mix r(x s ) and x s to generate a
new mixed image x m via a convex combination,where α ∈ [0, 1] is randomly
sampled from a uniform distribution u(0, 1). this data mixing strategy allows
continuous interpolation between the source domain and a randomly generated
out-of-distribution domain to improve model generalizability. finally, if the
foreground (i.e., lesion region) of x m has a higher mean intensity value than
the background (non-lesion region), we use x m as the final augmented image, x a
= x m . otherwise, we invert the image intensity of x m to obtainis the
maximum/minimum intensity of x m and 1 is a matrix with all elements being one
and the same dimension as x m . this intensity inversion operation is to ensure
the lesion region has higher intensity values than other regions, mimicking the
image characteristics of real-world pet data in our experiments. here we
calculate the mean intensity value of the background from the regions that have
a distance greater than half of the image width from the closest lesion
center.in our modeling, for each synthesized training image x s , we generate
multiple augmented images (i.e., 3), {x i a } 3 i=1 , and feed them into the
encoder e for feature learning. due to the distance preservation property of
random convolutions [33], the module a changes local textures but preserves
object shapes at different scales, and thus x s and {x i a } 3 i=1 should have
identical semantic content, such as lesion presence, quantity and positions.
therefore, they should have consistent representations in the feature space,
i.e., e (x s ) ≈ e (x i a ), i = 1, 2, 3. to this end, we place a cross-domain
consistency loss l con on top of the encoder e aswhere e is an expectation
operator, |e (x s )| is the number of elements in e (x s ), and || • || f
denotes the frobenius norm. unlike the previously reported work [33] promotes
consistent output-layer predictions, the loss l con in eq. ( 1) directly
encourages the encoder e to extract cross-domain consistent representations,
which improves model generalization more effectively for dense prediction tasks
[8], such as lesion detection. we hypothesize that forcing similar feature
encoding between x s and x i a can facilitate image content preservation for
lesion detection. in addition, we adopt a mean squared error (mse) to measure
the consistency, different from [33] using the kullback-leibler divergence for
image classification, which is not suitable for our application. note that the
convolution weights in module a are randomly sampled within each iteration and
are not updated during model training.
because of random convolution weights, the original synthesized x s and
augmented x a data can have substantially different image appearances.
consequently, the use of the loss l con in eq. (1) may not be sufficient to
enforce consistent feature encoding. to address this issue, we propose to use a
pretext task as an additional information resource for the encoder e and to
further promote domain-agnostic representation learning. specifically, we
incorporate a domain classifier c on top of the encoder e to perform a pretext
task of domain discrimination, i.e., predict whether each input image is from
the original synthesized data x s or augmented data x a . this domain
classification accompanies the main task of lesion detection (see fig. 2) to
assist with feature learning. in this way, the encoder e improves feature
invariance to domain changes by penalizing domain classification accuracy, while
retaining feature discriminativeness to lesion prediction via the decoder d.
this is different from other methods [2,25] that use intrinsic supervision
signals within a single image to perform an auxiliary task, e.g., solving jigsaw
puzzles, for model generalization enhancement.in general, the classifier c will
encourage the encoder e to learn discriminative features for accurate domain
classification. in order to make features invariant to different domains, we
reverse the gradient propagated from the domain classifier c with a
multiplication of -1 [3] and send this reversed gradient to the encoder e ,
while keeping all the other gradient flows unchanged during the backpropagation
for model training. note that the computation in forward propagation of our
network is the same as that in a standard feed-forward neural network. compared
with [3], we make the following significant improvements: 1) instead of back
propagating the reversed gradient from a single-valued prediction of the domain
label of the entire input image, we introduce a patch-based gradient reversal to
enhance feature representation invariance to local texture or style changes.
inspired by [9], we design the domain classifier c with a fully convolutional
network and produce a prediction map, where each element corresponds to a local
patch of input image, i.e., conducting small patch categorization. we then apply
the reversal operation to the gradient propagated from the prediction map and
feed it into the encoder e for feature learning. 2) motivated by [17], we remove
the sigmoid layer in [3] and replace the cross-entropy loss by an mse loss,
which can facilitate the adversarial training caused by the gradient reversal.
with the mse loss, the patch-based gradient reversal penalizes image structures
and enhances feature robustness and invariance to style shifts at the
local-patch level, so that the lesion detection model h (i.e., e followed by d)
learned with source data annotations is directly applicable to unseen domains
[4,24], based on the covariate shift assumption [20].formally, let x = {x s , x
a } denote the input data for the encoder e and z = {z s , z a } represent the
corresponding domain category labels, with z s and z a for the original source
images x s and corresponding random convolutionaugmented image x a ,
respectively. each label z ∈ z is a 3d image with all voxel intensity being 0's
for z ∈ z s or 1's for z ∈ z a . we define the domain classification objective l
cls as followswhere ẑ = c (e (x )) is the prediction of x . for source-domain
data (x s , y s ), the augmented images x a have the same gold-standard lesion
labels y a = y s , each of which is a 3d binary image with 1 s for lesion voxels
and 0 s for non-lesion regions. let y = {y s , y a }. we formulate the lesion
detection objective l det aswhere the first and second terms in eq. ( 4) are a
weighted binary cross-entropy loss and a dice loss, respectively. we add a
smooth term, = 10 -6 , to the dice loss to avoid division by zero. the y j and
ŷj are the j-th values of y and corresponding prediction ŷ, respectively. the β
controls the relative importance between the two losses, and γ emphasizes the
lesions in each image. the combo loss l det can further help address the data
imbalance issue [22], i.e., lesions have significantly fewer voxels than the
non-lesion regions including the background. with the losses in eqs. ( 1)-( 4),
we define the following full objective aswhere λ con and λ cls are weighting
parameters. note that while we minimize l for model training, we reverse the
gradient propagated from the domain classifier c before sending it to the
encoder e during the backpropagation.
datasets. we evaluate the proposed method with multiple 68 ga-dotatate pet liver
net image datasets that are acquired using different pet/ct scanners and/or
imaging protocols. the synthesized source-domain dataset contains 103 simulated
subjects, with an average of 5 lesions and 153 transverse slices per subject.
this dataset is synthesized using list mode data from a single real, healthy
subject acquired on a ge discovery mi pet/ct scanner with list mode
reconstruction [11,37]. we collect two additional real 68 ga-dotatate pet liver
net image datasets that serve as unseen domains. the first dataset (real1) has
123 real subjects with about 230 hepatic lesions in total and is acquired using
clinical reconstructions with a photomultiplier tube-based pet/ct scanner (ge
discovery ste). the second real-world dataset (real2) consists of 65 cases with
around 113 lesions and is acquired from clinical reconstructions using a digital
pet/ct scanner (ge discovery mi). following [28,38], we randomly split the
synthesized dataset and the real1 dataset into 60%, 20% and 20% for training,
validation and testing, respectively. due to the relatively small size of real2,
we use a two-fold cross-validation for model evaluation on this dataset. here we
split the real datasets to learn fully supervised models for a comparison with
the proposed method. implementation details and evaluation metrics. we implement
the encoder e and the decoder d with a u-net architecture [19], with four
downsampling and upsampling layers in the encoder and decoder, respectively. we
build the domain classifier c using three stacked stride-1 convolutional layers
of kernel size of 4, and each convolution is followed by a batch normalization
and a leaky relu activation [16]. we set β = 6, γ = 5 in eq. ( 4) and λ con = 1,
λ cls = 1 in eq. (5). we train the model using stochastic gradient descent with
nesterov momentum with learning rate = 5 × 10 -4 , momentum = 0.99 and batch
size = 1. we perform standard image augmentation including random scaling, noise
adding and image contrast adjustment before applying random convolutions in the
module a. in the testing stage, we adopt the model h to produce a prediction map
for each input image, and identify lesions with a threshold (i.e., 0.1) to
binarize the map followed by a connected component analysis, which helps detect
individual lesions by identifying connected regions from the binarized map. we
use precision, recall and f 1 score as model evaluation metrics
[21,28,38].comparison with state of the art. we compare our method with several
recent state-of-the-art sdg approaches, including causality-inspired sdg (cisdg)
[18], randconv [33], and learning to diversify (l2d) [27]. we run each model 5
times with different random seeds and report the mean and standard deviation.
table 1 presents the comparison results on the two unseen-domain datasets. our
method significantly outperforms the state-of-the-art approaches in terms of f 1
score, with p-value < 0.05 in student's t-test for almost all cases on both
datasets. in addition, our method gives lower standard deviation of f 1 than
others. this indicates that compared with the competitor approaches, our method
is relatively more effective and stable in learning generalizable
representations for lesion detection in a very challenging situation, i.e.,
learning with a single-source synthesized pet image dataset to generalize to
real clinical data.the qualitative results are provided in the supplementary
material.ablation study. in table 1, the baseline represents a lesion detection
model trained with the source data but without the data augmentation module a, l
con or l cls . we then evaluate different variants of our method by sequentially
adding one component to the baseline model: 1) using only the module a for model
training (aug.), 2) using module a and l con (aug.+l con ), and 3) using module
a, l con and l cls (ours). we also report the performance of the model, aug.+l
con +ggr, which does not use the proposed patch-based gradient reversal but
outputs a single-value prediction for the entire input image, i.e., global
gradient reversal (ggr). the u pper-bound means training with real-world images
and gold-standard labels from the testing datasets. we note that using the data
augmentation module a can significantly improve the lesion detection performance
compared with the baseline on the real1 dataset, and combining data augmentation
and patch gradient reversal can further close the gap to the u pper-bound model.
our method also outperforms the baseline model by a large margin on the real2
dataset, suggesting the effectiveness of our method. effects of parameters. we
evaluate the effects of λ con and λ cls of our method on lesion detection in
fig. 3. the lesion detection performance improves when increasing λ con from 0.1
to 1. however, a further emphasis on consistent feature encoding, e.g., λ con ≥
5, decreases the f 1 score. this suggests the importance of an appropriate λ con
value. in addition, we observe a similar trend of the f 1 curve for the λ cls ,
especially for the real1 dataset, and this indicates the necessity of the domain
classification pretext task.
we propose a novel sdg framework that uses only a single dataset for hepatic
lesion detection in real clinical pet images, without any human data
annotations. with a specific data augmentation module and a new patch-based
gradient reversal, the framework can learn domain-invariant representations and
generalize to unseen domains. the experiments show that our method outperforms
the reference baseline and recent state-of-the-art sdg approaches on
cross-scanner or -protocol real pet image datasets. a potential limitation may
be the need of a proper selection of weights for different tasks during model
training.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9 12.
identifying unusual patterns in data is of great interest in many applications
such as medical diagnosis, industrial defect inspection, or financial fraud
detection. finding anomalies in medical images is especially hard due to large
inter-patient variance of normality, the irregular appearance-, and often rare
occurrence of diseases. therefore, it is difficult and expensive to collect
large amounts of annotated samples that cover the full abnormality spectrum,
with supervised [7,15] fig. 1. morphaeus outperforms aes by generating id
reconstructions even for far ood cases (fig. 1a), enabling accurate pathology
localization (figure 1b). and self-supervised [10,16] methods only capturing
limited facets of the abnormal distribution [31]. however, since it is more
feasible to obtain large data sets with normal samples, it is common to detect
outliers by detecting patterns that deviate from the expected normative
distribution.reconstruction-based aes have emerged as a very popular framework
for unsupervised anomaly detection and are widely adopted in medical imaging
[2]. they provide straight-forward residual error maps, which are essential for
safetycritical domains such as medical image analysis. however, recent work
suggests that aes might reconstruct out-of-distribution (ood) samples even
better than in-distribution (id) samples [28], with the learned likelihood being
dominated by common low-level features [32]. while this can be useful for some
tasks such as reconstruction [34], or restoration [21], it often fails for
pathology detection as anomalies can be missed due to small residual errors. in
fig. 1 we show that aes that have only been trained on healthy chest x-rays are
also able to reconstruct ood samples like pathologies or hands. similarly,
perera et al. [24] showed that aes trained on the digit 8 can also reconstruct
digits 1,5,6 and 9.much effort has been made in the medical imaging community to
improve the limitations of traditional anomaly detection methods, particularly
in the context of brain mri. apart from the reduced dimensionality of the
bottleneck, several other techniques have been introduced to regularize aes
[11,20,29,38]. recently, self-supervised denoising aes [16] achieved sota
results on brain pathology segmentation. they explicitly feed noise-corrupted
inputs x = x+ to the network with the aim at reconstructing the original input
x. however, this is especially beneficial when the anomaly distribution is known
a priori. variational aes (vaes) [5,12,17,40] estimate the distribution over the
latent space that is regularized to be similar to a prior distribution, usually
a standard isotropic gaussian. generative adversarial networks have also been
applied to anomaly detection [24,33]. pidhorskyi et al. [26] trained aes with an
adversarial loss to detect ood samples. more recently, introspective variational
aes [8] use the vae encoder to differentiate between real and reconstructed
samples, achieving sota image generations and outlier detection performance.
recently, zhou et al. [39] investigated the limitations of aes for ood.
similarly, we believe that aes should have two properties: i) minimality: the
networks should be constrained to only reconstruct id samples and ii)
sufficiency: the decoder should have sufficient capacity to reconstruct id
samples with high accuracy. in contrast to [39], where the authors aim at
reconstructing only low-dimensional features needed for the classification task,
we are interested in reconstructing pseudo-healthy images to enable pixel-wise
localization of anomalies.in this work, we first investigate whether sota aes
can learn meaningful representations for anomaly detection. specifically, we
investigate whether aes can learn the healthy anatomy, i.e., absence of
pathology, and generate pseudohealthy reconstructions of abnormal samples on
challenging medical anomaly detection tasks. our findings are that sota aes
either do not efficiently constrain the latent space and allow the
reconstruction of anomalous patterns, or that the decoder cannot accurately
restore images from their latent representation. the imperfect reconstructions
yield high residual errors on normal regions (false positives) that can easily
overshadow residuals of interest, i.e., pathology [23]. we then propose
morphaeus, novel deformable aes to learn minimal and sufficient features for
anomaly detection and drastically reduce false positives. figure 1a shows that
morphaeus learns the training distribution of healthy chest x-rays and yields
pseudo-healthy id reconstructions even for far ood samples. this allows to
localize pathologies, as seen in fig. 1b.our manuscript advances the
understanding of anomaly detection by providing insights into what aes learn. in
summary, our contributions are:• we broaden the understanding of aes and
highlight their limitations.• we test whether sota aes can learn the training
distribution of the healthy population, accurately reconstruct inputs from their
latent representation and reliably detect anomalies. • as a solution, we propose
morphaeus, novel deformable aes that provide pseudo-healthy reconstructions of
abnormal samples and drastically reduce false positives, achieving sota
unsupervised pathology detection.
the widely held popular belief is that aes can learn the distribution of the
training data and identify outliers from inaccurate reconstructions of abnormal
samples [31]. this section aims to discuss the common assumptions of
unsupervised anomaly detection, specifically for aes, while also outlining the
challenges and desired properties associated with these techniques.
let x ⊂ r n be the data space that describes normal instances for a given
task.the manifold hypothesis implies that there exists a low-dimensional
manifold m ⊂ r d ⊂ x where all the points x ∈ x lie, with d n [9]. for example,
a set of images in pixel space x could have a compact representation describing
features like structure, shape, or orientation in m.given a set of unlabeled
data x 1 , .., x n ∈ x the objective of unsupervised representation learning is
to find a function f : r n → r d and its inverse g : r d → r n , such that x ≈
g(f (x)), with the mapping f defining the lowdimensional manifold m. the core
assumption of unsupervised anomaly detection is that once such functions f and g
are found, the learned manifold m would best describe the normal data samples in
x and results in high reconstruction errors for data-points x / ∈ x , that we
call anomalous. an anomaly score is therefore usually derived directly from the
pixel-wise difference:the nominal and abnormal distributions are considerably
separated from each other when x is from a different domain. however, anomalies
are often defects in otherwise normal images. in medical imaging, the set x
describes the healthy anatomy and the data set x usually contains images with
both healthy and pathological regions. the two distributions usually come from
the same domain and might overlap considerably. the core assumption is that only
the normal structures can be reconstructed from their latent representation very
well, with the pathological regions ideally replaced by healthy structures.
therefore x ≈ g(f (x)) ∈ x would represent the healthy synthesis of the abnormal
sample x and the residual |x -g(f (x))| would highlight only the abnormal
regions.
aes aim to extract meaningful representations from data, by learning to compress
inputs to a lower-dimensional manifold and reconstruct them with minimal error.
they use neural networks to learn the functions f and g, often denoted as
encoder e θ with parameters θ and decoder d φ parameterized by a set of
parameters φ. the embedding z = e(x|θ) is a projection of the input to a
lower-dimensional manifold z, also referred to as the bottleneck or latent
representation of x. the standard objective of aes is finding the set of
parameters θ and φ that minimize the residual, with the mean squared error (mse)
being a popular choice for the reconstruction error:in the introduction, we
presented the desired properties of aes for outlier detection, namely i)
reconstructions should match the training distribution and ii) decoders have
sufficient capacity to accurately restore inputs. figure 2 shows reconstructions
of spatial aes [2] for near ood samples, i.e., containing pathologies, and far
ood samples using real images of celebrities [19]. aes with few encoding layers
learn to copy and can reconstruct both near-and far ood. interestingly, with
increasing layer depth, aes can learn the prior over the training distribution,
avoid the reconstruction of pathologies, and project ood celebrities to the
closest chest x-ray counterparts. however, this comes at the cost of losing
spatial information and not reconstructing small details, e.g., ribs. the
resulting high residual error on healthy tissues (false positives) overshadows
the error on pathology [23], rendering standard aes unsuitable for anomaly
detection.
we propose morphaeus, deformable aes that learn minimal and sufficient features
for anomaly detection, see fig. 3. we use deep perceptual ae to provide
pseudo-healthy id reconstructions and leverage estimated deep deformation fields
to drastically reduce the false positives.pseudo-healthy reconstructions. given
a dataset x = {x 1 , .., x n } we optimize the encoder and decoder with
parameters θ, φ to minimize the mse loss between the input and its
reconstruction. for minimality, we propose to use deep aes constrained to
reconstruct only id samples (see fig. 2), but add a perceptual loss (pl) [14,37]
to encourage reconstructions that are perceptually similar to the training
distribution. the reconstruction loss is given by:with x rec = d φ (e θ (x)), p
l(x, x rec ) = l (v gg l (x)-v gg l (x rec )) 2 with v gg l being the output of
the l ∈ {1, 6, 11, 20}-th layer of a pre-trained vgg-19 encoder. we have
empirically found α = 0.05 to be a good weight to predict perceptually similar
reconstructions without compromising pixel-wise accuracy.local deformation.
imperfect reconstructions yield high residuals on normal regions which might
overshadow the residuals errors associated with anomalous regions. skip
connections [30] would allow the network to bypass the learned manifold and copy
anomalies at inference. instead, we propose to align the reconstruction with the
input using corresponding encoder and decoder features. we denote these shared
parameters as θ s ⊂ θ and φ s ⊂ φ. inspired by the advances in image
registration [1] and computer vision [4,36] we estimate dense deformation fields
φ to allow local morphometric adaptations:where x morph = x rec • φ, ψ are the
deformation parameters, lncc is the local normalized cross correlation, • is a
spatial transformer and β weights the smoothness constraint on the deformation
fields. we opted for the lncc instead of the mse to emphasize shape registration
and enhance robustness to intensity variations in the inputs and
reconstructions. by sharing encoder/decoder parameters, the deformation maps are
not only beneficial at inference time, but also guide the training process to
learn more accurate features. the full objective is given by the two losses:to
ensure a good initialization for the deformation estimation, we introduce the
deformation loss after 10 epochs. deformable registration between normal and
pathological samples is in itself an active area of research [18,25]. in
particular, the deformation could mask structural abnormalities at inference if
not constrained. in our experiments, we linearly increase β from 1e -3 to 3 to
constrain the deformation more as the reconstruction improves (see appendix for
details). nevertheless, recent advances allow the estimation of the optimal
registration parameters automatically [13]. if not specified otherwise, we
employ x morph for inference.
in this section, we investigate whether aes can learn the healthy anatomy, i.e.,
absence of pathology, and generate pseudo-healthy reconstructions of abnormal
chest x-ray images. pathology detection algorithms are often applied to finding
hyper-intense lesions, such as tumors or multiple sclerosis on brain scans.
however, it has been shown that thresholding techniques can outperform
learningbased methods [22]. in contrast, the detection of pathology on chest
radiographs is much more difficult due to the high variability and complexity of
nominal features and the diversity and irregularity of abnormalities.datasets.
we use the covid-19 radiography database on kaggle [6,27]. we used the rsna
dataset [35], which contains 10k cxr images of normal subjects and 6k lung
opacity cases. for the detection of covid-19, we used the padchest dataset [3]
containing cxr images manually annotated by trained radiologists. we used 1.3k
healthy control images and 2.5k cases of covid-19.results. of the baselines,
only adversarially-trained aes can reconstruct pseudo-healthy images from
abnormal samples, as shown in fig. 4. however, their imperfect reconstructions
overshadow the error on pathology leading to poor anomaly detection results, as
reflected in the ssim and auroc in table 1. spatial aes and daes have a tendency
to reproduce the input and produce reconstructions of structures that are not
included in the training distribution, such as medical devices and pathologies,
despite not being trained on ood data. this can lead to false negatives. daes
can avoid the reconstruction of some pathologies and achieve good anomaly
detection results. however, pathological regions that do not conform to the
learned noise model are completely missed, as shown in fig. 4. the next three
methods (ae-d, vae, and β-vae) produce blurry reconstructions, as it can be best
seen in the lpips score. morphaeus is the only method to yield accurate
pseudo-healthy reconstructions and effectively remove anomalies, such as
pathology or implanted medical devices. this enables to precisely localize
pathologies, considerably outperforming the baselines.ablation study: importance
of morphological adaptations. we evaluate the effectiveness of individual
components of morphaeus in fig. 5. aes without perceptual loss tend to not
reconstruct small but important features such as ribs and yield false positives
on healthy tissue. interestingly, aes with perceptual loss achieve more visually
appealing reconstructions, but fail at detecting anomalies because the
pathological region is overshadowed by false positive residuals on edges and
misaligned ribs. morphometric adaptations guide the networks to learn better
representations, reduce the number of false positives, and enable the
localization of pathologies. this considerably improves the detection results to
84.8 and 82.1 for aes with and without perceptual loss, respectively. it is
important to note that the deformations are not only beneficial at the time of
inference, but also drive the learning process towards better representations.
thereby, the average pathology detection increases from 66.8 to 80.2, even if no
adaptations are made during inference, i.e., using x rec instead of x morph for
inference (see appendix for details).
in this work, we have investigated whether aes learn meaningful representations
to solve pathology detection tasks. we stipulate that aes should have the
desired property of learning the normative distribution (minimality) and
producing highly accurate reconstructions of id samples (sufficiency). we have
shown that standard, variational, and recent adversarial aes generally do not
satisfy both conditions, nor are they very suitable for pathology detection
tasks where the distribution of normal and abnormal instances highly overlap.in
this paper, we introduced morphaeus, a novel deformable ae that demonstrated
notable performance improvement in detecting pathology. we believe our method is
adaptable to various anomaly types, and we are eager to extend our research to
different anatomies and imaging modalities, building upon promising early
experiments. however, it is important to address false positive detection, which
could be influenced by unlabelled artifacts like medical devices. our future
work aims to conduct a thorough analysis of false positives and explore
strategies to mitigate their impact, ultimately enhancing the accuracy.although
there are obstacles to overcome, aes remain a viable option for producing easily
understandable and interpretable outcomes. nevertheless, it is crucial to
continue improving the quality of the representations to advance unsupervised
anomaly detection. our findings demonstrate that morphaeus is capable of
learning superior representations, and can leverage the predicted dense
displacement fields to refine its predictions and minimize the occurrence of
false positives. this allows for accurate identification and localization of
diseases, resulting in sota unsupervised pathology detection on chest x-rays.
* 2.8 * 78.0 ± 0.5 76.1 ± 0.7 77.0 morphaeus (ours) 85.7 9.5 83.6 ±
pancreatic ductal adenocarcinoma (pdac) is one of the deadliest forms of human
cancer, with a 5-year survival rate of only 9% [16]. neoadjuvant chemotherapy
can increase the likelihood of achieving a margin-negative resection and avoid
unnecessary surgery in patients with aggressive tumor types [23]. providing
accurate and objective preoperative biomarkers is crucial for triaging patients
who are most likely to benefit from neoadjuvant chemotherapy. however, current
clinical markers such as larger tumor size and high carbohydrate antigen (ca)
19-9 level may not be sufficient to accurately tailor neoadjuvant treatment for
patients [19]. therefore, multi-phase contrast-enhanced ct has a great potential
to enable personalized prognostic prediction for pdac, leveraging its ability to
provide a wealth of texture information that can aid in the development of
accurate and effective prognostic models [2,10].previous studies have utilized
image texture analysis with hand-crafted features to predict the survival of
patients with pdacs [1], but the representational fig. 1. two examples of
spatial information between vessel (orange region) and tumor (green region). the
minimum distance, which refers to the closest distance between the superior
mesenteric artery (sma) and the pdac tumor region, is almost identical in these
two cases. we define the surface-to-surface distance based on point-to-surface
distance (weighted-average of red lines from ♦ to ) instead of point-to-point
distance (blue lines) to better capture the relationship between the tumor and
the perivascular tissue.here ♦ and are points sampled from subset vc and pc
defined in eq. power of these features may be limited. in recent years, deep
learning-based methods have shown promising results in prognosis models
[3,6,12]. however, pdacs differ significantly from the tumors in these studies.
a clinical investigation based on contrast-enhanced ct has revealed a dynamic
correlation between the internal stromal fractions of pdacs and their
surrounding vasculature [14]. therefore, focusing solely on the texture
information of the tumor itself may not be effective for the prognostic
prediction of pdac. it is necessary to incorporate tumor-vascular involvement
into the feature extraction process of the prognostic model. although some
studies have investigated tumor-vascular relationships [21,22], these methods
may not be sufficiently capable of capturing the complex dynamics between the
tumor and its environment.we propose a novel approach for measuring the relative
position relationship between the tumor and the vessel by explicitly using the
distance between them. typically, chamfer distance [7], hausdorff distance [8],
or other surfaceawareness metrics are used. however, as shown in fig. 1, these
point-to-point distances cannot differentiate the degree of tumor-vascular
invasion [18]. to address this limitation, we propose a learnable neural
distance that considers all relevant points on different surfaces and uses an
attention mechanism to compute a combined distance that is more suitable for
determining the degree of invasion. furthermore, to capture the tumor
enhancement patterns across multi-phase ct images, we are the first to combine
convolutional neural networks (cnn) and transformer [4] modules for extracting
the dynamic texture patterns of pdac and its surroundings. this approach takes
advantage of the visual transformer's adeptness in capturing long-distance
information compared to the cnn-onlybased framework in the original approach. by
incorporating texture information between pdac, pancreas, and peripancreatic
vessels, as well as the local tumor information captured by cnn, we aim to
improve the accuracy of our prognostic prediction model.in this study, we make
the following contributions: (1) we propose a novel approach for aiding survival
prediction in pdac by introducing a learnable neural distance that explicitly
evaluates the degree of vascular invasion between the tumor and its surrounding
vessels. (2) we introduce a texture-aware transformer block to enhance the
feature extraction approach, combining local and global information for
comprehensive texture information. we validate that the cross-attention is
utilized to capture cross-modality information and integrate it with in-modality
information, resulting in a more accurate and robust prognostic prediction model
for pdac. (3) through extensive evaluation and statistical analysis, we
demonstrate the effectiveness of our proposed method. the signature built from
our model remains statistically significant in multivariable analysis after
adjusting for established clinical predictors. our proposed model has the
potential to be used in combination with clinical factors for risk
stratification and treatment decisions for patients with pdac.
as shown in fig. 2, the proposed method consists of two main components. the
first component combines the cnn and transformer to enhance the extraction of
tumor dynamic texture features. the second component proposes a neural distance
metric between pdac and important vessels to assess their involvements.
recently, self-attention models, specifically vision transformers (vits [4]),
have emerged as an alternative to cnns in survival prediction [15,25]. our
proposed texture-aware transformer, inspired by mobilevit [13], aims to combine
both local information (such as pdac texture) and global information (such as
the relationship between pdac and the pancreas). this approach is different from
previous methods that rely solely on either cnn-based or transformer-based
backbones, focusing only on local or global information, respectively. the
texture-aware transformer (fig. 2) comprises three blocks, each consisting of a
texture-aware cnn block and a texture-aware self-attention block. these blocks
encode the input feature of an image f i ∈ r h×w ×d×c to the hidden feature f c
∈ r h×w ×d×c l using a 3 × 3 × 3 convolutional layer, followed by a we first
select related points set from the closest sub-surface on pdac and vessels
respectively. then we use a cross-attention block to obtain the neural distance.
finally, we concatenate features from three branches to obtain the survival
outcome oos.1 × 1 × 1 convolutional layer. the 3 × 3 × 3 convolution captures
local spatial information, while the 1 × 1 × 1 convolution maps the input tensor
to a higherdimensional space (i.e., c l > c). the texture-aware cnn block
downsamples the input, and the texture-aware self-attention block captures
long-range nonlocal dependencies through a patch-wise self-attention
mechanism.in the texture-aware self-attention block, the input feature f c is
divided into n non-overlapping 3d patches f u ∈ r v ×n ×cu , where v = hwd and n
= hw d/v is the number of patches, and h, w, d are the height, width, and depth
of a patch, respectively. for each voxel position within a patch, we apply a
multi-head self-attention block and a feed-forward block following [20] to
obtain the output feature f o . in this study, preoperative multi-phase ce-ct
pancreatic imaging includes the non-contrast phase, the pancreatic phase and
venous phase. therefore, we obtain three outputs from the transformer block with
the input of these phases, denoted asinstead of directly fusing the outputs as
in previous work, we employ a 3-way cross-attention block to extract
cross-modality information from these phases. the cross-attention is performed
on the concatenated self-attention matrix with an extra mask m ∈ {0, -∞} 3c×3c ,
defined as:here, q, k, v are the query, key, and value matrices, respectively,
obtained by linearly projecting the input f t o ∈ r 3c×d . the cross-modality
output f cross and in-modality output f t o are then concatenated and passed
through an average pooling layer to obtain the final output feature of the
texture branch, denoted as f t ∈ r ct .
between pdac and vesselsthe vascular involvement in patients with pdac affects
the resectability and treatment planning [5]. in this study, we investigate four
important vessels: portal vein and splenic vein (pvsv), superior mesenteric
artery (sma), superior mesenteric vein (smv), and truncus coeliacus (tc). we
used a semi-supervised nnunet model to segment pdac and the surrounding vessels,
following recent work [11,21]. we define a general distance between the surface
boundaries of pdac (p) and the aforementioned four types of vessels (v) as d(v,
p), which can be derived as follows:where v ∈ v and p ∈ p are points on the
surfaces of blood vessels and pdac, respectively. the point-to-surface distance
d ps (v, p) is the distance from a point v on v to p, defined as d ps (v, p) =
min p∈p v -p 2 2 , and vice versa. to numerically calculate the integrals in the
previous equation, we uniformly sample from the surfaces v and p to obtain the
sets v and p consisting of n v points and n p points, respectively. the distance
is then calculated between the two sets using the following equation:however,
the above distance treats all points equally and may not be flexible enough to
adapt to individualized prognostic predictions. therefore, we improve the above
equation in two ways. firstly, we focus on the sub-sets vc and pc of v and p,
respectively, which only contain the k closest points to the opposite surfaces p
and v, respectively. the sub-sets are defined as:secondly, we regard the entire
sets vc and pc as sequences and calculate the distance using a 2-way
cross-attention block (similar to eq. 1) to build a neural distance based on the
3d spatial coordinates of each point:neural distance allows for the flexible
assignment of weights to different points and is able to find positional
information that is more suitable for pdac prognosis prediction. in addition to
neural distance, we use the 3d-cnn model introduced in [22] to extract the
structural relationship between pdac and the vessels. specifically, we
concatenate each pdac-vessel pair x v s ∈ r 2×h×w ×d , where v ∈{pvsv, smv, sma,
tc} and obtain the structure feature f s ∈ r cs .finally, we concatenate the
features extracted from the two components and apply a fully-connected layer to
predict the survival outcome, denoted as o os , which is a value between 0 and
1. to optimize the proposed model, we use the negative log partial likelihood as
the survival loss [9].
dataset. in this study, we used data from shengjing hospital to train our method
with 892 patients, and data from three other centers, including guangdong
provincial people's hospital, tianjin medical university and sun yatsen
university cancer center for independent testing with 178 patients. the
contrast-enhanced ct protocol included non-contrast, pancreatic, and portal
venous phases. pdac masks for 340 patients were manually labeled by a
radiologist from shengjing hospital with 18 years of experience in pancreatic
cancer, while the rest were predicted using self-learning models [11,24] and
checked by the same annotator. other vessel masks were generated using the same
semisupervised segmentation models. c-index was used as our primary evaluation
metric for survival prediction. we also reported the survival auc, which
estimates the cumulative area under the roc curve for the first 36
months.implementation details: we used nested 5-fold cross-validation and
augmented the training data by rotating volumetric tumors in the axial direction
and randomly selecting cropped regions with random shifts. we also set the
output feature dimensions to c t = 64 for the texture-aware transformer, c s =
64 for the structure extraction and k = 32 for the neural distance. the batch
size was 16 and the maximum iteration was set to 1000 epochs, and we selected
the model with the best performance on the validation set during training for
testing. we implemented our experiments using pytorch 1.11 and trained the
models on a single nvidia 32g-v100 gpu.ablation study. we first evaluated the
performance of our proposed textureaware transformer (tat) by comparing it with
the resnet18 cnn backbone and vit transformer backbone, as shown in table 1. our
model leverages the strengths of both local and global information in the
pancreas and achieved the best result. next, we compared different methods for
multi-phase stages, including lstm, early fusion (fusion), and cross-attention
(cross) in our method. cross-attention is more effective and lightweight than
lstm. moreover, we separated texture features into in-phase features and
cross-phase features, which is more reasonable than early fusion.secondly, we
evaluated each component in our proposed method, as shown in fig. 2, and
presented the results in table 1. combining the texture-aware transformer and
regular structure information improved the results from 0.630 to 0.648, as tumor
invasion strongly affects the survival of pdac patients. we also employed a
simple 4-variable regression model that used only the chamfer distance of the
tumor and the four vessels for prognostic prediction. the resulting c-index of
0.611 confirmed the correlation of the distance with the survival, which is
consistent with clinical findings [18]. explicitly adding the distance measure
further improved the results. our proposed neural distance metric outperformed
traditional surface distance metrics like chamfer distance, indicating its
suitability for distinguishing the severity of pdac.
to further evaluate the performance of our proposed model, we compared it with
recent deep prediction methods [17,21] and report the results in table 2. we
modified baseline deep learning models [12,17] and used their network
architectures to take a single pancreatic phase or all three phases as inputs.
deepct-pdac [21] is the most recent method that considers both tumor-related and
tumor-vascular relationships using 3d cnns. our proposed method, which uses the
transformer and structure-aware blocks to capture tumor enhancement patterns and
tumor-vascular involvement, demonstrated its effectiveness with better
performance in both nested 5-fold cross-validation and the multi-center
independent test set.in table 3, we used univariate and multivariate cox
proportional-hazards models to evaluate our signature and other
clinicopathologic factors in the independent test set. the proposed risk
stratification was a significant prognostic factor, along with other factors
like pathological tnm stages. after selecting significant variables (p < 0.05)
in univariate analysis, our proposed staging remained strong in multivariable
analysis after adjusting for important prognostic markers like pt and resection
margins. notably, our proposed marker remained the strongest among all
pre-operative markers, such as tumor size and ca 19-9.neoadjuvant therapy
selection. to demonstrate the added value of our signature as a tool to select
patients for neoadjuvant treatment before surgery, we plotted kaplan-meier
survival curves in fig. 3. we further stratify patients by our signature after
grouping them by tumor size and ca19-9, two clinically used preoperative
criteria for selection, and also age. our signature could significantly stratify
patients in all cases and those in the high-risk group had worse outcomes and
might be considered as potential neoadjuvant treatment candidates (e.g. 33
high-risk patients with larger tumor size and high ca19-9).
in our paper, we propose a multi-branch transformer-based framework for
predicting cancer survival. our framework includes a texture-aware transformer
that captures both local and global information about the pdac and pancreas. we
also introduce a neural distance to calculate a more reasonable distance between
pdac and vessels, which is highly correlated with pdac survival. we have
extensively evaluated and statistically analyzed our proposed method,
demonstrating its effectiveness. furthermore, our model can be combined with
established high-risk features to aid in the patient selections who might
benefit from neoadjuvant therapy before surgery.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9 24.
existing tumor augmentation methods, including "copy-paste" strategy based
methods [15][16][17]19] and style-transfer based methods [5], only considered
content or style information when synthesizing new samples, which leads to a
distortion gap in content or domain space between the true image and synthetic
image, and further causes a distortion problem [14] as shown in fig. 1 (1). the
distortion problem damages the effectiveness of dcnns in feature representation
learning as proven in many studies [1,5,18]. therefore, a domain and content
simultaneously aware data augmentation method is urgently needed to eliminate
and avoid the distortion challenges during tumor generation. it remains,
however, a very challenging task because the content and domain space lack of
clear border, and the domain information always influences the distribution of
content. this is also the main reason that style transfer [7,8,10] still suffers
from spurious artifacts such as disharmonious colors and repetitive patterns,
and a large gap is still left between real artwork and synthetic style [2,3].
therefore, it's necessary to reduce the influence of the domain on content and
keep the content consistent during image generation.to overcome the above
challenges, a domain-aware and content-consistent tumor augmentation method,
named dcaug, is developed (fig. 1 experimental results on two public tumor
segmentation datasets show that dcaug improves the tumor segmentation accuracy
compared with state-of-theart tumor augmentation methods. in summary, our
contributions are as follows:-a content-aware and domain-aware tumor
augmentation method is proposed, which eliminates the distortion in content and
domain space between the true tumor image and synthetic tumor image. -our novel
dacl and cdcl disentangle the image information into two completely independent
parts: 1) domain-invariant content information; 2) individual-specific domain
information. it has the advantage of alleviating the challenge of distortion in
synthetic tumor images. -experimental results on two public tumor segmentation
datasets demonstrate that dcaug improves the diversity and quality of synthetic
tumor images.
formulation: given two images and the corresponding tumor labels {x a , y a },
{x b , y b }, tumor composition process can be formulated as:where,
respectively. there are two challenges need to be solved: 1) x b→a a , x a→b b ,
by adjusting the domain information of the copied tumor, making the copied tumor
have the same domain space as the target image to avoid domain distortion; 2)b ,
maintaining the domain-invariant content information consistency during tumor
copy to avoid content distortion.to achieve the above goals, a novel cross-cycle
framework (fig. 2) is designed, which consists of two generators and can
disentangle the tumor information into two solely independent parts: 1)
domain-invariant content information, 2) individual-specific domain information,
through two new learning strategies: 1) domain-aware contrastive learning
(dacl); 2) cross-domain consistency learning (cdcl). when generating new sample,
the domain-invariant content information is preserved by cdcl, while the
individual-specific domain information is adjusted by dacl based on the domain
space of target tumor image. the details are described as follows.
our domain-aware contrastive learning (dacl) strategy can adaptively adjust the
domain space of the transferred tumor and makes the domain space consistent for
domain adaptation. specifically, the input of dcaug is two combined images x b a
, x a b that consist of source images and tumor regions copied from another
image. the synthetic tumors x b→a a generated by the generator, the a as the
anchor, the positive and the negative sample, respectively. to find the domain
space of these samples for contrast, a fixed pre-trained style representation
extractor f is used to obtain domain representations for different images. thus,
dacl between the anchor, the positive, and the negative sample can be formulated
as:where d(x, y) is the l 2 distance between x and y, w i is weighting factor.
additionally, to further disentangle the individual-specific domain information,
a reversed process is designed. by utilizing the synthetic tumors x a→b b , x
b→a a , the reversed images x a→b a ,x b→a b can be construed as:the whole
reversed process receives the reversed images x a→b a , x b→a b as inputs and
tries to restore the original domain information of the synthetic tumor xa , xb
.where φ denotes the ith layer of the vgg-19 network, μ and σ represent the mean
and standard deviation of feature maps extracted by φ, respectively. in summary,
the total loss for the cross-cycle framework is) where α, β, and γ represent the
weight coefficients.
atlas dataset [11]: the atlas dataset consists of 229 t1-weighted mr images from
220 subjects with chronic stroke lesions. these images were acquired from
different cohorts and different scanners. the chronic stroke lesions are
annotated by a group of 11 experts. the dimension of the pre-processed images is
197 × 233 × 189 with an isotropic 1mm 3 resolution. identical with the study in
[17], we selected 50 images as the test set and the rest of the cases as the
training set. kits19 dataset [4]: the kits19 consists of 210 3d abdominal ct
images with kidney tumor subtypes and segmentation of kidney and kidney tumors.
these ct images are from more than 50 institutions and scanned with different ct
scanners and acquisition protocols. in our experiment, we randomly split the
published 210 images into a training set with 168 images and a testing set with
42 images. training details: the generator in dcaug is built on the rain [12]
backbone, all of the weights in generators are shared. our dcaug is implemented
using pytorch [13] and trained end-to-end with adam [9] optimization method. in
the training phase, the learning rate is initially set to 0.0001 and decreased
by a weight decay of 1.0 × 10 -6 after each epoch. the experiments were carried
out on one nvidia rtx a4000 gpu with 16 gb memory. the weight valule of α, β,
and γ is 1.0,1.0,1.0, separately. baseline: nnunet [6] is selected as the
baseline model. the default hyperparameters and default traditional data
augmentation (tda) including rotation, scaling, mirroring, elastic deformation,
intensity perturbation are used when model training. the maximum number of
training epochs was set to 500 for the two datasets. parts of tumors generated
are shown in fig. 3. and the dice coefficients of the segmentation results on
the same test set are computed to evaluate the effectiveness of methods.
experimental results in table 1 and fig. 4 show that compared with other
stateof-the-art methods, including mixup [16], cutmix [15], carvemix [17],
selfmix [19], stylemix [5], nnunet combined with dcaug achieves the highest
improvement on the two datasets, which convincingly demonstrates the innovations
and contribution of dcaug in generating higher quality tumor. and it is worth
noting that cutmix ("copy-paste"method that only considers content information)
even degrades the segmentation performance, which indicates that both content
and domain information has a significant influence on the tumor segmentation.the
representative segmentation scans are shown in fig. 4. our dcaug produced better
segmentation results than the competing methods, which further proves the
effectiveness of dcaug in tumor generation. what's more, the potential of dcaug
in an extremely low-data regime is also demonstrated. we randomly select 25% and
50% of data from the training set same as training data. dcaug also assists the
baseline model to achieve higher dice coefficients, which convincingly
demonstrates the effectiveness of dcaug in generating new tumor samples.
the necessity of considering both content and domain information in the tumor
generation is also demonstrated, three representative methods, mixup
("copy-paste"), cutmix ("copy-paste"), and stylemix (style-transfer), are
selected. the dcaug optimizes generated samples from above methods from content
and domain aspects to further improve the quality of generated samples. and the
nnuet are trained by optimized samples. from the segmentation performances
(table 2), we can notice that dcaug can further boost the quality of generated
samples produced by existing methods. specifically, the dcaug assists the mixup,
cutmix, and stylemix to obtain a 3.15%, 8.53%, and 0.60% improvement in
segmentation performance, respectively, which demonstrates that 1) it is
necessary to consider both content and domain information during samples
generation; 2) avoiding the content and domain distortion challenge can further
improve the quality of generated samples; 3) dcaug can alleviate the challenge
of distortion problem present in existing tumor augmentation methods.
in this paper, our domain-aware and content-consistent tumor augmentation method
eliminated the content distortion and domain gap between the true tumor and
synthetic tumor by simultaneously focusing the content information and domain
information. specifically, dcaug can maintain the domain-invariant content
information consistency and adaptive adjust individual-specific domain
information by a new cross-cycle framework and two novel contrastive learning
strategies when generating synthetic tumor. experimental results on two tumor
segmentation tasks show that our dcaug can significantly improve the quality of
the synthetic tumors, eliminate the gaps, and has practical value in medical
imaging applications.
dataset num means and standard deviations of the dice coefficients (%) tda mixup
cutmix carvemix selfmix stylemix dcaug atlas 25% 49.87 ± 32.19 49.18 ± 32.72
41.19 ± 33.98 55.16 ± 32.16 57.89 ± 31.05 52.84 ± 34.36 56.43 ± 32.33 50% 56.72
± 30.74 58.40 ± 29.35 54.25 ± 30.24 58.34 ± 31.32 58.81 ± 31.75 58.04 ± 30.39
59.75 ± 31.41 100% 59.39 ± 32.45 59.33 ± 33.06 56.11 ± 32.44 62.32 ± 31.10 63.5
± 31.06 64.00 ± 28.89 64.64 ± 29.91
cross-domain consistency learning (cdcl) strategy can preserve the
domaininvariant content information of tumor in the synthesized images x b→a a ,
x a→b b for avoiding content distortion. specifically, given the original
imagesproduced by generator, and the reconstructed images xa , xb generated by
the reversed process. the tumor can be first extracted from those imagesalthough
the domain space is various, the tumor content insideto evaluate the tumor
content inside cross-domain images, the content consistency losses, including l
a pixel (x a , xa ), l b pixel (x b , xb ), l a→b content , l b→a content , are
computed between those images for supervising the content change. the details of
content consistency loss are described in the next section.
in summary, three types of losses are used to supervise the cross-cycle
framework. specifically, given the original images x a , x b and the combined
images x b a , x b a , the synthesized images x a→b b , x b→a a are produced by
the generator, and the reconstructed images xa , xb are generated by the
reversed process.the pixel-wise loss (l pixel ) computes the difference between
original images and reconstructed images at the pixel level.to disentangle the
individual-specific domain information, the higher feature representations
extracted from pre-trained networks combined with cl are used:and two content
loss l b→a content , l a→b content are employed to maintain tumor content
information during the domain adaptation:
gastric cancer (gc) is the third leading cause of cancer-related deaths
worldwide [19]. the five-year survival rate for gc is approximately 33% [16],
which is mainly attributed to patients being diagnosed with advanced-stage
disease harboring unresectable tumors. this is often due to the latent and
nonspecific signs and symptoms of early-stage gc. however, patients with
early-stage disease have a substantially higher five-year survival rate of
around 72% [16]. therefore, early detection of resectable/curable gastric
cancers, preferably before the onset of symptoms, presents a promising strategy
to reduce associated mortality. unfortunately, current guidelines do not
recommend any screening tests for gc [22]. while several screening tools have
been developed, such as barium-meal gastric photofluorography [5], upper
endoscopy [4,7,9], and serum pepsinogen levels [15], they are challenging to
apply to the general population due to their invasiveness, moderate
sensitivity/specificity, high cost, or side effects. therefore, there is an
urgent need for novel screening methods that are noninvasive, highly accurate,
low-cost, and ready to distribute.non-contrast ct is a commonly used imaging
protocol for various clinical purposes. it is a non-invasive, relatively
low-cost, and safe procedure that exposes patients to less radiation dose and
does not require the use of contrast injection that may cause serious side
effects (compared to multi-phase contrastenhanced ct). with recent advances in
ai, opportunistic screening of diseases using non-contrast ct during routine
clinical care performed for other clinical indications, such as lung and
colorectal cancer screening, presents an attractive approach to early detect
treatable and preventable diseases [17]. however, whether early detection of
gastric cancer using non-contrast ct scans is possible remains unknown. this is
because early-stage gastric tumors may only invade the mucosal and muscularis
layers, which are difficult to identify without the help of stomach preparation
and contrast injection. additionally, the poor contrast between the tumor and
normal stomach wall/tissues on non-contrast ct scans and various shape
alterations of gastric cancer, further exacerbates this challenge.in this paper,
we propose a novel approach for detecting gastric cancer on non-contrast ct
scans. unlike the conventional "segmentation for classification" methods that
directly employ segmentation networks, we developed a clusterinduced mask
transformer that performs segmentation and global classification simultaneously.
given the high variability in shape and texture of gastric cancer, we encode
these features into learnable clusters and utilize cluster analysis during
inference. by incorporating self-attention layers for global context modeling,
our model can leverage both local and global cues for accurate detection. in our
experiments, the proposed approach outperforms nnunet [8] by 0.032 in auc, 5.0%
in sensitivity, and 4.1% in specificity. these results demonstrate the potential
of our approach for opportunistic screening of gastric cancer in asymptomatic
patients using non-contrast ct scans.
automated cancer detection. researchers have explored automated tumor detection
techniques on endoscopic [13,14], pathological images [20], and the prediction
of cancer prognosis [12]. recent developments in deep learning have
significantly improved the segmentation of gastric tumors [11], which is
critical for their detection. however, our framework is specifically designed
for noncontrast ct scans, which is beneficial for asymptomatic patients. while
previous studies have successfully detected pancreatic [25] and esophageal [26]
cancers on non-contrast ct, identifying gastric cancer presents a unique
challenge due to its subtle texture changes, various shape alterations, and
complex background, e.g., irregular gastric wall; liquid and contents in the
stomach.
recent studies have used transformers for natural and medical image segmentation
[21]. mask transformers [3,24,29] further enhance cnn-based backbones by
incorporating stand-alone transformer blocks, treating object queries in detr
[1] as memory-encoded queries for segmentation. cmt-deeplab [27] and
kmax-deeplab [28] have recently proposed interpreting the queries as clustering
centers and adding regulatory constraints for learning the cluster
representations of the queries. mask transformers are locally sensitive to image
textures for precise segmentation and globally aware of organtumor morphology
for recognition. their cluster representations demonstrate a remarkable balance
of intra-cluster similarity and inter-class discrepancy. therefore, mask
transformers are an ideal choice for an end-to-end joint segmentation and
classification system for detecting gastric cancer.
problem formulation. given a non-contrast ct scan, cancer screening is a binary
classification with two classes as l = {0, 1}, where 0 stands for"normal" and 1
for"gc" (gastric cancer). the entire dataset is denoted by, where x i is the
i-th non-contrast ct volume, with y i being the voxel-wise label map of the same
size as x i and k channels. here, k = 3 represents the background, stomach, and
gc tumor. p i ∈ l is the class label of the image, confirmed by pathology,
radiology, or clinical records. in the testing phase, only x i is given, and our
goal is to predict a class label for x i .
to address difficulties with tumor annotation on non-contrast cts, the
radiologists start by annotating a voxel-wise tumor mask on the
contrast-enhanced ct, referring to clinical and endoscopy reports as needed.
deeds [6] registration is then performed to align the contrast-enhanced ct with
the non-contrast ct and the resulting deformation field is applied to the
annotated mask. any misaligned ones are revised manually. in this manner (fig.
1d), a relatively coarse yet highly reliable tumor mask can be obtained for the
non-contrast ct image. cluster-induced classification with mask transformers.
segmentation for classification is widely used in tumor detection [25,26,32]. we
first train a unet [8,18] to segment the stomach and tumor regions using the
masks from the previous step. this unet considers local information and can only
extract stomach rois well during testing. however, local textures are inadequate
for accurate gastric tumor detection on non-contrast cts, so we need a network
of both local sensitivity to textures and global awareness of the organ-tumor
morphology. mask transformer [3,24] is a well-suited approach to boost the cnn
backbone with stand-alone transformer blocks. recent studies [27,28] suggest
interpreting object queries as cluster centers, which naturally exhibit
intra-cluster similarity and inter-class discrepancy. inspired by this, we
further develop a deep classification model on top of learnable cluster
representations.specifically, given image x ∈ r h×w ×d , annotation y ∈ r k×hw d
, and patient class p ∈ l, our model consists of three components: 1) a cnn
backbone to extract its pixel-wise features f ∈ r c×hw d (fig. 1a), 2) a
transformer module (fig. 1b), and 3) a multi-task cluster inference module (fig.
1c). the transformer module gradually updates a set of randomly initialized
object queries c ∈ r n ×c , i.e., to meaningful mask embedding vectors through
cross-attention between object queries and multi-scale pixel features,where c
and p stand for query and pixel features, q c , k p , v p represent linearly
projected query, key, and value. we adopt cluster-wise argmax from kmax-deeplab
[28] to substitute spatial-wise softmax in the original settings.we further
interpret the object queries as cluster centers from a cluster analysis
perspective. all the pixels in the convolutional feature map are assigned to
different clusters based on these centers. the assignment of clusters (a.k.a.
mask prediction) m ∈ r n ×hw d is computed as the cluster-wise softmax function
over the matrix product between the cluster centers c and pixel-wise feature
matrix f, i.e.,the final segmentation logits z ∈ r k×hw d are obtained by
aggregating the pixels within each cluster according to cluster-wise
classification, which treats pixels within a cluster as a whole. the aggregation
of pixels is achieved by z = c k m, where the cluster-wise classification c k is
represented by an mlp that projects the cluster centers c to k channels (the
number of segmentation classes).the learned cluster centers possess high-level
semantics with both intercluster discrepancy and intra-cluster similarity for
effective classification. rather than directly classifying the final feature
map, we first generate the clusterpath feature vector by taking the channel-wise
average of cluster centers c =additionally, to enhance the consistency between
the segmentation and classification outputs, we apply global max pooling to
cluster assignments r to obtain the pixel-path feature vector r ∈ r n . this
establishes a direct connection between classification features and segmentation
predictions. finally, we concatenate these two feature vectors to obtain the
final feature and project it onto the classification prediction p ∈ r 2 via a
two-layer mlp.the overall training objective is formulated as,where the
segmentation loss l seg (•, •) is a combination of dice and cross entropy
losses, and the classification loss l cls (•, •) is cross entropy loss.
dataset and ground truth. our study analyzed a dataset of ct scans collected
from guangdong province people's hospital between years 2018 and 2020, with
2,139 patients consisting of 787 gastric cancer and 1,352 normal cases. we used
the latest patients in the second half of 2020 as a hold-out test set, resulting
in a training set of 687 gastric cancer and 1,204 normal cases, and a test set
of 100 gastric cancer and 148 normal cases. we randomly selected 20% of the
training data as an internal validation set. to further evaluate specificity in
a larger population, we collected an external test set of 903 normal cases from
shengjing hospital. cancer cases were confirmed through endoscopy (and
pathology) reports, while normal cases were confirmed by radiology reports and a
two-year follow-up. all patients underwent multi-phase cts with a median spacing
of 0.75 × 0.75 × 5.0 mm and an average size of (512, 512, 108) voxel. tumors
were annotated on the venous phase by an experienced radiologist specializing in
gastric imaging using ctlabeler [23], while the stomach was automatically
annotated using a self-learning model [31].implementation details. we resampled
each ct volume to the median spacing while normalizing it to have zero mean and
unit variance. during training, we cropped the 3d bounding box of the stomach
and added a small margin of (32,32,4). we used nnunet [8] as the backbone, with
four transformer decoders, each taking pixel features with output strides of 32,
16, 8, and 4. we set the number of object queries n to 8, with each having a
dimension of 128, and included an eight-head self-attention layer in each block.
the patch size used during training and inference is (192, 224, 40) voxel. we
followed [8] to augment data. we trained the model with radam using a learning
rate of 10 -4 and a (backbone) learning rate multiplier of 0.1 for 1000 epochs,
with a frozen backbone of the pretrained nnunet [8] for the first 50 epochs. to
enhance performance, we added deep supervision by aligning the cross-attention
map with the final segmentation map, as per kmax-deeplab [27]. the hidden layer
dimension in the two-layer mlp is 128. we also trained a standard unet [8,18] to
localize the stomach region in the entire image in the testing phase.
for the binary classification, model performance is evaluated using area under
roc curve (auc), sensitivity (sens.), and specificity (spec.). and successful
localization of the tumors is considered when the overlap between the
segmentation mask generated by the model and the ground truth is greater than
0.01, measured by the dice score. a reader study was conducted with two
experienced radiologists, one from guangdong province people's hospital with 20
years of experience and the other from the first affiliated hospital of zhejiang
university with 9 years of experience in gastric imaging. the readers were given
248 non-contrast ct scans from the test set and asked to provide a binary
decision for each scan, indicating whether the scan showed gastric cancer. no
patient information or records were provided to the readers. readers were
informed that the dataset might contain more tumor cases than the standard
prevalence observed in screening, but the proportion of case types was not
disclosed. readers used itk-snap [30] to interpret the ct scans without any time
constraints. 1 presents a comparative analysis of our proposed method with three
baselines. the first two approaches belong to "segmentation for classification"
(s4c) [26,32], using nnunet [8] and transunet [2]. a case is classified as
positive if the segmented tumor volume exceeds a threshold that maximizes the
sum of sensitivity and specificity on the validation set. the third baseline
(denoted as "nnunet-joint") integrates a cnn classification head into unet [8]
and trained end-to-end. we obtain the 95% confidence interval of auc,
sensitivity, and specificity values from 1000 bootstrap replicas of the test
dataset for statistical analysis. for statistical significance, we conduct a
delong test between two aucs (ours vs. compared method) and a permutation test
between two sensitivities or specificities (ours vs. compared method and
radiologists).
our method outperforms baselines. our method outperforms three baselines (table
1) in all metrics, particularly in auc and sensitivity. the advantage of our
approach is that it captures the local and global information simultaneously in
virtue of the unique architecture of mask transformer. it also extracts
high-level semantics from cluster representations, making it suitable for
classification and facilitating a holistic decision-making process. moreover,
our method reaches a considerable specificity of 97.7% on the external test set,
which is crucial in opportunistic screening for less false positives and
unnecessary human workload.
as shown in fig. 2a, our ai model's roc curve is superior to that of two
experienced radiologists. the model achieves a sensitivity of 85.0% in detecting
gastric cancer, which significantly exceeds the mean performance of doctors
(73.5%) and also surpasses the best performing doctor (r2: 75.0%), while
maintaining a high specificity. a visual example is presented in fig. 2b. this
early-stage cancer (t1) is miss-detected by both radiologists, whereas
classified and localized precisely by our model.
in table 2, we report the performance of patient-level detection and tumor-level
localization stratified by tumor (t) stage. we compare our model's performance
with that of both radiologists. the results show that our model performs better
in detecting early stage tumors (t1, t2) and provides more precise tumor
localization. specifically, our model detects 60.0% (6/10) t1 cancers, and 77.8%
(7/9) t2 cancers, surpassing the best performing expert (50% t1, 55.6% t2).
meanwhile, our model maintains a reliable detection rate and credible
localization accuracy for t3 and t4 tumors (2 of 34 t3 tumors missed).comparison
with established screening tools. our method surpasses or performs on par with
established screening tools [4,7,10] in terms of sensitivity for gastric cancer
detection at a similar specificity level with a relatively large testing patient
size (n = 1151 by integrating the internal and external test sets), as shown in
table 3. this finding sheds light on the opportunity to employ automated ai
systems to screen gastric cancer using non-contrast ct scans.
we propose a novel cluster-induced mask transformer for gastric cancer detection
on non-contrast ct scans. our approach outperforms strong baselines and
experienced radiologists. compared to other screening methods, such as blood
tests, endoscopy, upper-gastrointestinal series, and me-nbi, our approach is
non-invasive, cost-effective, safe, and more accurate for detecting early-stage
tumors. the robust performance of our approach demonstrates its potential for
opportunistic screening of gastric cancer in the general population.
†compared baselines. table
optical colonoscopy is the standard of care screening procedure for the
prevention and early detection of colorectal cancer (crc). the primary goal of a
screening colonoscopy is polyp detection and preventive removal. it is well
known that many polyps go unnoticed during colonoscopy [22]. to deal with this
problem, computer-aided polyp detector (cade) was introduced [13][14][15][16]
and recently became commercially available [3]. the success of polyp detector
sparkled the development of new cad tools for colonoscopy, including polyp
characterization (cadx, or optical biopsy), extraction of various quality
metrics, and automatic reporting. many of those new cad applications require
aggregation of all available data on a polyp into a single unified entity. for
example, one would expect higher accuracy for cadx when it analyzes all frames
where a polyp is observed. clustering polyp detections into polyp entities is a
prerequisite for computing such quality metrics as polyp detection rate (pdr)
and polyps per colonoscopy (ppc), and for listing detected polyps in a
report.one may notice that the described task generally falls into the category
of the well known multiple object tracking (mot) problem [26,27]. while this is
true, there are a few factors specific to the colonoscopy setup: (a) due to
abrupt endoscope camera movements, targets (polyps) often go out of the field of
view, (b) because of heavy imaging conditions (liquids, debris, low
illumination) and non-rigid nature of the colon, targets may change their
appearance significantly, (c) many targets (polyps) are quite similar in
appearance. those factors limit the scope and accuracy of existing
frame-by-frame spatio-temporal tracking methods, which typically yield an
over-fragmented result. that is, the track is often lost, resulting in
relatively short tracklets (temporal sequences of same target detections in
multiple near-consecutive frames), see supplementary fig. 1.a recently published
method [2] addresses this limitation by combining spatial target proximity and
visual similarity to match a polyp detected in the current frame to "active"
polyp tracklets dynamically maintained by the system. the tracklets are built
incrementally, by adding a single frame detection to the matched tracklet,
one-by-one. however, this approach limits itself to use of closein-time
consistent detections, and cannot handle the frequent cases where polyp gets out
of the field of view and long range association is required.in this work we
propose an alternative approach that allows polyp detections grouping over an
extended period of time (up to 10 min), relaxing the spatiotemporal proximity
limitation. it involves two steps: (i) a short-term multi-object tracking, which
forms initial, relatively short tracklets, followed by (ii) a longerterm
tracklets grouping by appearance-based polyp re-identification (reid). as the
first step can be done by any generic multiple object tracking algorithm (e.g.
we use a tracking by detection method [27]), in this paper we focus on the
second step.to avoid manual data annotation, which is extremely ineffective in
our case, we turn to self-supervision and adapt the widely used contrastive
learning approach [5] to video input and object tracking scenario.as tracklet
re-identification is a sequence-to-sequence matching problem, the standard
solution is comparing sequences element-wise and then aggregating the
per-element comparisons, e.g. by averaging or max/min pooling [21] -the
so-called late fusion technique. we, on the other hand, follow an early fusion
approach by building a joint representation for the whole sequence. we use an
advanced transformer network [23] to leverage the attention paradigm for
nonuniform weighing and "knowledge exchange" between tracklet frames.we
extensively test the proposed method on hundreds of colonoscopy videos and
evaluate the contribution of method components using an ablation study. finally,
we demonstrate the effectiveness of the proposed reid method for improving the
accuracy of polyp characterization (cadx).to summarize, the three main
contributions of the paper are:-an adaptation of contrastive learning to video
input for the purpose of appearance based object tracking. -an early fusion,
joint multi-view object representation for reid, based on transformer networks.
-the application of polyp reid to boost the polyp cadx performance.
this work assumes the availability of an automatic polyp detector. quite a few
highly accurate polyp detectors were recently reported [14][15][16], detecting
(multiple) polyps in a single frame. our ultimate goal is to group those
detections into sets corresponding to distinct polyps.as briefly mentioned
above, the proposed approach starts with an initial grouping of polyp detections
using an off-the-shelf multiple object tracking algorithm. such a tracker is
expected to track polyps through consecutive frames as long as they do not leave
the camera field of view, forming disjoint, time separated polyp tracklets. in
this work we use the bytetrack [27] "tracking by detection" algorithm, but, in
principle, any other tracker could be used instead.the resulting tracklets are
typically relatively short, and there are quite a few tracklets corresponding to
the same polyp. to improve the result, we propose an appearance-based polyp
re-identification (reid), which groups multiple disjoint tracklets by their
visual appearance into a joint tracklet, associated with a single polyp. in what
follows we describe in detail the proposed reid component.as stated above, the
objective of reid is to ascertain whether two timeseparated, disjoint tracklets
belong to the same polyp. to this end we seek a tracklet representation that
allows measuring visual similarity between tracklets. the two basic alternatives
are either a single representation for the whole tracklet, or a sequence of
single-frame representations for each tracklet frame. we will consider both
options below.
to generate a single frame representation we train an embedding model that maps
a polyp image into a latent space, s.t. the vectors of different views of the
same polyp are placed closer, and of different polyps away from each other
[11].a straightforward approach to train such model is supervised learning,
which requires forming a large collection of polyp image pairs, manually labeled
as same/not same polyp [1]. such annotation turned out to be inaccurate and
expensive. in addition, finding hard negative pairs is especially challenging,
as images of two randomly sampled polyps are usually very dissimilar. moreover,
self-supervised techniques using extensive unannotated datasets has exhibited
substantial advantages within the medical domain [12].hence, we turn to simclr
[5], a contrastive self-supervised learning technique, which requires no manual
labeling. in simclr the loss is calculated over the whole batch where all input
samples serve as negatives of each other and positive samples are generated via
image augmentations. combined with the temperature mechanism this allows for
hard negative mining by prioritizing hard-to-distinguish pairs, resulting in a
more effective loss weighting scheme.one caveat of simclr is the difficulty to
generate augmentations beneficial for the learning process [5]. specifically for
colonoscopy, the standard image augmentations do not capture the diversity of
polyp appearances in different views (see fig. 1(c)).instead of customizing the
augmentations to fit the colonoscopy setup, we leverage the temporal nature of
videos, and take different polyp views from the same tracklet as positive
samples (see fig. 1(b)). formally, a batch is formed by sampling one tracklet
from n different procedures to ensure the tracklets belong to different polyps.
two polyp views i, j are sampled from each tracklet as positive pairs (same
polyp). let f be the embedding model. the loss function for the positive pair
(i, j) is defined as:where sim is the dot product and τ is the temperature
parameter [24]. the final loss is computed across all positive pairs in the
batch.tracklets represented as sequences of per-frame embeddings can be matched
by computing pair-wise distances between frames, followed by an aggregatione.g.
min/max/mean distance [4,10]. an example of similarities between frames can be
seen in supplementary fig. 2.
as discussed earlier, an alternative to the single frame approach, is a unified
representation for the whole tracklet. a commonly used practice is to compute
single frame embedding (for each view) and fuse them [8,21], e.g. by averaging.
the downside of those simple techniques is that they treat every frame in the
same way, including bad quality, repeating, non-informative views. we postulate
that learning a joint embedding of multiple views in an end-to-end manner will
produce a better representation of the visual properties of a polyp, by allowing
"knowledge exchange" between the tracklet frames.to achieve this, we employ a
transformer network [23], with the addition of bert [7] classification token
(cls). the attention mechanism enables both frame based intra attention and
selective weighting of the frames thus providing a more comprehensive tracklet
representation. the overview of the architecture is presented in fig. 2.
training this multi-view encoder is done similarly to training a single-view
encoder using simclr, but now, instead of pairs of frames, we deal with pairs of
tracklet. to generate positive tracklet pairs, we cannot apply the trick used
for single frames, where positive pairs are sampled within the same tracklet.
instead we generate "pseudo positive" pairs from existing tracklets. we
artificially split a tracklet into 3 disjoint segments, where the middle segment
is discarded, and the first and the last segments are used as a positive pair,
thus providing sufficiently different appearances of the same polyp as would
happen in real procedures. in addition, this type of sampling approach, which
effectively discards highly correlated samples from training, has been shown to
improve model performance in [17].
this section includes two parts. the first provides a stand-alone evaluation of
the proposed reid method. the second assesses the impact of reid on polyp
classification accuracy.
dataset. we use 22,283 colonoscopy videos, split into training (21,737) and test
(546) sets. these recordings were captured from standard colonoscopy procedures
conducted at six medical centers during the period of 2019 to 2022. the average
length of the recorded procedures is 15 min, with a median duration of 13 min.
for training, we automatically generated polyp tracklets using automatic polyp
detection and tracking as described in sect. 2.the tracking algorithm might
produce short and uninformative tracklets as well as outliers. the following
clean up steps were performed on the training set: we filtered out tracklets
shorter than 1 s or having less than 15 high confidence detections, as defined
in [27], and took only the longest tracklet from every procedure. the thresholds
were determined using analysis of the training set tracklets distribution. this
yielded the training set of 15,465 tracklets (mean duration of 377 frames or 29
s). for evaluation, the test set polyp tracklets were manually annotated
(timestamps and bounding boxes) by certified physicians. in addition, tracklet
pairs from the same procedure were manually labeled as either belonging to the
same polyp or not. this yielded 348 negative and 252 positive tracklet
pairs.training. we utilize resnet50v2 [9] as the single frame encoder, with an
mlp head projecting the representation into a 128-dimensional embedding vector.
we initialize the model using pre-trained imagenet [6] weights. while ima-genet
weights are not optimal for medical tasks [18,19], they offer training speedups
[18]. the multi-view encoder consists of 3 transformer encoder blocks with an
mlp projection head. we use lars optimizer [25] with the learning rate of 0.01
and τ = 0.1 as suggested in [5]. the batch size is set to 1024 for training both
the single frame and the multi-view encoder.we first train the single frame
encoder and use its weights to initialize the single frame module of the
multi-view encoder. due to memory limitations, we use 8 views per tracklet
during training, resulting in 1024 * 8 = 8192 images per training step. the
model was trained for 5,000 steps using cloud v3 tpus with 16 cores. the single
frame encoder has 24m parameters, and the multi-view encoder adds an additional
1m parameters.evaluation. we start by comparing various reid techniques
described in sect. 2. namely, we evaluate the accuracy of tracklet
re-identification using: (a) single-frame representation with pairwise distances
aggregation by min / max / mean functions [4,10]; (b) multi-view representation
by frame embeddings averaging; and, finally, (c) the joint embedding multi-view
model. we evaluate the performance using auc of the roc and precision-recall
curve (prc) for tracklet similarity scores over the test set (see table 1 and
supplementary fig. 3). one can see that the joint embedding multi-view model
outperforms all other techniques both on roc and prc.in addition, we evaluate
the effectiveness of reid by measuring the average polyp fragmentation rate
(fr), defined as the average number of tracklets polyps are split into.
obviously, lower fragmentation rate means better result (with the best
fragmentation of 1), but it may come at the expense of wrong tracklet matching
(false positive). we measure the fragmentation rate at the operating point of 5%
false positive rate. the number of polyp fragments is determined by matching
tracklets to manually annotated polyps and counting
in this section, we investigate the potential benefits of using polyp reid as
part of a cadx system. polyp cadx aims to assist physicians to figure out, in
real time, during the procedure, whether the detected polyp is an adenoma. most
reported cadx systems compute a classification score for each frame, and
aggregate scores from multiple frames to determine the final polyp
classification. grouping polyp frames into a tracklet, to be fed into the cadx,
is usually done by a spatio-temporal tracker [2]. longer tracklets provide more
information for polyp classification.here, we investigate if the proposed reid
model, used to group disjoint tracklets of the same polyp, can increase the
accuracy of cadx.data. we use 3290 colonoscopy videos split into train,
validation, and test sets (2666, 296, and 328 videos respectively). the videos
are processed by a polyp detector and tracker to form polyp tracklets. the
tracklets are then manually grouped together to build a single sequence for
every polyp. each polyp is annotated by a certified gastroenterologist as either
adenoma or non-adenoma.cadx. we trained a simple image classification cnn,
composed of a mobilenet [20] backbone, followed by an mlp layer with a sigmoid
activation, to predict the non-adenoma/adenoma score in [0, 1], for each frame.
the chosen architecture has 2.4m parameters and can run in real-time. the model
was trained on nvidia tesla v100 gpu for 200 epochs with a learning rate of
0.001, using adam optimizer.for evaluation, we used the model to predict the
classification score for each frame and aggregated the scores using soft voting
to achieve the final prediction for each tracklet.evaluation. to assess the
contribution of the reid to polyp classification, we compare the cadx results on
the test set, while using different grouping methods to merge multiple polyp
detections into tracklets. the 3 evaluated methods are: (1) manual annotation
(2) grouping by tracking, and (3) grouping by reid. the manually annotated
tracklets -the ground truth (gt) -are the longest sequences, containing all
frames of each polyp in the test set. in grouping by tracking, we use tracklets
generated by the spatio-temporal tracking algorithm [27]. finally, for reid, we
merge disjoint tracklets by their appearance using the reid model. by
construction, tracklets generated by methods ( 2) and (3) are subsets of the
corresponding manually annotated gt tracklet, and are assigned its polyp
classification label. a visualization of the resulting tracklets using different
grouping methods is provided in supplementary fig. 4. the number of resulting
tracklets in the test set for each grouping method and polyp labels distribution
are summarized in table 3. we ran the cadx model on tracklets generated by the 3
grouping methods. we compute the f 1 score and the auc for the tracklet
classification task. in addition, we measure the cadx sensitivity at
specificity=0.9. the results are summarized in table 4. the result on the
manually annotated data is the accuracy upper-bound and is brought as a
reference point. one can see that the reid based approach significantly improves
the cadx accuracy compared to the tracking-based grouping.
in this study we present a novel multi-view self-supervised learning method for
learning informative representations of a sequence of video frames. by jointly
encoding multiple views of the same object, we get more discriminative features
in comparison to traditional embedding fusion techniques. this approach can be
used to group disjoint tracklets generated by a spatio-temporal tracking
algorithm based on their appearance, by measuring the similarity between
tracklets representations. its applicability to medical contexts is of
particular relevance, as medical data annotation often requires specific
expertise and may be costly and time consuming. we use this method to train a
polyp re-identification model (reid) from large unlabeled data, and show that
using the reid model as part of a cadx system enhances the performance of polyp
classification. there are some limitations however in identifying polyps based
on their appearance, as it may be changed drastically during the procedure (for
example, during resection). in future work we may examine the use of reid for
additional medical applications, such as listing detected polyps in an automatic
report, bookmarking of specific areas of the colon during the procedure, and
calculation of clinical metrics such as polyp detection rate and polyps per
colonoscopy.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9_57.
colonoscopy plays a crucial role in identifying and removing early polyps and
reducing mortality rates associated with rectal cancer. over the past few years,
the research community has devoted great effort to understanding colonoscopy
videos using either optical flow [22,23] or temporal information aggregation
[5,12,16,19] between multiple frames.however, those works are mainly designed
based on the experience of previous natural video object detection studies,
ignoring the inherent uniqueness of the colonoscopy motion patterns. thus, we
rethink the video polyp detection task and conclude three core challenges in
colonoscopy videos. 1) fast motion speed. in fig. 1(a), we show the target
motion speed [26] 1 on imagenetvid [14] (natural) and ldpolypvideo [9]
(colonoscopy) dataset. the motion speed in imagenetvid evenly distributes in
three intervals. in contrast, most targets in ldpolypvideo fall in the fast
speed zone, leading to a large variance in the adjacent foreground features,
like motion blur or occlusion, as shown in fig. 1(c). thus we conjecture that
collaborating too many frames for polyp video detection will increase the
misalignment between adjacent frames and leads to poor detection performance.
figure 1(b) shows the performance of fgfa [26] on two datasets with increasing
reference frames. the different trends of the two lines confirm our hypothesis.
2) complex background. different from the common camera-fixed videos, the
camera-moving of colonoscopy video will introduce large disturbances between
adjacent frames (e.g., specular reflection, bubbles, water, etc.), as shown in
fig. 1(d). those abnormalities disrupt the integrity of background structures
and thus affect the effect of multi-frame fusion. 3) concealed polyps. as shown
in fig. 1(e), we noticed that some polyps could be seen as concealed objects in
the colonoscopy video since such polyps have a very similar appearance to the
intestine wall. the model will be confused by such frames in inference and
result in high false-positive or false-negative predictions.to address the above
issues, we propose the yona framework, which fully exploits the reference frame
information and only needs one adjacent reference frame for accurate video polyp
detection. specifically, we propose the foreground temporal alignment (fta)
module to explicitly align the foreground channel activation patterns between
adjacent features according to their foreground similarity. in addition, we
design the background dynamic alignment (bda) module after fta that further
learns the inter-frame background spatial dynamics to better eliminate the
influence of motion speed and increase the training robustness. finally,
parallel to fta and bda, we introduce the cross-frame boxassisted contrastive
learning (cbcl) that fully utilizes the box annotations to enlarge polyp and
background discrimination in embedding space.in summary, our contributions are
in three-folds: (1) to the best of our knowledge, we are the first to
investigate the obstacles to the development of existing video polyp detectors
and conclude that two-frame collaboration is enough for video polyp detection.
(2) we propose the yona, a novel framework for video polyp detection. it
composes the foreground and background alignment modules to align the features
under the fast-moving condition. it further introduces the cross-frame
contrastive learning module to enhance the model's discrimination ability of
polyps and intestine walls. (3) extensive experiments demonstrate that our yona
achieves new state-of-the-art performance on three large-scale public video
polyp detection datasets.
the whole pipeline is shown in fig. 2. we leverage the centernet [25] as the
base detector. given a clip of a colonoscopy video, we take the current frame as
anchor i a and its adjacent previous frame as reference i r . the binary maps m
a , m r are generated using the bounding box of anchor and reference, where the
foreground pixels are assigned with 1 while the background with 0. at each step,
yona first extracts multi-scale features from i a , i r using the backbone.
then, multi-scale features are fused and up-sampled to the resolution of the
first stage as the intermediate features f a , f r . then, we conduct foreground
temporal alignment (fig. 2(a)) on intermediate features to align their channel
activation pattern. next, the enhanced anchor feature f is further refined by
the background dynamic alignment module (fig. 2(b)) to mitigate the rapid
dynamic changes in the spatial field. the bda's output f * is used to compute
the detection loss. meanwhile, the intermediate features and binary maps are
used to calculate the contrastive loss during training to improve the model's
perception of polyp and background (fig. 2(c)).overall, the whole network is
optimized with the combination loss function in an end-to-end manner. the final
loss is composed of the same detection loss with centernet and our proposed
contrastive loss, formulated as l = l detection + λ contrast l contrast .
since the camera moves at a high speed, the changes in the frame are very
drastic for both foreground and background targets. as a result, multi-frame
(reference>3) fusion may easily incorporate more noise features into the
aggregation features. on the other hand, the occluded or distorted foreground
context may also influence the quality of aggregation. thus we propose to
conduct temporal alignment between adjacent features by leveraging the
foreground context of only one adjacent reference frame. it is designed to align
the certain channel's activation pattern of anchor feature to its preceding
reference feature. specifically, given the intermediate features f a , f r and
reference binary map m r , we first pooling f r to 1d channel pattern f r by the
binary map on the spatial dimension (r n ×c×h×w → r n ×c×1 ) and normalize it to
[0, 1]:then, the foreground temporal alignment is implemented by channel
attention mechanism, where the attention maps are computed by weighted
dot-product.we obtain the enhanced anchor feature by adding the attention maps
with the original anchor feature through skip connection to keep the gradient
flow.where α is the adaptive weight by similarity measuring. at the training
stage, the ground truth boxes of the reference frame are used to generate the
binary map m r . during the inference stage, we conduct fta only if the
validated bounding box of the reference frame exists, where "validated" denotes
the confidence scores of detected boxes are greater than 0.6. otherwise, we will
skip this process and feed the original inputs to the next module.adaptive
re-weighting by similarity measuring. as discussed above, due to video jitters,
adjacent frames may change rapidly at the temporal level, and directly fusing
the reference feature will introduce noisy information and misguide the
training. thus we designed an adaptive re-weighting method by measuring the
feature similarity, where the weight indicates the importance of the reference
feature to the anchor feature. specifically, if the foreground feature of the
reference is close to the anchor, it is assigned a larger weight at all
channels. otherwise, a smaller weight is assigned. for efficiency, we use the
cosine similarity metric [8] to measure the similarity, where f a is the 1d
channel pattern of f a computed with eq. 1:
the traditional convolutional-based object detector can detect objects well when
the background is stable. however, once it receives obvious interference, such
as light or shadow, the background changes may cause the degradation of spatial
correlation and lead to many false-positive predictions. motivated by the
inter-frame difference method [20], we first mine the dynamic field of adjacent
background contents, then consult to deformable convolution [3] to learn the
inherent geometric transformations according to the intensity of the dynamic
field. in practice, given the enhanced anchor feature f from fta and reference
feature f r , the inter-frame difference is defined as the element-wise
subtraction of enhanced anchor and reference feature. then a 1 × 1 convolution
is applied on the difference to generate dynamic field d, which encodes all
spatial dynamic changes between adjacent frames.finally, a 3 × 3 deformable
convolution embeds the spatial dynamic changes of d on the enhanced anchor
feature f .where d works as the deformable offset and f * is the final aligned
anchor feature.then the enhanced anchor feature is fed into three detection
heads composed of a 3 × 3 conv and a 1 × 1 conv to produce center, size, and
offset features for detection loss:where l focal is focal loss and l l1 is l1
loss.
typically, in colonoscopy videos, some concealed polyps appear very similar to
the intestine wall in color and texture. thus, an advanced training strategy is
required to distinguish such homogeneity. inspired by recent studies on
supervised contrastive learning [18], we select the foreground and background
region on both two frames guided by ground truth boxes to conduct contrastive
learning. in practice, given a batch of intermediate feature maps f a , f r ∈ r
n ×t ×c×h×w and corresponding binary maps m a , m r ∈ r n ×t ×h×w , we first
concatenate the anchor and reference at the batch-wise level as f ∈ r nt ×c×h×w
and m ∈ r nt ×h×w to exploit the cross-frame information. then we extract the
foreground and background channel patterns of cross-frame feature f using the
eq. 1 base on m (x, y) = 1 and m (x, y) = 0, respectively. after that, for each
foreground channel pattern, which is the "query", we randomly select another
different foreground feature as the "positive", while all the background
features in the same batch are taken as the "negatives". finally, we calculate
the one-step contrastive loss by infonce [18]:where q j ∈ r c , j = 0, ..., n t
is the query feature, i + ∈ r c and i -∈ r nt ×c are positives and negatives. n
j denote embedding collections of the negatives. we repeat this process until
every foreground channel pattern is selected and sum all steps as the final
contrastive loss:3 experimentswe evaluate the proposed method on three public
video polyp detection benchmarks: sun colonoscopy video database [7,10] (train
set: 19,544 frames, test set: 12,522 frames), ldpolypvideo [9] (train set:
20,942 frames, test set: 12,933 frames), and cvc-videoclinicdb [1] (train set:
7995 frames, test set: 2030 frames). for the fairness of the experiments, we
keep the same dataset settings for yona and all other methods. we use resnet-50
[6] as our backbone and centernet [25] as our base detector. following the same
setting in centernet, we set λ size = 0.1 and λ of f = 1. we set λ contrast =
0.3 by ablation study. detailed results are listed in the supplement. we
randomly crop and resize the images to 512 × 512 and normalize them using
imagenet settings. random rotation and flip with probability p = 0.5 are used
for data augmentation. we set the batch size n = 32. our model is trained using
the adam optimizer with a weight decay of 5 × 10 -4 for 64 epochs. the initial
learning rate is set to 10 -4 and gradually decays to 10 -5 with cosine
annealing. all models are trained with pytorch [11] framework. the training
setting of other competitors follows the best settings given in their paper.
quantitative comparison. the comparison results are shown in table 1. following
the standard of [1], the precision, recall, and f1-scores are used for
evaluation. firstly, compared with the centernet baseline, our yona with three
novel designs significantly improved the f1 score by 9.2%, 8.3%, and 7.4% on
three benchmarks, demonstrating the effectiveness of the model design. besides,
yona achieves the best trade-off between accuracy and speed compared with all
other image-based sotas across all datasets. second, for video-based
competitors, previous video object detectors with multiple frame collaborations
lack the ability for accurate detection on challenging datasets. specifically,
yona surpasses the second-best stft [19] by 2.2%, 3.0%, and 1.3% on f1 score on
three datasets and 33.8 on fps. all the results confirm the superiority of our
proposed framework for accurate and fast video polyp detection.qualitative
comparison. figure 3 visualizes the qualitative results of yona with other
competitors [19,25]. thanks to this one-adjacent-frame framework, our yona can
not only prevent the false positive caused by part occlusion (1st and 2nd clips)
but also capture useful information under severe image quality (2nd clip).
moreover, our yona shows robust performance even for challenging scenarios like
concealed polyps (3rd clip).
we investigated the effectiveness of each component in yona on the sun database,
as shown in table 2. it can be observed that all the modules are necessary for
precise detection compared with the baseline results. due to the large variance
of colonoscopy image content, the f1 score slightly decreases if directly adding
fta without the adaptive re-weighting strategy. adding the adaptive weight
greatly improves the f1 score by 5.4. moreover, we use other two mainstream
channel attention mechanisms to replace our proposed fta for comparison.
compared with them, our fta with adaptive weighting achieves the largest gain
over the baseline and higher fps. overall, by combining all the proposed
methods, our model can achieve new state-of-the-art performance. table 2.
ablation studies of yona under different settings. ada means the adaptive
re-weighting by similarity measuring; cw denotes the channel-wise attention [4];
ca denotes the channel-aware attention [19].fta cw [4] ca [19]
video polyp detection is a currently challenging task due to the fast-moving
property of colonoscopy video. in this paper, we proposed the yona framework
that requires only one adjacent reference frame for accurate and fast video
polyp detection. to address the problem of fast-moving polyps, we introduced the
foreground temporal alignment module, which explicitly aligns the channel
patterns of two frames according to their foreground similarity. for the complex
background content, we designed the background dynamic alignment module to
mitigate the large variances by exploiting the inter-frame difference.
meanwhile, we employed a cross-frame box-assisted contrastive learning module to
enhance the polyp and background discrimination based on box annotations.
extensive experiment results confirmed the effectiveness of our method,
demonstrating the potential for practical use in real clinical applications.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9_5.
breast cancer is a life-threatening disease that has surpassed lung cancer as
leading cancer in some countries and regions [20]. breast ultrasound is the
primary screening method for diagnosing breast cancer, and accurately
distinguishing between malignant and benign breast lesions is crucial. this task
is also an essential component of computer-aided diagnosis. since each frame in
an ultrasound video can only capture a specific view of a lesion, it is
essential to aggregate information from the entire video to perform accurate
automatic lesion diagnosis. therefore, in this study, we focus on the
classification of breast ultrasound videos for detecting malignant and benign
breast lesions. [15] and static images from busi [1]. we use a 2d resnet trained
on ultrasound images to get the features.while ultrasound videos offer more
information, prior studies have primarily focused on static image classification
[2,11,27]. obtaining ultrasound video data with pathology gold standard results
poses a major challenge. sonographers typically record keyframe images during
general ultrasound examinations, not entire videos. prospective collection
requires additional efforts to track corresponding pathological results.
consequently, while there are many breast ultrasound image datasets [1,28],
breast ultrasound video datasets remain scarce, with only one relatively small
dataset [15] containing 188 videos available currently.given the difficulties in
collecting ultrasound video data, we investigate the feasibility of enhancing
the performance of ultrasound video classification using a static image dataset.
to achieve this, we first analyze the relationship between ultrasound videos and
images. the images in the ultrasound dataset are keyframes of a lesion that
exhibit the clearest appearance and most typical symptoms, making them more
discriminative for diagnosis. although ultrasound videos provide more
information, the abundance of frames may introduce redundancy or vagueness that
could disrupt classification. from the aspect of feature distribution, as shown
in fig. 1, the feature points of static images are more concentrated, while the
feature of video frames sometimes are away from the class centers. frames far
from the centers are harder to classify. therefore, it is a promising approach
to guide the video model to pay more attention to important frames close to the
class center with the assistance of static keyframe images. meanwhile, our
approach aligns with the diagnosis of ultrasound physicians, automatically
evaluates the importance of frames, and diagnoses based on the information of
key frames. additionally, our method provides interpretability through key
frames.in this paper, we propose a novel keyframe guided attention network
(kga-net) to boost ultrasound video classification. our approach leverages both
image (keyframes) and video datasets to train the network. to classify videos,
we use frame attention to predict feature weights for all frames and aggregate
them to make the final classification. the feature weights determine the
contribution of each frame for the final diagnosis. during training, we
construct category feature centers for malignant and benign examples
respectively using center loss [26] on static image inputs and use the centers
to guide the training of video frame attention. specifically, we propose
coherence loss, which promotes the frames close to the centers to have high
attention weights and decreases the weights for frames far from the centers. due
to the feature centers being generated by the larger scale image dataset, it
provides more accurate and discriminative feature centers which can guide the
video frame attention to focus on important frames, and finally leads to better
video classification.our experimental results on the public busv dataset [15]
show that our kga-net significantly outperforms other video classification
models by using an external ultrasound image dataset. additionally, we
visualized attention values guided by the coherence loss. the frames with clear
diagnostic characteristics are given higher attention values. this phenomenon
makes our method more explainable and provides a new perspective for selecting
keyframes from video.in conclusion, our contributions are as follows:1. we
analyze the relationship between ultrasound video data and image data, and
propose the coherence loss to use image feature centers to guide the training of
frame attention. 2. we propose kga-net, which adopts a static image dataset to
boost the performance of ultrasound video classification. kga-net significantly
outperforms other video baselines on the busv dataset. 3. the qualitative
analysis of the frame attention verifies the explainability of our method and
provides a new perspective for selecting keyframes.
breast ultrasound classification. breast ultrasound (bus) plays an important
supporting role in the diagnosis of breast-related diseases. recent research
demonstrated the potential of deep learning for breast lesion classification
tasks [6,18,19,23,27]. [6,18] design ensemble methods to integrate the features
of multiple models to obtain higher accuracy. [19,23,27] utilize multi-task
learning to improve the model performance. however, all of them are based on
image datasets, such as busi [1], while few works focus on the video modality.
[14] design a pre-training model based on contrastive learning for ultrasound
video classification. [13,25] develop a keyframe extraction model for ultrasound
videos and utilized the extracted keyframes to perform various classification
tasks. however, these methods rely on keyframe supervision, which limits their
applicability. fortunately, the recent publicly available dataset busv [15] has
made the research on the task of bus video-based classification possible. in
this paper, we build our model based on this dataset.video recognition based on
neural networks. traditional methods are based on two-stream networks [9,10,24].
since i3d [3] was proposed, 3d cnns have dominated video understanding for a
long time. [21,22] decompose 3d convolution in different ways to reduce
computation complexity without losing performance. [8] designed two branches to
focus on temporal information and spatial features, respectively. however, 3d
cnns have a limited receptive field, and thus struggle to capture long-range
dependency. vision transformers [5,16] have become popular for their ability to
aggregate spatial-temporal information.to address computational complexity, mvit
[7] employed a hierarchical structure and video swin [17] introduced 3d shifted
window attention. our proposed kga-net is a simple framework that leverages the
frame attention module to aggregate multi-frame features efficiently.
as shown in fig. 2, our kga-net takes the video inputs and static image inputs
simultaneously to train the network. the coherence loss is proposed to guide the
frame attention by using the feature centers generated by the images. we will
then elaborate on each component in the following sections.
the video classification network is illustrated in fig. 2 (a). the model is
composed of a 2d cnn backbone, a frame attention module, and a classification
head. for an input video clip v composed of n frames, it is first processed by
the backbone network and the feature vectors of the frames {f i } n i=1 are
obtained. then, the frame attention module predicts the attention weight for
each frame using a fc and sigmoid layer, and then the features are aggregated by
the weights to form an integrated feature vector. formally,where w i denotes the
weight for the i th frame and fc is the fully-connected layer. then, the
features are aggregated byfinally, the classification head is applied to the
final result of lesion classification. to train the model, the cross-entropy
loss (ce loss) is applied to the classification prediction of the video. the
image classification network is used to assist in training the video model. we
use the same 2d cnn as the backbone network in the video classification network.
the model weights are shared for the two backbones for better generalization. to
promote the formation of feature centers, we apply the center loss [26] to the
image model besides the cross-entropy loss. in addition, the frame-level
cross-entropy loss is also applied to the video frames to facilitate training.
in this section, we introduce the coherence loss to guide the frame attention
with the assistance of the category feature centers. we use the same method as
center loss [26] to obtain the feature centers for the malignancy and benign
lesions, which are denoted as c mal and c benign , respectively.the distances of
frame features and the feature centers can measure the quality of the frames.
the frame features close to the centers are more discriminative for the
classification task. therefore, we use these distances to guide the generation
of frame attention. specifically, we push the frames close to the centers to
have higher attention weights and decrease the weights far from the centers. to
do this, for each video frame with feature f i , we first calculate the feature
distance from its corresponding class center. formally,where y ∈ {mal, benign}
is the label of the video v and d i is the computed distance of frame i.
afterward, we apply coherence loss to the attention weights w = [w 1 , w 2 ,
..., w n ] to make them have a similar distribution with the feature distances d
= [d 1 , d 2 , ..., d n ] . to supervise the distribution, the coherence loss is
defined as the l2 loss of the gram matrix of these two vectorswhereis the gram
matrix of normalized attention weights, andis the gram matrix of normalized
feature distances. note that lower distances correspond to stronger attention,
hence we use the opposite of w to get gram w .
to summarize, the total training loss of our kga-netl v ce and l i ce denote the
cross-entropy for video classification and image and frame classification. l
center means the center loss. λ is the weight for coherence loss. empirically,
we set λ = 1 in our experiments.during inference, to perform classification on
video data, the video classification network can be utilized individually for
prediction.
datasets. we use the public busv dataset [15] for video classification and the
busi dataset [1] as the image dataset. busv consists of 113 malignant videos and
75 benign videos. busi contains 445 images of benign lesions and 210 images of
malignant lesions. for the busv dataset, we use the official data split in [15].
all images of the busi dataset are adopted to train our kga-net. model details.
resnet-50 [12] pretrained on imagenet [4] is used as backbone. we use sgd
optimizer with an initial learning rate of 0.005, which is reduced by 10× at the
4,000th and 6,000th iteration. the total learning iteration number is 8,000. the
learning rate warmup is used in the first 1,000 iterations. for each batch, the
video clips and static images are both sampled and sent to the network. we use a
total batchsize of 16 and the sample probability of video clips and images is
1:1. we implement the model based on pytorch and train it with nvidia titan rtx
gpu cards.during inference, we use the video classification network
individually. in order to satisfy the fixed video length requirement of mvit
[7], we sample up to 128 frames of each video to form a video clip and predict
its classification result using all the models in experiments.
in this section, we compare our kga-net with other competitive video
classification models. however, comparing with ultrasound-video-based work is
challenging due to limited code accessibility and lack of keyframe detection
model in existing methods [13,14,25]. therefore, we compare our method with
strong video baselines on natural images. we include cnn-based models, i3d [3],
slow-fast [8], r(2+1)d [22], and csn [21], along with the popular
transformer-based model mvit [7]. for fairness comparison, we train these models
using both video and image data, treating images as static videos. evaluation
metrics are reported on the busv test set for performance assessment.as shown in
table 1, by leveraging the guidance of the image dataset, our kga-net
significantly surpasses all other models on all of the metrics. the video
classification model of our kga-net is composed of a standard 2d resnet-50 and a
light feature attention module, while the baseline models are with net
structures carefully designed for video analysis. therefore, the success of our
kga-net lies in the correct usage of the image guidance. the feature centers
formed by the image dataset with larger data size and clear appearance
effectively improve the accuracy of frame attention hence boosting the video
classification performance.
in this section, we ablate the contribution of each key design in our kga-net.
we observe their importance by removing these key components from the whole
network. the results are shown in table 2. the results of kga-net are shown in
the last row in table 2, while the components are ablated in the first three
rows. we use the same training schedule for all of the experiments.image
guidance is the main purpose of our method. to portray the effect of using the
image dataset, we train the kga-net using busv dataset alone in the first row of
table 2. without the image dataset, we generate the feature centers from the
video frames. as a result, the performance significantly drops due to the
decrease in dataset scale. it also shows that the feature centers generated by
the image dataset are more discriminative than that of the video dataset. it is
not only because the lesion number of busi is larger than busv, but also because
the images in busi are all the keyframes that contain typical characteristics of
lesions.frame attention and coherence loss are two essential modules of our
kga-net. we train a kga-net without the coherence loss in the third row of table
2.in the second row, we further replace the feature attention module with
feature averaging of video frames. it can be seen that both of these two modules
contribute to the overall performance according to auc and acc. it is worth
noting that these two models without coherence loss obtain very low sensitivity
and high specificity, which means the model predictions are imbalanced and
intend to make benign predictions. it is because that clear malignant
appearances usually only exist in limited frames in a malignant video. without
our coherence loss or frame attention, it is difficult for the model to focus on
typical frames that possess malignant features. this phenomenon certifies the
effectiveness of our kga-net to prevent false negatives in diagnosis. in fig. 3,
we illustrate video frames with their corresponding frame attention weights
predicted by kga-net. overall speaking, the frames with high attention weights
do have clear image appearances for diagnosis. for example, the first three
frames in fig. 3(b) clearly demonstrate the edge micro-lobulation and irregular
shapes, which lead to malignant judgment. furthermore, we plot the relationships
between the predicted attention values and the feature distances to the centers.
as shown in fig. 3(e), these two variables are linearly related, which indicates
that kga-net the attention weights are effectively guided by the feature
distances.the qualitative analysis proves the interpretability of our method,
which will benefit clinical usage. moreover, the attention weights reveal the
importance of each frame for lesion diagnosis. therefore, it can provide a new
perspective for the keyframe extraction task of ultrasound videos.
we propose kga-net, a novel video classification model for breast ultrasound
diagnosis. our kga-net takes as input both the video data and image data to
train the network. we propose the coherence loss to guide the training of the
video model by the guidance of feature centers of the images. our method
significantly exceeds the performance of other competitive video baselines. the
visualization of the attention weights validates the effectiveness and
interpretability of our kga-net.
advances in deep learning have been witnessed in many research areas over the
past decade. in medical field, automatic analysis of medical image data has
actively been studied. in particular, segmentation which identify region of
interest (roi) in an automatic way is an essential medical imaging process.
thus, deep learning-based segmentation has been utilized in various medical
domains such as brain, breast cancers, and colon polyps. among the popular
architectures, variants of u-net have been widely adopted due to their effective
encoderdecoder structure, proficient at capturing the characteristics of cells
in images. recently, it has been demonstrated that the attention modules
[4,17,20] enable deep learning networks to better extract robust features, which
can be applied in medical image segmentation to learn subtle medical features
and achieve higher performance [14,16,18,21].however, as image-only training
trains a model with pixels that constitute an image, there is a limit in
extracting fine-grained information about a target object even if transfer
learning is applied through a pre-trained model. recently, to overcome this
limitation, multi-modality studies have been conducted, aiming to enhance the
expressive power of both text and image features. for instance, clip [12] used
contrastive learning based on image-text pairs to learn the similarity between
the image of an object and the text describing it, achieving significant
performance gains in a variety of computer vision problems.the trend of
text-image multi-modality-based research on image processing has extended to the
medical field. [19] proposed a semantic matching loss that learns medical
knowledge to supplement the disadvantages of clip that cannot capture uncertain
medical semantic meaning. in [2], they trained to increase the similarity
between the image and text by calculating their influence on each other as a
weighted feature. for the segmentation task, lvit [10] generated the positional
characteristics of lesions or target objects as text labels. furthermore, it
proposed a double u-shaped structure consisting of a u-shaped vit that combines
image and text information and a u-shaped cnn that produces a segmentation mask.
however, when combining medical images with non-finegrained text information,
noise can affect the outcome.in this paper, we propose a new text-guided
cross-position attention module (cp am t g ) that combines text and image. in a
medical image, a position attention module (pam) effectively learns subtle
differences among pixels. we utilized pam which calculates the influence among
pixels of an image to capture the association between text and image. to this
end, we converted the global text representation generated from the text encoder
into a form, such as an image feature map, to create keys and values. the image
feature map generated from an image encoder was used as a query. learning the
association between text and image enables us to learn positional information of
targets in an image more effectively than existing models that learned
multi-modality from medical images. cp am t g showed an excellent segmentation
performance in our comprehensive experiments on various medical images, such as
cell, chest x-ray, and magnetic resonance image (mri). in addition, by applying
the proposed technique to the automatic roi setting module for the deep
learning-based diagnosis of sacroiliac arthritis, we confirmed that the proposed
method could be effective when it is used in a practical application of
computer-aided diagnosis.our main contributions are as follows:-we devised a
text-guided cross-position attention module (cp am t g ) that efficiently
combines text information with image feature maps. -we demonstrated the effect
of cp am t g on segmentation for various types of medical images. -for a
practical computer-aided diagnosis system, we confirm the effectiveness of the
proposed method in a deep learning-based sacroiliac arthritis diagnosis system.
in this section, we propose text-guided segmentation model that can effectively
learn the multi-modality of text and images. figure 1 shows the overall
architecture of the proposed model, which consists of an image encoder for
generating a feature map from an input image, a text encoder for embedding a
text describing the image, and a cross-attention module. the cross-attention
module allows the text to serve as a guide for image segmentation by using the
correlation between the global text representation and the image feature map. to
achieve robust text encoding, we adopt a transformer [17] structure which
performs well in natural language processing (nlp). for image encoding and
decoding, we employed u-net, widely used as a backbone in medical image
segmentation. to train our proposed model, we utilize a dataset consisting of
image and text pairs.
as transformer has demonstrated its effectiveness in handling the long-range
dependency in sequential data through self-attention [1], it performs well in
various fields requiring nlp or contextual information analysis of data. we used
a transformer (encoder t ) to encode the semantic information of the text
describing a medical image into a global text representation v t ∈ r 1×2c as v t
= encoder t (t ). here, the text semantics (t ) can be a sentence indicating the
location or characteristics of an interested region in an image such as a lesion
shown in fig. 1.to create a segmentation mask from medical images (i), we used
u-net [13] which has a relatively simple yet effective structure for biomedical
image segmen- tation. u-net operates as an end-to-end fully connected
network-based model consisting of a convolutional encoder and decoder connected
by skip connections. this architecture is particularly suitable for our purpose
because it can be successfully trained on a small amount of data. in the
proposed method, we used vgg-16 [15] as the encoder (encoder i ) to obtain the
image feature f i ∈ r c×h×w as f i = encoder i (i) and the decoder (decoder i )
that will generate the segmented image from the enhanced encoding vector
obtained by the cross-position attention which will be described in the
following subsection.the weights of text and image encoders were initialized by
the weights of clip's pre-trained transformer and vgg16 pre-trained on imagenet,
respectively, and fine-tuned by a loss function for segmentation which will be
described in sect. 3.
we introduce a text-guided cross-position attention module (cp am t g ) that
integrates cross-attention [3] with the position attention module (pam) [5] to
combine the semantic information of text and image. this module utilizes not
only the image feature map from the image encoder but also the global text
representation from the text encoder to learn the dependency between various
characteristics of text and image. pam models rich contextual relationships for
local features generated from fcns. it effectively captures spatial dependencies
among pixels by generating keys, queries, and values from feature maps. by
encoding broad contextual information into local features, and then adaptively
gathering spatial contexts, pam improves representation capability. in
particular, this correlation analysis among pixels can effectively analyze
medical images in which objects are relatively ambiguous compared to other types
of natural images.in fig. 2, we multiply the learnable parameter (l ∈ r 1×(hw )
) by the global text representation (v t ) to match the dimension of the text
feature with that of the image feature map as the text feature map f t is used
as key and value, and the image feature map f i is used as a query to perform
self-attention aswhere h q , h k , and h v are convolution layers with a kernel
size of 1, and q, k, and v are queries, keys, and values for
self-attention.finally, by upsampling the low-dimensional cp am t g obtained
through crossattention of text and image together with skip-connection, more
accurate segmentation prediction can express the detailed information of an
object.3 experiments
medical datasets. we evaluated cp am t g using three datasets: monuseg [8]
dataset, qata-cov19 [6] dataset, and sacroiliac joint (sij) dataset. the first
two datasets are the same benchmark datasets used in [10]. monuseg [8] contains
30 digital microscopic tissue images of several patients and qata-cov19 are
covid-19 chest x-ray images. the ratio of training, validation, and test sets
was the same as in [10]. sij is the dataset privately prepared for this study
which consists of 804 mri slices of nineteen healthy subjects and sixty patients
diagnosed with axial spondyloarthritis. among all mri slices, we selected the
gadoliniumenhanced fat-suppressed t1-weighted oblique coronal images, excluding
the first and last several slices in which the pelvic bones did not appear, and
added the text annotations for the slices.training and metrics. for a better
training, data augmentation was used. we randomly rotated images by -20 • ∼ +20
• and conducted a horizontal flip with 0.5 probability for only the monuseg and
qata-cov19 datasets. the batch size and learning rate were set to 2 and 0.001,
respectively. the loss function (l t ) for training is the sum of the binary
cross-entropy loss (l bce ) and the dice loss (l dice ):the mdice and miou
metrics, widely used to measure the performance of segmentation models, were
used to evaluate the performance of object segmentation. for experiments,
pytorch (v1.7.0) were used on a computer with nvidia-v100 32 gb gpu.
table 1 presents the comparison of image segmentation performance among the
proposed model and the u-net [13], u-net++ [22], attention u-net [11],medt [16],
and lvit [10] methods. analyzing the results in table 1, unlike natural image
segmentation, the attention module-based method (attention u-net) and
transformer-based method (medt) did not achieve significant performance gains
compared to u-net based methods (u-net and u-net++). by contrast, lvit and cp am
t g , which utilize both text and image information, significantly improved
image segmentation performance because of multimodal complementarity, even for
medical images with complex and ambiguous object boundaries. furthermore, cp am
t g achieves a better performance by 1 to 3% than lvit [10] on all datasets.
this means that the proposed cp am t g helps to improve segmentation performance
by allowing text information to serve as a guide for feature extraction for
segmentation.figure 3 shows the examples of segmentation masks obtained using
each method. in fig. 3, we marked the boundary of the target object with a red
box and showed the ground truth masks for these objects in the last column.
similar to the analysis that can be derived from table 1, fig. 3 shows that cp
am t g and lvit, which use text information together for image segmentation,
create a segmentation mask with more distinctive borders than other methods. in
particular, with sij, cp am t g accurately predicted the boundaries of even thin
bone parts compared to lvit. figure 3 also shows that even on the qata-cov19 and
monuseg datasets, cp am t g predicted the most accurate segmentation masks (see
the red box areas). from these results, we conjecture that the reasons for the
performance improvement of cp am t g are as follows. cp am t g independently
encodes the input text and image and then combines semantic information via a
cross-attention module. consequently, the two types of information (text and
image) do not act as noise from each other, and cp am t g achieves an improved
performance compared to lvit.
to validate the design of our proposed model, we perform an ablation study on
position attention and cp am t g . specifically, for the sij dataset, we
examined the effect of attention in extracting feature maps through comparison
with backbone networks (u-net) and pam. in addition, we investigated whether
text information about images serves as a guide in the position attention
process for image segmentation by comparing it with cp am t g . table 2
summarizes the result of each case. as can be observed in table 2, the
performance of pam was higher than that of the backbone. this indicates that pam
improves performance by learning associations between pixels for ambiguous
targets, as in medical images. in addition, the best performance results of cp
am t g show that text information provided helpful information in an image
segmentation process using the proposed model.
in this section, we confirm the effectiveness of the proposed segmentation
method through a practical bio-medical application as a deep learning-based
active sacroiliitis diagnosis system. mri is a representative means for early
diagnosis of "active sacroiliitis in axspa". as active sacroiliitis is a disease
that occurs between the pelvic bone and sacral bone, when a mr slice is input,
the diagnostic system first separates the area around the pelvic bone into an
roi patch and uses it as an input for the active sacroiliitis classification
network [7]. however, even in the same pelvis, the shape of the bone shown in mr
slices varies depending on the slice position of the mri and the texture of the
tissue around the bone is complex. this makes finding an accurate roi a
challenge.we segmented the pelvic bones in mri slices using the proposed method
to construct a fully automatic deep learning-based active sacroiliitis diagnosis
system, including roi settings from mri input images. figure 4 shows the results
of generating roi patches by dividing the pelvic bone from mri slices using the
proposed method. as presented in table 3, compared to the case of using the
original mri image without the roi setting, using the hand-crafted roi patch [9]
showed an average of 7% higher performance in recall, precision, and f1. it is
noticeable that the automatically set roi patch showed similar or better
performance than the manual roi patch for each measurement. this indicates that
the proposed method can be effectively utilized in practical applications of
computer-aided diagnosis.
in this study, we developed a new text-guided cross-attention module (cp am t g
) that learns text and image information together. the proposed model has a
composite structure of position attention and cross-attention in that the key
and value are from text data, and the query is created from the image. we use a
learnable parameter to convert text features into a tensor of the same dimension
as the image feature map to combine text and image information effectively. by
calculating the association between the reshaped global text representation and
each component of the image feature map, the proposed method outperformed image
segmentation performance compared to previous studies using both text and image
or image-only training method. we also confirmed that it could be utilized for a
deep-learning-based sacroiliac arthritis diagnosis system, one of the use cases
for practical medical applications. the proposed method can be further used in
various medical applications.
fig. 4. generating roi.
the spreading digitalisation of pathology labs has enabled the development of
deep learning (dl) tools that can assist pathologists in their daily tasks.
however, supervised dl methods require detailed annotations in whole-slide
images (wsis) which is time-consuming, expensive and prone to inter-observer
disagreements [6]. multiple instance learning (mil) alleviates the need for
detailed annotations and has seen increased adoption in recent years. mil
approaches have proven to work well in academic research on histopathology data
[1,17,29] as well as in commercial applications [26]. most mil methods for
digital pathology employ an attention mechanism as it increases the reliability
of the algorithms, which is essential for successful clinical adoption
[14].domain shift in dl occurs when the data distributions of testing and
training differs [20,34]. this remains a significant obstacle to the deployment
of dl applications in clinical practice [7]. to address this problem previous
work either use domain adaptation when data from the target domain is available
[32], or domain generalisation when the target data is unavailable [34]. domain
adaptation has been explored in the mil setting too [22,23,27]. however, it may
not be feasible to perform an explicit domain adaptation, and an already adapted
model could still experience problems with domain shifts. hence, it is important
to provide indications of the expected performance on a target dataset without
requiring annotations [5,25]. another related topic is out-of-distribution (ood)
detection [33] which aims to detect individual samples that are ood, in contrast
to our objective of estimating a difference of expected performances between
some datasets. for supervised algorithms, techniques of uncertainty estimation
have been used to measure the effect of domain shift [4,15,18] and to improve
the robustness of predictions [19,21,30]. however, the reliability of
uncertainty estimates can also be negatively affected by domain shifts [11,31].
alternatively, a drop in performance can be estimated by comparing the model's
softmax outputs [8] or some hidden features [24,28] acquired on in-domain and
domain shift datasets. although such methods have been demonstrated for
supervised algorithms, as far as we know no previous work has explored domain
shift in the specific context of mil algorithms. hence, it is not clear how well
they will work in such a scenario.in this work, we evaluate an attention-based
mil model on unseen data from a new hospital and propose a way to quantify the
domain shift severity. the model is trained to perform binary classification of
wsis from lymph nodes of breast cancer patients. we split the data from the new
hospital into several subsets to investigate clinically realistic scenarios
triggering different levels of domain shift. we show that our proposed
unsupervised metric for quantifying domain shift correlates best with the
changes in performance, in comparison to multiple baselines. the approach of
validating a mil algorithm in a new site without collecting new labels can
greatly reduce the cost and time of quality assurance efforts and ensure that
the models perform as expected in a variety of settings. the novel contributions
of our work can be summarised as:1. proposing an unsupervised metric named
fréchet domain distance (fdd) for quantifying the effects of domain shift in
attention-based mil; 2. showing how fdd can help to identify subsets of patient
cases for which mil performance is worse than reported on the in-domain test
data; 3. comparing the effectiveness of using uncertainty estimation versus
learnt representations for domain shift detection in mil.
our experiments center on an mil algorithm with attention developed for
classification in digital pathology. the two main components of our domain shift
quantification approach are the selection of mil model features to include and
the similarity metric to use, described below.
as the mil method for our investigation, we chose the
clustering-constrainedattention mil (clam) [17] because it well represents an
architecture of mil with attention, meaning that our approach can equally be
applied to many other such methods.
we explored several different feature sets that can be extracted from the
attention-based mil framework: learnt embedding of the instances (referred to as
patch features), and penultimate layer features (penultimate features). a study
is conducted to determine the best choices for type and amount of patch
features. as a baseline, we take a mean over all patches ignoring their
attention scores (mean patch features). alternatively, the patch features can be
selected based on the attention score assigned to them. positive evidence or
negative evidence are defined as the k patch features that have the k highest or
lowest attention scores, respectively. combined evidence is a combination of an
equal number of patch features with the highest and lowest attention scores. to
test if the reduction of the number of features in itself has a positive effect
on domain shift quantification, we also compare with k randomly selected patch
features.
fréchet inception distance (fid) [10] is commonly used to measure similarity
between real and synthetically generated data. inspired by fid, we propose a
metric named fréchet domain distance (fdd) for evaluating if a model is
experiencing a drop in performance on some new dataset. the fréchet distance
(fd) between two multivariate gaussian variables with means µ 1 , µ 2 and
covariance matrices c 1 , c 2 is defined as [3]:we are interested in using the
fd for measuring the domain shift between different wsi datasets x d . to this
end, we extract features from the mil model applied to all the wsis in x d , and
arrange these in a feature matrix f dd k uses the k aggregated positive evidence
features, but in the results we also compare to m d described from penultimate
features, mean patch features, and the other evidence feature selection
strategies.
grand challenge camelyon data [16] potentially large shift as some patients have
already started neoadjuvant treatment as well as the tissue may be affected from
the procedure of sentinel lymph node removal. 2a. 207 wsis with ductal carcinoma
(83 wsis with metastases): a small shift as it is the most common type of
carcinoma and relatively easy to diagnose. 2b. 68 wsis with lobular carcinoma
(28 wsis with metastases): potentially large shift as it is a rare type of
carcinoma and relatively difficult to diagnose.the datasets of lobular and
ductal carcinomas each contain 50 % of wsis from sentinel and axillary lymph
node procedures. the sentinel/axillary division is motivated by the differing dl
prediction performance on such subsets, as observed by jarkman et al. [13].
moreover, discussions with pathologists led to the conclusion that it is
clinically relevant to evaluate the performance difference between ductal and
lobular carcinoma. our method is intended to avoid requiring dedicated wsi
labelling efforts. we deem that the information needed to do this type of subset
divisions would be available without labelling since the patient cases in a
clinical setting would already contain such information. all datasets are
publicly available to be used in legal and ethical medical diagnostics research.
the goal of the study is to evaluate how well f dd k and the baseline methods
correlate with the drop in classification performance of attention-based mil
caused by several potential sources of domain shifts. in this section, we
describe the experiments we conducted.
we trained, with default settings, 10 clam models to classify wsis of breast
cancer metastases using a 10-fold cross-validation (cv) on the training data.
the test data was kept the same for all 10 models. the classification
performance is evaluated using the area under receiver operating characteristic
curve (roc-auc) and matthews correlation coefficient (mcc) [2]. following the
conclusions of [2] that mcc well represents the full confusion matrix and the
fact that in clinical practice a threshold needs to be set for a classification
decision, mcc is used as a primary metric of performance for domain shift
analysis while roc-auc is reported for completeness. whereas extremely large
variations in label prevalence could reduce the reliability of the mcc metric,
this is not the case here as label prevalence is similar (35-45%) in our test
datasets. for deep ensemble [15] we trained 4 additional clam models for each of
the 10 cv folds.
as there is no related work on domain shift detection in the mil setting, we
selected methods developed for supervised algorithms as baselines:-the model's
accumulated uncertainty between two datasets. deep ensemble [15] (de) and
difference in confidence with entropy [8] (doc) compare the mean entropy over
all data points. de uses an ensemble to estimate bettercalibrated uncertainty
than the single model in doc. -the accumulated confidence of a model across two
datasets. doc [8] can be measured on the mean softmax scores of two datasets. a
large difference indicates a potential drop in performance. -the hidden features
produced by an algorithm. representation shift [28] (rs) has shown promising
results in detecting domain shift in convolutional neural networks and it is the
method most similar to fdd. however, it is not trivial which hidden features of
mil that are most suitable for this task, and we evaluate several options (see
sect. 2.2) with both methods.for all possible pairs of camelyon and the other
test datasets, and for the 10 cv models, we compute the domain shift measures
and compare them to the observed drop in performance. the effectiveness is
evaluated by pearson correlation and visual investigation of corresponding
scatter plots. all results are reported as mean and standard deviation over the
10-fold cv.
the first part of our results is the performance of the wsi classification task
across the subsets, summarized in table 1. while showing similar trends, there
is some discrepancy in the level of domain shift represented by the datasets due
to the differences between the mcc and roc-auc measures.as we deemed mcc to
better represent the clinical use situation (see sect. 4.1), it was used for our
further evaluations. overall, the performance is in line with previously
published work [17,29]. we observe the largest domain shift in terms of mcc on
axillary nodes followed by lobular carcinoma and full brln datasets. there seems
to be no negative effect from processing the sentinel nodes data. clam models
achieved better performance on ductal carcinoma compared to the in-domain
camelyon test data.table 2 summarises the pearson correlation between the change
in performance, i.e., the mcc difference between camelyon and other test
datasets, and the domain shift measures for the same pairs. f dd 64 outperforms
the baselines substantially, and has the smallest standard deviation. figure 1
shows how individual drop in performance of model-dataset combinations are
related to the f dd 64 metric. for most models detecting larger drop in
performance (> 0.05) is easier on axillary lymph nodes data than on any other
analysed dataset. a study of the number and type of mil attention-based features
and fd and rs metrics is presented in fig. 2. the baseline of randomly selecting
patch features resulted in the worst outcome on domain shift detection. negative
evidence with fd achieved high pearson correlation when k = 4. however, the
results were among the worst with any other number of k. both combined and
positive evidence achieved peak performance of 0.68 (0.17) and 0.70 (0.13),
respectively, when fd and k = 64 were used. we conclude that in our setup the
best and most reliable performance of domain shift quantification is achieved by
positive evidence with fd and k = 64, i.e. f dd 64 .
mil is affected by domain shift. some previous work claim that mil is more
robust to domain shift as it is trained on more data due to the reduced costs of
data annotation [1,17]. we argue that domain shift will still be a factor to
consider as an algorithm deployed in clinical practice is likely to encounter
unseen varieties of data. however, it may require more effort to determine what
type of changes in data distribution are critical. our results show that domain
shift is present between the wsis from the same hospital (camelyon data) and
another medical centre (brln data). however, as clinically relevant subsets of
brln data are analysed, stark differences in performance and reliability
(indicated by the standard deviation) are revealed. therefore, having a reliable
metric for unsupervised domain shift quantification could bring value for
evaluating an algorithm at a new site. domain shift detection for supervised dl
struggles for mil. an important question is whether can we apply existing
techniques from supervised learning algorithms in the mil setting. the evaluated
baselines that use uncertainty and confidence aggregation for domain shift
detection, i.e., de and doc, showed poor ability to estimate the experienced
drop in performance (see table 2). it is known that supervised dl often suffers
from overconfident predictions [9]. this could be a potential cause for the
observed poor results by de and doc in our experiments. further investigation on
how to improve calibration could help to boost the applicability of uncertainty
and confidence measures.the proposed f dd k measure outperforms alternatives.
the highest pearson correlation between change in performance and a distance
metric is achieved by fréchet distance with 64 positive evidence features, f dd
64 (see table 2). rs 64 approach performed better than
uncertainty/confidence-based methods but still was substantially worse than f dd
64 . furthermore, f dd 64 resulted in the smallest standard deviation which is
an important indicator of the reliability of the metric. interestingly, using
penultimate layer features, which combine all patch features and attention
scores, resulted in much worse outcome than f dd 64 , 0.61 versus 0.70. thus, it
seems a critical component in domain shift measurement in attention-based mil is
to correctly make use of the attention scores. from fig. 1 we can see that if we
further investigated all modeldataset combinations that resulted in f dd 64
above 0.5, we would detect many cases with a drop in performance larger than
0.05. however, the drop is easier to detect on axillary and lobular datasets
compared to others. an interesting aspect is that the performance was better for
the out-of-domain ductal subset compared to in-domain camelyon wsis. in
practical applications, it may be a problem when the domain shift quantification
cannot separate between shifts having positive or negative effect on
performance. such differentiation could be the topic of future work.
we carried out a study on how clinically realistic domain shifts affect
attention-based mil for digital pathology. the results show that domain shift
may raise challenges in mil algorithms. furthermore, there is a clear benefit of
using attention for feature selection and our proposed f dd k metric for
quantification of expected performance drop. hence, f dd k could aid care
providers and vendors in ensuring safe deployment and operation of
attention-based mil in pathology laboratories.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9 16.
liver cancer is the third leading cause of cancer death world-wide in 2020 [14].
early detection and accurate diagnosis of liver tumors may improve overall
patient outcomes, in which imaging plays a key role [11]. computed tomography
(ct) is one of the most important imaging modalities for liver tumors. dynamic
contrast-enhanced (dce) ct is widely used for diagnostics, but it requires
iodine contrast injection which can cause reaction and potential risks in
patients. recently, non-contrast (nc) ct scans are gaining attention as they are
cheaper and safer to acquire, thus can be potential tools for opportunistic
tumor screening [18,20]. meanwhile, finding and diagnosing tumors in nc cts is
also extremely challenging because of the poor contrast between tumors and
normal tissues compared to those in dce cts. prior works on pancreas [18] and
esophagus [20] have shown that latest deep learning techniques can detect subtle
texture and shape changes in nc ct that even human eyes may miss. thus, we aim
to investigate the performance of liver tumor segmentation and classification in
nc cts. such an approach will be helpful to discover asymptomatic incidental
tumors [12] from routine nc ct scans indicated for general diagnostic purposes
at no additional cost and radiation exposure. after an incidental tumor is
found, the patient may undergo further imaging examination such as a multi-phase
dce ct for differential diagnosis [11], which can provide useful discriminative
information such as the vascularity of lesions and the pattern of contrast agent
enhancement [19]. liver is largest solid organ in body and is the site of many
tumor types [11]. therefore, accurate tumor type classification is important for
the decision of treatment plans and prognosis.many researchers have developed
algorithms to automatically segment [1,9,13,15,23] or classify [19,21,25] liver
tumors in ct to help radiologists improve their accuracy and efficiency. for
example, public datasets such as the liver tumor segmentation benchmark (lits)
[1] fostered a series of works aiming to segment liver tumors with improved
convolutional neural network (cnn) backbones [9,13] and lesion edge information
[15]. lits only has single-phase cts (venous phase). several studies
investigated methods to exploit multi-phase ct by methods such as hetero-phase
fusion [5] and modality-aware mutual learning [23]. there are few work
discussing liver tumor analysis in nc ct [5]. besides lesion segmentation,
cnn-based lesion classification algorithms have been studied to distinguish
common lesion types [19,21,25].in this paper, we build a comprehensive framework
to address both tumor screening and diagnosis. (1) tumor screening involves
finding tumor patients in a large pool of healthy subjects and patients. most
existing works in tumor segmentation and detection did not explicitly consider
it since their training and testing images are all tumor patients. such models
may generate false positives in real-world screening scenario when facing
diverse tumor-free images. we collect a large-scale dataset with both tumor and
non-tumor subjects, where the non-tumor subjects includes not only healthy ones,
but also patients with various diffuse liver diseases such as steatosis and
hepatitis to improve the robustness of the algorithm. (2) most works studied
liver tumor segmentation alone without differentiating tumor types, while a few
works classify liver tumors on cropped tumor patches [19,21,25]. meanwhile, we
learn tumor segmentation and classification with one network using an instance
segmentation framework [3]. we train two networks for nc and multi-phase dce
cts, respectively. (3) for evaluation, previous segmentation works typically use
pixel-level metrics such as dice coefficient. such metrics cannot reflect the
lesion-level accuracy (how many lesion instances are correctly detected and
classified) and may bias to large lesions when a patient has multiple tumors.
patient-level metrics (e.g. classifying whether a subject has malignant tumors)
are also useful for treatment recommendation in clinical practice [18,20].
therefore, we assess our algorithm thoroughly with pixel, lesion, and
patient-level metrics.algorithms for liver tumor segmentation have focused on
improving the feature extraction backbone of a fully-convolutional cnn
[9,13,15,23]. the pixelwise segmentation architectures may not be optimal for
lesion and patient-level evaluation metrics since they cannot consider a lesion
or an image holistically. recently, a series of mask transformer algorithms
[3,4,17] have emerged in the computer vision community and achieved the
state-of-the-art performance in instance segmentation tasks. in brief, they use
object queries to interact with image feature maps and with each other to
produce mask and class predictions for each instance. inspired by them, we
propose a novel end-to-end framework named pixel-lesion-patient network (plan)
for lesion segmentation and classification, as well as patient classification.
it contains three branches with bottomup cooperation: the segmentation map from
the pixel branch helps to initialize the lesion branch, which is an improved
mask transformer aiming to segment and classify each lesion; the patient branch
aggregates information from the whole image and predicts image-level labels of
each lesion type, with regularization terms to encourage consistency with the
lesion branch.we collected a large-scale multi-phase dataset containing 810
non-tumor subjects and 939 tumor patients. 4010 tumor instances of eight types
are extensively annotated based on pathological reports. on the non-contrast
tumor screening and diagnosis task, plan achieves 95.0%, 96.4%, and 0.965 in
patient-level sensitivity, specificity, and average auc for malignant and benign
patients, in contrast to 94.4%, 93.7%, and 0.889 for the widely-used nnu-net
[8]. on multi-phase dce ct, our lesion-level detection precision, recall, and
classification accuracy are 92.2%, 89.0%, 85.9%, outperforming nnu-net [8] and
mask2former [3]. we further conduct a reader study on a holdout set of 250
cases. our algorithm is on par with a senior radiologist (16 yrs experience),
showing the clinical significance of our results. our codes will be made public
upon institutional approval.
mask transformers are a series of latest works achieving superior accuracy on
various segmentation tasks [3,4,17,22]. different from traditional
fullyconvolutional segmentators [8] that predict a class label for each pixel,
mask transformers predict a class label and a binary mask for each object. take
mask2former [3] as an example. it includes a pixel encoder and a pixel decoder
that extract a high-resolution pixel embedding tensor p ∈ r m ×d×h×w from the
image, where m is the embedding dimension, d × h × w is the shape of the 3d
image. a group of q learnable feature vectors {q i ∈ r m } q i=1 are randomly
initialized as object queries. they are processed by a transformer decoder to
interact with multi-scale image features and each other using cross and
self-attention operations. after processing, each query is supposed to contain
information of one object, which can be used to predict the class probability c
∈ r c+1 of the object. here c is the number of object classes, and we add 1 to
indicate an additional "no-object" class if the query does not match with any
object. in training, mask2former uses bipartite matching [2] to assign each
query to a ground-truth object (or "no-object"). multiplying q i with p gives
the binary mask m i ∈ r d×h×w of object i. during inference, the class and mask
predictions of all queries can be merged by matrix multiplication to obtain the
final semantic segmentation result ŷ ∈ r c×d×h×w . we refer readers to [3] for
more details.mask transformers have various advantages when applied to our task.
they can classify a lesion as a whole instead of classifying each pixel, thus
can view each lesion holistically. cross-attention is used to aggregate global
features for each lesion. inter-lesion relation can also be exploited by
self-attention operations. in liver ct, inter-lesion relation is diagnostically
useful, e.g., metastases and cysts are often multiple. therefore, we pioneer
mask transformers' adaptation for lesion segmentation and classification in 3d
medical images. given a groundtruth or a predicted lesion mask image, we perform
connected component (cc) analysis and treat each cc as a lesion instance for
training and evaluation.
our goal is to segment the mask and classify the type of each tumor in a liver
ct. we also hope to make patient-level diagnoses for each ct scan. plan is
inspired by mask2former [3] with three key improvements: (1) a pixel branch is
added to provide anchor queries to the lesion branch. (2) the lesion branch is
composed of the transformer decoder in mask2former, and we improve its
segmentation loss to enhance recall of small lesions. (3) a patient branch is
attached to make dedicated image-level predictions with a proposed
lesion-patient consistency loss. our framework is shown in fig. 1.pixel branch
and anchor queries. the pixel branch is a convolutional layer after the pixel
decoder and learns to predict pixel-wise segmentation maps similar to
traditional segmentators. we do cc analysis to the predicted mask to extract
lesion instances, and then average the pixel embeddings inside each predicted
lesion to obtain a feature vector. the feature vectors are regarded as anchor
queries and work the same way as the randomly initialized queries in the lesion
branch. compared to the random queries in the original mask2former, the anchor
queries contain prior information of the lesions to be segmented, helping the
lesion branch to match with the lesion targets more easily [10].lesion branch
and foreground-enhanced sampling loss. similar to mask2former, the lesion branch
predicts a binary mask and a class label for each query, see fig. 1. mask2former
calculates its segmentation loss on k sampled pixels instead of on the whole
image, which is shown to both improve accuracy and reduce gpu memory usage [3].
however, in lesion segmentation, some tumors are very small compared to the
whole 3d image. the importance sampling strategy [3] can hardly select any
foreground pixels in such cases, so the loss only contains background pixels,
degrading the segmentation recall of small lesions. we propose a simple approach
to remedy this issue by sampling an extra n foreground pixels for each
lesion.patient branch. a patient-level diagnosis is useful for triage. for
example, diagnosing the subject as normal, benign, or malignant will result in
completely different treatments [24]. intuitively, we can also infer
patient-level labels from segmentation results by checking if there is any
lesion in the predicted mask. however, certain tumors are often related to signs
outside the tumor, e.g. hepatocellular carcinoma and cirrhosis,
cholangiocarcinoma and bile duct dilatation, etc. we equip plan with a dedicated
patient branch to aggregate such global information to make better patient-level
prediction. since one patient can have multiple liver tumors of different types,
in our problem, we give each image several hierarchical binary labels. the first
label classifies normal and tumor subjects (whether the image contains any
tumor); the second and third labels indicate the existence of respectively
benign and malignant tumors; the rest c labels suggest the existence of c
fine-grained types of tumors. we employ the dual-path transformer block [17] to
fuse multi-scale features from the pixel encoder and decoder to generate a
feature map, followed by global average pooling and a linear classification
layer to predict the c + 3 labels.a lesion-patient consistency loss is further
proposed to encourage coherence of the lesion and patient-level predictions.
inspired by multi-instance learning [6], we compute a pseudo patient-level
prediction c ∈ r c from the lesion-level predictions by max-pooling the class
probability of each class across all lesion queries (discarding the no-object
class). we also have the probability vector from the patient branch p ∈ r c
corresponding to the c fine-grained classes. then, we compute the l2 loss
between them:the overall loss of plan is listed in eq. 1, where l pixel is the
combined crossentropy (ce) and dice loss for the pixel branch as in nnu-net [8];
l lesion-class is the ce loss [3] for lesion classification in the lesion
branch; l lesion-mask is the combined ce and dice loss [3] for binary lesion
segmentation in the lesion branch with the foreground-enhanced sampling
strategy; l patient is the binary ce loss for the multi-label classification
task in the patient branch.
data. our dataset contains 810 normal subjects and 939 patients with liver
tumors. each normal subject has a non-contrast (nc) ct, while each patient has a
dynamic contrast-enhanced (dce) ct scan with nc, arterial, and venous phases. we
use deeds [7] to register nc and arterial phases to the venous phase, and then
invite a senior radiologist with 10 years of experience to annotate on the
multi-phase cts using ct labeler [16]. the 3d mask and the type of all liver
tumors are annotated based on pathological reports and magnetic resonance scans
if necessary. eight tumor types are considered in our study: hepatocellular
carcinoma (hcc), intrahepatic cholangiocarcinoma (icc), metastasis (meta),
hepatoblastoma (hepato), hemangioma (heman), focal nodular hyperplasia (fnh),
cyst, and others (all other tumor types). if a lesion's type cannot be
determined according to image signs [11] and pathology, it will be marked as
"unknown" and ignored in training and evaluation. in total, 4010 tumor instances
are annotated, whose volumes range from 11 to 3.7×10 6 mm 3 . detailed
statistics and examples of the lesions are shown in the supplementary material.
we train two separate networks for nc and dce cts. in the former setting, both
normal and patient data are used and randomly split into 1149 training, 100
validation, and 500 testing. in the latter one, only patient data are used with
641 training, 100 validation, and 200 testing. another hold-out set of 150
patients and 100 normal cts are used for reader study to compare our accuracy
with two radiologists. implementation details. each ct is resampled to
0.7×0.7×5mm in spacing. we first train an nnu-net on public datasets to segment
liver and surrounding organs (gallbladder, hepatic vein, spleen, stomach, and
pancreas), and then crop the liver region to train plan. to help plan
differentiate liver tumors and other organs, we train the network to segment
both tumors and organs using the predicted organ labels. plan is built on top of
the nnu-net framework [8]. its pixel encoder is a u-net encoder, whereas its
pixel decoder is a light-weight feature pyramid network [3]. the lesion branch
incorporates three transformer decoder blocks with masked attention [3] which
use feature maps of strides 16, 8, 4 from the pixel decoder. the number of
random queries is q = 20; the embedding dimension is m = 64; the number of
sampled pixels is k = 12544 [3], foreground pixels n = 3; the loss weight is 0.1
for the no-object class while 1 for other classes in the lesion branch [3]. the
weights in eq. 1 arewe use the radam optimizer with an initial learning rate of
0.0001. each training batch contains two patches of size 256 × 256 × 24. for dce
ct, the three phases form a 3-channel image as the network input. extensive data
augmentation is applied including random cropping, scaling, flipping, elastic
deformation, and brightness adjustment [8].during training, we first pretrain
the backbone and the pixel branch for 500 epochs, and then train the whole
network for another 500 epochs.patient-level results. this paper has three major
goals: tumor screening in nc ct (classifying a subject as normal or tumor),
preliminary diagnosis in nc ct (predicting the existence of malignant and benign
tumors), and fine-grained diagnosis in dce ct (predicting the existence of 8
tumor types). among the 8 tumor types, hcc, icc, meta, and hepato are malignant;
heman, fnh, and cyst are benign. "others" can be either malignant or benign,
thus are excluded in the preliminary diagnosis task. the nc test set contains
198 tumor cases, 202 completely normal cases, and 100 "hard" non-tumor cases
which may have larger image noise, artifact, ascites, diffuse liver diseases
such as hepatitis and steatosis. these cases are used to test the robustness of
the model in real-world screening scenario with diverse tumor-free images. we
compare plan with a widely-used strong baseline, nnu-net [8]. the recent mask
transformer, mask2former [3], is also adapted to 3d for comparison. for the
baselines, patient-level labels are inferred from their predicted masks by
counting lesion pixels. as displayed in table 1, plan achieves the best accuracy
on all tasks, especially in nc preliminary diagnosis tasks, which demonstrates
the effectiveness of its dedicated patient branch that can explicitly aggregate
features from the whole image.lesion and pixel-level results. in lesion-level
evaluation, we treat a prediction as a true positive if its overlap with a
ground-truth lesion is >0.2 in dice. lesions smaller than 3 mm in radius are
ignored. as shown in table 2, the pixellevel accuracy of nnu-net and plan are
comparable, but plan's lesion-level accuracy is consistently higher than
nnu-net. in this work, we focus more on patient and lesion-level metrics.
although nc images have low contrast, they can still be used to segment and
classify lesions with ∼ 80% precision, recall, and classification accuracy. it
implies the potential of nc ct, which has been understudied in previous works.
mask2former has higher precision but lower recall in nc ct, especially for small
lesions, while plan achieves the best recall using the foreground-enhanced
sampling loss. both plan and mask2former achieve better classification accuracy,
which illustrates the mask transformer architecture is good at lesion-level
classification.
in the reader study, we invited a senior radiologist with 16 years of experience
in liver imaging, and a junior radiologist with 2 years of experience. they
first read the nc ct of all subjects and provided a diagnosis of normal, benign,
or malignant. then, they read the dce scans and provided a diagnosis of the 8
tumor types. we consider patients with only one tumor type in this study. their
reading process is without time constraint. in table 3 and fig. 2, all methods
get good specificity probably because the normal subjects are completely
healthy. our model achieves comparable accuracy with the senior radiologist but
outperforms the junior one by a large margin in sensitivity and classification
accuracy. an ablation study for our method is shown in table 4. it can be seen
that our proposed anchor queries produced by the pixel branch, fes loss, and
lesionpatient consistency loss are useful for the final performance. the
efficacy of the lesion and patient branches has been analyzed above based on the
lesion and patient-level results. due to space limit, we will show the accuracy
for each tumor type and more qualitative examples in the supplementary
material.comparison with literature. in the pixel level, we obtain dice scores
of 77.2% and 84.2% using nc and dce cts, respectively. the current state of the
art (sota) of lits [1] achieved 82.2% in dice using cts in venous phase; [23]
achieved 81.3% in dice using dce ct of two phases. in the lesion level, our
precision and recall are 80.1% and 81.9% for nc ct, 92.2% and 89.0% for dce ct,
at 20% overlap. [25] achieved 83% and 93% for dce ct. sota of lits achieved
49.7% and 46.3% at 50% overlap. [21] classified lesions into 5 classes,
achieving 84% accuracy for dce and 49% for nc ct. we classify lesions into 8
classes with 85.9% accuracy for dce and 78.5% for nc ct. in the patient level,
[5] achieved auc=0.75 in nc ct tumor screening, while our auc is 0.985. in
summary, our results are superior or comparable to existing works.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9 8.
endoscopic operations are minimally invasive medical procedures which allow
physicians to examine inner body organs and cavities. during an endoscopy, a
thin, flexible tube with a tiny camera is inserted into the body through a small
orifice or incision. it is used to diagnose and treat a variety of conditions,
including ulcers, polyps, tumors, and inflammation. over 250 million endoscopic
procedures are performed each year globally and 80 million in the united states,
signifying the crucial role of endoscopy in clinical research and care.a
cardinal challenge in performing endoscopy is the limited field of view which
hinders navigation and proper visual assessment, potentially leading to high
detection miss-rate, incorrect diagnosis or insufficient treatment. these
limitations have fostered the development of computer-aided systems based on
artificial intelligence (ai), resulting in unprecedented performance over a
broad range of clinical applications [10,11,17,[23][24][25]. yet the success of
such ai systems heavily relies on acquiring annotated data which requires
experts of specific knowledge, leading to an expensive, prolonged process. in
the last few years, self-supervised learning (ssl [5][6][7][8]) has been shown
to be a revolutionary strategy for unsupervised representation learning,
eliminating the need to manually annotate vast quantities of data. training
large models on sizable unlabeled data via ssl leads to powerful representations
which are effective for downstream tasks with few labels. however, research in
endoscopic video analysis has only scratched the surface of ssl which remains
largely unexplored.this study introduces masked siamese networks (msns [2]), a
prominent ssl framework, into endoscopic video analysis where we focus on
laparoscopy and colonoscopy. we first experiment solely on public datasets,
cholec80 [32] and polypsset [33], demonstrating performance on-par with the top
results reported in the literature. yet, the power of ssl lies in large data
regimes. therefore, to exploit msns to their full extent, we collect and build
two sizable unlabeled datasets for laparoscopy and colonoscopy with 7, 700
videos (>23m frames) and 14, 000 videos (>2m frames) respectively. through
extensive experiments, we find that scaling the data size necessitates scaling
the model architecture, leading to state-of-the-art performance in surgical
phase recognition of laparoscopic procedures, as well as in polyp
characterization of colonoscopic videos. furthermore, the proposed approach
exhibits robust generalization, yielding better performance with only 50% of the
annotated data, compared with standard supervised learning using the complete
labeled dataset. this shows the potential to reduce significantly the need for
expensive annotated medical data.
there exist a wide variety of endoscopic applications. here, we focus on
colonoscopy and laparoscopy, which combined covers over 70% of all endoscopic
procedures. specifically, our study addresses two important common tasks,
described below.cholecystectomy phase recognition. cholecystectomy is the
surgical removal of the gallbladder using small incisions and specialized
instruments. it is a common procedure performed to treat gallstones,
inflammation, or other conditions affecting the gallbladder. phase recognition
in surgical videos is an important task that aims to improve surgical workflow
and efficiency. apart from measuring quality and monitoring adverse event, this
task also serves in facilitating education, statistical analysis, and evaluating
surgical performance. furthermore, the ability to recognize phases allows
real-time monitoring and decision-making assistance during surgery, thus
improving patient safety and outcomes. ai solutions have shown remarkable
performance in recognizing surgical phases of cholecystectomy procedures
[17,18,32]; however, they typically require large labelled training datasets. as
an alternative, ssl methods have been developed [12,28,30], however, these are
early-days methods that based on heuristic, often require external information
and leads to sub-optimal performance. a recent work [27] presented an extensive
analysis of modern ssl techniques for surgical computer vision, yet on
relatively small laparoscopic datasets.optical polyp characterization.
colorectal cancer (crc) remains a critical health concern and significant
financial burden worldwide. optical colonoscopy is the standard of care
screening procedure for preventing crc through the identification and removal of
polyps [3]. according to colonoscopy guidelines, all identified polyps must be
removed and histologically evaluated regardless of their malignant nature.
optical biopsy enables practitioners to remove pre-cancerous adenoma polyps or
leave distal hyperplastic polyps in situ without the need for pathology
examination, by visually predicting histology. however, this technique is highly
dependent on operator expertise [14]. this limitation has motivated the
development of ai systems for automatic optical biopsy, allowing non-experts to
also effectively perform optical biopsy during polyp management. in recent
years, various ai systems have been developed to this end [1,19]. however,
training such automatic optical biopsy systems relies on a large body of
annotated data, while ssl has not been investigated in this context, to the best
of our knowledge.3 self-supervised learning for endoscopy ssl approaches have
produced impressive results recently [5][6][7][8], relying on two key factors:
(i) effective algorithms for unsupervised learning and (ii) training on
large-scale datasets. here, we first describe masked siamese networks [2], our
chosen ssl framework. additionally, we present our large-scale data collection
(see fig. 2). through extensive experiments in sect. 4, we show that training
msns on these substantial datasets unlocks their potential, yielding effective
representations that transfer well to public laparoscopy and colonoscopy
datasets.
ssl has become an active research area, giving rise to efficient learning
methods such as simclr [7], swav [5] and dino [6]. recently, masked siamese
networks [2] have set a new state-of-the-art among ssl methods on the imagenet
benchmark [29], with a particular focus on the low data regime. this is of great
interest for us since our downstream datasets are typically of small size
[32,33]. we briefly describe msns below and refer the reader to [2] for further
details.during pretraining, on each image x i ∈ r n of a mini-batch of b ≥ 1
samples (e.g. laparoscopic images) we apply two sets of random augmentations to
generate anchor and target views, denoted by x a i and x t i respectively. we
convert each view into a sequence of non-overlapping patches and perform an
additional masking ("random" or "focal" styles) step on the anchor view by
randomly discarding some of its patches. the resultant anchor and target
sequences are used as inputs to their respective image encoders f θ a and f θ t
. both encoders share the same vision transformer (vit [16]) architecture where
the parameters θ t of the target encoder are updated via an exponential moving
average of the anchor encoder parameters θ a . the outputs of the networks are
the representation vectors z a i ∈ r d and z t i ∈ r d , corresponding to the
[cls] tokens of the networks. the similarity between each view and a series of k
> 1 learnable prototypes is then computed, and the results undergo a softmax
operation to yield the following probabilities p a i = sof tmaxwhere 0 < τ t < τ
a < 1 are temperatures and q ∈ r k×d is a matrix whose rows are the prototypes.
the probabilities are promoted to be the same by minimizing the cross-entropy
loss h(p t i , p a i ), as illustrated in fig. 1. in practice, a sequence of m ≥
1 anchor views are generated, leading to multiple probabilities {p a i,m } m m=1
. furthermore, to prevent representation collapse and encourage the model to
fully exploit the prototypes, a mean entropy maximization (me-max) regularizer
[2,22] is added, aiming to maximize the entropy h(p a ) of the average
prediction across all the anchor views pa. thus, the overall training objective
to be minimized for both θ a and q is where λ > 0 is an hyperparameter and the
gradients are computed only with respect to the anchor predictions p a i,m (not
the target predictions p t i ). applying msns on the large datasets described
below, generates representations that serve as a strong basis for various
downstream tasks, as shown in the next section.
laparoscopy. we compiled a dataset of laparoscopic procedures videos exclusively
performed on patients aged 18 years or older. the dataset consists of 7,877
videos recorded at eight different medical centers in israel. the dataset
predominantly consists of the following procedures: cholecystectomy (35%),
appendectomy (20%), herniorrhaphy (12%), colectomy (6%), and bariatric surgery
(5%). the remaining 21% of the dataset encompasses various standard laparoscopic
operations. the recorded procedures have an average duration of 47 min, with a
median duration of 40 min. each video recording was sampled at a rate of 1 frame
per second (fps), resulting in an extensive dataset containing 23.3 million
images. further details are given in the supplementary materials.colonoscopy. we
have curated a dataset comprising 13,979 colonoscopy videos of patients aged 18
years or older. these videos were recorded during standard colonoscopy
procedures performed at six different medical centers between the years 2019 and
2022. the average duration of the recorded procedures is 15 min, with a median
duration of 13 min. to identify and extract polyps from the videos, we employed
a pretrained polyp detection model [21,25,26]. using this model, we obtained
bounding boxes around the detected polyps. to ensure high-quality data, we
filtered out detections with confidence scores below 0.5. for each frame, we
cropped the bounding boxes to generate individual images of the polyps. this
process resulted in a comprehensive collection of 2.2 million polyp images.
in this section, we empirically demonstrate the power of ssl in the context of
endoscopy. our experimental protocol is the following: (i) first, we perform ssl
pretraining with msns over our unlabeled private dataset to learn informative
and generic representations, (ii) second we probe these representations by
utilizing them for different public downstream tasks. specifically, we use the
following two benchmarks. (a) cholec80 [32]: 80 videos of cholecystectomy
procedures resulting in nearly 200k frames at 1 fps. senior surgeons annotated
each frame to one out of seven phases. (b) polypsset [33]: a unified dataset of
155 colonoscopy videos (37,899 frames) with labeled polyp classes (hyperplastic
or adenoma) and bounding boxes. we use the provided detections to perform binary
classification. downstream task evaluation protocols. (a) linear evaluation: a
standard protocol consisting in learning a linear classifier on top of frozen
ssl features [6,20]. (b) temporal evaluation: a natural extension of the linear
protocol where we learn a temporal model on top of the frame-level frozen
features. we specifically use multi-stage temporal convolution networks (ms-tcn)
as used in [13,27]. this incorporates the temporal context which is crucial for
video tasks such as phases recognition. (c) fine-tuning: an end-to-end training
of a classification head on top of the (unfrozen) pretrained backbone. we
perform an extensive hyperparameter grid search for all downstream experiments
and report the test results for the models that exceed the best validation
results. we report the macro f1 (f-f1) as our primary metric. for phase
recognition we also report the per-video f1 (v-f1), computed by averaging the f1
scores across all videos [27].implementation details. for ssl we re-implemented
msns in jax using scenic library [15]. as our image encoders we train vision
transformer (vit [16]) of different sizes, abbreviated as vit-s/b/l, using 16
tpus. downstream experiments are implemented in tensorflow where training is
performed on 4 nvidia tesla v100 gpus. see the supplementary for further
implementation details. 1
scaling laws of ssl. we explore large scale ssl pretraining for endoscopy
videos. table 1 compares the results of pretraining with different datasets
(public and private) and model sizes. we pretrain the models with msn and then
report their downstream performances. we present results for the cholecystectomy
phase recognition task based on fine-tuned models and for the optical polyp
characterization task based on linear evaluation, due to the small size of the
public dataset. as baselines, we report fully-supervised resnet50 results,
trained on public datasets. we find that replacing resnet50 with vit-s, despite
comparable number of parameters, yields sub-optimal performance. ssl pretraining
on public datasets (without labels) provides comparable or better results than
fully supervised baselines. the performance in per-frame phase recognition is
comparable with the baseline. phase recognition per-video results improve by 1.3
points when using the msn pretraining, while polyp characterization improve by
2.2 points. importantly, we see that the performance gap becomes prominent when
using the large scale private datasets for ssl pretraining. here, per-frame and
per-video phase recognition performances improve by 6.7% and 8.2%, respectively.
when using the private colonoscopy dataset the macro f1 improves by 11.5%
compared to the fully supervised baseline. notice that the performance improves
with scaling both model and private data sizes, demonstrating that both factors
are crucial to achieve optimal performance. low-shot regime. next, we examine
the benefits of using msns to improve downstream performance in a low-shot
regime with few annotated samples. note that msns have originally been found to
produce excellent features for low data regime [2]. we train a linear classifier
on top of the extracted features and report the test classification results.
figure 3 shows the low-shot performance for the two endoscopic tasks. we report
results using a fraction k = {12%, 25%, 50%, 75%, 100%} of the annotated public
videos. we also report results for fully-supervised baselines trained on the
same fraction of annotated samples. each experiment is repeated three times with
a random sample of train videos, and we report the mean and standard deviation
(shaded area).as seen, ssl-based models provide enhanced robustness to limited
annotations. when examining the cholecystectomy phase recognition task, it is
evident that we can achieve comparable frame-level performance by using only 12%
of the annotated videos. using 25% of the annotated videos yields comparable
results to the fully supervised temporal models. optical polyp characterization
results show a similar trend, but with a greater degree of variability. using
small portions of polypset (12% and 25%) hindered the training process and
increased sensitivity to the selected portions. however, when using more than
50% of polypset, the training process stabilized, yielding results comparable to
the fully supervised baseline. this feature is crucial for medical applications,
given the time and cost involved in expert-led annotation processes.
table 2 details different design choices regarding our ssl pretraining.
ablations are done on vit-s trained over the public cholec80. we report results
on the validation set after linear evaluation. in table 2a), we see that the
method is robust to the number of prototypes, though over-clustering [4] with 1k
prototypes is optimal. in table 2b) and table 2c), we explore the effect of
random and focal masking. we see that 50% random masking (i.e. we keep 98 tokens
out of 196 for the global view) and using 4 local views gives the best of
performance. in table 2d) we study the effect of data augmentation. ssl
augmentation pipelines have been developed on imagenet-1k [7], hence, it is
important to re-evaluate these choices for medical images. surprisingly, we see
that augmentations primarily found to work well on imagenet-1k are also
effective on laparoscopic videos (e.g. color jiterring and horizontal flips). in
table2e), we look at the effect of the training length when starting from
scratch or from a good ssl pretrained checkpoint on imagenet-1k. we observe that
excellent performance is achieved with only 10 epochs of finetuning on medical
data when starting from a strong dino checkpoint [6]. table 2g) shows that
imagenet-1k dino is a solid starting point compared to other alternatives
[9,20,31,34]. finally, table2f) confirms the necessity of regularizing with
sinkhorn-knopp and me-max to avoid representation collapse by encouraging the
use of all prototypes.
this study showcases the use of masked siamese networks to learn informative
representations from large, unlabeled endoscopic datasets. the learnt
representations lead to state-of-the-art results in identifying surgical phases
of laparoscopic procedures and in optical characterization of colorectal polyps.
moreover, this methodology displays strong generalization, achieving comparable
performance with just 50% of labeled data compared to standard supervised
training on the complete labeled datasets. this dramatically reduces the need
for annotated medical data, thereby facilitating the development of ai methods
for healthcare.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43904-9_55.
head and neck (h&n) cancer refers to malignant tumors in h&n regions, which is
among the most common cancers worldwide [1]. survival prediction, a regression
task that models the survival outcomes of patients, is crucial for h&n cancer
patients: it provides early prognostic information to guide treatment planning
and potentially improves the overall survival outcomes of patients [2].
multi-modality imaging of positron emission tomography -computed tomography
(pet-ct) has been shown to benefit survival prediction as it offers both
anatomical (ct) and metabolic (pet) information about tumors [3,4]. therefore,
survival prediction from pet-ct images in h&n cancer has attracted wide
attention and serves as a key research area. for instance, head and neck tumor
segmentation and outcome prediction challenges (hecktor) have been held for the
last three years to facilitate the development of new algorithms for survival
prediction from pet-ct images in h&n cancer [5][6][7].traditional survival
prediction methods are usually based on radiomics [8], where handcrafted
radiomics features are extracted from pre-segmented tumor regions and then are
modeled by statistical survival models, such as the cox proportional hazard
(coxph) model [9]. in addition, deep survival models based on deep learning have
been proposed to perform end-to-end survival prediction from medical images,
where pre-segmented tumor masks are often unrequired [10]. deep survival models
usually adopt convolutional neural networks (cnns) to extract image features,
and recently visual transformers (vit) have been adopted for its capabilities to
capture long-range dependency within images [11,12]. these deep survival models
have shown the potential to outperform traditional survival prediction methods
[13]. for survival prediction in h&n cancer, deep survival models have achieved
top performance in the hecktor 2021/2022 and are regarded as state-of-the-art
[14][15][16]. nevertheless, we identified that existing deep survival models
still have two main limitations.firstly, existing deep survival models are
underdeveloped in utilizing complementary multi-modality information, such as
the metabolic and anatomical information in pet and ct images. for survival
prediction in h&n cancer, existing methods usually use single imaging modality
[17,18] or rely on early fusion (i.e., concatenating multi-modality images as
multi-channel inputs) to combine multi-modality information [11,[14][15][16]19].
in addition, late fusion has been used for survival prediction in other diseases
such as gliomas and tuberculosis [20,21], where multi-modality features were
extracted by multiple independent encoders with resultant features fused.
however, early fusion has difficulties in extracting intra-modality information
due to entangled (concatenated) images for feature extraction, while late fusion
has difficulties in extracting inter-modality information due to fully
independent feature extraction. recently, tang et al. [22] attempted to address
this limitation by proposing a multi-scale non-local attention fusion (mnaf)
block for survival prediction of glioma patients, in which multi-modality
features were fused via non-local attention mechanism [23] at multiple scales.
however, the performance of this method heavily relies on using tumor
segmentation masks as inputs, which limits its generalizability.secondly,
although deep survival models have advantages in performing end-to-end survival
prediction without requiring tumor masks, this also incurs difficulties in
extracting region-specific information, such as the prognostic information in
primary tumor (pt) and metastatic lymph node (mln) regions. to address this
limitation, recent deep survival models adopted multi-task learning for joint
tumor segmentation and survival prediction, to implicitly guide the model to
extract features related to tumor regions [11,16,[24][25][26]. however, most of
them only considered pt segmentation and ignored the prognostic information in
mln regions [11,[24][25][26]. meng et al. [16] performed survival prediction
with joint pt-mln segmentation and achieved one of the top performances in
hecktor 2022. however, this method extracted entangled features related to both
pt and mln regions, which incurs difficulties in discovering the prognostic
information in pt-/mln-only regions.in this study, we design an x-shape
merging-diverging hybrid transformer network (named xsurv, fig. 1) for survival
prediction in h&n cancer. our xsurv has a merging encoder to fuse complementary
anatomical and metabolic information in pet and ct images and has a diverging
decoder to extract region-specific prognostic information in pt and mln regions.
our technical contributions in xsurv are three folds: (i) we propose a
merging-diverging learning framework for survival prediction. this framework is
specialized in leveraging multi-modality images and extracting regionspecific
information, which potentially could be applied to many survival prediction
tasks with multi-modality imaging. (ii) we propose a hybrid parallel
cross-attention (hpca) block for multi-modality feature learning, where both
local intra-modality and global inter-modality features are learned via parallel
convolutional layers and crossattention transformers. (iii) we propose a
region-specific attention gate (rag) block for region-specific feature
extraction, which screens out the features related to lesion regions. extensive
experiments on the public dataset of hecktor 2022 [7] demonstrate that our xsurv
outperforms state-of-the-art survival prediction methods, including the
top-performing methods in hecktor 2022.
figure 1 illustrates the overall architecture of our xsurv, which presents an
x-shape architecture consisting of a merging encoder for multi-modality feature
learning and a diverging decoder for region-specific feature extraction. the
encoder includes two pet-/ct-specific feature learning branches with hpca blocks
(refer to sect. 2.1), while the decoder includes two pt-/mln-specific feature
extraction branches with rag blocks (refer to sect. 2.2). our xsurv performs
joint survival prediction and segmentation, where the two decoder branches are
trained to perform pt/mln segmentation and provide pt-/mln-related deep features
for survival prediction (refer to sect. 2.3). our xsurv also can be enhanced by
leveraging the radiomics features extracted from the xsurv-segmented pt/mln
regions (refer to sect. 2.4). our implementation is provided at
https://github.com/mungomeng/survival-xsurv.
assuming n conv , n self , and n cross are three architecture parameters, each
encoder branch consists of n conv conv blocks, n self hybrid parallel
self-attention (hpsa) blocks, and n cross hpca blocks. max pooling is applied
between blocks and the features before max pooling are propagated to the decoder
through skip connections. as shown in fig. 2(a), hpca blocks perform parallel
convolution and cross-attention operations. the convolution operations are
realized using successive convolutional layers with residual connections, while
the cross-attention operations are realized using swin transformer [27] where
the input x in (from the same encoder branch) is projected as q and the input x
cross (from the other encoder branch) is projected as k and v . in addition,
conv blocks perform the same convolution operations as hpca blocks but discard
cross-attention operations; hpsa blocks share the same overall architecture with
hpca blocks but perform self-attention within the input x in (i.e., the x in is
projected as q, k and v ). conv and hpsa blocks are used first and then followed
by hpca blocks, which enables the xsurv to learn both intra-and inter-modality
information. in this study, we set n conv , n self , and n cross as 1, 1, and 3,
as this setting achieved the best validation results (refer to the supplementary
materials). other architecture details are also presented in the supplementary
materials.the idea of adopting convolutions and transformers in parallel has
been explored for segmentation [28], which suggests that parallelly aggregating
global and local information is beneficial for feature learning. in this study,
we extend this idea to multimodality feature learning, which parallelly
aggregates global inter-modality and local intra-modality information via hpca
blocks, to discover inter-modality interactions while preserving intra-modality
characteristics.
as shown in fig. 1, each decoder branch is symmetric to the encoder branch and
thus includes a total of (n conv +n self +n cross ) conv blocks. the features
propagated from skip connections are fed into rag blocks for feature diverging
before entering the conv blocks in two decoder branches, where the output of the
former conv block is upsampled and concatenated with the output of the rag
block. as shown in fig. 2 the output of the last conv block in the pt/mln branch
is fed into a segmentation head, which generates pt/mln segmentation masks using
a sigmoid-activated 1 × 1 × 1 convolutional layer. in addition, the outputs of
all but not the first conv blocks in the pt/mln branches are fed into global
averaging pooling layers to derive pt-/mlnrelated deep features. finally, all
deep features are fed into a survival prediction head, which maps the deep
features into a survival score using two fully-connected layers with dropout, l2
regularization, and sigmoid activation.
following existing multi-task deep survival models [11,16,[24][25][26], our
xsurv is endto-end trained for survival prediction and pt-mln segmentation using
a combined loss: l = l surv +λ(l pt +l mln ), where the λ is a parameter to
balance the survival prediction term l surv and the pt/mln segmentation terms l
pt /mln . we follow [15] to adopt a negative log-likelihood loss [30] as the l
surv . for the l pt /mln , we adopt the sum of dice [31] and focal [32] losses.
the loss functions are detailed in the supplementary materials. the λ is set as
1 in the experiments as default.
our xsurv also can be enhanced by leveraging radiomics features (denoted as
radio-xsurv). following [16], radiomics features are extracted from the
xsurv-segmented pt/mln regions via pyradiomics [33] and selected by least
absolute shrinkage and selection operator (lasso) regression. the process of
radiomics feature extraction is provided in the supplementary materials. then, a
coxph model [9] is adopted to integrate the selected radiomics features and the
xsurv-predicted survival score to make the final prediction. in addition,
clinical indicators (e.g., age, gender) also can be integrated by the coxph
model.
we adopted the training dataset of hecktor 2022 (refer to
https://hecktor.grand-cha llenge.org/), including 488 h&n cancer patients
acquired from seven medical centers [7], while the testing dataset was excluded
as its ground-truth labels are not released. each patient underwent pretreatment
pet/ct and has clinical indicators. we present the distributions of all clinical
indicators in the supplementary materials. recurrence-free survival (rfs),
including time-to-event in days and censored-or-not status, was provided as
ground truth for survival prediction, while pt and mln annotations were provided
for segmentation. the patients from two centers (chum and chuv) were used for
testing and other patients for training, which split the data into 386/102
patients in training/testing sets. we trained and validated models using 5-fold
cross-validation within the training set and evaluated them in the testing
set.we resampled pet-ct images into isotropic voxels where 1 voxel corresponds
to 1 mm 3 . each image was cropped to 160 × 160 × 160 voxels with the tumor
located in the center. pet images were standardized using z-score normalization,
while ct images were clipped to [-1024, 1024] and then mapped to [-1, 1]. in
addition, we performed univariate and multivariate cox analyses on the clinical
indicators to screen out the prognostic indicators with significant relevance to
rfs (p < 0.05).
we implemented our xsurv using pytorch on a 12 gb geforce gtx titan x gpu. our
xsurv was trained for 12,000 iterations using an adam optimizer with a batch
size of 2. each training batch included the same number of censored and
uncensored samples. the learning rate was set as 1e-4 initially and then reset
to 5e-5 and 1e-5 at the 4,000 th and 8,000 th training iteration. data
augmentation was applied in real-time during training to minimize overfitting,
including random affine transformations and random cropping to 112 × 112 × 112
voxels. validation was performed after every 200 training iterations and the
model achieving the highest validation result was preserved.in our experiments,
one training iteration (including data augmentation) took roughly 4.2 s, and one
inference iteration took roughly 0.61 s.
we compared our xsurv to six state-of-the-art survival prediction methods,
including two traditional radiomics-based methods and four deep survival models.
the included traditional methods are coxph [9] and individual coefficient
approximation for risk estimation (icare) [34]. for traditional methods,
radiomics features were extracted from the provided ground-truth tumor regions
and selected by lasso regression. the included deep survival models are deep
multi-task logistic regression and coxph ensemble (deepmtlr-coxph) [14],
transformer-based multimodal networks for segmentation and survival prediction
(tmss) [11], deep multi-task survival model (deepmts) [24], and
radiomics-enhanced deepmts (radio-deepmts) [16]. deepmtlr-coxph, icare, and
radio-deepmts achieved top performance in hecktor 2021 and 2022. for a fair
comparison, all methods took the same preprocessed images and clinical
indicators as inputs. survival prediction and segmentation were evaluated using
concordance index (c-index) and dice similarity coefficient (dsc), which are the
standard evaluation metrics in the challenges [6,7,35].we also performed two
ablation studies on the encoder and decoder separately: (i) we replaced
hpca/hpsa blocks with conv blocks and compared different strategies to combine
pet-ct images. (ii) we removed rag blocks and compared different strategies to
extract pt/mln-related information.
the comparison between our xsurv and the state-of-the-art methods is presented
in table 1. our xsurv achieved a higher c-index than all compared methods, which
demonstrates that our xsurv has achieved state-of-the-art performance in
survival prediction of h&n cancer. when radiomics enhancement was adopted in
xsurv and deepmts, our radio-xsurv also outperformed the radio-deepmts and
achieved the highest cindex. moreover, the segmentation results of multi-task
deep survival models (tmss, deepmts, and xsurv) are also reported in table 1.
our xsurv achieved higher dscs than tmss and deepmts, which demonstrates that
our xsurv can locate pt and mln more precisely and this infers that our xsurv
has better learning capability. we attribute these performance improvements to
the use of our proposed merging-diverging learning framework, hpca block, and
rag block, which can be evidenced by ablation studies.the ablation study on the
pet-ct merging encoder is shown in table 2. we found that using pet alone
resulted in a higher c-index than using both pet-ct with early or late fusion.
this finding is consistent with wang et al. [19]'s study, which suggests that
early and late fusion cannot effectively leverage the complementary information
in pet-ct images. as we have mentioned, early and late fusion have difficulties
in extracting intra-and inter-modality information, respectively. our encoder
first adopts conv/hpsa blocks to extract intra-modality information and then
leverages hpca blocks to discover their interactions, which achieved the highest
c-index. for pt and mln segmentation, our encoder also achieved the highest
dscs, which indicates that our encoder also can improve segmentation. in
addition, mnaf blocks [22] were compared and showed poor performance. this is
likely attributed to the fact that leveraging non-local attention at multiple
scales has corrupted local spatial information, which degraded the segmentation
performance and distracted the model from pt and mln regions. to relieve this
problem, in tang et al.'s study [22], tumor segmentation masks were fed into the
model as explicit guidance to tumor regions. however, it is intractable to have
segmentation masks at the inference stage in clinical practice.the ablation
study on the pt-mln diverging decoder is shown in table 3. we found that, even
without adopting ag, using a dual-branch decoder for pt and mln segmentation
resulted in a higher c-index than using a single-branch decoder, which
demonstrates the effectiveness of our diverging decoder design. adopting vanilla
ag [29] or rag in the dual-branch decoder further improved survival prediction.
compared to the vanilla ag, our rag contributed to a larger improvement, and
this enabled our decoder to achieve the highest c-index. in the supplementary
materials, we visualized the attention maps produced by rag blocks, where the
attention maps can precisely locate pt/mln regions and screen out
pt-/mln-related features. for pt and mln segmentation, using a single-branch
decoder for pt-or mln-only segmentation achieved the highest dscs. this is
expected as the model can leverage all its capabilities to segment only one
target. nevertheless, our decoder still achieved the second-best dscs in both pt
and mln segmentation with a small gap.
we have outlined an x-shape merging-diverging hybrid transformer network (xsurv)
for survival prediction from pet-ct images in h&n cancer. within the xsurv, we
propose a merging-diverging learning framework, a hybrid parallel
cross-attention (hpca) block, and a region-specific attention gate (rag) block,
to learn complementary information from multi-modality images and extract
region-specific prognostic information for survival prediction. extensive
experiments have shown that the proposed framework and blocks enable our xsurv
to outperform state-of-the-art survival prediction methods on the
well-benchmarked hecktor 2022 dataset.
the examination of tissue and cells using microscope (referred to as histology)
has been a key component of cancer diagnosis and prognostication since more than
a hundred years ago. histological features allow visual readout of cancer
biology as they represent the overall impact of genetic changes on cells
[20].the great rise of deep learning in the past decade and our ability to
digitize histopathology slides using high-throughput slide scanners have fueled
interests in the applications of deep learning in histopathology image analysis.
the majority of the efforts, so far, focus on the deployment of these models for
diagnosis and classification [27]. as such, there is a paucity of efforts that
embark on utilizing machine learning models for patient prognostication and
survival analysis (for example, predicting risk of cancer recurrence or expected
patient survival). while prognostication and survival analysis offer invaluable
insights for patient management, biological studies and drug development
efforts, they require careful tracking of patients for a lengthy period of time;
rendering this as a task that requires a significant amount of effort and
funding.in the machine learning domain, patient prognostication can be treated
as a weakly supervised problem, which a model would predict the outcome (e.g.,
time to cancer recurrence) based on the histopathology images. their majority
have utilized multiple instance learning (mil) [8] that is a two-step learning
method. first, representation maps for a set of patches (i.e., small fields of
view), called a bag of instances, are extracted. then, a second pooling model is
applied to the feature maps for the final prediction. different mil variations
have shown superior performances in grading or subtype classification in
comparison to outcome prediction [10]. this is perhaps due to the fact that
mil-based technique do not incorporate patch locations and interactions as well
as tissue heterogeneity which can potentially have a vital role in defining
clinical outcomes [4,26].to address this issue, graph neural networks (gnn) have
recently received more attention in histology. they can model patch relations
[17] by utilizing message passing mechanism via edges connecting the nodes
(i.e., small patches in our case). however, most gnn-based models suffer from
over smoothing [22] which limits nodes' receptive fields [3]. while local
contexts mainly capture cell-cell interactions, global patterns such as immune
cell infiltration patterns and tumor invasion in normal tissue structures (e.g.,
depth of invasion through myometrium in endometrial cancer [1]) could capture
critical information about outcome [10]. hence, locally focused methods are
unable to benefit from the coarse properties of slides due to their high
dimensions which may lead to poor performance.this paper aims to investigate the
potential of extracting fine and coarse features from histopathology slides and
integrating them for risk stratification in cancer patients. therefore, the
contributions of this work can be summarized as: 1) a novel graph-based model
for predicting survival that extracts both local and global properties by
identifying morphological super-nodes; 2) introducing a fine-coarse feature
distillation module with 3 various strategies to aggregate interactions at
different scales; 3) outperforming sota approaches in both risk prediction and
patient stratification scenarios on two datasets; 4) publishing two large and
rare prostate cancer datasets containing more than 220 graphs for active
surveillance and 240 graphs for brachytherapy cases. the code and graph
embeddings are publicly available at https://github.com/pazadimo/all-in 2
related works
utilizing weakly supervised learning for modeling histopathology problems has
been getting popular due to the high resolution of slides and substantial time
and financial costs associated with annotating them as well as the development
of powerful deep discriminative models in the recent years [24].such models are
used to perform nuclei segmentation [18], identify novel subtypes [12], or later
descendants are even able to pinpoint sub-areas with a high diagnostic value
[19].
mil-based models have been utilized for outcome prediction [29,32] which can
also be integrated with attention-based variants [14]. gnns due to their
structural preserving capacity [28] have drawn attention in various histology
domains by constructing the graph on cells or patches. however, current
gnn-based risk assessment variants are only focused on short-range interactions
[16,17] or consider local contexts [10]. we hypothesize that graph-based models'
performance in survival prediction improves by leveraging both fine and coarse
properties.
figure 1 summarizes our proposed end-to-end solution. below, we have provided
details of each module.
for p n , which is the n-th patient, a set of patches {patch j } m j=1 is
extracted from the related whole slide images. in addition, a latent vector z j
∈ r 1×d is extracted from patch j using our encoder network (described in sect.
3.2) that results in feature matrix z n ∈ r m ×d for p n . finally, a specific
graph (g n ) for the n-th patient (p n ) can be constructed by assuming patches
as nodes. also, edges are connected based on the patches' k-nearest neighbour in
the spatial domain resulting in an adjacency matrix a n . therefore, for each
patient such as p n , we have a graph defined by adjacency matrix a n with size
m × m and features matrix z n (g n = graph(z n , a n )). we estimate k
super-nodes as matrix s n ∈ r k×d representing groups of local nodes with
similar properties as coarse features for p n 's slides. the final model ( θ )
with parameters θ utilizes g n and s n to predict the risk associated with this
patient:
due to computational limits and large number of patches available for each
patient, we utilize a self-supervised approach to train an encoder to reduce the
inputs' feature space size. therefore, we use dino [9], a knowledge distillation
model (kdm), with vision transformer (vit) [13] as the backbone. it utilizes
global and local augmentations of the input patch j and passes them to the
student (s θ1,v it ) and teacher (t θ2,v it ) models to find their respective
representations without any labels. then, by using distillation loss, it makes
the representations' distribution similar to each other. finally, the fixed
weights of the teacher model are utilized in order to encode the input patches.
gnn's objective is to find new nodes' embeddings via integrating local
neighbors' interactions with individual properties of patches. by exploiting the
message passing mechanism, this module iteratively aggregates features from
neighbors of each vertex and generates the new node representations. we employ
two graph convolution isomorphism operators (ginconv) [30] with the generalized
form as:where is a small positive value and i is the identity matrix. also, φ
denotes the weights of two mlp layers. x n ∈ r m ×d and x n ∈ r m ×d are
ginconv's input and output feature matrices for p n , which x n equals z n for
the first layer.
in order to find the coarse histo-morphological patterns disguised in the local
graph, we propose extracting k super-nodes, which each represents a weighted
cluster of further processed local features. intuitively, the number of
super-nodes k should not be very large or small, as the former encourages them
to only represent local clusters and the latter leads to larger clusters and
loses subtle details. we exploit the mincut [5] idea to extract super-nodes in a
differentiable process after an auxiliary ginconv to focus more on large-scale
interactions and to finally learn the most global correlated super-nodes.
inspired by the relaxation form of the known k-way mincut problem, we create a
continuous cluster matrix c n ∈ r m ×k using mlp layers and can finally estimate
the super-nodes features (s n ∈ r m ×d ) as:where w 1 , w 2 are mlps' weights.
hence, the extracted nodes are directly dependent on the final survival-specific
loss. in addition, two additional unsupervised weighted regularization terms are
optimized to improve the process:mincut regularizer. this term is motivated by
the original mincut problem and intends to solve it for the the patients' graph.
it is defined as:where d n is the diagonal degree matrix for a n . also, t r(.)
represents the trace of matrix and a n,norm is the normalized adjacency matrix.
r mincu t 's minimum value happens when t r(therefore, minimizing r mincu t
causes assigning strongly similar nodes to a same super-node and prevent their
association with others.orthogonality regularizer. r mincu t is non-convex and
potent to local minima such as assigning all vertexes to a super-node or having
multiple super-nodes with only a single vertex. r orthogonal penalizes such
solutions and helps the model to distribute the graph's features between
super-nodes. it can be formulated as:where ||.|| f is the frobenius norm, and i
is the identity matrix. this term pushes the model's parameters to find coarse
features that are orthogonal to each other resulting in having the most useful
global features. overall, utilizing these two terms encourages the model to
extract supernodes by leaning more towards the strongly associated vertexes and
keeping them against weakly connected ones [5], while the main survival loss
still controls the global extraction process.
we propose our fine-coarse morphological feature distillation module to leverage
all-scale interactions in the final prediction by finding a local and a global
patientlevel representations ( ĥl,n , ĥg,n ). assume that x n ∈ r m ×d and s n ∈
r k×d are the feature matrices taken from local gnn (sect. 3.3) and super-nodes
for p n , respectively. we explore 3 different attention-based feature
distillation strategies for this task, including:-dual attention (da): two gated
self-attention modules for local and global properties with separate weights (w
φ,l , w φ,g , w k,l , w k,g , w q,l , w q,g ) are utilized to find patches
scores α l ∈ r 1×m and α g ∈ r 1×k and the final features ( ĥl,n , ĥg,n ) as:)
where x n,i and s n,i are rows of x n and s n , respectively, and the final
representation ( ĥ) is generated as ĥ = cat( ĥl , ĥg ).-mixed guided attention
(mga): in the first strategy, the information flows from local and global
features to the final representations in parallel without mixing any knowledge.
the purpose of this policy is the heavy fusion of fine and coarse knowledge by
exploiting shared weights (w φ,shared , w k,shared , w q,shared , w v,shared )
in both routes and benefiting from the guidance of local representation on
learning the global one by modifying eq. ( 7) to:-mixed co-attention (mca):
while the first strategy allows the extreme separation of two paths, the second
one has the highest level of mixing information. here, we take a balanced policy
between the independence and knowledge mixture of the two routes by only sharing
the weights without using any guidance.
we utilize two prostate cancer (pca) datasets to evaluate the performance of our
proposed model. the first set (pca-as) includes 179 pca patients who were
managed with active surveillance (as). radical therapy is considered
overtreatment in these patients, so they are instead monitored with regular
serum prostate-specific antigen (psa) measurements, physical examinations,
sequential biopsies, and magnetic resonance imaging [23]. however, as may be
over-or under-utilized in low-and intermediate-risk pca due to the uncertainty
of current methods to distinguish indolent from aggressive cancers [11].
although majority of patients in our cohort are classified as low-risk based on
nccn guidelines [21], a significant subset of them experienced disease upgrade
that triggered definitive therapy (range: 6.2 to 224 months after diagnosis).the
second dataset (pca-bt) includes 105 pca patients with low to high risk disease
who went through brachytherapy. this treatment involves placing a radioactive
material inside the body to safely deliver larger dose of radiation at one time
[25]. the recorded endpoint for this set is biochemical recurrence with time to
recurrence ranging from 11.7 to 56.1 months. we also utilized the prostate
cancer grade assessment (panda) challenge dataset [7] that includes more than
10,000 pca needle biopsy slides (no outcome data) as an external dataset for
training the encoder of our model.
we evaluate the models' performance in two scenarios utilizing several objective
metrics. implementation details are available in supplementary material.hazard
(risk) prediction. we utilize concordance-index (c-index) that measures the
relative ordering of patients with observed events and un-censored cases
relative to censored instances [2]. using c-index, we compare the quality of
hazard ranking against multiple methods including two mil (deepset [31], amil
[14]) and graph-based (dgc [17] and patch-gcn [10]) models that were utilized
recently for histopathology risk assessment. c-index values are available in
table 1. the proposed model with all strategies outperforms baselines across all
sets and is able to achieve 0.639 and 0.600 on pca-as and pca-bt, while the
baselines, at best, obtain 0.555, and 0.572, respectively. statistical tests
(paired t-test) on c-indices also show that our model is statistically better
than all baselines in pca-as and also superior to all models, except dgc, in
pca-bt. superior performance of our mca policy implies that balanced
exploitation of fine and coarse features with shared weights may provide more
robust contextual information compared to using mixed guided information or
utilizing them independently.patient stratification. the capacity of stratifying
patients into risk groups (e.g., low and high risk) is another criterion that we
employ to assess the utility of models in clinical practice. we evaluate model
performances via kaplan-meier curve [15] (cut-off set as the ratio of patients
with recurrence within 3 years of therapy initiation for pca-bt and the ratio of
upgraded cases for pca-as), logrank test [6] (with 0.05 as significance level),
and median outcome associated with risk groups (table 1 and fig. 2). our model
stratified pca-as patients into high-and low-risk groups with median time to
progression of 36.5 and 131.7 months, respectively. moreover, pca-bt cases
assigned to high-and low-risk groups have median recurrence time of 21.86 and
35.7 months. while none of the baselines are capable of assigning patients into
risk groups with statistical significance, our distillation policies achieve
significant separation in both pca-as and pca-bt datasets; suggesting that
global histo-morphological properties improve patient stratification
performance. furthermore, our findings have significant clinical implications as
they identify, for the first time, highrisk prostate cancer patients who are
otherwise known to be low-risk based on clinico-pathological parameters. this
group should be managed differently from the rest of the low-risk prostate
cancer patients in the clinic. therefore, providing evidence of the predictive
(as opposed to prognostic) clinical information that our model provides. while a
prognostic biomarker provides information about a patient's outcome (without
specific recommendation on the next course of action), a predictive biomarker
gives insights about the effect of a therapeutic intervention and potential
actions that can be taken.ablation study. we perform ablation study (table 2) on
various components of our framework including local nodes, self-supervised
vit-based encoder, and most importantly, super-nodes in addition to fine-coarse
distillation module. although our local-only model is still showing superior
results compared to baselines, this analysis demonstrates that all modules are
essential for learning the most effective representations. we also assess the
impact of our vit on the baselines (full-results in appendix), showing that it
can, on average, improve their performance by an increase of ∼ 0.03 in c-index
for pca-as. however, the best baseline with vit still has poorer performance
compared to our model in both datasets, while the number of parameters (reported
for vit embeddings' size in table 1) in our full-model is about half of this
baseline. achieving higher c-indices in our all model versions indicates the
important role of coarse features and global context in patient risk estimation
in addition to local patterns.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 74.
automatic nucleus segmentation has captured wide research interests in recent
years due to its importance in pathological image analysis [1][2][3][4].
however, as shown in fig. 1, the variations in image modalities, staining
protocols, scanner types, and tissues significantly affect the appearance of
nucleus images, resulting in notable gap between source and target domains
[5][6][7]. if a number of target domain samples are available before testing,
one can adopt domain adaptation algorithms to transfer the knowledge learned
from the source domain to the target domain [8][9][10]. unfortunately, in
real-world applications, it is usually expensive and time-consuming to collect
new training sets for the ever changing target domains; moreover, extra
computational cost is required, which is usually unrealistic for the end users.
therefore, it is highly desirable to train a robust nucleus segmentation model
that is generalizable to different domains.in recent years, the research on
domain generalization (dg) has attracted wide attention. most existing dg works
are proposed for classification tasks [12,13] and they can be roughly grouped
into data augmentation-, representation learning-, and optimization-based
methods. the first category of methods [14][15][16][17] focus on the way to
diversify training data styles and expect the enriched styles cover those
appeared in target domains. the second category of methods aim to obtain
domain-invariant features. this is usually achieved via improving model
architectures [18][19][20] or introducing novel regularization terms [21,22].
the third category of methods [23][24][25][26] develop new model optimization
strategies, e.g., meta-learning, that improve model robustness via artificially
introducing domain shifts during training.it is a consensus that a generalizable
nucleus segmentation model should be robust to image appearance variation caused
by the change in staining protocols, scanner types, and tissues, as illustrated
in fig. 1. in this paper, we argue that it is also desirable to be robust to the
ratio between foreground (nucleus) and background pixel numbers. this ratio
changes the statistics of each feature map channel, and affects the robustness
of normalization layers, e.g., instance normalization (in). we will empirically
justify its impact in sect. 2.3.
in this paper, we adopt a u-net-based model similar to that in [1] as the
baseline. it performs both semantic segmentation and contour detection for
nucleus instances. the area of each nucleus instance is obtained via subtraction
between the segmentation and contour prediction maps [1]. details of the
baseline model is provided in the supplementary material. to handle domain
variations, we adopt in rather than batch normalization (bn) in the u-net
model.our proposed distribution-aware re-coloring model (darc) is illustrated in
fig. 2. compared with the baseline, darc replaces the in layers with the
proposed distribution-aware instance normalization (dain) layers. darc first
re-colors each image to relieve the influence caused by image acquisition
conditions. the re-colored image is then fed into the u-net encoder and the
ratio prediction head. this head predicts the ratio between foreground and
background pixel numbers. with the predicted ratio, the dain layers can estimate
feature statistics more robustly and facilitate more accurate nucleus
segmentation.
we propose the re-coloring (rc) method to overcome the color change in different
domains. specifically, given a rgb image i, e.g., an h&e or ihc stained image,
we first obtain its grayscale image i g . we then feed i g into a simple module
t that consists of a single residual block and a 1 × 1 convolutional layer with
output channel number of 3. in this way, we obtain an initial re-colored image i
r .however, de-colorization results in the loss of fine-grained textures and may
harm the segmentation accuracy. to handle this problem, we compensate i r with
the original semantic information contained in i. recent works [40] show that
semantic information can be reflected via the order of pixels according to their
gray value. therefore, we adopt the sort-matching algorithm [41] to combine the
semantic information in i with the color values in i r . details of rc is
presented in algorithm 1, in which sort and argsort denote channel-wisely
sorting the values and obtaining the sorted values and indices respectively, and
the input rgb image i ∈ r h×w ×3 ; the module t whose input and output channel
numbers are 1 and 3, respectively; output:the re-colored image io ∈ r assignv
alue denotes re-assembling the sorted values according to the provided indices.
details of the module t are included in the supplementary material. via rc, the
original fine-grained structure information from i g is recovered in i r . in
this way, the re-colored image is advantageous in two aspects. first, the
appearance difference between pathological images caused by the change in
scanners and staining protocols is eliminated. second, the re-colored image
preserves fine-grained structure information, enabling precise instance
segmentation to be possible.
due to dramatic domain gaps, feature statistics may differ significantly between
domains [5][6][7], which means that feature statistics obtained from the source
domain may not apply to the target domain. therefore, existing dg works usually
replace bn with in for feature normalization [12,19]. however, for
dense-prediction tasks like semantic segmentation or contour detection, adopting
in alone cannot fully address the feature statistics variation problem. this is
because feature statistics are also relevant to the ratio between foreground and
background pixel numbers. specifically, an image with more nucleus instances
produces more responses in feature maps and thus higher feature statistic
values, and vice versa. the difference in this ratio causes interference to
nucleus segmentation.
original feature maps x ∈ r h×w ×c . the c-dimensional feature vector on its
pixel (i, j) is denoted as xij;the modules eμ and e δ that re-estimate feature
statistics; δsra ∈ r 1×1×c that is obtained via running mean of δs in the
training stage;the momentum factor α used to update δsra ; (optional) δs = f
(ρ); output:normalized feature maps y ∈ r h×w ×c ; to verify the above
viewpoint, we evaluate the baseline model under different foreground-background
ratios. specifically, we first remove the foreground pixels via in-painting
[27], and then pad the original testing images with the obtained background
patches. we adopt b to denote the ratio between the size of the obtained new
image and the original image size. compared with the original images, the new
images have the same foreground regions but more background pixels, and thus
have different foreground-background ratios. finally, we evaluate the
performance of the baseline model with different b values. experimental results
are presented in table 1. it is shown that the value of b affects the model
performance significantly.the above problem is common in nucleus segmentation
because pathological images from different organs or tissues tend to have
significantly different foreground-background ratios. however, this phenomenon
is often ignored in existing research. to handle this problem, we propose the
distribution-aware instance normalization (dain) method to re-estimate feature
statistics that account for different ratios of foreground and background
pixels. details of dain is presented in algorithm 2. the structures of e μ and e
δ are included in the supplemental materials. as shown in fig. 2, to obtain the
foreground-background ratio ρ of one input image, we first feed it to the model
encoder with δs ra as the additional input. δs ra acts as pseudo residuals of
feature statistics and is obtained in the training stage via averaging δs in a
momentum fashion. the output features by the encoder are used to predict the
foreground-background ratio ρ with a ratio-prediction head (rph). ρ is then
utilized to estimate the residuals of feature statistics: δs = f (ρ). here, f is
a 1 × 1 convolutional layer that transforms ρ to a feature vector whose
dimension is the same as the target layer's channel number. after that, the
input image is fed into the model again with δs as additional input and finally
makes more accurate predictions.the training of rph requires an extra loss term
l rph , which is formulated as bellow:where ρ g denotes the ground truth
foreground-background ratio, and l bce and l mse denote the binary cross entropy
loss and the mean squared error, respectively.
the proposed method is evaluated on four datasets, including two h&e stained
image datasets consep [3] and cpm17 [28] and two ihc stained datasets deepliif
[29] and bc-deepliif [29,32]. consep [3] contains 28 training and 14 validation
images, whose sizes are 1000×1000 pixels. the images are extracted from 16
colorectal adenocarcinoma wsis, each of which belongs to an individual patient,
and scanned with an omnyx vl120 scanner within the department of pathology at
university hospitals coventry and warwickshire, uk. cpm17 [28] contains 32
training and 32 validation images, whose sizes are 500 × 500 pixels. the images
are selected from a set of glioblastoma multiforme, lower grade glioma, head and
neck squamous cell carcinoma, and non-small cell lung cancer whole slide tissue
images. deepliif [29] contains 575 training and 91 validation images, whose
sizes are 512 × 512 pixels. the images are extracted from the slides of lung and
bladder tissues. bc-deepliif [29,32] contains 385 training and 66 validation
ki67 stained images of breast carcinoma, whose sizes are 512 × 512 pixels.
in the training stage, patches of size 224 × 224 pixels are randomly cropped
from the original samples. during training, the batch size is 4 and the total
number of training iterations is 40,000. we use adam algorithm for optimization,
and the learning rate is initialized as 1e -3 , which is gradually decreased to
1e -5 during training. we adopt the standard augmentation, like image color
jittering and gaussian blurring. in all experiments, the segmentation and
contour detection predictions are penalized using the binary cross entropy loss.
in this paper, the models are compared using the aji [33] and dice scores. in
the experiments, models trained on one of the datasets will be evaluated on the
three unseen ones. to avoid the influence of the different sample numbers of the
datasets, we calculate the average scores within each unseen domain respectively
and then average them across domains.in this paper, we re-implement some
existing popular domain generalization algorithms for comparisons under the same
training conditions. specifically, we re-implement the tent [34], bin [19], dsu
[20], frequency amplitude normalization (ampnorm) [36,37], san [35] and efdmix
[40]. we also evaluate the stain normalization [38] and stain mix-up [39]
methods that are popular in pathological image analysis. their performances are
presented in table 2. darc all replaces all normalization layers with dain,
while darc enc replaces the normalization layers in the encoder with dain and
uses bn in its decoder. as shown in table 2, darc enc achieves the best average
performance among all methods. specifically, darc enc improves the baseline
model's average aji and dice scores by 4.81% and 7.04%. compared with the other
domain generalization methods, dain, dain w/o ratio, darc all and darc enc
achieve impressive performances on bc-deepliif, which justify that re-estimating
the instancewise statistics is important for improving the domain generalization
ability of models trained on bc-deepliif. qualitative comparisons are presented
in fig. 3. moreover, the complexity analysis between the baseline model and darc
enc is presented in table 3.we separately evaluate the effectiveness of rc and
dain, and present the results in table 2. also, we train a variant model without
foreground-background ratio prediction, which is denoted as 'dain w/o ratio' in
table 2. compared with the baseline model, rc improves the average aji and dice
scores by 1.41% and 2.59%, and dain improves the average aji and dice scores by
1.13% and 4.08%. compared with the variant model without foreground-background
ratio prediction, dain improves the average aji and dice scores by 0.90% and
3.74%. finally, the combinations of rc and dain, i.e., darc all and darc enc ,
achieve the best average scores. as shown in table 2, darc enc improves darc all
by 1.24% and 0.77% on aji and dice scores respectively. this is because after
the operations by rc and dain in the encoder, the obtained feature maps are much
more robust to the domain gaps, which enables the decoder to adopt the fixed
statistics maintained during training. moreover, using the fixed statistics is
helpful to prevent the decoder from the influence of varied
foreground-background ratios on feature statistics.
in this paper, we propose the darc model for generalizable nucleus segmentation.
to handle the domain gaps caused by varied image acquisition conditions, darc
first re-colors the input image while preserving its fine-grained structures as
much as possible. moreover, we find that the performance of instance
normalization is sensitive to the varied ratios in foreground and background
pixel numbers. this problem is well addressed by our proposed dain. compared
with existing works, darc achieves significantly better performance on average
across four benchmarks.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 57.
accurate spatial characterization of tumor immune microenvironment is critical
for precise therapeutic stratification of cancer patients (e.g. via
immunotherapy). currently, this characterization is done manually by individual
pathologists on standard hematoxylin-and-eosin (h&e) or singleplex
immunohistochemistry (ihc) stained images. however, this results in high
interobserver variability among pathologists, primarily due to the large (> 50%)
disagreement among pathologists for immune cell phenotyping [10]. this is also a
big cause of concern for publicly available h&e/ihc cell segmentation datasets
with immune cell annotations from single pathologists. multiplex staining
resolves this issue by allowing different tumor and immune cell markers to be
stained on the same tissue section, avoiding any phenotyping guesswork from
pathologists. multiplex staining can be performed using expensive multiplex
immunofluorescence (mif) or via cheaper multiplex immunohistochemistry (mihc)
assays. mif staining (requiring expensive scanners and highly skilled lab
technicians) allows multiple markers to be stained/expressed on the same tissue
section (no co-registration needed) while also providing the utility to turn
on/off individual markers as needed. in contrast, current brightfield mihc
staining protocols relying on dab (3,3'-diaminobenzidine) alcohol-insoluble
chromogen, even though easily implementable with current clinical staining
protocols, suffer from occlusion of signal from sequential staining of
additional markers. to this effect, we introduce a new brightfield mihc staining
protocol using alcoholsoluble aminoethyl carbazole (aec) chromogen which allows
repeated stripping, restaining, and scanning of the same tissue section with
multiple markers. this requires only affine registration to align the digitized
restained images to obtain non-occluded signal intensity profiles for all the
markers, similar to mif staining/scanning.in this paper, we introduce a new
dataset that can be readily used out-ofthe-box with any artificial intelligence
(ai)/deep learning algorithms for spatial characterization of tumor immune
microenvironment and several other use cases.to date, only two denovo stained
datasets have been released publicly: bci h&e and singleplex ihc her2 dataset
[7] and deepliif singleplex ihc ki67 and mif dataset [2], both without any
immune or tumor markers. in contrast, we release the first denovo mif/mihc
stained dataset with tumor and immune markers for more accurate characterization
of tumor immune microenvironment. we also demonstrate several interesting use
cases: (1) ihc quantification of cd3/cd8 tumor-infiltrating lymphocytes (tils)
via style transfer, (2) virtual translation of cheap mihc stains to more
expensive mif stains, and (3) virtual tumor/immune cellular phenotyping on
standard hematoxylin images.
the complete staining protocols for this dataset are given in the accompanying
supplementary material. images were acquired at 20× magnification at moffitt
cancer center. the demographics and other relevant information for all eight
head-and-neck squamous cell carcinoma patients is given in table 1.
after scanning the full images at low resolution, nine regions of interest
(rois) from each slide were chosen by an experienced pathologist on both mif and
mihc images: three in the tumor core (tc), three at the tumor margin (tm), and
three outside in the adjacent stroma (s) area. the size of the rois was
standardized at 1356×1012 pixels with a resolution of 0.5 µm/pixel for a total
surface area of 0.343 mm 2 . hematoxylin-stained rois were first used to align
all the mihc marker images in the open source fiji software using affine
registration. after that, hematoxylin-and dapi-stained rois were used as
references to align mihc and mif rois again using fiji and subdivided into
512×512 patches, resulting in total of 268 co-registered mihc and mif patches
(∼33 co-registered mif/mihc images per patient).
we compared mif and mihc assays for concordance in marker intensities. the
results are shown in fig. 2. this is the first direct comparison of mif and mihc
using identical slides. it provides a standardized dataset to demonstrate the
equivalence of the two methods and a source that can be used to calibrate other
methods.
in this section, we demonstrate some of the use cases enabled by this
high-quality ai-ready dataset. we have used publicly available state-of-the-art
tools such as adaptive attention normalization (adaattn) [8] for style transfer
in the ihc cd3/cd8 quantification use case and deepliif virtual stain
translation [2,3] in the remaining two use cases.
we generate a stylized ihc image (fig. 3) using three input images: (1)
hematoxylin image (used for generating the underlying structure of cells in the
stylized image), (2) its corresponding mif cd3/cd8 marker image (used for
staining positive cells as brown), and (3) sample ihc style image (used for
transferring its style to the final image). the complete architecture diagram is
given in the supplementary material. specifically, the model consists of two
sub-networks:(a) marker generation: this sub-network is used for generating mif
marker data from the generated stylized image. we use a conditional generative
adversarial network (cgan) [4] for generating the marker images. the cgan
network consists of a generator, responsible for generating mif marker images
given an ihc image, and a discriminator, responsible for distinguishing the
output of the generator from ground truth data. we first extract the brown (dab
channel) from the given style ihc image, using stain deconvolution. then, we use
pairs of the style images and their extracted brown dab marker images to train
this sub-network. this sub-network improves staining of the positive cells in
the final stylized image by comparing the extracted dab marker image from the
stylized image and the input mif marker image at each iteration.
this sub-network creates the stylized ihc image using an attention module, given
(1) the input hematoxylin and the mif marker images and (2) the style and its
corresponding marker images. for synthetically generating stylized ihc images,
we follow the approach outlined in adaattn [8]. we use a pre-trained vgg-19
network [12] as an encoder to extract multi-level feature maps and a decoder
with a symmetric structure of vgg-19. we then use both shallow and deep level
features by using adaattn modules on multiple layers of vgg. this sub-network is
used to create a stylized image using the structure of the given hematoxylin
image while transferring the overall color distribution of the style image to
the final stylized image. the generated marker image from the first sub-network
is used for a more accurate colorization of the positive cells against the blue
hematoxylin counterstain/background; not defining loss functions based on the
markers generated by the first sub-network leads to discrepancy in the final
brown dab channel synthesis.for the stylized ihc images with ground truth
cd3/cd8 marker images, we also segmented corresponding dapi images using our
interactive deep learning impartial [9] tool
https://github.com/nadeemlab/impartial and then classified the segmented masks
using the corresponding cd3/cd8 channel intensities, as shown in fig. 4. we
extracted 268 tiles of size 512×512 from this final segmented and co-registered
dataset. for the purpose of training and testing all the models, we extract four
images of size 256 × 256 from each tile due to the size of the external ihc
images, resulting in a total of 1072 images. we randomly extracted tiles from
the lyon19 challenge dataset [14] to use as style ihc images. using these
images, we created a dataset of synthetically generated ihc images from the
hematoxylin and its marker image as shown in fig. 3.we evaluated the
effectiveness of our synthetically generated dataset (stylized ihc images and
corresponding segmented/classified masks) using our generated dataset with the
nuclick training dataset (containing manually segmented cd3/cd8 cells) [6]. we
randomly selected 840 and 230 patches of size 256 × 256 from the created dataset
for training and validation, respectively. nuclick training and validation sets
[6] comprise 671 and 200 patches, respectively, of size 256 × 256 extracted from
lyon19 dataset [14]. lyon19 ihc cd3/cd8 images are taken from breast, colon, and
prostate cancer patients. we split their training set into training and
validation sets, containing 553 and 118 images, respectively, and use their
validation set for testing our trained models. we trained three models including
unet [11], fpn [5], unet++ [15] with the backbone of resnet50 for 200 epochs and
early stopping on validation score with patience of 30 epochs, using binary
cross entropy loss and adam optimizer with learning rate of 0.0001. as shown in
table 2, models trained with our synthetic training set outperform those trained
solely with nuclick data in all metrics.we also tested the trained models on
1,500 randomly selected images from the training set of the lymphocyte
assessment hackathon (lysto) [1], containing image patches of size 299 × 299
obtained at a magnification of 40× from breast, prostate, and colon cancer whole
slide images stained with cd3 and cd8 markers. only the total number of
lymphocytes in each image patch are reported in this dataset. to evaluate the
performance of trained models on this dataset, we counted the total number of
marked lymphocytes in a predicted mask and calculated the difference between the
reported number of lymphocytes in each image with the total number of
lymphocytes in the predicted mask by the model. in table 2, the average
difference value (diffcount) of lymphocyte number for the whole dataset is
reported for each model. as seen, the trained models on our dataset outperform
the models trained solely on nuclick data.
unlike clinical dab staining, as shown in style ihc images in fig. 3, where
brown marker channel has a blue hematoxylin nuclear counterstain to stain for
all the cells, our mihc aec-stained marker images (fig. 5) do not stain for all
the cells including nuclei. in this use case, we show that mihc marker images
can be translated to higher quality mif dapi and marker images which stain
effectively for all the cells. we used the publicly available deepliif virtual
translation module [2,3] for this task. we trained deepliif on mihc cd3
aecstained images to infer mif dapi and cd3 marker. some examples of testing the
trained model on cd3 images are shown in fig. 5. we calculated the mean squared
error (mse) and structural similarity index (ssim) to evaluate the quality of
the inferred modalities by the trained model. the mse and ssim for mif dapi was
0.0070 and 0.9991 and for mif cd3 was 0.0021 and 0.9997, indicating high
accuracy of mif inference.
there are several public h&e/ihc cell segmentation datasets with manual immune
cell annotations from single pathologists. these are highly problematic given
the large (> 50%) disagreement among pathologists on immune cell phenotyping
[10]. in this last use case, we infer immune and tumor markers from the standard
hematoxylin images using again the public deepliif virtual translation module
[2,3]. we train the translation task of deepliif model using the hematoxylin,
immune (cd3) and tumor (panck) markers. sample images/results taken from the
testing dataset are shown in fig. 6.
we have released the first ai-ready restained and co-registered mif and mihc
dataset for head-and-neck squamous cell carcinoma patients. this dataset can be
used for virtual phenotyping given standard clinical hematoxylin images, virtual
clinical ihc dab generation with ground truth segmentations (to train
highquality segmentation models across multiple cancer types) created from
cleaner mif images, as well as for generating standardized clean mif images from
neighboring h&e and ihc sections for registration and 3d reconstruction of
tissue specimens. in the future, we will release similar datasets for additional
cancer types as well as release for this dataset corresponding whole-cell
segmentations via impartial https://github.com/nadeemlab/impartial.
this study is not human subjects research because it was a secondary analysis of
results from biological specimens that were not collected for the purpose of the
current study and for which the samples were fully anonymized. this work was
supported by msk cancer center support grant/core grant (p30 ca008748) and by
james and esther king biomedical research grant (7jk02) and moffitt merit
society award to c. h. chung. it is also supported in part by the moffitt's
total cancer care initiative, collaborative data services, biostatistics and
bioinformatics, and tissue core facilities at the h. lee moffitt cancer center
and research institute, an nci-designated comprehensive cancer center
(p30-ca076292).
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 68.
skin cancer, the most prevalent cancer globally, has seen increasing incidences
over recent decades [1]. it constitutes a third of all cancer diagnoses,
affecting one in five americans [2]. basal cell carcinoma (bcc), comprising 70%
of cases, has surged by 20-80% in the last 30 years, exerting a significant
healthcare strain. timely bcc diagnosis is crucial to avoid complex treatments.
although histological evaluation remains the gold standard for detection [3],
deep learning and computer vision advancements can streamline this process.
scanned traditional histology slides result in whole slide images (wsis) that
can be analyzed by deep learning models, significantly easing the histological
evaluation burden. recent advancements underscore the promise of this approach
[4][5][6]. existing skin cancer detection methods [7][8][9] typically employs
models like inception net and resnet, designed for natural images like those in
the imagenet dataset. the significant variance in pathology and natural images
can compromise these models' accuracy. neural architecture search (nas)
addresses this issue by auto-designing superior models [10][11][12][13],
exploring a vast architecture space. however, current nas methods often overlook
fairness in architecture ranking, impeding the discovery of top-performing
models.in this study, we utilized the nas approach to identify the optimal
network for skin cancer detection. to improve the efficiency and accuracy of the
search, we developed a new framework named sc-net, which focuses on identifying
highly valuable architectures. we observed that conventional nas methods often
overlook fairness ranking during the search, hindering the search for optimal
solutions. our sc-net framework addresses this by ensuring fair training and
precise ranking. the efficacy of sc-net was confirmed by our experimental
results, with our resnet50 achieving 96.2% top-1 accuracy and 96.5% auc,
outperforming baseline methods by 4.8% and 4.7% respectively.figure 1 shows the
proposed framework, which integrates two modules. module (a) extracts the region
of interest (roi) from wsi and generates patches, while module (b) uses optimal
model architecture from nas to analyze features from patches and generate
classifications.
the proposed method (fig. 2) involves dividing the input wsi into patches for
training a supernet and the search for optimal architectures [14]. section 2.2
provides further details about the supernet. a balanced evolutionary algorithm
is then used to select the optimal structure from the search space, with the
candidate structures' performance evaluated using mini-batch patch data. we
evaluate the searched architectures on the skin cancer dataset.
to extract an optimal architecture γ ∈ g from a vast search space g, a
weightsharing strategy [15][16][17] is used to prevent training from scratch.
the search leverages a supernet s with weights w, with each path γ inheriting
weights from w. this makes one-shot nas a two-step optimization process:
supernet training and architecture search. the original dataset is typically
split into training d t and validation datasets dv. the weights w of the
supernet s are trained by uniformly sampling the network width d and optimizing
the sub-network with weights wd ⊂ w. the optimization function is defined as
follows:where u (d) is a uniform distribution of network widths, e is the
expected value of random variables, and l t is the training loss function. then,
the optimal network width d * corresponds to the network width with the best
performance (e.g. classification accuracy) on the validation dataset, i.e.,where
f p is the resource budget of flops. the search for eq. 2 can be efficiently
performed by various algorithms, such as random or evolutionary search [18].
afterward, the performance of the searched optimal width d * is analyzed by
training from scratch.
current approaches for neural architecture search [19][20][21] often employ a
unilaterally augmented (ua) principle to evaluate each width, resulting in
unfair training of channels in the supernet. as illustrated in fig. 3(a), to
search for a dimension d at a layer with a maximum of n channels, the ua
principle assigns the left d channels in the supernet to indicate the
corresponding architecture aswhere γ a (d) means the selected d channels from
the left (smaller-index) side. however, the ua principle leads to channel
training imbalance in the supernet due to its constraints, as illustrated in
fig. 3(a). channels with smaller indices are used for various sizes, resulting
in over-training of the left channel kernels since widths are uniformly sampled.
the unfairness can be quantified by t , which represents how often a channel is
utilized, reflecting its level of training. given a layer has a maximum of n
channels, the t for the i-th channel under the ua principle iscorrespondingly,
the probability of i -th channel being trained can be expressed as p i = n-i+1 n
. therefore, channels closer to the left will get more attempts during training,
which leads the degree of training to vary widely between channels. this
introduces evaluation bias and leads to sub-optimal results.to mitigate
evaluation bias on width, we propose a new sc-net that promotes the fairness of
channels during training. as shown in fig. 3(b), in the proposed supernet, each
width is simultaneously evaluated by the sub-networks corresponding to the left
and right channels. this can be seen as two identical networks s l and s r that
are bilaterally coupled and evaluated using the ua principle, but counting
channels in reverse order. therefore, the number of all channels used for
evaluating the width d can be expressed as:where represents the union of two
lists with repeatable elements. in detail, the left channel in s l follows the
same ua principle setup as in eq. ( 3), while for the right channel in s r , we
count channels from the right h r (d) = [(nd + 1) :(nd)]. therefore, the
training degree of each channel is the sum of the two supernets s l and s r .
since the channels are counted from the right within s r , the training degree
of the d-th channel on the left corresponds to the training degree of the
(n-d+1)-th channel on the right in eq. ( 4). therefore, the training degree t
(d) of the d-th channel in our proposed method istherefore, the training degree
t for each channel will always be equal to the same constant value of the width,
independent of the channel index, ensuring fairness in terms of channel (filter)
levels. thus the network width can be fairly ranked using our network.
using a trained sc-net, the architecture can be evaluated. however, the search
space involved in nas is large, with more than 10 20 possible architectures,
requiring an evolutionary search using the multi-objective nsga-ii algorithm to
improve the search performance. during the evolutionary search, the width d of
each network is represented by the average precision of its corresponding left
and right paths in the supernet s, as shown in eq. ( 9). the optimal width (not
subnetwork) is determined as the one that achieves the best performance when
trained from scratch. here, s l and s r refer to the two paths of s that
correspond to the width d during the training process.
3 experiments the dataset, comprised of 194 skin slides acquired from the
southern sun pathology laboratory, includes 148 bcc cases and 46 other types
(common nevus, scc), all manually annotated by a dermatopathologist. bcc slides
served as positive samples and the rest as negatives. these slides were scanned
at ×20 magnification with a 0.44 µm pixel size using a leica aperio at2 scanner.
the patient data were separated between training and testing to prevent overlap.
details are shown in table 1. the experimental setup involved training models on
two nvidia rtx a6000 gpus using pytorch. these models, initialized from a
zero-mean gaussian with standard deviation σ = 0.001, were trained for 200
epochs with a batch size of 256. training used the adam optimizer with a dynamic
learning rate reduction strategy, starting with a learning rate of 5e-5
following a cosine schedule.
we validated our algorithm using the curated skin cancer dataset and sc-net as a
supernet, testing both heavy and light models. we performed a search on resnet50
and mobilenetv2 models, compared against original resnet50 (ori resnet50) and
mobilenetv2 (ori mobilenetv2) models as baselines. the resulting models are
denoted as s resnet50 and s mobilenetv2.comparison with related methods. to
ensure a fair comparison on our dataset, we selected several papers in the field
of pathological image analysis, such as [9,22,23], as well as others using the
ua principle, such as [18,24].evaluation metrics. our model was evaluated on: as
shown in table 2, the s resnet50 model outperformed in all metrics, showing
4.8%, 4.5%, 5.4%, 4.7% and 4.7% improvements in accuracy, sensitivity,
specificity, f 1 score, and auc, respectively, over ori resnet50, and surpassing
effect of sc-net as a supernet.
in this paper, we introduce sc-net, a novel nas framework for skin cancer
detection in pathology images. by formulating sc-net as a balanced supernet, we
ensure fair ranking and treatment of all potential architectures. with scnet and
evolutionary search, we obtained optimal architectures, achieving 96.2% top-1
and 96.5% accuracy on a skin cancer dataset, improvements of 4.8% and 4.7% over
baselines. future work will apply our approach to larger datasets for
wider-scale validation.
this study was performed in line with the principles of the declaration of
helsinki. ethics approval was granted by csiro health and medical human research
ethics committee (chmhrec). the ethics approval number is 2021 030 lr, and the
validity period is from 07 apr 2021 to 31 dec 2024.
pathology is widely recognized as the gold standard for disease diagnosis [15].
as the demand for intelligently pathological image analysis continues to grow,
an increasing number of researchers have paid attention to this field
[12,14,25]. however, pathological image analysis remains a challenging task due
to the complex and heterogeneous nature [19] of obtained whole slide images
(wsis), as well as their huge gigapixel-level size [20]. to address this issue,
multiple instance learning (mil) [1] is usually applied to formulate
pathological image analysis tasks into weakly supervised learning problems. in
the mil setting, the entire wsi is regarded as a bag and tiled patches are
instances. the primary challenge of mil arises from its weakly supervised
nature, i.e. only the baglevel label for the entire wsi is provided, while
labels for individual patches are usually unavailable. although mil-based
methods have shown impressive potential in solving a wide range of pathological
image analysis tasks including cancer grading and subtype diagnosis [23],
prognosis prediction [18], genotyperelated tasks such as gene mutation
prediction [4], etc., it is still an open question regarding learning an
informative and effective representation of the entire wsi for down-streaming
task based on mil architecture.current mil methods can be broadly categorized
into two types: bag-level mil and instance-level mil. bag-level mil [9,17], also
known as embeddingbased mil, involves converting patches (instances) into
low-dimensional embeddings, which are then aggregated into wsi (bag)-level
representations to conduct the analysis tasks [22]. the aggregator can take
different architectures such as an attention module [7,13], convolutional neural
network (cnn), transformer [16], or graph neural network [10,28]. instance-level
mil [2,8,24], on the other hand, focuses its learning process at the instance
level, and then obtains the bag-level prediction by simply aggregating instance
predictions. bag-level mil incorporates instance embeddings to create a bag
representation, converting the mil into a supervised learning problem.
furthermore, it can extract contextual information and correlations between
instances. nonetheless, bag-level mil needs to learn informative embeddings of
instances and adjust the contributions of these instance embeddings to generate
the bag representation simultaneously, which faces the risk of obtaining a
suboptimal model given the limited training samples in practice. the
instance-level mil, however, faces the problem of noisy labels, which is caused
by the common strategy of assigning the wsi labels to patches and the fact that
there are lots of patches irrelevant to the wsi labels [3,6].considering these
conventional mil methods usually utilize either bag-level or instance-level
supervision, leading to suboptimal performance. in this paper, we format the
instance-level mil as a noisy label learning task and propose to solve it by
designing an instance-level supervision based on the label disambiguation [21].
then we propose to combine bag-level and instance-level supervision to improve
the performance of mil. the bag-level and instance-level supervision can
corporately optimize the instance embedding learning process and welllearned
instance embeddings can facilitate the aggregation module to generate the bag
representation. the co-supervision design also makes the mil to be a multi-task
learning framework, where the bag-level supervision channel works to globally
summarise the wsi for prediction and the instance-level supervision channel can
locally identify key relevant patches. the detailed contributions can be
summarized as follows:1) we propose a novel mil method for pathological image
analysis that leverages a specially-designed residual transformer backbone and
organically integrates both transformer-based bag-level and
label-disambiguation-based instancelevel supervision for performance
enhancement. 2) we develop a label-disambiguation module that leverages
prototypes and confidence bank to tackle the weakly supervised nature of
instance-level supervision and reduce the impact of assigned noisy labels.3) the
proposed framework outperforms state-of-the-art (sota) methods on public
datasets and in a practical clinical task, demonstrating its superiors in wsi
analysis. besides, ablation studies illustrate the superiority of our
co-supervision design compared to using only one type of supervision. 2 method
the overall framework of the proposed iib-mil is shown in fig. 1. similar to
previous works [27], iib-mil first transforms input huge-size wsi to a set of
patch embeddings to simplify the following learning task using a pre-trained
encoder, i.e. efficientnet-b0. then a specially-designed residual transformer
backbone works to calibrate the obtained patch embeddings and encode the context
information and correlation of patches. after that, iib-mil utilizes both a
transformer-based bag-level and a label-disambiguation-based instance-level
supervision to cooperatively optimize the model, where the bag-level loss is
calculated referring to the wsi labels, while the instance loss is calculated
referring to pseudo patch labels calibrated by the label-disambiguation module.
since bag-level supervision channel is trained to globally summarise information
of all patches for prediction, the bag-level outputs are used as the final
predictions during the test stage.
assume there is a set of n wsis denoted by s = {s 1 , s 2 , ..., s n }. each wsi
s i has a wsi-level label y i ∈ {1, ..., c}, where c represents category number.
in each s i , there exist m i tiled patches without patch-level labels. to
reduce the computational cost, we used a frozen pre-trained encoder to transform
patches into k dimensional embeddings {e i,our proposed iib-mil comprehensively
integrates obtained embeddings {e i,j , ...} to generate accurate wsi
classification.
before bag-level and instance-level supervision channels, we design a residual
transformer backbone t (•) : r k → r d to calibrate the obtained patch
embeddings and encode the context information and correlation of patches. t (•)
maps patch embeddings {e i,j , ...} to a lower-dimensional feature space,
denoted as {x i,j , ...}, where x i,j = t (e i,j ), x i,j ∈ r d is the
calibrated embedding, t (•) is composed of transformer layers and skip
connections (details are given in the supplementary.).
at the core of instance-level supervision is the label disambiguation module,
which serves to rectify the imprecise labels that have been assigned to patches.
it comprises prototypes and a confidence bank, takes instance features and
instance classifier predictions as inputs, and generates soft labels as outputs
(fig. 1 (b)). the prototypes, denoted as p ∈ r c×d , are initialized with
all-zero vectors and employ momentum-based updates using selected instance
features x with the highest probability prob inst of belonging to their
corresponding categories. prototype labels z are determined based on the
proximity of patch features to the prototypes. confidence b ∈ r n ×m ×c is
initialized with all wsi labels and uses momentum-based updates with z. detailed
steps are summarized as follows:step 1: obtain the instance classifier output.
the instance-level classifier, denoted as f inst (•), takes x i,j ∈ r d as input
and outputs the predicted instance probability prob inst i,j ∈ r c , as:the
probability that x i,j is predicted as class c is denoted asstep 2: obtain the
prototype labels. at t time, the prototype vector for the category c is p c,t ∈
r d . to update p c,t , we select a set of instance features set c,t that have
the highest probabilities prob inst i,j,c of belonging to category c.
specifically, we define set c,t as:where k is the number of top instance
features to select. then, we use a momentum-based update rule to obtain p c,t+1
:where α is the momentum coefficient that automatically decreases from α = 0.95
to α = 0.8 across epochs. then, we can obtain prototype labels z i,j ∈ r c using
the following equation:the resulting prototype label z i,j ∈ r c is a one-hot
vector that indicates the category of the j-th instance in the i-th wsi.step 3:
obtain soft labels from the confidence bank specifically, at time t, the
pseudo-target b i,j,t ∈ r c of the instance embedding e i,j is updated by the
following:where β is the momentum update parameter with a default value of β =
0.99.step 4: calculate instance-level loss. we compute instance-level loss using
the cross-entropy function:here, b i,j,c and prob inst i,j,c are the c-th
component of the pseudo-target b i,j and predicted probability prob inst i,j ,
respectively.
for bag-level supervision, instance features x i ∈ r m ×d go through a
transformer-based aggregator a(•) : r m ×d → r d and a wsi classifier f bag (•)
: r d → r c in turn (architecture details are given in the supplementary.). then
we obtain the predicted probability of wsi s i as:the bag-level loss function is
given by:where y i ∈ r c is the label of wsi s i .
in the training phase, we employ a warm-up strategy in which we update only the
prototypes and do not update the confidence bank during the first few epochs.
our approach is trained end-to-end, and the total loss function is :where λ is
the hyperparameter that controls the relative importance of the two losses.
we evaluate our model with three datasets. (1) luad-gm dataset: the objective is
to predict the epidermal growth factor receptor (egfr) gene mutations in
patients with lung adenocarcinoma (luad) using 723 whole slide image (wsi)
slices, where 47% of cases have egfr mutations. (2) tcga-nsclc and tcga-rcc
datasets: cancer type classification is performed using the cancer genome atlas
(tcga) dataset. the tcga-nsclc dataset comprised two subtypes, lung squamous
cell carcinoma (lusc) and lung adenocarcinoma (luad), while the tcga-rcc dataset
included three subtypes: renal chromophobe cell carcinoma (kich), renal clear
cell carcinoma (kirc), and renal papillary cell carcinoma (kirp).
the dataset was randomly split into three parts: training, validation, and
testing, with 60%, 20%, and 20% of the samples, respectively. wsis were
preprocessed by cropping them into 1120 × 1120 patches, without overlap. the
proposed model was implemented in pytorch, trained on a 32gb tesla v100 gpu,
using adamw [11] optimizer. the batch size was set to 4, with a learning rate of
1e -4 and a weight decay of 1e -5 . [7,12], cnn-mil [20], dsmil [9], clam [13],
vit-mil [5], transmil [16], setmil [27], and dtfd [26]. all methods were
evaluated in three tasks, namely gene mutation prediction (with or without egfr
mutation), tcga-nsclc subtype classification, and tcga-rcc subtype
classification. iib-mil achieved aucs of 85.62%, 98.11%, and 99.57%. we can also
find iib-mil outperformed other sota methods, in the three tasks with at least
1.78%, 0.74%, and 0.56% performance enhancement (auc), respectively.
we conducted ablation studies to assess the efficacy of each component in
iib-mil. the results, in table 2, indicate that all of the designed components,
including the label disambiguation module, instance-level supervision, and
baglevel supervision, contribute to the success of iib-mil. we also investigated
the impact of the warm-up epoch number and found that selecting an appropriate
value, such as warmup = 10, can lead to better model performance. furthermore,
we examined the impact of the weighting factor λ, and the outcomes indicated
that assigning greater importance to instance-level supervision (λ = 5) helps
iib-mil enhance its performance, thus demonstrating the effectiveness of the
designed label-disambiguation-based instance-level supervision.
figure 2(a) shows the t-sne plot of the obtained patch features from the
backbone of the iib-mil. the patches are unsupervisedly clustered into groups
based on their features, indicated by various colors. the numbers displayed
within each group represent the average likelihood of the egfr mutation
predicted by the patches. with the help of the label-disambiguation-based
instance-level supervision, iib-mil can identify highly positive and negative
related patches to the wsi-label, i.e., the cyan-blue group and yellow group.
double-checked by pathologists, we find that the cyan-blue group consists of
patches from lung adenocarcinoma and the yellow group consists of patches from
the squamous cells. this finding aligns with the domain knowledge of
pathologists. figure 2(b) investigates the contribution of each patch in
predicting egfr mutation. the resulting heatmap shows the decision mechanism of
iib-mil in the accurate distinguishment between egfr mutation-positive and
negative samples.
this paper presents iib-mil, a novel mil approach for pathological image
analysis. iib-mil utilizes a label disambiguation module to establish more
precise instance-level supervision. it then combines the instance-level and
bag-level supervision to enhance the performance of the iib-mil. experimental
results demonstrate that iib-mil surpasses current sota techniques on publicly
available datasets, and holds significant potential for addressing more complex
clinical applications, such as predicting gene mutations. furthermore, iib-mil
can identify highly relevant patches, providing pathologists with valuable
insights into underlying mechanisms.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 54.
histopathological images are regarded as the 'gold standard' in the diagnosis of
cancers. with the advent of the whole slide image (wsi) scanner, deep learning
has gained its reputation in the field of computational pathology [1][2][3].
however, wsis are extremely large in the size and lack of pixel-level
annotations, making it difficult to adopt the traditional supervised learning
methods for wsi classification [4].to address this issue, multiple instance
learning (mil) has been successfully applied to the wsi classification task as a
weakly supervised learning problem [5][6][7]. in this context, a wsi is
considered as a bag, and the cropped patches within the slide are the instances
in this bag. however, the lesion regions usually only account for a small
portion of the wsi, resulting in a large number of negative patches. when the
positive and negative instances in the bag are highly imbalanced, the mil models
are prone to incorrectly discriminate these positive instances when using simple
aggregation operations. to this end, several attention-based mil models, such as
abmil [8] and dsmil [9], apply variants of the attention mechanism to re-weight
instance features. thereafter, the recent works develop the transformer-based
architectures to better model long-range instance correlations via
self-attention [10][11][12][13]. however, since the average bag size of a wsi is
more than 8000 at 20 × magnification, it is computationally infeasible to use
the conventional transformer and other stacked self-attention network
architectures in mil-related tasks.recently, prototypical learning is applied in
wsi analysis to identify representative instances in the bag [14]. some works
adopt the k-means clustering on all instances in a bag to obtain k cluster
centers i.e., instance prototypes, and then use these prototypes to represent
the bags [15,16]. these clustering-based mil algorithms can significantly reduce
the redundant instances, and thereby improving the training efficiency for wsi
classification. however, it is different for k-means to specify the cluster
number as well as the initial cluster centers, and different initial values may
lead to different cluster results, thus affecting the performance of mil.
besides, affected by the feature extractor, the clustering-based mil algorithms
may ignore the most important instances that contain critical diagnostic
information. therefore, it is necessary to develop a method that can fully
exploit the potential complementary information between critical instances and
prototypes to improve representation learning of prototypes.on the other hand,
when pathologists analysis the wsis, they always observe the tissues at various
resolutions [17]. inspired by this diagnostic manner, some works use multi-scale
information of wsis to improve diagnostic accuracy. for example, li et al. [9]
adopted a pyramidal concatenation mechanism to fuse the multi-scale features of
wsis, in which the feature vectors of low-resolution patches are replicated and
concatenated with the those of their corresponding high-resolution patches; hou
et al. [18] propose a heterogeneous graph neural network to learn the
hierarchical representation of wsis from a heterogeneous graph, which is
constructed by the feature and spatial-scaling relationship of multi-resolution
patches. however, since the number of patches at each resolution is quite
different, it requires complex pre-processing to spatially align feature vectors
of patches in different resolutions. therefore, it is significant to develop an
efficient and effective patch aggregation strategy to learn multi-scale
information from wsis.in this work, we propose a multi-scale prototypical
transformer (mspt) for wsi classification. the mspt includes two key components:
a prototypical transformer (pt) and a multi-scale feature fusion module (mffm).
the specifically developed pt uses a clustering algorithm to extract instance
prototypes from the bags, and then re-calibrates these prototypes at each scale
with the self-attention mechanism in transformer [19]. mffm is designed to
effectively fuse multi-scale information of wsis, which utilizes the mlp-mixer
[20] to learn effective representations by aggregating the multi-scale
prototypes generated by the pt. the mlp-mixer adopts two types of mlp layers to
allow information communication in different dimensions of data.the
contributions of this work are summarized as follows:1) a novel prototypical
transformer (pt) is proposed to learn superior prototype representation for wsi
classification by integrating prototypical learning into the transformer
architecture. it can effectively re-calibrate the cluster prototypes as well as
reduce the computational complexity of the transformer. 2) a new multi-scale
feature fusion module (mffm) is developed based on the mlp-mixer to enhance the
information communication among phenotypes. it can effectively capture
multi-scale information in wsi to improve the performance of wsi classification.
mil is a typical weakly supervised learning method, where the training data
consists of a set of bags, and each bag contains multiple instances. the goal of
mil is to learn a classifier that can predict the label of a bag based on the
instances in it. in binary classification, a bag can be marked as negative if
all in-stances in the bag are negative, otherwise, the bag is labeled as
positive with at least one positive instance. in the mil setting, a wsi is
considered as a bag and the numerous cropped patches in wsi are regarded as
instances in the bag. a wsi dataset t can be defined as:where x i denotes a
patient, y i the label of x i , i j i is the j-th instance of x i , n is the
number of patients and n is the number of instances.
the overall architecture of mspt is shown in fig. 1. a wsi is first divided into
nonoverlapping patches at different resolutions, and a pre-trained resnet18 [21]
is used to extract features from each patch. the learned multi-scale features
are then fed into the proposed mspt, which consists of a pt and an mffm, to
re-calibrate cluster prototypes at each scale and fuse multi-scale information
of wsi. finally, a wsi-level classifier is trained to predict the bag
label.pre-training. it is a time consuming and tedious task for pathologists to
annotate the patch-level labels in gigapixel wsis, thus, a common practice is to
use a pre-trained encoder network to extract instance-level features, such as an
imagenet pre-trained encoder or a self-supervised pre-trained encoder. in this
work, we follow [9] to adopt simclr [22] to pre-training the patch encoder at
different resolutions. simclr is a self-supervised learning algorithm to
pre-trainng a network by maximizing the similarity between positive pairs and
minimizing the similarity between negative pairs [22]. after pre-training, the
extracted instances of different scales are fed into mspt for prototype learning
and multi-scale learning.
most tissues in wsis are redundancy, and therefore, we introduce the instance
prototypes to reduce redundant instances. specifically, for each instance bag x
bag ∈ r n×d k , the k-means clustering algorithm is applied on all instances to
get k centers (prototypes). these cluster prototypes can be used as instances to
represent a new bag p bag ∈ r k×d k . however, the k-means clustering algorithm
is sensitive to the initial selection of cluster centers, i.e. different
initializations can lead to different results, and the final result may not be
the global optimal solution. it is essential to try different initializations
and choose the one with the lowest error. however, the wsi dataset generally has
a long sequence of instances, which makes the clustering algorithms
computationally expensive and slow down as the size of the bag increases.to
solve the issue above, we propose to apply the self-attention (sa) mechanism in
transformer to re-calibrate these cluster prototypes. as shown in fig. 1, the
optimization process can be divided into two steps: 1) the initial cluster
prototype bag p bag is obtained in the pre-processing stage by using the k-means
clustering on x bag ; ; 2) pt uses x bag to optimize p bag via the
self-attention mechanism in transformer. the detailed process is as
follows:where w q , w k , w v ∈ r d k ×d k are trainable matrices of query p bag
and the key-value pair (x bag , x bag ), respectively, and a map ∈ r k×n is the
attention matrix to compute the weight of x bag . thus, the computational
complexity of sa is o(nm) instead of o n 2 , and the k is much less than n.
specifically, for a single clustering prototype p k ∈ p, the sa layer scores the
pairwise similarity between p k and x n for all x n ∈ x, which can be written as
a row vector [a k1 , a k2 , a k3 , . . . , a kn ] in a map . these attention
scores are then weighted to x bag to update the p k ∈ r 1×d k for completing the
calibration of the clustering prototypes p ∈ r k×d k .as mentioned above,
existing clustering-based mil methods use the k-means clustering to identify
instances prototypes in the bag, where the most important instances that contain
the key semantic information may be ignored. on the contrary, our pt can
efficiently use all the instances to update the cluster prototypes multiple
times. therefore, the combination of bag instances is no longer static and
fixed, but diverse and dynamic. it means that different new bags can be fed into
the mffm each time. in addition, by applying pt to each scale, the number of
cluster prototypes obtained at different scales is consistent, so there is no
need for additional operations to align multi-scale features.multi-scale feature
fusion module (mffm). to fuse the output clustered prototypes at different
scales in mspt, we proposed an mffm, which consists of an mlp-mixer and a gated
attention pooling (gap). the mlp-mixer is used to enhance the information
communication of the prototype representation, and the gap is used to get the
wsi-level representation for wsi classification.as shown in fig. 2, the mixer
layer of mlp-mixer contains one token-mixing mlp and one channel-mixing mlp,
each consisting of two fully-connected layers and a gelu activation function
[23]. token-mixing mlp is a cross-location operation to mix all prototypes,
while channel-mixing mlp is a pre-location operation to mix features of each
prototype. thus, mlp-mixer allows the information communication between
different prototypes and prototype features to learn superior representation
through information aggregation. specifically, the procedure of mffm is
described as follows:we first perform the feature concatenation operation on the
multi-scale output clustering prototypes p20× , p10× , p5× to construct a
feature pyramid p:where d k is the feature vector dimension of the
prototypes.then, the p is fed to the mlp-mixer to obtain the corresponding
hidden feature representation h ∈ r k×3d k as follows:where ln denotes the layer
normalization, σ denotes the activation function implemented by gelu,k are the
weight matrices of mlp layers.c and d s are tunable hidden widths in the
token-mixing and channel-mixing mlp, respectively. finally, the h is fed to the
gated attention pooling (gap) [8] to get the wsi-level representation z ∈ r 1×3d
k for wsi classification:where y ∈ r 1×d out is the class label probability of
the bag, and d out is the number of classes.
to evaluate the effectiveness of mspt, we conducted experiments on two public
dataset, namely camelyon16 [24] and tcga-nsclc. camelyon16 is a wsi dataset for
the automated detection of metastases in lymph node tissue slides. it includes
270 training samples and 129 testing samples. after pre-processing, a total of
2.4 million patches at ×20 magnification, 0.56 million patches at ×10
magnification, and 0.16 million patches at ×5 magnification, with an average of
about 5900, 1400, and 400 patches per bag. the tcga-nsclc dataset includes two
sub-types of lung cancer, i.e., lung squamous cell carcinoma (tgca-lusc) and
lung adenocarcinoma (tcga-luad). we collected a total of 854 diagnostic slides
from the national cancer institute data portal (https:// portal.gdc.cancer.gov).
the dataset yields 4.3 million patches at 20× magnification, 1.1 million patches
at 10× magnification, and 0.30 million patches at 5× magnification with an
average of about 5000, 1200, and 350 patches per bag.
in wsi pre-processing, each slide is cropped into non-overlapping 256 × 256
patches at different magnifications, and a threshold is set to filter out
background ones. after patching, we use a pre-trained resnet18 model to convert
each 256 × 256 patch into a 512-dimensional feature vector. we selected accuracy
(acc) and area under curve (auc) as evaluation metrics. for camelyon16 dataset,
we reported the results of the official testing set. for tcga-nsclc, we
conducted five cross-validation on the 854 slides, and the results are reported
in the format of mean ±sd (standard deviation).
for the feature extractor, we employed the simclr encoder trained by lee et al.
[9] for the camelyon16 and tcga datasets. but [9] only trained simclr encoders
at 20× and 5× magnification, to align with that setting, we used the same
settings to train the simclr encoder at 10× magnification on both datasets. for
the proposed mspt, the adam optimizer was used to update the model weights, the
initial learning rate of 1e-4 with a weight decay of 1e-5. the mini-batch size
was set as 1. the mspt models were trained for 150 epochs and they would early
stop if the loss would not decrease in the past 30 epochs. all models were
implemented by python 3.8 with pytorch toolkit 1.11.0 on a platform equipped
with an nvidia geforce rtx 3090 gpu.
comparison algorithms. the proposed mspt was compared to state-of-the-art
milbased algorithms: 1) the traditional pooling operators, such as mean-pooling
and maxpooling; 2) the attention-based algorithms, including abmil [8] and dsmil
[9]; 3) the transformer-based algorithm transmil [11]; 4) the clustering-based
algorithm remix [16]. 1 shows the comparison results on the camelyon16 and
tcga-nsclc datasets. in camelyon16, it can be found that the proposed mspt
outperforms all the compared algorithms with the best accuracy of 0.9536, and
auc of 0.9869. compared to other algorithms, mspt improves at least 0.78%, and
1.07% on classification acc and auc, indicating the effectiveness of mffm to
learn the multi-scale information of wsis. in addition, pt achieves the best
classification results in the single-resolution methods and outperforms remix on
all indices, which proves pt can effectively re-calibrate the clustering
prototypes.in tcga-nsclc, the proposed mspt algorithm again outperforms all the
compared algorithms on all indices. it achieves the best classification
performance of 0.9289 ± 0.011 and 0.9622 ± 0.015 on the acc and auc. moreover,
mspt improves at least 0.78% and 1.03%, respectively, on the corresponding
indices compared with all other algorithms.
to evaluate the contribution of pt and mffm in the proposed mspt, we further
conducted a series of ablation studies.investigation of the number of prototypes
in pt. to evaluate the effectiveness of the pt, we first changed the number of
prototypes k in the range of {1, 2, 4, 8, 16, 32} to get the optimal k for each
dataset. then, the following two variants were compared with pt: (1) full-bag:
the first variant was only trained on all the instances; (2) prototype-bag: the
second variant was only trained on the cluster prototypes.as shown in fig. 3,
the horizontal axes denote the number of prototypes, and the vertical axes
denote the classification accuracy. in the camelyon16 dataset, the performance
of both pt and prototype-bag increases with the increase of k value, and
achieves the best results with k = 16. in the tcga-nsclc dataset, pt always
outperforms the fullbag and prototype-bag. these experimental results
demonstrate that pt can effectively re-calibrate the clustering prototypes to
achieve superior results. investigation of multi-scale fusion. we further
compared our mffm with several other fusion strategies, including (1)
concatenation: this variant concatenated the cluster prototypes of each
magnification before the classifier. (2) ms-max: this variant used max-pooling
on the cluster prototypes for each magnification, and then added them. (3)
ms-attention: this variant used attention-pooling [8] on the cluster prototypes
for each magnification, and then added them.table 2 gives the results on the
camelyon16 and tcga-nsclc datasets. compared with other multi-scale variants,
the proposed mspt improves acc by at least 0.78% and 0.85% on camelyon16 and
tcga-nsclc, respectively, which proves that the mlp-mixer in mffm can
effectively enhance the information communication among phenotypes and their
features, thus improving the performance of feature aggregation. more studies.
we provide more empirical studies, i.e., the effect of the multiresolution
scheme, the visualization results, and the training budgets, in supplementary
materials to better understand mspt.
in summary, we propose an mspt for wsi classification that combine the
prototypebased learning and multi-scale learning to generate powerful wsi-level
representation. the mspt reduces redundant instances in wsi bags by replacing
instances with updatable instance prototypes, and avoids complicated procedures
to align patch features at different scales. extensive experiments validate the
effectiveness of the proposed mspt. in the future, we will develop an attention
mechanism based on the magnification level to re-weight the features from
different scales before fusion in mspt.
contrast-enhanced ultrasound (ceus) as a modality of functional imaging has the
ability to assess the intensity of vascular perfusion and haemodynamics in the
thyroid nodule, thus considered a valuable new approach in the determination of
benign vs. malignant nodules [1]. in practice, ceus video allows the dynamic
observation of microvascular perfusion through intravenous injection of contrast
agents. according to clinical experience, for thyroid nodules diagnosis, there
are two characteristic that are important when analyzing ceus video. 1) dynamic
microvessel perfusion. as shown in fig. 1(a), clinically acquired ceus records
the dynamic relative intensity changes (microvessel perfusion pattern)
throughout the whole examination [2]. 2) infiltrative expansion of microvessel.
many microvessels around nodules are constantly infiltrating and growing into
the surrounding tissue. as shown in fig. 1(b), based on the difference in lesion
size displayed by the two modalities, clinical practice shows that gray us
underestimates the size of lesions, and ceus video overestimates the size of
some lesions [3]. although the radiologist's cognition of microvascular invasive
expansion is fuzzy, they think it may promote diagnosing thyroid nodules [1].
used the spatial feature enhancement for disease diagnosis based on dynamic
ceus. furthermore, by combing the us modality, chen et al. [5] proposed a
domain-knowledge-guided temporal attention module for breast cancer diagnosis.
however, due to artifacts in ceus, sota classification methods often fail to
learn regions where thyroid nodules are prominent (as in appendix fig. a1) [6].
even the sota segmentation methods cannot accurately identify the lesion area
for blurred lesion boundaries, thus, the existing automatic diagnosis network
using ceus still requires manual labeling of pixel-level labels which will lose
key information around the tissues [7]. in particular, few studies have
developed the ceus video based diagnostic model inspired by the dynamic
microvessel perfusion, or these existing methods generally ignore the influence
of microvessel infiltrative expansion. whether the awareness of infiltrative
area information can be helpful in the improvement of diagnostic accuracy is
still unexplored.here, we propose an explanatory framework for the diagnosis of
thyroid nodules based on dynamic ceus video, which considers the dynamic
perfusion characteristics and the amplification of the lesion region caused by
microvessel infiltration. our contributions are twofolds. first, the temporal
projection attention (tpa) is proposed to complement and interact with the
semantic information of microvessel perfusion from the time dimension. second,
we adopt a group of confidence maps instead of binary masks to perceive the
infiltrative expansion area from gray us to ceus of microvessels for improving
diagnosis.
the architecture of the proposed framework is shown in fig. 2. the tasks of
lesion area recognition and differential diagnosis are pixel-level and
image-level classifications, and some low-level features of these two tasks can
be shared interactively [8]. we first fed the ceus video i ∈ r c×t ×h×w into the
cross-task feature extraction (cfa) module to jointly generate the features f
iden and f cls for lesion area recognition and differential diagnosis,
respectively. after that, in the temporal-based lesions area recognition (tlar)
module, an enhanced v-net with the tpa is implemented to identify the relatively
clear lesion area which are visible on both gray us and ceus video. because
microvessel invasion expansion causes the tumor size and margin depicted by ceus
video to be larger than that of gray us, we further adopt a group of confidence
maps based on sigmoid alpha functions (saf) to aware the infiltrative area of
microvessels for improving diagnosis. finally, the confidence maps are fused
with f cls and fed into a diagnosis subnetwork based on lightweight c3d [9] to
predict the probability of benign and malignant. in the cfa, we first use the 3d
inception block to extract multi-scale features f muti . the 3d inception block
has 4 branches with cascaded 3d convolutions. multiple receptive fields are
obtained through different branches, and then group normalization and relu
activation are performed to obtain multi-scale features f muti . then, we use
the cross-task feature adaptive unit to generate the features f iden and f cls
required for lesions area recognition and thyroid nodules diagnosis via the
following formula [10]:where ω iden , ω cls are the learnable weights.
the great challenge of automatic recognition of lesion area from ceus video is
that the semantic information of the lesion area is different in the ceus video
of the different microvessel perfusion periods. especially in the perfusion
period and the regression period, the semantic information of lesions cannot be
fully depicted in an isolated ceus frame. thus, the interactive fusion of
semantic information of the whole microvessel perfusion period will promote the
identification of the lesion area, and we design the temporal projection
attention (tpa) to realize this idea. we use v-net as the backbone, which
consists of four encoder/decoder blocks for tlar, and the tpa is used in the
bottleneck of the v-net.
16 after four down-sampling operations in encoder, its original 3d feature map
is projected [11] to 2d plane to get keys and queries:16 , and we use global
average pooling (gap) and global maximum pooling (gmp) as temporal projection
operations. here,16 is obtained by a single convolution. this operation can also
filter out the irrelevant background and display the key information of the
lesions. after the temporal projection, a group convolution with a group size of
4 is employed on k to extract the local temporal attention l ∈ r c× h 16 × w 16
. then, we concatenate l with q to further obtain the global attention g ∈ r
c×1× h 16 × w 16 by two consecutive 1 × 1 2d convolutions and dimension expend.
those operations are described as follows:where gonv(•) is the group
convolution, σ denotes the normalization, "⊕" is the concatenation operation.
the global attention g encodes not only the contextual information within
isolated query-key pairs but also the attention inside the keys [12]. after
that, based on the 2d global attention g, we multiply v and g to calculate the
global temporal fusion attention map16 to enhance the feature
representation.meanwhile, to make better use of the channel information, we use
3 16 . then, we use parallel average pooling and full connection operation to
reweight the channel information of f 4th to obtain the reweighted featurethe
obtained global temporal fusion attention maps m are fused with the reweighted
feature f 4th to get the output features f fin . finally, we input f fin into
the decoder of the tlar to acquire the feature map of lesion.
we design a mia module to learn the infiltrative areas of microvessel. the
tumors and margin depicted by ceus may be larger than those depicted by gray us
because of continuous infiltrative expansion. inspired by the continuous
infiltrative expansion, a series of flexible sigmoid alpha functions (saf)
simulate the infiltrative expansion of microvessels by establishing the distance
maps from the pixel to lesion boundary. here, the distance maps [13] are denoted
as the initial probability distribution p d . then, we utilize iterative
probabilistic optimization (ipo) unit to produce a set of optimized probability
maps p = {p 1 , p2 . . . pn } to aware the microvessel infiltration for thyroid
nodules diagnosis. based on saf and ipo, ceus-based diagnosis of thyroid nodules
can make full use of the ambiguous information caused by microvessel
infiltration.
it is generally believed that the differentiation between benign and malignant
thyroid nodules is related to the pixels around the boundaries of the lesion
[14], especially in the infiltrative areas of microvessel [3]. therefore, we
firstly build the initial probability distribution p d based on the distance
between the pixels and the annotation boundaries by using saf in order to aware
the infiltrative areas. here, saf is defined as follows:where α is the
conversion factor for generating initial probability distribution p d (when α →
∞, the generated p d is binary mask); c is used to control the function value
within the range of [0, 1]; (i, j) is the coordinate point in feature map; d (i,
j) indicates the shortest distance from (i, j) to lesion's boundaries.iterative
probabilistic optimization (ipo) unit. based on the fact that inceptext [15] has
experimentally demonstrated that asymmetric convolution can effectively solve
the problem of highly variable size and aspect ratio, we use asymmetric
convolution in the ipo unit. asymmetric convolution-based ipo unit can optimize
the initial distribution p d to generate optimized probability maps p that can
reflect the confidence of benign and malignant diagnosis. specifically, with the
ipo, our network can make full use of the prediction information in low-level
iteration layer, which may improve the prediction accuracy of high-level
iteration layer. in addition, the parameters in the high-level iteration layer
can be optimized through the back-propagation gradient from the high-level
iteration layer. ipo unit can be shown as the following formula:where "⊕"
represents the concatenation operation;convblock consists of a group of
asymmetric convolutions (e.g., conv1 × 5, conv5 × 1 and conv1 × 1); n denotes
the number of the layers of ipo unit. with the lesion's feature map f 0 from the
tlar module, the initial distribution p d obtained by saf is fed into the first
optimize layer of ipo unit to produce the first optimized probability map p1 .
then, p1 is contacted with f 0 , and used to generate optimized probability map
p2 through the continuous operation based on saf and the second optimize layer
of ipo unit. the optimized probability map pi-1 provides prior information for
producing the next probability map pi . in this way, we can get a group of
probability map p to aware the microvascular infiltration.
with continuous probability map p obtained from mia, p are multiplied with the
feature f cls . then, these maps are fed into a lightweight c3d to predict the
probability of benign and malignant, as shown in fig. 2. we use the mean square
error l mse to constrain the generation of p . assuming that the generated p is
ready to supervise the classification network, we want to ensure that the
probability maps can accurately reflect the classification confidence. thus, we
design a task focus loss l ta to generate confidence maps p , as follows:where g
i is the label of pi , which is generated by the operation of saf(d (i,j) , α i
); pi denotes pixel in the image domain ω, σ is a learnable parameter to
eliminate the hidden uncertainty information.for differentiating malignant and
benign, we employ a hybrid loss l total that consists of the cross-entropy loss
l cls , the loss of l mse computing optimized probability maps p , and task
focus loss l ta . the l total is denoted as follows:where λ 1 , λ 2 , λ 3 are
the hyper-parameters to balance the corresponding loss. as the weight parameter,
we set λ 1 , λ 2 , λ 3 are 0.5,0.2,0.3 in the experiments.
dataset. our dataset contained 282 consecutive patients who underwent thyroid
nodule examination at nanjing drum tower hospital. all patients performed
dynamic ceus examination by an experienced sonographer using an iu22 scanner
(philips healthcare, bothell, wa) equipped with a linear transducer l9-3 probe.
these 282 cases included 147 malignant nodules and 135 benign nodules. on the
one hand, the percutaneous biopsy based pathological examination was implemented
to determine the ground-truth of malignant and benign. on the other hand, a
sonographer with more than 10 years of experience manually annotated the nodule
lesion mask to obtain the pixel-level groundtruth of thyroid nodules
segmentation. all data were approved by the institutional review board of
nanjing drum tower hospital, and all patients signed the informed consent before
enrollment into the study.implementation details. our network was implemented
using pytorch framework with the single 12 gb gpu of nvidia rtx 3060. during
training, we first pre-trained the talr backbone via dice loss for 30 epochs and
used adam optimizer with learning rate of 0.0001. then, we loaded the
pre-trained weights to train the whole model for 100 epochs and used adam
optimizer with learning rate of 0.0001. here, we set batch-size to 4 during the
entire training process the ceus consisted the full wash-in and wash-out phases,
and the resolution of each frame was (600 × 800). in addition, we carried out
data augmentation, including random rotation and cropping, and we resize the
resolution of input frames to (224 × 224). we adopted 5-fold cross-validation to
achieve quantitative evaluation. three indexes including dice, recall, and iou,
were used to evaluate the lesion recognition task, while five indexes, namely
average accuracy (acc), sensitivity (se), specificity (sp), f1-score (f1), and
auc, were used to evaluate the diagnosis task.experimental results. as in table
1, we compared our method with sota method including v-net, unet3d, transunet.
for the task of identifying lesions, the index of recall is important, because
information in irrelevant regions can be discarded, but it will be disastrous to
lose any lesion information. v-net achieved the highest recall scores compared
to others; thus, it was chosen as the backbone of tlar. table 1 revealed that
the modules (tpa, saf, and ipo) used in the network greatly improved the
segmentation performance compared to baseline, increasing dice and recall scores
by 7.60% and 7.23%, respectively. for the lesion area recognition task, our
method achieved the highest dice of 85.54% and recall of 90.40%, and the
visualized results were shown in fig. 3. to evaluate the effectiveness of the
baseline of lightweight c3d, we compared the results with sota video
classification methods including c3d, r3d, r2plus1d and convlstm. for fair
comparison, all methods used the manually annotated lesion mask to assist the
diagnosis. experimental results in table 2 revealed that our baseline network
could be useful for the diagnosis. with the effective baseline, the introduced
modules including tlar, saf and ipo further improved the diagnosis accuracy,
increasing the accuracy by 9.5%. the awareness of microvascular infiltration
using saf and ipo unit was helpful for ceus-based diagnosis, as it could improve
the diagnosis accuracy by 7.69% (as in table 2). as in appendix fig. a1,
although sota method fails to focus on lesion areas, our method can pinpoint
discriminating lesion areas. influence of α values. the value of α in saf is
associated with simulating microvessel infiltration. figure 3 (c) showed that
the diagnosis accuracy increased along with the increment of α and then tended
to become stable when α was close to 9. therefore, for balancing the efficiency
and performance, the number of ipo was set as n = 3 and α was set as α = {1, 5,
9} to generate a group of confidence maps that can simulate the process of
microvessel infiltration. (more details about the setting of n is in appendix
fig. a4 of the supplementary material.)
the microvessel infiltration leads to the observation that the lesions detected
on ceus tend to be larger than those on gray us. considering the microvessel
infiltration, we propose an method for thyroid nodule diagnosis based on ceus
videos. our model utilizes a set of confidence maps to recreate the lesion
expansion process; it effectively captures the ambiguous information caused by
microvessel infiltration, thereby improving the accuracy of diagnosis. this
method is an attempt to eliminate the inaccuracy of diagnostic task due to the
fact that gray us underestimates lesion size and ceus generally overestimates
lesion size. to the best of our knowledge, this is the first attempt to develop
an automated diagnostic tool for thyroid nodules that takes into account the
effects of microvessel infiltration. the way in which we fully exploit the
information in time dimension through tpa also makes the model more clinically
explanatory.
networkunet3d[18] v-net[16] transunet[17] v-net+ tpa v-net+ tpa+saf v-net+
tpa+ipo ours
nuclei segmentation is a fundamental step in histology image analysis. in recent
advances, with a large amount of labeled data, fully-supervised learning methods
can easily achieve reasonable results [1][2][3][4][5]. however, accurate
pixel-level annotation of nuclei is not always accessible for segmentation
labeling is a laborintensive and time-consuming procedure. methods to relieve
the high dependency on the accurate annotations of nuclei are highly
needed.unsupervised learning (ul) methods achieved great success in the data
dependency problem for nuclei segmentation, which learns from the structural
properties in the data without any manual annotations. based on the character of
these methods, we can group them into two categories: the traditional ul methods
and the deep learning ul methods. traditional ul nuclei segmentation methods
include watershed [6], contour detection [7], clustering [8,9] and random field
[10]. these methods focus on either pixel value or shape information but fail to
take advantage of both of them. moreover, due to the heavily rely on preset
parameters, these traditional methods also show weak robustness.therefore, some
researchers [11][12][13][14][15] resort to deep ul segmentation models to better
utilize both pixel value and shape information and develop a robust approach.
the common and effective way is to employ image clustering by maximizing mutual
information between image and predicted labels to distinguish foreground and
background regions. many image-clustering-based deep ul methods for natural
tasks still achieve strong performances in nuclei segmentation. kanezaki et al.
[11] constrain a convolutional neural network (cnn) with superpixel level
segmentation results. ji et al. [12] propose the invariant information
clustering. while reasonable results are obtained, these deep clustering-based
methods still suffer difficulties: (i) poor segmentation of the regions between
adjacent nuclei. deep clustering models succeed in transferring images to
highdimensional feature space and obtaining image segmentation results by means
of clustering pixels' features. however, the regions between adjacent nuclei are
similar to the nuclei regions in terms of color values and textures (as shown in
fig. 1). deep clustering-based methods experience difficulties in dealing with
these regions due to the lack of supervision. (ii) underutilization of
intra-image self similarity (iiss) information. as shown in fig. 1, in terms of
value, shape and texture, nuclei show a similar appearance within the same image
but vary greatly among different images 1 . this phenomenon offers valuable
information for networks to use but the current clustering models do not take
this into account.to address the above issues and motivated by the iiss
property, we hereby propose a novel self-similarity-driven segmentation network
(ssimnet) for unsupervised nuclei segmentation. as shown in fig. 2, instead of
designing complex discriminative network architectures, our framework derives
knowledge from the iiss property to aid the segmentation. specifically, we
obtain candidate nuclei with some unsupervised image processing. for the
obtained candidates, it is common that adjacent nuclei merged into one candidate
due to imperfect staining and low image quality, which violate the iiss
property. hence, we filter the candidates based on a custom-designed index that
roughly measures if a candidate contains multiple nuclei. the remaining
candidates are used as pseudo labels, which we use to train a u-net (aka
ssimnet) to discover the hierarchical features that distinguish nuclei pixels
from the background. finally, we apply the learned ssimnet to produce the final
nuclei segmentation.to validate the effectiveness of our method, we conduct
extensive experiments on the monuseg dataset [16,17] based on ten existing
unsupervised segmentation methods [9,[11][12][13][14][15][18][19][20]. our
method outperforms all comparison methods with an average dice score of 0.792
and aggregated jaccard index of 0.498 on the monuseg dataset which is close to
the supervised method.
as shown in fig. 2, our ssimnet aims at unsupervised segmentation of nuclei from
histology images. specifically, by using a matrix factorization on hematoxylin
and eosin (h&e) stained histology images, we get the hematoxylin channel image
for clustering, active contour refining and softening to generate the final soft
candidate label. then according to the designed unsupervised evaluation metric
driven from the iiss property, an ssimnet is trained with highlyrated soft
pseudo labels and corresponding original patches. last, while testing on the
test image, to adapt the network to learn nucleus similarity within the same
image, we fine tune the network with soft pseudo labels of some patches in
current test images. in the following, we elaborate on each part in detail.
channel decomposition. suppose that we are given a training set i s = {i s i } n
i=1 of histopathology images without any manual annotation. for each image,
stained tissue colors are results from light attenuation, which depends on the
type and amount of dyes that the tissues have absorbed. this property is
prescribed by the beer-lambert law:where i ∈ r 3×n represents the histology
image with three color channels and n pixels, i 0 is the illuminating light
intensity of sample with i 0 = 255 for 8-bit images in our cases, w ∈ r 3×r is
the stain color matrix that encodes the color appearance of each stain with r
representing the number of stains, and h ∈ r r×n is the stain density map. in
this work, we follow the sparse non-negative matrix factorization in [21] to get
the stain color matrices w = {w i } n i=1 and stain density maps h = {h i } n
i=1 for i s . note that usually histopathology images are stained with h&e and
nuclei mainly absorb hematoxylin [22]; therefore, r = 2. we reconstruct the
nuclei stain map with the first channel of w and h:clustering and active
contour. we transform i t into cielab color space and invoke the fuzzy c-means
method (fcm) with 2 clusters to obtain the candidate foreground pixels. to
reduce the noise in clustering results, we use active contour method as a
smoothing operation to get hard candidate labels:label smoothing. since hard
label is overconfident at the border of nuclei, which is detrimental to the
training of the network, we soften the hard label one by one for each connected
component in p i using the following formulation:where 4), we obtain our soft
candidate labels p from p.
so far, soft candidate labels pi have been acquired for each image i s i .
however, it is common that adjacent nuclei are merged into one candidate due to
imperfect staining and imaging conditions, which violate the iiss property. to
this, we conduct data purification to build a reliable training set for
subsequent learning.data purification. we sample k patches with overlap from
original image i s i . the sampled results are expressed as patch tissuewe
design the unsupervised shape measure index (usmi) and calculate it using the
algorithm in fig. 3(left). based on thresholding the usmi, we obtain pairs (y i
, u i ) n •k i=1 . note that the smaller usmi is, the more the pseudo label
conforms to prior knowledge. sorting these pairs by usmi from the smallest to
largest, only maintain the first α%(0 < α < 100) of data pairs as). figure
3(right) shows a separation of candidates into two groups (yellow and blue) with
a typical yellow patch containing merger nuclei and a blue patch containing
isolated nuclei.to further separate possible adjacent nuclei in a blue patch, we
follow [23] to construct the voronoi label as in fig. 2 by setting the center of
connected component as 1, constructing voronoi diagram, setting voronoi edge as
0, and ignoring other pixels. then, a voronoi tri-label set z can be
acquired.ssimnet learning and finetuning. by denoting our segmentation network
as f , our final loss function to supervise the network training can be
formulated as:where l bce is the binary cross-entropy loss and l ce is the
cross-entropy loss. also, we can obtain tissue patches and corresponding pseudo
labels for each image in the test set termed asbefore evaluation, we first fine
tune our network f using set k for several epochs. as shown in the ablation
study, this operation is simple but effective. and this fine tuning process can
help the network capture the size and shape information in the current test
slice.
monuseg. multi-organ nuclei segmentation [16,17] (monuseg) is used to evaluated
our ssimnet. the monuseg dataset consists of 44 h&e stained histopathology
images with 28,846 manually annotated nuclei. with 1000 × 1000 pixel resolution,
these images were extracted from whole slide images from the the cancer genome
atlas (tcga) repository, representing 9 different organs from 44
individuals.cpm17. the cpm17 dataset [24] is also derived from tcga repository.
the training and test set each consisted of 32 images tiles selected and
extracted from a set of non-small cell lung cancer (nsclc), head and neck
squamous cell carcinoma (hnscc), glioblastoma multiforme (gbm) and lower grade
glioma (lgg) tissue images. moreover, each type cancer has 8 tiles and the size
of patch is 500 × 500 or 600 × 600.settings. we compare our ssimnet with several
current unsupervised segmentation methods. we follow the dcgn [15] to conduct
comparison experiments. we crop the image indataset into patches of 256 × 256
pixels for training. all the methods were trained for 150 epochs on monuseg and
200 epochs on cpm17 each time and experimented with an initial learning rate of
5e -5 and a decay of 0.98 per epoch. our experiment repeated ten times on
monuseg dataset and only once on cpm17 dataset for an augmented convenience.
specially for our ssimnet training, we set α = 70% for data purification and λ =
0.9 for loss in training. moreover, we fine tune the network with only five
epochs for each image on test set with optimizer parameter saved in checkpoint.
to evaluate the effectiveness of ssimnet, we compare it with several deep
learning based and conventional unsupervised segmentation methods on the
mentioned datasets, including minibatch k-means (termed as mkmeans), gaussian
mixture model [9] (termed as gmm), invariant information clustering [12] (termed
as iic), double dip [18], deep clustering via adaptive gmm model [19] (termed as
dcagmm), deep image clustering [13] (termed as dic), kim's work [20], kanezaki's
work [11], deep conditional gmm [14] (termed as dcgmm), and deep constrained
gaussian network [15] (termed as dcgn).for the methods without public codes, we
report the results from the original publications for a fair comparison. the
results are shown in table 1.as table 1 shows, firstly, our ssimnet outperforms
all other unsupervised model and performs even close to fully supervised u-net
under the metrics of dice coefficient and aggregated jaccard index (aji).
secondly, while the recall of all comparison methods is higher than precision,
our ssimnet's recall (0.772) is lower than precision (0.820) and also lower than
the state-of-the-art method's recall (0.834). the reason lies in that our method
considers mining as strong prior knowledge from tissue slice itself, which
renders a tighter constraint on our model, leading the model to predict a lower
confidence in the easilyconfused region. moreover, figure 4 shows the
visualization of two test slice. it also conforms the effectiveness of our
method on eliminating the model confusion in the region between adjacent nuclei
and the ability in capturing nuclei shape.besides, we conduct an additional
comparison experiment based on cpm17 dataset to demonstrate the generalization
of our method. as shown in table 2, our method again achieves the top
performances. moreover, as the image size of cpm17 is smaller than that of
monuseg, the performance gain is not as big as on the monuseg dataset.
we perform ablation studies by disabling each component to the ssimnet framework
to evaluate their effectiveness. as shown in table 3, each component in our
ssimnet can bring different degrees of improvement, which shows that all of the
label softening, data purification and finetuning process are significant parts
of our ssimnet and play an indispensable role in achieving superior performance.
in this paper, we propose an ssimnet framework for label-free nuclei
segmentation. motivated by the intra-image self similarity (iiss) property,
which characterize the histology images and nuclei, we design a series of
operations to capture the prior knowledge and generate pseudo labels as
supervision signal, which is used to learn the ssimnet for final nuclei
segmentation. the iiss property renders us a tighter prior constraint for better
model building compared to other unsupervised nuclei segmentation. comprehensive
experimental results demonstrate that ssimnet achieves the best performances on
the benchmark monuseg and cpm17 datasets, outperforming other unsupervised
segmentation methods.
the tumor microenvironment (tme) is comprised of cancer, immune (e.g. b
lymphocytes, and t lymphocytes), stromal, and other cells together with
noncellular tissue components [3,5,24,30]. it is well acknowledged that tumors
evolve in close interaction with their microenvironment. quantitatively
characterizing tme has the potential to predict tumor aggressiveness and
treatment response [3,23,24,30]. different types of lymphocytes such as cd4+
(helper t cells), cd8+ (cytotoxic t cells), cd20+ (b cells), within the tme
naturally interact with tumor and stromal cells. studies [5,9] have shown that
quantifying spatial interplay of these different cell families within the tme
can provide more prognostic/predictive value compared to only measuring the
density of a single biomarker such as tumor-infiltrating lymphocytes (tils)
[3,24]. immunotherapy (io) is the standard treatment for patients with advanced
non-small cell lung cancer (nsclc) [19] but only 27-45% of patients respond to
this treatment [21]. therefore, better algorithms and improved biomarkers are
essential for identifying which cancer patients are most likely to respond to io
in advance of treatment. quantitative features that relate to the complex
spatial interplay between different types of b-and t-cells in the tme might
unlock attributes that are associated with io response. in this study, we
introduce a novel approach called triangular analysis of geographical interplay
of lymphocytes (triangil), representing a unique and interpretable way to
characterize the distribution, and higher-order interaction of various cell
families (e.g., cancerous cells, stromal cells, lymphocyte subtypes) across
digital histopathology slides. we demonstrate the efficacy of triaangil for
characterizing tme in the context of predicting 1) response to io with immune
checkpoint inhibitors (ici), 2) overall survival (os), in patients with nsclc,
and 3) providing novel insights into the spatial interplay between different
immune cell subtype. triangil source code is publicly available at
http://github.com/sarayar/triangil.
many studies have only looked at the density of a single biomarker (e.g. tils),
to show that a high density of tils is associated with improved patient survival
and treatment response in nsclc [3,24]. other works have attempted to
characterize the spatial arrangement of cells in tme using computational
graphbased approaches. these approaches include methods that connect cells
regardless of their type (1) using global graphs (gg) such as voronoi that
connect all nuclei [2,14], or (2) using cell cluster graphs (ccg) [16] to create
multiple nuclear subgraphs based on cell-to-cell proximity to predict tumor
aggressiveness and patient outcome [16]. others have explored (3) the spatial
interplay between two different cell types [5].one example approach is spatial
architecture of til (spatil) [9] which attempted to characterize the interplay
between immune and cancer cells and has proven to be helpful in predicting the
recurrence in early-stage nsclc. all of these approaches point to overwhelming
evidence that spatial architecture of cells in tme is critical in predicting
cancer outcome. however, these approaches have not been able to exploit
higher-order interactions and dependencies between multiple cell types (> 2),
relationships that might provide additional actionable insights. the
contributions of this work include:(1) triangil is a computational framework
that characterizes the architecture and relationships of different cell types
simultaneously. instead of measuring only simple two-by-two relations between
cells, it seeks to identify triadic spatial relations (hyperedges [18,20] have
shown great capabilities in solving complex problems in the biomedical field,
these tend to be black-box in nature. a key consideration in cancer immunology
is the need for actionable insights into the spatial relationships between
different types of immune cells. not only does triangil provide predictions that
are on par or superior compared to dl approaches, but also provides a way to
glean insights into the spatial interplay of different immune cell types. these
complex interactions enhance our understanding of the tme and will help pave the
way for new therapeutic strategies that leverage these insights.
our approach consists of constructing heterogeneous graphs step by step and
quantifying them by extracting features from them. the graphs are defined by g =
(v, e), where v is the set of vertices (nodes) v = {v 1 , ...v n } with τ n
vertex types, and e is the collection of pairs of vertices from v, e = {e 1 ,
...e m }, which are called edges and φ n is the mapping function that maps every
vertex to one of n differential marker expressions in this dataset φ n : v → τ n
. g is represented by an adjacency matrix a that allows one to determine edges
in constant time.
the inputs of triangil are the coordinates of nuclear centroids and the
corresponding cell types. in the triangil procedure, the centroid of each
nucleus in a family is represented as a node of a graph. triangil is agnostic of
the method used for identifying the coordinates and types. once the different
cell families are identified (fig. 1-b), a list is generated for all possible
sets comprising of membership from three [12,18] different cell families. by
focusing on every set, triangil allows for capturing higher-order and balanced
spatial triadic relationships [4] between cell families, while keeping the
computational complexity relatively low. therefore, we initiate the process with
the first set on the list (α, β, γ), build heterogeneous graphs, extract
features, and then select another set until we have explored all possible triads
of cell families on the list. the three main steps of triangil is as follows:1)
quantifying in absence of one family: first, we build a proximity graph (g 1 )
on nodes of α, β, γ based on the euclidean distance of every two nodes. two
nodes will be connected if their distance is shorter than a given "interaction
rate", θ regardless of their family and cell type (fig. 1-c, 1-c1). the
interaction rate is a hyper-parameter that controls how close we expect the
distance to be so that we consider some interaction between the nodes.we then
exclude the nodes of α from all the interactions by removing its edges from g1
and characterize the relationship of β and γ (fig. 1-c2). next, we extract a
series of features including clustering coefficient, average degree from the
resulting subgraph. we repeat this process by removing all the edges of β (fig.
1-c3) and then γ (fig. 1-c4). in this manner, a total of 126 features
(supplemental table 1) are extracted (42 features for absence of one family ×3).
2) triangulation-based connections: a delaunay triangulation is constructed by
the nodes of α, β, γ (fig. 1-d, and 1-d1). delaunay triangulation is a planar
graph formed by connecting the vertices in a way that ensures no point lies
within the circumcircle of any triangle formed by the vertices [7].we then
extract 10 features relating to edge length and vertex count. next, we prune
long edges (d 1 ) where the euclidean distance between connected nodes is more
than the "interaction rate" (fig. 1-d2). next, a series of features were
extracted from the remaining subgraph (e.g. number of edges between the nodes of
α and β, β and γ, α and γ; complete list of features in supplemental table 1).3)
triangular interactions: as illustrated in algorithm 1, from the unpruned
delaunay triangulation that includes the nodes of α, β, γ, we select those
triangles (closed triads [8,25]) that link nodes from three distinct families
(fig. 1-e, 1-e1). in other words, we remove triangles with more than one vertex
from a single family. next, we call gettrianglefeatures() function to quantify
triangular relationships by extracting features from the resulting subgraphs
(e.g. perimeter and area of triangles; complete list of features in supplemental
table 1).
the cohort employed in this study was composed of pre-treatment tumor biopsy
specimens from patients with nsclc from five centers (two centers for training
(s t ) and three centers for independent validation (s v )). the entire analysis
was carried out using 122 patients in experiment 1 (73 in s t , and 49 in s v )
and 135 patients in experiment 2 (81 in s t , and 54 in s v ). specimens were
analyzed with a multiplexed quantitative immunofluorescence (qif) panel using
the method described in [22]. from each whole slide image, 7 representative
tiles were obtained and used to train the software inform to define background,
tumor and stromal compartments. then, individual cells were segmented based on
nuclear dapi staining and the segmentation performance was controlled by direct
visualization of samples by a trained observer. next, the software was trained
to identify cell subtypes based on marker expression (cd8, cd4, cd20, ck for
tumor epithelial cells and absence of these markers for stromal cells).
the efficacy of triangil was compared against five different approaches.
for every patient, multiple density measures including the number of different
cells types and their ratios are calculated [3,24] (supplemental table 2).
tree were constructed [2,14] on all nuclei regardless of their type.
architectural features (e.g., perimeter, triangle area, edge length) were then
calculated on these global graphs for each patient.
for every patient, subgraphs are built on nuclei regardless of their type and
only based on their euclidean distance. local graph metrics (e.g. clustering
coefficient) [16] are then calculated from these subgraphs. spatil: for each
patient, first, subgraphs are built on individual cell types based on a distance
parameter. the convex hulls are then constructed on these subgraphs. after
selecting every two cell types, features are extracted from their convex hulls
(e.g. the number of clusters of each cell type, area intersected between
clusters [9]; complete list of combinations in supplemental table 3).
a recent study [31] demonstrated that transformer-based [29] gnns are able to
learn the arrangement of tiles across pathology images for survival analysis.
here, for each tile in the slide, a delaunay graph was constructed regardless of
cell subtypes, and tile-level feature representations (e.g.side length minimum,
maximum, mean, and standard deviation, triangle area minimum, maximum, mean, and
standard deviation) were aggregated by a transformer according to their spatial
arrangement [31]. our approach utilized the weisfeiler-lehman (wl) test [15] for
embedding graphs into euclidean feature space. well-known approaches, such as
graphsage [10], are considered as continuous approximations to the wl test.
therefore, our gnn is a valid baseline for heterogeneous graphs.
design: triangil was also trained to differentiate between patients who
responded to io and those who did not. for our study, the responders to io were
identified as those patients with complete response, partial response, and
stable disease, and non-responders were patients with progressive disease. a
linear discriminant analysis (lda) classifier was trained on s t to predict
which patients would respond to io. for creating the model, the minimum
redundancy maximum relevance (mrmr) method [1] was used to select the top
features. the same procedure using mrmr and lda was performed for the
comparative hand-crafted approaches. the ability to identify responders post-io
was assessed by the area under the receiver operating characteristic curve (auc)
in s v .
the two top predictive triangil features were found to be the number of edges
between stroma and cd4+ cells, and the number of edges between stroma and tumor
cells with more interactions between stromal cells and both cd4+ and tumor cells
being associated with response to io. this finding is concordant with other
studies [13,17,22,27] that stromal tils were significantly associated with
improved os. therefore, triangil approach is not only predictive of treatment
response but more critically it enables biological interpretations that a dl
model might not be able to provide. in s v , this lda classifier was able to
distinguish responders from non-responders to io with au c t ri =0. design: s t
was used to construct a least absolute shrinkage and selection operator (lasso)
[28] regularized cox proportional hazards model [6] using the triangil features,
to obtain risk score for each patient. lasso features are listed in supplemental
table 4. the median risk score in s t was used as a threshold in both s t and s
v to dichotomize patients into low-risk/high-risk categories. kaplan-meier (km)
survival curves [26] were plotted and the model performance was summarized by
hazard ratio (hr), with corresponding (95% confidence intervals (ci)) using the
log-rank test, and harrell's concordance index (c-index) on s v . the c-index
evaluates the correlation between risk predictions and survival times, aiming to
maximize the discrimination between high-risk and low-risk patients [11]. os is
the time between the initiation of io to the death of the patient. the patients
were censored if the date of death was unknown.result: figure 2 presents some
triangil features in a field of view for a patient with long-term survival and
another with short-term survival. more triangular relationships, shorter
triangle edges, and smaller triangles with smaller perimeters are found in the
long-term survival case when analyzing the triadic interactions within
tumor-stroma-cd4, thereby suggesting higher relative presence and closer
interaction of these cell families. figure 3 illustrates the km plots for the
six approaches. we also calculated the concordance index (c-index) for the two
prognostic approaches in s v . the c-index for triangil and gnn methods were
0.64, and 0.63 respectively. therefore, overall triangil worked marginally
better than gnn, with much higher biological interpretability.
we presented a new approach, triangular analysis of geographical interplay of
lymphocytes (triangil), to quantitatively chartacterize the spatial arrangement
and relative geographical interplay of multiple cell families across
pathological images. compared to previous spatial graph-based methods, triangil
quantifies the spatial interplay between multiple cell families, providing a
more comprehensive portrait of the tumor microenvironment. triangil was
predictive of response after io (n = 122) and also demonstrated a strong
correlation with os in nsclc patients treated with io (n = 135). triangil
outperformed other graph-and dl-based approaches, with the added benefit of
provoding interpretability with regard to the spatial interplay between cell
families. for instance, triangil yielded the insight that more interactions
between stromal cells and both cd4+ and tumor cells appears to be associated
with better response to io. although five cell families were studies in this
work, triangil is flexible and could include other cell types (e.g.,
macrophages). future work will entail larger validation studies and also
evaluation on other use cases.
histopathology relies on hematoxylin and eosin (h&e) stained biopsies for
microscopic inspection to identify visual evidence of diseases. hematoxylin has
a deep blue-purple color and stains acidic structures such as dna in cell
nuclei. eosin, alternatively, is red-pink and stains nonspecific proteins in the
cytoplasm and the stromal matrix. pathologists then examine highlighted tissue
characteristics to diagnose diseases, including different cancers. a correct
diagnosis, therefore, is dependent on the pathologist's training and prior
exposure to a wide variety of disease subtypes [30]. this presents a challenge,
as some disease variants are extremely rare, making visual identification
difficult. in recent years, deep learning methods have aimed to alleviate this
problem by designing discriminative frameworks that aid diagnosis [15,28].
segmentation models find applications in spatial identification of different
nuclei types [6]. however, generative modeling in histopathology is relatively
unexplored. generative models can be used to generate histopathology images with
specific characteristics, such as visual patterns identifying rare cancer
subtypes [4]. as such, generative models can be sampled to emphasize each
disease subtype equally and generate more balanced datasets, thus preventing
dataset biases getting amplified by the models [7]. generative models have the
potential to improve the pedagogy, trustworthiness, generalization, and coverage
of disease diagnosis in the field of histology by aiding both deep learning
models and human pathologists. synthetic datasets can also tackle privacy
concerns surrounding medical data sharing. additionally, conditional generation
of annotated data adds even further value to the proposition as labeling medical
images involves tremendous time, labor, and training costs. recently, denoising
diffusion probabilistic models (ddpms) [8] have achieved tremendous success in
conditional and unconditional generation of real-world images [3]. further, the
semantic diffusion model (sdm) demonstrated the use of ddpms for generating
images given semantic layout [27]. in this work, (1) we leverage recently
discovered capabilities of ddpms to design a first-of-its-kind nuclei-aware
semantic diffusion model (nasdm) that can generate realistic tissue patches
given a semantic mask comprising of multiple nuclei types, (2) we train our
framework on the lizard dataset [5] consisting of colon histology images and
achieve state-of-the-art generation capabilities, and (3) we perform extensive
ablative, qualitative, and quantitative analyses to establish the proficiency of
our framework on this tissue generation task.
deep learning based generative models for histopathology images have seen
tremendous progress in recent years due to advances in digital pathology,
compute power, and neural network architectures. several gan-based generative
models have been proposed to generate histology patches [16,31,33]. however,
gans suffer from problems of frequent mode collapse and overfitting their
discriminator [29]. it is also challenging to capture long-tailed distributions
and synthesize rare samples from imbalanced datasets using gans. more recently,
denoising diffusion models have been shown to generate highly compelling images
by incrementally adding information to noise [8]. success of diffusion models in
generating realistic images led to various conditional [12,21,22] and
unconditional [3,9,19] diffusion models that generate realistic samples with
high fidelity. following this, a morphology-focused diffusion model has been
presented for generating tissue patches based on genotype [18]. semantic image
synthesis is a task involving generating diverse realistic images from semantic
layouts. gan-based semantic image synthesis works [20,24,25] generally struggled
at generating high quality and enforcing semantic correspondence at the same
time. to this end, a semantic diffusion model has been proposed that uses
conditional denoising diffusion probabilistic model and achieves both better
fidelity and diversity [27]. we use this progress in the field of conditional
diffusion models and semantic image synthesis to formulate our nasdm framework.
in this paper, we describe our framework for generating tissue patches
conditioned on semantic layouts of nuclei. given a nuclei segmentation mask, we
intend to generate realistic synthetic patches. in this section, we (1) describe
our data preparation, (2) detail our stain-normalization strategy, (3) review
conditional denoising diffusion probabilistic models, (4) outline the network
architecture used to condition on semantic label map, and (5) highlight the
classifier-free guidance mechanism that we employ at sampling time.
we use the lizard dataset [5] to demonstrate our framework. this dataset
consists of histology image regions of colon tissue from six different data
sources at 20× objective magnification. the images are accompanied by full
segmentation annotation for different types of nuclei, namely, epithelial cells,
connective tissue cells, lymphocytes, plasma cells, neutrophils, and
eosinophils. a generative model trained on this dataset can be used to
effectively synthesize the colonic tumor micro-environments. the dataset
contains 238 image regions, with an average size of 1055 × 934 pixels. as there
are substantial visual variations across images, we construct a representative
test set by randomly sampling a 7.5% area from each image and its corresponding
mask to be held-out for testing. the test and train image regions are further
divided into smaller image patches of 128 × 128 pixels at two different
objective magnifications: (1) at 20×, the images are directly split into 128 ×
128 pixels patches, whereas (2) at 10×, we generate 256 × 256 patches and resize
them to 128 × 128 for training. to use the data exhaustively, patching is
performed with a 50% overlap in neighboring patches. as such, at (1) 20× we
extract a total of 54,735 patches for training and 4,991 patches as a held-out
set, while at (2) 20× magnification we generate 12,409 training patches and 655
patches are held out.
a common issue in deep learning with h&e stained histopathology slides is the
visual bias introduced by variations in the staining protocol and the raw
materials of chemicals leading to different colors across slides prepared at
different labs [1]. as such, several stain-normalization methods have been
proposed to tackle this issue by normalizing all the tissue samples to mimic the
stain distribution of a given target slide [17,23,26]. in this work, we use the
structure preserving color normalization scheme introduce by vahadane et al.
[26] to transform all the slides to match the stain distribution of an
empirically chosen slide from the training dataset.
in this section, we describe the theory of conditional denoising diffusion
probabilistic models, which serves as the backbone of our framework. a
conditional diffusion model aims to maximize the likelihood p θ (x 0 | y), where
data x 0 is sampled from the conditional data distribution, x 0 ∼ q(x 0 | y),
and y represents the conditioning signal. a diffusion model consists of two
intrinsic processes. the forward process is defined as a markov chain, where
gaussian noise is gradually added to the data over t timesteps aswhere {β} t=1:t
are constants defined based on the noise schedule. an interesting property of
the gaussian forward process is that we can sample x t directly from x 0 in
closed form. now, the reverse process, p θ (x 0:t | y), is defined as a markov
chain with learned gaussian transitions starting from pure noise, p(x t ) ∼ n(0,
i), and is parameterized as a neural network with parameters θ as(hence, for
each denoising step from t to t -1,it has been shown that the combination of q
and p here is a form of a variational auto-encoder [13], and hence the
variational lower bound (vlb) can be described as a sum of independent terms, l
vlb := l 0 + ... + l t -1 + l t , where each term corresponds to a noising step.
as described in ho et al. [8], we can randomly sample timestep t during training
and use the expectation e t,x0,y, to estimate l vlb and optimize parameters θ.
the denoising neural network can be parameterized in several ways, however, it
has been observed that using a noiseprediction based formulation results in the
best image quality [8]. overall, our nasdm denoising model is trained to
predicting the noise added to the input image given the semantic layout y and
the timestep t using the loss described as follows:note that the above loss
function provides no signal for training σ θ (x t , y, t). therefore, following
the strategy in improved ddpms [8], we train a network to directly predict an
interpolation coefficient v per dimension, which is turned into variances and
optimized directly using the kl divergence between the estimated distribution p
θ (x t-1 | x t , y) and the diffusion posterior q(x t-1 | x t , x 0 ) as). this
optimization is done while applying a stop gradient to (x t , y, t) such that l
vlb can guide σ θ (x t , y, t) and l simple is the main guidance for (x t , y,
t). overall, the loss is a weighted summation of the two objectives described
above as follows:(5)
nasdm requires our neural network noise-predictor θ (x t , y, t) to effectively
process the information from the nuclei semantic map. for this purpose, we
leverage a modified u-net architecture described in wang et al. [27], where
semantic information is injected into the decoder of the denoising network using
multi-layer, spatially-adaptive normalization operators. as denoted in fig. 1,
we construct the semantic mask such that each channel of the mask corresponds to
a unique nuclei type. in addition, we also concatenate a mask comprising of the
edges of all nuclei to further demarcate nuclei instances.
to improve the sample quality and agreement with the conditioning signal, we
employ classifier-free guidance [10], which essentially amplifies the
conditional distribution using unconditional outputs while sampling. during
training, the conditioning signal, i.e., the semantic label map, is randomly
replaced with a null mask for a certain percentage of samples. this leads to the
diffusion model becoming stronger at generating samples both conditionally as
well as unconditionally and can be used to implicitly infer the gradients of the
log probability required for guidance as follows: where ∅ denotes an empty
semantic mask. during sampling, the conditional distribution is amplified using
a guidance scale s as follows:
in this section, we first describe our implementation details and training
procedure. further, we establish the robustness of our model by performing an
ablative study over objective magnification and classifier-guidance scale. we
then perform quantitative and qualitative assessments to demonstrate the
efficacy of our nuclei-aware semantic histopathology generation model. in all
following experiments, we synthesize images using the semantic masks of the
held-out dataset at the concerned objective magnification. we then compute
fréchet inception distance (fid) and inception score (is) metrics between the
synthetic and real images in the held-out set.
our diffusion model is implemented using a semantic unet architecture (sect.
3.4), trained using the objective in (5). following previous works [19], we set
the trade-off parameter λ as 0.001. we use the adamw optimizer to train our
model. additionally, we adopt an exponential moving average (ema) of the
denoising network weights with 0.999 decay. following ddpm [8], we set the total
number of diffusion steps as 1000 and use a linear noising schedule with respect
to timestep t for the forward process. after normal training with a learning
rate of 1e -4, we decay the learning rate to 2e -5 to further finetune the model
with a drop rate of 0.2 to enhance the classifier-free guidance capability
during sampling. the whole framework is implemented using pytorch and trained on
4 nvidia tesla a100 gpus with a batch-size of 40 per gpu. code will be made
public on publication or request.
in this study, we test the effectiveness of the classifier-free guidance
strategy. we consider the variant without guidance as our baseline. as seen in
fig. 2, increase in guidance scale initially results in better image quality as
more detail is added to visual structures of nuclei. however, with further
increase, the image quality degrades as the model overemphasizes the nuclei and
staining textures. as described in sect. 3.1, we generate patches at two
different objective magnifications of 10× and 20×. in this section, we contrast
the generative performance of the models trained on these magnification levels
respectively. from the table on right, we observe that the model trained at 20×
objective magnification produces better generative metrics. note that we only
train on a subset on 20× mag. to keep the size of the training data constant.
to the best of our knowledge, ours is the only work that is able to synthesize
histology images given a semantic mask, making a direct quantitative comparison
tricky. however, the standard generative metric fréchet inception distance (fid)
measures the distance between distributions of generated and real images in the
inception-v3 [14] latent space, where a lower fid indicates that the model is
able to generate images that are very similar to real data. therefore, we
compare fid and is metrics with the values reported in existing works [18,32]
(ref.table 1) in their own settings. we can observe that our method outperforms
all existing methods including both gans-based methods as well as the recently
proposed morphology-focused generative diffusion model.
we perform an expert pathologist review of the patches generated by the model.
we use 30 patches, 17 synthetic and 13 real for this review. we have two experts
assess the overall medical quality of the patches as well as their consistency
with the associated nuclei masks on likert scale. the survey used for the review
can be found on a public google survey1 . it can be seen from this survey (fig.
4) that the patches generated by the model are found to be more realistic than
even the patches in our real set. we now qualitatively discuss the proficiency
of our model in generating realistic visual patterns in synthetic histopathology
images (refer fig. 3). we can see that the model is able to capture convincing
visual structure for each type of nuclei. in the synthetic images, we can see
that the lymphocytes are accurately circular, while neutrophils and eosinophils
have a more lobed structure. we also observe that the model is able to mimic
correct nucleus-to-cytoplasm ratios for each type of nuclei. epithelial cells
are less dense, have a distinct chromatin structure, and are larger compared to
other white blood cells. epithelial cells are most difficult to generate in a
convincing manner, however, we can see that model is able to capture the nuances
well and generates accurate chromatin distributions.
in this work, we present nasdm, a nuclei-aware semantic tissue generation
framework. we demonstrate the model on a colon dataset and qualitatively fig. 4.
qualitative review: compiled results from a pathologist review. we have experts
assess patches for, their overall medical quality (left), as well as, their
consistency with the associated mask (right). we observe that the patches
generated by the model do better on all metrics and majority are imperceptible
from real patches.and quantitatively establish the proficiency of the framework
at this task. in future works, further conditioning on properties like
stain-distribution, tissuetype, disease-type, etc. would enable patch generation
in varied histopathological settings. additionally, this framework can be
extended to also generate semantic masks enabling an end-to-end tissue
generation framework that first generates a mask and then synthesizes the
corresponding patch. further, future works can explore generation of patches
conditioned on neighboring patches, as this enables generation of larger tissue
areas by composing patches together.
radiotherapy, one of the mainstream treatments for cancer patients, has gained
notable advancements in past decades. for promising curative effect, a
high-quality radiotherapy plan is demanded to distribute sufficient dose of
radiation to the planning target volume (ptv) while minimizing the radiation
hazard to organs at risk (oars). to achieve this, radiotherapy plans need to be
manually adjusted by the dosimetrists in a trial-and-error manner, which is
extremely labor-intensive and time-consuming [1,2]. additionally, the quality of
treatment plans might be variable among radiologists due to their different
expertise and experience [3]. consequently, it is essential to develop a robust
methodology to automatically predict the dose distribution for cancer patients,
relieving the burden on dosimetrists and accelerating the radiotherapy
procedure.recently, the blossom of deep learning (dl) has promoted the automatic
medical image processing tasks [4][5][6], especially for dose prediction
[7][8][9][10][11][12][13][14]. for example, nguyen et al. [7] modified the
traditional 2d unet [15] to predict the dose of prostate cancer patients. wang
et al. [10] utilized a progressive refinement unet (prunet) to refine the
predictions from low resolution to high resolution. besides the above unetbased
frameworks, song et al. [11] employed the deeplabv3+ [16] to excavate contextual
information from different scales, thus obtaining accuracy improvements in the
dose prediction of rectum cancer. mahmood et al. [12] utilized a generative
adversarial network (gan)-based method to predict the dose maps of oropharyngeal
cancer. furthermore, zhan et al. [13] designed a multi-organ constraint loss to
enforce the deep model to better consider the dose requirements of different
organs. following the idea of multi-task learning, tan et al. [8] utilized
isodose line and gradient information to promote the performance of dose
prediction of rectum cancer. to ease the burden on the delineation of ptv and
oars, li et al. [17] constructed an additional segmentation task to provide the
dose prediction task with essential anatomical knowledge.although the above
methods have achieved good performance in predicting dose distribution, they
suffer from the over-smoothing problem. these dl-based dose prediction methods
always apply the l 1 or l 2 loss to guide the model optimization which
calculates a posterior mean of the joint distribution between the predictions
and the ground truth [17,18], leading to the over-smoothed predicted images
without important high-frequency details [19]. we display predicted dose maps
from multiple deep models in fig. 1. as shown, compared with the ground truth,
i.e., (5) in fig. 1, the predictions from (1) to (3) are blurred with fewer
high-frequency details, such as ray shapes. these high-frequency features formed
by ray penetration reveal the ray directions and dose attenuation with the aim
of killing the cancer cells while protecting the oars as much as possible, which
are critical for radiotherapy. consequently, exploring an automatic method to
generate high-quality predictions with rich high-frequency information is
important to improve the performance of dose prediction. currently, diffusion
model [20] has verified its remarkable potential in modeling complex image
distributions in some vision tasks [21][22][23]. unlike other dl models, the
diffusion model is trained without any extra assumption about target data
distribution, thus evading the average effect and alleviating the over-smoothing
problem [24]. figure 1 (4) provides an example in which the diffusion-based
model predicts a dose map with shaper and clearer boundaries of ray-penetrated
areas. therefore, introducing a diffusion model to the dose prediction task is a
worthwhile endeavor.in this paper, we investigate the feasibility of applying a
diffusion model to the dose prediction task and propose a diffusion-based model,
called diffdp, to automatically predict the clinically acceptable dose
distribution for rectum cancer patients. specifically, the diffdp consists of a
forward process and a reverse process. in the forward process, the model employs
a markov chain to gradually transform dose distribution maps with complex
distribution into gaussian distribution by progressively adding pre-defined
noise. then, in the reverse process, given a pure gaussian noise, the model
gradually removes the noise in multiple steps and finally outputs the predicted
dose map. in this procedure, a noise predictor is trained to predict the noise
added in the corresponding step of the forward process. to further ensure the
accuracy of the predicted dose distribution for both the ptv and oars, we design
a dl-based structure encoder to extract the anatomical information from the ct
image and the segmentation masks of the ptv and oars. such anatomical
information can indicate the structure and relative position of organs. by
incorporating the anatomical information, the noise predictor can be aware of
the dose constraints among ptv and oars, thus distributing more appropriate dose
to them and generating more accurate dose distribution maps.overall, the
contributions of this paper can be concluded as follows: (1) we propose a novel
diffusion-based model for dose prediction in cancer radiotherapy to address the
over-smoothing issue commonly encountered in existing dl-based dose prediction
methods. to the best of our knowledge, we are the first to introduce the
diffusion model for this task. (2) we introduce a structure encoder to extract
the anatomical information available in the ct images and organ segmentation
masks, and exploit the anatomical information to guide the noise predictor in
the diffusion model towards generating more precise predictions. (3) the
proposed diffdp is extensively evaluated on a clinical dataset consisting of 130
rectum cancer patients, and the results demonstrate that our approach
outperforms other state-of-the-art methods.
an overview of the proposed diffdp model is illustrated in fig. 2, containing
two markov chain processes: a forward process and a reverse process. an image
set of cancer patient is defined as {x, y}, where x ∈ r h ×w ×(2+o) represents
the structure images, "2" signifies the ct image and the segmentation mask of
the ptv, and o denotes the total number of segmentation mask of oars. meanwhile,
y ∈ r h ×w ×1 is the corresponding dose distribution map for x. concretely, the
forward process produces a sequence of noisy images {y 0 , y 1 , . . . , y t },
y 0 = y by gradually adding a small amount of noise to y in t steps with the
noise increased at each step and a noise predictor f is constructed to predict
the noise added to y t-1 by treating y t , anatomic information from x and
embedding of step t as input. to obtain the anatomic information, a structure
encoder g is designed to extract the crucial feature representations from the
structure images. then, in the reverse process, the model progressively deduces
the dose distribution map by iteratively denoising from y t using the
well-trained noise predictor.
the framework of diffdp is designed following the denoising diffusion
probabilistic models (ddpm) [25] which contains a forward process and a reverse
process. by utilizing both processes, the diffdp model can progressively
transform the gaussian noise into complex data distribution.forward process. in
the forward process, the diffdp model employs the markov chain to progressively
add noise to the initial dose distribution map y 0 ∼ q(y 0 ) until the final
disturbed image y t becomes completely gaussian noise which is represented as y
t ∼ n (y t | 0, i ). this forward process can be formulated as:where α t is the
unlearnable standard deviation of the noise added to y t-1 . herein, the α t (t
= 1, . . . , t ) could accumulate during the forward process, which can be
treated as the noise intensity γ t = t i=1 α i . based on this, we can directly
obtain the distribution of y t at any step t from y 0 through the following
formula:where the disturbed image y t is sampled using:in which ε t ∼ n (0, i )
is random noise sampled from normal gaussian distribution.reverse process. the
reverse process also harnesses the markov chain to progressively convert the
latent variable distribution p θ (y t ) into distribution p θ (y 0 )
parameterized by θ . corresponding to the forward process, the reverse one is a
denoising transformation under the guidance of structure images x that begins
with a standard gaussian distribution y t ∼ n (y t | 0, i ). this reverse
inference process can be formulated as:where μ θ (x, y t , t) is a learned mean,
and σ t is a unlearnable standard deviation. following the idea of [16], we
parameterize the mean of μ θ as:where ε t,θ is a function approximator intended
to predict ε t from the input x, y t and γ t . consequently, the reverse
inference at two adjacent steps can be expressed as:where z t ∼ n (0, i ) is a
random noise sampled from normal gaussian distribution. more derivation
processes can be found in the original paper of diffusion model [25].
vanilla diffusion model has difficulty preserving essential structural
information and produce unstable results when predicting dose distribution maps
directly from noise with a simple condition mechanism. to address this, we
design a structure encoder g that effectively extracts the anatomical
information from the structure images guiding the noise predictor to generate
more accurate dose maps by incorporating extracted structural knowledge.
concretely, the structure encoder includes five operation steps, each with a
residual block (resblock) and a down block, except for the last one. the
resblock consists of two convolutional blocks (convblock), each containing a 3 ×
3 convolutional (conv) layer, a groupnorm (gn) layer, and a swish activation
function.the residual connections are reserved for preventing gradient
vanishment in the training.the down block includes a 3 × 3 conv layer with a
stride of 2. it takes structure image x as input, which includes the ct image
and segmentation masks of ptv and oars, and evacuates the compact feature
representation in different levels to improve the accuracy of dose prediction.
the structure encoder is pre-trained by l 1 loss and the corresponding feature
representation x e = g(x) is then fed into the noise predictor.
the purpose of the noise predictor f (x e , y t , γ t ) is to predict the noise
added on the distribution map y t with the guidance of the feature
representation x e extracted from the structure images x and current noise
intensity γ t in each step t. inspired by the great achievements of unet [15],
we employ a six-level unet to construct the noise predictor. specifically, the
encoder holds the similar architecture with the structure encoder while the
decoder comprises five deconvolution blocks to fulfill the up-sampling
operation, and each contains an up block and two resblock, except for the last
one which discards the up block. in each up block, the nearest neighbor
up-sampling and a conv layer with a kernel size of 1 are used. a bottleneck with
two resblocks and a self-attention module is embedded between the encoder and
decoder.in the encoding procedure, to guide the noise predictor with essential
anatomical structure, the feature representations respectively extracted from
the structure images x and noisy image y t are simultaneously fed into the noise
predictor. firstly, y t is encoded into feature maps through a convolutional
layer. then, these two feature maps are fused by element-wise addition, allowing
the structure information in x to be transferred to the noise predictor. the
following two down-sampling operations retain the addition operation to complete
information fusion, while the last three use a cross-attention mechanism to gain
similarity-based structure guidance at deeper levels.in the decoding procedure,
the noise predictor restores the feature representations captured by the encoder
to the final output, i.e., the noise ε t,θ = f (x e , y t , γ t ) in step t. the
skip connections between the encoder and decoder are reserved for multi-level
feature reuse and aggregation.
the main purpose of the diffdp model is to train the noise predictor f and
structure encoder g, so that the predicted noise ε t,θ = f (g(x), y t , γ t ) in
the reverse process can approximate the added noise ε t in the forward process.
to achieve this, we define the objective function as:for a clearer
understanding, the training procedure is summarized in algorithm 1.
training procedure 1: input: input image pairs where is the structure image and
is the corresponding dose distribution map, the total number of diffusion steps
2: initialize: randomly initialize the noise predictor and pre-trained structure
encoder 3: repeat 4: sample 5: sample 6: perform the gradient step on equation
(9) 7: until converged
we accomplish the proposed network in the pytorch framework. all of our
experiments are conducted through one nvidia rtx 3090 gpu with 24 gb memory and
a batch size of 16 with an adaptive moment estimation (adam) optimizer. we train
the whole model for 1500 epochs (about 1.5m training steps) where the learning
rate is initialized to 1e-4 and reset to 5e-5 after 1200 epochs. the parameter t
is set to 1000. additionally, the noise intensity is initialized to 1e-2 and
decayed to 1e-4 linearly along with the increase of steps.
dataset and evaluations. we measure the performance of our model on an in-house
rectum cancer dataset which contains 130 patients who underwent volumetric
modulated arc therapy (vmat) treatment at west china hospital. concretely, for
every patient, the ct images, ptv segmentation, oars segmentations, and the
clinically planned dose distribution are included. additionally, there are four
oars of rectum cancer containing the bladder, femoral head r, femoral head l,
and small intestine. we randomly select 98 patients for model training, 10
patients for validation, and the remaining 22 patients for test. the thickness
of the cts is 3 mm and all the images are resized to the resolution of 256 × 256
before the training procedure.we measure the performance of our proposed model
with multiple metrics. considering dm represents the minimal absorbed dose
covering m% percentage volume of ptv, we involve d 98 , d 2 , maximum dose (d
max ), and mean dose (d mean ) as metrics. besides, the heterogeneity index (hi)
is used to quantify dose heterogeneity [26]. to quantify performance more
directly, we calculate the difference ( ) of these metrics between the ground
truth and the predicted results. more intuitively, we involve the dose volume
histogram (dvh) [27] as another essential metric of dose prediction performance.
when the dvh curves of the predictions are closer to the ground truth, we can
infer higher prediction accuracy.comparison with state-of-the-art methods. to
verify the superior accuracy of our proposed model, we select multiple
state-of-the-art (sota) models in dose prediction, containing unet (2017) [7],
gan (2018) [12], deeplabv3+ (2020) [11], c3d (2021) [9], and prunet (2022) [10],
for comparison. the quantitative comparison results are listed in table . 1
where our method outperforms the existing sotas in terms of all metrics.
specifically, compared with deeplabv3+ with the second-best accuracy in hi
(0.0448) and d 98 (0.0416), the results generated by the proposed are 0.0035 and
0.0014 lower, respectively. as for d 2 and d max , our method gains overwhelming
performance with 0.0008 and 0.0005, respectively. moreover, the paired t-test is
conducted to investigate the significance of the results. the p-values between
the proposed and other sotas are almost all less than 0.05, indicating that the
enhancement of performance is statistically meaningful.besides the quantitative
results, we also present the dvh curves derived by compared methods in fig. 3.
the results are compared on ptv as well as two oars: bladder and small
intestine. compared with other methods, the disparity between the dvh curves of
our method and the ground truth is the smallest, demonstrating the superior
performance of the proposed. furthermore, we display the visualization
comparison in fig. 4. as we can see, the proposed model achieves the best visual
quality with clearer and sharper high-frequency details (as indicated by red
arrows). furthermore, the error map of the proposed is the darkest, suggesting
the least disparity compared with the ground truth.ablation study. to study the
contributions of key components of the proposed method, we conduct the ablation
experiments by 1) removing the structure encoder from the proposed method and
concatenating the anatomical images x and noisy image y t together as the
original input for diffusion model (denoted as baseline); 2) the proposed diffdp
model. the quantitative results are given in table 2. we can clearly see the
performance for all metrics is enhanced with the structure encoder,
demonstrating its effectiveness in the proposed model.
in this paper, we introduce a novel diffusion-based dose prediction (diffdp)
model for predicting the radiotherapy dose distribution of cancer patients. the
proposed method involves a forward and a reverse process to generate accurate
prediction by progressively transferring the gaussian noise into a dose
distribution map. moreover, we propose a structure encoder to extract anatomical
information from patient anatomy images and enable the model to concentrate on
the dose constraints within several essential organs. extensive experiments on
an in-house dataset with 130 rectum cancer patients demonstrate the superiority
of our method.
in the past few years, the development of histopathological whole slide image
(wsi) analysis methods has dramatically contributed to the intelligent cancer
diagnosis [4,10,15]. however, due to the limitation of hardware resources, it is
difficult to directly process gigapixel wsis in an end-to-end framework. recent
studies usually divide the wsi analysis into multiple stages.generally, multiple
instance learning (mil) is one of the most popular solutions for wsi analysis
[14,17,18]. mil methods regard wsi recognition as a weakly supervised learning
problem and focus on how to effectively and efficiently aggregate
histopathological local features into a global representation. several studies
introduced attention mechanisms [9], recurrent neural networks [2] and graph
neural network [8] to enhance the capacity of mil in structural information
mining. more recently, transformer-based structures [13,19] are proposed to
aggregate long-term relationships of tissue regions, especially for large-scale
wsis. these transformer-based models achieved state-of-the-art performance in
sub-type classification, survival prediction, gene mutant prediction, etc.
however, these methods still rely on at least patient-level annotations. in the
networkbased consultation and communication platforms, there is a vast quantity
of unlabeled wsis not effectively utilized. these wsis are usually without any
annotations or definite diagnosis descriptions but are available for
unsupervised learning. in this case, self-supervised learning (ssl) is gradually
introduced into the mil-based framework and is becoming a new paradigm for wsi
analysis [1,11,16]. typically, chen et al. [5] explored and posed a new
challenge referred to as slide-level self-learning and proposed hipt, which
leveraged the hierarchical structure inherent in wsis and constructed multiple
levels of the self-supervised learning framework to learn high-resolution image
representations. this approach enables mil-based frameworks to take advantage of
abundant unlabeled wsis, further improving the accuracy and robustness of tumor
recognition.however, hipt is a hierarchical learning framework based on a greedy
training strategy. the bias and error generated in each level of the
representation model will accumulate in the final decision model. moreover, the
vit [6] backbone used in hipt is originally designed for nature sense images in
fixed sizes whose positional information is consistent. however,
histopathological wsis are scale-varying and isotropic. the positional embedding
strategy of vit will bring ambiguity into the structural modeling. to relieve
this problem, kat [19] built hierarchical masks based on local anchors to
maintain multi-scale relative distance information in the training. but these
masks are manually defined which is not trainable and lacked orientation
information. the current embedding strategy for wsi structural description is
not complete.in this paper, we propose a novel whole slide image representation
learning framework named position-aware masked autoencoder (pama), which
achieves slide-level representation learning by reconstructing the local
representations of the wsi in the patch feature space. pama can be trained
end-to-end from the local features to the wsi-level representation. moreover, we
designed a position-aware cross-attention mechanism to guarantee the correlation
of localto-global information in the wsis while saving computational resources.
the proposed approach was evaluated on a public tcga-lung dataset and an
in-house endometrial dataset and compared with 6 state-of-the-art methods. the
results have demonstrated the effectiveness of the proposed method.the
contribution of this paper can be summarized into three aspects. (1) we propose
a novel whole slide image representation learning framework named position-aware
masked autoencoder (pama). pama can make full use of abundant unlabeled wsis to
learn discriminative wsi representations. (2) we propose a position-aware
cross-attention (paca) module with a kernel reorientation (kro) strategy, which
makes the framework able to maintain the spatial integrity and semantic
enrichment of slide representation during the selfsupervised training. (3) the
experiments on two datasets show our pama can achieve competitive performance
compared with sota mil methods and ssl methods.
mae [7] is a successful ssl framework that learns image presentations by
reconstructing the masked image in the original pixel space. we introduced this
paradigm to wsi-level representation learning. the flowchart of the proposed
work is illustrated in fig. 1. first, we divided wsis into non-overlapping image
patches and meanwhile removed the background without tissue regions based on a
threshold (as shown in fig. 1(i)). then, we applied the self-supervised learning
framework dino [3] for patch feature learning and extraction. afterward, the
features for a wsi are represented as x ∈ r np×d f , where d f is the dimension
of the feature and n p is the number of patches in the wsi. inspired by kat
[19], we extracted multiple anchors by clustering the location coordinates of
patches for the auxiliary description of the wsi structure. we assigned
trainable representations for these anchors, which are formulated as k ∈ r n k
×d f , where n k is the number of anchors in the wsi. here, we regard each
anchor as an observation point of the tissue and assess the relative distance
and orientation from the patch positions to the anchor positions. specifically,
a polar coordinate system is built on each anchor position, and the polar
coordinates of all the patches on the system are recorded. finally, a relative
distance matrix d ∈ n n k ×np and relative polar angle matrix p ∈ n n k ×np are
obtained, where d ij ∈ d and p ij ∈ p respectively represent the distance and
polar angle of the i-th patch in the polar coordinate system that takes the
position of the j-th anchor as the pole. then, we can formulate a wsi as s = {x,
k, d, p}.
figure 1(ii) illustrates the procedure of wsi representation learning. referring
to mae [7], we random mask patch tokens with a high masking ratio (i.e. 75% in
our experiments). the remaining tokens (as shown in fig. 1(b)) are fed into the
encoder. each encoder block sequentially consists of layernorm, paca module,
layernorm, and multilayer perceptron (mlp), as shown in fig. 1(c). then, masked
tokens are appended into encoded tokens to conduct the full set of tokens, which
is shown in fig. 1(d). next, the decoder reconstructs the slide representation
in feature space. finally, mean squared error (mse) loss is built between the
reconstructed patch features and the original patch features. referring to mae
[7], a trainable token is appended to the patch tokens to extract the global
representation. after training, the pre-trained encoder will be employed as the
backbone for various downstream tasks.
to preserve the structure information of the tissue, we propose the
positionaware cross-attention (paca) module, which is the core of the encoder
and decoder blocks. the structure of paca is shown in fig. 1
the message passing between the anchors and patches is achieved by a
bidirectional cross-attention between the patches and anchors. first, the
anchors collect the local information from the patches, which is formulated
aswhere w l ∈ r d f ×de , l = q, k, v are learnable parameters with d e denoting
the dimension of the head output, σ represents the softmax function, and ϕ d and
ϕ p are the embedding functions that respectively take the distance and polar
angle as input and output the corresponding trainable embedding values.
symmetrically, each patch token catches the information of all anchors into
their own local representations by the equationsthe two-way communication makes
the patches and anchors timely transmit local information and perceive the
dynamic change of global information. the embedding of relative distance and
polar angle information helps the model maintain the semantic and structural
integrity of the wsi and meanwhile prevents the wsi representation from
collapsing to the local area throughout the training process.in terms of
efficiency, the computational complexity of self-attention is o(n p 2 ) where n
p is the number of patch tokens. in contrast, our proposed paca's complexity is
o(n k × n p ) where n k is the number of anchors. notice that n k << n p , the
complexity is close to o(n p ), i.e. linear correlation with the size of the
wsi.
as for the polar angle matrix p ∈ n n k ×np , we specify the horizontal
direction of all the anchors as the initial polar axis. in natural scene images,
there is natural directional conspicuousness of semantics. for instance, in the
case of a church, it is most likely to find a door below the windows rather than
be located above them. but histopathology images have no absolute definition of
direction. the semantics of wsi will not change with rotation and flip. namely,
it is isotropic. embedding the orientation information with a fixed polar axis
will lead to ambiguities in various slides.to address this problem, we design a
kernel reorientation (kro) strategy to dynamically update the polar axis during
the training. as shown in fig. 1(iv), we equally divide the polar coordinate
system into n bins and calculate the sum of the attention scores from each bin.
then, the orientation with the highest score is recognized as the new polar axis
for the anchor. based on the updated polar axis, we can then amend p (n) to p
(n+1) . the detailed algorithm is described in algorithm 1.
we evaluated the proposed method on two datasets, the public tcga-lung and the
in-house endometrial dataset, which are introduced as follows.algorithm 1:
kernel reorientation algorithm.input: p (n) ∈ n h×n k ×np : the relative polar
angle matrix of n-th block, where h is the head number of multi-head attention,
n k is the number of anchors in the wsi, np is the number of patches in the wsi;
a (n) ∈ r h×n k ×np : the attention matrix from anchors to patches, defined asd
score : a dictionary taking the angle as key for storing attention scores;
output: p (n+1) ∈ r h×n k ×np : the updated polar angle matrix.h,i,max = arg max
d score ; // find the orientation that has the highest attention score. for j in
np do ph,i,max ;// reorientation.
tcga-lung dataset is collected from the cancer genome atlas (tcga) data portal.
the dataset includes a total of 3,064 wsis, which consist of three categories,
namely tumor-free (normal), lung adenocarcinoma (luad), and lung squamous cancer
(lusc), endometrial dataset includes 3,654 wsis of endometrial pathology, which
includes 8 categories, namely well/moderately/low-differentiated endometrioid
adenocarcinoma, squamous differentiation carcinoma, plasmacytoid carcinoma,
clear cell carcinoma, mixed-cell adenocarcinoma, and benign tumor.each dataset
was randomly divided into training, validation and test sets according to 6:1:3
while keeping each category of data proportionally. we conducted wsi multi-type
classification experiments on the two datasets. the validation set was used to
perform an early stop. the results of the test set were reported for comparison.
the wsi representation pre-training stage uses all training data and does not
involve any supervised information. during the downstream classification task,
the pre-trained encoder is utilized as the slide representation extractor, and
the [cls ] token is fed into the following classifier consisting of a multilayer
perceptron (mlp) and a fully connected layer. following the protocol in
selfsupervised learning [7], we evaluated the quality of pre-training with the
two approaches: 1) fine-tuning is to train the whole network parameters,
including wsi encoder and classifier; 2) linear probing is to freeze the encoder
and only train the classifier. the usage of [cls ] token refers to the mae [7]
framework, which was concatenated with patch tokens. during pre-training, the
[cls ] token is not involved in loss computation, but it continuously interacts
with kernels and receives global information. after pre-training, the
pre-trained parameters of the [cls ] token will be loaded for fine-tuning and
linear probing.to ensure the uniformity of patch features, we choose dino [3] to
extract patch features on the magnification under 20× lenses. accuracy (acc) and
area under the roc curve (auc) are employed as evaluation metrics. we
implemented all the models in python 3.8 with pytorch 1.7 and cuda 10.2 and run
the experiments on a computer with 4 gpus of nvidia geforce 2080ti.
we first conducted experiments on the endometrial dataset to verify the
effectiveness of self-supervised learning for wsi analysis under label-limited
conditions. the results are shown in fig. 2, where the performance obtained with
different ratios of labeled training wsis are compared. mae [7] based on the
patch features is implemented as the baseline. furthermore, we applied the
proposed distance and polar angle embedding to the self-attention module of mae
[7], which is referred to as mae+ in fig. 2. overall, pama consistently achieves
significantly better performance across all the label ratios than mae [7] and
hipt [5]. these results have demonstrated the effectiveness of pama in wsi
representation pre-training. moreover, pama achieves the best stability in aucs
and accs when the label ratios are reduced from 85% to 10%. this is of practical
importance as it reduces the dependence on a large number of labeled wsis for
training robust wsi analysis models. meanwhile, it means that we can utilize the
unlabeled wsis to improve the capacity of the models with the help of pama. hipt
[5] is a two-stage self-learning framework, which first leverages dino [3] to
pre-train patches (256 × 256) divided from regions (4096 × 4096) and then
utilizes dino-4k [5] to pre-train regions of wsis. the multi-stage framework
accumulated the training bias and noise, which caused an auc gap of hipt [5] to
mae [7] and pama, especially trained with only 10% labeled wsis. we also
observed a significant improvement when comparing mae+ with mae [7]. it
indicates the proposed distance and polar angle embedding strategy is more
appreciated than the positional embedding of vit [6] to describe the structure
of histopathological wsis. please refer to the supplementary materials for more
detailed results.
then, we conducted ablation experiments to verify the necessity of the proposed
structural embedding strategy. the detailed results are shown in table 1, where
all the models were fine-tuned with 35% training wsis. it shows that the auc
decreases by 0.019 and 0.021, respectively, when the distance or polar angle
embedding is discarded. and, when removing both the distance and polar angle
embedding, the auc drops by 0.034. these results demonstrate that local and
global spatial information is crucial for pama to learn wsi representations.
finally, we additionally compared the proposed pama with four weaklysupervised
methods, dsmil [12], transmil [13], setmil [18] and kat [19]. the results are
shown in table 2. overall, pama consistently achieves the best performance. in
comparison with the second-best methods, pama achieves an increase of
0.015/0.011 and 0.025/0.009 in aucs on tcga and endometrial datasets,
respectively, by using 35%/100% labeled wsis. moreover, pama reveals the most
robust capacity when reducing the training data from 100% to 35%, with auc
decreasing slightly from 0.988 to 0.982 and from 0.851 to 0.829 on the two
datasets. transmil [13], setmil [18] and kat [19] are state-ofthe-art methods
for histopathological image classification. they all considered the spatial
adjacency of patches but neglected the orientation relationships of the patches.
it is the main reason that the three methods cannot surpass our method even with
100% training wsis.
in this paper, we proposed an effective self-supervised representation learning
framework for wsi analysis. the experiments on two large-scale datasets have
demonstrated the effectiveness of pama in the condition of limited-label. the
results have shown superiority to the existing weakly-supervised and
selfsupervised mil methods. future work will focus on training the wsi
representation model based on datasets across multiple organs, thus promoting
the generalization ability of the model for different downstream tasks.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 69.
cervical cancer is a common and severe disease that affects millions of women
globally, particularly in developing countries [9]. early diagnosis is vital for
successful treatment, which can significantly increase the cure rate [17]. in
recent years, computer-aided diagnosis (cad) methods have become an important
tool in the fight against cervical cancer, as they aim to improve the accuracy
and efficiency of diagnosis.several computer-aided cervical cancer screening
methods have been proposed for whole slide images (wsis) in the literature. most
of them are detectionbased methods, which typically contain a detection model as
well as some postprocessing modules in their frameworks. for instance, zhou et
al. [29] proposed a three-step framework for cervical thin-prep cytologic test
(tct) [12]. the first step involves training a retinanet [13] as a cell
detection network to localize suspiciously abnormal cervical cells from wsis. in
the second step, the patches centered on these detected cells are processed
through a classification model, to refine the judgment of whether they are
positive or negative. finally, the positive patches refined by the patch-level
classification are further combined to produce an overall positive/negative
diagnosis for the wsi at the sample level.some methods improve the final
classification performance by improving the detection model to identify positive
cells more reliably. cao et al. [1] improved the detection performance by
incorporating clinical knowledge and attention mechanism into their cell
detection model of attfpn. wei et al. [24] adopted the yolo [20] architecture
with a variety of convolution kernels of different sizes to accommodate diverse
cell clusters. other methods improve the classification performance by changing
the post-processing modules behind the detection model. cheng et al. [5]
proposed a progressive identification method that leveraged multi-scale visual
cues to identify abnormal cells and then an rnn [27] for sample-level
classification. zhang et al. [28] used gat [23] to model the relation of the
suspicious positive cells provided by detection, thus obtaining a global
description of the wsi and performing sample-level classification.these methods
have achieved good results through continuous improvement on the detection-based
pipeline, but there are some common drawbacks. first, they are not able to get
rid of their reliance on detection models, which means they have a high need for
expensive detection data labeling to train the detection model. cervical cancer
cell detection datasets involve labeling individual and small bounding boxes in
a large number of cells. it often requires multiple experienced pathologists to
annotate [15], which is very time-consuming and labor-intensive. second, the
widely used detection-based pipeline has not fully utilized the massive
information in wsis. a wsi is typically large (sized of about 20000 × 20000
pixels). a lot of data would be wasted if only a small part of annotated images
(e.g., corresponding to positive cells and bounding boxes) was used as training
data. finally, many existing methods focus on detecting and classifying
individual cells. the tendency to neglect effective integration of the overall
information across the entire wsi results in poor performance in sample-level
classification.to address the aforementioned issues, we propose a detection-free
pipeline in this paper, which does not rely on any detection model. instead, our
pipeline requires only sample-level diagnosis labels, which are naturally
available in clinical scenarios and thus get rid of additional image labeling.
to attain this goal, we have designed a two-stage pipeline as in fig. 1. in the
coarse-grained stage, we crop and downsample a wsi into multiple images, and
conduct sample-level classification roughly based on all resized images. the
coarse-grained classification yields attention scores, from which we perform
attention guided selection to localize these key patches from the original wsi.
then, in the fine-grained stage, we use these key patches for fine prediction of
the sample. the two stages in our pipeline adopt the same network design (i.e.,
encoder + pooling trans-former), which makes our solution friendly to develop
and to use. we also adopt contrastive learning to effectively utilize the
massive information in wsis when training the encoder for classification. as a
summary, our pipeline surpasses previous detection-based methods and achieves
state-of-the-art performance with large-scale training. our experiments show
that our method becomes more effective when increasing the data size for
training. moreover, while many pathological images are also based on wsis, our
pipeline has a high potential to extend to other pathological tasks.
fig. 1. the overview of our proposed method. for feasibility of computation, we
crop a wsi into mutiple images. the cropped images are passed through the
coarse-grained and fine-grained stages, where only sample-level diagnosis labels
of wsis, instead of any additional manual labeling, are required for training.
the overview of our two-stage pipeline is shown in fig. 1. the input wsi is
typically too big to be directly processed by a common deep learning pipeline
[18], so we crop each wsi into local images sized 1024 × 1024 pixels. the images
are then processed through the coarse-grained and fine-grained stages in order
to obtain the wsilevel classification results, respectively. in general, the
purpose of the coarsegrained stage is to replace the detection model and
identify local images that may contain abnormal positive cells. the fine-grained
stage then integrates these key regions, producing refined classification for
the sample.to complete sample-level classification, both stages share basically
the same network architecture. the input images are first processed by a cnn
encoder to extract features. then, we propose the pooling transformer, which is
modified from the basic transformer module in sect. 2, to integrate these
features for wsi classification. additionally, the input images for both stages
are 256 × 256. in the coarse-grained stage, in order to allow the model to
examine as many local images as possible, we resize the cropped local images
from 1024 × 1024 to 256 × 256. in the fine-grained stage, we enlarge suspicious
local abnormality and thus crop input images to 256 × 256 from 1024 × 1024.for
the coarse-grained stage, after passing the resized local images through encoder
and pooling transformer, we obtain a rough prediction result at the sample
level. we then use the cross-entropy (ce) loss to minimize the difference
between the predicted wsi label and the ground truth. in addition, we calculate
the attention score to identify the local image inputs that are most likely to
yield positive reading. we describe the attention score aswhere x 0 represents
classification token (which is a commonly used setting in transformer [7,22]),
and d x0 is 512 in our implementation, f represents the feature vector of a
certain input local image. after calculating attention scores, we preserve top-8
(resized) local images with the highest scores from the entire wsi for
subsequent fine-grained classification.next, in the fine-grained stage, each
local image that has passed attention guided selection is cropped into 16
patches of the size 256 × 256. we expect that those patches contain positive
cells and are thus critical to diagnosis at the sample level. the network of the
fine-grained stage is the same as that of the coarse-grained stage, but the
weights of the encoder is pre-trained in an unsupervised manner (sect. 2). the
same ce loss supervised by sample-level ground truth is used for the
fine-grained stage here. for inference, the output of the fine-grained stage
will be treated as the final result of the test wsi. pooling transformer. we use
a transformer network to aggregate features of multiple inputs and to derive the
sample-level outcome in both coarse-grained and fine-grained stages. we have
observed that different local images of the same sample often have patterns of
grouped similarity (such as the first two images in the upper-right of fig. 1).
for negative samples, most of the local images are similar with each other. for
positive samples, the images of abnormal cells are inclined to be grouped into
several clusters.therefore, inspired by [2,14], we propose pooling transformer
that is effective to reduce the redundancy and distortion from the input images.
the pooling transformer in fig. 2 is designed to integrate all inputs toward the
samplelevel diagnosis. to remove redundant features, between two transformer
layers, we use the affinity propagation algorithm [8] to cluster the inputs into
several classes. within each clustered class, we average the features and
aggregate a single token. finally, the classification (cls) token is
concatenated with all tokens after clustering-based pooling, and passed through
the rest of the network to obtain the classification result. in this way, we
find that the similar yet redundant input features can be fused, making the
network more concise and efficient to calculate the attention between pooled
features.
to make full use of wsi data and provide a better feature encoder, inspired by
moco [4,11] and other contrastive learning methods [25,26] pre-training on
imagenet [6], we also perform pretraining for fine-grained encoder on a large
scale of pathology images. generally, large-scale pre-training usually requires
a massive dataset and a suitable loss function. for data, wsi naturally has the
advantage of having a large amount of training data. a wsi (20000 × 20000) can
be cropped into about 5000-6000 patches (256 × 256). therefore, we only need
2,000-3,000 wsi samples to obtain a dataset that can even be compared to
imagenet in quantity. for the loss function, there are typically two ways: one
is like mae [10] to model the loss function using masks, and the other is to use
contrastive learning as in moco and clip [19]. in our task, since the structural
features of cells are relatively weak compared to natural images, it is not
suitable to model the loss function using masks. therefore, we adopt a
contrastive learning approach.specifically, in the same training batch, a patch
(256 × 256, the same to the input size of the fine-grained stage) and its
augmented patch are treated as a positive pair (note that here
"positive/negative" is defined in the context of contrastive learning), and
their features are required to be as similar as possible. meanwhile, their
features are required to be as dissimilar as possible from those of other
patches. so the loss function can be described asf i and f ia represent the
positive pair, and f j represents another patch negatively paired with f i .
using this method, we can pre-train a feature encoder in an unsupervised manner
and initialize it into our encoder for the fine-grained stage.
dataset and experimental setup. in this study, we have collected 5384 cervical
cytopathological wsi by 20x lens, each with 20000 × 20000 pixels, from our
collaborating hospitals. among them, there 2853 negative samples, and 2531
positive samples (962 ascus, and 1569 high-level positive samples). all wsis
only have diagnosis labels at the sample level, without annotation boxes at the
cell level. and all sample labels are strictly diagnosed according to the tbs
[16] criterion by a pathologist with 35 years of clinical experience. we conduct
the experiment with initial learning rate of 1.0 × 10 -4 , batch size of 4, and
sgd optimizer [21] for 30 epochs each stage. for contrastive pre-training, we
follow the settings of mocov2 [3] and trained for 300 epochs.comparison to sota
methods. in this section, we experiment to compare our method with popular
state-of-the-art (sota) methods, which are all fully supervised and
detection-based. to the best of our knowledge, there are few good methods to
train cervical cancer classification models in weakly supervised or unsupervised
learning ways. no methods can achieve the detection-free goal either.all the
detection-based methods are evaluated in the following way. first, we label a
dataset with cell-level bounding boxes to train a detection model. the detection
dataset has 3761 images and 7623 cell-level annotations. after obtaining the
suspicious cell patches provided by the detection model, we use the subsequent
classification models used in these sota works to classify them and obtain the
final classification results. as shown in table 1 for fair five-fold
cross-validation, our method outperforms all compared detection-based methods.
while our method has a large margin with most methods in the table, the
improvement against [28] (top-ranked in current detection-based methods) is
relatively limited. on one hand, in [28], gat aggregates local patches that are
detected by retinanet. and the attention mechanism of gat is similar with the
transformer used in our pipeline to certain extent. on the other hand, the
result implies that our coarse-grained task has replaced the role of cell
detection in early works. thus, we conclude that a detection model trained with
an expensive annotated dataset is not necessary to build a cad pipeline for
cervical abnormality.ablation study. in this section, we experiment to
demonstrate the effectiveness of all the proposed parts in our pipeline. we
divide all 5384 samples into five independent parts for five-fold
cross-validation, and the results are shown in table 2. here, cg means the
classification passes only the coarse-grained stage. as can be seen, its
performance is low, in that the resized images sacrifices the resolution and
thus perform poorly for image-based classification. fg refers to classifying in
the fine-grained stage. it is worth noting that without the attention scores
provided by the coarse-grained stage, we have no way of knowing which local
images might contain suspicious positive cells. thus, we use random selection to
experiment for fg only, as exhaustively checking all local images is
computationally forbidden. as can be seen, the classification result is the
lowest because it lacks enough access to the key image content in wsis. by
combining the two stages for attention guided selection, it is effective to
improve the classification performance compared to the two previous experiments.
here, for the cases of cg, fg and cg+fg, an original transformer network without
clustering-based pooling is used. in addition, as shown in the last two rows of
the table, both pooling transformer (pt) and unsupervised pretraining (cl)
contribute to our pipeline. ultimately, we combine them together to achieve the
best performance.
in order to further demonstrate the huge potential of our method, we also
perform an ablation study on the number of samples used for training and compare
the time consuming of the different methods. for the experiment of sample
numbers, we compare the best fully supervised detection-based method
(retinanet+gat [28]) with ours under the sample numbers of 500, 1000, 2000, and
5384. as shown by table 3 and left of fig. 3, the traditional detection-based
method has quickly encountered a saturation bottleneck as the amount of data
increases. although our method initially has poorer performance, it has shown an
impressive growth trend. and at our current maximum data number (5384), the
proposed pipeline has already exceeded the performance of the detection-based
method. the above results also demonstrate that our new pipeline method has
greater potential, even though it requires no cell-level image annotation. for
inference time consuming, as shown in right of fig. 3, our method has shorter
inference time and a good balance between accuracy and inference time.
in this paper, we propose a novel two-stage detection-free pipeline for wsi
classification of cervical abnormality. our method does not rely on detection
models and eliminates the need for expensive cell-level data annotation. by
leveraging just sample-level diagnosis labels, we achieve results that are
competitive with fully supervised detection-based methods. through the use of
the proposed pooling transformer and unsupervised pre-training, our method makes
full use of information within wsis, resulting in improved efficiency in the use
of pathological images. importantly, our method offers even greater advantages
with increasing amounts of data. and also, by utilizing attention weights, we
can calculate attention scores to visually represent the importance of each
image in the sample, making it easier for doctors to make judgments. relevant
visualization results can be found on our project homepage. admittedly, our
method has some limitations, such as slow training. accelerating the training of
massive data can be our next optimization direction.
colorectal cancer is a prevalent form of cancer characterized by colorectal
adenocarcinoma, which develops in the colon or rectum's inner lining and
exhibits glandular structures [5]. these glands play a critical role in protein
and carbohydrate secretion across various organ systems. histological
examinations using hematoxylin and eosin staining are commonly conducted by
pathologists to evaluate the differentiation of colorectal adenocarcinoma [15].
the extent of gland formation is a crucial factor in determining tumor grade and
differentiation. accurate segmentation of glandular instances on histological
images is essential for evaluating glandular morphology and assessing colorectal
adenocarcinoma malignancy. however, manual annotation of glandular instances is
a time-consuming and expertise-demanding process. hence, automated methods for
glandular instance segmentation hold significant value in clinical practice.
automated segmentation has been explored using deep learning techniques [21,33],
including u-net [17], fcn [13], siamese network [10,11] and their variations for
semantic segmentation [31]. there are also methods that combine information
bottleneck for detection and segmentation [23]. additionally, twostage instance
segmentation methods like mask r-cnn [7] and blendmask [3] have been utilized,
combining object detection and segmentation sub-networks. however, these methods
may face difficulties in capturing different cell shapes and distinguishing
tightly positioned gland boundaries. limitations arise from image scaling and
cropping, leading to information loss or distortion, resulting in ineffective
boundary recognition and over-/under-segmentation. to overcome these
limitations, we aim to perform gland instance segmentation to accurately
identify the target location and prevent misclassification of background
tissue.recently, diffusion model [9] has gained popularity as efficient
generative models [16]. in the task of image synthesis, diffusion model has
evolved to achieve state-of-the-art performance in terms of quality and mode
coverage compared with gan [32]. furthermore, diffusion model has been applied
to various other tasks [18]. diffusiondet [4] treats the object detection task
as a generative task on the bounding box space in images to handle projection
detection. several studies have explored the feasibility of using diffusion
model in image segmentation [26]. these methods generate segmentation maps from
noisy images and demonstrate better representation of segmentation details
compared to previous deep learning methods.in this paper, we propose a new
method for gland instance segmentation based on the diffusion model. (1) our
method utilizes a diffusion model to perform denoising and tackle the task of
gland instance segmentation in histology images. the noise boxes are generated
from gaussian noise, and the predicted ground truth (gt) boxes and segmentation
masks are performed during the diffusion process. (2) to improve segmentation,
we use instance-aware techniques to recover lost details during denoising. this
includes employing a filter and a multi-scale mask branch to create a global
mask and refine finer segmentation details. (3) to enhance object-background
differentiation, we utilize conditional encoding to augment intermediate
features with the original image encoding. this method effectively integrates
the abundant information from the original image, thereby enhancing the
distinction between the objects and the surrounding background. our proposed
method was trained and tested on the 2015 mic-cai gland segmentation (glas)
challenge dataset [20] and colorectal adenocarcinoma gland (crag) dataset [6]
(as shown in fig. 1), and the experiment results demonstrate the efficacy of the
method. to preserve multi-scale information, we introduce a mask branch that
operates on f mask . by applying convolutions with weights assigned from filters
to f mask , we obtain instance masks.
in this section, we present the architecture of our proposed method, which
includes an image encoder, an image decoder, and a mask branch. the network
structure is shown in fig. 2.
we propose to perform subsequent operations on the features of the original
image, so we use an image encoder for advanced feature extraction. the image
encoder takes the original image as input and we use a convolutional neural
network such as resnet [8] for feature extraction and a feaure pyramid network
(fpn) [12] is used to generate a multi-scale feature map for resnet backbone
following.the input image is x and the output is a high-level feature f r .where
f is the resnet. the image encoder operates only once and uses the f r as
condition to progressively refine and generate predictions from the noisy boxes.
we designed our model based on the diffusion model [30], which typically uses
two markov chains divided into two phases: a forward diffusion process and a
reverse denoising process. the components of diffusion model are a learning
reverse process called p θ (z t-1 |z t ) that creates samples by converting
noise into samples from q(z 0 ) and a forward diffusion process called q(z t |z
t-1 ) that gradually corrupts data from some target distribution into a normal
distribution. the forward diffusion process is defined as:a variance schedule β
t ∈ (0, 1), t ∈ {1, ..., t } determines the amount of noise that is introduced
at each stage. alternatively, we can obtain a sample of z t from direct z 0 as
follows:where ᾱt = t s=0 (1β s ), ∼ n (0, i). our image decoder is based on
diffusion model, which can be viewed as a noise-to-gt denoising process. in this
setting, the data samples consist of a set of bounding boxes represented as z 0
, where z 0 is a set of n boxes.the neural network f θ (z t , t) is trained to
predict z 0 from the z t based on the corresponding image x. in addition, to
achieve complementary information by integrating the segmentation information
from z t into the original image encoding, we introduce conditional encoding,
which uses the encoding features of the current step to enhance its intermediate
features.where d represent the decoder, e represent the encoder and m ∈ {1, ...,
t }. we use instance aware filters (iaf) during iterative sampling, which allows
sharing parameters between steps.where f t f is the output feature of the
filter.
we have also utilized dynamic mask head [22] to predict masks in our study.in
this stage, we use the mask branch to fuse the different scale information of
the fpn and output the mask feature f mask . the diffusion process decodes roi
features into local masks, and multi-scale features can be supplemented with
more detailed information for predicting global masks to compensate for the
detail lost in the diffusion process, and we believe that instance masks require
a larger perceptual domain because of the higher demands on instance edges.
specifically, the instance mask can be generated by convolving the feature map f
mask from the mask branch and f t f from the iaf , which is calculated as
follows:where the predicted instance mask is denoted by s ∈ r h×w . the mask fcn
head, denoted by mf h, is comprised of three 1 × 1 convolutional layers. we
enhance our loss function by incorporating two components, l d and l s , and
utilize the γ parameter to optimize the balance between these two losses.where
the l s in our model represents the measure of overlap between the predicted
instance mask and the ground truth s gt [14], and the l d is the loss of
diffusiondet. the optimal value for the parameter γ is usually determined based
on achieving the best overall performance on the validation set. in this work,
we chose γ = 5 to balance these two losses.
we presented the segmentation results of our model compared to the ground truth
in fig. 3, and provided both qualitative and quantitative evaluations that
validate the effectiveness of our proposed network for gland instance
segmentation.
we evaluated the effectiveness of the proposed model on two datasets: the glas
dataset and the crag dataset. the glas dataset comprises 85 training and 80
testing images, divided into 60 images in test a and 20 images in test b. the
crag dataset consists of 173 training and 40 testing images. we have adopted
vahadane method for stain normalization [1]. furthermore, to enhance the
training dataset and mitigate the risk of overfitting, we employed random
combinations of image flipping, translation, gaussian blur, brightness
variation, and other augmentation techniques.we assessed the segmentation
results using three metrics from the glas challenge: (1) object f1, which
measures the accuracy of detecting individual glands, (2) object dice, which
evaluates the volume-based accuracy of gland segmentation, and (3) object
hausdorff, which assesses the shape similarity between the segmentation result
and the ground truth. we assigned each method three ranking numbers based on
these metrics and computed their sum to determine the final ranking for each
method's overall performance.
in our experiments, we choose the resnet-50 with fpn as the backbone in the
proposed method. the backbone is pretrained on imagenet. image decoder, mask
branch and mask fcn head are trained end-toend. we trained on the glas and crag
datasets in a python 3.8.3 environment on ubuntu 18.04, using pytorch 1.10 and
cuda 11.4. during training, we utilized an sgd optimizer with a learning rate of
2.5 × 10 -5 and the weight decay as 10 -4 . we set diffusion timesteps t = 1000
and chose a linear schedule from β 1 = 10 -4 to β t = 0.02. training was
performed on a100 gpu with a batch size of 2.
we conducted experiments to evaluate the performance of our proposed model by
comparing it with the dse model [27], the dmcn [28], the dcan [2], the spl-net
[29], the doubleu-net [24], the mild-net [6], the gcsba-net [25], and the mpcnn
[19]. table 1 provides an overview of the average performance of these
models.our proposed model demonstrated a enhancement in performance, surpassing
the second-best method on both test a and test b datasets. specifically, on test
a, we observed an improvement of 0.006, 0.01, and 1.793 in object f1, object
dice, and object hausdorf. similarly, on test b, resulting in an improvement of
0.022, 0.014 and 3.694 in object f1, object dice, and object hausdorf,
respectively. although test b presented a more challenging task due to the
presence of complex morphology in the images, our proposed model demonstrated
accurate segmentation in all cases. the experimental results highlighted the
effectiveness of our approach in improving the accuracy of gland instance
segmentation.
the proposed model was additionally evaluated on the crag dataset by comparing
it against the gcsba-net, doubleu-net, dse model, mild-net, and dcan. the
average performance of these models is shown in table 2. our experimental
results demonstrate that our proposed method achieves superior performance, with
improvements of 0.017, 0.012, and 4.026 for object f1, object dice, and object
hausdorff, respectively, compared to the second-best method. these results
demonstrate the effectiveness of our method in segmenting different
datasets.ablation studies: our network utilizes the mask branch and conditional
encoding to enhance performance and segmentation quality. ablation studies on
the glas and crag datasets confirm the effectiveness of these modules (table 3).
the mask branch is responsible for multi-scale feature extraction and fusion
with the backbone network, as well as refining the image decoder's output.
without the mask branch, direct usage of original image features lacks
multi-scale information and results in less accurate segmentation. conditional
encoding is employed to establish a connection between input image features
in this paper, we propose a diffusion model based method for gland instance
segmentation. by considering instance segmentation as a denoising process based
on diffusion model. our model contains three main parts: image encoder, image
decoder, and mask branch. by utilizing a diffusion model with conditional
encoding for denoising, we are able to improve the precision of instance
localization while compensating for the missing details in the diffusion model.
by incorporating multi-scale information fusion, our approach results in more
accurate segmentation outcomes. experimental results on the glas dataset and
crag dataset show that our method surpasses state-of-the-art approach,
demonstrating its effectiveness. although our method demonstrates excellent
performance in gland instance segmentation, challenges arise in certain
scenarios characterized by irregular shapes, flattening, and overlapping. in
such cases, our network tends to classify multiple small targets with unclear
boundaries as a single object, indicating limitations in segmentation accuracy
when dealing with high aggregation or overlap. this limitation may stem from the
difficulty in accurately distinguishing fine details between instances and the
incorrect identification of boundaries.to address these limitations, future work
will focus on improving segmentation performance in challenging scenarios by
specifically targeting three identified limitations: (1) incorporate random
noise during training to reduce reliance on bounding box information for
denoising; (2) explore more efficient methods for cross-step denoising in the
diffusion model to improve processing time without compromising segmentation
accuracy; and (3) develop a more effective conditional encoding method to
provide accurate instance context for noise filtering in discriminative tasks
like nuclear segmentation.
immunohistochemical (ihc) staining is a widely used technique in pathology for
visualizing abnormal cells that are often found in tumors. ihc chromogens
highlight the presence of certain antigens or proteins by staining their
corresponding antibodies. for instance, the her2 (human epidermal growth factor
receptor 2) biomarker is associated with aggressive breast tumor development and
is essential in forming a precise treatment plan. despite its capability to
provide highly valuable diagnostic information, the process of ihc staining is
very labor-intensive, time-consuming and requires specialized histotechnologists
and laboratory equipments [2]. such factors hinder the general availability of
ihc staining in histopathological applications.at the other end of the spectrum,
h&e (hematoxylin and eosin) staining, as the gold standard in histological
staining, highlights the tissue structures and cell morphology. in routine
diagnostics, on account of its much lower cost, an h&e-stained slide is prepared
by pathologists in order to determine whether or not to also apply the ihc
stains for a more precise assessment of the disease. therefore, it is of great
interest to have an algorithm that can automatically translate an h&e-stained
slide into one that could be considered to have been stained with ihc while
accurately predicting the target expression levels.to that end, researchers have
recently proposed to use gan-based image-to-image translation (i2it) algorithms
for transforming h&e-stained slides into ihc. despite the progress, the
outstanding challenge in training such i2it frameworks is the lack of aligned
h&e-ihc image pairs, or in other words, the inconsistencies in the h&e-ihc
groundtruth pairs. to explain, since re-staining a slice is physically
infeasible, a matching pair of h&e-ihc slices are taken from two depth-wise
consecutive cuts of the same tissue then stained and scanned separately. this
inevitably prevents pixel-perfect image correspondences due to the
slice-to-slice changes in cell morphology, staining-induced degradation (e.g.
tissue-tearing), imaging artifacts that may vary among slices (e.g. camera
out-offocus) and multi-slice registration errors. an example pair of patches is
shown in fig. 1 and another pair with significant inconsistencies is shown in
fig. 2(a)(c). in the latter, comparing the groundtruth ihc image to the input
h&e image, one can clearly see the inconsistencies -nearly the entire left half
of the tissue present in the h&e image is missing.as a result, recent advances
in h&e-to-ihc i2it have mostly avoided using the inconsistent gt pairs and
instead have imposed the cycle-consistency constraint [6,8,13]. moreover,
existing approaches have also exploited using expert annotations such as
per-cell labels [9], semantic masks [8] and patch-level labels [8,13]. as for
the prior works that directly utilize the h&e-ihc pairs for supervision, a
variant of pix2pix [4] that uses a gaussian pyramid based reconstruction loss to
accommodate the noisy gt is proposed in [7]. however, the robustness of such
approaches that punish absolute errors in the generated image to dealing with gt
inconsistencies remains unclear.in this paper, we argue that the ihc slides,
despite the disparities vis-a-vis their h&e counterparts, can still serve as
useful targets for stain translation. the work we present in this paper is based
on the important realization that even when pairs of consecutive tissue slices
do not yield images that are pixel-perfect aligned, it is highly likely that the
corresponding patches in the two stains share the same diagnostic label. for
example, if the levels of expression in a region of the her2 slide are high, the
corresponding region in the h&e slide is highly likely to contain a high density
of cancerous cells. therefore, we set our goal to meaningfully leverage such
correlations to benefit the h&e-to-ihc i2it while being resilient to any
inconsistencies.toward this goal, we propose a supervised patchwise contrastive
loss named the adaptive supervised patchnce (asp) loss. our formulation of this
loss was inspired by the recent research findings that contrastive loss benefits
model robustness under label noise [3,12]. furthermore, based on the observation
that any dissimilarity between the patch embeddings at corresponding locations
in the generated and groundtruth ihc images is indicative to the level of
inconsistency of the gt at that location, we employ an adaptive weighting scheme
in asp. by down-weighting the contrastive loss at locations with low
similarities, i.e. high inconsistencies, our proposed asp loss helps the network
learn more robustly.lastly, to support further research in virtual
ihc-restaining, we present the multi-ihc stain translation (mist) as a new
public dataset. the mist dataset contains 4k+ training and 1k testing aligned
h&e-ihc patches for each of the following ihc stains that are critical for
breast cancer diagnostics: her2, ki67, er (estrogen receptor) and pr
(progesterone receptor). we evaluated existing i2it methods and ours for
multiple ihc stains and demonstrate the superior performance achieved by our
method both qualitatively and quantitatively.
before getting to our asp loss, we need to first introduce the sp loss as a
robust means to learning from inconsistent gt image pairs. the sp loss was
inspired by the findings in recent literature that demonstrate the positive
effect of contrastive learning on boosting model robustness against label noise
[3,12,14]. it takes the same form as the patchnce loss as introduced in [11],
except that it is applied on the generated-gt image pair (instead of the
input-generated pair).the goal of the patchnce loss is to ensure the content is
consistent across translation by maximizing the mutual information between the
input and the corresponding output. it does so by minimizing a patch-based
infonce loss [10], which encourages the network to associate the corresponding
patches with each other in the learned embedding space, while disassociating
them from the noncorresponding ones. mathematically, the infonce loss takes the
form:where v, v + and v -are the embeddings of the anchor, positive and negative
samples, respectively. with infonce, the patchnce loss is set up as
follows:given the anchor embedding ẑy of a patch in the output image, the
positive z x is the embedding of the corresponding patch from the input image,
while the negatives z x are embeddings of the non-corresponding ones, i.e.as for
the sp loss, given the embedding of an output patch ẑy as anchor, we now
designate the embedding of the corresponding patch in the groundtruth image z y
as the positive and the embeddings of the non-corresponding ones z y as the
negatives. we then use the same infonce-based contrastive learning objective,
i.e. l sp = l infonce (ẑ y , z y , z y ). a depiction of both the patchnce loss
and the sp loss is given in fig. 1. it is worth noting that, despite the fact
that a similar patchwise constrastive loss was proposed in [1] for supervised
i2it, it is one of our contributions in this paper to explicitly exploit the
robustness of this contrastive loss in the context of h&e-to-ihc translation
where the gt pairs can be highly inconsistent for reasons mentioned previously.
we think that the key factor behind the robustness of l sp to inconsistent gt
compared to, say, the mse loss, is its relativeness. instead of using an
absolute loss term that may not work well on inconsistent groundtruth pairs, l
sp punishes dissimilarities between the anchor and the positive in the learned
latent space, relative to those between the anchor and the negatives.
to learn selectively from more consistent groundtruth locations, we further
propose to augment the supervised patchnce loss in an adaptive manner. the key
idea here is to automatically recognize patch locations that are inconsistent
and adapt the sp loss so that the severely inconsistent patch locations will
have lesser effects on training. to measure the consistency at a given patch
location, we use the cosine similarity between the embeddings of the generated
ihc patch and the corresponding gt patch. in fig. 2, we show an example pair of
generated vs gt ihc images that contain significant inconsistencies and their
anchor-positive similarity heat map. for pairs of embeddings produced by a
trained network, a high similarity value indicates good correspondence between
the groundtruth patches while a low similarity value indicates
inconsistencies.directly motivated by this observation, we first propose a
weighting scheme for the sp loss. more specifically, we assign lower weights to
patch locations that have low anchor-positive similarity values to alleviate the
negative impacts the inconsistent targets may have on training. at training time
t, the weight is a function of the anchor-positive cosine similarity. examples
of the weight function h(•) are shown in fig. 3(b). the weight functions are
monotonic increasing so that the more confident patch locations are always
treated with more importance.in order to make the weighting scheme work in
practice, we must also account for the phase of training. the intuition is that,
during the initial phase of training, the network is not going to be able to
discriminate between consistent patch locations from those that are
inconsistent. additionally, as shown in fig. 3(a), the histograms of the
anchor-positive similarity evolve rather slowly over the training epochs.
therefore, it would not make sense to reinforce the weighting function in the
beginning of the training as much as near the end of the training.to that end,
we further augment the weight so that it is also a function of the training
iterations. such scheduling of the weights is done so that in the beginning of
the training, the weights are uniform in order not to wrongly bias the network
when the embeddings are still indiscriminative. and as training progresses, the
selective weighting scheme is gradually enforced so that the inconsistent patch
locations are treated with reduced weights. we call this gradual process of
shifting the learning focus weight scheduling. let t denote the current
iteration and t the total number of training iterations. then weight scheduling
is achieved by using a scheduling function g( t t ). various options of g(•) are
shown in fig. 3(c). subsequently, combining the weighting function with the
scheduling function, we can write the following formula for the final weight:we
refer to the new augmented supervised patchnce loss as the adaptive supervised
patchnce (asp) loss, which can be expressed as: where w l t = s w l,s t is a
normalization factor that maintains the total magnitude of the loss after
applying the weights. finally, the overall learning objective for our generator
is as follows:where l gp is the gaussian pyramid based reconstruction loss from
[7].
datasets. the following datasets are used in our experiments: the breast cancer
immunohistochemical (bci) challenge dataset [7] and our own mist dataset that is
now in the public domain. the publicly available portion of bci contains 3396 h
h&e-her2 patches for training and 500 of the same for testing. note that we have
additionally normalized the brightness levels of all bci images to the same
level. due to the page limit, from the mist dataset, here we only present
detailed results on her2 and er. for mist her2 , we extracted 4642 paired
patches for training and 1000 for testing from 64 wsis. and for mist er , we
extracted 4153 patches for training, and 1000 for testing from 56 wsis. all wsis
were taken at 20× magnification. all patches are of size 1024 × 1024 and
non-overlapping. additional results on mist ki67 and mist pr are provided in the
supplementary materials.implementation details. for all of our models, we used
resnet-6blocks as the generator and a 5-layer patchgan as the discriminator. we
trained our networks with random 512 × 512 crops and a batch size of one. the
adam optimizer [5] was used with a linear decay scheduler (as shown in fig.
3(c)) and an initial learning rate of 2 × 10 -4 . the hyperparameters in eq. (
4) are set as: λ patchnce = 10.0, λ asp = 10.0 and λ gp = 10.0.evaluation
metrics. we compare the methods using both paired and unpaired evaluation
metrics. to compare a pair of images, generated and groundtruth, we use the
standard ssim (structural similarity index measure) and phv (perceptual hash
value) as described in [8]. as for the unpaired metrics, we use the fid (fréchet
inception distance) and the kid (kernel inception distance).qualitative
evaluations. in fig. 4, we compare visually the generated ihc images by our
framework. it can be observed that by using either l sp or l asp , the
pathological representations in the generated images are significantly more
accurate. and by using l asp , such representations appear to be more
consistent. quantitative evaluations. the full results comparing existing i2it
methods to ours are tabulated in tab. 1. overall, it can be observed that the
proposed framework with the asp loss consistently outperforms existing methods
across all three datasets. for those methods, fig. 5 visually illustrates the
extent of hallucinations which we believe is the reason for their poor
quantitative performance. subsequently, in tab.
in this paper, we have proposed the adaptive supervised patchnce (asp) loss for
learning h&e-to-ihc stain translation with inconsistent gt image pairs. the
adaptive logic in asp is based on the intuition that inconsistent patch
locations should contribute less to learning. we demonstrated that our proposed
framework is able to achieve significant improvements both qualitatively and
quantitatively over the existing approaches for translations to multiple ihc
stains. finally, we have made public our multi-ihc stain translation dataset
with the hope to assist further research towards accurate h&e-to-ihc stain
translation.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 61.
histopathology is considered the gold standard for diagnosing and treating many
cancers [19]. the tissue slices are usually scanned into whole slide images
(wsis) and serve as important references for pathologists. unlike natural
images, wsis typically contain billions of pixels and also have a pyramid
structure, as shown in fig. 1. such gigapixel resolution and expensive
pixel-wise annotation efforts pose unique challenges to constructing effective
and accurate models for wsi analysis. to overcome these challenges, multiple
instance learning (mil) has become a popular paradigm for wsi analysis.
typically, mil-based wsi analysis methods have three steps: (1) crop the huge
wsi into numerous image patches; (2) extract instance features from the cropped
patches; and (3) aggregate instance features together to obtain slide-level
prediction results. many advanced mil models emerged in the past few years. for
instance, abmil [9] and deepattnmil [18] incorporated attention mechanisms into
the aggregation step and achieved promising results. recently, graph-transformer
architecture [17] has been proposed to learn short-range local features through
gnn and long-range global features through transformer simultaneously. such
graph-transformer architecture has also been introduced into wsi analysis
[15,20] to mine the thorough global and local correlations between different
image patches. however, current graph-transformer-based wsi analysis models only
consider the representation learning under one specific magnification, thus
ignoring the rich multi-resolution information from the wsi pyramids.different
resolution levels in the wsi pyramids contain different and complementary
information [3]. the images at a high-resolution level contain cellularlevel
information, such as the nucleus and chromatin morphology features [10]. at a
low-resolution level, tissue-related information like the extent of tumorimmune
localization can be found [1], while the whole wsi describes the entire tissue
microenvironment, such as intra-tumoral heterogeneity and tumor invasion [3].
therefore, analyzing from only a single resolution would lead to an incomplete
picture of wsis. some very recent works proposed to characterize and analyze
wsis in a pyramidal structure. h2-mil [7] formulated wsi as a hierarchical
heterogeneous graph and hipt [3] proposed an inheritable vit framework to model
wsi at different resolutions. whereas these methods only characterize local or
global correlations within the wsi pyramids and use only unidirectional
interaction between different resolutions, leading to insufficient capability to
model the rich multi-resolution information of the wsi pyramids.in this paper,
we present a novel hierarchical interaction graph-transformer framework (i.e.,
higt) to simultaneously capture both local and global information from wsi
pyramids with a novel bidirectional interaction module. specifically, we
abstract the multi-resolution wsi pyramid as a heterogeneous hierarchical graph
and devise a hierarchical interaction graph-transformer architecture to learn
both short-range and long-range correlations among different image patches
within different resolutions. considering that the information from different
resolutions is complementary and can benefit each other, we specially design a
bidirectional interaction block in our hierarchical interaction vit mod- ule to
establish communication between different resolution levels. moreover, a fusion
block is proposed to aggregate features learned from the different levels for
slide-level prediction. to reduce the tremendous computation and memory cost, we
further adopt the efficient pooling operation after the hierarchical gnn part to
reduce the number of tokens and introduce the separable self-attention mechanism
in hierarchical interaction vit modules to reduce the computation burden. the
extensive experiments with promising results on two public wsi datasets from
tcga projects, i.e., kidney carcinoma (kica) and esophageal carcinoma (esca),
validate the effectiveness and efficiency of our framework on both tumor
subtyping and staging tasks. the codes are available at https://
github.com/hku-medai/higt.
figure 1 depicts the pipeline of higt framework for better exploring the
multiscale information in hierarchical wsi pyramids. first, we abstract each wsi
as a hierarchical graph, where the feature embeddings extracted from
multiresolution patches serve as nodes and the edge denotes the spatial and
scaling relationships of patches within and across different resolution levels.
then, we feed the constructed graph into several hierarchical graph convolution
blocks to learn the short-range relationship among graph nodes, following
pooling operations to aggregate local context and reduce the number of nodes. we
further devise a separable self-attention-based hierarchical interaction
transformer architecture equipped with a novel bidirectional interaction block
to learn the long-range relationship among graph nodes. finally, we design a
fusion block to aggregate the features learned from the different levels of wsi
pyramids for final slide-level prediction.
as shown in fig. 1, a wsi is cropped into numerous non-overlapping 512 × 512
image patches under different magnifications (i.e., ×5, ×10) by using a sliding
window strategy, where the otsu algorithm [4] is used to filter out the
background patches. afterwards, we employ a pre-trained kimianet [16] to extract
the feature embedding of each image patch. the feature embeddings of the
slide-level t (thumbnail), region-level r (×5), and the patch-level p (×10) can
be represented as,where t, r i , p i,j ∈ r 1×c correspond to the feature
embeddings of each patch in thumbnail, region, and patch levels, respectively. n
is the total number of the region nodes and m is the number of patch nodes
belonging to a certain region node, and c denotes the dimension of feature
embedding (1,024 in our experiments). based on the extracted feature embeddings,
we construct a hierarchical graph to characterize the wsi, following previous h
2 -mil work [7]. specifically, the cropped patches serve as the nodes of the
graph and we employ the extracted feature embedding as the node embeddings.
there are two kinds of edges in the graph: spatial edges to denote the
8-adjacent spatial relationships among different patches in the same levels, and
scaling edges to denote the relationship between patches across different levels
at the same location.
to learn the short-range relationship among different patches within the wsi
pyramid, we propose a new hierarchical graph message propagation operation,
called raconv+. specifically, for any source node j in the hierarchical graph,
we define the set of it all neighboring nodes at resolution k as n k and k ∈
k.here k means all resolutions. and the h k is the mean embedding of the node
j's neighboring nodes in resolution k. and h j is the embedding of the
neighboring nodes of node j in resolution k and h j ∈ n k . the formula for
calculating the attention score of node j in resolution-level and
node-level:where α j,j is the attention score of the node j to node j and h j is
the source node j embedding. and u , v , a and b are four learnable layers. the
main difference from h2-mil [6] is that we pose the non-linear leakyrelu between
a and u , b and v , to generate a more distinct attention score matrix which
increases the feature differences between different types of nodes [2].
therefore, the layer-wise graph message propagation can be represented as:where
a represents the attention score matrix, and the attention score for the j-th
row and j -th column of the matrix is given by eq. ( 2). at the end of the
hierarchical gnn part, we use the ihpool [6] progressively aggregate the
hierarchical graph.
we further propose a hierarchical interaction vit (hivit) to learn long-range
correlation within the wsi pyramids, which includes three key components:
patch-level (pl) blocks, bidirectional interaction (bi) blocks, and region-level
(rl) blocks.
given the patch-level feature set p = n i=1 p i , the pl block learns long-term
relationships within the patch level:where l = 1, 2, ..., l is the index of the
hivit block. p l(•) includes a separable self attention (ssa) [13], 1×1
convolution, and layer normalization in sequence. note that here we introduced
ssa into the pl block to reduce the computation complexity of attention
calculation from quadratic to linear while maintaining the performance
[13].bidirectional interaction block. we propose a bidirectional interaction
(bi) block to establish communication between different levels within the wsi
pyramids. the bi block performs bidirectional interaction, and the interaction
progress from region nodes to patch nodes is:where the se(•) means the
sequeeze-and-excite layer [8] and the r l i means the i-th region node in r l ,
and pl+1 i,k is the k-th patch node linked to the i-th region node after the
interaction. besides, another direction of the interaction is,where the mean(•)
is the operation to get the mean value of patch nodes set p l+1 i associated
with the i-th region node and p l+1 1 ∈ r 1×c and the c is the feature channel
of nodes, and rl+1 is the region nodes set after interaction.region-level block.
the final part of this module is to learn the long-range correlations of the
interacted region-level nodes:where l = 1, 2, ..., l is the index of the hivit
module,and rl(•) has a similar structure to p l(•).
in the final stage of our framework, we design a fusion block to combine the
coarse-grained and fine-grained features learned from the wsi pyramids.
specifically, we use an element-wise summation operation to fuse the
coarse-grained thumbnail feature and patch-level features from the hierarchical
interaction gnn part, and then further fuse the fine-grained patch-level
features from the hivit part with a concatenation operation. finally, a 1 × 1
convolution and mean operation followed by a linear projection are employed to
produce the slide-level prediction.
datasets and evaluation metrics. we assess the efficacy of the proposed higt
framework by testing it on two publicly available datasets (kica and esca) from
the cancer genome atlas (tcga) repository. the datasets are described below in
more detail:-kica dataset. the kica dataset consists of 371 cases of kidney
carcinoma, of which 279 are classified as early-stage and 92 as late-stage. for
the tumor typing task, 259 cases are diagnosed as kidney renal papillary cell
carcinoma, while 112 cases are diagnosed as kidney chromophobe. -esca dataset.
the esca dataset comprises 161 cases of esophageal carcinoma, with 96 cases
classified as early-stage and 65 as late-stage. for the tumor typing task, there
are 67 squamous cell carcinoma cases and 94 adenocarcinoma cases.experimental
setup. the proposed framework was implemented by pytorch [14] and pytorch
geometric [5]. all experiments were conducted on a workstation with eight nvidia
geforce rtx 3090 (24 gb) gpus. the shape of all nodes' features extracted by
kimianet is set to 1 × 1024. all methods are trained with a batch size of 8 for
50 epochs. the learning rate was set as 0.0005, with adam optimizer. the
accuracy (acc) and area under the curve (auc) are used as the evaluation metric.
all approaches were evaluated with five-fold cross-validations (5-fold cvs) from
five different initializations. comparison with state-of-the-art methods. we
first compared our proposed higt framework with two groups of state-of-the-art
wsi analysis methods: (1) non-hierarchical methods including: abmil [9], clam-sb
[12], deep-attnmil [18], ds-mil [11], la-mil [15], and (2) hierarchical methods
including: h2-mil [7], hipt [3]. for la-mil [15] method, it was introduced with
a single-scale graph-transformer architecture. for h2-mil [7] and hipt [3], they
were introduced with a hierarchical graph neural network and hierarchical
transformer architecture, respectively. the results for esca and kica datasets
are summarized in table 1 and table 2, respectively. overall, our model achieves
a content result both in auc and acc of classifying the wsi, and especially in
predicting the more complex task (i.e. staging) compared with the sota
approaches. even for the non-hierarchical graph-transformer baseline la-mil and
hierarchical transformer model hipt, our model approaches at least around 3% and
2% improvement on auc and acc in the classification of the staging of the kica
dataset. therefore we believe that our model benefits a lot from its used
modules and mechanisms.ablation analysis. we further conduct an ablation study
to demonstrate the effectiveness of the proposed components. the results are
shown in table 3. in its first row, we replace the raconv+ with the original
version of this operation. and in the second row, we replace the separable self
attention with a canonical transformer block. the third row changes the
bidirectional interaction mechanism into just one direction from region-level to
patch-level. and the last row, we remove the fusion block from our model.
finally, the ablation analysis results show that all of these modules we used
actually improved the prediction effect of the model to a certain extent.
computation cost analysis. we analyze the computation cost during the
experiments to compare the efficiency between our methods and existing
state-ofthe-art approaches. besides we visualized the model size (mb) and the
training memory allocation of gpu (gb) v.s. performance in kica's typing and
staging task plots in fig. 2. all results demonstrate that our model is able to
maintain the promising prediction result while reducing the computational cost
and model size effectively.
in this paper, we propose higt, a framework that simultaneously and effectively
captures local and global information from the hierarchical wsi. firstly, the
constructed hierarchical data structure of the multi-resolution wsi is able to
offer multi-scale information to the later model. moreover, the redesigned
h2-mil and hivit capture the short-range and long-range correlations among
varying magnifications of wsi separately. and the bidirectional interaction
mechanism and fusion block can facilitate communication between different levels
in the transformer part. we use ihpool and apply the separable self attention to
deal with the inherently high computational cost of the graph-transformer model.
extensive experimentation on two public wsi datasets demonstrates the
effectiveness and efficiency of our designed framework, yielding promising
results. in the future, we will evaluate on other complex tasks such as survival
prediction and investigate other techniques to improve the efficiency of our
framework.
. 2. computational analysis of our framework and some selected sota methods.
from left to right are scatter plots of typing auc v.s. gpu memory allocation,
staging auc v.s. gpu memory allocation, typing auc v.s. model size, staging auc
v.s. model size.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 73.
image classification is a significant challenge in medical image analysis.
although some classification methods achieve promising performance on balanced
and clean medical datasets, balanced datasets with high-accuracy annotations are
time-consuming and expensive. besides, pruning clean and balanced datasets
require a large amount of crucial clinical data, which is insufficient for
large-scale deep learning. therefore, we focus on a more practical yet
unexplored setting for handling imbalanced medical data with noisy labels,
utilizing all available lowcost data with possible noisy annotations. noisy
imbalanced datasets arise due to the lack of high-quality annotations [11] and
skewed data distributions [18] where the number of instances largely varies
across different classes. besides, the class hardness problem where
classification difficulties vary for different categories presents another
challenge in removing label noise. due to differences in disease epidemicity and
collection difficulty, rare anomalies or anatomical features render diseases
with low epidemicity easier to detect. however, existing techniques [12,23,24]
fail to jointly address these scenarios, leading to inadequate classification
outcomes. therefore, noisy-labeled, imbalanced datasets with various class
hardness remain a persistent challenge in medical classification.existing
approaches for non-ideal medical image classification can be summarized into
noisy classification, imbalanced recognition, and noisy imbalanced
identification. noisy classification approaches [3,7,23] conduct noise-invariant
learning depending on the big-loss hypothesis, where classifiers trained with
clean data with lower empirical loss aid with de-noising identification.
however, imbalanced data creates different confidence distributions of clean and
noisy data in the majority class and minority class as shown in fig. 1, which
invalidates the big-loss assumption [3,4]. imbalanced recognition approaches
[9,15,21] utilize augmented embeddings and imbalance-invariant training loss to
re-balance the long-tailed medical data artificially, but the disturbance from
noisy labels leads to uncasual feature learning, impeding the recognition of
tail classes. noisy longtailed identification technique [25] has achieved
promising results by addressing noise and imbalance concerns sequentially.
however, the class hardness problem leads to vague decision boundaries that
hinders accurate' noise identification.in this work, we propose a multi-stage
noise removal framework to address these concerns jointly. the main
contributions of our work include: 1) we decompose the negative effects in
practical medical image classification, 2) we minimize the invariant risk to
tackle noise identification influenced by multiple factors, enabling the
classifier to learn causal features and be distribution-invariant, 3) a
re-scaling class-aware gaussian mixture modeling (cgmm) approach is proposed to
distinguish noise labels under various class hardness, 4) we evaluate our method
on two medical image datasets, and conduct thorough ablation studies to
demonstrate our approach's effectiveness.
in the noisy imbalanced classification setting, we denote a medical dataset as
{(x i , y i )} n i=1 where y i is the corresponding label of data x i and n is
the total amount of instances. here y i may be noisy. further, we split the
dataset according to class categories. then, we have {d j } m j=1 , where m is
the number of classes; d j denotes the subset for class j. in each subset
containing n j samples, the data pairs are expressed as {(x j i , y j i )} nj
i=1 . without loss of generality, we order the classes as n 1 > n 2 > ... > n m
-1 > n m . further, we denote the backbone as h(•; θ), x → z mapping data
manifold to the latent manifold, the classifier head as g(•; γ), z → c linking
latent space to the category logit space, and the identifier as f(•; φ), z → c.
we aim to train a robust medical image classification model composed of a
representation backbone and a classifier head on label noise and imbalance
distribution, resulting in a minimized loss on the testing dataset:
we decompose the non-linear mapping p(y = c|x) as a product of two space
mappings p g (y = c|z) • p h (z|x). given that backbone mapping is independent
of noisy imbalanced effects, we conduct further disentanglement by defining e as
the negative effects and p as constant for fixed probability mappings:the
induction derives from the assumption that the incorrect mapping p g (y = c|z,
e) conditions on both pure latent to logits mapping p g (y = c|z) and adverse
effects p g (y = c|e). by bayes theorem, we decompose the effect into imbalance,
noise, and mode (hardness), where the noise effect depends on skew distribution
and hardness effect; and the hardness effect is noise-invariant. currently,
noise removal methods only address pure noise effects (p g (e n |y = c)), while
imbalance recognition methods can only resolve imbalanced distribution, which
hinders the co-removal of adverse influences. furthermore, the impact of
hardness effects has not been considered in previous studies, which adds an
extra dimension to noise removal. in essence, the fundamental idea of noisy
classification involves utilizing clean data for classifier training, which
determines the importance of noise identification and removal. to address these
issues, we propose a mapping correction approach that combines independent noise
detection and removal techniques to identify and remove noise effectively.
traditional learning with noisy label methods mainly minimize empirical risk on
training data. however, they fail to consider the influence of imbalanced
distributions, which might cause a biased gradient direction on the optimization
subspace. following [25], we minimize the invariant risk [2] across
multi-environment for independent detector learning. by assuming that the robust
classifier performs well on every data distribution, we solve the optimizing
object by finding the optima to reduce the averaged distance for gradient shift:
minwhere ε represents an environment (distribution) for classifier f φ and
backbone h θ ; and l denotes the empirical loss for classification. since the
incorrect mapping is not caused by feature representation, the backbone h θ is
fixed during the optimization. by transferring the constraints into a penalty in
the optimizing object, we solve this problem by learning the constraint scale ω
[2]:ideally, the noise removal process is distribution-invariant if data is
uniformly distributed w.r.t. classes. by the law of large numbers, all
constructed distributions should be symmetric according to the balanced
distribution to obtain a uniform expectation. to simplify this assumption, we
construct three different data distributions [25] composed of one uniform
distribution and two symmetric skewed distributions instead of theoretical
settings. in practice, all environments are established from the training set
with the same class categories.
existing noise labels learning methods [1,13] cluster all sample loss or
confidence scores with beta mixture model or gaussian mixture model into noisy
and clean distributions. from the perspective of clustering, definite and
immense gaps between two congregate groups contribute to more accurate
decisions. however, in medical image analysis, an overlooked mismatch exists
between class hardness and difficulty in noise identification. this results in
ineffectiveness of global cluster methods in detecting label noises across all
categories. to resolve the challenge, we propose a novel method called rescaling
class-aware gaussian mixture modeling (rcgm) which clusters each category data
independently by fitting confidence scores q ij from ith class into two gaussian
distributions as p n i (x n |μ n , σ n ) and p c i (x c |μ c , σ c ). the mixed
gaussian p m i (•) is obtained by linear combinations α ik for each
distribution:which produces more accurate and independent measurements of label
quality. rather than relying on the assumption that confidence distributions of
training samples depend solely on their label quality, rcgm solves the effect of
class hardness in noisy detection by individually clustering the scores in each
category. this overcomes the limitations of global clustering methods and
significantly enhances the accuracy of noise identification even when class
hardness varies.instead of assigning a hard label to the potential noisy data as
[8] which also employs a class-specific gmm to cluster the uncertainty, we
further re-scale the confidence score of class-wise noisy data. let x ij be the
jth in class i, then its probability of having a clean label is:which is then
multiplied by a hyperparameter s if the instance is predicted as noise to reduce
its weight in the finetuning. with a pre-defined noise selection threshold as τ
, we have the final clean score as:
in contrast to two-stage noise removal and imbalance classification techniques,
our approach applies a multi-stage protocol: warm-up phases, noise removal
phases, and fine-tuning phases as shown in fig. 2. in the warm-up stage, we
train backbone h and classifier g a few epochs by assuming that g only remembers
clean images with less empirical loss. in the noise removal phases, we learn
classinvariant probability distributions of noisy-label effect with mer and
remove class hardness impact with rcgm. finally, in the fine-tuning phases, we
apply mixup technique [13,25,26] to rebuild a hybrid distribution from noisy
pairs and clean pairs by:where α kl := v(x k ) v(x l ) denotes the balanced
scale; and {(x kl , ŷkl )} are the mixed clean data for classifier fine-tuning.
sqrt sampler is applied to re-balance the data, and cross-stage kl [12] and ce
loss are the fine-tuning loss functions.
we evaluated our approach on two medical image datasets with imbalanced class
distributions and noisy labels. the first dataset, ham10000 [22], is a
dermatoscopic image dataset for skin-lesion classification with 10,015 images
divided into seven categories. it contains a training set with 7,007 images, a
validation set with 1,003 images, and a testing set with 2,005 images. following
the previous noisy label settings [25], we add 20% noise to its training set by
randomly flipping labels. the second dataset, chaoyang [29], is a histopathology
image dataset manually annotated into four cancer categories by three
pathological experts, with 40% of training samples having inconsistent
annotations from the experts. to emulate imbalanced scenarios, we prune the
class sizes of the training set into an imbalanced distribution as [5].
consequently, chaoyang dataset consists of a training set with 2,181 images, a
validation set with 713 images, and a testing set with 1,426 images, where the
validation and testing sets have clean labels. the imbalanced ratios [12] of
ham10000 and chaoyang are 59 and 20, respectively. the evaluation metrics are
macro-f1, b-acc, and mcc.
we mainly follow the training settings of fcd [12]. resnet-18 pretrained on the
imagenet is the backbone. the batch size is 48. learning rates are 0.06, 0.001,
0.06 and 0.006 with the cosine schedule for four stages, respectively. we train
our models by sgd optimizer with sharpness-aware term [6] for 90, 90, 90, and 20
epochs. the size of input image is 224 × 224. the scale and threshold in rcgm
are 0.6 and 0.1, respectively.
we compare our model with state-of-the-art methods which contain noisy methods
(including dividemix [13], nl [16], gce [27], co-learning [19]), imbalance
methods (including focal loss [14], sqrt-rs [17], pg-rs [10], cb-focal [5], eql
[21], eql v2 [20], cece [5], clas [28], fcd [12]), and noisy imbalanced
classification methods (including h2e [25], nl+sqrt-rs, gce+sqrt-rs, gce+focal).
we train all approaches under the same data augmentations and network
architecture.
as shown in fig. 3, we evaluate the effectiveness of the components in our
method by decomposing them on extensive experiments. we choose the first stage
of fcd [12] as our baseline. figure 3a and 3b show that only using mer or rcgm
achieves better performance than our strong baseline on both datasets. for
example, mer achieves 5.37% and 1.15% improvements on ham10000 and chaoyang,
respectively, demonstrating the effectiveness of our noise removal techniques.
further, our multi-stage noise removal technique outperforms single mer and
rcgm, revealing that the decomposition for noise effect and hardness effect
works on noisy imbalanced datasets. we find that the combination of mer and rcgm
improves more on chaoyang dataset. this is because chaoyang has more possible
label noise than ham10000 caused by the high annotating procedure. from fig. 3c,
we observe the accuracy trends are as the scale increases and achieve the peak
around 0.6. it indicates the re-scaling process for noise weight deduction
contributes to balancing the feature learning and classification boundary
disturbance from the mixture of noisy and clean data. furthermore, similar
performance trends reveal the robustness of scale s.
we propose a multi-step framework for noisy long-imbalanced medical image
classification. we address three practical adverse effects including data noise,
imbalanced distribution, and class hardness. to solve these difficulties, we
conduct multi-environment risk minimization (mer) and rescaling class-aware
gaussian mixture modeling (rcgm) together for robust feature learning.extensive
results on two public medical image datasets have verified that our framework
works on the noisy imbalanced classification problem. the main limitation of our
work is the manually designed multi-stage training protocol which lacks
simplicity compared to end-to-end training and warrants future simplification.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 30.
colorectal cancer is the third most common malignant tumor, and nearly half of
all patients with colorectal cancer develop liver metastasis during the course
of the disease [6,16]. liver metastases after surgery of colorectal cancer is
the major cause of disease-related death. colorectal cancer liver metastases
(crlm) have therefore become one of the major focuses in the medical field.
patients with colorectal cancer typically undergo contrast-enhanced computed
tomography (cect) scans multiple times during follow-up visits after surgery for
early detection of crlm, generating a 5d dataset. in addition to the axial,
sagittal, and coronal planes in 3d ct scans, the data comprises
contrast-enhanced multiple phases as its 4th dimension, along with different
timestamps as its 5th dimension. radiologists heavily rely on this data to
detect the crlm in the very early stage [15].extensive existing works have
demonstrated the power of deep learning on various spatial-temporal data, and
can potentially be applied towards the problem of crlm. for example, originally
designed for natural data, several mainstream models such as e3d-lstm [12],
convlstm [11] and predrnn [13] use convolutional neural networks (cnn) to
capture spatial features and long short-term memory (lstm) to process temporal
features. some other models, such as simvp [4], replace lstms with cnns but
still have the capability of processing spatiotemporal information. these models
can be adapted for classification tasks with the use of proper classification
head.however, all these methods have only demonstrated their effectiveness
towards 3d/4d data (i.e., time-series 2d/3d images), and it is not clear how to
best extend them to work with the 5d cect data. part of the reason is due to the
lack of public availability of such data. when extending these models towards 5d
cect data, some decisions need to be made, for example: 1) what is the most
effective way to incorporate the phase information? simply concatenating
different phases together may not be the optimal choice, because the positional
information of the same ct slice in different phases would be lost.2) shall we
use uni-directional lstm or bi-direction lstm? e3d-lstm [12] shows
uni-directional lstm works well on natural videos while several other works show
bi-directional lstm is needed in certain medical image segmentation tasks
[2,7].in this paper, we investigate how state-of-art deep learning models can be
applied to the crlm prediction task using our 5d cect dataset. we evaluate the
effectiveness of bi-directional lstm and explore the possible method of
incorporating different phases in the cect dataset. specifically, we show that
the best prediction accuracy can be achieved by enhancing e3d-lstm [12] with a
bi-directional lstm and a multi-plane structure. when patients undergo cect
scans to detect crlm, typically three phases are captured: the unenhanced plain
scan phase (p), the portal venous phase (v), and the arterial phase (a). the p
phase provides the basic shape of the liver tissue, while the v and a phases
provide additional information on the liver's normal and abnormal blood vessel
patterns, respectively [10]. professional radiologists often combine the a and v
phases to determine the existence of metastases since blood in the liver is
supplied by both portal venous and arterial routes.
our dataset follows specific inclusion criteria:-no tumor appears on the ct
scans. that means patients have not been diagnosed as crlm when they took the
scans.-patients were previously diagnosed with colorectal cancer tnm stage i to
stage iii, and recovered from colorectal radical surgery. -patients have two or
more times of cect scans.-we already determined whether or not the patients had
liver metastases within 2 years after the surgery, and manually labeled the
dataset based on this. -no potential focal infection in the liver before the
colorectal radical surgery.-no metastases in other organs before the liver
metastases.-no other malignant tumors.our retrospective dataset includes two
cohorts from two hospitals. the first cohort consists of 201 patients and the
second cohort includes 68 patients. each scan contains three phases and 100 to
200 ct slices with a resolution of 512×512. patients may have different numbers
of ct scans, ranging from 2 to 6, depending on the number of follow-up visits.
ct images are collected with the following acquisition parameters: window width
150, window level 50, radiation dose 120 kv, slice thickness 1 mm, and slice gap
0.8 mm. all images underwent manual quality control to exclude any scans with
noticeable artifacts or blurriness and to verify the completeness of all slices.
additional statistics on our dataset are presented in table 1 and examples of
representative images are shown in fig. 1. the dataset is available upon
request.
numerous state-of-the-art deep learning models are available to effectively
process 4d data. in this paper, we will evaluate some of the most popular
ones:1) saconvlstm, introduced by lin et al. [9], incorporates the
self-attention mechanism into the convlstm [11] structure, which improves the
ability to capture spatiotemporal correlations compared to traditional lstm. 2)
e3d-lstm, introduced by wang et al. [12], integrates 3d cnns into lstm cells to
capture both short-and long-term temporal relations. they used 3d-cnns to handle
the 3d data at each timestamp and lstms to compute information at different
timestamps. 3) predrnn-v2, introduced by wang et al. [13,14], uses
spatiotemporal lstm (st-lstm) by stacking multiple convlstm units and connecting
them in a zigzag pattern to handle spatiotemporal data of 4 dimensions. 4) simvp
[4], introduced by gao et al., uses cnn as the translator instead of lstm.all of
these models need to be modified to handle 5d cect datasets. a straightforward
way to extend them is simply concatenating the a phase and v phase together,
thus collapsing the 5d dataset to 4d. however, such an extension may not be the
best way to incorporate the 5d spatiotemporal information, because the
positional information of the same ct slice in different phases would be lost.
below we explore an alternative modification multi-plane bi-directional lstm
(mpbd-lstm), based on e3d-lstm, to handle the 5d data.
the most basic building block in mpbd-lstm is the 3d-lstm modules. each 3d-lstm
module is composed of two e3d-lstm cells [12]. additionally, inspired by the
bi-directional lstm used in medical image segmentation task [2], we replace the
uni-directional connections with bidirectional connections by using the backward
pass in the 2nd e3d-lstm cell in each 3d-lstm module. this allows us to further
jointly compute information from different timestamps and gives us more accurate
modeling of temporal dynamics. the inner structure of one such module is shown
in fig. 2(b). aside from the two e3d-lstm cells, it also includes an output gate
σ. each 3d-lstm module will generate an output y v,t , which can be calculated
as [3]:where -→ h v,t and ←h v,t are the output hidden state of the forward pass
and backward pass of phase v at timestamp t, and σ is the function which is used
to combine these two outputs, which we choose to use a summation function to get
the summation product of these two hidden states. therefore, the output of the
bi-directional lstm module presented in fig. 2(b) can be represented as:in which
⊕ stands for summation. after this, the output y v,t0 is passed into the
bi-directional lstm module in the next layer and viewed as input for this
module.figure 2(a) illustrates how mpbd-lstm uses these 3d-lstm building blocks
to handle the multiple phases in our ct scan dataset. we use two planes, one for
the a phase and one for the v phase, each of which is based on a backbone of
e3d-lstm [12] with the same hyperparameters. we first use three 3d-cnn encoders
(not displayed in fig. 2(a)) as introduced in e3d-lstm to extract the features.
each encoder is followed by a 3d-lstm stack (the "columns") that processes the
spatiotemporal data for each timestamp. the stacks are bidirectionally
connected, as we described earlier, and consist of two layers of 3d-lstm modules
that are connected by their hidden states. when the spatiotemporal dataset
enters the model, it is divided into smaller groups based on timestamps and
phases. the 3d-lstm stacks process these groups in parallel, ensuring that the
ct slices from different phases are processed independently and in order,
preserving the positional information. after the computation of the 3d-lstm
modules in each plane, we use an average function to combine the output hidden
states from both planes.an alternative approach is to additionally connect two
planes by combining the hidden states of 3d-lstm modules and taking their
average if a module receives two inputs. however, we found that such design
actually resulted in a worse performance. this issue will be demonstrated and
discussed later in the ablation study.in summary, the mpbd-lstm model comprises
two planes, each of which contains three 3d-lstm stacks with two modules in each
stack. it modifies e3d-lstm by using bi-directional connected lstms to enhance
communication between different timestamps, and a multi-plane structure to
simultaneously process multiple phases.
we selected 170 patients who underwent three or more cect scans from our
original dataset, and cropped the images to only include the liver area, as
shown in fig. 1. among these cases, we identified 49 positive cases and 121
negative cases. to handle the imbalanced training dataset, we selected and
duplicated 60% of positive cases and 20% of negative cases by applying standard
scale jittering (ssj) [5]. for data augmentation, we randomly rotated the images
from -30 • to 30 • and employed mixup [17]. we applied the same augmentation
technique consistently to all phases and timestamps of each patient's data. we
also used spline interpolated zoom (siz) [18] to uniformly select 64 slices. for
each slice, the dimension was 256 × 256 after cropping. we used the a and v
phases of cect for our crlm prediction task since the p phase is only relevant
when tumors are significantly present, which is not the case in our dataset. the
dimension of our final input is (3 × 2 × 64 × 64 × 64), representing (t × p × d
× h × w ), where t is the number of timestamps, p is the number of different
phases, d is the slice depth, h is the height, and w is the width.
as the data size is limited, 10-fold cross-validation is adopted, and the ratio
of training and testing dataset is 0.9 and 0.1, respectively. adam optimizer [8]
and binary cross entropy loss function [1] are used for network training. for
mpbd-lstm, due to gpu memory constraints, we set the batch size to one and the
number of hidden units in lstm cells to 16, and trained the model till converge
with a learning rate of 5e-4. each training process required approximately 23 gb
of memory and took about 20 h on an nvidia titan rtx gpu. we ran the 10 folds in
parallel on five separate gpus, which allowed us to complete the entire training
process in approximately 40 h. we also evaluated e3d-lstm [12], predrnn-v2 [14],
saconvlstm [9], and simvp [4]. as this is a classification task, we evaluate all
models' performance by their auc scores. table 2 shows the auc scores of all
models tested on our dataset. additional data on accuracy, sensitivity
specificity, etc. can be found in the supplementary material. the mpbd-lstm
model outperforms all other models with an auc score of 0.790. notably, simvp
[4] is the only cnn-based model we tested, while all other models are
lstm-based. our results suggest that lstm networks are more effective in
handling temporal features for our problem compared with cnn-based models.
furthermore, predrnn-v2 [14], which passes memory flow in a zigzag manner of
bi-directional hierarchies, outperforms the uni-directional lstm-based
saconvlstm [9]. although the architecture of predrnn-v2 is different from
mpbd-lstm, it potentially supports the efficacy of jointly computing
spatiotemporal relations in different timestamps. ablation study on model
structures. as shown in table 3, to evaluate the effectiveness of multi-plane
and bi-directional connections, we performed ablation studies on both
structures. first, we removed the multi-plane structure and concatenated the a
and v phases as input. this produced a one-dimensional bi-directional lstm (fig.
2(a), without the gray plane) with an input dimension of 3 × 128 × 64 × 64,
which is the same as we used on other models. the resulting auc score of 0.774
is lower than the original model's score of 0.790, indicating that computing two
phases in parallel is more effective than simply concatenating them. after this,
we performed an ablation study to assess the effectiveness of the bi-directional
connection. by replacing the bi-directional connection with a uni-directional
connection, the mpbd-lstm model's performance decreased to 0.768 on the original
dataset. this result indicates that the bi-directional connection is crucial for
computing temporal information effectively, and its inclusion is essential for
achieving high performance in mpbd-lstm. also, as mentioned previously, we
initially connected the 3d-lstm modules in two planes with their hidden states.
however, as shown in table 3, we observed that inter-plane connections actually
decreased our auc score to 0.786 compared to 0.790 without the connections. this
may be due to the fact that when taking ct scans with contrast, different phases
have a distinct focus, showing different blood vessels as seen in fig. 1.
connecting them with hidden states in the early layers could disrupt feature
extraction for the current phase. therefore, we removed the inter-plane
connections in the early stage, since their hidden states are still added
together and averaged after they are processed by the lstm layers. ablation
study on timestamps and phases. we conducted ablation studies using ct images
from different timestamps and phases to evaluate the effectiveness of
time-series data and multi-phase data. the results, as shown in table 4,
indicate that mpbd-lstm achieves auc scores of 0.660, 0.676, and 0.709 if only
images from timestamps t0, t1, and t2 are used, respectively. these scores
suggest that predicting crlm at earlier stages is more challenging since the
features about potential metastases in ct images get more significant over time.
however, all of these scores are significantly lower than the result using ct
images from all timestamps. this confirms the effectiveness of using a
time-series predictive model. additionally, mpbd-lstm obtains auc scores of
0.653 and 0.752 on single a and v phases, respectively. these results suggest
that the v phase is more effective when predicting crlm, which is consistent
with medical knowledge [15]. however, both of these scores are lower than the
result of combining two phases, indicating that a multi-phase approach is more
useful.
error analysis. in fig. 1, patients b and c are diagnosed with positive crlm
later. mpbd-lstm correctly yields a positive prediction for patient b with a
confidence of 0.82, but incorrectly yields a negative prediction for patient c
with a confidence of 0.77. with similar confidence in the two cases, the error
is likely due to the relatively smaller liver size of patient c. beyond this
case, we find that small liver size is also present in most of the false
negative cases. a possible explanation would be that smaller liver may provide
less information for accurate prediction of crlm. how to effectively address
inter-patient variability in the dataset, perhaps by better fusing the 5d
features, requires further research from the community in the future.
in this paper, we put forward a 5d cect dataset for crlm prediction. based on
the popular e3d-lstm model, we established mpbd-lstm model by replacing the
uni-directional connection with the bi-directional connection to better capture
the temporal information in the cect dataset. moreover, we used a multiplane
structure to incorporate the additional phase dimension. mpbd-lstm achieves the
highest auc score of 0.790 among state-of-the-art approaches. further research
is still needed to improve the auc.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 37.
accurate diagnosis plays an important role in achieving the best treatment
outcomes for people with cancer [1]. identification of cancer biomarkers permits
more granular classification of tumors, leading to better diagnosis, prognosis,
and treatment decisions [2,3]. for many cancers, clinically reliable genomic,
molecular, or imaging biomarkers have not been identified and biomarker
identification techniques (e.g., fluorescence in situ hybridization) have
limitations that can restrict their clinical use. on the other hand,
histological analysis of hematoxylin and eosin (h&e)-stained pathology slides is
widely used in cancer diagnosis and prognosis. however, visual examination of
h&e-stained slides is insufficient for classification of some tumors because
identifying morphological differences between molecularly defined subtypes is
beyond the limit of human detection.the introduction of digital pathology (dp)
has enabled application of machine learning approaches to extract otherwise
inaccessible diagnostic and prognostic information from h&e-stained whole slide
images (wsis) [4,5]. current deep learning approaches to wsi analysis typically
operate at three different histopathological scales: whole slidelevel,
region-level, and cell-level [4]. although cell-level analysis has the potential
to produce more detailed and explainable data, it can be limited by the
unavailability of sufficiently annotated training data. to overcome this
problem, weakly supervised and multiple instance learning (mil) based approaches
have been applied to numerous wsi classification tasks [6][7][8][9][10].
however, many of these models use embeddings derived from tiles extracted using
pretrained networks, and these often fail to capture useful information from
individual cells. here we describe a new embedding extraction method that
combines tile-level embeddings with a cell-level embedding summary. our new
method achieved better performance on wsi classification tasks and had a greater
level of explainability than models that used only tile-level embeddings.
transfer learning using backbones pretrained on natural images is a common
method that addresses the challenge of using data sets that largely lack
annotation. however, using backbones pretrained on natural images is not optimal
for classification of clinical images [11]. therefore, to enable the use of
large unlabeled clinical imaging data sets, as the backbone of our neural
network we used a resnet50 model [12]. the backbone was trained with the
bootstrap your own latent (byol) method [13] using four publicly available data
sets from the cancer genome atlas (tcga) and three data sets from private
vendors that included healthy and malignant tissue from a range of organs [14].
following standard practice, we extracted tiles with dimensions of 256 × 256
pixels from wsis (digitized at 40 × magnification) on a spatial grid without
overlap. extracted tiles that contained artifacts were discarded (e.g., tiles
that had an overlap of >10% with background artifacts such blurred areas or pen
markers). we normalized the tiles for stain color using a u-net model for stain
normalization [15] that was trained on a subset of data from one of the medical
centers in the camelyon17 data set to ensure homogeneity of staining [16].to
create the tile-level embeddings, we used the method proposed by [17] to
summarize the convolutional neural network (cnn) features with nonnegative
matrix factorization (nmf) for k = 2 factors. we observed that the feature
activations within the last layer of the network were not aligned with the
cellular content. although these features may still have been predictive, they
were less interpretable, and it was more difficult to know what kind of
information they captured. conversely, we observed that the self-supervised
network captured cellular content and highlighted cells within the tiles (fig.
1). therefore, the tile-level embeddings were extracted after dropping the last
layer (i.e., dropping three bottleneck blocks in resnet50) from the pretrained
model.
tiles extracted from wsis may contain different types of cells, as well as
noncellular tissue such as stroma and blood vessels and nonbiological features
(e.g., glass). celllevel embeddings may be able to extract useful information,
based on the morphological appearance of individual cells, that is valuable for
downstream classification tasks but would otherwise be masked by more dominant
features within tile-level embeddings.we extracted deep cell-level embeddings by
first detecting individual cellular boundaries using stardist [18] and
extracting 32 × 32-pixel image crops centered around each segmented nucleus to
create cell-patch images. we then used the pre-trained resnet50 model to extract
cell-level embeddings in a similar manner to the extraction of the tilelevel
embeddings. since resnet50 has a spatial reduction factor of 32 in the output of
the cnn, the 32 × 32-pixel image had a 1:1 spatial resolution in the output. to
ensure the cell-level embeddings contained features relevant to the cells, prior
to the mean pooling in resnet50 we increased the spatial image resolution to 16
× 16 pixels in the output from the cnn by enlarging the 32 × 32-pixel cell-patch
images to 128 × 128 pixels and skipping the last 4-layers in the network.because
of heterogeneity in the size of cells detected, each 32 × 32-pixel cellpatch
image contained different proportions of cellular and noncellular features.
higher proportions of noncellular features in an image may cause the resultant
embeddings to be dominated by noncellular tissue features or other background
features. therefore, to limit the information used to create the cell-level
embeddings to only cellular features, we removed portions of the cell-patch
images that were outside of the segmented nuclei by setting their pixel values
to black (rgb 0, 0, 0). finally, to prevent the size of individual nuclei or
amount of background in each cell-patch image from dominating over the celllevel
features, we modified the resnet50 global average pooling layer to only average
the features inside the boundary of the segmented nuclei, rather than averaging
across the whole output tensor from the cnn.
to create a combined representation of the tile-level and cell-level embeddings,
we first applied a nuclei segmentation network to each tile. only tiles with ≥
10 cells per tile, excluding any cells which overlapped the tile border, were
included for embedding extraction. for the included tiles, we extracted the
tile-level embeddings as described in sect. 2.1 and for each detected cell we
extracted the cell-level embeddings as described in sect. 2.2. we then
calculated the mean and standard deviation of the vectors of the cell-level
embeddings for each tile and concatenated those to each tile-level embedding.
this resulted in a combined embedding representation with a total size of 1536
pixels (1024 + 256 + 256).in addition to the wsi classification results
presented in the next sections, we also performed experiments to compare the
ability of combined embeddings and tile-level embeddings to predict
nuclei-related features that were manually extracted from the images and to
identify tiles where nuclei had been ablated. the details and results of these
experiments are available in supplementary materials and provide further
evidence of the improved ability to capture cell-level information when using
combined embeddings compared with tile-level embeddings alone.
for each classification task we compared different combinations of tile-level
and celllevel embeddings using a mil framework. we also compared two different
mil architectures to aggregate the embeddings for wsi-level prediction.the first
architecture used an attention-mil (a-mil) network [19] (the code was adapted
from a publicly available implementation [20]). we trained the network with a
0.001 learning rate and tuned the batch size (48 or 96) and bag sample size
(512, 1024, or 2048) for each classification task separately. when comparing the
combined embedding extraction method with the tile-level only embeddings,
parameters were fixed to demonstrate differences in performance without
additional parameter tuning.transformer (xformer) was used as the second mil
architecture [21]. we used three xformer layers, each with eight attention
heads, 512 parameters per token, and 256 parameters in the multi-layer
perceptron layers. the space complexity of the xformer was quadratic with the
number of tokens. while some wsis had up to 100,000 tiles, we found, in
practice, that we could not fit more than 6000 tokens in the memory.
consequently, we used the nyströformer xformer variant [22] since it consumes
less memory (the code was adapted from a publicly available implementation
[23]). this xformer has two outputs, was trained with the adam optimizer [24]
with default parameters, and the loss was weighted with median frequency
balancing [25] to assign a higher weight to the less frequent class. like a-mil,
the batch and bag sample sizes were fixed for each classification task. during
testing a maximum of 30,000 tiles per slide were used. the complete flow for wsi
classification is shown in fig. 2. the models were selected using a validation
set, that was a random sample of 20% of the training data. all training was done
using pytorch version 1.12.1 (pytorch.org) on 8 nvidia tesla v100 gpus with cuda
version 10.2.
we tested our feature representation method in several classification tasks
involving wsis of h&e-stained histopathology slides. the number of slides per
class for each classification task are shown in fig. 3. for breast cancer human
epidermal growth factor receptor 2 (her2) prediction, we used data from the
herohe challenge data set [26]. to enable comparison with previous results we
used the same test data set that was used in the challenge [27]. for prediction
of estrogen receptor (er) status, we used images from the tcga-breast invasive
carcinoma (tcga-brca) data set [28] for which the er status was known.for these
two tasks we used artifact-free tiles from tumor regions detected with an
in-house tumor detection model.for breast cancer metastasis detection in lymph
node tissue, we used wsis of h&estained healthy lymph node tissue and lymph node
tissue with breast cancer metastases from the publicly available camelyon16
challenge data set [16,29]. all artifact-free tissue tiles were used.for cell of
origin (coo) prediction of activated b-cell like (abc) or germinal center b-cell
like (gcb) tumors in diffuse large b-cell lymphoma (dlbcl), we used data from
the phase 3 goya (nct01287741) and phase 2 cavalli (nct02055820) clinical
trials, hereafter referred to as ct1 and ct2, respectively. all slides were
h&e-stained and scanned using ventana dp200 scanners at 40× magnification. ct1
was used for training and testing the classifier and ct2 was used only as an
independent holdout data set. for these data sets we used artifact-free tiles
from regions annotated by expert pathologists to contain tumor tissue.
for the her2 prediction, er prediction, and metastasis detection classification
tasks, combined embeddings outperformed tile-level only embeddings irrespective
of the downstream classifier architecture used (fig. 4). in fact, for the her2
classification task, combined embeddings obtained using the xformer architecture
achieved, to our knowledge, the best performance yet reported on the herohe
challenge data set (area under the receiver operating characteristic curve
[auc], 90%; f1 score, 82%).for coo classification in dlbcl, not only did the
combined embeddings achieve better performance than the tile-level only
embeddings with both the xformer and a-mil architectures (fig. 5) on the ct1
test set and ct2 holdout data set, but they also had a significant advantage
versus tile-only level embeddings in respect of the additional insights they
provided through cell-level model explainability (sect. 4.1).
tile-based approaches in dp often use explainability methods such as
gradient-weighted class activation mapping [30] to highlight parts of the image
that correspond with certain category outputs. while the backbone of our model
was able to highlight individual cells, there was no guaranteed correspondence
between the model activations and the cells. to gain insights into cell-level
patterns that were very difficult or impossible to obtain from tile-level only
embeddings, we applied an explainability method that assigned attention weights
to the cellular average part of the embedding.
where e ij ∈ r 256 is the cellular embedding extracted from every detected cell
in the tile j i ∈ 1, 2, . . . , n j where n j is the number of cells in the tile
j. this can be rewritten as a weighted average of the cellular embeddingswhere w
i ∈ r 256 are the per cell attention weights that if initialized to 0 result in
the original cellular average embedding. the re-formulation does not change the
result of the forward pass since w i are not all equal. note that the weights
are not learned through training but calculated per cell at inference time to
get the per cell contribution. we computed the gradient of the output category
(of the classification method applied on top of the computed embedding) with
respect to the attention weights w i : grad i = ∂score i /∂w i and visualized
cells that received positive and negative gradients using different
colors.visual example results. examples of our cellular explainability method
applied to weakly supervised tumor detection on wsis from the camelyon16 data
set using a-mil are shown in fig. 6. cells with positive attention gradients
shifted the output towards a classification of tumor and are labeled green.
cells with negative attention gradients are labeled red. when reviewed by a
trained pathologist, cells with positive gradients had characteristics
previously associated with breast cancer tumors (e.g., larger nuclei, more
visible nucleoli, differences in size and shape). conversely, negative cells had
denser chromatin and resembled other cell types (e.g., lymphocytes). these
repeatable findings demonstrate the benefit of using cell-level embeddings and
our explainability method to gain a cell-level understanding of both correct and
incorrect slide-level model predictions (fig. 6). we also applied our
explainability method to coo prediction in dlbcl.in this case, cells with
positive attention gradients that shifted the output towards a classification of
gcb were labeled green and cells with negative attention gradients that shifted
the classification towards abc were labeled red. cells with positive attention
gradients were mostly smaller lymphoid cells with low grade morphology or were
normal lymphocytes, whereas cells with negative attention gradients were more
frequently larger lymphoid cells with high grade morphology (fig. 6).
we describe a method to capture both cellular and texture feature
representations from wsis that can be plugged into any mil architecture (e.g.,
cnn or xformer-based), as well as into fully supervised models (e.g., tile
classification models). our method is more flexible than other methods (e.g.,
hierarchical image pyramid transformer) that usually capture the hierarchical
structure in wsis by aggregating features at multiple levels in a complex set of
steps to perform the final classification task. in addition, we describe a
method to explain the output of the classification model that evaluates the
contributions of histologically identifiable cells to the slide-level
classification. tilelevel embeddings result in good performance for detection of
tumor metastases in lymph nodes. however, introducing more cell-level
information, using combined embeddings, resulted in improved classification
performance. in her2 and er prediction tasks for breast cancer we demonstrate
that addition of a cell-level embedding summary to tilelevel embeddings can
boost model performance by up to 8%. finally, for coo prediction in dlbcl and
breast cancer metastasis detection in lymph nodes, we demonstrated the potential
of our explainability method to gain insights into previously unknown
associations between cellular morphology and disease biology.
vulvovaginal candidiasis (vvc) is a type of fungal infection caused by candida,
which results in discomforting symptoms, including itching and burning in the
genital area [4,18]. it is the most prevalent human candidal infection,
estimated to afflict approximately 75% of all women at least once in their
lifetime [1,20], resulting in huge consumption of medical resources. currently,
thin-layer cytology (tct) [6] is one of the main tools for screening cervical
abnormalities. manual reading upon whole slide image (wsi) of tct is
time-consuming and labor-intensive, which limits the efficiency and scale of
disease screening. therefore, automatic computer-aided screening for candida
would be a valuable asset, which is low-cost and effective in the fight against
infection.previous studies for computer-aided vvc diagnosis were mainly based on
pap smears rather than wsis. for example, momenzadeh et al. [11] implemented
automatic diagnosis based on machine learning. peng et al. [13] compared
different cnn models on vvc classification. some works also applied deep
learning to classify candida in other body parts [2,24]. in recent years, tct
has become mainstream in cervical disease screening compared to pap smear [8].
many systems of automatic computer-aided wsi screening have been designed for
cytopathology [22,23], and histopathology [17,21]. however, partially due to the
limited data and annotation, screening for candidiasis is mostly
understudied.computer-aided diagnosis for candidiasis through wsi is highly
challenging (see examples in fig. 1). (1) candida is hard to localize in a large
wsi, especially due to its long-stripe shape, low-contrast appearance, and often
occlusion with respect to nearby cells. the representation of candida is easily
dominated by other objects in deep layers of a network. (2) in addition to
occupying only a small image space for each candida, the overall candida
quantity in wsis is also low compared to the number of other cells. the class
imbalance makes it difficult to conduct discriminative learning and to find
candida. (3) the staining of different samples leads to the huge style gap
between wsis. while collecting more candida data may contribute to a more robust
network, such efforts are dwarfed by the inhomogeneity of wsis, which adds to
the risk of overfitting. all of the above issues make it difficult for
diagnostic models to focus on candida, thus resulting in poor classification
performance and generalization capability.in this paper, we find that the
attention for a deep network to focus on candida is the key to the high
performance of the screening task. and we propose a series of strategies to make
the model focus on candida progressively. our contributions are summarized into
three parts: (1) we use a detection task to pre-train the encoder of the
classification model, moving the network's attention away from individual cells
and onto candida-like objects; (2) we propose skip self-attention (ssa) to take
into account multi-scale semantic and texture fea- tures, improving network
attention to the candida that is severely occluded or with long hyphae; (3)
contrastive learning [3] is applied to alleviate the overfitting risk caused by
the style gap and to improve the ability to discern candida.
we use a hierarchical framework for cervical candida screening, concerning the
huge size of wsi and the infeasibility of handling a wsi scan in one shot. the
overall pipeline of our framework is presented in fig. 2. given a wsi, we first
crop it into multiple images, each of which is sized 1024×1024. for each cropped
image, we conduct image-level classification to find out whether it suffers from
suspicious candida infection. the image-level classifier produces a score and
feature representation of the image under consideration. then scores and
features from all cropped images are reorganized and aggregated by a transformer
for final classification by a fully connected (fc) layer.
we use a pre-trained detection model as prior to initialize the classification
model. in experimental exploration, we find that, if we train the detection
network directly, the bounding-box annotation indicates the location of candida
and can rapidly establish a rough understanding of the morphology of candida.
however, the positioning task coming with detection lacks enough granularity,
resulting in relatively low precision to discern cell edges or folding from
candida. meanwhile, directly training a classification model is usually easier
to converge. however, in such a task, as candida occupies only a few pixels in
an image, it is difficult for the classifier to focus on the target. that, the
attention of the classifier may spread across the entire image, leading to
overfitted training quickly.therefore, we argue that the detection and
classification tasks are complementary to solve our problem. particularly, we
pre-train a detector and inherit its advantages in the classifier. we use
retinanet [10], which is composed of a backbone attached with fpn (feature
pyramid network, fpn, [9]) and a detection head, as shown in (fig. 3). we chose
the same encoder architecture (resnet [5]) for the detection and classification
networks. to train the encoder with the detection task (fig. 3), we use
bounding-box annotations to supervise fig. 3. attention guided image-level
classification (corresponding to the classification model in fig. 2). the same
parameters are used between modules marked "shared".retinanet. we then
initialize the classification network by directly loading the encoder parameters
and freezing the first few layers during the training of the classification
network. note that pre-training not only discards the complex positioning task
but also makes it easier for the classification network to converge especially
in the early stage of training.
we design a novel skip self-attention (ssa) module to fuse discriminative
features of candida from different scales. at a fine-grained level, the hyphae
and spores of candida are usually the basis for judging. yet we need to
distinguish them from easily distorting factors such as contaminants in wsis and
edges of nearby cells. at a coarse-grained level, there is the phenomenon that a
candida usually links multiple host cells and yields a string of them. thus it
is necessary to combine long-range visual cues that span several cells to derive
the decision related to candida.cnn-based methods have achieved excellent
performance in computer-aided diagnosis including cervical cancer [22]. however,
the unique shape and appearance of the candidate incur troubles for cnn-based
classifiers, whose spatial field of view can be relatively limited. in recent
years, vision transformer (vit) has been widely used in visual tasks for its
global attention mechanism [14], sensitivity to shape information in images
[19], and robustness to occlusion [12]. nevertheless, such a transformer can be
hard to train for our task, due to the large image size, huge network
parameters, and huge demand for training data. therefore, to adapt to the shape
and appearance of candida, we propose the ssa module and apply it to vit for
efficient learning. specifically, we use the pre-trained cnn-based encoder to
extract features for each cropped image. the feature maps extracted after the
first layer is considered low-level, which contains fine-grained texture
information. on the contrary, the feature maps extracted from the last layer are
high-level, which represents semantics regarding candida. to combine the low-and
high-level features, we regard the low-level features as queries (q), and the
high-level features as keys (k) and values (v). for each patch in the vit
scheme, the transformer decoder computes the attention between low-and
high-level features and combines them. the class token 'cls' is used for the
final classification. the combined feature maps can offer more representative
information so that the classifier focuses more on different scales to
long-range candida. meanwhile, the extra ssa structure is simple, which causes a
low computation burden.
as mentioned in sect. 1, the style gap is another problem, which makes
overfitting more severe. in this part, we adopt the strategy of contrastive
learning to alleviate such problems and further optimize the attention of the
network. our approach has two key goals: (1) to ensure that the features from
the original image remain consistent after undergoing various image
augmentations, and (2) to construct an image without the region of candida,
resulting in highly dissimilar features compared to the original.inspired by a
weakly supervised learning segmentation method [7], we construct a contrastive
learning method, which will be described in detail in the following sections, as
shown in fig. 3.to achieve this, we use augmentation and the attention map
generated during the training process to construct three types of images and
apply contrastive learning to the features extracted from them. for a given
image i, we use image augmentation to generate i aug and use the encoder
attached with ssa to extract feature, f c aug . the attention map a is
transformed from f c aug by an attention extractor. the attention extractor uses
fc (the same params as that of the classifier) to reduce the channels of
features (except the class token) to 2 (candida and others), then reshape the
features representing candida to a feature map, and applies bilinear
interpolation to upsample it to the same size of i. equation 1 normalizes a to
the interval [0,1], obtaining m to represent the likelihood of candida
distribution. we get the masked image i masked by subtracting m from i.where σ
and s are used to adjust the range of values, set to 0.5 and 10.as shown in fig.
3, the features f , f aug and f masked from the three types of images i, i aug
and i masked by the shared classifier. in our task, we hope that the style gap
does not affect the feature extraction of the image, so the distance between f
aug and f should be attracted. at the same time, we hope that f masked should
not contain the characteristics of candida, which is repelled from f aug . to
achieve our goal, we introduce triplet loss [15] for contrastive learning as
shown in the first part of eq. 2.in addition, we use two constraints, leading to
more stable and robust training. if our network has effective attention, the
masked image should not contain any candida, so the score of the candida
category after the mask s(i masked ) should be minimized. we use the attention
mining loss to handle this, as shown in the second part of eq. 2. additionally,
we need the attention to cover only the partial area around candida, without
false positive regions. otherwise, attention maps that cover the whole image can
also result in low l tri . we take the average grayscale of attention map m as a
restriction, as shown in the last part of eq. 2. l tri ,l am , and l focus are
combined as l cl to constrain each other and take full advantage of contrastive
learning, as shown in eq. 2.finally, we use the cross-entropy loss l ce to
calculate the classification loss with labels. the total loss during training
can be expressed as shown in eq. 3. α is a hyper-parameter, set to 0.1.(3)
with the strategies above, we have built the classifier for all cropped images
in a wsi. then we can finish the pipeline of wsi-level classification, which is
shown as part of fig. 2. specifically, for each cropped image, we conduct
image-level classification to find out whether it suffers from suspicious
candida infection.the image-level classification also produces a score, as well
as the feature representation of the image under consideration. then, we
reorganize features from all images by ranking their scores and preserving that
with top-k scores. we complete the aggregation of the top-k features by the
transformer and make the wsi-level decision by an fc layer in the final.
datasets and experimental setup. our samples were collected by a collaborating
clinical institute in 2021. each sample is scanned into a wsi following standard
cytology protocol, which can be further cropped to 500 images sized
1024×1024.for pre-training the detector, we prepare 1467 images with the size of
1024×1024 pixels, all of which have bounding-box annotations. the ratio of
training and validation is 4:1.for training of the image-level classification
model, we use 1940 positive images (1467 of which are used in detector
pre-training) and 2093 negative images. all images used to pre-train the
detector are categorized as training data here. the rest 473 images are split in
5-fold cross-validation, from which we collect experimental results and report
later. the ratio of training, validation, and testing is 3:1:1. at the wsi
level, we use two datasets. dataset-small is balanced with 100 positive wsis and
100 negative wsis. we conduct a 5-fold cross-validation, and the ratio of
training, validation, and testing is 3:1:1. we further validate upon an
imbalanced dataset-large of 7654 wsis. there are only 140 positive wsis in this
dataset, which is closer to real world. these two wsi-level datasets have no
overlay with the data used to train the above detection and classification
tasks.for implementation details, the models are implemented by pytorch and
trained on 4 nvidia tesla v100s gpus. all parameters are optimized by adam for
100 epochs with the initial learning rate of 3 × 10 -4 . the batch sizes of the
detection task, image-level classification, and wsi-level classification are 8,
8, 16, respectively. to aggregate wsi classification, we use top-10 cropped
images and their features. we report the performance using five common metrics:
area under the receiver operating characteristic curve (auc), accuracy (acc),
sensitivity (sen), specificity (spe), and f1-score.comparisons for image-level
classification. we conduct an ablation study to evaluate the contribution of
pre-training (pt), skip self-attention (ssa), and contrastive learning (cl) for
the image-level classification, as shown in table 1. it is observed that with
all our proposed components, the network reaches the highest auc 97.20, which is
11.49% higher than the baseline. pt shows improvement in all situations, as a
reasonable initial focus provides a solid foundation. ssa and cl can bring 2.89%
and 6.38% improvement respectively compared to the method without each of them.
it shows that ssa and cl can perform better when the model already has the basic
ability to localize candida, i.e., after pt.to verify whether our model focuses
on important regions of the input image for accurate classification, we
visualize the model's attention using grad-cam [16]. we present two examples in
fig. 4. we can see in fig. 4(b) that the baseline's attention is very scattered
spatially. after pt, the model can focus on the candida area, edges of cells,
and folds that resemble candida, as shown in fig. 4(c). after adding the ssa
module, more texture information is used to distinguish with cells, as shown in
fig. 4(d). finally, cl helps the model better narrow its attention, focusing on
the most important part as shown in fig. 4(e). these comparisons demonstrate
that our proposed method effectively guides and corrects the model's
attention.comparisons for wsi-level classification. we compare our proposed
method to other methods in the whole slide of cervical disease screening. to
save computation, we did not verify the performance of the methods that
performed too poorly on dataset-small. the detection-based method [23] uses a
detection network to get suspicious candida and classify wsis with average
confidence. resnet trained without our method is the same as the baseline in
table 1. at the wsi level, we compare our method with traditional classifiers
and a multiinstance learning method transmil [17]. we both considered the
original trans-mil with pre-trained resnet-50 and the modified version with our
image-level encoder. table 2 shows that our method reaches the highest auc of
95.78% and is the most stable. our attention-based method brings 6% improvement
of accuracy on data-small compared to other methods with the same wsi-level
method 'threshold'. transformer shows a better capacity of feature aggregation
than other wsi-level classifiers, raising the auc on dataset-large to 84.18%.
we introduced a novel attention-guided method for vvc screening, which can
progressively correct the attention of the model. we pre-train a detection task
for the initialization, then add ssa to fuse features from coarse and
fine-grained, and finally narrower attention with contrastive learning. after
obtaining accurate attention and good generalization for the image-level
classifier, we reorganized and ensemble features from slices, and make a
diagnosis. both numerical metrics and visualization results show the
effectiveness of our model. in the future, we would like to explore the method
of weakly supervised learning to make use of a huge number of unlabeled images
and jointly train the image-level and wsi-level models.
cervical cancer accounts for 6.6% of the total cancer deaths in females
worldwide, making it a global threat to healthcare [6]. early cytology screening
is highly effective for the prevention and timely treatment of cervical cancer
[23].nowadays, thin-prep cytologic test (tct) [1] is widely used to screen
cervical cancers according to the bethesda system (tbs) rules [21]. typically
there are five types of cervical squamous cells under tct examinations [5],
including normal class or negative for intraepithelial malignancy (nilm),
atypical squamous cells of undetermined significance (asc-us), low-grade
squamous intraepithelial lesion (lsil), atypical squamous cells that cannot
exclude hsil (asc-h), and high-grade squamous intraepithelial lesion (hsil). the
nilm cells have no cytological abnormalities while the others are manifestations
of cervical abnormality to a different extent. by observing cellular features
(e.g., nucleus-cytoplasm ratio) and judging cell types, pathologists can provide
a diagnosis that is critical to the clinical management of cervical
abnormality.after scanning whole-slide images (wsis) from tct samples, automatic
tct screening is highly desired due to the large population versus the limited
number of pathologists. as the wsi data per sample has a huge size, the idea of
identifying abnormal cells in a hierarchical manner has been proposed and
investigated by several studies using deep learning [3,27,31]. in general, these
solutions start with the extraction of suspicious cell patches and then conduct
patch-level classification. the promising performance of cell classification at
the patch level is critical, which contributes to sample-level diagnosis after
integrating outcomes from many patches in a wsi. however, such a patchlevel
classification task requires a large number of annotated training data. and the
efforts in collecting reliably annotated data can hardly be negligible, which
requires high expertise due to the intrinsic difficulty of visually reading
wsis.to alleviate the shortage of sufficient data to supervise classification,
one may adopt traditional data augmentation techniques, which yet may bring
little improvement due to scarcely expanded data diversity [26]. thus,
synthesizing cytopathological images for cervical cells is highly desired to
effectively augment training data. existing literature on pathological image
synthesis has explored the generation of histopathological images [10,28]. in
cytopathological images, on the contrary, cervical cells can be spatially
isolated from each other, or are highly squeezed and even overlapped. the
spatial relationship of individual cells is complex, adding diversity to the
image appearance of color, morphology, texture, etc. in addition, the
differences between cell types are mainly related to nuanced cellular
attributes, thus requiring fine granularity in modulating synthesized images
toward the expected cell types. therefore, the task to synthesize realistic
cytopathological images becomes very challenging.aiming at augmenting the
performance of cervical abnormality screening, we develop a novel conditional
generative adversarial network in this paper, namely cellgan, to synthesize
cytopathological images for various cell types. we leverage fastgan [16] as the
backbone for the sake of training stability and computational efficiency. to
inject cell type for fine-grained conditioning, a non-linear mapping network
embeds the class labels to perform layer-wise feature modulation in the
generator. meanwhile, we introduce the skip-layer global context (sgc) module to
capture the long-range dependency of cells for precisely modeling their spatial
relationship. we adopt an adversarial learning scheme, where the discriminator
is modified in a projection-based way [20] for matching condi- tional data
distribution. to the best of our knowledge, our proposed cellgan is the first
generative model with the capability to synthesize realistic cytopathological
images for various cervical cell types. the experimental results validate the
visual plausibility of cellgan synthesized images, as well as demonstrate their
data augmentation effectiveness on patch-level cell classification.
the dilemma of medical image synthesis lies in the conflict between the limited
availability of medical image data and the high demand for data amount to train
reliable generative models. to ensure the synthesized image quality given
relatively limited training samples, the proposed cellgan is built upon fastgan
[16] towards stabilized and fast training for few-shot image synthesis. by
working in a class-conditional manner, cellgan can explicitly control the
cervical squamous cell types in the synthesized cytopathological images, which
is critical to augment the downstream classification task. the overall
architecture of cellgan is presented in fig. 1, and more detailed structures of
the key components are displayed in supplementary materials.
the generator of cellgan has two input vectors. the first input of the class
label y, which adopts one-hot encoding, provides class-conditional information
to indicate the expected cervical cell type in the synthesized image i syn . the
second input of the 128-dimensional latent vector z represents the remaining
image information, from which i syn is gradually expanded. we stack six upblocks
to form the main branch of the generator.to inject cell class label y into each
upblock, we follow a similar design to stylegan [13]. specifically, the class
label y is first projected to a class embedding c via a non-linear mapping
network, which is implemented using four groups of fully connected layers and
leakyrelu activations. we set the dimensions of class embedding c to the same as
the latent vector z. then, we pass c through learnable affine transformations,
such that the class embedding is specialized to the scaling and bias parameters
controlling adaptive instance normalization (adain) [13] in each upblock. the
motivation for the design above comes from our hypothesis that the
class-conditional information mainly encodes cellular attributes related to cell
types, rather than common image appearance. therefore, by modulating the feature
maps at multiple scales, the input class label can better control the generation
of cellular attributes.we further introduce the skip-layer global context (sgc)
module into the generator (see fig. 2 in supplementary materials), to better
handle the diversity of the spatial relationship of the cells. our sgc module
reformulates the idea of gcnet [4] with the design of sle module from fastgan
[16]. it first performs global context modeling on the low-resolution feature
maps, then transforms global context to capture channel-wise dependency, and
finally merges the transformed features into high-resolution feature maps. in
this way, the proposed sgc module learns a global understanding of the
cell-to-cell spatial relationship and injects it into image generation via
computationally efficient modeling of long-range dependency.
in an adversarial training setting, the discriminator forces the generator to
faithfully match the conditional data distribution of real cervical
cytopathological images, thus prompting the generator to produce visually and
semantically realistic images. for training stability, the discriminator is
trained as a feature encoder with two extra decoders. in particular, five
resnet-like [7] down-blocks are employed to convert the input image into an 8 ×
8 × 512 feature map. two simple decoders reconstruct downscaled and randomly
cropped versions of input images i crop and i resize from 8 2 and 16 2 feature
maps, respectively. these decoders are optimized together with the discriminator
by using a reconstruction loss l recon that is represented below:where t denotes
the image processing (i.e., 1 2 downsampling and 1 4 random cropping) on real
image i real , f is the processed intermediate feature map from the
discriminator dis, and dec stands for the reconstruction decoder. this simple
self-supervised technique provides a strong regularization in forcing the
discriminator to extract a good image representation.to provide more detailed
feedback from the discriminator, patchgan [12] architecture is adopted to output
an 8 × 8 logit map by using a 1 × 1 convolution on the last feature map. by
penalizing image content at the scale of patches, the color fidelity of
synthesized images is guaranteed as illustrated in our ablation study (see fig.
3). to align the class-conditional fake and real data distributions in the
adversarial setting, the discriminator directly incorporates class labels as
additional inputs in the manner of projection discriminator [20]. the class
label is projected to a learned 512-dimensional class embedding and takes
innerproduct at every spatial position of the 8 × 8 × 512 feature map. the
resulting 8×8 feature map is then added to the aforementioned 8×8 logit map,
composing the final output of the discriminator.for the objective function, we
use the hinge version [15] of the standard adversarial loss l adv . we also
employ r 1 regularization l reg [17] as a slight gradient penalty for the
discriminator. combining all the loss functions above, the total objective l
total to train the proposed cellgan in an adversarial manner can be expressed
as:where λ reg is empirically set to 0.01 in our experiments.3 experimental
results
dataset. in this study, we collect 14,477 images with 256 × 256 pixels from
three collaborative clinical centers. all the images are manually inspected to
contain different cervical squamous cell types. in total, there are 7,662 nilm,
2,275 asc-us, 2,480 lsil, 1,638 asc-h, and 422 hsil images. all the 256×256
images with their class labels are selected as the training data.implementation
details. we use the learning rate of 2.5 × 10 -4 , batch size of 64, and adam
optimizer [14] to train both the generator and the discriminator for 100k
iterations. spectral normalization [19], differentiable augmentation [30] and
exponential-moving-average optimization [29] are included in the training
process. fréchet inception distance (fid) [8] is used to measure the overall
semantic realism of the synthesized images. all the experiments are conducted
using an nvidia geforce rtx 3090 gpu with pytorch [22].
we compare cellgan with the state-of-the-art generative models for
classconditional image synthesis, i.e., biggan [2] from cgans [18] and latent
diffusion model (ldm) [25] from diffusion models [9]. as shown in fig. 2, biggan
to verify the effects of key components in the proposed cellgan, we conduct an
ablation study on four model settings in table 2 and fig. 3. we denote the
models in fig. 3 from left to right as model i, model ii, model iii, and
cellgan. the visual results of model i suffer from severe color distortions
while the other models do not, indicating that the patchgan-based discriminator
can guarantee color fidelity by patch-level image content penalty. the abnormal
cells generated by model i and model ii tend to have highly similar cellular
features. in contrast, model iii and cellgan can accurately capture the
morphological characteristics of different cell types. this phenomenon suggests
that the implementation of the class mapping network facilitates more
distinguishable feature representations for different cell types. by comparing
the synthesized images from model iii with cellgan, it is observed that adopting
sgc modules can yield more clear cell boundaries, which demonstrates the
capability of sgc module in modeling complicated cell-to-cell relationships in
image space. the quantitative results further state the effects of the
components above.
to validate the data augmentation capacity of the proposed cellgan, we conduct
5-fold cross-validations on the cell classification performances of two classi-
in each fold, one group is selected as the testing data while the other four are
used for training. for different data settings, we synthesize 2,000 images for
each cell type using the corresponding generative method, and add them to the
training data of each fold. we use the learning rate of 1.0 × 10 -4 , batch size
of 64, and sgd optimizer [24] to train all the classifiers for 30 epochs. random
flip is applied to all data settings since it is reasonable to use traditional
data augmentation techniques simultaneously in practice.the experimental
accuracy, precision, recall, and f1 score are listed in table 3. it is shown
that both the classifiers achieve the best scores in all metrics using the
additional synthesized data from cellgan. compared with the baselines, the
accuracy values of resnet-34 and densenet-121 are improved by 5.25% and 4.05%,
respectively. meanwhile, the scores of other metrics are all improved by more
than 4%, indicating that our synthesized data can significantly enhance the
overall classification performance. thanks to the visually plausible and
semantically realistic synthesized data, cellgan is conducive to the improvement
of cell classification, thus serving as an efficient tool for augmenting
automatic abnormal cervical cell screening.
in this paper, we propose cellgan for class-conditional cytopathological image
synthesis of different cervical cell types. built upon fastgan for training
stability and computational efficiency, incorporating class-conditional
information of cell types via non-linear mapping can better represent
distinguishable cellular features. the proposed sgc module provides the global
contexts of cell spatial relationships by capturing long-range dependencies. we
have also found that the patchgan-based discriminator can prevent potential
color distortion. qualitative and quantitative experiments validate the semantic
realism as well as the data augmentation effectiveness of the synthesized images
from cellgan.meanwhile, our current cellgan still has several limitations.
first, we cannot explicitly control the detailed attributes of the synthesized
cell type, e.g., nucleus size, and nucleus-cytoplasm ratio. second, in this
paper, the synthesized image size is limited to 256×256. it is worth conducting
more studies for expanding synthesized image size to contain much more cells,
such that the potential applications can be extended to other clinical scenes
(e.g., interactively training pathologists) in the future.
comparison between state-of-the-art generative models and the proposed cellgan
(↓: lower is better).
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2_47.
thyroid cancer is the most common cancer of the endocrine system, accounting for
2.1% of all malignant cancers [1]. clinically, pathologists rely on the
six-category of "the bethesda system for reporting thyroid cytopathology"
(tbsrtc) [2,3] to distinguish the cell morphology in the stained cytopathologic
sections. the emergence of computational pathology allows automatic diagnosis of
thyroid cancer, and nuclei segmentation becomes one of the most critical
diagnostic tasks [4,5], as the shapes of nuclei, whether round, oval, or
elongated, can provide valuable information for further analysis [6]. for
example, small and scattered thyroid cells with a light hue and relatively low
cell density are usually low-grade and indicative of early-stage cancer; whereas
large and dark cells with extreme-dense agglomeration are usually middle-or
late-grade [3]. correspondingly, accurate location of cell boundaries is
essential for both pathologists and computer-aided diagnosis (cad) systems to
assist decision [7].however, nuclei segmentation in thyroid cytopathology is
still challenged by the varying cellularity of images from different tbsrtc
categories [3,8]. for example, benign cells (i & ii) present high sparsity and
are difficult to be distinguished from background tissues, thus may account for
a relatively small proportion when equal images are involved in a training set
[3]. by contrast, high-grade cells (v & vi) are densely packed and severely
clustered, thus much more are presented in a training set. in this way, an
unbalanced distribution across different categories resulted, correspondingly,
the training leads to biased models with lower accuracy [9,10]. such distinct
morphological differences can be characterized by the tbsrtc category, which
thus inspires us to utilize the handy image-wise grading labels to guide the
nuclei segmentation model learning from unbalanced datasets. we also noticed
that another challenge for accurate nuclei identification is the heavy reliance
on large-scale high-quality annotations [11]. moreover, amongst multiple
annotation paradigms [12], pixellevel labeling is the most time-consuming and
laborious, whereas the image-wise diagnostic labels, i.e. tbsrtc categories, are
comparatively simpler. despite the labeling intensity, prevalent nuclei
segmentation methods, e.g., cia-net [13], ca 2.5 -net [14], and clusterseg [15],
are limited to pixel-wise annotations, where the potential benefits of
integrating accessible image-wise labels are unaware.to narrow the gap
discussed, we propose a novel tbsrtc-category-aware nuclei segmentation
framework. our contributions are three-fold. (1) we propose a cytopathology
nuclei segmentation network named tcsegnet, to provide supplementary guidance to
facilitate the learning of nuclei boundaries. innovatively, our approach can
help reduce bias in the learning process of the segmentation model with the
routine unbalanced training set. (2) we expand tcsegnet to semi-tcsegnet to
leverage image-wise labels in a semi-supervised learning manner, which
significantly reduces the reliance on annotation-intensive pixel-wise labels.
additionally, an hsv-intensity noise is designed specifically for cytopathology
images to boost the generalization ability. (3) we establish a dataset of
thyroid cytopathology image patches of 224 × 224, where 4,965 image labels are
provided following tbsrtc, and 1,473 of them are densely annotated [3] (to be on
github upon acceptance). to the best of our knowledge, it is the first
publicized thyroid cytopathology dataset of both image-wise and pixel-wise
labels. the annotated dataset well alleviates the insufficiency of an open
cytopathology dataset for computer-assisted analysis (fig. 1).
overview. we propose a novel tbsrtc-category aware segmentation network
(tcsegnet) to segment nuclei boundaries in cytopathology images, which is guided
by tbsrtc-category label to learn from unbalanced data. our model uses a cnn and
transformer dual-path u-shape architecture, where the cnn captures the local
features, and the transformer extracts the global features for a more
comprehensive representation of nuclei allocation [16]. considering the spatial
distributions of thyroid cells in cytopathology images, our design provides
extended global information for more accurate segmentation. our approach employs
short connections to allow effective communication of local and global
representations [17]. formally, the overall segmentation loss l seg to train our
model is a combination of the binary cross-entropy loss (bce), i.e.where ŷ is
the prediction from tcsegnet and y is and pixel-wise annotation, respectively.
subscript ni and nb denote the nuclei area and boundary, respectively.
superscripts cnn and trans write for the cnn branch and transformer branch,
respectively. we set the balancing coefficient γ ni to 1 and γ nb to 10.
additionally, to ensure the consistency between the two branches, we impose a
dice consistency loss (l cons ) between the nuclei instance predictions from the
cnn branch and the transformer branch, namelytbsrtc-category label guidance
block. in tcsegnet, we introduce a tbsrtc-category label guidance block to
address the learning issue from unbalanced routine datasets. this block consists
of two learnable fully connected layers that process the feature extracted by
the cnn and transformer branches separately, which obtains image-wise
tbsrtc-category prediction denoted as ŷcnn cls and ŷtrans cls . correspondingly,
to train this block, we use a cross-entropy loss function (ce) that provides an
extra supervision signal to help the network learn from unbalanced datasets,
defined as follows:where y cls is the image-wise tbsrtc-category label, and the
balancing coefficient γ cls is set to 3, as the global feature captured by the
transformer branch is tightly correlated with the image-level classification
tag. finally, the overall loss for tcsegnet becomesextension to semi-supervised
learning. to leverage images that only have image-wise labels, we extend to a
semi-supervised mean teacher [18] framework called semi-tcsegnet. in this
framework, both the student and teacher share the same full-supervised nuclei
segmentation architecture of tcsegnet. the weights of the teacher θ t are
updated with the exponential moving average (ema) of the weights of student θ s
, and smoothing coefficient α = 0.99, following the previous work [19].
formally, the weights of the teacher at e-th epoch are updated by where e max is
the maximum epoch number.hsv-intensity noise. the traditional method of
integrating gaussian noise in the mean teacher [18] may be problematic when
working with cytopathology images that have an imbalanced color distribution. to
address this issue, we generate a novel intensity-based noise, which can
adaptively behave stronger in the dark nuclei areas and weaker in bright
cytoplasm or background regions. we first sample η from a gaussian distribution
n 0, σ 2 , where σ is the standard deviation computed from the pixel values of
the v channel in hsv space. the gaussian noise η serves as the basis for
generating the intensity-based noise, which is obtained byspecifically, x v is
the pixel value of the image's v channel in hsv space, and hyper-parameter λ v
is set as 0.5 to control the amplitude of the intensity-based noise. finally,
the value of the obtained noise is clamped to [-0.2, 0.2] before being added to
the images.
image dataset. we construct a clinical thyroid cytopathology dataset with images
of both image-wise and pixel-wise labels as a benchmark (appear in github upon
acceptance) some representative images are presented in fig. 2, together with
the profile of the dataset. the dataset comprises 4,965 h&e stained image
patches and labels of tbsrtc, where a subset of 1,473 images was densely
annotated for nuclei boundaries by three experienced cytopathologists and
reached a total number of 31,064 elaborately annotated nuclei. patient-level
images were partitioned first for training and test images, and patch-level
curation was performed. we divided the dataset with image-wise labels into 80%
training samples and the remaining 20% testing samples. our collection of
thyroid cytopathology images was granted with an ethics approval document. table
1. quantitative comparisons in both fully-supervised and semi-supervised
manners. the best performance is highlighted in bold, where we can observe that
both tcsegnet and its semi-supervised extension outperform state-of-the-art.
fully-supervised mask r-cnn [20] 0.657 0.500 swin-unet [21] 0.671 0.516 segnet
[22] 0.676 0.587 unet++ [23] 0.784 0.691 ca 2.5 -net [14] 0.838 0.732 cia-net
[13] 0.854 0.775 clusterseg [15] 0.857 0.761 tcsegnet (ours) 0.877
0.788semi-supervised pseudoseg [24] 0.734 0.612 cross pseudo seg [25] 0.737
0.618 cross teaching [26] 0.795 0.704 ps-clusterseg [15] 0.866 0.775 mtmt-net
[27] 0.878 0.789 semi-tcsegnet (ours) 0.889 0.805 implementations. the proposed
method and compared methods are implemented on a single nvidia geforce rtx 3090
gpu card. we employ a conformer [16] with 12 transformer layers and 5 cnn blocks
as the encoder in tcsegnet. both tcsegnet and semi-tcsegnet use sgd optimizer
with a momentum of 0.9 and a weight decay of 10 -4 . the initial learning rate
lr 0 is set to 5 × 10 -3 , and the learning rate for e th epoch is determined by
the poly strategy [28], i.e., lr e = lr 0 × (1 -e/e max ) 0.9 , where e max =
280 is the total epoch number. we set the batch size for tcsegnet to 8, and for
semi-tcsegnet to 10, i.e. 8 fully-annotated images and 2 partially-annotated
images per batch.compared methods and evaluation metrics. we compared tcsegnet
with the fully-supervised counterparts, including method specific for
segmentation in general image [20,22], medical image [21,23], and nuclei
[13][14][15]. we also compared semi-tcsegnet with semi-supervised methods
[15,[24][25][26][27]. we used the officially released code published along with
their papers for all the compared methods. intersection over union (iou) and
dice score were applied as the evaluation metrics, where a higher value
indicated a better semantic segmentation performance. experimental results. the
results in table 1 indicated that tcsegnet can achieve the highest performance
by a dice score of 87.7% and an iou of 78.8%. the performance values in the
challenging regions are highlighted with red boxes in fig. 3, together with the
line charts in fig. 4 (a,b). our approach is capable to address the current
issue in the recognition and segmentation of small isolated cells graded in the
i category, which is always ignored by the unbalanced pixel-wise cell morphology
with other approaches. also, it yields that the incorporation of tbsrtc-category
can contribute to a partial alleviation of a biased model, resulting in more
satisfying segmentation performance experimentally. furthermore, the fact that
the tbsrtc-category label is easy to obtain endows the applicability of our
model to various circumstances that nuclei in various sizes, shapes, and dyeing
styles can be accurately recognized and segmented. consequently, it can serve as
a guarantee for the validity and accuracy of the subsequent analysis in real
clinical practice. moreover, with the semi-supervised learning, semi-tcsegnet
can further boost the performance to an 88.9% dice score, and 80.5% iou, by
leveraging additional data with image-wise tbsrtccategory labels solely. the
performance improvement of 1.2% dice, 1.7% iou, together with the general
improvement is shown in the boxplot in fig. 4 (c,d), as a demonstration of the
advantage using full data resources with semi-tcsegnet. ablation study. to
evaluate the effectiveness of each functional block and demonstrate the
functionality of semi-supervised learning, we illustrate the ablation study in
table 2. the results indicate that performance improvement is accumulated with
increasing data size. besides, training with a classificationlearning block
alone can increase the nuclei segmentation performance by 1.7% and 2.6% in the
dice score and iou, respectively. meanwhile, trained with specially designed
hsv-intensity noise can also increase the performance by 0.9% dice and 1.4% iou,
showing its potential for generation ability improvement. importantly, the
benefits from the two blocks are orthonormal, where semi-tcsegnet achieves the
optimal performance with the utilization of both.
in this paper, we propose a tbsrtc-category aware nuclei segmentation framework
tcsegnet, that leverages easy-to-obtain image-wise diagnostic category to
facilitate nuclei segmentation. importantly, it addresses the challenge of
distinguishing nuclei across different cell scales in an unbalanced dataset. we
also extend the framework to a semi-supervised learning fashion to overcome the
issue of lacking annotated training samples. moreover, we construct the first
thyroid cytopathology dataset with both image-wise and pixel-wise labels, which
we believe can it facilitate future research in this field. as the spatial
distribution, shape, and area information from nuclear segmentation is
supportive of diagnostic decisions, we will further leverage the segmentation
result for malignancy analysis and also explore the potential of spatial
information for unlabeled data exploration in the future.
breast cancer (bc) is one of the most common malignant tumors in women worldwide
and it causes nearly 0.7 million deaths in 2020 [26]. the pathological process
is usually the golden standard approach for bc diagnosis, which relies on
leveraging diverse complementary information from multi-modal data. in addition
to obtaining the histological characteristics of tumors from hematoxylin and
eosin (h&e) staining images, immunohistochemical (ihc) staining images are also
widely used for pathological diagnoses, such as the human epidermal growth
factor receptor 2 (her2), the estrogen receptor (er), and the progesterone
receptor (pr) [22]. with the development of deep learning, there are a lot of
multi-modal fusion methods for cancer diagnosis [6,7,20,21].recently, with the
development of transformer, multi-modal pre-training has achieved great success
in the fields of computer vision (cv) and natural language processing (nlp).
according to the data format, there are two main multi-modal pre-training
approaches, as shown in fig. 1. one is based on isomorphic data, such as
vision-language pre-training [5] and vision-speech-text pre-training [3]. the
other is based on heterogeneous data. bachmann et al. [2] proposed multi-mae to
pre-train models with intensity images, depth images, and segmentation maps. in
the field of medical image analysis, it is widely recognized that using
multi-modal data can produce more accurate diagnoses than using single-modal
data. however, the development of multi-modal pre-training methods has been
limited due to the scarcity of paired multi-modal data. most methods focus on
chest x-ray vision-language pre-training [8,11]. to our best knowledge, there is
no work for multi-modal pre-training based on pathological heterogeneous data.in
this paper, we propose a multi-modal pre-training method based on masked
autoencoders for bc downstream tasks. our model consists of three parts, i.e.,
the modal-fusion encoder, the mixed attention, and the modal-specific decoder.
we choose paired h&e and ihc (only her2) staining images, which are cropped into
non-overlapped patches as the input of our model. we randomly mask some patches
by a ratio and feed the remaining patches into the modalfusion encoder to get
corresponding tokens. then the mixed attention module is used to take the
intra-modal and inter-modal correlation into account. finally, we use
modal-specific decoders to reconstruct the original h&e and ihc staining images
respectively. our contributions are summarized as follows:we propose a
multi-modal pre-training via masked autoencoders mmp-mae for bc diagnosis. to
our best knowledge, this is the first pre-training work based on multi-modal
pathological data. we evaluate the proposed method on two public datasets as
herohe challenge and bci challenge, which shows that our method achieves
state-of-theart performance. i=1 and {yi} λ 2 n i=1 into the modal-fusion
encoder to extract the patch tokens {fi} λ 1 n i=1 and {gi} λ 2 n i=1 . then we
use intra-modal attention and inter-modal attention to take patch correlation
into account. x and y are reconstructed by modal-specific decoders respectively.
the proposed mmp-mae consists of three modules, i.e., the modal-fusion encoder,
the mixed attention, and the modal-specific decoder, as shown in fig. 2. a pair
of h&e and her2 images are cropped into regular non-overlapping patches. we mask
some of the patches of two modalities with a ratio. the remained patches are fed
into the modal-fusion encoder to get the corresponding tokens. then we use the
mixed attention module to extract intra-modal and inter-modal complementary
information. finally, the modal-specific tokens are fed into the modal-specific
decoders to reconstruct the original h&e and her2 images. the pre-trained
modal-fusion encoder could be used for downstream tasks (e.g., her2 status
prediction and her2 image generation based on h&e images).
modal-fusion encoder. we use vit-base [12] as the backbone of the modalfusion
encoder, which contains a linear projection, 12 transformer blocks, and a
multi-layer perceptron (mlp) head. we remove the mlp head and use the remained
part to extract patch tokens. an image is cropped into several nonoverlapping
patches, and these patches are mapped to d dimension tokens with the linear
projection and added position embeddings to retain positional information. each
transformer block consists of a multi-head self-attention layer (mhsa) mixed
attention. the mixed attention module contains intra-modal attention and
inter-modal attention, as shown in fig. 3. the intra-modal attention is the
original transformer block, which consists of mhsa, mlp, lns, and residual
connections. it is defined asalgorithm 1. transformer processing flow.f l ←
ffn(ln(f l )) + f l 10: end for output: class token and patch tokens f l ∈ r (n
+1)×d fig. 4. workflow of two downstream tasks. in the her2 staining image
generation task, we remain the structure of gan and replace the generator with
our pre-trained model. in the her2 status prediction task, we replace the
feature extractor with our pre-trained model to obtain representations with her2
semantics.in inter-modal attention, we replace mhsa with the multi-head
cross-attention (mhca) module. we use mhca to leverage diverse complementary
information between two modalities.modal-specific decoder. each modal-specific
decoder is a shallow block with two transformer layers. different from the
transformer encoder, the target of the transformer decoder is used to
reconstruct the original image.reconstruction loss. given a pair of h&e image x
and her2 image y , which is cut into 16 × 16 non-overlapping patches {x i } n
i=1 and {y i } n i=1 . we mask some of the patches randomly with the ratio λ 1
and λ 2 (λ 1 + λ 2 = 1). the remained patches are fed into the modal-fusion
encoder and the output is corresponding patch tokens {f i } λ1n i=1 and {g i }
λ2n i=1 . we randomly generate masked patch tokens {e x j }(1-λ1)n j=1and {e y j
}(1-λ2)n j=1, which are learnable vectors for masked patch prediction. the input
of the mixed attention module is the full set of tokens {f i , eand {g i , e y j
}
, which include both the remaining patch tokens and the masked patch tokens.
after the process of the mixed attention module, h&e and her2 patch tokens are
fed into the modal-specific decoders respectively to reconstruct the original
h&e image x and her2 image y . the reconstruction loss is computed by the mean
squared error between the original images x, y and the generative images x , y ,
which is computed aswe use an adjustable hyperparameter θ to balance the losses
of two modalities. the final loss l is defined as
the pre-trained encoder could be used for downstream tasks, as shown in fig. 4.
we choose two relevant tasks: her2 image generation based on h&e images and her2
status prediction. in the her2 generation task, we replace the generator of
pyramid pix2pix, a generative adversarial network (gan) in [16], with our
pre-trained encoder and a light-weight decoder. the weights of the pre-trained
encoder are fixed, and the light-weight decoder in the generator and the
discriminator are learnable. we use pairs of h&e and ihc images for gan
training.in the her2 status prediction task, we replace the universal extractor
resnet-50 [14] with our pre-trained encoder. we use clam-mil [19] as the
aggregator in our training process.3 experimental results
acrobat challenge. the automatic registration of breast cancer tissue (acrobat)
challenge [27] provides h&e wsis and matched ihc wsis (er, pr, her2, and ki67),
which consists of 750 training cases, 100 validation cases, and 300 testing
cases. we choose paired h&e and her2 wsis for pre-training. we extract the key
points and descriptors from paired wsis using sift [18] and superpoint [10].
then the extracted key points and descriptors are matched using ransac [13] and
superglue [25]. we repeat this procedure several times on the rotated,
downsampled, or transformed moving wsi to fetch the best transformation based on
mean squared error (mse) loss between source and target wsis' descriptors. after
that, the selected transformation is optimized across different levels of wsis
by gradient descent with local normalized cross-correlation (ncc) as its cost
function. in the final phase of nonrigid registration, we use the optimized
transformation to get the initial displacement field, which is optimized across
different levels of wsis by gradient update. the loss function of which is the
weighted sum of ncc and diffusive regularization. we resize the displacement
field and apply it to the original moving wsi. after all the wsi pairs are well
registered, we convert the padded h&e image to grayscale and apply median blur
to it. next, the otsu threshold is applied to extract the foreground area, which
is cropped into non-overlapping 256 × 256 images. finally, all the chosen images
(around 0.35 million) from wsi in the same pair are saved for mmp-mae
pre-training.bci challenge. breast cancer immunohistochemical image generation
challenge [16] consists of 3896 pairs of images for training and 977 pairs for
testing, which are used to generate her2 images based on h&e images.
experiments are implemented in pytorch [24] and with 4 nvidia a100 tensor core
gpus. we pre-train our mmp-mae on the acrobat dataset with adamw [17] and the
learning rate of 1e -4 . the batch size of pre-training is 1024 and it takes
about 30 h for 100 epochs. we use warmup for the first 10 epochs and the
learning rate is set to 1e -6 .in the her2 staining image generation task, we
use 2 gpus with a batch size of 4. the learning rate is 2e -4 and the optimizer
is adam. we use the learning rate decay strategy for stable training. peak
signal to noise ratio (psnr) and structural similarity (ssim) are used as the
evaluation indicators for the quality of the her2 generated images.in the her2
status prediction task, we use 1 gpu with a batch size of 1 (wsi level). the
learning rate is 1e -4 and the adam optimizer is used. four standard metrics are
used to measure the her2 status prediction results, including the area under the
receiver operator characteristic curve (auc), precision, recall, and f1-score.
her2 staining image generation. three methods on bci datasets are compared in
our experiments, as shown in table 1. cyclegan is a representative unsupervised
method, which doesn't need paired images for training. so cyclegan focuses more
on style transformation, and it is difficult to match the cell-level information
in detail. pix2pix and pyramid pix2pix use paired data, which obtain better
results than cyclegan. pyramid pix2pix uses the multiscale constraint, which
performs better than pix2pix. our method is based on the framework of pyramid
pix2pix and we replace the generator with our pretrained encoder and a
lightweight decoder. our mmp-mae further improves the performance, which
achieves higher psnr by 1.60, and ssim by 0.007. the visualization on the
acrobat dataset also shows our model could learn the modality-related
information, as shown in fig. 5.her2 status prediction. we compare our method
with the top five methods reported in herohe challenge review [9]. most of these
methods use the multinetwork ensemble strategy and extra datasets. team macaroon
uses the came-lyon dataset [4] for tumor classification. team mitel uses bach
dataset [1] for tumor classification. team piaz and dratur both use a
multi-network ensemble strategy to improve their performances. team irisai first
segment the tumor area and then predict the her2 status. mmp-mae still achieves
competitive results by using a single pre-trained model, which is shown in table
2. our model improves and f1-score by 6%. the results show our model
pre-training has the ability to predict status from one modality.
in this paper, we propose a novel multi-modal pre-training framework, mmp-mae
for bc diagnosis. mmp-mae use paired h&e and her2 staining images for
pre-training, which could be used for several downstream tasks such as her2
staining image generation and her2 status prediction only by h&e modality. both
the experiment results on bci and herohe datasets show our pretrained mmp-mae
demonstrates strong transfer ability. our future work will expand our work to
more modalities.
automatic segmentation of tumor lesions from pathological images plays an
important role in accurate diagnosis and quantitative evaluation of cancers.
recently, deep learning has achieved remarkable performance in pathological
image segmentation when trained with a large and well-annotated dataset
[6,13,20]. however, obtaining dense annotations for pathological images is
challenging and time-consuming, due to the extremely large image size (e.g.,
10000 × 10000 pixels), scattered spatial distribution, and complex shape of
lesions.semi-supervised learning (ssl) is a potential technique to reduce the
annotation cost via learning from a limited number of labeled data along with a
large amount of unlabeled data. existing ssl methods can be roughly divided into
two categories: consistency-based [9,14,23] and pseudo label-based [2] methods.
the consistency-based methods impose consistency constraints on the predictions
of an unlabeled image under some perturbations. for example, mean teacher
(mt)-based methods [14,23] encourage consistent predictions between a teacher
and a student model with noises added to the input. xie et al. [21] introduced a
pairwise relation network to exploit semantic consistency between each pair of
images in the feature space. luo et al. [9] proposed an uncertainty rectified
pyramid consistency between multi-scale predictions. jin et al. [7] proposed to
encourage the predictions of auxiliary decoders and a main decoder to be
consistent under perturbed hierarchical features. pseudo label-based methods
typically generate pseudo labels for labeled images to supervise the network
[4]. since using a model's prediction to supervise itself may over-fit its bias,
chen et al. [2] proposed cross pseudo supervision (cps) where two networks learn
from each other's pseudo labels generated by argmax of the output prediction.
mc-net+ [19] utilized multiple decoders with different upsampling strategies to
obtain slightly different outputs, and each decoder's probability output was
sharpened to serve as pseudo labels to supervise the others. however, the pseudo
labels are not accurate and contain a lot of noise, using argmax or sharpening
operation will lead to over-confidence of potentially wrong predictions, which
limits the performance of the models. additionally, some related works advocated
the entropy-minimization methods. typical entropy minimization (em) [15] that
aims to reduce the uncertainty or entropy in a system. wu et al. [17] directly
applied entropy minimization to the segmentation results.in this work, we
propose a novel and efficient method based on cross distillation with multiple
attentions (cdma) for semi-supervised pathological image segmentation. firstly,
a multi-attention tri-branch network (mtnet) is proposed to efficiently obtain
diverse outputs for a given input. unlike mc-net+ [19] that is based on
different upsampling strategies, our mtnet uses different attention mechanisms
in three decoder branches that calibrate features in different aspects to obtain
diverse and complementary outputs. secondly, inspired by the observation that
smoothed labels are more effective for noise-robust learning found in recent
studies [10,22], we propose a cross decoder knowledge distillation (cdkd)
strategy to better leverage the diverse predictions of unlabeled images. in
cdkd, each branch serves as a teacher of the other two branches using soft label
supervision, which reduces the effect of noise for more robust learning from
inaccurate pseudo labels than argmax [2] and sharpening-based [19] pseudo
supervision in existing methods. differently from typical knowledge distillation
(kd) methods [5,24] that require a pre-trained teacher to generate soft
predictions, our method efficiently obtains the teacher and student's soft
predictions simultaneously in a single forward pass. in addition, we apply an
uncertainty minimization-based regularization to the average probability
prediction across the decoders, which not only increases the network's
confidence, but also improves the inter-decoder consistency for leveraging
labeled images.the contribution of this work is three-fold: 1) a novel framework
named cdma based on mtnet is introduced for semi-supervised pathological image
segmentation, which leverages different attention mechanisms for generating
diverse and complementary predictions for unlabeled images; 2) a cross decoder
knowledge distillation method is proposed for robust and efficient learning from
noisy pseudo labels, which is combined with an average prediction-based
uncertainty minimization to improve the model's performance; 3) experimental
results show that the proposed cdma outperforms eight state-of-the-art ssl
methods on the public digestpath dataset [3].
as illustrated in fig. 1, the proposed cross distillation of multiple attentions
(cdma) framework for semi-supervised pathological image segmentation consists of
three core modules: 1) a tri-branch network mtnet that uses three different
attention mechanisms to obtain diverse outputs, 2) a cross decoder knowledge
distillation (cdkd) module to reduce the effect of noisy pseudo labels based on
soft supervision, and 3) an average prediction-based uncertainty minimization
loss to further regularize the predictions on unlabeled images.
attention is an effective network structure design in fully supervised image
segmentation [12,16]. it can calibrate the feature maps for better performance
by paying more attention to the important spatial positions or channels with
only a few extra parameters. however, it has been rarely investigated in
semi-supervised segmentation tasks. to more effectively exploit attention
mechanisms for semisupervised pathological image segmentation, our proposed
mtnet consists of a shared encoder and three decoder branches that are based on
channel attention (ca), spatial attention (sa) and simultaneous channel and
spatial attention (csa), respectively. the encoder consists of multiple
convolutional blocks that are sequentially connected to a down-sampling layer,
and each decoder has multiple convolutional blocks that are sequentially
connected by an up-sampling layer. for a certain decoder, it uses ca, sa or sca
at the convolutional block at each resolution level to calibrate the features.ca
branch uses channel attention blocks to calibrate the features in the first
decoder. a channel attention block highlights important channels in a feature
map and it is formulated as:where f represents an input feature map. p ool s avg
and p ool s max represent average pooling and max-pooling across the spatial
dimension, respectively. mlp and σ denote multi-layer perception and the sigmoid
activation function respectively. f c is the output feature map calibrated by
channel attention.sa branch leverages spatial attention to highlight the most
relevant spatial positions and suppress the irrelevant regions in a feature map.
an sa block is:where conv denotes a convolutional layer. p ool c avg and p ool c
max are average and max-pooling across the channel dimension, respectively. ⊕
means concatenation.csa branch calibrates the feature maps using a csa block for
each convolutional block. a csa block consists of a ca block followed by an sa
block, taking advantage of channel and spatial attention simultaneously.due to
the different attention mechanisms, the three decoder branches pay attention to
different aspects of feature maps and lead to different outputs. to further
improve the diversity of the outputs and alleviate over-fitting, we add a
dropout layer and a feature noise layer η [11] before each of the three
decoders. for an input image, the logit predictions obtained by the three
branches are denoted as z ca , z sa and z csa , respectively. after using a
standard softmax operation, their corresponding probability prediction maps are
denoted as p ca , p sa and p csa , respectively.
since the three branches have different decision boundaries, using the
predictions from one branch as pseudo labels to supervise the others would avoid
each branch over-fitting its bias. however, as the predictions for unlabeled
training images are noisy and inaccurate, using hard or sharpened pseudo labels
[2,19] would strengthen the confidence on incorrect predictions, leading the
model to overfit the noise [10,22]. to address this problem, we introduce cdkd
to enhance the ability of our mtnet to leverage unlabeled images and eliminate
the negative impact of noisy pseudo labels. it forces each decoder to be
supervised by the other two decoders' soft predictions. following the practice
of kd [5], a temperature calibrated softmax (t-softmax) is used to soften the
probability maps:where z c represents the logit prediction for class c of a
pixel, and pc is the soft probability value for class c. temperature t is a
parameter to control the softness of the output probability. note that t = 1
corresponds to a standard softmax function, and a larger t value leads to a
softer probability distribution with higher entropy. when t < 1, eq. 3 is a
sharpening function.let pca , psa and pcsa represent the soft probability map
obtained by t-softmax for the three branches, respectively. with the other two
branches being the teachers, the kd loss for the csa branch is:where kl() is the
kullback-leibler divergence function. note that the gradient of l csa kd is only
back-propagated to the csa branch, so that the knowledge is distilled from the
teachers to the student. similarly, the kd losses for the ca and sa branches are
denoted as l ca kd and l sa kd , respectively. then, the total distillation loss
is defined as:
minimizing the uncertainty (e.g., entropy) [15] has been shown to be an
effective regularization for predictions on unlabeled images, which increases
the model's confidence on its predictions. however, applying uncertainty
minimization to each branch independently may lead to inconsistent predictions
between the decoders where each of them is very confident, e.g., two branches
predict the foreground probability of a pixel as 0.0 and 1.0 respectively. to
avoid this problem and further encourage inter-decoder consistency for
regularization, we propose an average prediction-based uncertainty
minimization:where p = (p csa + p ca + p sa )/3 is the average probability map.
c and n are the class number and pixel number respectively. p c i is the average
probability for class c at pixel i. note that when l um for a pixel is close to
zero, the average probability for class c of that pixel is close to 0.0 (1.0),
which drives all the decoders to predict it as 0.0 (1.0) and encourages
inter-decoder consistency.finally, the overall loss function for our cdma
is:where l sup = (l csa sup + l ca sup + l sa sup )/3 is the average supervised
learning loss for the three branches on the labeled training images, and the
supervised loss for each branch calculates the dice loss and cross entropy loss
between the probability prediction (p csa , p ca and p sa ) and the ground truth
label. λ 1 and λ 2 are the weights of l cdkd and l um respectively. note that l
cdkd and l um are applied on both labeled and unlabeled training images.
dataset and implementation details. we used the public digestpath dataset [3]
for binary segmentation of colonoscopy tumor lesions from whole slide images
(wsi) in the experiment. the wsis were collected from four medical institutions
of ×20 magnification (0.475 μm/pixel) with an average size of 5000 × 5000. we
randomly split 130 malignant wsis into 100, 10, and 20 for training, validation
and testing, respectively. for ssl, we investigated two annotation ratios: 5%
and 10%, where only 5 and 10 wsis in the training set were taken as annotated
respectively. labeled wsis were randomly selected. for computational
feasibility, we cropped the wsis into patches with a size of 256 × 256.at
inference time for segmenting a wsi, we used a sliding window of size 256×256
with a stride of 192 × 192.the cdma framework was implemented in pytorch, and
all experiments were performed on one nvidia 2080ti gpu. mtnet was implemented
by extending deeplabv3+ [1] into a tri-branch network, where the three decoders
were equipped with ca, sa and csa blocks respectively. the encoder used a
backbone of resnet50 pre-trained on imagenet. the kernel size of conv in the sa
block is 7 × 7. sgd optimizer was used for training, with weight decay 5 × 10 -4
, momentum 0.9 and epoch number 150. the learning rate was initialized to 10 -3
and decayed by 0.1 every 50 epochs. the hyper-parameter setting was λ 1 = λ 2 =
0.1, t = 10 based on the best results on the validation set. the batch size was
16 (8 labeled and 8 unlabeled patches). for data augmentation, we adopted random
flipping, random rotation, and random gaussian noise. for inference, only the
csa branch was used due to the similar performance of the three branches after
converge and the increased inference time of their ensemble, and no
post-processing was used. dice similarity coefficient (dsc) and jaccard index
(ji) were used for quantitative evaluation. comparison with state-of-the-art
methods. our cdma was compared with eight existing ssl methods: 1) entropy
minimization (em) [15]; 2) mean teacher (mt) [14]; 3) uncertaitny-aware mean
teacher (uamt) [23]; 4) r-drop [18] that introduces a dropout-based consistency
regularization between two networks; 5) cps [2]; 6) hierarchical consistency
enforcement (hce) [7]; 7) cnn&transformer [8] that introduces cross-supervision
between cnn and transformer; 8) mc-net+ [19] that imposes mutual consistency
between multiple slightly different decoders. they were also compared with the
lower bound of supervised learning (sl) that only learns from the labeled
images. all these methods used the same backbone of deeplabv3+ [1] for a fair
comparison.quantitative evaluation of these methods is shown in table 1. in the
existing methods, mc-net+ [19] and cps [2] showed the best performance for both
of the two annotation ratios. our proposed cdma achieved a better performance
than all the existing methods, with a dsc score of 69.72% and 72.24% when the
annotation ratio was 5% and 10%, respectively. figure 2 shows a qualitative
comparison between different methods. it can be observed that our cdma yields
less mis-segmentation compared with cps [2] and mc-net+ [19]. ablation study.
for ablation study, we set the baseline as using the proposed mtnet with three
different decoders for supervised learning from labeled images only. it obtained
an average dsc of 65.02% and 68.61% under the two annotation ratios
respectively. the proposed l cdkd was compared with two variants: l cdkd
(argmax) and l cdkd (t =1) that represent using hard pseudo labels and standard
probability output obtained by softmax for cdkd respectively. table 2 shows that
our l cdkd obtained an average dsc of 68.84% and 71.49% under the two annotation
ratios respectively, and it outperformed l cdkd (argmax) and l cdkd (t =1),
demonstrating that our cdkd based on softened probability prediction is more
effective in dealing with noisy pseudo labels. by introducing our average
prediction-based uncertainty minimization l um , the dsc was further improved to
69.72% and 72.24% under the two annotation ratios respectively. in addition,
replacing our l um by applying entropy minimization to each branch respectively
(l um ) led to a dsc drop by around 0.65%. then, we compared different mtnet
variants: 1) mtnet(dual) means a dualbranch structure (removing the csa branch);
2) mtnet(csa×3) means all the three branches use csa blocks; 3) mtnet(-atten)
means no attention block is used in all the branches; and 4) mtnet(ensb) means
using an ensemble of the three branches for inference. note that all these
variants were trained with l cdkd and l um . the results in the second section
of table 2 show that using the same structures for different branches, i.e.,
mtnet(-atten) and mtnet(csa×3), had a lower performance than using different
attention blocks, and using three attention branches outperformed just using two
attention branches. it can also be found that using csa branch for inference had
a very close performance to mtnet(ensb), and it is more efficient than the
later.
we have presented a novel semi-supervised framework based on cross distillation
of multiple attentions (cdma) for pathological image segmentation. it employs a
multi-attention tri-branch network to generate diverse predictions based on
channel attention, spatial attention, and simultaneous channel and spatial
attention, respectively. different attention-based decoder branches focus on
various aspects of feature maps, resulting in disparate outputs, which is
beneficial to semi-supervised learning. to eliminate the negative impact of
incorrect pseudo labels in training, we employ a cross decoder knowledge
distillation (cdkd) to enforce each branch to learn from soft labels generated
by the other two branches. experimental results on a colonoscopy tissue
segmentation dataset demonstrated that our cdma outperformed eight
state-of-the-art ssl methods. in the future, it is of interest to apply our
method to multi-class segmentation tasks and pathological images from different
organs.
* 68.32±21.18 * 52.35±21.53 * 53.62±20.32 * em [15] 67.09±24.28 * 70.01±22.24 *
54.55±22.40 * 56.96±21.70 * mt [14] 67.46±23.10 * 70.19±21.72 * 54.68±21.27 *
56.38±21.21 * * 70.09±22.07 * 55.40±22.54 * 57.64±21.80 * ours (csa branch)
automatic identification of lesions from dermoscopic images is of great
importance for the diagnosis of skin cancer [16,22]. currently, deep learning
mod-els, especially those based on deep convolution neural networks, have
achieved remarkable success in this task [17,18,22]. however, this comes at the
cost of a large amount of labeled data that needs to be collected for each
class. to alleviate the labeling burden, semi-supervised learning has been
proposed to exploit a large amount of unlabeled data to improve performance in
the case of limited labeled data [10,15,19]. however, it still requires a small
amount of labeled data for each class, which is often impossible in real
practice. for example, there are roughly more than 2000 named dermatological
diseases today, of which more than 200 are common, and new dermatological
diseases are still emerging, making it impractical to annotate data from scratch
for each new disease category [20]. however, since there is a correlation
between new and known diseases, a priori knowledge from known diseases is
expected to help automatically identify new diseases [9].one approach to address
the above problem is novel class discovery (ncd) [7,9,24], which aims to
transfer knowledge from known classes to discover new semantic classes. most ncd
methods follow a two-stage scheme: 1) a stage of fully supervised training on
known category data and 2) a stage of clustering on unknown categories [7,9,24].
for example, han et al. [9] further introduced self-supervised learning in the
first stage to learn general feature representations. they also used ranking
statistics to compute pairwise similarity for clustering. zhong et al. [24]
proposed openmix based on the mixup strategy [21] to further exploit the
information from known classes to improve the performance of unsupervised
clustering. fini et al. [7] proposed uno, which unifies multiple objective
functions into a holistic framework to achieve better interaction of information
between known and unknown classes. zhong et al. [23] used neighborhood
information in the embedding space to learn more discriminative representations.
however, most of these methods require the construction of a pairwise similarity
prediction task to perform clustering based on pairwise similarity pseudo labels
between samples. in this process, the generated pseudo labels are usually noisy,
which may affect the clustering process and cause error accumulation. in
addition, they only consider the global alignment of samples to the category
center, ignoring the local inter-sample alignment thus leading to poor
clustering performance.in this paper, we propose a new novel class discovery
framework to automatically discover novel disease categories. specifically, we
first use contrastive learning to pretrain the model based on all data from
known and unknown categories to learn a robust and general semantic feature
representation. then, we propose an uncertainty-aware multi-view
cross-pseudo-supervision strategy to perform clustering. it first uses a
self-labeling strategy to generate pseudo-labels for unknown categories, which
can be treated homogeneously with ground truth labels. the
cross-pseudo-supervision strategy is then used to force the model to maintain
consistent prediction outputs for different views of unlabeled images.in
addition, we propose to use prediction uncertainty to adaptively adjust the
contribution of the pseudo labels to mitigate the effects of noisy pseudo
labels. finally, to encourage local neighborhood alignment and further refine
the pseudo labels, we propose a local information aggregation module to
aggregate the information of the neighborhood samples to boost the clustering
performance. we conducted extensive experiments on the dermoscopy dataset isic
2019, and the experimental results show that our method outperforms other
state-of-the-art comparison algorithms by a large margin. in addition, we also
validated the effectiveness of different components through extensive ablation
experiments.
given an unlabeled dataset {x u i } n u i=1 with n u images, where x u i is the
ith unlabeled image. our goal is to automatically cluster the unlabeled data
into c u clusters. in addition, we also have access to a labeled dataset {x l i
, y l i } n l i=1 with n l images, where x l i is the ith labeled image and y l
i ∈ y = 1, . . . , c l is its corresponding label. in the novel class discovery
task, the known and unknown classes are disjoint, i.e., c l ∩ c u = ∅. however,
the known and unknown classes are similar, and we aim to use the knowledge of
the known classes to help the clustering of the unknown classes. the overall
framework of our proposed novel class discovery algorithm is shown in fig. 1.
specifically, we first learn general and robust feature representations through
contrastive learning. then, the uncertainty-aware multi-view
cross-pseudo-supervision strategy is used for joint training on all category
data. finally, the local information aggregation module benefits the ncd by
aggregating the useful information of the neighborhood samples.contrastive
learning. to achieve a robust feature representation for the ncd task, we first
use noise contrastive learning [8] to pretrain the feature extractor network,
which effectively avoids model over-fitting to known categories. specifically,
we use x i and x i to represent different augmented versions of the same image
in a mini-batch. the unsupervised contrastive loss can be formulated as:where z
i = e(x i ) is the deep feature representation of the image x i , e is the
feature extractor network, and τ is the temperature value. 1 is the indicator
function.in addition, to help the feature extractor learn semantically
meaningful feature representations, we introduce supervised contrastive learning
[12] for labeled known category data, which can be denoted as:where n (i)
represents the sample set with the same label as x i in a mini-batch data. |n
(i)| represents the number of samples. the overall contrastive loss can be
expressed as:, where μ denotes the balance coefficient. b l is the labeled
subset of mini-batch data.uncertainty-aware multi-view cross-pseudo-supervision.
we now describe how to train uniformly on known and unknown categories using the
uncertainty-aware multi-view cross-pseudo-supervision strategy. specifically, we
construct two parallel classification models m 1 and m 2 , both of them composed
of a feature extractor and two category classification heads, using different
initialization parameters. for an original image x i , we generate two augmented
versions of x i , x v1 i and x v2 i . we then feed these two augmented images
into m 1 and m 2 to obtain the predictions for x v1 i and x v2 i :the prediction
outputs are obtained by concatenating the outputs of the two classification
heads and then passing a softmax layer [7]. then, we can compute the ensemble
predicted output of m 1 and m 2 :next, we need to obtain training targets for
all data. for an input image x i , if x i is from the known category, we
construct the training target as one hot vector, where the first c l elements
are ground truth labels and the last c u elements are 0. if x i is from the
unknown category, we set the first c l elements to 0 and use pseudo labels for
the remaining c u elements.we follow the self-labeling method in [1,3] to
generate pseudo labels. specifically, the parameters in the unknown category
classification head can be viewed as prototypes of each category, and our
training goal is to distribute a set of samples uniformly to each prototype
while maximizing the similarity between samples and prototypes [1]. let p = p u
1 ; . . . ; p u bu ∈ r bu×c u denotes the ensemble prediction of data of unknown
categories in a mini-batch, where b u represents the number of samples. here we
only consider the output of the unknown categories head due to the samples
coming from unknown categories [7]. we obtain the pseudo label by optimizing the
following objective:where y = y u 1 ; . . . ; y u bu ∈ r bu×c u will assign b u
unknown category samples to c u category prototypes uniformly, i.e., each
category prototype will be selected b u /c u times on average. s is the search
space. h is the entropy function used to control the smoothness of y. δ is the
hyperparameter. the solution to this objective can be calculated by the
sinkhorn-knopp algorithm [6]. after generating the pseudo-labels, we can combine
them with the ground truth labels of known categories as training targets for
uniform training.to mitigate the effect of noisy pseudo labels, we propose to
use prediction uncertainty [14] to adaptively adjust the weights of pseudo
labels. specifically, we first compute the variance of the predicted outputs of
the models for the different augmented images via kl-divergence:where e
represents the expected value. if the variance of the model's predictions for
different augmented images is large, the pseudo label may be of low quality, and
vice versa. then, based on the prediction variance of the two models, the
multi-view cross-pseudo supervision loss can be formulated as:where l ce denotes
the cross-entropy loss. y v1 and y v2 are the training targets.local information
aggregation. after the cross-pseudo-supervision training described above, we are
able to assign the instances to their corresponding clustering centers. however,
it ignores the alignment between local neighborhood samples, i.e., the samples
are susceptible to interference from some irrelevant semantic factors such as
background and color. here, we propose a local information aggregation to
enhance the alignment of local samples. specifically, as shown in fig. 1, we
maintain a first-in-first-out memory bankduring the training process, which
contains the features of n m most recent samples and their pseudo labels. for
each sample in the current batch, we compute the similarity between its features
and the features of each sample in the memory bank:then based on this feature
similarity, we obtain the final pseudo labels as:k , where ρ is the balance
coefficient. by aggregating the information of the neighborhood samples, we are
able to ensure consistency between local samples, which further improves the
clustering performance.
dataset. to validate the effectiveness of the proposed algorithm, we conduct
experiments on the widely used public dermoscopy challenge dataset isic 2019
[4,5]. the dataset contains a total of 25,331 dermoscopic images from eight
categories: melanoma (mel), melanocytic nevus (nv), basal cell carcinoma (bcc),
actinic keratosis (ak), benign keratosis (bkl), dermatofibroma (df), vascular
lesion (vasc), and squamous cell carcinoma (scc). since the dataset suffers from
severe category imbalance, we randomly sampled 500 samples from those major
categories (mel, nv, bcc, bkl) to maintain category balance. then, we construct
the ncd task where we treat 50% of the categories (ak, mel, nv, bcc) as known
categories and the remaining 50% of the categories (bkl, scc, df, vasc) as
unknown categories. we also swap the known and unknown categories to form a
second ncd task. for task 1 and task 2, we report the average performance of 5
runs. implementation details. we used resnet-18 [11] as the backbone of the
classification model. the known category classification head is an l2
-normalized linear classifier with c l output units. the unknown category
classification head consists of a projection layer with 128 output units,
followed by an l2 -normalized linear classifier with c u output units. in the
first contrastive learning pre-training step, we used sgd optimizer to train the
model for 200 epochs and gradually decay the learning rate starting from 0.1 and
dividing it by 5 at the epochs 60, 120, and 180. μ is set to 0.5, τ is set to
0.5. in the joint training phase, we fix the parameters of the previous feature
extractor and only fine-tune the parameters of the classification head. we use
the sgd optimizer to train the model for 200 epochs with linear warm-up and
cosine annealing (lr base = 0.1, lr min = 0.001), and the weight decay is set to
1.5 × 10 -4 . for data augmentation, we use random horizontal/vertical flipping,
color jitter, and gaussian blurring following [7]. for pseudo label, we use the
sinkhorn-knopp algorithm with hyperparameters inherited from [7]: δ = 0.05 and
the number of iterations is 3. we use a memory bank m of size 100 and the
hyperparameter ρ is set to 0.6. the batch size in all experiments is 512. in the
inference phase, we only use the output of the unknown category classification
head of m 1 [9]. following [9,23,24], we report the clustering performance on
the unlabeled unknown category dataset. we assume that the number of unknown
categories is known and it can also be obtained by the category number
estimation method proposed in [9]. [9] 0.5652 0.2571 0.2203 0.4284 0.1164 0.1023
rankstats+ [9] 0.5845 0.2633 0.2374 0.4362 0.1382 0.1184 openmix [24] 0.6083
0.2863 0.2512 0.4684 0.1519 0.1488 ncl [23] 0.5941 0.2802 0.2475 0.4762 0.1635
0.1573 uno [7] 0.6131 0.3016 0.2763 0.4947 0.1692 0.1796 ours 0.6654 0.3372
0.3018 0.5271 0.1826 0.2033following [2,9], we use the average clustering
accuracy (acc), normalized mutual information (nmi) and adjusted rand index
(ari) to evaluate the clustering performance of different algorithms.
specifically, we first match the clustering assignment and ground truth labels
by the hungarian algorithm [13]. after the optimal assignment is determined, we
then compute each metric. we implement all algorithms based on the pytorch
framework and conduct experiments on 8 rtx 3090 gpus.comparison with
state-of-the-art methods. we compare our algorithms with some state-of-the-art
ncd methods, including rankstats [9], rankstats+ (rankstats with incremental
learning) [9], openmix [24], ncl [23], uno [7]. we also compare with the
benchmark method (baseline), which first trains a model using known category
data and then performs clustering on unknown category data. table 1 shows the
clustering performance of each comparison algorithm on different ncd tasks. it
can be seen that the clustering performance of the benchmark method is poor,
which indicates that the model pre-trained using only the known category data
does not provide a good clustering of the unknown category. moreover, the
state-of-the-art ncd methods can improve the clustering performance, which
demonstrates the effectiveness of the currently popular two-stage solution.
however, our method outperforms them, mainly due to the fact that they need to
generate pairwise similarity pseudo labels through features obtained based on
self-supervised learning, while ignoring the effect of noisy pseudo labels.
compared with the best comparison algorithm uno, our method yields 5.23% acc
improvement, 3.56% nmi improvement, and 2.55% ari improvement on task1, and
3.24% acc improvement, 1.34% nmi improvement, and 2.37% ari improvement on
task2, which shows that our method is able to provide more reliable pseudo
labels for ncd.ablation study of each key component. we performed ablation
experiments to verify the effectiveness of each component. as shown in table 2,
cl is contrastive learning, umcps is uncertainty-aware multi-view
cross-pseudosupervision, and lia is the local information aggregation module. it
can be observed that cl brings a significant performance gain, which indicates
that contrastive learning helps to learn a general and robust feature
representation for ncd. in addition, umcps also improves the clustering
performance of the model, which indicates that unified training helps to the
category information interaction. lia further improves the clustering
performance, which indicates that local information aggregation helps to provide
better pseudo labels. finally, our algorithm incorporates each component to
achieve the best performance.ablation study of contrastive learning. we further
examined the effectiveness of each component in contrastive learning. recall
that the contrastive learning strategy includes supervised contrastive learning
for the labeled known category data and unsupervised contrastive learning for
all data. as shown in table 3, it can be observed that both components improve
the clustering performance of the model, which indicates that scl helps the
model to learn semantically meaningful feature representations, while ucl makes
the model learn robust unbiased feature representations and avoid its
overfitting to known categories.uncertainty-aware multi-view
cross-pseudo-supervision. we also examine the effectiveness of uncertainty-aware
multi-view cross-pseudosupervision. we compare it with 1) w/o cps, which does
not use cross-pseudosupervision, and 2) cps, which uses cross-pseudo-supervision
but not the uncertainty to control the contribution of the pseudo label. as
shown in table 3, it can be seen that cps outperforms w/o cps, which indicates
that cps encourages the model to maintain consistent predictions for different
augmented versions of the input images, and enhances the generalization
performance of the model. umcps achieves the best clustering performance, which
shows its ability to use uncertainty to alleviate the effect of noisy pseudo
labels and avoid causing error accumulation.
in this paper, we propose a novel class discovery framework for discovering new
dermatological classes. our approach consists of three key designs. first,
contrastive learning is used to learn a robust feature representation. second,
uncertainty-aware multi-view cross-pseudo-supervision strategy is trained
uniformly on data from all categories, while prediction uncertainty is used to
alleviate the effect of noisy pseudo labels. finally, the local information
aggregation module further refines the pseudo label by aggregating the
neighborhood information to improve the clustering performance. extensive
experimental results validate the effectiveness of our approach. future work
will be to apply this framework to other medical image analysis tasks.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 3.
cancers are a group of heterogeneous diseases reflecting deep interactions
between pathological and genomics variants in tumor tissue environments [24].
different cancer genotypes are translated into pathological phenotypes that
could be assessed by pathologists [24]. high-resolution pathological images have
proven their unique benefits for improving prognostic biomarkers prediction via
exploring the tissue microenvironmental features [1,10,12,13,18,25]. meanwhile,
genomics data (e.g., mrna-sequence) display a high relevance to regulate cancer
progression [3,29]. for instance, genome-wide molecular portraits are crucial
for cancer prognostic stratification and targeted therapy [16]. despite their
importance, seldom efforts jointly exploit the multimodal value between cancer
image morphology and molecular biomarkers. in a broader context, assessing
cancer prognosis is essentially a multimodal task in association with
pathological and genomics findings. therefore, synergizing multimodal data could
deepen a crossscale understanding towards improved patient prognostication.the
major goal of multimodal data learning is to extract complementary contextual
information across modalities [4]. supervised studies [5][6][7] have allowed
multimodal data fusion among image and non-image biomarkers. for instance, the
kronecker product is able to capture the interactions between wsis and genomic
features for survival outcome prediction [5,7]. alternatively, the coattention
transformer [6] could capture the genotype-phenotype interactions for prognostic
prediction. yet these supervised approaches are limited by feature
generalizability and have a high dependency on data labeling. to alleviate label
requirement, unsupervised learning evaluates the intrinsic similarity among
multimodal representations for data fusion. for example, integrating image,
genomics, and clinical information can be achieved via a predefined unsupervised
similarity evaluation [4]. to broaden the data utility, the study [28] leverages
the pathology and genomic knowledge from the teacher model to guide the
pathology-only student model for glioma grading. from these analyses, it is
increasingly recognized that the lack of flexibility on model finetuning limits
the data utility of multimodal learning. meanwhile, the size of multimodal
medical datasets is not as large as natural vision-language datasets, which
necessitates the need for data-efficient analytics to address the training
difficulty.to tackle above challenges, we propose a pathology-and-genomics
multimodal framework (i.e., pathomics) for survival prediction (fig. 1). we
summarized our contributions as follows. (1) unsupervised multimodal data
fusion. our unsupervised pretraining exploits the intrinsic interaction between
morphological and molecular biomarkers (fig. 1a). to overcome the gap of
modality heterogeneity between images and genomics, we project the multimodal
embeddings into the same latent space by evaluating the similarity among them.
particularly, the pretrained model offers a unique means by using
similarity-guided modality fusion for extracting cross-modal patterns. (2)
flexible modality finetuning. a key contribution of our multimodal framework is
that it combines benefits from both unsupervised pretraining and supervised
finetuning data fusion (fig. 1b). as a result, the task-specific finetuning
broadens the dataset usage (fig 1b andc), which is not limited by data modality
(e.g., both singleand multi-modal data). (3) data efficiency with limited data
size. our approach could achieve comparable performance even with fewer
finetuned data (e.g., only use 50% of the finetuned data) when compared with
using the entire finetuning dataset.
overview. figure 1 illustrates our multimodal transformer framework. our method
includes an unsupervised multimodal data fusion pretraining and a supervised
flexible-modal finetuning. from fig. 1a, in the pretraining, our unsupervised
data fusion aims to capture the interaction pattern of image and genomics
features. overall, we formulate the objective of multimodal feature learning by
converting image patches and tabular genomics data into groupwise embeddings,
and then extracting multimodal patient-wise embeddings. more specifically, we
construct group-wise representations for both image and genomics modalities. for
image feature representation, we randomly divide image patches into groups;
meanwhile, for each type of genomics data, we construct groups of genes
depending on their clinical relevance [22]. next, as seen in fig. 1b andc, our
approach enables three types of finetuning modal modes (i.e., multimodal,
image-only, and genomics-only) towards prognostic prediction, expanding the
downstream data utility from the pretrained model. group-wise image and genomics
embedding. we define the group-wise genomics representation by referring to n =
8 major functional groups obtained from [22]. each group contains a list of
well-defined molecular features related to cancer biology, including
transcription factors, tumor suppression, cytokines and growth factors, cell
differentiation markers, homeodomain proteins, translocated cancer genes, and
protein kinases. the group-wise genomics representation is defined as g n ∈ r
1×dg , where n ∈ n , d g is the attribute dimension in each group which could be
various. to better extract high-dimensional group-wise genomics representation,
we use a self-normalizing network (snn) together with scaled exponential linear
units (selu) and alpha dropout for feature extraction to generate the group-wise
embedding g n ∈ r 1×256 for each group.for group-wise wsis representation, we
first cropped all tissue-region image tiles from the entire wsi and extracted
cnn-based (e.g., resnet50) d idimensional features for each image tile k as h k
∈ r 1×di , where d i = 1, 024, k ∈ k and k is the number of image patches. we
construct the group-wise wsis representation by randomly splitting image tile
features into n groups (i.e., the same number as genomics categories).
therefore, group-wise image representation could be defined as i n ∈ r kn×1024 ,
where n ∈ n and k n represents tile k in group n. then we apply an
attention-based refiner (abr) [17], which is able to weight the feature
embeddings in the group, together with a dimension deduction (e.g.,
fully-connected layers) to achieve the group-wise embedding. the abr and the
group-wise embedding i n ∈ r 1×256 are defined as:where w,v1 and v2 are the
learnable parameters.patient-wise multimodal feature embedding. to aggregate
patient-wise multimodal feature embedding from the group-wise representations,
as shown in fig. 1a, we propose a pathology-and-genomics multimodal model
containing two model streams, including a pathological image and a genomics data
stream.in each stream, we use the same architecture with different weights,
which is updated separately in each modality stream. in the pathological image
stream, the patient-wise image representation is aggregated by n group
representations as, where p ∈ p and p is the number of patients. similarly, the
patient-wise genomics representation is aggregated as g p ∈ r n ×256 . after
generating patient-wise representation, we utilize two transformer layers [27]
to extract feature embeddings for each modality as follows:where msa denotes
multi-head self-attention [27] (see appendix 1), l denotes the layer index of
the transformer, and h p could either be i p or g p . then, we construct global
attention poolings [17] as eq. 1 to adaptively compute a weighted sum of each
modality feature embeddings to finally construct patientwise embedding as i p
embedding ∈ r 1×256 and g p embedding ∈ r 1×256 in each modality.multimodal
fusion in pretraining and finetuning. due to the domain gap between image and
molecular feature heterogeneity, a proper design of multimodal fusion is crucial
to advance integrative analysis. in the pretraining stage, we develop an
unsupervised data fusion strategy by decreasing the mean square error (mse) loss
to map images and genomics embeddings into the same space. ideally, the image
and genomics embeddings belonging to the same patient should have a higher
relevance between each other. mse measures the average squared difference
between multimodal embeddings. in this way, the pretrained model is trained to
map the paired image and genomics embeddings to be closer in the latent space,
leading to strengthen the interaction between different modalities.in the single
modality finetuning, even if we use image-only data, the model is able to
produce genomic-related image feature embedding due to the multimodal knowledge
aggregation already obtained from the model pretraining. as a result, our
cross-modal information aggregation relaxes the modality requirement in the
finetuning stage. as shown in fig. 1b, for multimodal finetuning, we deploy a
concatenation layer to obtain the fused multimodal feature representation and
implement a risk classifier (fc layer) to achieve the final survival
stratification (see appendix 2). as for single-modality finetuning mode in fig.
1c, we simply feed i p embedding or g p embedding into risk classifier for the
final prognosis prediction. during the finetuning, we update the model
parameters using a log-likelihood loss for the discrete-time survival model
training [6](see appendix 2).
datasets. all image and genomics data are publicly available. we collected wsis
from the cancer genome atlas colon adenocarcinoma (tcga-coad) dataset
(cc-by-3.0) [8,21] and rectum adenocarcinoma (tcga-read) dataset (cc-by-3.0)
[8,20], which contain 440 and 153 patients. we cropped each wsi into 512 × 512
non-overlapped patches. we also collected the corresponding tabular genomics
data (e.g., mrna sequence, copy number alteration, and methylation) with overall
survival (os) times and censorship statuses from cbioportal [2,14]. we removed
the samples without the corresponding genomics data or ground truth of survival
outcomes. finally, we included 426 patients of tcga-coad and 145 patients of
tcga-read.experimental settings and implementations. we implement two types of
settings that involve internal and external datasets for model pretraining and
finetuning. as shown in fig 2a, we pretrain and finetune the model on the same
dataset (i.e., internal setting). we split tcga-coad into training (80%) and
holdout testing set (20%). then, we implement four-fold cross-validation on the
training set for pretraining, finetuning, and hyperparameter-tuning. the test
set is only used for evaluating the best finetuned models from each
cross-validation split. for the external setting, we implement pretraining and
finetuning on the different datasets, as shown in fig 2b ; we use tcga-coad for
pretraining; then, we only use tcga-read for finetuning and final evaluation. we
implement a five-fold cross-validation for pretraining, and the best pretrained
models are used for finetuning. we split tcga-read into finetuning (60%),
validation (20%), and evaluation set (20%). for all experiments, we calculate
the average performance on the evaluation set across the best models.the number
of epochs for pretraining and finetuning are 25, the batch size is 1, the
optimizer is adam [19], and the learning rate is 1e-4 for pretraining and 5e-5
for finetuning. we used one 32gb tesla v100 sxm2 gpu and pytorch. the
concordance index (c-index) is used to measure the survival prediction
performance. we followed the previous studies [5][6][7] to partition the overall
survival (os) months into four non-overlapping intervals by using the quartiles
of event times of uncensored patients for discretized-survival c-index
calculation (see appendix 2). for each experiment, we reported the average
c-index among three-times repeated experiments. conceptionally, our method
shares a similar idea to multiple instance learning (mil) [9,23]. therefore, we
include two types of baseline models, including the mil-based models (deepset
[30], ab-mil [17], and transmil [26]) and mil multimodal-based models (mcat [6],
porpoise [7]). we follow the same data split and processing, as well as the
identical training hyperparameters and supervised fusion as above. notably,
there is no need for supervised finetuning for the baselines when using
tcga-coad (table 1), because the supervised pretraining is already applied to
the training set.
in table 1, our approach shows improved survival prediction performance on both
tcga-coad and tcga-read datasets. compared with supervised baselines, our
unsupervised data fusion is able to extract the phenotype-genotype interaction
features, leading to achieving a flexible finetuning for different data
settings. with the multimodal pretraining and finetuning, our method outperforms
state-of-the-art models by about 2% on tcga-coad and 4% tcga-read. we recognize
that the combination of image and mrna sequencing data leads to reflecting
distinguishing survival outcomes. remarkably, our model achieved positive
results even using a single-modal finetuning when compared with baselines (more
results in appendix 3.1). in the meantime, on the tcga-read, our single-modality
finetuned model achieves a better performance than multimodal finetuned baseline
models (e.g., with model pretraining via image and methylation data, we have
only used the image data for finetuning and achieved a c-index of 74.85%, which
is about 4% higher than the best baseline models). we show that with a
single-modal finetuning strategy, the model could generate meaningful embedding
to combine image-and genomicrelated patterns. in addition, our model reflects
its efficiency on the limited finetuning data (e.g., 75 patients are used for
finetuning on tcga-read, which are only 22% of tcga-coad finetuning data). in
table 1, our method could yield better performance compared with baselines on
the small dataset across the combination of images and multiple types of
genomics data. approach broadens the scope of dataset inclusion, particularly
for model finetuning and evaluation, while enhancing model efficiency on
analyzing multimodal clinical data in real-world settings. in addition, the use
of synthetic data and developing a foundation model training will be helpful to
improve the robustness of multimodal data fusion [11,15].
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 60. ablation analysis. we verify the
model efficiency by using fewer amounts of finetuning data in finetuning. for
tcga-coad dataset, we include 50%, 25%, and 10% of the finetuning data. for the
tcga-read dataset, as the number of uncensored patients is limited, we use 75%,
50%, and 25% of the finetuning data to allow at least one uncensored patient to
be included for finetuning. as shown in fig. 3a, by using 50% of tcga-coad
finetuning data, our approach achieves the c-index of 64.80%, which is higher
than the average performance of baselines in several modalities. similarly, in
fig. 3b, our model retains a good performance by using 50% or 75% of tcga-read
finetuning data compared with the average of c-index across baselines (e.g.,
72.32% versus 64.23%). for evaluating the effect of cross-modality information
extraction in the pretraining, we kept supervised model training (i.e., the
finetuning stage) while removing the unsupervised pretraining. the performance
is lower 2%-10% than ours on multi-and single-modality data. for evaluating the
genomics data usage, we designed two settings: (1) combining all types of
genomics data and categorizing them by groups; (2) removing category information
while keeping using different types of genomics data separately. our approach
outperforms the above ablation studies by 3%-7% on tcga-read and performs
similarly on tcga-coad. in addition, we replaced our unsupervised loss with
cosine similarity loss; our approach outperforms the setting of using cosine
similarity loss by 3%-6%.
developing data-efficient multimodal learning is crucial to advance the survival
assessment of cancer patients in a variety of clinical data scenarios. we
demonstrated that the proposed pathomics framework is useful for improving the
survival prediction of colon and rectum cancer patients. importantly, our
approach opens up perspectives for exploring the key insights of intrinsic
genotypephenotype interactions in complex cancer data across modalities. our
finetuning
cervical cancer is the second most common cancer among adult women. if diagnosed
early, it can be effectively treated and cured [19]. nevertheless, delayed
diagnosis of cervical cancer until an advanced stage will have a negative impact
on patient prognosis and consume medical resources. currently, early screening
of cervical cancer is recommended worldwide as an effective method to prevent
and treat cervical cancer. thin-prep cytologic test (tct) is the most common and
effective screening method for detecting cervical abnormal and premalignant
cervical lesions [5]. conventionally it is performed by visually examining the
stained cells collected through smearing on a glass slide, and generating a
diagnosis report using the descriptive diagnosis method of the bethesda system
(tbs) [15]. although tct has been widely used in clinical applications and has
significantly reduced the mortality rates caused by cervical cancer, it is still
unavailable for population-wide screening [18]. this is partly due to its
laborintensive, time-consuming, and high cost [1]. therefore, there is a high
demand for automated cervical abnormality screening to facilitate efficient and
accurate identification of cervical abnormalities.with the development of deep
learning [10], several attempts have been made to identify cervical abnormal
cells using convolutional neural networks (cnns). for example, cao et al. [2]
developed an attention feature pyramid network (attfpn) for automatic abnormal
cervical cell detection in cervical cytopathological images to assist
pathologists in making more accurate diagnoses. chen et al. [3] proposed a new
framework that decomposes tasks and compares cells for cervical lesion cell
detection. liang et al. [11] proposed to explore contextual relationships to
boost the performance of cervical abnormal cell detection. lin et al. [22]
presented an automatic cervical cell detection approach based on the
dense-cascade r-cnn. it is worth mentioning that all of the aforementioned
detection methods inevitably produce false positive results, which should be
further refined by pathologists for manual checking or classification models
established for automatic screening. to solve this problem, zhou et al. [23]
proposed a three-stage method including cell-level detection, image-level
classification, and case-level diagnosis obtained by an svm classifier. zhu et
al. [24] developed an artificial intelligence assistive diagnostic solution,
which integrated yolov3 [16] for detection, xception, and patch-based models to
boost classification.although the above-mentioned attempts can improve the
screening performance significantly, there are several issues that need to be
addressed: 1) object detection methods often require accurate annotated data to
guarantee performance with robustness and generalization. however, due to legal
limitations, the scarcity of positive samples, and especially the subjectivity
differences between cytopathologists for manual annotations [20], it is likely
to generate noisy samples that affect the performance of the detection model. 2)
conventional object detection methods intend to directly extract the feature
from the object area to locate and classify the object simultaneously. however,
in clinical practice pathologists usually examine the target cells by comparing
them to the surrounding cells to determine whether they are abnormal. therefore,
the visual feature correlations between the target cells and their surroundings
can provide valuable information to aid the screening process, which also needs
to be utilized when designing the cervical abnormal cell detection network.to
address these issues, we propose a novel method for cervical abnormal cell
detection using distillation from local-scale consistency refinement. inspired
by knowledge distillation, we construct a pre-trained patch correction network
(pcn), which is designed to exploit the supervised information from the pcn to
reduce the impact of noisy labels and utilize the contextual relationships
between cells. in our approach, we begin by utilizing retinanet [12] to locate
suspicious cells and crop the top-k suspicious cells into patches. then we feed
them into the pcn to obtain classification scores and propose a ranking loss to
refine the classifier of the detection network by correcting the score of the
detection model. in addition, we propose an roi-correlation consistency (rcc)
loss between roi features and local-scale features from the pcn, which
encourages the detector to explore the feature correlations of the suspicious
cells. our proposed method achieves improved performance during inference
without changing the detector structure.
the proposed framework is shown in fig. 1, which includes cervical abnormal cell
detection and the pcn. concerning the huge size of the whole slide image (wsi)
and the infeasibility to handle a wsi scan for detection, we crop the wsi into
images with the size of 1024 × 1024 as input to the detection. firstly, we
choose retinanet as our cervical abnormal cell detection, which uses a feature
pyramid network (fpn) backbone and attaches two subnetworks to obtain bounding
boxes and classification scores. we implement the detection to locate the
suspicious lesion cervical cells and extract the top k patches from the original
image. besides, we add the roi align layer [17] to the output of the fpn and
generate roi features. then these patches are fed into the pcn to obtain refined
scores and local-scale features. subsequently, our ranking loss is employed to
correct the score of the detection, followed by the rcc loss to capture the
contextual relationships between the extracted cells for further optimizing the
detection model. the distillation process involves leveraging the learned
knowledge and expertise from the pcn to refine the detection results of
retinanet.
in fig. 1, the detection can automatically locate the suspicious cervical
abnormal cells by providing their bounding boxes with the confidence scores. due
to the intrinsic architecture limitation of the detection and incomplete
annotations, the confidence scores output by the retinanet may not be accurate,
so we need another classification model to regrade the representative patches.
our framework leverages a local-scale classification refinement mechanism to
guide the training of the detection model. we adopt se-resnext-50 [8] as the
pcn, which has demonstrated its effectiveness in this field. the pcn is employed
to refine and enhance the retinanet proposal classifier, which is trained from a
large number of patches collected in advance with more excellent classification
performance.more specifically, the input image is processed by the base detector
f d (•) firstly to obtain the primary proposal information. the proposed pcn f c
(•) takes the top-k patches as inputs, which are cropped from original images
according to the proposal location, denoted as i p = cr(i, p), where cr(•)
denotes the crop function, i and p denote input image and proposal boxes
predicted by f d (•), respectively. similar to the retinanet proposal classifier
in f d (•), the pcn f c (•) outputs a classification distribution vector s c .
therefore, the proposed pcn f c (•) can be represented as:(the key idea is to
augment the base detector f d (•) with the pcn f c (•) in parallel to enhance
the proposal classification capability.
due to the inaccurate confidence scores output by retinanet, false positive
cells are inevitable after detection. hence, a good correction network is
required to generate more precise scores. in this work, the suspicious ranking
of the detected patches is updated by applying pcn to them. the detector is
optimized by interscale pairwise ranking loss. specifically, the ranking loss is
given by:where s c is the classification refinement score and s d is the
detection score, which enforces s d > s c + margin in training. we set margin =
0.05. such a design can enable retinanet to take the prediction score as
references, and utilize refined scores from pcn to obtain more confident
predictions. the ranking loss optimizes the detection to generate higher
confidence scores than the previous prediction, thereby suppressing false
positives and enabling the detection network to better distinguish between
positive and negative cells.
in order to solve the problem of mismatched inputs to the detection and
classification models, we add the roi align layer to the output of the fpn.
however, for cervical abnormal cell detection, normal and abnormal cells may
have very similar appearances, which might not be sufficient for conducting
effective differentiation. in clinical practice, to determine whether a cervical
cell is normal or abnormal, cytopathologists usually compare it to the
surrounding reference cells. therefore, we studied the correlation between the
top k rois to help more accurate classification of abnormal cells.based on the
consistency strategy [14], which enhances the consistency of the intrinsic
relation among different models, we propose roi-correlation consistency, which
regularizes the network to maintain the consistency of the semantic relation
between patches under roi features and local-scale features, and thereby
encourage the detector to explore the feature interaction between cells from the
extracted patches to improve the network performance.we model the structured
relation among different patches with a case-level gram matrix [6]. given an
input mini-batch with b samples, where b denotes the batch size. and each sample
undergoes the roi align layer to obtain the top k rois, we denote the activation
map of rois as f r ∈ r b×k×h×w ×c , where h and w are the spatial dimension of
the feature map, and c is the channel number. we set k = 10, h = 7, w = 7, c =
256. we average pooling the feature map f r along the spatial dimension and
reshape it into a r ∈ r bk×c , and then the case-wise gram matrix g r ∈ r bk×bk
is computed as:where g ij is the inner product between the vectorized activation
map a r i and a r j , whose intuitive meaning is the similarity between the
activations of i th roi and j th roi within the input mini-batch. the final roi
relation matrix r r is obtained by conducting the l2 normalization for each row
g r i of g r , which is expressed as:the proposed pcn f c (•) takes the b×k
proposals of box regressor as inputs, we denote the local-scale feature map by
pcn as f c ∈ r b×k×h ×w ×c , and set h = 56, w = 56. we perform average pooling
on the feature map f c across the spatial dimension and then reshape it into a c
∈ r bk×hw c , the case-wise gram matrix g c ∈ r bk×bk and the final relation
matrix r c are computed as:the rcc requires the correlation matrix to be stable
under roi features and local-scale features to preserve the semantic relation
between patches. we then define the proposed rcc loss as:where x is the
proposals from the sampled mini-batch, r c (x) and r r (x) are the correlation
matrices computed on x under different network. by minimizing l rcc during the
training process, the network could be enhanced to capture the intrinsic
relation between patches, thus helping to extract additional semantic
information from cells.
to better optimize the retinanet detector in a reinforced way, we take the
following training strategy, which consists of three major stages. in the first
stage, we collect images with doctors' labels for training and initialized the
detection net. in the second stage, we train pcn with cross-entropy loss until
convergence.in the last stage, we freeze the pcn and optimize the detector. the
detector is optimized using the total objective function, which is written as
follows:where l cls and l reg are the ordinary detection loss for each detection
head in retinanet. l cls is a cross-entropy loss for classification and l reg is
a smooth-l 1 loss for bounding box regression. l rank is the classification
ranking loss,l rrc is the rcc loss. α and β are hyper-parameters that denote the
different weights of loss. during inference, only the optimized detector is used
to output the final detection results without any additional modules.3
experimental results
dataset. for cervical cell detection, our dataset includes 3761 images of 1024 ×
1024 pixels cropped from wsis. our private dataset was collected and
qualitycontrolled according to a standard protocol involving three pathologists:
a, b, and c. pathologist a had 33 years of experience in reading cervical
cytology images, while pathologists b and c had 10 years of experience each.
initially, the images were randomly assigned to pathologist b or c for initial
labeling. later, the assigned pathologist's annotations were reviewed and
verified by the other pathologist. any discrepancies found were checked and
re-labeled by pathologist a. these images were divided into the training set and
the testing set according to the ratio of 9:1. we also collect a new dataset of
5000 positive and negative 224 × 224 cell patches to train the
pcn.implementation details. the backbone of the suspicious cell detection
network is retinanet with resnet-50 [7]. the backbone of the pre-trained patch
classification network is se-resnext-50. all parameters are optimized by adam
[9] with an initial learning rate of 4 × 10 -5 . we set α to 0.25 and β to 1
during training. the model is implemented by pytorch on 2 nvidia tesla p100
gpus. we conduct a quantitative evaluation using two metrics: the coco-style
[13] average precision (ap) and average recall (ar). we calculate the average ap
over multiple iou thresholds from 0.5 to 0.95 with a step size of 0.05, and
individually evaluated ap at the iou thresholds of 0.5 and 0.75 (denoted as ap.5
and ap.75), respectively.
comparison with sota methods. we compare the performance of our proposed method
against known methods for cervical lesion detection as well as representative
methods for object detection. table 1 presents the results, from which several
observations can be drawn. (1) among the models for object detection, retinanet
is generally superior to the other models. (2) based on retinanet, our method
improves the detection performance significantly, especially ap.5 shows great
performance improvement. this confirms the necessity and effectiveness of
introducing the classification ranking and roi-correlation consistency schemes
for cervical lesion detection.ablation study. we also perform an ablation study
to further evaluate the contributions of each part in our method. in addition,
to further show the effectiveness of our method, we visualize the feature maps
of retinanet and the proposed method in fig. 1. those feature maps are from the
conv3 stages of the class-subnet backbone. specifically, we sum and average the
features in the channel dimension, and upsample them to the original image size.
as shown in fig. 2, our method can really learn better feature representations
for abnormal cells, with the help of our proposed classification ranking
refinement and roi-correlation consistency learning. by model learning, our
method can gradually enhance the features of abnormal cell regions while
repressing noise or other suspicious but non-lesion regions.
in this paper, we integrate a distillation strategy that uses the knowledge
learned from the pre-trained pcn to guide the training of the detection model to
minimize the effects of noisy labels and explore the feature interaction between
cells. our method constructs retinanet with the pcn module which provides the
refined scores and local-scale features of extracted patches. specifically, we
propose the ranking loss by utilizing refined scores to optimize the retinanet
proposal classifier by reducing the impact of noisy labels. in addition, the roi
features generated by the detector and local-scale features from the pcn are
used for correlation consistency learning, which explores the extracted cells'
relationship. our work can achieve better performance without adding new modules
during inference. experiments demonstrate the effectiveness and robustness of
our method on the task of cervical abnormal cell detection.
reports the detailed ablation results, from which several observations can be
drawn.(1) compared with the baseline model, retinanet, our classification
ranking loss achieves considerably
staining is a vital process in preparing tissue samples for histology studies.
specifically, with dyes such as hematoxylin and eosin, transparent tissue
elements can be transformed into distinguishable features [1]. however, stain
styles can vary significantly across different pathology labs or institutions.
these variations can be due to the difference in staining materials, protocols,
or processes among different pathologists or digital scanners [16,23]. yet, the
stain variations can cause inconsistencies between human domain experts [11];
and also hinder the performance of computer-aided diagnostic (cad) systems
[5,7]. moreover, experiments have shown that stain variations can lead to a
significant decrease in the accuracy and reproducibility of deep learning
algorithms in histology analysis. consequently, it is crucial to minimize
staining variations to ensure reliable, consistent, and accurate cad systems.to
address the issue of stain variations between different domains, stain style
transfer has been proposed. while the conventional color matching [22] and stain
separation methods [19] used to be popular; learning-based approaches have
become increasingly dominant, because they eliminate the need for challenging
manual selection of the template images. for example, stain-to-stain translation
(stst) [25] approaches stain style transfer within a fully supervised 'pix2pix'
framework [12]. another approach, called staingan [26], improves on stst by
tailoring a cyclegan [34] to get rid of the dependence on learning from paired
histology images and enable an unsupervised learning manner. these methods have
shown promising results in reducing staining variations. existing learning-based
methods for stain style transfer are primarily confined to generative
adversarial networks (gans) [6] and autoencoder (ae) [2], as depicted in fig.
1(a) and (b) respectively. however, gan approaches and ae suffer from the
training of extra discriminators and challenging alignment of the posterior
distributions, respectively [27]. in contrast, diffusion models, such as the
prevalent denoising diffusion probabilistic model (ddpm) [9], have emerged as an
alternative approach that can achieve competitive performance in various
image-related tasks, such as image generation, inpainting, super-resolution, and
etc [3]. importantly, diffusion models offer several advantages over gans and
aes, including tractable probabilistic parameterization, stable training
procedures, and theoretical guarantees [3]. additionally, they can avoid some of
the challenges encountered by gans and aes, such as the alignment of posterior
distributions or training extra discriminators, leading to a simpler model and
training process. however, the applicability of diffusion models to histology
stain style transfer remains unexplored. while the current diffusion models
focus on image synthesis [9] or supervised image-to-image transaction [24], they
are not applicable to our circumstance, as obtaining paired histology slides
with different stain styles is not feasible in real clinical practice [27].
therefore, we design an innovative cycle-consistent diffusion model that allows
the transfer of representations between latent spaces at different time steps
with the same morphological structure preserved in an unsupervised manner, as
shown in fig. 1(c).the major contributions are three-fold, summarized as
follows.(1) we propose staindiff, which is the first attempt at a pure denoising
diffusion probabilistic model for stain transfer. more innovatively, unlike
existing diffusion models, staindiff is capable of learning from unpaired
histology images, making it a more flexible and practical solution. the model is
superior to gan-based methods as the training of additional discriminators is
free, and also spares for the difficulty in the alignment of posterior
probabilities in ae-based approaches.(2) we also propose a self-ensemble scheme
to further improve and stabilize the style transfer performance in staindiff.
this scheme utilizes the stochastic property of the diffusion model to generate
multiple slightly different outputs from one input at the inference stage. (3) a
broad range of histology tasks, such as stain normalization between multiple
clients, can be conveniently achieved with minor adjustment to the loss in
staindiff.
overview. the goal of this work is to design a diffusion model [9] to transfer
the stain style between two domains, i.e., x a , x b . however, the traditional
training paradigm of conditional ddpms with paired images (x a 0 , x b 0 ) ∈ x a
× x b is not feasible, as they are unavailable in the context of histology. to
overcome this limitation, we design an innovative diffusion framework for stain
style transfer, named staindiff, which leverages the success of cyclegan [34]
and style-gan [26] and thus can be trained in an unsupervised manner with a
novel cycle-consistency constraint. specifically, staindiff comprises two
forward processes that perturb the histology image of two stain style domains to
noise respectively, and two corresponding reverse processes that attempt to
reconstruct noise back to original images from the perturbed ones. the overall
training process is depicted in fig. 2. forward process. parameterized by the
markov chain, the forward process in staindiff follows the vanilla ddpm by
perturbing the histology images gradually with gaussian noise, until all
structures and morphological context information are lost. formally, given a
histology image x a 0 with respect to the stain style domain a, a transition
kernel q progressively generates a sequence oft thorough the following
equation:where n (•) denotes the gaussian distribution, i is the identity
matrix. the hyper-parameters β t s follow a linear rule as defined in ddpm [9]
to guarantee ) learns to reverse the eq. ( 1) and generate images characterized
by stain style a and b respectively, by gradually removing the noise initialized
from gaussian prior. to ensure conservative outputs [24], l1-norm denoising
objective l d [4] is leveraged to train the denoising networks in the transition
kernels. due to the absence of pixel-topixel paired histology of both stain
styles, it is infeasible to learn the interplay between them in a supervised
manner as in most previous works. consequently, a pair of auxiliary transform
networksare designed to learn the transfer between the latent variables across
the two domains in an unsupervised fashion, using a novel cycle-consistency
constraint. formally, this constraint ensures that two cycles as depicted in
fig. 2(b), derive an identity mapping, i.e.,where • denotes the composition of
operations. it follows the cycle-consistency constraint formulated bywhere xa
t+1 and xb t+1 are defined by eq. ( 2); e denotes the expectation; • is the
l1-norm. finally, the overall loss function is l = l d + γl c , balanced by the
coefficient γ. inference process and self-ensemble. we describe the inference
stage of staindiff by transferring the histology images from stain style a to b;
while the inverse, namely from b to a, is similar. given a histology image input
x a 0 characterized by stain style a, we begin by perturbing it s steps with eq.
(1) to derive x a s . choosing the optimal value for s is important, as a large
s (e.g., s = t ) leads to the loss of the contextual and structural information;
while a small valued s (e.g., s = 1) fails to inject sufficient noise for
staindiff to transfer style. an ideal range for s is a small subset from [1, t ]
that is centered by xb 0,i . the graphical model for the inference process and
the proposed self-ensemble scheme are summarized in fig. 3.extension to stain
normalization. the stain transfer primarily addresses the domain gap between two
stain styles, which is mathematically formulated as a one-to-one mapping.
meanwhile, in some clinical settings, multiple institutions or hospitals are
involved, where stain normalization is usually employed for multiple stain
styles to one style alignment. the proposed symmetric staindiff structure can be
easily adapted to support stain normalization, with minimal change to the loss
in eq. ( 3). concretely, we assume that domain a comprises multiple stain styles
and domain b identifies the targeted stain style. by discarding the second term
in eq. ( 3), staindiff becomes asymmetric and focuses specifically on the
transfer from domain a to b. this modification allows us to use staindiff for
stain normalization without any other adjustments to the inference process.
dataset-a: mitos-atypia 14 challenge1 . this dataset aims to measure the style
transfer performance on 284 histology frames. each slide is digitized by two
different scanners, resulting in stain style variations. for a fair comparison,
we follow the settings in previous work [26] by using 10,000 unpaired patches
randomly cropped from the first 184 slides of both scanners as the training set.
meanwhile, 500 paired patches are generated from the remaining 100 slides as the
test set, where we use pearson correlation coefficient (pc), structural
similarity index (ssim) [31] and feature similarity index for image quality
assessment (fsim) [33] as the evaluation metrics. ( 2) dataset-b: the cancer
genome atlas (tcga). this dataset evaluates the performance of stain
normalization quantified by the downstream nine-category tissue structure
classification accuracy [27]. domain a contains histology of multiple stain
styles, which are collected from 186 wsis from tcga-coad and nct-crc-he-100k
[14]; and domain b is the target style, curated from 25 wsis in crc-val-he-7k
[14].
all experiments are implemented in python 3.8.13 with pytorch 1.12.1 on two
nvidia geforce rtx 3090 gpu cards with 24gib of memory each in parallel. we
leverage the adam optimizer with a learning rate of 2 × 10 -4 , and a batch size
of 4. the learning scheme follows previous work [18], where the training process
continues for 100 epochs if the overall loss did not decrease to the average
loss of the previous 20 epochs. for staindiff, we set the diffusion time t =
1000, the balancing coefficient γ = 1, and ensemble number m = 10. all
experiments are repeated for 7 runs with different fixed random seeds i.e., {0,
1, 2, 3, 4, 5, 6}; and metrics are reported in the form of mean±standard
deviation. ablation study. table 1 and 2 show that incorporating a self-ensemble
scheme can both boost the performance of staindiff, and bring down the
variations, demonstrating its effectiveness in stabilizing the stain transfer
and normalization. to further investigate the effect of ensemble number m, we
conduct ablation on dataset-a. experimentally, the fsim when m = 1, 5, 10, 15,
20, 50 are 0.742, 0.749, 0.753, 0.756, 0.759, 0.759 respectively. while a slight
performance gain can be achieved with higher m values than 10, the ensemble
becomes more timeconsuming, as the cost time is linear to m. it implies an
optimal m should be selected as a trade-off between the performance and
computational time, such as 10 in this work.
in this paper, we propose staindiff, a denoising diffusion model for
histological stain style transfer, hence a model can get rid of the challenging
issues in mainstream networks, such as the mode collapses in gans or alignment
between posterior distributions in aes. innovatively, by imposing a
cycle-consistent constraint imposed on latent spaces, staindiff enables learning
from unpaired histology images, making it widely applicable to real clinical
settings. one future work will explore efficient sampling diffusion models,
e.g., ddim [28], to address the long sampling time issue as inherited from ddpm.
another direction is to investigate other formulations of the diffusion model in
the context of stain transfer, such as score-based or score-sde diffusion models
[32]. these extensions will fully expand the scope of our work, hence further
advancing towards a comprehensive solution of stain style transfer in histology
images.
a t ; 0, i). identically, we can progress the latent variablesx b 1 , x b 2 , •
• • , x bt for the histology image x b 0 from the stain style domain b in the
same fashion as eq. (1).
2 = 3 5 t . afterwards, the latent variable x a s is transferred into the
corresponding latent space with respect to stain style b with auxiliary
transform network g b , which gives us xb s = g b (x a s ). next, we use the
p-sample [9] iteratively to denoise the xb s and obtain the transferred image xb
0 . as the sampling is a stochastic process, different
whole slide imaging is capable of effectively digitizing specimen slides,
showing both the microscopic detail and the larger context, without any
significant manual effort. due to the enormous resolution of the whole slide
images (wsis), a classification based on straight-forward convolutional neural
network architectures is not feasible. multiple instance learning
[8,10,13,18,20] (mil) represents a methodology (with a high momentum indicated
by a large number of recent publications) to deal with these huge images
corresponding to single (global) labels. in the mil setting, wsis correspond to
labeled bags, whereas extracted patches correspond to unlabeled bag instances.
mil approaches typically consist of a feature extraction stage, a mil pooling
stage and a following downstream classification. state-of-the-art approaches
mainly rely on convolutional neural network architectures for feature
extraction, often in combination with attention [10,11] or self-attention [12].
for training the feature extraction stage, classical supervised and
self-supervised learning is employed [10,11]. while the majority of methods rely
on separate learning stages, also end-to-end approaches have been proposed
[3,14]. in spite of the large amount of data, the number of labeled samples in
mil (represented by the number of individual, globally labelled wsis) is often
small and/or imbalanced [6]. general data augmentation strategies, such as
rotations, flipping, stain augmentation and normalization and affine
transformations, are applicable to increase the amount of data [15]. all of
these methods are performed in the image domain.here, we consider feature-level
data augmentation directly applied to the representation extracted using a
convolutional neural network. these methods can be easily combined with
image-based augmentation and show the advantage of a high computational
efficiency (since operations are efficient and pre-computed features can be
used) [10,11]. for example, li et al. [11] proposed an augmentation strategy
based on sampling the patch-descriptors to generate several bags for an
individual wsi. in this paper, we focus on the interpolations of patch
descriptors based on the idea of zhang et al [21], which is referred to as
mixup. this method was originally proposed as data agnostic approach which also
shows good results if applied to image data [2,4,16]. variations were proposed,
to be applied to latent representations [17] as well as to balance data sets
[6]. due to the structure of mil training data, we identified several options to
perform interpolation-based data augmentation.the main contribution of this work
is a set of novel data augmentation strategies for mil, based on the
interpolation of patch descriptors. inspired by the (linear) mixup approach
[21], we investigated several ways to translate this idea to the mil setting.
beyond linear interpolation, we also defined a more flexible and novel
multilinear approach. for evaluation, a large experimental study was conducted,
including 2 histological data sets, 5 deep learning configurations for mil, 3
common data augmentation strategies and 4 mixup settings. we investigated the
classification of wsis containing thyroid cancer tissues [1,5]. to obtain an
improved understanding of reasons behind the experimental results, we also
investigate the feature distributions.
in this paper, we consider mil approaches relying on separately trained feature
extraction and classification stages [9,10,12]. the proposed augmentation
methods are applied to the patch descriptors obtained after the feature
extraction stage. this strategy is highly efficient during training since the
features are only computed once (per patch) and for augmentation only simple
arithmetic operations are applied to the (smaller) feature vectors. image-based
data augmentation strategies (such as stain-augmentation, rotations or
deformations) can be combined easily with the feature-based approaches but
require individual feature extraction during training. however, to avoid the
curse of meta-parameters and thereby experiments these methods are not
considered here.in the original mixup formulation of zhang et al. [21],
synthetic samples x are generated such that x = α • x i + (1 -α) • x j , where x
i and x j are randomly sampled raw input feature vectors. corresponding labels y
are generated such that y = α • y i + (1 -α) • y j , where y i and y j are the
corresponding one-hot label encodings. the weight α is drawn from a uniform
distribution between 0 and 1.a single input (corresponding to a wsi) of a mil
approach with a separate feature extraction stage [10] can be expressed as a
p-tupel x = (x 1 , ..., x p ) with x i being the feature vector of an individual
patch and p being the number of patches per wsi. the method proposed by zhang et
al. cannot directly be applied to these tupels. however, there are several
options to adapt the basic idea to the changed setting.
inter-mixup refers to the generation of synthetic feature vectors by linearly
combining feature vectors of a pair of wsis (see fig. 1 (a)). all features of a
wsi with index w can be represented by x (w) , such that x (w) = (x (w ) 1 , ...
, x (w ) p ) . to generate a new synthetic sample x (u) based on two samples x
(w) and x (v) , we introduce the operationwith α being a uniformly sampled
random weight (α ∈ [0, 1]). the wsi indexes v and w are uniformly sampled from
the set of indexes. the index u ranges from the 1 to the number of extracted wsi
descriptors. since the new synthetic descriptors are individually generated in
each epoch, there is no benefit if the number of extracted wsi descriptors is
increased. we fix this number to the number of wsis in the training data set, in
order to keep the number of training iterations per epoch consistent.two
different configurations are considered. firstly, we investigate the
interpolation between wsis of the same class (v1). secondly, interpolation
between all wsis is performed, which also includes the interpolation between the
labels (v2). in the case of v2, also the one-hot-encoded label vectors are
linearly combined, such that ythe random values, α, v and w are selected
individually for each individual wsi and each epoch. before applying the mixup
operation, the vector tupel is randomly shuffled (as performed in all
experiments).intra-wsi combinations (intra-mixup) refers to the generation of
synthetic descriptors by combining feature vectors within an individual wsi (see
fig. 1 (b)). a new synthetic patch descriptor x k is created based on the
randomly selected descriptors x i and x j , such that x k = α • x i + (1 -α) • x
j , with i and j being random indices (uniformly sampled from {1, 2, ..., p })
and α being a uniformly sampled random value a (α ∈ [0, 1]). the index k ranges
from 1 to the number of extracted descriptors per patch. this number was kept
stable (1024) during all experiments. the thereby obtained vector tupel (x 1 ,
..., x p ) finally represents the synthetic wsi-based image descriptor. besides
performing combinations for each wsi during training, selective interpolation
can be useful to keep real samples within the training data. this can be easily
achieved by choosing (x 1 , ..., x p ) with a chance of β and (x 1 , ..., x p )
otherwise. while the intra-mixup method described before represents a linear
interpolation method, we also investigated a multilinear approach by computing x
k such that x k = α•x i +(1-α)•x j with α being a random vector and • being the
element-wise product. this element-wise linear (multilinear) approach enables
even higher variability in the generated samples.
as experimental architecture, use the dual-stream mil approach proposed by li et
al [10]. since this model combines both, embedding-based and an instance-based
encoding, the effect of both paths can be individually investigated without
changing any other architectural details. since the method represents a
state-of-the-art approach, it further serves as well-performing baseline. in
instance-based mil, the information per patch is first condensed to a single
scalar value, representing the classification per patch. finally, all of these
patch-based values are aggregated. in embedding-based mil, the information per
patch is translated into a feature vector. all feature vectors from a wsi are
then aggregated followed by a classification. in the investigated model [10] an
instance-and an embedding-based pathway are employed in parallel and are merged
in the end by weighted addition. the embedding-based pathway contains an
attention mechanism, to higher weight patches that are similar to the so-called
critical instance. the model makes use of an individual feature extraction
stage. due to the limited number of wsis, we did not train the feature
extraction stage [7], but utilize a pre-trained network instead. specifically,
we applied a resnet18 pre-trained on the image-net challenge data, due to the
high performance in previous work on similar data [5]. resnet18 was assessed as
particularly appropriate due to the rather low dimensional output (512
dimensions). we actively decided not to use a self-supervised contrastive
learning approach [10] as feature extraction stage since invariant features
could interfere with the effect of data augmentation. we investigated various
settings consisting of instancebased only (inst), embedding-based only (emb) and
the dual-stream approach with weightings 3/1, 2/2 (balanced) and 1/3 for the
instance and the embedding-based pathways.as comparison, several other
augmentation methods on feature level are investigated including random
sampling, selective random sampling and random noise. random sampling
corresponds to the random selection of patches (feature vectors) from each wsi.
thereby the amount of investigated data per wsi is reduced with the benefit of
increasing the variability of the data. in the experiments, we adjust the sample
ratio q between the patch-based features for training and testing. a q of 50 %
indicates that 512 descriptors are used for training while for testing always a
fixed number of 1024 is used. selective random sampling corresponds to the
random sampling strategy, with the difference that the ratio of features is not
fixed but drawn from a uniform random distribution (u (q, 100 %)). here, a q of
50 % indicates that for each wsi, between 512 and 1024 feature vectors are
selected. in the case of the random noise setting, to each feature vector x i ,
a random noise vector r is added (x i = x i + r). the elements of r are randomly
sampled (individually for each x i ) from a normal distribution n (0, σ ).to
incorporate for the fact that the feature dimensions show different magnitudes,
σ is computed as the product of the meta parameter σ and the standard deviation
of the respective feature dimension.in this work, we aimed at distinguishing
different nodular lesions of the thyroid, focusing especially on benign
follicular nodules (fn) and papillary carcinomas (pc). this differentiation is
crucial, due to the different treatment options, in particular with respect to
the extent of surgical resection of the thyroid gland [19]. the data set
utilized in the experiments consists of 80 wsis overall. one half (40) of the
data set consists of frozen and the other half (40) of paraffin sections [5]),
representing the different modalities. all images were acquired during clinical
routine at the kardinal schwarzenberg hospital. procedures were approved by the
ethics committee of the county of salzburg (no. 1088/2021). the mean and median
age of patients at the date of dissection was 47 and 50 years, respectively. the
data set comprised 13 male and 27 female patients, corresponding to a slight
gender imbalance. they were labeled by an expert pathologist with over 20 years
experience. a total of 42 (21 per modality) slides were labeled as papillary
carcinoma while 38 (19 per modality) were labeled as benign follicular nodule.
for the frozen sections, fresh tissue was frozen at -15 • celsius, slides were
cut (thickness 5 µm) and stained immediately with hematoxylin and eosin. for the
paraffin sections, tissue was fixed in 4 % phosphate-buffered formalin for 24 h.
subsequently formalin fixed paraffin embedded tissue was cut (thickness 2 µm)
and stained with hematoxylin and eosin. the images were digitized with an
olympus vs120-ld100 slide loader system. overviews at a 2x magnification were
generated to manually define scan areas, focus points were automatically defined
and adapted if needed. scans were performed with a 20x objective (corresponding
to a resolution of 344.57 nm/pixel). the image files were stored in the oympus
vsi format based on lossless compression. q q q q q q q q 0 25 % 50 % 75 % 100 %
0.4 the data set was randomly separated into training (80 %) and test data (20
%). the whole pipeline, including the separation, was repeated 32 times to
achieve representative scores. due to the almost balanced setting, the overall
classification accuracy (mean and standard deviation) is finally reported. adam
was used as optimizer. the models were trained for 200 epochs with an initial
learning rate of 0.0002. random shuffling of the vector tupels (shuffling within
the wsis) was applied for all experiments.the patches were randomly extracted
from the wsi, based on uniform sampling. for each patch, we checked that at
least 75 % of the area was covered with tissue (green color channel) in order to
exclude empty areas [5]. to obtain a representation independent of the wsi size,
we extracted 1024 patches with a size of 256 × 256 pixel per wsi, resulting in
1024 patch-descriptors per wsi [5]. for feature extraction, a resnet18 network,
pretrained on the image-net challenge was deployed [10]. data and source code
are publicly accessible via https://gitlab.com/mgadermayr/mixupmil. we use the
reference implementation of the dual-stream mil approach [10]. to obtain further
insight into the feature distribution, we randomly selected patch descriptor
pairs and computed the euclidean distances. in detail, we selected 10,000 pairs
(a) from different classes, (b) from different wsis (similar and dissimilar
classes), (c,d) from the same class and different wsis, and (e) from the same
wsi.
figure 2 shows the mean overall classification accuracy and standard deviations
obtained with each individual combination. the columns represent the frozen
(left) and paraffin data set (right). the top row (a) shows the baseline scores
of embedding-based, instance-based and the 3 combinations. subfigure (b) show
the scores obtained with baseline data augmentation for embedding-based and
dual-stream mil. subfigure (c) shows the scores obtained with interpolation
between patches between (inter-mixup) and within wsis (intra-mixup). without
data augmentation, scores between 0.49 and 0.72 were obtained for frozen and
scores between 0.41 and 0.81 for the paraffin data set. to limit the number of
figures and due to the fact that instance-based mil showed weak scores only, in
the following part the focus is on embedding-based and combined-mil (2/2) only.
with baseline data augmentation, scores between 0.69 and 0.73 were achieved for
the frozen and between 0.78 and 0.83 for the paraffin data set. inter-mixup
exhibited scores up to 0.71 for the frozen and up to 0.79 for the paraffin data
set. intra-mixup showed average accuracy up to 0.78 for the frozen and up to
0.84 for the paraffin data set. the best scores were obtained with the
multilinear setting. in fig. 3, the distributions of the descriptor (euclidean)
distances between (a-d) patches from different different wsis (inter-wsi) and
(e) patches within a single wsi (intra-wsi) are provided. the mean distances
range from 171.3 to 177.8 for the inter-wsi settings. in the intra-wsi setting,
a mean distance of 134.8 was obtained. based on the used common box plot
variation (whiskers length is less than 1.5× the interquartile range), a large
number of data points was identified as outliers. however, these points are not
considered as real outliers, but occur due to the asymmetrical data distribution
(as indicated by the violin plot in the background).
in this work, we proposed and examined novel data augmentation strategies based
on the idea of interpolations of feature vectors in the mil setting.
instance-based mil did not show any competitive scores. obviously the model
reducing each patch to a single value is not adequate for the classification of
frozen or paraffin sections from thyroid cancer tissues. the considered
dual-stream approach, including an embedding and instance-based stream,
exhibited slightly improved average scores, compared to embedding-based mil
only. in our analysis, we focused on the embedding-based configuration and on
the balanced combined approach (referred to as 2/2). with the baseline data
augmentation approaches, the maximum improvements were 0.03, and 0.02 for the
frozen, and 0.01, and 0.05 for the paraffin data set. the inter-mixup approach
did not show any systematic improvements. independently of the chosen strategy
(v1, v2), concerning the combination within or between classes, we did not
notice any positive trend. the multilinear intra-mixup method, however,
exhibited the best scores for 3 out of 4 combinations and the best overall mean
accuracy for both, the frozen and the paraffin data set. also a clear trend with
increasing scores in the case of an increasing ratio of augmented data (β) is
visible. the linear method showed a similar, but less pronounced trend.
obviously, the straightforward application of the mixup scheme (as in case of
the inter-mixup approach), is inappropriate for the considered setting. an
inhibiting factor could be a high inter-wsi variability leading to incompatible
feature vectors (which are too far away from realistic samples in the feature
space). to particularly investigate this effect, we performed 2 different
inter-mixup settings (v1 & v2), with the goal of identifying the effect of mixed
(and thereby more dissimilar) or similar classes during interpolation. the
analysis of the distance distributions between patch representations confirmed
that, the variability between wsis is clearly larger than the variability within
wsis. in addition, the results showed that the variability between classes is,
on patch-level, not clearly larger than the variability within a class.
obviously variability due to the acquisition outweigh any disease specific
variability. this could provide an explanation for the effectiveness of
intra-mixup approach compared to the (similarly) poorly performing inter-mixup
settings. we expect that stain normalization methods (but not stain
augmentation) could be utilized to align the different wsis to provide a more
appropriate basis for inter-wsi interpolation. with regard to the different data
sets, we noticed a stronger, positive effect in case of the frozen section data
set. this is supposed to be due to the clearly higher variability of the frozen
sections corresponding with a need for a higher variability in the training
data. we also noticed a stronger effect of the solely embedding-based
architecture (also showing the best overall scores). we suppose that this is due
to the fact that the additional loss of the dual-stream architecture exhibits a
valuable regularization tool to reduce the amount of needed training data. with
the proposed intra-mixup augmentation strategy, this effect diminishes, since
the amount and quality of training data is increased.to conclude, we proposed
novel data augmentation strategies based on the idea of interpolations of image
descriptors in the mil setting. based on the experimental results, the
multilinear intra-mixup setting proved to be highly effective, while the
inter-mixup method showed inferior scores compared to a state-of-the-art
baseline. we learned that there is a clear difference between combinations
within and between wsis with a noticeable effect on the final classification
accuracy. this is supposedly due to the high variability between the wsis
compared to a rather low variability within the wsis. in the future, additional
experiments will be conducted including stain normalization methods and larger
benchmark data sets to provide further insights.
breast cancer (bc) is the most common cancer diagnosed among females and the
second leading cause of cancer death among women after lung cancer [1]. the bc
differs greatly in clinical behavior, ranging from carcinoma in site to
aggressive metastatic disease [2,3]. thus, effective and accurate prognosis of
bc as well as stratifying cancer patients into different subgroups for
personalized cancer management has attracted more attention than ever
before.among different types of imaging biomarkers, histopathological images are
generally considered the golden standard for bc prognosis since they can confer
important cell-level information that can reflect the aggressiveness of bc [4].
recently, with the availability of digitalized whole-slide pathological images
(wsis), many computational models have been employed for the prognosis
prediction of various subtypes of bc. for instance, lu et al [5] presented a
novel approach for predicting the prognosis of er-positive bc patients by
quantifying nuclear shape and orientation from histopathological images. liu et
al [6] developed a gradient boosting algorithm to predict the disease
progression for various subtypes of bc. however, due to the high-cost of
collecting survival information from the patients, it is still a challenge to
build effective machine learning models for specific bc subtypes with limited
annotation data.to deal with the above challenges, several researchers began to
design domain adaption algorithms, which utilize the labeled data from a related
cancer subtype to help predict the patients' survival in the target domain.
specifically, alirezazadeh et al [7] presented a new representation
learning-based unsupervised domain adaption method to predict the clinical
outcome of cancer patients on the target domain. zhang et al [8] proposed a
collaborative unsupervised domain adaptation algorithm, which conducts
transferability-aware adaptation and conquers label noise in a collaborative
way. other studies include xu et al [9] developed graph neural networks for
unsupervised domain adaptation in histopathological image analysis, based on a
backbone for embedding input images into a feature space, and a graph neural
layer for propagating the supervision signals of images with labels.although
much progress has been achieved, most of the existing studies applied the
feature alignment strategy to reduce the distribution difference between source
and target domains. however, such transfer learning methods neglected to take
the interaction among different types of tissues into consideration. for
example, it is widely recognized that tumor-infiltrating lymphocytes (tils) and
its correlation with tumors reveal a similar role in the prognosis of different
brca subtypes. for instance, kurozumi et al [10] revealed that high tils
expression was correlated with negative estrogen receptor (er) expression and
high histological grade (p < 0.001). lu et al [11] utilized the tils spatial
pattern for survival analysis in different breast cancer subtypes including
er-negative, er-positive, and triple-negative. it can be expected that better
prognosis performance can be achieved if we leveraged the tils-tumor interaction
information to resolve the survival analysis task on the target domain.based on
the above considerations, in this paper, we proposed a tils-tumor interactions
guided unsupervised domain adaptation (t2uda) algorithm to predict the patients'
survival on the target bc subtype. specifically, t2uda first applied the graph
attention network (gats) to learn node embeddings and the spatial interactions
between tumor and tils patches in wsi. in order to preserve the node-level and
interaction-level similarities across different domains, we not only aligned the
embedding for different types of nodes but also designed a novel tumor-tils
interaction alignment (ttia) module to ensure that the distribution of the
interaction weights are similar in both domains. we evaluated the performance of
our method on the breast invasive carcinoma (brca) cohort derived from the
cancer genome atlas (tcga), and the experimental results indicated that t2uda
outperforms other domain adaption methods for predicting patients' clinical
outcomes.
we summarized the proposed t2uda network in fig. 1, which consists of three
parts, i.e., graph attention network-based framework, feature alignment(fa), and
tils-tumor interaction alignment(ttia). next, we will introduce each part in
detail. data pre-processing. we obtained valid patches of 512 × 512 pixels from
pathological images and segment the tils and tumor tissues using a pre-trained
u-net++ model. then we calculated the tumor and tils area ratios in each patch
and selected 300 patches with the largest ratios of each tissue type. based on
the selected patches, we constructed a graph g = (v, e) for each wsi.
here, given patches as nodes v , we first calculated the pairwise distance among
different nodes, and select the top 10 percent connections with the smallest
distance values as edges e. for each node in v , we followed the study in [12],
which applied the resnet-101 model to extract node features. then, the principal
component analysis (pca) is implemented to reduce dimensionalities of the node
features to 128.
to characterize the interaction between different tils and tumor patches, we
employed gat [13], which has been proven to be useful in describing the spatial
interaction between different tissues across wsis. our gat-based framework
consisted of 3 gat layers interleaved with 3 graph pooling layers [14](shown in
fig. 1). the input of the gat layer are we calculated the attention coefficients
among different nodes, which can be formulated as:furthermore, a softmax
function was then adopted to normalize the attention coefficients e ij :where n
i represents all neighbors of node i. the new feature vector v i for node i was
calculated via a weighted sum:finally, the output features of each gat layer
were aggregated in the readout layer. we fed the generated output features from
each readout layer into the cox hazard proportional regression model for the
final prognosis predictions.feature alignment. in the proposed gat-based
transfer learning framework, the feature alignment component was employed on its
first two layers. then, for the node embeddings with different types (tils and
tumor) in both the source and target domain, we performed a mean pooling
operation to obtain their aggregated features. next, we aligned the aggregated
tumor or tils features from the two domains separately using maximum mean
discrepancy(mmd) [15].here, we adopted mmd for feature alignment due to its
ability to measure the distance between two distributions without explicit
assumptions on the data distribution, we showed the objective function of mmd in
our method as follows:where h is a hilbert space, f represents the features from
the source, f represents the feature from the target, r represents the layer
number, k ∈ {l, t } referred to tils or tumor node. in addition, n denotes the
number of source samples, while m refers to the number of target samples.
tils-tumor interaction alignment. to accurately characterize the interaction
between tils and tumors, we further analyzed the extracted interaction weights
by dividing them into 10 intervals (i.e., bins). for each interval, we
calculated the sum of all source domain interaction weights as i s k and the sum
of all target domain interaction weights as i t k , where k represents the k-th
interval. consequently, we obtained two vectors and applied softmax on each of
them for normalization that can be denoted as. in order to measure the
dissimilarity between p i and q i , the kullback-leibler (kl) divergence is
adapted on the third layer of gat, which can be formulated as:according to eq.(
5), we can ensure that the weight distributions for the til-tumor interaction
are consistent in the source and target domain, which will benefit the following
survival analysis. it is beneficial for the target domain.
the cox proportional hazard model was applied to predict the patients' clinical
outcome [16], and its negative log partial likelihood function can be formulated
as:where x i represents the output of the last layer for the prognosis task and
r (t i ) is the risk set at time t i , which represents the set of patients that
are still under risk before time t. in addition, δ i is an indicator variable.
sample i refers to censored patient ifoverall objective. to achieve
domain-adaptive prognosis prediction, the final loss function included the cox
loss, fa loss, and ttia loss as the following formula:where α and β represent
the weights assigned to the importance of fa component and ttia component
respectively.
datasets. we conducted our experiments on the breast invasive carcinoma (brca)
dataset from the cancer genome atlas (tcga). specifically, the brca dataset
includes 661 patients with hematoxylin and eosin (he)-stained pathological
imaging and corresponding survival information. among the collected brca
patients in tcga, the number of er positive(er+) and er negative(er-) patients
are 515 and 146, respectively. we hope to investigate if the proposed t2uda
could be used to help improve the prognosis performance of (er+) or (er-) with
the aid of the survival information on its counterpart.
the dimension of intermediate layers in gat was 256. the pooling ratio in
sagpool was set to 0.7. α and β were tuned from {0.01, 0.1}. during training,
the model was trained for 150 epochs for both the main experiment and all
comparative experiments. we used the adam optimizer with a learning rate tuned
from {1e -5, 1e -4}. we evaluated the performance of our model using the
concordance index (ci) and area under the curve (auc) as performance metrics.
both ci and auc range from 0 to 1, with larger values indicating better
prediction performance and vice versa [1].
in this study, we compared the performance of our proposed model with several
existing domain adaptation methods, including 1) ddc [17]: utilize the maximum
mean discrepancy (mmd) to calculate the domain difference loss between source
and target data and optimize both classification loss and disparity loss. 2)
dann [18]: an adversarial learning method that used gradient backpropagation to
extract domain-independent features. 3) mdd [19]: an adversarial training method
that combined metric learning and domain adaptation. 4) deepjdot [20]: an
unsupervised domain adaptation method based on optimal transport that
simultaneously learns features and optimizes classifiers by measuring joint
feature/label differences. 5) source only: it was trained on the source domain
and applied directly to the target domain. 6) t2uda-v1: it was a variant of
t2uda which didn't use ttia. the experimental results were presented in table
1.the results presented in table 1 revealed several key observations. first, our
proposed method outperformed feature alignment-based methods such as ddc and
deepjdot in terms of both ci and auc values. the reason lies in that these
methods only transferred the knowledge at the feature level and neglected the
inter-relationship between tils and tumors. second, our method outperformed
adversarial-based methods such as dann and mdd, as the high heterogeneity
between the target and source domains results in negative transfer through
adversarial training. instead of directly aligning regions, our proposed method
focused on similar tils-tumor interactions and aligning patches of the same
tissue.we also evaluated the contributions of the key components of our
framework and found that t2uda performed better than source only and t2uda-v1,
which shows the advantage of minimizing differences in tils-tumor interaction
weights.in addition, we also evaluated the patient stratification performance of
different methods. as shown in fig. 3, our proposed t2uda outperformed feature
alignment-based methods (such as ddc and deepjdot), adversarial-based methods
(such as dann and mdd), and t2uda-v1 in stratification performance, proving that
considering the interaction between tils and tumors as migration knowledge leads
to better prognostic results.we also examined the consistency of important edges
in each group of stratified patients based on the tils-tumor interaction weights
calculated by the gat-based framework in the source and target domains. as seen
in fig. 4(a), for both the source and target domains, the proportion of edges
that connect tils and tumor regions in the low-risk group was higher than that
in the highrisk group, showing that the interaction between tils and tumors
played a critical role in prognostic prediction in different bc subtypes.
furthermore, as shown in fig. 4(b), the weights of the edges connecting tumor
and tils regions were higher for patients in the low survival risk group in both
source and target domains. this was consistent with our knowledge that brisk
interaction between tils and tumor regions indicates a better clinical outcome
and demonstrates the transferability of this knowledge.
in this paper, we presented an unsupervised domain adaptation algorithm that
leverages tils-tumor interactions to predict patients' survival in a target bc
subtype(t2uda). our results demonstrated that the relationship between tils and
tumors is transferable and can be effectively used to improve the accuracy of
survival prediction models. to the best of our knowledge, t2uda was the first
method to successfully achieve interrelationship transfer between tils and
tumors across different cancer subtypes for prognosis tasks.
pathological image-omic analysis is the cornerstone of modern medicine and
demonstrates promise in a variety of different tasks such as cancer diagnosis
and prognosis [12]. with the recent advance of digital pathology and sequencing
technologies, modern cancer screening has jointly incorporated genomics and
histology analysis of whole slide images (wsis).though deep learning techniques
have revolutionized medical imaging, designing a task-specific algorithm for
image-omic multi-modality analysis is challenging. (1) the gigapixel wsis, which
generally yield 15,000 foreground patches during pre-processing, make
attention-based backbones [6] hard to extract precise image (wsi)-level
representations. (2) learning features from genomics data which have tens of
thousands of genes make models such as transformer [16] impractical to use due
to its quadratic computation complexity. (3) image-omic feature fusion [2,3] may
fail to model high-order relevance and the inherent structural characteristics
of each modality, making the fusion less effective.specifically, to our
knowledge, most multi-modality techniques have been designed for modalities such
as chest x-ray and reports [1,17,23], ct and x-ray [18], ct and mri [21], h&e
cross-staining [22] via global feature, local feature or multi-granularity
alignment. but, none of these works considers the challenges in wsis and genes
processing. besides, vision-language models in the computer vision community
stand out for their remarkable versatility [13,14]. nevertheless, constrained by
computing resources, the most commonly used multimodal representation learning
strategy, contrastive learning, which relies on a large number of negative
samples to avoid model collapse [8], is not affordable for gigapixel wsis
analysis. a big domain gap also hampers their usage in leveraging the structural
characteristic of tumor micro-environment and genomic assay. recently, the
literature corpus has proposed some methods for accomplishing specific
image-omic tasks via kronecker product fusion [2] or co-attention mapping
between wsis and genomics data [3]. but, the kronecker product overly concerns
feature interactions between modalities while ignoring high-order relevance,
w.r.t. decision boundaries across multiple samples, which is critical to
classification tasks. as for the co-attention module, it is unidirectional and
cannot localize significant regions from genetic data with a large amount of
information.in this paper, we propose a task-specific framework dubbed
gene-induced multimodal pre-training (gimp) for image-omic classification.
concretely, we first propose a transformer-based gene encoder, group multi-head
self attention (groupmsa), to capture global structured features in gene
expression cohorts. next, we design a pre-training paradigm for wsis, masked
patch modeling (mpm), masking random patch embeddings from a fixed-length
contiguous subsequence of a wsi. we assume that one patch-level feature
embedding can be reconstructed by its adjacent patches, and this process
enhances the learning ability for pathological characteristics of different
tissues. our mpm only needs to recover the masked patch embeddings in a
fixed-length subsequence rather than processing all patches from wsis.
furthermore, to model the high-order relevance of the two modalities, we combine
cls tokens of paired image and genomic data to form unified representations and
propose a triplet learning module to differentiate patient-level positive and
negative samples in a mini-batch. it is worth mentioning that although our
unified representation fuses features from the whole gene expression cohort and
partial wsis in a mini-batch, we can still learn high-order relevance and
discriminative patient-level information between these two modalities in
pre-training thanks to the triplet learning module. in addition, note that our
proposed method is different from self-supervised pre-training. specifically, we
focus not only on superior representation learning capability, but also
category-related feature distributions, w.r.t. intra-and inter-class variation.
with the training process going on, complete information from wsis can be
integrated and the fused multimodal representations with high discrimination
will make it easier for the classifier to find the classification hyperplane.
experimental results demonstrate that our gimp achieves significant improvement
in accuracy than other image-omic competitors, and our multimodal framework
shows competitive performance even without pre-training.
given a multimodal dataset d consisting of pairs of wsi pathological images and
genomic data (x i , x g ), our gimp learns feature representations via
accomplishing masked patch modeling and triplets learning. as shown in fig. 1,
the overall framework consists of three parts: 1) group-based genetic encoder
groupmsa (sect. 2.1), 2) efficient patch aggregator (sect. 2.2) and 3)
gene-induced multimodal fusion (sect. 2.3). in the subsequent sections, we will
introduce each part of our proposed framework in detail.
in this section, we propose group multi-head self attention (groupmsa), a
specialized gene encoder to capture structured features in genomic data cohorts.
specifically, inspired by tokenisation techniques in natural language processing
[16], the input expression cohort x g ∈ r nge is partitioned into n f
nonoverlapping fragments, and we then use a linear projection head to acquire
fragment features h f ∈ r n f ×d , where d is the hidden dimension. next, we
introduce an intra-and-inter attention module to capture local and global
information in h f . firstly, the fragment features are divided into groups and
there are n gr learnable group tokens linked to each group resulting in (n f /n
gr + 1) tokens per group. then the prepared tokens are fed to a vanilla
multi-head self-attention (msa) block to extract intra-group information. after
that, we model cross-group interactions by another msa layer on the global scale
with the locally learned group tokens and a final classification token cls ge ∈
r d . finally, groupmsa could learn dense semantics from the genomic data
cohort.
let's denote the whole slide pathological image with h×w spatial resolution and
c channels by x i ∈ r h×w ×c . we follow the preprocessing strategy of clam [11]
to acquire patch-level embedding sequence, i.e., each foreground patch with
256×256 pixels is fed into an imagenet-pretrained resnet50 and the background
region is discarded. let h p = h j | h j ∈ r 1024 np j=1 denote the sequence of
patch embeddings corresponding to wsi x i and note that the total patch number n
p is image-specific. since the quadratic computational complexity of the
standard self-attention mechanism is usually unaffordable in wsi analysis due to
its long instances sequence, we employ nystrom-based attention algorithm [20] to
aggregate patch embeddings and yield image-level predictions. specifically, the
input sequence h p is first embedded into a d-dimensional feature space and
combined with a classification token cls img , yielding h 0 p ∈ r (np+1)×d .
then we perform different projection operations on h 0 p :are downsampling
matrices obtained from clustering tokens in q l and k l for layer l ∈ {0, 1}.
in this section, we first describe the formulation of masked patch modeling.
then we introduce the overall pipeline of our pre-training framework and
illustrate how to apply it to downstream classification tasks.masked patch
modeling. in wsis, the foreground patches are spatially contiguous, which means
the adjacent patches have similar feature embeddings. thus, we propose a masked
patch modeling (mpm) pre-training strategy that masks random patch embeddings
from a fixed-length contiguous subsequencej=i in h p and reconstruct the
invisible information. the fixed subsequence length l is empirically set to
6,000 and the sequences shorter than l are duplicated to build mini batches.
besides, the masking ratio is set to 50% and the set of masked subscripts is
denoted as m ∈ r 0.5l . next, a two-layer nystrom-based patch aggregator
followed by a lightweight reconstruction decoder are adopted to process the
masked sequence h mpm and the reconstructed sequence is denoted as. note that we
reconstruct the missing feature embeddings rather than the raw pixels of the
masked areas, which is different from traditional mim methods like simmim [19]
and mae [5]. in this way, the model could consider latent pathological
characteristics of different tissues, which makes the pretext task more
challenging. the reconstruction l 1 loss is computed by:where 1[•] is the
indicator function.gene-induced triplet learning. the transformer-based
backbones in the classification task require the cls token to be able to extract
accurate global information, which is even more important yet difficult in wsis
due to the long sequence challenge. in addition, in order to construct the
mini-batch, the subsequences we intercept in the mpm pre-training phase may not
be sufficiently representative of the image-level characteristics. to overcome
these issues, we further propose a gene-induced triplet learning module, which
uses pathological images and genomic data as input and extracts high-order and
discriminative features via cls tokens. firstly, we pre-train the groupmsa
module by patientlevel annotations in advance and froze it in the following
iterations. next, a learnable cls token cls img for wsis is added to the input
masked sequence h mpm . after extracting the input patch embeddings and gene
sequence separately, we concatenate cls img and cls ge as cls pat ∈ r 2d to
represent patient-level characteristics.suppose we obtain a triplet list {x, x +
, x -} during current iteration, where x, x + , x -are concatenated tokens of
anchor cls pat , positive cls pat , and negative cls pat , respectively. to
enhance the global modeling capability, i.e., extracting more precise
patient-level features, we expect that the distance between the anchor and the
positive sample gets closer, while the negative sample is farther away. the loss
function for optimizing triplet learning is computed by:δ indicates a threshold,
e.g., δ = 0.8. finally, the loss function for gimp pretraining is:multimodal
fine-tuning. applying the pre-trained backbone to image-omic classification task
is straightforward, since gimp pre-training allows it to learn representative
patient-level features. we use a simple multi-layer perceptron (mlp) head to map
cls pat to the final class predictions p , which can be written as p =
softmax(mlp(cls pat )).3 experiments
datasets. we verify the effectiveness of our method on the caner genome atlas
(tcga) non-small cell lung cancer (nsclc) dataset, which contains two cancer
subtypes, i.e., lung squamous cell carcinoma (lusc) and lung adenocarcinoma
(luad). after pre-processing [11], the patch number extracted from wsis at 20×
magnification varies from 485 to 148,569. we collect corresponding rna-seq fpkm
data for each patient and the length of the input genomic sequence is 60,480.
among 946 image-omic pairs, 470 of them belong to luad and 476 cases are lusc.
we randomly split the data into 567 for training, 189 for validation and 190 for
testing.implementation details. the pre-training process of all algorithms is
conducted on the training set, without any extra data augmentation. note that
our genetic encoder, groupmsa, is fully supervised pre-trained on unimodal
genetic data to accelerate convergence and it is frozen during gimp training
process. the maximum pre-training epoch for all methods is set to 100 and we
finetune the models at the last epoch. during fine-tuning, we evaluate the model
on the validation set after every epoch, and save the parameters when it
performs the best. adamw [10] is used as our optimizer and the learning rate is
10 -4 with cosine decline strategy. the maximum number of fine-tune epoch is 70.
at last, we measure the performance on the test set. training configurations are
consistent throughout the fine-tuning process to ensure fair comparisons. all
experiments are conducted on a single nvidia geforce rtx 3090.
we conduct comparisons between gimp and three competitors under different
settings. firstly, we compare our proposed patch aggregator with the current
state-of-the-art deep mil models on unimodal tcga-nsclc dataset, i.e., only
pathological wsis are included as input. as shown in table 1, our proposed patch
aggregator outperforms all the compared attention based multiple instance
learning baselines in classification accuracy. in particular, 1.6% higher than
the abmil [6] 0.7737 dsmil [9] 0.7566 clam-sb [11] 0.8519 clam-mb [11] 0.8889
transmil [15] 0.8836 pathology w/o pre-train gimp (w/o groupmsa) 0.8995 porpoise
[4] 0.9524 pathomic fusion [2] 0.9684 mcat [3] 0.9632 w/o pre-train gimp (ours)
0.9737 mgca [17] 0.9105 biovil [1] 0.9316 refers [23] 0 second best compared
method transmil [15]. we then explore the superiority of gimp by comparing to
state-of-the-art medical multi-modal approaches. we particularly compare our
method to biovil [1], mgca [17] and refers [23], three popular multimodal
pre-training algorithms in medical text-image classification task. we can
observe in the table that, our gimp raises acc from 91.05% to 99.47% on
tcga-nsclc dataset. even without pre-training stage, gimp shows competitive
performance compared to porpoise [4], pathomic fusion [2], and mcat [3], three
influential image-omic classification architectures. we further explore why gimp
works by insightful interpretation of the proposed method with t-sne
visualisation. figure 2 shows the feature mixtureness of pre-trained cls pat
extracting global information on training set. compari- son between fig. 2 (a)
and (b) indicates that the addition of the genomic data is indispensable in
increasing the inter-class distance and reducing the intra-class distance, which
confirms our motivation that gene-induced multimodal fusion could model
high-order relevance and yield more discriminative representations. moreover,
compared to the mentioned self-supervised methods biovil [1] and mgca [17] in
fig. 2 (c) and (d), cls pat with gimp pre-trained are well separated between
luad and lusc, i.e., gimp pays more attention to the categoryrelated feature
distribution and could extract more discriminative patient-level features during
triplet learning.
table 2 summarizes the results of ablation study. we first evaluate the
effectiveness of the proposed groupmsa. in the first two rows, groupmsa achieves
0.53% improvement compared to snn [7], a popular genetic encoders used in
porpoise [4] and pathomic fusion [2]. we then analyze the effect of adding
genetic modality during pre-training. the evaluation protocol is first
pre-training, and then fine-tuning on downstream multimodal classification task.
"aggregator + mpm" means gimp only uses wsis as input and reconstructs the
missing patch embeddings during the pre-training phase. since the fixed
subsequence length l = 6000 is used in our setting, it is sometimes smaller than
the original patch number, e.g., the maximum size 148,569, the pre-trained model
without genetic guidance may be not aware of sufficiently accurate patientlevel
characteristics, i.e., ineffectively focused on normal tissues. "aggregator +
triplet" indicates using unimodal image features to build triplets. we can
likewise find that the lack of precise global representation leads to worse
performance. finally, we evaluate the necessity of the mpm module. "aggregator +
groupmsa + triplet" means gimp only combines the cls tokens of each modality and
calculates triplet loss during pre-training. we can observe a performance drop
without mpm module, e.g., from 99.47% to 95.26%, which demonstrates that local
pathological information is equally critical as high-order relevance.
in this paper, we propose a novel multimodal pre-training method to exploit the
complementary relationship of genomic data and pathological images. concretely,
we introduce a genetic encoder with structured learning capabilities and an
effective gene-induced multimodal fusion module which combines two pretraining
objectives, triplet learning and masked patch modeling. experimental results
demonstrate the superior performance of the proposed gimp compared to other
state-of-the-art methods. the contribution of each proposed component of gimp is
also demonstrated in the experiments.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2_49.
whole slide image (wsi) classification is a crucial process to diagnose diseases
in digital pathology. owing to the huge size of a wsi, the conventional wsi
classification process consists of patch decomposition and per-patch
classification, followed by the aggregation of per-patch results using multiple
instance learning (mil) for the final per-slide decision [7]. mil constructs
bag-of-features (bof) that effectively handles imperfect patch labels, allowing
weakly supervised learning using per-slide labels for wsi classification.
although mil does not require perfect per-patch label assignment, it is
important to construct good feature vectors that are easily separated into
different classes to make the classification more accurate. therefore, extensive
research has been conducted on metric and representation learning [11,13] aimed
at developing improved feature representation. recently, contrastive learning
has demonstrated its robustness in the representational ability of the feature
extractor, which employs self-supervised learning with a contrastive loss that
forces samples from the same class to stay closer in the feature space (and vice
versa). simclr [3] introduced the utilization of data augmentation and a
learnable nonlinear transformation between the feature embedding and the
contrastive loss to generally improve the quality of feature embedding. moco [6]
employed a dynamic dictionary along with a momentum encoder in the contrastive
learning model to serve as an alternative to the supervised pre-trained imagenet
model in various computer vision tasks. pcl [9] and hcsc [5] integrated the
k-means clustering and contrastive learning model by introducing prototypes as
latent variables and assigning each sample to multiple prototypes to learn the
hierarchical semantic structure of the dataset. these prior works used cosine
distance as their distance measurement, which computes the angle between two
feature vectors as shown in fig. 1(a). although cosine distance is a commonly
used distance metric in contrastive learning, we observed that the cosine
distance approximates the difference between local neighbors and is insufficient
to represent the distance between far-away points on a complicated, nonlinear
manifold.the main motivation of this work is to extend the current contrastive
learning to represent the nonlinear feature manifold inspired by manifold
learning. owing to the manifold distribution hypothesis [8], the relative
distance between high-dimensional data is preserved on a low-dimensional
manifold. isomap [12] is a well-known manifold learning approach that represents
the manifold structure by using geodesic distance (i.e., the shortest path
length between points on the manifold). there are several previous works that
use manifold learning for image classification and reconstruction tasks, such as
lu et al. [10] and zhu et al. [14]. however, the use of geodesic distance on the
feature manifold for image classification is a recent development. aziere et al.
[2] applied the random walk algorithm on the nearest neighbor graph to compute
the pairwise geodesic distance and proposed the n-pair loss to maximize the
similarity between samples from the same class for image retrieval and
clustering applications. gong et al. [4] employed the geodesic distance computed
using the dijkstra algorithm on the knearest neighbor graph to measure the
correlation between the original samples and then further divided each class
into sub-classes to deal with the problems of high spectral dimension and
channel redundancy in the hyperspectral images. however, this method captured
the nonlinear data manifold structure on the original data (not on the feature
vectors) only once at the beginning stage, which is not updated in the further
training process.in this study, we propose a hybrid method that combines
manifold learning and contrastive learning to generate a good feature extractor
(encoder) for histopathology image classification. our method uses the
sub-classes and prototypes as in conventional contrastive learning, but we
propose the use of geodesic distance in generating the sub-classes to represent
the non-linear feature manifold more accurately. by doing this, we achieve
better separation between features with large margins, resulting in improved mil
classification performance. the main contributions of our work can be summarized
as follows:-we introduce a novel integration of manifold geodesic distance in
contrastive learning, which results in better feature representation for the
non-linear feature manifold. we demonstrate that the proposed method outperforms
conventional cosine-distance-based contrastive learning methods. -we propose a
geodesic-distance-based feature clustering for efficient contrastive loss
evaluation using prototypes without brute-force pairwise feature similarity
comparison while approximating the overall manifold geometry well, which results
in reduced computation. -we demonstrate that the proposed method outperforms
other state-of-theart (sota) methods with a much smaller number of sub-classes
without complicated prototype assignment (e.g., hierarchical clustering).to the
best of our knowledge, this work is the first attempt to leverage manifold
geodesic distance in contrastive learning for histopathology wsi classification.
the overview of our proposed model is illustrated in fig. 2. it is composed of
two stages: (1) train the feature extractor using deep manifold embedding
learning and (2) train the wsi classifier using the deep manifold embedding
extracted from the first stage. the input wsis are pre-processed to extract 256
× 256 × 3 dimensional patches from the tumor area at a 10× magnification level.
patches with less than 50% tissue coverage are excluded from the experiment.
as illustrated in fig. 2(a), we first feed the patches into a feature extractor
f , which is composed of an encoder, a pooling layer, and a multi-perceptron
layer.the output is then passed through two different paths, namely, deep
manifold and softmax paths. loss functions. for the deep manifold training, we
adopted two losses: (1) intra-subclass loss l intra and (2) inter-subclass loss
l inter . the main idea in intra-subclass loss is to make the samples from the
same sub-class stay near their respective sub-class prototype. l intra is
formulated as follows:where x i j is the i-th patch in the j-th batch, j
represents the total number of batches, i represents the total number of patches
per batch, f (•) is the feature extractor, and p + indicates the positive
prototype of the patch (i.e., the prototype of the subclass containing x i j ).
the prototype of each sub-class is computed by simply taking the mean of all the
patch features that belong to each sub-class. inter-subclass loss l inter is
proposed to make the sub-classes from a different class far apart from one
another. the formulation of l inter is as shown below:where another path via
softmax is simply trained on outputs from the feature extractor with the ground
truth slide-level labels y by the cross-entropy loss l ce , which is defined as
follows:where y is the ground truth slide-level label and ŷ is predicted label.
finally, the total loss for the first stage is defined as follows:
as illustrated in fig. 2(b), in the second stage, the pre-trained feature
extractor from the previous stage is then deployed to extract features for bag
generation. a total of 50 bags are generated for each wsi, in which each bag is
composed of the concatenation of the features from 100 patches in 512
dimensions. these bags are fed into a classifier with two layers of multiple
perceptron layers (512 neurons) and a softmax layer and then trained with a
binary cross-entropy loss.after the classification, majority voting is applied
to the predicted labels of the bags to derive the final predicted label for each
wsi.
we tested our proposed method on two different tasks: (1) intrahepatic
cholangiocarcinomas(ihccs) subtype classification and (2) liver cancer type
classification. the dataset for the former task was collected from 168 patients
with 332 wsis from seoul national university hospital. ihccs can be further
categorized into small duct type (sdt) and large duct type (ldt). using gene
mutation information as prior knowledge, we collected wsis with wild kras and
mutated idh genes for use as training samples in sdt, and wsis with mutated kras
and wild idh genes for use in ldt. the rest of the wsis were used as testing
samples. the liver cancer dataset for the latter task was composed of 323 wsis,
in which the wsis can be further classified into hepatocellular carcinomas
(hccs) (collected from pathology ai platform [1]) and ihccs. we collected 121
wsis for the training set, and the remaining wsis were used as the testing set.
we used a pre-trained vgg16 with imagenet as the initial encoder, which was
further modified via deep manifold model training using the proposed manifold
and cross-entropy loss functions. the number of nearest neighbors k and the
number of sub-classes n were set to 5 and 10, respectively. in the deep manifold
embedding learning model, the learning rates were set to 1e-4 with a decay rate
of 1e-6 for the ihccs subtype classification and to 1e-5 with a decay rate of
1e-8 for the liver cancer type classification. the k-nearest neighbors graph and
the geodesic distance matrix are updated once every five training epochs, which
is empirically chosen to balance running time and accuracy. to train the mil
classifier, we set the learning rate to 1e-3 and the decay rate to 1e-6. we used
batch sizes 64 and 4 for training the deep manifold embedding learning model and
the mil classification model, respectively. the number of epochs for the deep
manifold embedding learning model was 50, while 50 and 200 epochs for the ihccs
subtype classification and liver cancer type classification, respectively. as
for the optimizer, we used stochastic gradient decay for both stages. the result
shown in the tables is the average result from 10 iterations of the mil
classification model.
the performance of different models from two different datasets is reported in
this section. for the baseline model, we chose the pre-trained vgg16 feature
extractor with an mil classifier, which is the same as our proposed model except
that the encoder is retrained using the proposed loss. two sota methods using
contrastive learning and clustering, pcl [9] and hcsc [5], are compared with our
method in this study. the mil classification result of the ihccs subtype
classification is shown in table 1. our proposed method outperformed the
baseline cnn by about 4% increment in accuracy, precision, recall, and f1 score.
note that our method only used 20 sub-classes but outperformed pcl (using 2300
sub-classes) by 4% and hcsc (using 112 sub-classes) by 5% in accuracy. the
result of liver cancer type classification is also shown in table 1. our method
achieved about 5% improvement in accuracy against the baseline and 1% to 2%
improvement in accuracy against the sota methods. moreover, it outperformed the
sota methods with far fewer prototypes and without complicated hierarchical
prototype assignments. to further evaluate the effect of prototypes, we
conducted an ablation study for different prototype assignment strategies as
shown in table 2. here, global prototypes imply assigning a single prototype per
class while local prototypes imply assigning multiple prototypes per class (one
per sub-class). when both are used together, it implies a hierarchical prototype
assignment where local prototypes interact with the corresponding global
prototype. as shown in this result, the model with local prototypes only
performed about 4% higher than did the model with global prototypes only.
meanwhile, the combination of both prototypes achieved a similar performance to
that of the model with local prototypes only. since the hierarchical (global +
local) assignment did not show a significant improvement but instead increased
computation, we used only local sub-class prototypes in our final experiment
setting. 1)-( 4) are the patches from sdt and ( 5)-( 8) are the patches from
ldt.since one of our contributions is the use of geodesic distance, we assessed
the efficacy of the method by comparing it with the performance using cosine
distance, as shown in table 3. to measure the performance of the
cosine-distancebased method, we simply replaced our proposed manifold loss with
nt-xent loss [3], which uses cosine distance in their feature similarity
measurement. two cosine distance experiments were conducted as follows: (1) use
only their groundtruth class without further dividing the samples into
sub-classes (i.e., global prototypes) and (2) divide the samples from each class
into 10 sub-classes by using k-means clustering (i.e., local prototypes). as
shown in table 3, using multiple local prototypes shows slightly better
performance compared to using global prototypes. by switching the nt-xent loss
with our geodesic-based manifold loss, the overall performance is increased by
about 2%. figure 3 visually compares the effect of the geodesic and cosine
distance-based losses. two scatter plots are t-sne projections of feature
vectors from the encoders trained using geodesic distance and cosine distance,
respectively. red dots represent sdt samples and blue dots represent ldt samples
from the ihccs dataset (corresponding histology thumbnail images are shown on
the right). in this example, all eight cases are correctly classified by the
method using geodesic distance while all cases are incorrectly classified by the
method using cosine distance. it is clearly shown that geodesic distance can
correctly measure the feature distance (similarity) on the manifold so that sdt
and ldt groups are located far away in the t-sne plot, whereas cosine distance
failed to separate these groups and they are located nearby in the plot.
in this paper, we proposed a novel geodesic-distance-based contrastive learning
for histopathology image classification. unlike conventional
cosine-distancebased contrastive learning methods, our method can represent
nonlinear feature manifold better and generate better discriminative features.
one limitation of the proposed method is the extra computation time for graph
generation and pairwise distance computation using the dijkstra algorithm. in
the future, we plan to optimize the algorithm and apply our method to other
datasets and tasks, such as multi-class classification problems and natural
image datasets.
fig. 3. comparison of geodesic and cosine distance in feature space.(
. this study was approved by the institutional review board of seoul national
university hospital (irb no.h-1011-046-339). this work was partially supported
by the national research foundation of korea (nrf-2019m3e5d2a01063819,
nrf-2021r1a6a1a13044830), the institute for information & communications
technology planning & evaluation (iitp-2023-2020-0-01819), the korea health
industry development institute (hi18c0316), the korea institute of science and
technology (kist) institutional program (2e32210 and 2e32211) and a korea
university grant.
prostate cancer (pca) diagnosis and grading rely on histopathology analysis of
biopsy slides [1]. however, prostate biopsies are known to have sampling error
as pca is heterogenous and commonly multifocal, meaning cancer legions can be
missed during the biopsy procedure [2]. if significant pca is detected on
biopsies and the patient has organ-confined cancer with no contraindications,
radical prostatectomy (rp) is the standard of care [3,4]. following rp, the
prostate is processed and slices are mounted onto slides for analysis. radical
prostatectomy histopathology samples are essential for validating the
biopsydetermined grade group [5,6]. analysis of whole-mount slides, meaning
slides that include slices of the entire prostate, provide more precise tumor
boundary detection, identification of various tumor foci, and increased tissue
for identifying morphological patterns not visible on biopsy due to a larger
field of view.field effect refers to the spread of genetic and epigenetic
alterations from a primary tumor site to surrounding normal tissues, leading to
the formation of secondary tumors. understanding field effect is essential for
cancer research as it provides insights into the mechanisms underlying tumor
development and progression. tumor-associated stroma, which consists of various
cell types, such as fibroblasts, smooth muscle cells, and nerve cells, is an
integral component of the tumor microenvironment that plays a critical role in
tumor development and progression. reactive stroma, a distinct phenotype of
stromal cells, arises in response to signaling pathways from cancerous cells and
is characterized by altered stromal cells and increased extracellular matrix
components [7,8]. reactive stroma is often associated with tumor-associated
stroma and is thought to be a result of field effects in prostate cancer.
altered stroma can create a pro-tumorigenic environment by producing a multitude
of chemokines, growth factors, and releasing reactive oxygen species [9,10],
which can lead to tumor development and aggressiveness [11]. therefore,
investigating the histological characterization of tumor-associated stroma is
crucial in gaining insights into the field effect and tumor progression of
prostate cancer.manual review for tumor-associated stroma is time-consuming and
lacks quantitative metrics [12,13]. several automated methods have been applied
to analyze the tumor-stroma relationship; however, most of them focus on
identifying a tumor-stroma ratio rather than finding reactive stroma tissue or
require pathologist input. machine learning algorithms have been used to
quantify the percentage of tumor to stroma in bladder cancer patients, but
required dichotomizing patients based on a threshold [14]. software has been
used to segment tumor and stroma tissue in breast cancer patient samples, but
the method required constant supervision by a pathologist [15]. similarly, akoya
biosciences inform software was used to quantify reactive stroma in pca, but
this method required substantial pathologist input to train the software [16].
fully automated deep-learning methods have been developed to identify
tumor-associated stroma in breast cancer biopsies, achieving an auc of 0.962 in
predicting invasive ductal cancer [13]. however, identifying tumor-associated
stroma in prostate biopsies and whole-mount histopathology slides remains
challenging.analyzing tumor-associated stroma in prostate cancer requires
combining whole-mount and biopsy histopathology slides. biopsy slides provide
information on the presence of pca, while whole-mount slides provide information
on the extent and distribution of pca, including more information on
tumor-associated stroma. combining the information from both modalities can
provide a more accurate understanding of the tumor microenvironment. in this
work, we explore the field effect in prostate cancer by analyzing
tumor-associated stroma in multimodal histopathological images. our main
contributions can be summarized as follows:-to the best of our knowledge, we
present the first deep-learning approach to characterize prostate
tumor-associated stroma by integrating histological image analysis from both
whole-mount and biopsy slides. our research offers a promising computational
framework for in-depth exploration of the field effect and cancer progression in
prostate cancer. -we proposed a novel approach for stroma classification with
spatial graphs modeling, which enable more accurate and efficient analysis of
tumor microenvironment in prostate cancer pathology. given the spatial nature of
cancer field effect and tumor microenvironment, our graph-based method offers
valuable insights into stroma region analysis. -we developed a comprehensive
pipeline for constructing tumor-associated stroma datasets across multiple data
sources, and employed adversarial training and neighborhood consistency
regularization techniques to learn robust multimodal-invariant image
representations.
accurately analyzing tumor-associated stroma requires a critical pre-processing
step of segmenting stromal tissue from the background, including epithelial
tissue. this segmentation task is challenging due to the complex and
heterogeneous appearance of the stroma. to address this, we propose utilizing
the pointrend model [17], which can handle complex shapes and appearances and
produce smooth and accurate segmentations through iterative object boundary
refinement. moreover, the model's efficiency and ability to process large images
quickly make it suitable for analyzing whole-mount slides. by leveraging the
pointrend model, we can generate stromal segmentation masks for more precise
downstream analysis.
to capture the spatial nature of field effect and analyze tumor-associated
stroma, modeling spatial relationships between stroma patches is essential. the
spatial relationship can reveal valuable information about the tumor
microenvironment, and neighboring stroma cells can undergo similar phenotypic
changes in response to cancer. therefore, we propose using a spatial patch graph
to capture the highorder relationship among stroma tissue regions. we construct
the stroma patch graph using a k-nearest neighbor (knn) graph and neighbor
sampling. the knn graph connects each stroma patch to its k nearest neighboring
patches. given a central stroma patch, we iteratively add neighboring patches to
construct the patch graph until we reach a specified layer number l to control
the subgraph size. this process results in a tree-like subgraph with each layer
representing a different level of spatial proximity to the central patch. the
use of neighbor sampling enables efficient processing of large images and allows
for stochastic training of the model.to predict tumor-associated binary labels
of stroma patches, we employ a message-passing approach that propagates patch
features in the spatial graph. to achieve this, we use graph convolutional
networks with attention, also known as graph attention networks (gats) [18]. the
gat uses an attention mechanism on node features to construct a weighting kernel
that determines the importance of nodes in the message-passing process. in our
case, the patch graph g is constructed using the stroma patches as vertices, and
we connect the nodes with edges based on their spatial proximity. each vertex v
i is associated with a feature vector h vi ∈ r n , which is first extracted by
resnet-50 model [19]. the gat layer is defined aswhere w ∈ r m ×n is a learnable
matrix transforming n -dimensional features to m -dimensional features. n e vi
is the neighborhood of the node v i connected by e in g. gat uses attention
mechanism to construct the weighting coefficients as:where t represents
transposition, is the concatenation operation, and ρ is leakyrelu function. the
final output of gat module is the tumor-associated probability of the input
patch. and the module was optimized using the crossentropy loss l gat in an
end-to-end fashion.
the labeling of tumor-associated stroma can be affected by various factors,
which can result in noisy labels. one of the reasons for noisy labels is the
irregular distribution of the field effect, which makes it challenging to define
a clear boundary between the tumor-associated and normal stroma regions.
additionally, the presence of tumor heterogeneity and the varied distribution of
tumor foci can further complicate the labeling process.to address this issue, we
propose applying neighbor consistency regularization (ncr) [20] to prevent the
model from overfitting to incorrect labels. the assumption is that overfitting
happens to a lesser degree before the final classifier, and this is supported by
moit [21], which suggests that feature representations are capable of
distinguishing between noisy and clean examples during model training. based on
this assumption, ncr introduces a neighbor consistency loss to encourage similar
predictions of stroma patches that are similar in feature space. this loss
penalizes the divergence of a patch prediction from a weighted combination of
its neighbors' predictions in feature space, where the weights are determined by
their feature similarity. specifically, the loss function is designed as
follows:where d kl is the kl-divergence loss to quantify the discrepancy between
two probability distributions, t represents the temperature and nn k (v i ) is
the set of k nearest neighbors of v i in the feature space.
biopsy and whole-mount slides provide complementary multi-modal information on
the tumor microenvironment, and combining them can provide a more comprehensive
understanding of tumor-associated stroma. however, using data from multiple
modalities can introduce systematic shifts, which can impact the performance of
a deep learning model. specifically, whole-mount slides typically contain larger
tissue sections and are processed using different protocols than biopsy slides,
which can result in differences in image quality, brightness, and contrast.
these technical differences can affect the pixel intensity distributions of the
images, leading to systematic shifts in the features that the deep learning
model learns to associate with tumor-associated stroma. for instance, a model
trained on whole-mount slides only may not generalize well to biopsy slides due
to systematic shifts, hindering model performance in the clinical application
scenario.to address the above issues, we propose an adversarial multi-modal
learning (aml) module to force the feature extractor to produce
multimodal-invariant representations on multiple source images. specifically, we
incorporate a source discriminator adversarial neural network as auxiliary
classifier. the module takes the stroma embedding as an input and predicts the
source of the image (biopsy or whole-mount) using multilayer perceptron (mlp)
with cross-entropy loss function l aml . the overall loss function of the entire
model is computed as:where hyper-parameters α and β control the impact of each
loss term. all modules were concurrently optimized in an end-to-end manner. the
stroma classifier and source discriminator are trained simultaneously, aiming to
effectively classify tumor-associated stroma while impeding accurate source
prediction by the discriminator. the optimization process aims to achieve a
balance between these two goals, resulting in an embedding space that encodes as
much information as possible about tumor-associated stroma identification while
not encoding any information on the data source. by adopting the adversarial
learning strategy, our model can maintain the correlated information and shared
characteristics between two modalities, which will enhance the model's
generalization and robustness.
in our study, we utilized three datasets for tumor-associated stroma
analysis.(1) dataset a comprises 513 tiles extracted from the whole mount slides
of 40 patients, sourced from the archives of the pathology department at
cedars-sinai medical center (irb# pro00029960). it combines two sets of tiles:
224 images from 20 patients featuring stroma, normal glands, low-grade and
highgrade cancer [22], along with 289 images from 20 patients with dense
high-grade cancer (gleason grades 4 and 5) and cribriform/non-cribriform glands
[23]. each tile measures 1200×1200 pixels and is extracted from whole slide
images captured at 20x magnification (0.5 microns per pixel). the tiles were
annotated at the pixel-level by expert pathologists to generate stroma tissue
segmentation masks and were cross-evaluated and normalized to account for stain
variability.(2) dataset b included 97 whole mount slides with an average size of
over 174,000×142,000 pixels at 40x magnification. the prostate tissue within
these slides had an average tumor area proportion of 9%, with an average tumor
area of 77 square mm. an expert pathologist annotated the tumor region
boundaries at the region-level, providing exhaustive annotations for all tumor
foci. (3) dataset c comprised 6134 negative biopsy slides obtained from 262
patients' biopsy procedures, where all samples were diagnosed as negative. these
slides are presumed to contain predominantly normal stroma tissues without
phenotypic alterations in response to cancer. dataset a was utilized for
training the stroma segmentation model. extensive data augmentation techniques,
such as image scaling and staining perturbation, were employed during the
training process. the model achieved an average test dice score of 95.57 ± 0.29
through 5-fold cross-validation. this model was then applied to generate stroma
masks for all slides in datasets b and c. to precisely isolate stroma tissues
and avoid data bleeding from epithelial tissues, we only extracted patches where
over 99.5% of the regions were identified as stroma at 40x magnification to
construct the stroma classification dataset.for positive tumor-associated stroma
patches, we sampled patches near tumor glands within annotated tumor region
boundaries, as we presumed that tumor regions represent zones in which the
greatest amount of damage has progressed. for negative stroma patches, we
calculated the tumor distance for each patch by measuring the euclidean distance
from the patch center to the nearest edge of the labeled tumor regions. negative
stroma patches were then sampled from whole mount slides with a gleason group
smaller than 3 and a tumor distance larger than 5 mm. this approach aims to
minimize the risk of mislabeling tumor-associated stroma as normal tissue.
setting a 5mm threshold accounts for the typically minimal inflammatory
responses induced by prostate cancers, particularly in lower-grade cases. to
incorporate multi-modal information, we randomly sampled negative stroma patches
from all biopsy slides in dataset c. overall, we selected over 1.1 million
stroma patches of size 256×256 pixels at 40x magnification for experiments.
during model training and testing, we performed stain normalization and standard
image augmentation methods.
for constructing knn-based patch graphs, we limited the graph size by setting k
= 4 and layer number l = 3. we controlled the strength of the ncr and aml terms
by setting α = 0.25 and β = 0.5, respectively. the adam optimizer with a
learning rate of 0.0005 was used for model training. all models were implemented
using pytorch on a single tesla v100 gpu. to evaluate the model performance, we
perform 5-fold cross-validation, where all slides are stratified by source
origin and divided into 5 subsets. in each cross-validation trial, one subset
was taken as the test set while the remaining subsets constituted the training
set. we measure the prediction performance using the area under the receiver
operating characteristic (auroc), f1 score, precision, and recall. to evaluate
the effectiveness of our proposed method, we conducted an ablation study by
comparing the performance of different model variants presented in table 1.
specifically, the base model is the resnet-50 feature extractor for
tumor-associated stroma classification. each model variant included a different
combination of modules presented in method sections. we systematically add one
or more modules to the base model to evaluate their performance contribution.
the results show that the full model outperforms the base model by a large
margin with 10.04% in auroc and 10.97% in f1 score, and each module contributes
to the overall performance. compared to the base model, the addition of the gat
module resulted in a significant improvement in all metrics, suggesting spatial
information captured by the patch graph was valuable for stroma classification.
the most notable performance improvement was achieved by the aml module, with a
5.72% increase in auroc and 5.55% increase in recall. this improvement indicates
that aml helps the model better capture the multimodal-invariant features that
are associated with tumor-associated stroma while reducing the false negative
prediction by eliminating the influence of systematic shift cross modalities.
finally, the addition of the ncr module further increased the average model
performance and improved the model robustness across 5 folds. this suggests that
ncr was effective in handling noisy labels and improving model's generalization
ability.
in conclusion, our study introduced a deep learning approach to accurately
characterize the tumor-associated stroma in multi-modal prostate histopathology
slides. our experimental results demonstrate the feasibility of using deep
learning algorithms to identify and quantify subtle stromal alterations,
offering a promising tool for discovering new diagnostic and prognostic
biomarkers of prostate cancer. through exploring field effect in prostate
cancer, our work provides a computational system for further analysis of tumor
development and progression. future research can focus on validating our
approach on larger and more diverse datasets and expanding the method to a
patient-level prediction system, ultimately improving prostate cancer diagnosis
and treatment.
the ability to predict the future risk of patients with cancer can significantly
assist clinical management decisions, such as treatment and monitoring [21].
generally, pathologists need to manually assess the pathological images obtained
by whole-slide scanning systems for clinical decision-making, e.g., cancer
diagnosis and prognosis [20]. however, due to the complex morphology and
structure of human tissues and the continuum of histologic features phenotyped
across the diagnostic spectrum, it is a tedious and time-consuming task to
manually assess the whole slide image (wsi) [12]. moreover, unlike cancer
diagnosis and subtyping tasks, survival prediction is a future state prediction
task with higher difficulty. therefore, automated wsi analysis method for
survival prediction task is highly demanded yet challenging in clinical
practice.over the years, deep learning has greatly promoted the development of
computational pathology, including wsi analysis [9,17,24]. due to the huge size,
wsis are generally cropped to numerous patches with a fixed size and encoded to
patch features by a cnn encoder (e.g., imagenet pretrained resnet50 [11]) for
further analysis. the attention-dominated learning frameworks (e.g., abmil [13],
clam [18], dsmil [16], transmil [19], scl-wc [25], hipt [4], nagcn [9]) mainly
aim to find the key instances (e.g., patches and tissues) for wsi representation
and decision-making, which prefers the needle-ina-haystack tasks, e.g., cancer
diagnosis, cancer subtyping, etc. to handle cancer survival prediction, some
researchers integrated some attribute priors into the network design [5,26]. for
example, patch-gcn [5] treated the wsi as point cloud data, and the patch-level
adjacent relationship of wsi is learned by a graph convolutional network (gcn).
however, the fixed-size patches cropped from wsi mainly contain single-level
biological entities (e.g., cells), resulting in limited structural information.
deepattnmisl [26] extracted the phenotype patterns of the patient via a
clustering algorithm, which provides meaningful medical prior to guide the
aggregation of patch features. however, this cluster analysis strategy only
focuses on a single sample, which cannot describe the whole picture of the
pathological components specific to the cancer type. additionally, existing
learning frameworks often ignore the capture of contextual interactions of
pathological components (e.g., tumor, stroma, lymphocyte, etc.), which is
considered as important evidence for cancer survival prediction tasks [1,6].
therefore, wsi-based cancer survival prediction still remains a challenging
task.in summary, to better capture the prognosis-related information in wsi, two
technical key points should be fully investigated: (1) an analysis strategy to
mine more comprehensive and in-depth prior of wsis, and (2) a promising learning
network to explore the contextual interactions of pathological components. to
this end, this paper presents a novel multi-scope analysis driven learning
framework, called hierarchical graph transformer (hgt), to pertinently resolve
the above technical key points for more reliable and interpretable w. hou and y.
he-contributed equally to this work. wsi-based survival prediction. first, to
mine more comprehensive and in-depth attribute priors of wsi, we propose a
multi-scope analysis strategy consisting of in-slide superpixels and cross-slide
clustering, which can not only extract the spatial prior but also identify the
semantic prior of wsis. second, to explore the contextual interactions of
pathological components, we design a novel learning network, i.e., hgt, which
consists of a hierarchical graph convolution layer and a transformer-based
prediction head. specifically, based on the extracted spatial topology, the
hierarchical graph convolution layer in hgt progressively aggregate the
patch-level features to the tissue-level features, so as to learn the
topological features of variant microenvironments ranging from fine-grained
(e.g., cell) to coarse-grained (e.g., necrosis, epithelium, etc.). then, under
the guidance of the identified semantic prior, the tissue-level features are
further sorted and assigned to form the feature embedding of pathological
components. furthermore, the contextual interactions of pathological components
are captured with the transformer-based prediction head, leading to reliable
survival prediction and richer interpretability. extensive experiments on three
cancer cohorts (i.e., crc, tcga-lihc and tcga-kirc) demonstrates the
effectiveness and interpretability of our framework. our codes are available at
https:// github.com/baeksweety/superpixel transformer.
figure 1 illustrates the pipeline of the proposed framework. due to the huge
size, wsis are generally cropped to numerous patches with a fixed size (i.e.,
256×256) and encoded to patch features v patch ∈ r n×d in the embedding space d
by a cnn encoder (i.e., imagenet pretrained resnet50 [11]) for further analysis,
where n is the number of patches, d = 1024 is the feature dimension. the goal of
wsi-based cancer survival prediction is to learn the feature embedding of v in a
supervised manner and output the survival risk o ∈ r 1 .however, conventional
patch-level analysis cannot model complex pathological patterns (e.g., tumor
lymphocyte infiltration, immune cell composition, etc.), resulting in limited
cancer survival prediction performance. to this end, we proposed a novel
learning network, i.e., hgt, which utilized the spatial and semantic priors
mined by a multi-scope analysis strategy (i.e., in-slide superpixel and
cross-slide clustering) to represent and capture the contextual interaction of
pathological components. our framework consists two modules: a hierarchical
graph convolutional network and a transformer-based prediction head.
unlike cancer diagnosis and subtyping, cancer survival prediction is a quite
more challenging task, as it is a future event prediction task which needs to
consider complex pathological structures [20]. however, the conventional
patch-level analysis is difficult to meet this requirement. therefore, it is
essential to extract and combine higher-level topology information for better
wsi representation. in-slide superpixel. as shown in fig . 1, we first employ a
simple linear iterative clustering (slic) [2] algorithm to detect
non-overlapping homogeneous tissues of the foreground of wsi at a low
magnification, which can be served as the spatial prior to mine the hierarchical
topology of wsi. intuitively, the cropped patches and segmented tissues in a wsi
can be considered as hierarchical entities ranging from fine-grained level
(e.g., cell) to coarse-grained level (e.g., necrosis, epithelium, etc.). based
on the in-slide superpixel, the tissue adjacency matrix e tissue ∈ r m×m can be
obtained, where m denote the number of superpixels. then, the patches in each
superpixel are further connected in an 8-adjacent manner, thus generating patch
adjacency matrix e patch ∈ r n×n . the spatial assignment matrix between cropped
patches and segmented tissues is denoted aspatch graph convolutional layer.
based on the spatial topology extracted by in-slide superpixel, the patch graph
convolutional layer (patch gcl) is designed to learn the feature of the
fine-grained microenvironment (e.g., cell) through the message passing between
adjacent patches, which can be represented as:where σ(•) denotes the activation
function, such as relu. graphconv denotes the graph convolutional operation,
e.g., gcnconv [15], graphsage [10], etc.tissue graph convolutional layer. third,
based on the spatial assignment matrix a spa , the learned patch-level features
can be aggregated to the tissue-level features which contain the information of
necrosis, epithelium, etc.where [•] t denote the matrix transpose operation. the
tissue graph convolutional layer (tissue gcl) is further designed to learn the
feature of this coarse-grained microenvironment, which can be represented as:
clinical studies have shown that cancer survival prediction requires considering
not only the biological morphology but also the contextual interactions of tumor
and surrounding tissues [1]. however, existing analysis frameworks for wsi often
ignore the capture of contextual interactions of pathological components (e.g.,
tumor, stroma, lymphocyte, etc.), resulting in limited performance and
interpretability. therefore, it is necessary to determine the feature embedding
of pathological components and investigate their contextual interactions for
more reliable predictions.cross-slide clustering. as shown in fig. 1, we perform
the k-means algorithm on the encoded patch features of all training wsis to
generate k pathological components p ∈ r k×d in the embedding space d. p
represents different pathological properties specific to the cancer type.
formally, the feature embedding of each tissue in space d is defined as the mean
feature embeddings of the patches within the tissue. and then, the pathological
component label of each tissue is determined as the component closest to the
euclidean distance of the tissue in space d. the semantic assignment matrix
between segmented tissues and pathological components is denoted as a sem ∈ r
m×k .transformer architecture. under the guidance of the semantic prior
identified by cross-slide clustering, the learned tissue features v tissue can
be further aggregated, forming a series meaningful component embeddings p
specific to the cancer type.then we employed a transformer [22] architecture to
mine the contextual interactions of p and output the predicted survival risk. as
shown in fig. 1, p is concatenated with an extra learnable regression token r
and attached with positional embeddings e p os , which are processed by:where p
out is the output of transformer, mhsa is the multi-headed self-attention [22],
ln is layer normalization and mlp is multilayer perceptron. finally, the
representation of the regression token at the output layer of the transformer,
i.e., [p out ] 0 , is served as the predicted survival risk o.
for the network training, cox loss [26] is adopted for the survival prediction
task, which is defined as:where δ i denote the censorship of i-th patient, o(i)
and o(j) denote the survival output of i-th and j-th patient in a batch,
respectively. gpus. our graph convolutional model is implemented by pytorch
geometric [7].
the initial number of superpixels of slic algorithm is set to {600, 700, 600},
and the number of clusters of k-means algorithm is set to {16, 16, 16} for crc,
tcga-lihc and tcga-kica cohorts. the non linearity of gcn is relu. the number of
transformer heads is 8, and the attention scores of all heads are averaged to
produce the heatmap of contextual interactions. hgt is trained with a mini-batch
size of 16, and a learning rate of 1e -5 with adam optimizer for 30
epochs.evaluation metric. the concordance index (ci) [23] is used to measure the
fraction of all pairs of patients whose survival risks are correctly ordered. ci
ranges from 0 to 1, where a larger ci indicates better performance. moreover, to
evaluate the ability of patients stratification, the kaplan-meier (km) analysis
is used [23]. in this study, we conduct a 5-fold evaluation procedure with 5
runs to evaluate the survival prediction performance for each method. the result
of mean ± std is reported.
we compared seven state-of-the-art methods (sotas), i.e., deepsets [27], abmil
[13], deepattnmisl [26], clam [18], dsmil [16], patchgcn [5], and transmil [19].
we also compared three baselines of our method, i.e., w/o patch gcl, w/o tissue
gcl and w/o transformer. for fair comparison, same cnn extractor (i.e. imagenet
pretrained resnet50 [11]), and survival prediction loss (i.e. cox loss [26]) is
adopt for all methods. table 1 and fig. 2 show the results of ci and km-analysis
of each method, respectively. generally, most mil methods, i.e., deepsets,
abmil, dsmil, transmil mainly focus on a few key instances for prediction, but
they do not have significant advantages in cancer prognosis. furthermore, due to
the large size of crc dataset and relatively high model complexity, patch-gcn
and transmil encountered a memory overflow when processing the crc dataset,
which limits their clinical application. deepattnmisl has a certain semantic
perception ability for patch, which achieves better performance in lihc cohort.
patchgcn is capable to capture the local contextual interactions between patch,
which also achieves satisfied performance in kirc cohort. as our method has
potential to explore the contextual interactions of pathological components,
which more in line with the thinking of pathologists for cancer prognosis. our
method achieves higher ci and relatively low p-value (< 0.05) of km analysis on
both three cancer cohorts, which consistently outperform the sotas and
baselines. in addition, the feature aggregation of the lower levels (i.e., patch
and tissue) are guided by the priors, and the mhsa is only executed on
pathological components, resulting in high efficiency even on the crc dataset.
[13] 0.580 ± 0.005 0.634 ± 0.005 0.617 ± 0.094 deepattnmisl [26] 0.570 ± 0.001
0.644 ± 0.009 0.584 ± 0.019 clam [18] 0.575 ± 0.010 0.641 ± 0.002 0.635 ± 0.006
dsmil [16] 0.550 ± 0.016 0.626 ± 0.005 0.603 ± 0.022 patchgcn [
we selected the crc dataset for further interpretable analysis, as it is one of
the leading causes of mortality in industrialized countries, and its
prognosis-related factors have been widely studied [3,8]. we trained an encoded
feature based classification model (i.e., a mlp) on a open-source colorectal
cancer dataset (i.e., nct-crc-he-100k [14]), which is annotated with 9 classes,
including: adipose tissue (adi); background (back); debris (deb); lymphocytes
(lym); mucus (muc); muscle (mus); normal colon mucosa (norm); stroma (str);
tumor (tum). the trained classification model can be used to determine the
biological semantics of the pathological components extracted by our model with
a major voting rule. figure 3 shows the original image, spatial topology,
proportion and biological meaning of pathological components, and its contextual
interactions of a typical case from crc cohort. it can be seen that the
interaction between component 1 (tum) and component 9 (str) has gained the
highest attention of the network, which is consistent with the existing
knowledge [3,8]. moreover, there is also concentration of interaction in some
other interactions, which may potentially imply some new biomarkers.
in this paper, we propose a novel learning framework, i.e., multi-scope analysis
driven hgt, to effectively represent and capture the contextual interaction of
pathological components for improving the effectiveness and interpretability of
wsi-based cancer survival prediction. experimental results on three clinical
cancer cohorts demonstrated our model achieves better performance and richer
interpretability over the existing models. in the future, we will evaluate our
framework on more tasks and further statistically analyze the interpretability
of our model to find more pathological biomarkers related to cancer prognosis.
ultrasound is a widely-used imaging modality for clinical cancer screening. deep
learning has recently emerged as a promising approach for ultrasound lesion
detection. while previous works focused on lesion detection in still images [25]
and offline videos [9,11,22], this paper explores real-time ultrasound video
lesion detection. real-time lesion prompts can assist radiologists during
scanning, thus being more helpful to improve the accuracy of diagnosis. this
task requires the model to infer faster than 30 frames per second (fps) [19] and
only previous frames are available for current frame processing.previous
general-purpose detectors [1,2] report simple and obvious fps when applied to
ultrasound videos, e.g. the red box in fig. 1(a). these fps, attributable to
non-lesion anatomies, can mislead junior readers. these anatomies appear like
lesions in certain frames, but typically show negative symptoms in adjacent
frames when scanned from different positions. so experienced radiologists will
refer to corresponding regions in previous frames, denoted as temporal contexts
(tc), to help restrain fps. if tc of a lesion-like region exhibit negative
symptoms, denoted as negative temporal contexts (ntc), radiologists are less
likely to report it as a lesion [15]. although important, the utilization of ntc
remains unexplored. in natural videos, as transitions from non-objects to
objects are implausible, previous works [1,2,20] only consider inter-object
relationships. as shown in sect. 4.4, the inability to utilize ntc is a key
issue leading to the fps reported by general-purpose detectors.to address this
issue, we propose a novel ultradet model to leverage ntc. for each region of
interest (roi) r proposed by a basic detector, we extract temporal contexts from
previous frames. to compensate for inter-frame motion, we generate deformed
grids by applying inverse optical flow to the original regular roi grids,
illustrated in fig. 1. then we extract the roi features from the deformed grids
in previous frames and aggregate them into r. we call the overall process
negative temporal context aggregation (ntca). the ntca module leverages
roi-level ntc which are crucial for radiologists but ignored in previous works,
thereby effectively improving the detection performance in a reliable and
interpretable way. we plug the ntca module into a basic real-time detector to
form ultradet. experiments on cva-bus dataset [9] demonstrate that ultra-det,
with real-time inference speed, significantly outperforms previous works,
reducing about 50% fps at a recall rate of 0.90.our contributions are four-fold.
(1) we identify that the failure of generalpurpose detectors on ultrasound
videos derives from their incapability of utilizing negative temporal contexts.
(2) we propose a novel ultradet model, incorpo-rating an ntca module that
effectively leverages ntc for fp suppression. (3) we conduct extensive
experiments to demonstrate the proposed ultradet significantly outperforms the
previous state-of-the-arts. (4) we release high-quality labels of the cva-bus
dataset [9] to facilitate future research.
real-time video object detection is typically achieved by single-frame
detectors, often with temporal information aggregation modules. one-stage
detectors [5,8,16,21] use only intra-frame information, detr-based detectors
[20,26] and faster r-cnn-based detectors [1,2,7,14,23,28] are also widely
utilized in video object detection. they aggregate temporal information by
mining inter-object relationships without considering ntc.ultrasound lesion
detection [10] can assist radiologists in clinical practice. previous works have
explored lesion detection in still images [25] and offline videos [9,11,22].
real-time video lesion detection is underexplored. in previous works, yolo
series [17,24] and knowledge distillation [19] are used to speed up inference.
however, these works use single-frame detectors or post-process methods while
learnable inter-frame aggregation modules are not adopted. thus their
performances are far from satisfactory.optical flow [3] is used to guide
ultrasound segmentation [12], motion estimation [4] and elastography [13]. for
the first time, we use inverse optical flow to guide temporal context
information extraction. in real-time video lesion detection, given the current
frame i t and a sequence of t previous frames as {i τ } t-1 τ =t-t , the goal is
to detect lesions in i t by exploiting the temporal information in previous
frames as illustrated in fig. 2.
the basic real-time detector comprises three main components: a lightweight
backbone (e.g. resnet34 [6]), a region proposal network (rpn) [14], and a
temporal relation head [2]. the backbone is responsible for extracting feature
map f τ of frame i τ . the rpn generates proposals consisting of boxes b τ and
proposal features q τ using roi align and average pooling:whereto aggregate
temporal information, proposals from all t + 1 frames are fed into the temporal
relation head and updated with inter-lesion information extracted via a relation
operation [7]:where l = 1, • • • , l represent layer indices, b and q are the
concatenation of all b τ and q τ , and q 0 = q. we call this basic real-time
detector basicdet. the basicdet is conceptually similar to rdn [2] but does not
incorporate relation distillation since the number of lesions and proposals in
this study is much smaller than in natural videos.
in this section, we present the negative temporal context aggregation (ntca)
module. we sample t ctxt context frames from t previous frames, then extract
temporal contexts (tc) from context frames and aggregate them into proposals. we
illustrate the ntca module in fig. 3 and elaborate on details as follows.
inverse optical flow align. we propose the inverse optical flow align (iof
align) to extract tc features. for the current frame i t and a sampled context
frame i τ with τ < t, we extract tc features from the context feature map f τ
with the corresponding regions. we use inverse optical flowto transform the rois
from frame t to τ : o t→τ = flownet(i t , i τ ) where h, w represent height and
width of feature maps. the flownet(i t , i τ ) is a fixed network [3] to predict
optical flow from i t to i τ . we refer to o t→τ as inverse optical flow because
it represents the optical flow in inverse chronological order from t to τ . we
conduct iof align and average pooling to extract c t,τ :where iofalign(f τ , b t
, o t→τ ) extracts context features in f τ from deformed grids generated by
applying offsets o t→τ to the original regular grids in b t , which is
illustrated in the fig. 1(b).temporal aggregation. we concatenate c t,τ in all t
ctxt context frames to form c t and enhance proposal features by fusing c t into
q t :where [18]. we refer to the concatenation of all tc-enhanced proposal
features in t + 1 frames as q ctxt . to extract consistent tc, the context
frames of t previous frames are shared with the current frame.
we integrate the ntca module into the basicdet introduced in sect. 3.1 to form
the ultradet model, which is illustrated in fig. 2. the head of ultradet
consists of stacked ntca and relation modules:during training, we apply
regression and classification losses l = l reg + l cls to the current frame. to
improve training efficiency, we apply auxiliary losses l aux = l to all previous
t frames. during inference, the ultradet model uses the current frame and t
previous frames as inputs and generates predictions only for the current frame.
this design endows the ultradet with the ability to perform real-time lesion
detection.
cva-bus dateset. we use the open source cva-bus dataset that consists of 186
valid videos, which is proposed in cva-net [9]. we split the dataset into
train-val (154 videos) and test (32 videos) sets. in the train-val split, there
are 21423 frames with 170 lesions. in the test split, there are 3849 frames with
32 lesions. we focus on the lesion detection task and do not utilize the
benign/malignant classification labels provided in the original
dataset.high-quality labels. the bounding box labels provided in the original
cva-bus dataset are unsteady and sometimes inaccurate, leading to jiggling and
inaccurate model predictions. we provide a new version of high-quality labels
that are re-annotated by experienced radiologists. we reproduce all baselines
using our high-quality labels to ensure a fair comparison. visual comparisons of
two versions of labels are available in supplementary materials. to facilitate
future research, we will release these high-quality labels.
pr80, pr90. in clinical applications, it is important for detection models to be
sensitive. so we provide frame-level precision values with high recall rates of
0.80 and 0.90, which we denote as pr80 and pr90, respectively. fp80, fp90. we
further report lesion-level fp rates as critical metrics. framelevel fps are
linked by iou scores to form fp sequences [24]. the number of fp sequences per
minute at recall rates of 0.80 and 0.90 are reported as fp80 and fp90,
respectively. the unit of lesion-level fp rates is seq/min. ap50. we provide
ap50 instead of map or ap75 because the iou threshold of 0.50 is sufficient for
lesion localization in clinical practice. higher thresholds like 0.75 or 0.90
are impractical due to the presence of blurred lesion edges.r@16. to evaluate
the highest achievable sensitivity, we report the frame-level average recall
rates of top-16 proposals, denoted as r@16.
ultradet settings. we use flownets [3] as the fixed flownet in iof align and
share the same finding with previous works [4,12,13] that the flownet trained on
natural datasets generalizes well on ultrasound datasets. we set the pooling
stride in the flownet to 4, the number of ultradet head layers l = 2, the number
of previous frames t = 15 and t ctxt = 2, and the number of proposals is 16. we
cached intermediate results of previous frames and reuse them to speed up
inference. other hyper-parameters are listed in supplementary materials.shared
settings. all models are built in pytorch framework and trained using eight
nvidia geforce rtx 3090 gpus. we use resnet34 [6] as backbones and set the
number of training iterations to 10,000. we set the feature dimensions of
detection heads to 256 and baselines are re-implemented to utilize only previous
frames. we refer to our code for more details.
quantitative results. we compare performances of real-time detectors with the
ultradet in table 1. we perform 4-fold cross-validation and report the mean
values and standard errors on the test set to mitigate fluctuations. the
ultradet outperforms all previous state-of-the-art in terms of precision and fp
rates.especially, the pr90 of ultradet achieves 90.8%, representing a 5.4%
absolute improvement over the best competitor, ptseformer [20]. moreover, the
fp90 of ultradet is 5.7 seq/min, reducing about 50% fps of the best competitor,
ptseformer. although cva-net [9] achieve comparable ap50 with our method, we
significantly improve precision and fp rates over the cva-net [9]. importance of
ntc. in fig. 4(a), we illustrate the fp ratios that can be suppressed by using
ntc. the determination of whether fps can be inhibited by ntc is based on manual
judgments of experienced radiologists. we find that about 50%-70% fps of
previous methods are suppressible. however, by utilizing ntc in our ultradet, we
are able to effectively prevent this type of fps.inference speed. we run
inference using one nvidia geforce rtx 3090 gpu and report the inference speed
in table 1. the ultradet achieves an inference speed of 30.4 fps and already
meets the 30 fps requirement. using ten-sorrt, we further optimize the speed to
35.2 fps, which is sufficient for clinical applications [19].qualitative
results. effectiveness of each sub-module. we ablate the effectiveness of each
submodule of the ntca module in table 2. specifically, we replace the iof align
with an roi align and the temporal aggregation with a simple average pooling in
the temporal dimension. the results demonstrate that both iof align and temporal
aggregation are crucial, as removing either of them leads to a noticeable drop
in performance. design of the ntca module. besides roi-level tc aggregation in
ultradet, feature-level aggregation is also feasible. we plug the optical flow
feature warping proposed in fgfa [28] into the basicdet and report the results
in table 3.
we find roi-level aggregation is more effective than feature-level, and
bothlevel aggregation provides no performance gains. this conclusion agrees with
radiologists' skills to focus more on local regions instead of global
information.
in this paper, we address the clinical challenge of real-time ultrasound lesion
detection. we propose a novel negative temporal context aggregation (ntca)
module, imitating radiologists' diagnosis processes to suppress fps. the ntca
module leverages negative temporal contexts that are essential for fp
suppression but ignored in previous works, thereby being more effective in
suppressing fps. we plug the ntca module into a basicdet to form the ultradet
model, which significantly improves the precision and fp rates over previous
state-ofthe-arts while achieving real-time inference speed. the ultradet has the
potential to become a real-time lesion detection application and assist
radiologists in more accurate cancer diagnosis in clinical practice.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2_1.
whole slide scanning is increasingly used in disease diagnosis and pathological
research to visualize tissue samples. compared to traditional microscopebased
observation, whole slide scanning converts glass slides into gigapixel digital
images that can be conveniently stored and analyzed. however, the high
resolution of wsis also makes their automated classification challenging [15].
patchbased classification is a common solution to this problem [3,8,24]. it
predicts the slide-level label by first predicting the labels of small, tiled
patches in a wsi. this approach allows for the direct application of existing
image classification models, but requires additional patch-level labeling.
unfortunately, patch-level labeling by histopathology experts is expensive and
time-consuming. therefore, many weakly-supervised [8,24] and semi-supervised
[3,5] methods have been proposed to generate patch-level pseudo labels at a
lower cost. however, the lack of reliable supervision directly hinders the
performance of these methods, and serious class-imbalance problems could arise,
as tumor patches may only account for a small portion of the entire wsi [12].in
contrast, mil-based methods have become increasingly preferred due to their only
demand for slide-level labels [18]. the typical pipeline of mil methods is shown
in fig. 1, where wsis are treated as bags, and tiled patches are considered as
instances. the aim is to predict whether there are positive instances, such as
tumor patches, in a bag, and if so, the bag is considered positive as well. in
practice, a fixed imagenet pre-trained feature extractor g(•) is usually used to
convert the tiled patches in a wsi into feature maps due to limited gpu memory.
these instance features are then aggregated by a(•) into a slide-level feature
vector to be sent to the bag-level classifier f (•) for mil training. due to the
high computational cost, end-to-end training of the feature extractor and bag
classifier is prohibitive, especially for high-resolution wsis. as a result,
many methods focus solely on improving a(•) or f (•), leaving g(•) untrained on
the wsi dataset (as shown in fig. 2(b)). however, the domain shift between wsi
and natural images may lead to sub-optimal representations, so recently there
have been methods proposed to fine-tune g(•) using self-supervised techniques
[4,12,21] or weakly-supervised techniques [10,13,23] (as shown in fig. 2(c)).
nevertheless, since these two processes are still trained separately with
different supervision signals, they lack joint optimization and may still leads
to inconsistency within the entire mil pipeline. to address the challenges
mentioned above, we propose a novel mil framework called icmil, which can
iteratively couple the patch feature embedding process with the bag-level
classification process to enhance the effectiveness of mil training (as
illustrated in fig. 2(d)). unlike previous works that mainly focused on
designing sophisticated instance aggregators a(•) [12,14,20] or bag classifiers
f (•) [9,16,25], we aim to bridge the loss back-propagation process from f (•)
to g(•) to improve g(•)'s ability to perceive slide-level labels. specifically,
we propose to use the bag-level classifier f (•) to initialize an instance-level
classifier f (•), enabling f (•) to use the category knowledge learned from
bag-level features to determine each instance's category. in this regard, we
further propose a teacher-student [7] approach to effectively generate pseudo
labels and simultaneously fine-tune g(•). after fine-tuning, the domain shift
problem is alleviated in g(•), leading to better patch representations. the new
representations can be used to train a better bag-level classifier in return for
the next round of iteration.in summary, our contributions are: (1) we propose
icmil which bridges the loss propagation from the bag classifier to the patch
embedder by iteratively coupling them during training. this framework fine-tunes
the patch embedder based on the bag-level classifier, and the refined
embeddings, in turn, help train a more accurate bag-level classifier. (2) we
propose a teacher-student approach to achieve effective and robust knowledge
transfer from the bag-level classifier f (•) to the instance-level
representation embedder g(•). (3) we conduct extensive experiments on two
datasets using three different backbones and demonstrate the effectiveness of
our proposed framework.
the general idea of icmil is shown in fig. 3, which is inspired by the
expectation-maximization (em) algorithm. em has been used with mil in some
previous works [13,17,22], but it was only treated as an assisting tool for
aiding the training of either g(•) or f (•) in the traditional mil pipelines. in
contrast, we are the first to consider the optimization of the entire mil
pipeline as an em alike problem, utilizing em for coupling g(•) and f (•)
together iteratively. to begin with, we first employ a traditional approach to
train a bag-level classifier f (•) on a given dataset, with patch embeddings
generated by a fixed resnet50 [6] pre-trained on imagenet [19] (step 1 in fig.
3). subsequently, this f (•) is considered as the initialization of a hidden
instance classifer f (•), generating pseudo-labels for each instance-level
representation. this operation is feasible when the bag-level representations
aggregated by a(•) are in the same hidden space as the instance representations,
and most aggregation methods (e.g., max pooling, attention-based) satisfy this
condition since they essentially make linear combinations of instance-level
representations.next, we freeze the weights of f (•) and fine-tune g(•) with the
generated pseudo-labels (step 2 in fig. 3), of which the detailed implementation
is presented in sect. 2.3. after this, g(•) is fine-tuned for the specific wsi
dataset, which allows it to generate improved representations for each instance,
thereby enhancing the performance of f (•). moreover, with a better f (•), we
can use the iterative coupling technique again, resulting in further performance
gains and mitigation to the distribution inconsistencies between instance-and
bag-level embeddings.
although most instance aggregators are compatible with icmil, they still have an
impact on the efficiency and effectiveness of icmil. in addition to that a(•)
has to project the bag representations to the same hidden space as the instance
representations, it also should avoid being over-complicated. otherwise, a(•)
may lead to larger difference between the decision boundaries of bag-level
classifer f (•) and instance-level classifier f (•), which may cause icmil
taking more time to converge.therefore, in our experiments, we choose to use the
attention-based instance aggregation method [9] which has been widely used in
many of the existing mil frameworks [9,16,25]. for a bag that contains k
instances, attention-based aggregation method firstly learns an attention score
for each instance. then, the aggregated bag-level representation h is defined
as:where a k is the attention score for the k-th instance h k in the bag.
obviously, h and h k remains in the same hidden space, satisfying the
prerequisite of icmil.
we propose a novel teacher-student model for accurate and robust label
propagation from f (•) to g(•). the model's architecture is depicted in fig. 4.
in contrast to the conventional approach of generating all pseudo labels and
retraining g(•) from scratch, our proposed method can simultaneously process the
pseudo label generation and g(•) fine-tuning tasks, making it more flexible.
moreover, incorporating augmented inputs in the training process allows for the
better utilization of supervision signals, resulting in a more robust g(•). we
also introduce a learnable f (•) to self-adaptively modifying the instance-level
decision boundary for more effective fine-tuning of the embedder. specifically,
we freeze the weights of g(•) and f (•) and set them as the teacher. we then
train a student patch embedding network, g (•), to learn category knowledge from
the teacher. for a given patch input x, the teacher generates the corresponding
pseudo label, while the student receives an augmented image x and attempts to
generate a similar prediction to that of the teacher through a consistency loss
l c . this loss function is defined as:where f (•) and f (•) are teacher
classifer and student classifier respectively, f (•) c indicates the c-th
channel of f (•), and c is the total number of channels. additionally, during
training, a learnable instance-level classifier is used on the student to
back-propagate the gradients to g (•). the initial weights of f (•) are the same
as those of f (•), as the differences in the instance-and bag-level
classification boundaries is expected to be minor. to make f (•) not so
different from f (•) during training, a weight similarity loss, l w , is further
imposed to constrain it by drawing closer their each layer's outputs under the
same input. by applying l w , the patch embeddings from g (•) can still suit the
bag-level classification task well, rather than being tailored solely for the
instance-level classifier f (•). l w is defined as:where f (•) l c indicates the
c-th channel of l-th layer's output in f (•). the overall loss function for this
step is l c + αl w , with α set to 0.5 in our experiments.
our experiments utilized two datasets, with the first being the publicly
available breast cancer dataset, camelyon16 [1]. this dataset consists of a
total of 399 wsis, with 159 normal and 111 metastasis wsis for the training set,
and the remaining 129 for test. although patch-level labels are officially
provided in camelyon16, they were not used in our experiments.the second dataset
is a private hepatocellular carcinoma (hcc) dataset collected from sir run run
shaw hospital, hangzhou, china. this dataset comprises a total of 1140 valid
tumor wsis scanned at 40× magnification, and the objective is to identify the
severity of each case based on the edmondson-steiner (es) grading. the ground
truth labels are binary classes of low risk and high risk, which were provided
by experienced pathologists.
for camelyon16, we tiled the wsis into 256×256 patches on 20× magnification
using the official code of [25], while for the hcc dataset the patches are
384×384 on 40× magnification following the pathologists' advice. for both
datasets, we used an imagenet pre-trained resnet50 to initialize g(•). the
instance embedding process was the same of [16], which means for each patch, it
would be
(w/ ab-mil) 90.0 firstly embedded into a 1024-dimension vector, and then be
projected to a 512dimension hidden space for further bag-level training. for the
training of bag classifier f (•), we used an initial learning rate of 2e-4 with
adam [11] optimizer for 200 epochs with batch size being 1. camelyon16 results
are reported on the official test split, while the hcc dataset used a 7:1:2
split for training, validation and test. for the training of patch embedder
g(•), we used an initial learning rate of 1e-5 with adam [11] optimizer with the
batch size being 100. three metrics were used for evaluation. namely, area under
curve (auc), f1 score, and slide-level accuracy (acc). experiments were all
conducted on a nvidia tesla m40 (12gb).
ablation study. the results of ablation studies are presented in table 1. from
table 1(a), we can learn that as the number of icmil iteration increases, the
performance will also go up until reaching a stable point. since the number of
instances is very large in wsi datasets, we empirically recommend to choose to
run icmil one iteration for fine-tuning g(•) to achieve the balance between
performance gain and time consumption. from table 1(b), it is shown that our
teacher-student-based method outperforms the naïve "pseudo label generation"
method for fine-tuning g(•), which demontrates the effectiveness of introducing
the learnable instance-level classifier f (•).comparison with other methods.
experimental results are presented in table 2. as shown, our icmil framework
consistently improves the performance of three different mil baselines (i.e.,
max pool, ab-mil, and dtfd-mil), demonstrating the effectiveness of bridging the
loss back-propagation from bag calssifier to embedder. it proves that a more
suitable patch embedding can greatly enhance the overall mil classification
framework. when used with the state-of-the-art mil method dtfd-mil, icmil
further increases its performance on camelyon16 by 0.5% auc, 2.1% f1, and 1.6%
acc. results on the hcc dataset also proves the effectiveness of icmil, despite
the minor difference on the relative performance of baseline methods. mean
pooling performs better on this dataset due to the large area of tumor in the
wsis (about 60% patches are tumor patches), which mitigates the impact of
average pooling on instances. also, the performance differences among different
vanilla mil methods tends to be smaller on this dataset since risk grading is a
harder task than camelyon16. in this situation, the quality of instance
representations plays a crucial role in generating more separable bag-level
representations. as a result, after applying icmil on the mil baselines, these
methods all gain great performance boost on the hcc dataset. furthermore, fig. 5
displays the instance-level and bag-level representations of camelyon16 dataset
before and after applying icmil on ab-mil backbone.the results indicate that one
iteration of g(•) fine-tuning in icmil significantly improves the instance-level
representations, leading to a better aggregated baglevel representation
naturally. besides, the bag-level representations are also more closely aligned
with the instance representations, proving that icmil can reduce the
inconsistencies between g(•) and f (•) by coupling them together for training,
resulting in a better separability.
in this work, we propose icmil, a novel framework that iteratively couples the
feature extraction and bag classification stages to improve the accuracy of mil
models. icmil leverages the category knowledge in the bag classifier as pseudo
supervision for embedder fine-tuning, bridging the loss propagation from
classifier to embedder. we also design a two-stream model to efficiently
facilitate such knowledge transfer in icmil. the fine-tuned patch embedder can
provide more accurate instance embeddings, in return benefiting the bag
classifier. the experimental results show that our method brings consistent
improvement to existing mil backbones.
. this work was supported by the national key research and development project
(no. 2022yfc2504605), national natural science foundation of china (no.
62202403) and hong kong innovation and technology fund (no. prp/034/22fx). it
was also supported in part by the grant in aid for scientific research from the
japanese ministry for education, science, culture and sports (mext) under the
grant no. 20kk0234, 21h03470.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 45.
histology is critical for accurately diagnosing all cancers in modern medical
imaging analysis. however, the complex scanning procedure for histological
wholeslide images (wsis) digitization may result in the alteration of tissue
structures, due to improper removal, fixation, tissue processing, embedding, and
storage [11]. typically, these changes in tissue details can be caused by
various extraneous factors such as bubbles, tissue folds, uneven illumination,
pen marks, altered staining, and etc [13]. formally, the changes in tissue
structures are known as artifacts. the presence of artifacts not only makes the
analysis more challenging for pathologists but also increases the risk of
misdiagnosis for computer-aided diagnosis (cad) systems [14]. particularly, deep
learning models, which have become increasingly prevalent in histology analysis,
have shown vulnerability to the artifact, resulting in a two-times increase in
diagnosis errors [18]. [19] formulates the artifact restoration as an
image-to-image transfer problem. it leverages two pairs of the generator and
discriminator to learn the transfer between the artifact and artifactfree image
domains. (b) diffusion probabilistic model [5] (ours) formulates artifact
restoration as a regional denoising process.in real clinical practice,
rescanning the wsis that contain artifacts can partially address this issue.
however, it may require multiple attempts before obtaining a satisfactory wsi,
which can lead to a waste of time, medical resources, and deplete tissue
samples. discarding the local region with artifacts for deep learning models is
another solution, but it may result in the loss of critical contextual
information. therefore, learning-based artifact restoration approaches have
gained increasing attention. for example, cyclegan [19] formulates the artifact
restoration as an image-to-image transfer problem by learning the transfer
between the artifact and artifact-free image domains from unpaired images, as
depicted in fig. 1(a). however, existing artifact restoration solutions are
confined to generative adversarial networks (gans) [2], which are difficult to
train due to the mode collapse and are prone to suffer from unexpected stain
style mistransfer. to address these issues, we make the first attempt at a
diffusion probabilistic model for artifact restoration approach [5], as shown in
fig. 1(b). innovatively, our framework formulates the artifact restoration as a
regional denoising process, which thus can to the most extent preserve the stain
style and avoid the loss of contextual information in the non-artifact region.
furthermore, our approach is trained solely with artifact-free images, which
reduces the difficulty in data collection.the major contributions are two-fold.
(1) we make the first attempt at a denoising diffusion probabilistic model for
artifact removal, called artifusion. this approach differs from gan-based
methods that require either paired or unpaired artifacts and artifact-free
images, as our artifusion relies solely on artifact-free images, resulting in a
simplified training process. (2) to capture the local-global correlations in the
gradual regional artifact restoration process, we innovatively propose a
swin-transformer denoising architecture to replace the commonly-used u-net and a
time token scheme for optimal swin-transformer denoising. extensive evaluations
on real-world histology datasets and downstream tasks demonstrate the
superiority of our framework in artifact removal performance, which can generate
reliable restored images while preserving the stain style.
overall pipeline. the proposed histology artifact restoration diffusion model
artifusion, comprises two stages, namely the training, and inference. during the
training stage, artifusion learns to generate regional histology tissue
structures based on the contextual information from artifact-free images. in the
inference stage, artifusion formulates the artifact restoration as a gradual
denoising process. specifically, it first replaces the artifact regions with
gaussian noise, and then gradually restores them to artifact-free images using
the contextual information from nearby regions.diffusion training stage. the
proposed artifusion learns the capability of generating local tissue
representation from contextual information during the training stage. to achieve
this, we follow the formulations of ddpm [5], which involve a forward process
that gradually injects gaussian noise into an artifact-free image and a reverse
process that aims to reconstruct images from noise. during the forward process,
we can obtain a noisy version of x t for arbitrary timestep t ∈ n[0, t ] using a
gaussian transition kernel q, where β t ∈ (0, 1) are predefined hyper-parameters
[5]. simultaneously, the reverse process trains a denoising network p θ (x t-1
|x in t ), which is parameterized by θ, to reverse the forward process q(x t |x
t-1 ). the overall training objective l is defined as the variational lower
bound of the negative log-likelihood, given by:this formulation is extended in
ddpm [5] to be further written as:where artifact restoration in inference stage.
during the inference stage, we first use a threshold method to detect the
artifact region in the input image x 0 . then, unlike the conventional diffusion
models [5] that aim to generate the entire image, artifusion selectively
performs denoising resampling only in the artifact region to maximally preserve
the original morphology and stain style in the artifact-free region, as shown in
fig. 2. specifically, we represent the artifactfree region and the artifact
region in the input image as x 0 (1-m) and x 0 m, respectively [10], where m is
a boolean mask indicating the artifact region and is the pixel-wise
multiplication operator. to perform the denoising resampling, we write the input
image x in t at each reverse step from t to t -1 as the sum of the diffused
artifact-free region and the denoised artifact region, i.e.,where ).
consequently, the final restored image is obtained asswin-transformer denoising
network. to capture the local-global correlation and enable the denoising
network to effectively restore the artifact regions, we propose a novel
swin-transformer-basedr [9] denoising network for artifusion. as shown in fig.
3, our network follows a u-shape architecture, where the encoder, bottleneck,
and decoder modules all employ swin-transformer as the basic building block.
additionally, we introduce an innovative auxiliary time token to inject the time
information. in an arbitrary time step t during the training process, to obtain
a time token, we first embed the scalar t by learnable linear layers, with
weights that are specific to each swin-transformer block. in contrast to
existing u-net based denoising networks [5], we propose a better interaction
between hidden features and time information by concatenating the time token to
feature tokens before passing them to the attention layers. the resulting tokens
are then processed by the attention layers, and the auxiliary time token is
discarded to retain the original feature dimension to fit the swin-transformer
block design after the attention layers.
dataset. to evaluate the performance of artifact restoration, a training set is
curated from a subset of camelyon17 [8] 1 . it comprises a total number of 2445
artifact-free images and another 2547 images with artifacts, where all
histological images are scaled to the resolution of 256 × 256 pixels at the
magnitude of 20×. the test set uses another public histology image dataset [6]
with 462 artifact-free images 2 , where we obtain the paired artifact images by
the manually-synthesized artifacts [18]. fig. 4. artifact restoration on five
real-world artifact images. we observe that artifusion can successfully overcome
the drawback of stain style mistransfer in cyclegan. we also illustrate the
gradual denoising process in the artifact region by artifusion, at time step t =
0, 50, 100, 150. it highlights the ability of artifusion to progressively remove
artifacts from the histology image, resulting in a final restored image that is
both visually pleasing and scientifically accurate.implementations. we implement
the proposed artifusion and its counterpart in python 3.8.10 and pytorch 1.10.0.
all experiments are carried out in parallel on two nvidia rtx a4000 gpu cards
with 16 gib memory. hyperparameters are as follows: a learning rate of 10 -4
with adam optimizer, the total timesteps is set to 250.compared methods and
evaluation metrics. as a proof-of-concept attempt at a generative-models-based
artifact restoration framework in the histology domain, currently, there are
limited available literature works and opensourced codes for comparison.
consequently, we leverage the prevalent cycle-gan [19] as the baseline for
comparison, because of its excellent performance in the image transfer, and also
its nature that requires no paired data can fit our circumstance. unlike
cyclegan which requires both artifact-free images and artifact images,
artifusion only relies on artifact-free images, leading to a size of the
training set that is half that of cyclegan. for a fair compaison, we train the
cyclegan with two configurations, namely (#1) using the entire dataset, and (#2)
using only half the dataset, where the latter uses the same number of the
training samples as artifusion. regarding the ablation, we compare the proposed
swin-transformer denoising network with the conventional u-net [5] (denoted as
'u-net'), and the time token scheme with the direct summation scheme (denoted as
'add'). we use the following metrics: l 2 distance (l2) with respect to the
artifact region, the mean-squared error (mse) over the whole image, structural
similarity index (ssim) [15], peak signal-tonoise ratio (psnr) [1],
feature-based similarity index (fsim) [17] and signal to reconstruction error
ratio (sre) [7].table 1. quantitative comparison of artifusion with cyclegan on
artifact restoration performance. the ↓ indicates the smaller value, the better
performance; and vice versa. 1, where some exemplary images are illustrated in
fig. 4. our results demonstrate the superiority of artifusion over gan in the
context of artifact restoration, with a large margin observed in all evaluation
metrics. for instance, artifusion can reduce the l2 and mse by more than 50%,
namely from 1 × 10 4 to 0.5 × 10 4 and from 0.55 to 0.25 respectively. it
implying that our method can to the large extent restore the artifact regions
using the global information. in addition, artifusion can improve other metrics,
including ssim, psnr, fsim and sre by 0.0204, 5.72, 0.1028 and 4.02
respectively, indicating that it can preserve the stain style during the
restoration process. moreover, our ablation study shows that the
swin-transformer denoising network can outperform the conventional u-net,
highlighting the significance of capturing global correlation for local artifact
restoration. finally, the concatenating time token with feature tokens can bring
an improvement in terms of all evaluation matrices, making it a better fit for
the transformer architecture than the direct summation scheme in u-net [5]. in
summary, our ablations confirm the effectiveness of all the components in our
method. evaluations by downstream classification task. we further evaluate the
proposed artifact restoration framework on a downstream tissue classification
task. to this end, we use the public dataset nct-crc-he-100k for training and
crc-val-he-7k for testing, which together contains 100, 000 training samples and
7, 180 test samples. we consider the performance on the original unprocessed
data, denoted as 'clean', as the upper bound. then, we manually synthesize the
artifact (denoted as 'artifact') and evaluate the classification performance
with restoration approaches cyclegan and artifusion. in table 3, comparisons
show that the presence of artifacts can result in a significant performance
decline of over 5% across all five network architectures. importantly, the
classification accuracy on images restored with artifusion is consistently
higher than those restored with cyclegan, demonstrating the superiority of our
model. these results highlight the effectiveness of artifusion as a practical
pre-processing method for histology analysis.
in this paper, we propose artifusion, the first attempt at a diffusion-based
artifact restoration framework for histology images. with a novel
swin-transformer denoising backbone, artifusion is able to restore regional
artifacts using the context information, while preserving the tissue structures
in artifact-free regions as well as the stain style. experimental results on a
public histological dataset demonstrate the superiority of our proposed method
over the state-of-the-art gan counterpart. consequently, we believe that our
proposed method has the potential to benefit the medical community by enabling
more accurate diagnosis or treatment planning as a pre-processing method for
histology analysis. future work includes investigating the extension of
artifusion to more advanced diffusion models such as score-based or score-sde
models [16].
sample t o (1-m) is artifact-free region diffused for t times using the gaussian
transition kernel i.e. x sample t ∼ n ( √ ᾱt x 0 , (1-ᾱt i)) with ᾱt = t i=1
(1-β i ); and x
gout is the most common inflammatory arthritis and musculoskeletal ultrasound
(mskus) scanning is recommended to diagnose gout due to the non-ionizing
radiation, fast imaging speed, and non-invasive characteristics of mskus [7].
however, misdiagnosis of gout can occur frequently when a patient's clinical
characteristics are atypical. traditional mskus diagnosis relies on the
experience of the radiologist which is time-consuming and labor-intensive.
although convolutional neural networks (cnns) based ultrasound classification
models have been successfully used for diseases such as thyroid nodules and
breast cancer, conspicuously absent from these successful applications is the
use of cnns for gout diagnosis from mskus images. there are significant
challenges in cnn based gout diagnosis. firstly, the gout-characteristics
contain various types including double contour sign, synovial hypertrophy,
synovial effusion, synovial dislocation and bone erosion, and these
gout-characteristics are small and difficult to localize in mskus. secondly, the
surrounding fascial tissues such as the muscle, sarcolemma and articular capsule
have similar visual traits with gout-characteristics, and we found the existing
cnn models can't accurately pay attention to the gout-characteristics that
radiologist doctors pay attention to during the diagnosis process (as shown in
fig. 1). due to these issues, sota cnn models often fail to learn the gouty
mskus features which are key factors for sonographers' decision.in medical image
analysis, recent works have attempted to inject the recorded gaze information of
clinicians into deep cnn models for helping the models to predict correctly
based on lesion area. mall et al. [9,10] modeled the visual search behavior of
radiologists for breast cancer using cnn and injected human visual attention
into cnn to detect missing cancer in mammography. wang et al. [15] demonstrated
that the eye movement of radiologists can be a new supervision form to train the
cnn model. cai et al. [3,4] developed the sononet [1] model, which integrates
eye-gaze data of sonographers and used generative adversarial networks to
address the lack of eye-gaze data. patra et al. [11] proposed the use of a
teacher-student knowledge transfer framework for us image analysis, which
combines doctor's eye-gaze data with us images as input to a large teacher
model, whose outputs and intermediate feature maps are used to condition a
student model. although these methods have led to promising results, they can be
difficult to implement due to the need to collect doctors' eye movement data for
each image, along with certain restrictions on the network structure.different
from the existing studies, we propose a novel framework to adjust the general
cnns to "think like sonographers" from three different levels. (1) where to
adjust: modeling sonographers' gaze map to emphasize the region that needs
adjust; (2) what to adjust: classify the instances to systemically detect
predictions made based on unreasonable/biased reasoning and adjust; (3) how to
adjust: developing a training mechanism to strike the balance between gout
prediction accuracy and attention reasonability.
fig. 2. the overall framework of the proposed method.figure 2 presents the
overall framework, which controls cnns to "think like sonographers" for gout
diagnosis from three levels. 1) where to adjust: we model the sonographers' gaze
map to emphasize the region that needs control. this part learns the eye gaze
information of the sonographers which is collected by the eye-tracker. 2) what
to adjust: we divide instances into four categories to reflect whether the model
prediction given to the instance is reasonable and precise. 3) how to adjust: a
training mechanism is developed to strike the balance between gout diagnosis and
attention accuracy for improving cnn.
it is essential to obtain the gaze map corresponding to each mskus to emphasize
the region where gouty features are obvious. inspired by studies of saliency
model [8], we integrate transformer into cnns to capture multi-scale and
longrange contextual visual information for modeling sonographers' gaze map.
this gaze map learns the eye gaze information, collected by the eye-tracker, of
the sonographers when they perform diagnosis. as shown in fig. 2, this part
consists of a cnn encoder for extracting multi-scale feature, a
transformer-encoder for capturing long-range dependency, and a cnn decoder for
predicting gaze map.the mskus image i 0 ∈ r h×w ×3 is first input into cnn
encoder that contains five convolution blocks. the output feature maps from the
deeper last three convolution blocks are denoted as f 0 , f 1 , f 2 and are
respectively fed into transformer encoders to enhance the long-range and
contextual information. during the transformer-encoder, we first flatten the
feature maps produced by the cnn encoder into a 1d sequence. considering that
flatten operation leads to losing the spatial information, the absolute position
encoding [14] is combined with the flatten feature map via element-wise addition
to form the input of the transformer layer. the transformer layer contains the
standard multi-head self-attention (msa) and multi-layer perceptron (mlp)
blocks. layer normalization (ln) and residual connection are applied before and
after each block respectively.in the cnn decoder part, a pure cnn architecture
progressively up-samples the feature maps into the original image resolution and
implements pixel-wise prediction for modeling sonographers' gaze map. the cnn
decoder part includes five convolution blocks. in each block, 3 × 3 convolution
operation, batch normalization (bn), relu activation function, and 2-scale
upsampling that adopts nearest-neighbor interpolation is performed. in addition,
the transformer's output is fused with the feature map from the decoding process
by an element-wise product operation to further enhance the long-range and
multi-scale visual information. after five cnn blocks, a 3 × 3 convolution
operation and sigmoid activation is performed to output the predicted
sonographers' gaze map. we use the eye gaze information of the sonographers
which is collected by the eye-tracker to restrain the predicted sonographers'
gaze map. the loss function is the sum of the normalized scanpath saliency
(nss), the linear correlation coefficient (cc), kullback-leibler divergence
(kld) and similarity (sim) [2].
common cnn classification models for gout diagnosis often fail to learn the
gouty mskus features including the double contour sign, tophus, and snowstorm
which are key factors for sonographers' decision. a cam for a particular
category indicates the discriminative regions used by the cnn to identify that
category. inspired by cam technique, it is needed to decide whether the
attention region given to an cnn model is reasonable for diagnosis of gout. we
firstly use the grad-cam technique [12] to acquire the salient attention region
s cam that cnn model perceives for differential diagnosis of gout. to ensure the
scale of the attention region s cam is the same as the sonographers' gaze map s
sono which is modeled by saliency model, we normalize s cam to the values
between 0 and 1, get s cam . then we make bit-wise intersection over union(iou)
operations with the s sono and s cam to measure how well the two maps overlap.
note that we only calculate the part of s cam that is greater than 0.5. for
instances whose iou is less than 50%, we consider that the model's prediction
for that instance is unreasonable. as shown in fig. 3, when cnn do prediction,
we can divide the instances into four categories: rp: reasonable precise: the
attention region focusses on the gouty features which are important for
sonographers' decision, and the diagnosis is precise. rip: reasonable imprecise:
although attention region focusses on the gouty features, while the diagnosis
result is imprecise. up: unreasonable precise: although the gout diagnosis is
precise, amount of attention is given to irrelevant feature of mskus image. uip:
unreasonable imprecise: the attention region focusses on irrelevant features,
and the diagnosis is imprecise.our target of adjustment is to reduce imprecise
and unreasonable predictions. in this way, cnns not only finish correct gout
diagnosis, but also acquire the attention region that agreements with the
sonographers' gaze map.
we proposed a training mechanism (algorithm 1) which can strike the balance
between the gout diagnosis error and the reasonability error of attention region
to promote the cnns to "think like sonographers". in addition to reducing the
diagnosis error, we also want to minimize the difference between sonographers'
gaze map s sono and normalized salient attention region s cam , which directly
leads to our target: the total loss function can be expressed as the weighted
sum the gout diagnosis error and the reasonability error, as follows:the gout
diagnosis error l diagnosis is calculated by the cross-entropy loss, and the
reasonability is calculated by the l1-loss. this training mechanism uses the
quadrant of instances to identify whether samples' attention needs to adjusted.
for mskus sample in the quadrant of up, α can be set 0.2 to control the cnn pay
more attention to reasonability. correspondingly, for sample in rip, α can be
set 0.8 to make cnn pay more attention to precise. for sample in rp and uip, α
can be set 0.5 to strike the balance between accuracy and reasonability. gaze
data collection. we collected the eye movement data with the tobii 4c
eye-tracker operating at 90 hz. the mskus images were displayed on a 1920 × 1080
27-inch lcd screen. the eye tracker was attached beneath the screen with a
magnetic mounting bracket. sonographers were seated in front of the screen and
free to adjust the chair's height and the display's inclination. binary maps of
the same size as the corresponding mskus images were generated using the gaze
data, with the pixel corresponding to the point of gaze marked with a'1' and the
other pixels marked with a'0'. a sonographer gaze map s was generated for each
binary map by convolving it with a truncated gaussian kernel g(σ x,y ), where g
has 299 pixels along x dimension, and 119 pixels along y dimension.
evaluation metrics. five metrics were used to evaluate model performance:
accuracy (acc), area under curve (auc), correlation coefficient (cc), similarity
(sim) and kullback-leibler divergence (kld) [2]. acc and auc were implemented to
assess the gout classification performance of each model, while cc, sim, and kld
were used to evaluate the similarity of the areas that the model and
sonographers focus on during diagnoses.evaluation of "thinking like
sonographers" mechanism. to evaluate the effectiveness of our proposed mechanism
of "thinking like sonographers" (tls) that combines "where to adjust", "what to
adjust" and "how to adjust", we compared the gout diagnosis results of several
classic cnn classification [5,6,13] models without/with our tls mechanism. the
results, shown in table 1, revealed that using our tls mechanism led to a
significant improvement in all metrics. specifically, for acc and auc, the model
with our tls mechanism achieved better results than the model without it.
resnet34 with tls acquired the highest improvement in acc with a 4.41% increase,
and resnet18 with tls had a 0.027 boost in auc. our tls mechanism consistently
performed well in improving the gout classification performance of the cnn
models. more comparison results were shown in appendix fig. a1 and fig. a2. the
cc, sim, and kld metrics were utilized to assess the similarity between the cams
of classification models and the collected gaze maps, providing an indication of
whether the model was able to "think" like a sonographer. table 1 showed that
the models with our tls mechanism achieved significantly better results in terms
of cc and sim (i.e., higher is better), as well as a decline of more than 1.50
in kld (lower is better), when compared to the original models. this indicated
that the models with tls focused on the areas shown to be similar to the actual
sonographers. furthermore, fig. 4 illustrated the qualitative results of cams of
models with and without tls mechanism. the original models without tls paid more
attention to noise, textures, and artifacts, resulting in unreasonable gout
diagnosis. with tls, however, models could focus on the crucial areas in
lesions, allowing them to think like sonographers.
to evaluate the prediction's stability under the predicted gaze map from the
generation model in "where to adjust", we conducted three t-test studies.
specifically, we trained two classification models (m c and m p ), using the
actual collected gaze maps, and the predicted maps from the generation model,
respectively. during the testing, we used the collected maps as input for m c
and m p to get classification results r cc and r p c . similarly, we used the
predicted maps as input for m c andm p as shown in table 2, the p-values of
t-test (1)( 2) and ( 3) are all greater than 0.005, suggesting that no
significant difference was observed between the classification results obtained
from different generative strategies. this implied that our training mechanism
was model-insensitive. consequently, it was possible to use predicted gaze maps
for both the training and testing phases of the classification models without
any notable performance decrease. this removed the need to collect eye movement
maps during the training and testing phases, significantly lightening the
workload of data collection. therefore, our tls mechanism, which involved
predicting the gaze maps, could potentially be used in clinical environments.
this would allow us to bypass the need to collect the real gaze maps of the
doctors while classifying newly acquired us images, and thus improved the
clinical implications of our mechanism, "thinking like sonographers".
in this study, we propose a framework to adjust cnns to "think like
sonographers", and diagnose gout from mskus images. the mechanism of "thinking
like sonographers" contains three levels: where to adjust, what to adjust, and
how to adjust. the proposed design not only steers cnn models as we intended,
but also helps the cnn classifier focus on the crucial gout features. extensive
experiments show that our framework, combined with the mechanism of "thinking
like sonographers" improves performance over the baseline deep classification
architectures. additionally, we can bypass the need to collect the real gaze
maps of the doctors during the classification of newly acquired mskus images,
thus our method has good clinical application values.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43987-2 16.
abdominal organ segmentation from medical images is an essential work in
clinical diagnosis and treatment planning of abdominal lesions [17]. recently,
deep learning methods based on convolution neural network (cnn) have achieved
impressive performance in medical image segmentation tasks [2,24]. however,
their success relies heavily on large-scale high-quality pixel-level annotations
that are too expensive and time-consuming to obtain, especially for multiple
organs in 3d volumes. weakly supervised learning with a potential to reduce
annotation costs has attracted great attention. commonly-used weak annotations
include dots [6,11], scribbles [1,11,13,15], bounding boxes [5], and imagelevel
tags [20,25]. compared with the other weak annotations, scribbles can provide
more location information about the segmentation targets, especially for objects
with irregular shapes [1]. therefore, this work focuses on exploring
high-performance models for multiple abdominal organ segmentation based on
scribble annotations.training cnns for segmentation with scribble annotations
has been increasingly studied recently. existing methods are mainly based on
pseudo label learning [11,15], regularized losses [10,18,22] and consistency
learning [7,13,26]. pseudo label learning methods deal with unannotated pixels
by generating fake semantic labels for learning. for example, luo et al. [15]
introduced a network with two slightly different decoders that generate
dynamically mixed pseudo labels for supervision. liang et al. [11] proposed to
leverage minimum spanning trees to generate low-level and high-level affinity
matrices based on color information and semantic features to refine the pseudo
labels. arguing that the pseudo label learning may be unreliable, tang et al.
[22] introduced the conditional random field (crf) regularization loss for image
segmentation directly. obukhov et al. [18] proposed to incorporate the gating
function with crf loss considering the directionality of unsupervised
information propagation. recently, consistency strategies that encourage
consistent outputs of the network for the same input under different
perturbations have achieved increasing attentions. liu et al. [13] introduced
transformation-consistency based on an uncertaintyaware mean teacher [4] model.
zhang et al. [26] proposed a framework composed of mix augmentation and cycle
consistency. although these scribble-supervised methods have achieved promising
results, their performance is still much lower than that of fully-supervised
training, leaving room for improvement.differently from most existing weakly
supervised methods that are designed for 2d slice segmentation with a single or
few organs, we propose a highly optimized 3d triple-branch network with one
encoder and three different decoders, named tdnet, to learn from scribble
annotations for segmentation of multiple abdominal organs. particularly, the
decoders are assigned with different dilation rates [25] to learn features from
different receptive fields that are complementary to each other for
segmentation, which also improves the robustness of dealing with organs at
different scales as well as the feature learning ability of the shared encoder.
considering the features at different scales learned in these decoders, we fuse
these multi-dilated predictions to obtain more accurate soft pseudo labels
rather than hard labels [15] that tend to be over-confidence predictions. for
more stable unsupervised learning, we use voxel-wise uncertainty to rectify the
soft pseudo labels and then impose consistency constraints on the output of each
branch. in addition, we extend the consistency to the class-related information
level [23] to constrain inter-class affinity for better distinguishing them.
specifically, we generate the class affinity matrices in different decoders and
encourage them to be consistent after projection in different views. the
contributions of this paper are summarized as follows: 1) we propose a novel 3d
triple-branch multi-dilated network called tdnet for scribblesupervised
segmentation. by equipping with varying dilation rates, the network can better
leverage multi-scale context for dealing with organs at different scales.2) we
propose two novel consistency loss functions, i.e., uncertainty-weighted soft
pseudo label consistency (uspc) loss and multi-view projection-based
class-similarity consistency (mpcc) loss, to regularize the prediction from the
pixel-wise and class-wise perspectives respectively, which helps the
segmentation network obtain reliable predictions on unannotated pixels. 3)
experiments results show our proposed method outperforms five existing
scribble-supervised methods on the public dataset word [17] for multiple
abdominal organ segmentation.
figure 1 shows the proposed framework for scribble-supervised medical image
segmentation. we introduce a network with one encoder and three decoders with
different dilation rates to learn multi-scale features. the decoders' outputs
are averaged to generate a soft pseudo label that is rectified by uncertainty
and then used to supervise each branch. to better deal with multi-class
segmentation, a class similarity consistency loss is also used for
regularization.for the convenience of following description, we first define
several mathematical symbols. let x, s be a training image and the corresponding
scribble annotation, respectively. let c denote the number of classes for
segmentation, and ω = ω s ∪ ω u denote the whole set of voxels in x, where ω s
is the set of labeled pixels annotated in s, and ω u is the unlabeled pixel set.
as shown in fig. 1(a), the proposed tdnet consists of a shared encoder (θ e )
and three independent decoders (θ d1 , θ d2 , θ d3 ) with different dilation
rates to mine unsupervised context from different receptive fields.
specifically, decoders using convolution with small dilation rates can extract
detailed local features but their receptive fields are small for understanding a
global context. decoders using convolution with large dilation rates can better
leverage the global information but may lose some details for accurate
segmentation. in this work, our tdnet is implemented by introducing two
auxiliary decoders into a 3d unet [3]. the dilation rate in the primary decoder
and the two auxiliary decoders are 1, 3 and 6 respectively, with the other
structure parameters (e.g., kernel size, channel number etc.) being the same in
the three decoders. to further introduce perturbations for obtaining diverse
outputs, the three branches are initialized with kaiming initialization, xavier
and normal initialization methods, respectively. in addition, the bottleneck's
output features are randomly dropped out before sending into the auxiliary
decoders. the probability prediction maps obtained by the three decoders are
denoted as p 1 , p 2 and p 3 , respectively.
uncertainty-weighted soft pseudo label consistency (uspc). as the three decoders
capture features at different scales that are complementary to each other, an
ensemble of them would be more robust than a single branch. therefore, we take
an average of p 1 , p 2 , p 3 to get a better soft pseudo label p = (p 1 + p 2 +
p 3 )/3 that is used to supervise each branch during training. however, p may
also contain noises and be inaccurate, and it is important to highlight reliable
pseudo labels while suppressing unreliable ones. thus, we propose a
regularization term named uncertainty-weighted soft pseudo label consistency
(uspc) between p n (n = 1, 2, 3) and p :where pi refers to the prediction
probability at voxel i in p , and pn,i is the corresponding prediction
probability at voxel i in pn . kl() is the kullback-leibler divergence. w i is
the voxel-wise weight based on uncertainty estimation:where the uncertainty is
estimated by entropy. c is the class index, and p c i means the probability for
class c at voxel i in the pseudo label. note that a higher uncertainty leads to
a lower weight. with the uncertainty-based weighting, the model will be less
affected by unreliable pseudo labels.
for multi-class segmentation tasks, it is important to learn inter-class
relationship for better distinguishing them. in addition to using l usp c for
pixel-wise supervision, we consider making consistency on class relationship
across the outputs of the decoders as illustrated in fig. 1. in order to save
computing resources, we project the soft pseudo labels along each dimension and
then calculate the affinity matrices, which also strengthens the class
relationship information learning. we first project the soft prediction map of
the n-th decoder p n ∈ r c×d×h×w in axial view to a tensor with the shape of c ×
1 × h × w . it is reshaped into c ×(w h) and multiplied by its transposed
version, leading to a class affinity matrixsimilarly, p n is projected in the
sagittal and coronal views, respectively, and the corresponding normalized class
affinity matrices are denoted as q sagittal n and q coronal n , respectively.
here, the affinity matrices represents the relationship between any pair of
classes along the dimensions. then we constraint the consistency among the
corresponding affinity matrices by multi-view projection-based class-similarity
consistency (mpcc) loss:where v ∈ {axial, sagittal, coronal} is the view index,
and qv is the average class affinity matrix in a certain view obtained by the
three decoders.
to learn from the scribbles, the partially cross-entropy (pce) loss is used to
train the network, where the labeled pixels are considered to calculate the
gradient and the other pixels are ignored [21]:where s represents the one-hot
scribble annotation, and ω s is the set of labeled pixels in s. the total object
function is summarized as:where α t and β t are the weights for the unsupervised
losses. following [13], we define α t based on a ramp-up function: α t = α • e
(-5(1-t/tmax) 2 ) , where t denotes the current training step and t max is the
maximum training step. we define β t = β • e (-5(1-t/tmax) 2 ) in a similar way.
in this way, the model can learn accurate information from scribble annotations,
which also avoids getting stuck in a degenerate solution due to low-quality
pseudo labels at an early stage.
we used the publicly available abdomen ct dataset word [17] for experiments,
which consists of 150 abdominal ct volumes from patients with rectal cancer,
prostate cancer or cervical cancer before radiotherapy. each ct volume contains
159-330 slices of 512×512 pixels, with an in-plane resolution of 0.976 × 0.976
mm and slice spacing of 2.5-3.0 mm. we aimed to segment seven organs: the liver,
spleen, left kidney, right kidney, stomach, gallbladder and pancreas. following
the default settings in [17], the dataset was split into 100 for training, 20
for validation and 30 for testing, respectively, where the scribble annotations
for foreground organs and background in the axial view of the training volumes
had been provided and were used in model training. for pre-processing, we cut
off the hounsfield unit (hu) values with a fixed window/level of 400/50 to focus
on the abdominal organs, and normalized it to [0, 1]. we used the
commonlyadopted dice similarity coefficient (dsc), 95% hausdorff distance (hd 95
) and the average surface distance (asd) for quantitative evaluation.our
framework was implemented in pytorch [19] on an nvidia 2080ti with 11 gb memory.
we employed the 3d unet [3] as the backbone network for all experiments, and
extended it with three decoders by embedding two auxiliary decoders with
different dilation rates, as detailed in sect. 2.1. to introduce perturbations,
different initializations were applied to each decoder, and random perturbations
(ratio = (0, 0.5)) were introduced in the bottleneck before the auxiliary
decoders. the stochastic gradient descent (sgd) optimizer with momentum of 0.9
and weight decay of 10 -4 was used to minimize the overall loss function
formulated in eq. 5, where α=10.0 and β=1.0 based on the best performance on the
validation set. the poly learning rate strategy [16] was used to decay learning
rate online. the batch size, patch size and maximum iterations t max were set to
1, [80, 96, 96] and 6 × 10 4 respectively. the final segmentation results were
obtained by using a sliding window strategy. for a fair comparison, we used the
primary decoder's outputs as the final results during the inference stage and
did not use any post-processing methods. note that all experiments were
conducted in the same experimental setting. the existing methods are implemented
with the help of open source codebase from [14].table 1. quantitative comparison
between our method and existing weakly supervised methods on word testing set. *
denotes p-value < 0.05 (paired t-test) when comparing with the second place
method [15]. the best values are highlighted in bold.
fullysup [3] pce tv [9] ustm [13]
we compared our method with five weakly supervised segmentation methods with the
same set of scribbles, including pce only [12], total variation loss (tv) [9],
uncertainty-aware self-ensembling and transformation-consistent model (ustm)
[13], entropy minimization (em) [8] and dynamically mixed pseudo labels
supervision (dmpls) [15]. they were also compared with the upper bound by using
dense annotation to train models (fullysup) [3]. the results in table 1 show
that our method leads to the best dsc, asd and hd 95 .compared with the second
best method dmpls [15], the average dsc was increased by 2.67 percent points,
and the average asd and hd 95 were decreased by 5.44 mm and 16.16 mm,
respectively. it can be observed that tv [9] obtained a worse performance than
pce, which is mainly because that method classifies pixels by minimizing the
intra-class intensity variance, making it difficult to achieve good segmentation
due to the low contrast. figure 2 shows a visual comparison between our method
and the other weakly supervised methods on the word dataset (word 0014.nii). it
can be obviously seen that the results obtained by our method are closer to the
ground truth, with less mis-segmentation in both slice level and volume level.
we then performed ablation experiments to investigate the contribution of each
part of our method, and the quantitative results on the validation set are shown
in table 2, where l usp c (-ω) means using l usp c without pixelwise uncertainty
rectifying. baseline refers to a triple-branch model with different
initializations and random feature-level dropout in the bottleneck, supervised
by pce only. it can be observed that by using l usp c (-ω) with mutiple
decoders, the model segmentation performance is greatly enhanced with average
dsc increasing by 7.70%, asd and hd 95 decreasing by 16.11 mm and 48.87 mm,
respectively. by equipping each decoders with different dilation rates, the
model's performance is further improved, especially in terms of asd and hd 95 ,
which proves our hypothesis that learning features from different scales can
improve the segmentation accuracy. replacing l usp c (-ω) with l usp c further
improved the dsc to 84.21%, and reduced the asd and hd 95 by 0.52 mm and 1.01 mm
through utilizing the uncertainty information. visual comparison in fig. 3
demonstrates that over-segmentation can be mitigated by using different dilation
rates in the three decoders, and using the uncertainty-weighted pseudo labels
can further improve the segmentation accuracy with small false positive regions
removing.additionally, table 2 shows that combining l usp c and l mp cc obtained
the best performance, where the average dsc, asd and hd 95 were 84.75%, 2.64 mm
and 7.91 mm, respectively, which demonstrates the effectiveness of the proposed
class similarity consistency. in order to find the optimal number of decoders,
we set the decoder number to 2, 3 and 4 respectively. the quantitative results
in the last three rows of table 2 show that using three decoders outperformed
using two and four decoders.
in this paper, we proposed a scribble-supervised multiple abdominal organ
segmentation method consisting of a 3d triple-branch multi-dilated network with
two-level consistency constraints. by equipping each decoder with different
dilation rates, the model leverages features at different scales to obtain
high-quality soft pseudo labels. in addition to mine knowledge from unannotated
pixels, we also proposed uspc loss and mpcc loss to learn unsupervised
information from the uncertainty-rectified soft pseudo labels and class affinity
matrix information respectively. experiments on a public abdominal ct dataset
word demonstrated the effectiveness of the proposed method, which outperforms
five existing scribble-based methods and narrows the performance gap between
weakly-supervised and fully-supervised segmentation methods. in the future, we
will explore the effect of our method on sparser labels, such as a volumetric
data with scribble annotations on one or few slices.
breast cancer is the most prevalent form of cancer among women and can have
serious physical and mental health consequences if left unchecked [5]. early
detection through mammography is critical for early treatment and prevention
[19]. mammograms provide images of breast tissue, which are taken from two
views: the cranio-caudal (cc) view, and the medio-lateral oblique (mlo) view
[4]. by identifying breast cancer early, patients can receive targeted treatment
before the disease progresses.deep neural networks have been widely adopted for
breast cancer diagnosis to alleviate the workload of radiologists. however,
these models often require a large number of manual annotations and lack
interpretability, which can prevent their broader applications in breast cancer
diagnosis. radiologists typically focus on areas with breast lesions during
mammogram reading [11,22], which provides valuable guidance. we propose using
real-time eye tracking information from radiologists to optimize our model. by
using gaze data to guide model training, we can improve model interpretability
and performance [24].radiologists' eye movements can be automatically and
unobtrusively recorded during the process of reading mammograms, providing a
valuable source of data without the need for manual labeling. previous studies
have incorporated radiologists' eye-gaze as a form of weak supervision, which
directs the network's attention to the regions with possible lesions [15,23].
leveraging gaze from radiologists to aid in model training not only increases
efficiency and minimizes the risk of errors linked to manual annotation, but
also can be seamlessly implemented without affecting radiologists' normal
clinical interpretation of mammograms.mammography primarily detects two types of
breast lesions: masses and microcalcifications [16]. the determination of the
benign or malignant nature of masses is largely dependent on the smoothness of
their edges [13]. the gaze data can guide the model's attention towards the
malignant masses. microcalcifications are small calcium deposits which exhibit
irregular boundaries on mammograms [9]. this feature makes them challenging to
identify, often leading to missed or false detection by models. radiologists
need to magnify mammograms to differentiate between benign scattered
calcifications and clustered calcifications, the latter of which are more likely
to be malignant and necessitate further diagnosis. leveraging gaze data can
guide the model to locate malignant calcifications.in this work, we propose a
novel diagnostic model, namely mammo-net, which integrates radiologists' gaze
data and interactive information between cc-view and mlo-view to enhance
diagnostic performance. to the best of our knowledge, this is the first work to
integrate gaze data into multi-view mammography classification. we utilize class
activation map (cam) [18] to calculate the attention maps for the model.
additionally, we apply pyramid loss to maintain consistency between
radiologists' gaze heat maps and the model's attention maps at multiple scales
of the pyramid [1]. our model is designed for singlebreast cases. mammo-net
extracts multi-view features and utilizes transformerbased attention to
mutualize information [21]. furthermore, there are differences between
multi-view mammograms of the same patient, arising from variations in breast
shape and density. capturing these multi-view shared features can be a challenge
for models. to address this issue, we develop a novel method called
bidirectional fusion learning (bfl) to extract shared features from multi-view
mammograms.our contributions can be summarized as follows:• we emphasize the
significance of low-cost gaze to provide weakly-supervised positioning and
visual interpretability for the model. additionally, we develop a pyramid loss
that adapts to the supervised process. • we propose a novel breast cancer
diagnosis model, namely mammo-net. this model employs transformer-based
attention to mutualize information and uses bfl to integrate task-related
information to make accurate predictions. • we demonstrate the effectiveness of
our approach through experiments using mammography datasets, which show the
superiority of mammo-net.2 proposed method
the pipeline of mammo-net is illustrated in fig. 1. mammo-net feeds two-view
mammograms of the same breast into two resnet-style [7] cnn branch networks. we
use several resnet blocks pre-trained on imagenet [3] to process mammograms.
then, we use global average pooling (gap) and fully connected layers to compute
the feature vectors produced by the model. before the final residual block, we
employ cross-view attention to mutualize multi-view information. our proposed
method employs bfl to effectively fuse multi-view information to improve
diagnostic accuracy. additionally, by integrating gaze data from radiologists,
our proposed model is able to generate more precise attention maps.the fusion
network combines multi-view feature representations using a stack of
linear-activation layers and a fully connected layer, resulting in a
classification output.
in this module, we utilize cam to calculate the attention map for the network by
examining gradient-based activations in back-propagation. after that, we employ
pyramid loss to make the network attention being consistent with the supervision
of radiologists' gaze heat maps, guiding the network to focus on the same lesion
areas as the radiologists. this module guides the network to accurately extract
pathological features.
at the final convolutional layer of our model, the activation of the ith feature
map f i (x, y) at coordinates (x, y) is associated with a weight w k i for class
k. this allows us to generate the attention map h k for class k as:pyramid loss.
to enhance the learning of important attention areas, we propose a pyramid loss
constraint that requires consistency between the network and gaze attention
maps. the pyramid loss is based on using a pyramid representation of the
attention map:where h is the network attention map generated by the cam and r is
the radiologist's gaze heat map. square error (mse) between the attention maps
generated by the radiologist and the model at each level of the gaussian
pyramid. this allows the model to mimic the attention of radiologists and
enhance diagnostic performance. moreover, the pyramid representation enables the
model to learn from the important pathological regions on which radiologists are
focusing, without the need for precise pixel-level information. layernorm is
also employed to address the issue of imprecise gaze data. this reduces noise in
the consistency process by performing consistency loss only in the regions where
radiologist spent most time.
transformer-based mutualization model. we use transformer-based attention to
mutualize information from the two views at the level of the spatial feature
map. for each attention head, we compute embeddings for the source and target
pixels. our model does not utilize positional encoding, as it encodes the
relative position of each pixel and is not suitable for capturing information
between different views of mammograms [21]. the target view feature maps are
transformed into q, the source view feature maps are transformed into k, and the
original source feature maps are transformed into v . we can then obtain a
weighted sum of the features from the source view for each target pixel using
[21]:subsequently, the output is transformed into attention-based feature maps x
and mutualized with the feature maps y from the other view. the mutualized
feature maps are normalized and used for subsequent calculations:bidirectional
fusion learning. to enable the fusion network to retain more of the shared
features between the two views and filter out noise, we propose to use bfl to
learn a fusion representation that maximizes the cross-view mutual information.
the optimization target is to generate a fusion representation i from multi-view
representations p v , where v ∈ {cc, mlo}. we employ the noise-contrastive
estimation framework [6] to maximize the mutual information, which is a
contrastive learning framework:where s(i, p v ) evaluates the correlation
between multi-view fused representations and single-view representations
[17]:where n (i) is a reconstruction of p v generated by a fully connected
network n from i and the euclidean norm || • || 2 is applied to obtain
unit-length vectors.in contrastive learning, we consider the same patient
mammograms as positive samples and those from different patient mammograms in
the same batch p i v = p v \{p i v } as negative samples [17]. minimizing the
similarity between the same patient mammograms enables the model to learn shared
features. maximizing the dissimilarity between different patient mammograms
enhances the model's robustness.in short, we require the fusion representation i
to reversely reconstruct multiview representations p v so that more
view-invariant information can be passed to i. by aligning the prediction n (i)
to p v , we enable the model to decide how much information it should receive
from each view.the overall loss function for this module is the sum of the
losses defined for each view:
we use binary cross entropy loss (bce) between the network prediction and the
ground-truth as the classification loss. in conclusion, we have proposed a total
of three loss functions to guide the model training: l bce , l bf l , and l p
yramid . the overall loss function is defined as the sum of these three loss
functions, with coefficients λ and μ used to adjust their relative weights:3
experiments and results
mammogram dataset. our experiments were conducted on cbis-ddsm [12] and inbreast
[16]. the cbis-ddsm dataset contains 1249 exams that have been divided based on
the presence or absence of masses, which we used to perform mass classification.
the inbreast dataset contains 115 exams with both masses and
micro-calcifications, on which we performed benign and malignant classification.
we split the inbreast dataset into training and testing sets in a 7:3 ratio. it
is worth noting that the official inbreast dataset does not provide image-level
labels, so we obtained these labels following shen et al. [20].eye gaze dataset.
eye movement data was collected by reviewing all cases in inbreast using a tobii
pro nano eye tracker. the scenario is shown in appendix and can be accessed at
https://github.com/jamesqfreeman/miceye. participated radiologist has 11 years
of experience in mammography screening.
we trained our model using the adam optimizer [10] with a learning rate of 10 -4
(partly implemented by mindspore). to overcome the problem of limited data, we
employed various data augmentation techniques, including translation, rotation,
and flipping. to address the problem of imbalanced classes, we utilized a
weighted loss function that assigns higher weights to malign cases in order to
balance the number of benign and malign cases. the coefficients λ and μ of l
overall were set to 0.5 and 0.2, respectively, based on 5-fold cross validation
on the training set. the network was trained for 300 epochs. we used accuracy
(acc) and the area under the roc curve (auc) [25] as our evaluation metrics, and
we selected the final model based on the best validation auc. considering the
relatively small size of our dataset, we used resnet-18 as the backbone of our
network. 1, we compare our model to other methods and find that our model
performs better. lopez et al. [14] proposed the use of hypercomplex networks to
mimic radiologists. by leveraging the properties of hypercomplex algebra, the
model is able to continually process two mammograms together. lee et al. [26]
proposed a 2-channel approach that utilizes a gaussian model to capture the
spatial correlation between lesions across two views, and an lt-gan to achieve a
robust mammography classification. we also compare our model with other methods
that use eye movement supervision as shown in table 1. the ga-net [23] proposed
a resnet-based model with class activation mapping guided by eye gaze data. we
developed a multi-view model using this approach for a fair comparison, and
found that our method performed better. we believe that one possible reason for
the inferior performance of ga-net compared to mammo-net might be the use of a
simple mse loss by ga-net, which neglects the coarse nature of the gaze data.
jiang et al. [8] proposed a double-model that fuses gaze maps with original
images before training. however, this model did not consider the gap between
research and clinical workflow. this model requires gaze input during both the
training and inference stages, which limits its practical use in hospitals
without eyetrackers. in contrast, our method does not rely on gaze input during
inference stage.
visualization. figure 2 illustrates the visualization of our proposed model on
three representative exams from the inbreast dataset that includes masses,
calcifications, and a combination of both. for each exam, we present gaze heat
maps generated from eye movement data. the preprocessing process is shown in
fig. 5 (see appendix). to make an intuitive comparison, we exhibit attention
maps generated by the model under both unsupervised and gaze-supervised cases.
each exam is composed of two views, i.e., the cc-view and the mlo-view. more
exams can be found in fig. 6 (see appendix).the results of the visualization
demonstrate that the model's capability in localizing lesions becomes more
precise when radiologist attention is incorporated in the training stage. the
pyramid loss improves the model's robustness even when the radiologist's gaze
data is not entirely focused on the breast. this intuitively demonstrates the
effectiveness of training the model with eye-tracking supervision.ablation
study. we perform an ablation analysis to assess each component (radiologist
attention, cross-view attention and bfl) in mammo-net. table 1 suggests that
each part of the proposed framework contributes to the increased performance.
this shows the benefits of adapting the model to mimic the radiologist's
decision-making process.
in this paper, we have developed a breast cancer diagnosis model to mimic the
radiologist's decision-making process. to achieve this, we integrate gaze data
as a form of weak supervision for both lesion positioning and interpretability
of the model. we also utilize transformer-based attention to mutualize
multi-view information and further develop bfl to fully fuse multi-view
information. our experimental results on mammography datasets demonstrate the
superiority of our proposed model. in future work, we intend to explore the use
of scanning path analysis as a means of obtaining insights into the
pathology-relevant regions of lesions.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43990-2 7.
the increased availability of radiological data and rapid advances in medical
image analysis has led to an exponential growth in prediction models that
utilize features extracted from clinical imaging scans to detect and diagnose
diseases and predict response to treatment [1][2][3][4]. however, variations in
the acquisition and reconstruction of ct scans result in quantitative image
features with poor reproducibility [5,6]. several studies have demonstrated that
differences in dose, slice thickness, reconstruction method, and reconstruction
kernel negatively impact radiomic feature reproducibility. predicting prediction
model performance is confounded by how medical images are acquired and
reconstructed [7][8][9][10]. many studies have developed techniques to address
sources of ct parameter variability [5,11]. however, inverse problems such as
recovering full radiation dose scans from lower dose scans are inherently
ill-posed. a range of outputs may be possible when using image restoration
algorithms, including potential artifacts that impact the performance of
downstream algorithms. like gans or variational autoencoders, normalizing flows
is a method for learning complex data representations but with an explicit
ability to infer the output as a probability distribution and with the added
benefit of more stable training [12]. while normalizing flows has shown success
in image synthesis tasks for natural images [13], few studies have examined them
in medical image harmonization tasks. denker et al. employed a normalizing flow
model conditioned on ldct reconstruction by filtered backprojection to improve
reconstruction quality from the raw sinogram data [14]. this paper presents
ctflow, which aims to utilize normalizing flows to harmonize variations in image
appearance of ct scans by maximizing the explicit likelihood of a target
condition (e.g., 100% dose, medium kernel, 1 mm slice thickness) given a ct scan
that was acquired using different parameters (50% dose, sharp kernel, 1 mm slice
thickness). normalizing flow has two important advantages: 1) the translated
low-dose cts have minimal artifacts because the output is a maximum likelihood
estimate that closely matches the target reference distribution, and 2) unlike
gans, which are susceptible to mode collapse, ctflow can generate multiple
solutions to reduce inference uncertainty. we demonstrate how ctflow compares
with current state-of-the-art methods for mitigating dose and reconstruction
kernel differences. we evaluated using image quality metrics and a lung nodule
detection task.
this study used two unique datasets: (1) the ucla low-dose chest ct dataset, a
collection of 186 exams acquired using siemens ct scanners at an equivalent dose
of 2 mgy following an institutional review board-approved protocol. the raw
projection data of scans were exported, and poisson noise was introduced, as
described in zabic et al. [15], at levels equivalent to 10% of the original
dose. projection data were then reconstructed into an image size of 512 × 512
using three reconstruction kernels (smooth, medium, sharp) at 1.0 mm slice
thickness. the dataset was split into 80 scans for training, 20 for validation,
and 86 for testing. (2) aapm-mayo clinic low-dose ct "grand challenge" dataset,
a publicly available grand challenge dataset consisting of 5,936 abdominal ct
images from 10 patient cases reconstructed at 1.0 mm slice thickness. each case
consists of a paired 100% "normal dose" scan and a simulated 25% "low dose"
scan. images from eight patient cases were used for training, and two cases were
reserved for validation. all images were randomly cropped into patches of 128 ×
128 pixels, generated from regions containing the body. this dataset was only
used for evaluating image quality against other harmonization techniques.
in this section, we describe the normalizing flows and modifications that were
made to improve computational efficiency. deterministic approaches to image
translation (e.g., using a convolutional neural network) attempt to find a
mapping function y = g θ (x) that takes an input image x and outputs an image y
that mimics the appearance of a target condition. for example, x could be an
image acquired using a low dose protocol (e.g., 25% dose, smooth kernel), and y
represents the image acquired at the target acquisition and reconstruction
parameter (e.g., 100% dose, medium kernel). flow-based image translation aims to
approximate the density function y|x (y|x, θ) using maximum likelihood
estimation. which can be trained by maximizing the log-likelihood. in practice,
a multilayer flow operation is preferred because a single-layer flow cannot
represent complex non-linear relationships within the data. f is decomposed into
a series of invertible neural network layers h n where h n = f n θ (h n-1 ;
e(x)), n represents the number of layers, and e(x) represents a deep
convolutional neural network that extracts salient feature maps of x upon which
the flow layers are conditioned. for an n-layer flow model, the objective is to
maximizeonce the training is complete, the decoding function g θ (z; x) is
applied using random latent variable z, which is drawn from the independent and
identically distributed gaussian density function. the use of z allows us to
generate a range of possible restored images y , conditioned on the same input
image x. flow layers. flow layers must meet two requirements: 1) be invertible
and 2) be a tractable jacobian determinant. to compute the second term in eq. 2,
we apply the triangulation trick developed by dinh et al. [16] we use affine
coupling layers with a conditional variable. we first equally split the channels
into h n 1 , h n 2 and apply an affine transformation on h n 2 while keeping an
identity transform on h n 1 . we apply scale and shift factor computed by a
shallow convolutional neural network (cnn) given h n 1 in spatial coordinates i,
j to compute the n + 1 layer flow of h n+1 2 . finally, we concatenated the
splitting components back to obtain the next layer flowthus, by definition,
jacobian of h n+1 is a lower triangular matrix. figure 2a depicts the components
of the flow module, which are described below:• activation normalization: a
channel-wise batch normalization [17] was applied, yielding an output with zero
mean and unit variance. • invertible 1 x 1 conv: following the approach in [18],
we utilized a learnable 1x1 convolution h n i,j = wh n-1 i,j where w is a square
matrix with dimension c × c (c is the number of channels). each spatial element
i, j in h is multiplied by this 1x1 convolution matrix. the log determinant is
computed using plu factorization.• feature conditional affine. we compute the
scale and shift factor from e(x) again using a shallow cnn to apply the n-th
layer flow transformation h. the motivation is to impose a relationship between
feature maps extracted e(x) and activation maps h.the deep convolutional neural
network extractor e(x) is based on residual-in-residual dense blocks (rrdb)
[19]. this network contains 14 rrdb blocks and is our feature extractor for
low-dose images. the rrdb network was trained using l 1 loss for 60k iterations.
the batch size was 16 and the learning rate was set to 2e-4. the adam optimizer
was used with β 1 = 0.9, β 2 = 0.99. after training, all layers of rrdb were
frozen and used only for feature extraction. feature maps were derived from 2,
6, 10, 14 block outputs. afterward, the outputs of each block were concatenated
into e(x).multiscale architecture. since the flow approach is invertible, input
x and latent space vector z must have the same dimensions. however, in most
cases, y|x (y|x, θ) is a lowdimensional manifold in high-dimensional input
space. computation is inefficient when a flow model is imposed with a higher
dimensionality than the dimension of true latent space. given the multiscale
architecture in realnvp, we can simplify the model and improve the density
estimation at multiple levels. the overall multiscale architecture is depicted
in fig. 2b, where we equally divide each output z into (z out , z next ), while
recursively feeding z next to the next level. once all levels have been reached,
z out is outputted, representing the maximum log-likelihood estimation.network
training. we trained ctflow using a batch size of 16 and 50k iterations. the
learning rate was set to 1e-4 and halved at 50%, 75%, 90%, and 95% of the total
training steps. a negative log-likelihood loss was used.
we conducted two experiments to evaluate ctflow: image quality metrics and
impact on the performance of a lung nodule computer-aided detection (cade)
algorithm.image quality. using the grand challenge dataset, we assessed image
quality and compared it with other previously published low-dose ct denoising
techniques. we computed image quality metrics using the peak signal-to-noise
ratio (psnr), structural similarity (ssim), and learned perceptual image patch
similarity (lpips) [20]. our comparison was conducted using adversarial-based
approaches (wgan using mean squared error loss, wgan using perceptual loss, and
a 3d spectral-norm gan called sngan [21][22][23], previously developed in our
group), a convolutional neural networkbased approach (srresnet) [24], and a
denoising algorithm based on collaborative filtering block-matching and 3d
filtering (bm3d) [25].nodule detection. we evaluated the ability of ctflow to
harmonize differences in reconstruction kernels and their effect on the
performance of a lung nodule detection algorithm. our cade system was based on
the retinanet model, a composite model comprised of a backbone network called
feature pyramid net and two subnetworks responsible for object classification
with bounding box regression. the model was trained and validated on the
lidc-idri dataset, a public de-identified dataset of diagnostic and low-dose ct
scans with annotations from four experienced thoracic radiologists. as part of
the training process, we only considered nodules annotated by at least three
readers in the lidc dataset. a total of 7,607 slices (with 4,234 nodule
annotations) were used for training and 2,323 slices (with 1,454 nodule
annotations) for testing in a single train-test split. a bounding box was then
created around the union of all the annotator contours to serve as the reference
for the detection model. after training for 200 epochs with focal loss and adam
optimizer, the model achieved an average precision (ap@0.5) of 0.62 on the
validation set.we hypothesized that the ctflow models should yield better
consistency in lung nodule detection performance compared with not normalizing
or other state-of-the-art methods. as a comparison, we trained a 3d sngan model
using the same training and validation set as ctflow to perform the same task.
we trained three separate ctflow and sngan models to map scans reconstructed
using smooth, medium, or sharp kernels to a reference condition. we computed the
f1 score (the harmonic mean of the cade algorithm's precision and recall) when
executing the model on the ctflow and sngan normalized scans. we then determined
the concordance correlation coefficient [26] on the f1 scores, comparing the f1
score of the model when executed on the normalized scan to when executed on the
reference scan.
on the grand challenge dataset, ctflow took 3 days to train on an nvidia rtx
8000 gpu. the peak gpu memory usage was 39 gb. unlike gans that required two
loss functions, our network was optimized with only one loss function. the
negative log-likelihood loss was stable and decreased monotonically.
table 1 summarizes the results for image quality metrics, while fig. 3 depicts
the same representative slice outputted by each method. while bm3d and srresnet
generated the highest psnr and ssim, the images were overly smooth and lacked
high-frequency components. important texture details were lost in the
restoration, which may negatively impact downstream tasks (e.g., radiologist
interpretation, cad algorithm performance) that rely on maintaining texture
features to characterize lesions. ctflow achieved 6% better perceptual quality
than sngan.
table 2 summarizes the ccc values for each kernel pair. mcbride [27] suggested
the following guidelines for interpreting lin's concordance correlation
coefficient. poor: <0:9; moderate: 0.90 to 0.95; substantial: 0.95 to 0.99;
perfect: >0.99 and above. ctflow achieved ccc scores within the "perfect" range
when assessing the agreement in f1 scores when given images reconstructed using
varying kernels.
we developed ctflow, a normalizing flows approach to mitigating variations in ct
scans. we demonstrated that ctflow achieved consistent performance across image
quality metrics, yielding the best perceptual quality score. moreover, ctflow
was better than a gan-based method in maintaining consistent lung nodule
detection performance. compared to generative models, the normalizing flows
approach offers exact and efficient likelihood computation and generates diverse
outputs that are closer to the target distribution.we note several limitations
of this work. in our evaluations, we trained separate ctflow and comparison
models for each mapping (e.g., transforming a 'smooth' kernel to a 'medium'
kernel scan), allowing us to troubleshoot models more easily. a single model
conditioned on different doses and kernels would be more practical. also, ctflow
depends on tuning a variance parameter; better psnr and ssim may have been
achieved with the optimization of this parameter. finally, this study focused on
mitigating the effect of a single ct parameter, either dose (in image quality)
or kernel (in nodule detection). in the real world, multiple ct parameters
interact (dose and kernel); these more complex interactions are being
investigated as part of future work.one underexplored area of normalizing flow
is its ability to generate the full distribution of possible outputs. using this
information, we can estimate where high uncertainty exists in the model output,
providing information to downstream image processing steps, such as
segmentation, object detection, and classification. for example, chan et al.
[28] applied an approximate bayesian inference scheme based on posterior
regularization to improve uncertainty quantification on covariate-shifted data
sets, resulting in improved prognostic models for prostate cancer. investigating
how this uncertainty can be incorporated into downstream tasks, such as our lung
nodule cade algorithm, is also part of future work.
. this work was supported by the national institute of biomedical imaging and
bioengineering of the national institutes of health under awards r56 eb031993
and r01 eb031993. the authors thank john m. hoffman, nastaran emaminejad, and
michael mcnitt-gray for providing access to the ucla low-dose ct dataset. the
content is solely the responsibility of the authors and does not necessarily
represent the official views of the national institutes of health.
colorectal cancer (crc) remains a major health burden with elevated mortality
worldwide [1]. most cases of crc arise from adenomatous polyps or sessile
serrated lesions in 5 to 10 years [9]. colonoscopy is considered the gold
standard for the detection of colorectal polyps. polyp segmentation is a
fundamental task in the computer-aided detection (cade) of polyps during
colonoscopy, which is of great significance in the clinical prevention of
crc.traditional machine learning approaches in polyp segmentation primarily
focus on learning low-level features, such as texture, shape, or color
distribution [14]. in recent years, encoder-decoder based deep learning models
such as u-net [12], unet++ [22], resunet++ [6], and pranet [4] have dominated
the field. furthermore, transformer [3,17,19,20] models have also been proposed
for polyp segmentation, and achieve the state-of-the-art(sota)
performance.despite significant progress made by these binary mask supervised
models, challenges remain in accurately locating polyps, particularly in complex
clinical scenarios, due to their insensitivity to complex lesions and high
false-positive rates. more specifically, most polyps have an elliptical shape
with well-defined boundaries. however, supervised segmentation learning solely
based on binary masks may not be effective in discriminating polyps in complex
clinical scenarios. endoscopic images often contain pseudo-polyp objects with
strong boundaries, such as colon folds, blood vessels, and air bubbles, which
can result in false positives. in addition, sessile and flat polyps have
ambiguous and challenging boundaries to delineate. to address these limitations,
qadir et al. [11] proposed using gaussian masks for supervised model training.
this approach reduces false positives significantly by assigning less attention
to outer edges and prioritizing surface patterns. however, this method has
limitations in accurately segmenting polyp boundaries, which are crucial for
clinical decision-making.therefore, the primary challenge lies in enhancing
polyp segmentation performance in complex scenarios by precisely preserving the
polyp segmentation boundaries, while simultaneously maximizing the decoder's
attention on the overall pattern of the polyps.in this paper, we propose a novel
transformer-based polyp segmentation framework, petnet, which addresses the
aforementioned challenges and achieves sota performance in locating polyps with
high precision. our contributions are threefold:• we propose a novel
gaussian-probabilistic guided semantic fusion method for polyp segmentation,
which improves the decoder's global perception of polyp locations and
discrimination capability for polyps in complex scenarios.• we evaluate the
performance of petnet on five widely adopted datasets, demonstrating its
superior ability to identify polyp camouflage and small polyp scenes, achieving
state-of-the-art performance in locating polyps with high precision.
furthermore, we show that petnet can achieve a speed of about 27fps in edge
computing devices (nvidia jetson orin). • we design several polyp instance-level
evaluation metrics, considering that conventional pixel-level calculation
methods cannot explicitly and comprehensively evaluate the overall performance
of polyp segmentation algorithms.
as shown in fig. 1, petnet is an end-to-end polyp segmentation framework
consists of three core module groups. (1) the encoder group employs a vision
transformer backbone [18] cascaded with a mixed transformer attention layer to
encode long-range dependent features at four scales. (2) the
gaussian-probabilistic modeling group consists of a gaussian probabilistic
guided unet-like decoder branch(gudb) and gaussian probabilistic-induced
transition(git) modules. (3) the ensemble binary decoders group includes a
unet-like structure branch(udb) [12], a fusion module(fus), and a cascaded
fusion module(cfm) [3].
to balance the trade-off between computational speed and feature representation
capability, we utilize the pre-trained pvtv2-b2 model [18] as the backbone.mixed
transformer attention(mta) layer is composed of local-global gaussian-weighted
self-attention (lgg-sa) and external attention (ea). we add a mta layer to
encode the last level features, enhancing the model's semantic representation
and accelerating the training process [16]. moreover, the encoder output
features are presented as {x e i } 4 i=1 with channels of [2c, 4c, 8c, 16c].
to incorporate both polyp location probability and surface pattern information
in a progressive manner, we propose the gaussian probabilistic-induced
transition (git) method. this method involves the interaction between a gaussian
auxiliary decoder and multiple binary decoders in a layer-wise fashion, as shown
in fig. 2.gaussian probabilistic mask. inspired by [11] and [21], in addition to
utilizing binary representation, polyps can also be represented as probability
heatmaps with blurred borders. we present a method of converting the binary
polyp maskw ×h×1 by utilizing elliptical gaussian kernels. specifically, for
every polyp in a binary mask, after masking other polyp pixels as background, we
calculatewhere (x o , y o ) is the mass of each polyp in the binary image f (x,
y). to rotate the output 2d gaussian masks according to the orientation, we set
a, b, c as followings,where σ 2 x and σ 2 y are the polyp size-adaptive standard
deviations [21], and θ is the orientation of each polyp [11]. finally, we
determine the final gaussian probabilistic mask p g for all polyps within an
image mask by computing the element-wise maximum.gaussian guided unet-like
decoder branch. the gaussian guided unet-like decoder branch(gudb) module is a
simple unet-like decoding branch supervised by gaussian probabilistic masks. we
employ four levels of encoder output features, and adjust encoder featureswith
channels of [c, 2c, 2c, 2c] in each level. at the final layer, a 1 × 1
convolution is used to convert the feature vector to one channel, producing a
size of h × w × 1 gaussian mask.gaussian probabilistic-induced transition
module. we use the gaussian probabilistic-induced transition module(git) to
achieve transition between binary features and gaussian features. given the
features originally sent to the decoder as binary features {x b i } 4 i=1 , and
the transformed encoder features sent to gudb as x g . we first splits 4 levels
of x b and x g into fixed groups as:where m is the corresponding number of
groups. then, we periodically arrange groups of x b i,m and x g i,m for each
level, and generate the regrouped feature q i ∈ r (ci+cg)×hi×wi in an
multi-layer sandwiches manner. soft grouping convolution [7] is then applied to
provide parallel nonlinear projections at multiple fine-grained sub-spaces (fig.
2). we further introduce residual learning in a parallel manner at different
group-aware scales. the final outputci×hi×wi is obtained for the udb decoder.
considering the computation cost, the binary features x b ← x e for the fus
decoder have channel numbers of [4c, 4c, 4c, 4c]. the fus decoder and cfm share
identical transited output features, while cfm exclusively utilizes the last
three levels of features.
during colonoscopy, endoscopists often use the two-physician observation
approach to improve the detection rate of polyps. building on this manner, we
propose the ensemble method that integrates multiple simple decoders to enhance
the detection and discrimination of difficult polyp samples. we demonstrate the
effectiveness of our approach using three commonly used convolutional decoders.
after git process, diverse level of gaussian probabilistic-induced binary
features were sent to these decoders. the output mask p is obtained by
element-wise summation of p i , where i represents the binary decoder
index.fusion module. as shown in fig. 1, set x i, i∈(1, 2, 3, 4) represent
multiscale mixed features. twice convolution following with bilinear
interpolation are applied to transform these feature with same 4c channels as x
1 , x 2 , x 3 , x 4 . afterward, we get x out with the resolution of h/4 × w/4 ×
c1 through following formula, where f represents twice 3×3 convolution:unet
decoder branch and cfm module. the structure of the udb is similar to that of
the gudb, except for the absence of channel reduction prior to decoding. in our
evaluation, we also examine the decoder cfm utilized in [3], which shares the
same input features (excluding the first level) as the fus.3 experiments
to evaluate models fairly, we completely follow p ranet [4] and use five public
datasets, including 548 and 900 images from clinicdb [2] and kvasir-seg [5] as
training sets, and the remaining images as validation sets. we also test the
generalization capability of all models on three unseen datasets (etis [13] with
196 images, cvc-colondb [8] with 380 images, and endoscene [15] with 60 images).
training settings are the same as [3].
our loss function formulates as l = n i=1 l i + λl g , andwhere n is the total
number of binary decoders, l g represents the l1 loss between the ground truth
gaussian mask g g and gudb prediction mask p g . λ is a hyperparameter used to
balance the binary and gaussian losses. furthermore, we employ intermediate
decoder outputs to calculate auxiliary losses for convergence acceleration.
conventional evaluation metrics for polyp segmentation are typically limited to
pixel-level calculations. however, metrics that consider the entire polyp are
also crucial. here we assess our model from both pixel-level and instance-level
perspectives.pixel-level evaluation is based on mean intersection over union
(miou ), mean dice coefficient (mdic), and weighted f 1 score (wf m ). for polyp
instance evaluation, a true positive (tp) is defined when the detection centroid
is located within the polyp mask. false positives (fps) occur when a wrong
detection output is provided for a negative region, and false negatives (fns)
occur when a polyp is missed in a positive image. finally, we compute
sensitivity nsen = t p/(t p + f n) × 100, precision npre = t p/(t p + f p ) ×
100, and nf 1 = 2 × (sen × pre)/(sen + pre) × 100 based on the number count for
instance evaluation.
training and learning ability. table s1 displays the results of our model's
training and learning performance. our model achieves comparable performance to
the sota model on the kvasir-seg and clinicdb datasets. notably, our model
yields superior results in false-positive instance evaluation.generalization
ability. the generalization results are shown in table 1. we conduct three
unseen datasets to test models' generalizability. results show that p et net
achieves excellent generalization performance compared with previous models.
most importantly, our false-positive instance counts(45 in etis and 55 in
cvc-colondb) reduce significantly of other models. we also observe a performance
mismatch phenomenon in pixel-level evaluation and instance-level
evaluation.small polyp detection ability. the detection capability results of
small polyps are shown in table 2. diminutive polyps are hard to precisely
detect, while they are the major targets of optical biopsies performed by
endoscopists. we selected images from two unseen datasets with 0∼2% polyp
labeled area to perform the test. as shown, p et net demonstrates great strength
in both datasets, which indicates that one of the major advantages of our model
lies in detecting small polyps with lower false-positive rates.ablation
analysis. table 3 presents the results of our ablation study, where we
investigate the contribution of the two key components of our model, namely the
gaussian-probabilistic guided semantic fusion method and ensemble decoders. we
observe that while the impact of each binary decoder varies, all sub binary
decoders contribute to the overall performance. furthermore, the git method
significantly enhances instance-level evaluation without incurring performance
penalty in pixel-level evaluation, especially in unseen datasets.
fig. s1 shows that our proposed model, p et net, outperforms sota models in
accurately identifying polyps under complex scenarios, including lighting
disturbances, water reflections, and motion blur. faluire cases are shown in
fig. s2.
furthermore, we deployed p et net on the edge computing device nvidia jetson
orin and optimized its performance using tensorrt. our results demonstrate that
p et net achieves real-time denoising and segmentation of polyps with high
accuracy, achieving a speed of 27 frames per second on the device(video s1).
based on intrinsic characteristics of the endoscopic polyp image, we
specifically propose a novel segmentation framework named petnet consisting of
three key module groups. experiments show that p et net consistently outperforms
most current cutting-edge models on five challenging datasets, demonstrating its
solid robustness in distinguishing other intestinal analogs. most importantly, p
et net shows better sensitivity to complex lesions and diminutive polyps.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43990-2_54.
breast cancer (bc) is the most common cancer in women and incidence is
increasing [14]. with the wide adoption of population-based mammography
screening programs for early detection of bc, millions of mammograms are
conducted annually worldwide [23]. developing artificial intelligence (ai) for
abnormality detection is of great significance for reducing the workload of
radiologists and facilitating early diagnosis [21]. besides using the
data-driven manner, to achieve accurate diagnosis and interpretation of the
ai-assisted system output, it is essential to consider mammogram domain
knowledge in a model-driven fashion.authenticated by the bi-rads lexicon [12],
the asymmetry of bilateral breasts is a crucial clinical factor for identifying
abnormalities. in clinical practice, radiologists typically compare the
bilateral craniocaudal (cc) and mediolateral oblique (mlo) projections and seek
the asymmetry between the right and left views. notably, the right and the left
view would not have pixel-level symmetry differences in imaging positions for
each breast and biological variations between the two views. leveraging
bilateral mammograms (bi-mg) is one of the key steps to detect asymmetrical
abnormalities, especially for subtle and non-typical abnormalities. to mimic the
process of radiologists, previous studies only extracted simple features from
the two breasts and used fusion techniques to perform the classification
[6,20,22,24,25]. besides these simple feature-fusion methods, recent studies
have demonstrated the powerful ability of transformerbased methods to fuse
information in multi-view (mv) analysis (cc and mlo view of unilateral breasts)
[1,16,26]. however, most of these studies formulate the diagnosis as an mv
analysis problem without dedicated comparisons between the two breasts.the
question of "what the bi-mg would look like if they were symmetric?" is often
considered when radiologists determine the symmetry of bi-mg. it can provide
valuable diagnostic information and guide the model in learning the diagnostic
process akin to that of a human radiologist. recently, two studies explored
generating healthy latent features of target mammograms by referencing
contralateral mammograms, achieving state-of-the-art (sota) classification
performance [18,19]. none of these studies is able to reconstruct a normal
pixellevel symmetric breast in the model design. image generation techniques for
generating symmetric bi-mg have not yet been investigated. visually, the
remaining parts after the elimination of asymmetrical abnormalities are the
appearance of symmetric bi-mg. a more interpretable and pristine strategy is
disentanglement learning [9,17] which utilizes synthetic images to supervise the
model in separating asymmetric anomalies from normal regions at the image
level.in this work, we present a novel end-to-end framework, disasymnet, which
consists of an asymmetric transformer-based classification (asyc) module and an
asymmetric abnormality disentanglement (asyd) module. the asyc emulates the
radiologist's analysis process of checking unilateral and comparing bi-mg for
abnormalities classifying. the asyd simulates the process of disentangling the
abnormalities and normal glands on pixel-level. additionally, we leverage a
self-adversarial learning scheme to reinforce two modules' capacity, where the
feedback from the asyc is used to guide the asyd's disentangling, and the asyd's
output is used to refine the asyc in detecting subtle abnormalities. to
in this study, the "asymmetric" refers to the visual differences on perception
level that can arise between the left and right breasts due to any abnormality,
including both benign and malignant lesions. thus, a paired bi-mg is considered
symmetrical only if both sides are normal and the task is different from the
malignancy classification study [13]. the paired bi-mg of the same projection is
required, which can be formulated as i = {x r , x l , y asy , y r , y l }. here,
x ∈ r h×w represents a mammogram with the size of h × w , x r and x l correspond
to the right and left view respectively. y r , y l , y asy ∈ {0, 1} are binary
labels, indicating abnormality for each side, and the asymmetry of paired bi-mg.
the framework of our disasymnet is illustrated in fig. 1. specifically, the asyc
module takes a pair of bi-mg as input and predicts if it is asymmetric and if
any side is abnormal. we employ an online class activation mapping (cam) module
[10,11] to generate heatmaps for segmentation and localization. subsequently,
the asyd module disentangles the abnormality from the normal part of the bi-mg
through the self-adversarial learning and synthesis method.
the asyc module consists of shared encoders ψ e and asymmetric transformer
layers ψ asyt to extract features and learn bilateral-view representations from
the paired mammograms. in this part, we first extract the starting features f of
each side (f r , f l represent the right and left features respectively) through
ψ e in the latent space for left-right inspection and comparison, which can be
denoted as f = ψ e (x). then the features are fed into the ψ asyt .unlike other
mv transformer methods [1,16] that use only cross-attention (ca), our asymmetric
transformer employs self-attention (sa) and ca in parallel to aggregate
information from both self and contralateral sides to enhance the side-by-side
comparison. this is motivated by the fact that radiologists commonly combine
unilateral (identifying focal suspicious regions according to texture, shape,
and margin) and bilateral analyses (comparing them with symmetric regions in the
contralateral breasts) to detect abnormalities in mammography [6]. as shown in
the right of fig. 1, starting features f are transformed into query (f q ), key
(f k ), and value (f v ) vectors through feed-forward network (ffn) layers. the
sa and ca modules use multi-head attention (mha),with the number of heads h = 8,
which is a standard component in transformers and has already gained popularity
in medical image fields [1,16,26]. in the sa, the query, key, and value vectors
are from the same features,while in the ca, we replace the key and value vectors
with those from the contralateral features,then, the starting feature f , and
the attention features f sa and f ca are concatenated in the channel dimension
and fed into the ffn layers to fuse the information and maintain the same size
as f . the transformer block is repeated n = 12 times to iteratively integrate
information from bi-mg, resulting in the output feature f r out , f l out = ψ n
=12 asyt (f r , f l ). to predict the abnormal probability ŷ of each side, the
output features f out are fed into the abnormal classifier. for the asymmetry
classification of paired mammograms, we compute the absolute difference of the
output features between the right and left sides (f asy out = abs(f r out -f l
out ), which for maximizing the difference between the two feature) and feed it
into the asymmetry classifier. we calculate the classification loss using the
binary cross entropy loss (bce) l bce , denoted as l diag = l cls (y asy , y r ,
y l , x r , x l ) = l bce (y asy , ŷasy )+l bce (y, ŷ).
what would the bi-mg look like when the asymmetrical abnormalities have been
removed? unlike previous studies [18,19], which only generated normal features
in the latent space, our asyd module use weights shared u-net-like decoders ψ g
, to generate both abnormal (x ab ) and normal (x n ) images for each side
through a two-channel separation, as x n , x ab = ψ g (f out ). we constrain the
model to reconstruct images realistically using l1 loss (l l1 ) with the
guidance of cams (m ), as follows, l rec = l l1 ((1 -m )x, (1 -m )x n ) + l l1
(m x, x ab ). however, it is difficult to train the generator in a supervised
manner due to the lack of annotations of the location for asymmetrical pairs.
inspired by previous self-adversarial learning work [10], we introduce a frozen
discriminator ψ d to impose constraints on the generator to address this
challenge. the frozen discriminator comprises the same components as asyc. in
each training step, we update the discriminator parameters by copying them from
the asyc for leading ψ g to generate the symmetrical bi-mg. the ψ d enforces
symmetry in the paired bi-mg, which can be denoted as l dics = l cls (y asy = 0,
y r = 0, y l = 0, x r n , x l n ). furthermore, we use generated normal bi-mg to
reinforce the ability of asyc to recognize subtle asymmetry and abnormal cues,
as l ref ine = l cls (y asy , y r , y l , x r n , x l n ).
to alleviate the lack of annotation pixel-wise asymmetry annotations, in this
study, we propose a random synthesis method to supervise
disentanglement.training with synthetic artifacts is a low-cost but efficient
way to supervise the model to better reconstruct images [15,17]. in this study,
we randomly select the number n ∈ [1, 2, 3] of tumors t from a tumor set t
inserting into one or both sides of randomized selected symmetric bi-mg (x r , x
l |y asy = 0). for each tumor insertion, we randomly select a position within
the breast region. the tumors and symmetrical mammograms are combined by an
alpha blending-based method [17], which can be denoted by x|fake =the alpha
weights α k is a 2d gaussian distribution map, in which the co-variance is
determined by the size of k-th tumor t, representing the transparency of the
pixels of the tumor. some examples are shown in fig. 1. the tumor set t is
collected from real-world datasets. specifically, to maintain the rule of
weaklysupervised learning of segmentation and localization tasks, we collect the
tumors from the ddsm dataset as t and train the model on the inbreast
dataset.when training the model on other datasets, we use the tumor set
collected from the inbreast dataset. thus, the supervised reconstruction loss is
l syn = l l1 (x|real, x n |fake), where x|real is the real image before
synthesis and x n |fake is the disentangled normal image from the synthesised
image x|fake.
for each training step, there are two objectives, training asyc and asyd module,
and then is the refinement of asyc. for the first, the loss function can be
denoted bythe values of weight terms λ 1 , λ 2 , λ 3 , and λ 4 are
experimentally set to be 1, 0.1, 1, and 0.5, respectively. the loss of the
second objective is l ref ine as aforementioned.
this study reports experiments on four mammography datasets. the inbreast
dataset [7] consists of 115 exams with bi-rads labels and pixel-wise
anno-tations, comprising a total of 87 normal (bi-rads = 1) and 342 abnormal
(bi-rads = 1) images. the ddsm dataset [3] consists of 2,620 cases, encompassing
6,406 normal and 4,042 (benign and malignant) images with outlines generated by
an experienced mammographer. the vindr-mammo dataset [8] includes 5,000 cases
with bi-rads assessments and bounding box annotations, consisting of 13,404
normal (bi-rads = 1) and 6,580 abnormal (bi-rads = 1) images. the in-house
dataset comprises 43,258 mammography exams from 10,670 women between 2004-2020,
collected from a hospital with irb approvals. in this study, we randomly select
20% women of the full dataset, comprising 6,000 normal (bi-rads = 1) and 28,732
abnormal (bi-rads = 1) images. due to a lack of annotations, the in-house
dataset is only utilized for classification tasks. each dataset is randomly
split into training, validation, and testing sets at the patient level in an
8:1:1 ratio, respectively (except for that inbreast which is split with a ratio
of 6:2:2, to keep enough normal samples for the test).table 1. comparison of
asymmetric and abnormal classification tasks on four mammogram datasets. we
report the auc results with 95% ci. note that, when ablating the "asyc ", we
only drop the "asyt" and keep the encoders and classifiers.
the mammogram pre-processing is conducted following the pipeline proposed by
[5]. then we standardize the image size to 1024 × 512 pixels. for training
models, we employ random zooming and random cropping for data augmentation. we
employ the resnet-18 [2] with on imagenet pre-trained weights as the common
backbone for all methods. the adam optimizer is utilized with an initial
learning rate (lr) of 0.0001, and a batch size of 8. the training process on the
inbreast dataset is conducted for 50 epochs with a lr decay of 0.1 every 20
epochs. for the other three datasets, the training is conducted separately on
each one with 20 epochs and a lr decay of 0.1 per 10 epochs. all experiments are
implemented in the pytorch framework and an nvidia rtx a6000 gpu (48 gb). the
training takes 3-24 h (related to the size of the dataset) on each dataset.to
assess the performance of different models in classification tasks, we calculate
the area under the receiver operating characteristic curve (auc) metric. the 95%
confidence interval (ci) of auc is estimated using bootstrapping (1,000 times)
for each measure. for the segmentation task , we utilize intersection over union
(iou), intersection over reference (ior), and dice coefficients. for the
localization task , we compute the mean accuracies of iou or ior values above a
given threshold, following the approach [11]. specifically, we evaluated the
mean accuracy with thresholds for iou at 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, and 0.7,
while the thresholds for ior are 0.1, 0.25, 0.5, 0.75, and 0.9.
we compare our proposed disasymnet with single view-based baseline resnet18,
attention-driven method ham [11], mv-based late-fusion method [4], current sota
mv-based methods cross-view-transformer (cvt) [16], and attention-based mv
methods proposed by wang et al., [20] on classification, segmentation, and
localization tasks. we also conduct an ablation study to verify the
effectiveness of "asyc ", "asyd", and "synthesis". note that, the asymmetric
transformer (asyt) is a core component of our proposed "asyc ". thus, when
ablating the "asyc ", we only drop the asyt and keep the encoders and
classifiers. the features from the bi-mg are simply concatenated and passed to
the classifier.
for the classification task, the auc results of abnormal classification are
shown in table 1. our method outperforms all the single-based and mv-based
methods in these classification tasks across all datasets. furthermore, the
ablation studies demonstrate the effectiveness of each proposed model component.
in particular, our "asyc" only method already surpasses the cat method,
indicating the efficacy of the proposed combination of sa and ca blocks over
using ca alone. additionally, our "asyd" only method improves the performance
compared to the late-fusion method, demonstrating that our disentanglement-based
self-adversarial learning strategy can refine classifiers and enhance the
model's ability to classify anomalies and asymmetries. the proposed "synthesis"
method further enhances truth from the "synthesis" method, our generator tends
to excessively remove asymmetric abnormalities at the cost of leading to the
formation of black holes or areas that are visibly darker than the surrounding
tissue because of the limitation of our discriminator and lack of pixel-level
supervision. the incorporation of proposing synthetic asymmetrical bi-mg during
model training can lead to more natural symmetric tissue generation.
we present, disasymnet, a novel asymmetrical abnormality disentangling-based
self-adversarial learning framework based on the image-level class labels only.
our study highlights the importance of considering asymmetry in mammography
diagnosis in addition to the general multi-view analysis. the incorporation of
pixel-level normal symmetric breast view generation boosts the classification of
bi-mg and also provides the interpretation of the diagnosis. the extensive
experiments on four datasets demonstrate the robustness of our disasymnet
framework for improving performance in classification, segmentation, and
localization tasks. the potential of leveraging asymmetry can be further
investigated in other clinical tasks such as bc risk prediction.
over 430,000 new cases of renal cancer were reported in 2020 in the world [1]
and this number is expected to rise [22]. when the tumor size is large (greater
than 7 cm) often the whole kidney is removed, however, when the tumor size is
small (less than 4 cm), partial nephrectomy is the preferred treatment [20] as
it could preserve kidney's function. thus, early detection of kidney tumors can
help to improve patient's prognosis. however, early-stage renal cancers are
usually asymptomatic, therefore they are often incidentally found during other
examinations [19], which includes non-contrast ct (ncct) scans.segmentation of
kidney tumors on ncct images adds challenges compared to contrast-enhanced ct
(cect) images, due to low contrast and lack of multiphase images. on cect
images, the kidney tumors have different intensity values compared to the normal
tissues. there are several works that demonstrated successful segmentation of
kidney tumors with high precision [13,21]. however, on ncct images, as shown in
fig. 1b, some tumors called isodensity tumors, have similar intensity values to
the surrounding normal tissues. to detect such tumors, one must compare the
kidney shape with tumors to the kidney shape without the tumors so that one can
recognize regions with protuberance.3d u-net [3] is the go-to network for
segmenting kidney tumors on cect images. however, convolutional neural networks
(cnns) are biased towards texture features [5]. therefore, without any
intervention, they may fail to capture the protuberance caused by isodensity
tumors on ncct images.in this work, we present a novel framework that is capable
of capturing the protuberances in the kidneys. our goal is to segment kidney
tumors including isodensity types on ncct images. to achieve this goal, we
create a synthetic dataset, which has separate annotations for normal kidneys
and protruded regions, and train a segmentation network to separate the
protruded regions from the normal kidney regions. in order to segment whole
tumors, our framework consists of three networks. the first is a base network,
which extracts kidneys and an initial tumor region masks. the second
protuberance detection network receives the kidney region mask as its input and
predicts a protruded region mask. the last fusion network receives the initial
tumor mask and the protruded region mask to predict a final tumor mask. this
proposed framework enables a better segmentation of isodensity tumors and boosts
the performance of segmentation of kidney tumors on ncct images. the
contribution of this work is summarized as follows:1. present a pioneering work
for segmentation of kidney tumors on ncct images. 2. propose a novel framework
that explicitly captures protuberances in a kidney to enable a better
segmentation of tumors including isodensity types on ncct images. this framework
can be extended to other organs (e.g. adrenal gland, liver, pancreas). 3. verify
that the proposed framework achieves a higher dice score compared to the
standard 3d u-net using a publicly available dataset.
the release of two public ct image datasets with kidney and tumor masks from the
2019/2021 kidney and kidney tumor segmentation challenge [8] (kits19, kits21)
attracted researchers to develop various methods for segmentation.looking at the
top 3 teams from each challenge [6,11,13,17,21], all teams utilized 3d u-net [3]
or v-net [16], which bears a similar architecture. the winner of kits19 [13]
added residual blocks [7] to 3d u-net and predicted kidney and tumor regions
directly. however, the paper notes that modifying the architecture resulted in
only slight improvement. the other 5 teams took a similar approach to nnu-net's
coarse-to-fine cascaded network [12], where it predicts from a low-resolution
image in the first stage and then predicts kidneys and tumors from a
high-resolution image in the second stage. thus, although other attempts were
made, using 3d u-net is the go-to method for predicting kidneys and tumors. in
our work, we also make use of 3d u-net, but using this network alone fails to
learn some isodensity tumors. to overcome this issue, we developed a framework
that specifically incorporates protuberances in kidneys, allowing for an
effective segmentation of tumors on ncct images.in terms of focusing on
protruded regions in kidneys, our work is close to [14,15]. [14] developed a
computer-aided diagnosis system to detect exophytic kidney tumors on ncct images
using belief propagation and manifold diffusion to search for protuberances. an
exophytic tumor is located on the outer surface of the kidney that creates a
protrusion. while this method demonstrated high sensitivity (95%), its false
positives per patient remained high (15 false positives per patient). in our
work, we will not only segment protruded tumors but also other tumors as well.
the first base network is responsible for predicting kidney and tumor region
masks. our architecture is based on 3d u-net, which has an encoder-decoder style
architecture, with few modifications. to reduce the required size of gpu memory,
we only use the encoder that has only 16 channels at the first resolution, but
instead we make the architecture deeper by having 1 strided convolution and 4
max-pooling layers. in the decoder, we replace the up-convolution layers with a
bilinear up-sampling layer and a convolution layer. in addition, by only having
a single convolution layer instead of two in the original architecture at each
resolution, we keep the decoder relatively small. throughout this paper, we
refer this architecture as our 3d u-net.the second protuberance detection
network is the same as the base network except it starts from 8 channels instead
of 16. we train this network using synthetic datasets. the details of the
dataset and training procedures are described in sect. 3.2.the last fusion
network combines the outputs from the base network and the protuberance
detection network and makes the final tumor prediction. in detail, we perform a
summation of the initial tumor mask and the protruded region mask, and then
concatenate the result with the input image. this is the input of the last
fusion network, which also has the same architecture as the base network with an
exception of having two input channels. this fusion network do not just combine
the outputs but also is responsible for removing false positives from the base
network and the protuberance detection network.our combined three network is
fully differentiable, however, to train efficiently, we train the model in 3
steps.
in the first step, we train the base network, which is a standard segmentation
network, to extract kidney and tumor masks from the images. we use a sigmoid
function for the last layer. and as a loss function, we use the dice loss [16]
and the cross-entropy loss equally.
in the second step, we train the protuberance detection network alone to
separate protruded regions from the normal kidney masks. here, we only use the
crossentropy loss and label smoothing with a smoothing factor of = 0.01.
synthetic dataset. to enable a segmentation of protruded regions only, a
separate annotation of each region is usually required. however, annotating such
areas is time-consuming and preparing a large number of data is challenging.
alternatively, we create a synthetic dataset that mimics a kidney with
protrusions. the synthetic dataset is created through the following steps:1.
randomly sample a kidney mask without protuberance and a tumor mask. 2. apply
random rotation and scaling to the tumor mask. 3. randomly insert the tumor mask
into the kidney mask. 4. if both of the following conditions are met, append to
the dataset.where k i is a voxel value (0 or 1) in the kidney mask and t i is a
voxel value in the tumor mask. equation 1 ensures that only up to 30% of the
kidney is covered with a tumor. equation 2ensures that not all tumors are
covered by the kidney (at least 5% of the tumor is protruded from the kidney).
in the final step, we train the complete network jointly. although our network
is fully differentiable, since there is no separate annotation for protruded
regions other from the synthetic dataset, we freeze the parameters in
protuberance detection network.the output of the protuberance detection network
will likely have more false positives than the base network since it has no
access to the input image. thus, when the output of the protuberance detection
network is concatenated with the output of the base network, the fusion network
can easily reduce the loss by ignoring the protuberance detection network's
output, which is suboptimal. to avoid this issue, we perform summation not
concatenation to avoid the model from ignoring all output from the protuberance
detection network. we then clip the value of the mask to the range of 0 and 1.
as a result, the input to the fusion network has two channels. the first channel
is the input image, and the second channel is the result of summation of the
initial tumor mask and the protruded region mask. we concatenate the input image
so that the last network can remove false positives from the predicted masks as
well as predicting the missing tumor regions from the protuberance detection
network.we use the dice loss and the cross-entropy loss as loss functions for
the fusion network. we also keep the loss functions in the base network for
predicting kidneys and tumors. the loss function for tumors in the base network
acts like an intermediate supervision. our network shares some similarities with
the stacked hourglass network [18] where the network consists of multiple u-net
like hourglass modules and has intermediate supervision at the end of each
hourglass module. by having multiple modules in this manner, the network can fix
the initial mistakes in early modules and corrects in later modules.
no prior work exists that uses ncct images from kits19 [8,9]. thus, we first
created our baseline model and compared the performance with existing methods on
cect images. this allows us to ensure that our baseline model and training
procedure is appropriate. we then trained the model using ncct images and
compared with our proposed method.
we used a dataset from kits19 [8] which contains both cect and ncct images. for
cect images, there are 210 images for training and validation and, 90 images for
testing. for ncct images, there are 108 images, which are different series of
the 210 images. the ground truth masks are only available for the 210 cect
images. thus, we transfer the masks to ncct images. this is achieved by
extracting kidney masks and adjusting the height of each kidney. the ground
truth mask contains a kidney label and a kidney tumor label. cysts are not
annotated separately and included in the kidney label on this dataset. the data
can be downloaded from the cancer imaging archive (tcia) [4,9].the images were
first clipped to the intensity value range of [-90, 210] and normalized from -1
to 1. the voxel spacings were normalized to 1 mm. during the training, the
images were randomly cropped to a patch size of 128×128×128 voxels. we applied
random rotation, random scaling and random noise addition as data
augmentation.during the step2 phase of the training, where we used the synthetic
dataset, we created 10,000 masks using the method from sect. 3.2. we applied
some augmentations during training to input masks to simulate the incoming
inputs from the base network. the output of the base network is not binarized to
keep gradient from flowing, so the values are in the range [0, 1] and the edge
of kidneys are usually smooth. therefore, we applied gaussian blurring, gaussian
noise addition and intensity value shifting.
our model was trained using sgd with a 0.9 momentum and a weight decay of 1e-7.
we employed a learning rate scheduler, which we warm-up linearly from 0.0001 to
0.1 during the first 30% (for step1 and step3) or 10% (for step2) of the total
training steps and decreased following the cosine decay learning rate.a
mini-batch size of 8, 16 and 4 were used, and trained for 250k, 100k and 100k
steps during step1 to 3 respectively. we conducted our experiments using jax
(v.0.4.1) [2] and haiku (v.0.0.9) [10]. we trained the model using a single
nvidia rtx a5000 gpu.for the experiment on cect images, we used the dice score
as our evaluation metrics following the same formula from kits19. for the
experiment on ncct images, we also evaluated the sensitivity and false positives
per image (fps/image). we calculated as true positive when the predicted mask
has the dice score greater than 0.5, otherwise we calculated as false negative.
on the other hand, false positives were counted when the predicted mask did not
overlap with any ground truth masks.
to show that our model is properly tuned, we compare our baseline model with an
existing method using cect images. as can be seen from table 1, our model showed
comparable scores to the winner of kits19 challenge. we used this baseline model
as our base network for the experiments on ncct images.
table 2 shows our experimental results and ablation studies on ncct images. the
proposed method (table 2-bottom) outperformed the baseline model (table 2-top).
the ablation studies show that adding each component (cect images and the
protuberance detection network) resulted in an increase in the performance.
while adding cect images contributed the most for the increase in tumor dice and
sensitivity, adding the protuberance detection network further pushed the
performance. however, the false positives per image (fps/image) increased from
0.283 to 0.421. the protuberance detection network cannot distinguish the
protrusions that were caused by tumors or cysts, so the output from this network
has many fps at this stage. thus, the fusion network has to eliminate cysts by
looking again the input image, however, it may have failed to eliminate some
cysts (fig. 3 second row).
in this paper, we proposed a novel framework for kidney tumor segmentation on
ncct images. to cope with isodensity tumors, which have similar intensity values
to their surrounding tissues, we created a synthetic dataset to train a network
that extracts protuberance from the kidney masks. we combined this network with
the base network and fusion network. we evaluated our method using the publicly
available kits19 dataset, and showed that the proposed method can achieve a
higher sensitivity than existing approach. our framework is not limited to
kidney tumors but can also be extended to other organs (e.g., adrenal gland,
liver, pancreas).
evolution, the change of pigmented skin lesions, is a risk factor for melanoma
[1]. therefore, longitudinal tracking of skin lesions over the whole body is
beneficial for early detection of melanoma [5]. however, establishing skin
lesion correspondences across multiple scans from different patient visits has
not been well investigated in the context of full-body imaging.several
techniques have been proposed to match skin lesions across pairs of 2d images
[9][10][11][12][13][14]16,17,25]. early work used geometric constraints imposed
by initial matches of skin lesions (manual selection or automatic detection) to
align images and further match other skin lesions [10,16,17,25]. mirzaalian and
colleagues published a series of works for establishing lesion correspondence in
image space [11][12][13][14]. li et al. [9] used a cnn to output a 2d vector
field for pixel-wise correspondences between the two input images. though
effective at matching skin lesions across pairs of images, the extension of
these methods to the context of total body photography (tbp) for longitudinal
tracking remains a challenge.several works have been proposed for tackling the
skin lesion tracking problem over the full body [7,8,21,22]. however, they are
either only applicable in well-controlled environments or do not extend to the
tracking of lesions across scans at different visits. recently, the concept of
finding lesion correspondence using a 3d representation of the human body has
been explored in [26] and [2] by using a template mesh. however, accurately
deforming a template mesh to fit varying body shapes is challenging when the
scanned shape deviates from the template, leading to large errors in downstream
tasks such as establishing shape correspondence. additionally, [26] does not
take advantage of texture, while [2] uses texture in a common uv map that may
lead to failures when geodesically close locations on the surface are mapped to
distant sites in the texture map (e.g. when the two locations are on opposite
sides of a texture seam).we propose a novel framework for finding skin lesion
correspondence iteratively using geometric and texture information (fig. 1). we
demonstrate the effectiveness of the proposed method in localizing lesion
correspondence across scans in a manner that is robust to changes in body pose
and camera viewing directions. our code is available at
https://github.com/weilunhuang-jhu/ lesioncorrespondencetbp3d.
given a set of lesions of interest (lois) x in the source mesh, we would like to
find their corresponding positions y in the target mesh. formally, we assume we
are given source and target meshes, m 0 and m 1 , with vertex setswe achieve
this by computing a dense correspondence map φ l, : v 0 → v 1 , initially
defined using geometric information and refined using textural information. then
we use that to define a map taking lesions of interest on the source to
positions on the target φ : x → v 1 .
we define an initial dense correspondence between source and target vertices by
leveraging the sparse landmark correspondences [3,6]. we do this by mapping
source and target vertices into a high-dimensional space, based on their
proximity to the landmarks, and then comparing positions in the high-dimensional
space.concretely, we define maps k : v k → r s , associating a vertex v ∈ v k
with an s-dimensional feature descriptor that describes the position of v
relative to the landmarks:whereis the geodesic distance function on m k . we use
the reciprocal of geodesic distance so that landmarks closer to v contribute
more significantly to the feature vector. given this mapping, we create an
initial dense correspondence between the source and target vertices, φ l, : v 0
→ v 1 by mapping a source vertex v ∈ v 0 to the target vertex with the most
similar feature descriptor (with similarity measured in terms of the normalized
cross-correlation):
while feature descriptors of corresponding vertices on the source and target
mesh are identical when 1) the landmarks are in perfect correspondence, and 2)
the source and target differ by an isometry, neither of these assumptions holds
in realworld data. to address this, we use local texture to assign an additional
feature descriptor to each vertex and use these texture-based descriptors to
refine the coarse correspondence given by φ l, : v 0 → v 1 . various texture
descriptors have been proposed, e.g. shot [20,23,24], rops [4], and echo [15].
we selected the echo descriptor for its better descriptiveness and robustness to
noise. letting k (v) ∈ r n denote the echo descriptor of vertex v ∈ v k , our
goal is to refine the dense correspondence so that corresponding source and
target vertices also have similar descriptors. however, to avoid problems with
repeating (local) textures, we would also like the correspondence to stay close
to the correspondence defined by the landmarks.we achieve this as follows: to
every source vertex v ∈ v 0 we associate a region r v ⊂ v 1 of target vertices
that are either close to φ l, (v) (the corresponding vertex on v 1 as predicted
by the landmarks) or have similar geometric feature descriptors:(given this
region, we define the target vertex corresponding to a source as the vertex
within the region that has the most similar echo descriptor (using the
normalized cross-correlation as before).in practice, we compute the echo
descriptor over three different radii, obtaining three descriptors for each
vertex,the selection of three different radii in echo descriptors is done to
accommodate different sizes of lesions and their surrounding texture, and the
values are empirically determined. this gives a mapping φ l, : v 0 → v 1 defined
in terms of the weighted sum of cross-correlations:whereis the texture score of
the target vertex v and w i is the weight of the cross-correlation between the
echo descriptors computed at each radius.
while each source loi has a corresponding position on the target mesh as given
by φ l, :, not all correspondences are localized with high confidence when 1)
the local texture is not well-preserved across scans and 2) the local region r v
does not include the true correspondence. to address this, we adapt our
algorithm for computing the correspondence map φ : x → v 1 by iteratively
growing the set of landmarks to include loi correspondences about which we are
confident, similar to the way in which a human annotator would label lesion
correspondence (fig. 2). iteratively anchor confident correspondences. we
iteratively compute correspondence maps φ k l, :, with the superscript denoting
the k th iteration. for each map φ k l, and every loi x ∈ x, we determine if we
are confident in the correspondence {x, φ k l, (x)} by evaluating a binary
function χ k l : x → {0, 1}. denoting by x the subset of lois about which we are
confident, we add the pairs {x , φ k l, (x )} to the landmark set l and remove
the loi x ∈ x from x. we iterate this process until all the correspondences of
lois are confidently found or a maximum number of iterations (k) have been
performed.lesion correspondence confidence is measured using three criteria: i)
texture similarity, ii) agreement between geometric and textural
correspondences, and iii) the unique existence of a similar lesion within a
region. to quantify uniqueness, we compute the set of target vertices whose
textural descriptor is similar to that of the loi:and consider the diameter of
the set (defined in terms of the mean of the distances of vertices in s δ x from
the centroid of s δ x ). putting this together, we define confidence asx ) < ε5,
(6) where the initial values of thresholds ε i are empirically chosen. to
further support establishing correspondences, we relax the thresholds ε i in
subsequent iterations, allowing us to consider correspondences that are further
away and about which we are less confident.final correspondence map. having
mapped every high-confidence loi to a corresponding target vertex, we must
complete the correspondence for the remaining low-confidence lois. we note that
for a low-confidence loi x ∈ x, the texture in the source mesh is not
well-matched to the texture in the target, for any v ∈ r x . (otherwise the
first term in χ k l would be large.) to address this, we would like to focus on
landmark-based similarity. however, by definition of r x , for all v ∈ r x , we
know that the landmark descriptors of x and v will all be similar, so that c l,
will not be discriminating. instead, we use a standard transformation to turn
distances into similarities. specifically, we define geometric score between a
source loi x and a target vertex v ∈ r x in terms of the geodesic distance
between v and the corresponding position of x in v 1 , as predicted by the
landmark descriptors:where σ is the maximum geodesic distance from a vertex
within r x to φ k l, (x). therefore, for a remaining loi, we define its
corresponding target vertex as the vertex with the highest weighted sum of the
geometric and texture scores:where w 1 and w 2 are the weights for combining the
scores.
we evaluated our methods on two datasets. the first dataset is from skin3d [26]
(annotated 3dbodytex [18,19]). the second dataset comes from a 2d imaging-rich
total body photography system (irtbp), from which the 3d textured meshes are
derived from photogrammetry 3d reconstruction. the number of vertices is on
average 300k and 600k for skin3d and irtbp datasets respectively. the runtime
using 10 iterations is several minutes (on average) on an intel i7-11857g7
processor. example data of the two datasets can be found in the supplement.
average correspondence localization error (cle) for individual subjects, defined
as the geodesic distance between the ground-truth and the estimated lesion
correspondence, is shown in fig. 3. to interpret cle in a clinical application,
the localized correspondence is successful if its cle is less than a threshold
criterion.we measured the success rate as the percentage of the correctly
localized skin lesions over the total number of skin lesion pairs in the
dataset.to compare our result to the existing method [26], we compute our
success rates with the threshold criterion at 10 mm. as shown in table 1, the
performance of our method is comparable to the previously reported longitudinal
accuracy. the qualitative result of the localized correspondence in the skin3d
dataset is shown in fig. 4. a table of parameters used in the experiments can be
found in the supplement. table 1. comparison of the success rate on skin3d
dataset. each metric is computed on a pair of meshes (for one subject) and
averaged across paired meshes with the standard deviation shown in brackets. the
method texture radius 50 and combined radius 50 are defined in fig. 5.skin3d
[26]
we believe that the geometric descriptor only provides a coarse correspondence
while the local texture is more discriminating. figure 5 shows the success rate
under different threshold criteria for the proposed methods. since we have two
combinations of defining source and target for two scans, we measured the result
in both to ensure consistency (fig. 5(a) and (b)). as expected, we observed that
using geometric information with body landmarks and geodesic distances is
insufficient for localizing lesion correspondence accurately. however, the
correspondence map φ l, refined with local texture may lead to correspondences
with large cle when using a large region r x . the figure shows the
discriminating power and the large-error-prone property of using φ l, to
localize lesion correspondence with one iteration (relatively high success rates
under strict criteria and relatively low success rates under loose criteria,
compared to the correspondence map combining geometric and texture scores in eq.
8). the figure also shows the effectiveness of the proposed algorithm when the
iterative anchor mechanism is used to localize lesion correspondence, having
consistently higher success rates with the criteria within 20 mm. iterative
algorithm is the proposed algorithm with the anchor mechanism. shape is the
method using φ 1 l, . texture radius 25 and texture radius 50 are the methods
using φ 1 l, with ε1 (eq. 3) selected at 25 mm and 50 mm. combined radius 25 and
combined radius 50 are the methods using eq. 8 with one iteration.
the evolution of a skin lesion is an important sign of a potentially cancerous
growth and total body photography is useful to keep track of skin lesions
longitudinally. we proposed a novel framework that leverages geometric and
texture information to effectively find lesion correspondence across tbp scans.
the framework is evaluated on a private dataset and a public dataset with
success rates that are comparable to those of the state-of-the-art method.the
proposed method assumes that the local texture enclosing the lesion and its
surroundings should be similar from scan to scan. this may not hold when the
appearance of the lesion changes dramatically (e.g. if the person acquires a
tattoo). also, the resolution of the mesh affects the precision of the positions
of landmarks and lesions. in addition, the method may not work well with
longitudinal data that has non-isometric deformation due to huge variations in
body shape, inconsistent 3d reconstruction, or a dramatic change in pose and,
therefore, topology, such as an open armpit versus a closed one.in the future,
the method needs to be evaluated on longitudinal data with longer duration and
new lesions absent in the target. in addition, an automatic method to determine
accurate landmarks is desirable. note that although we rely on the manual
selection of landmarks, the framework is still preferable over manually
annotating lesion correspondences when a subject has hundreds of lesions. as the
3d capture of the full body becomes more prevalent with better quality in tbp,
we expect that the proposed method will serve as a valuable step for the
longitudinal tracking of skin lesions.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43990-2_25.
data augmentation (da) is a key factor in the success of deep neural networks
(dnn) as it artificially enlarges the training set to increase their
generalization ability as well as robustness [22]. it plays a crucial role in
medical image analysis [8] where annotated datasets are only available with
limited size. dnns have already successfully supported radiologists in the
interpretation of magnetic resonance images (mri) for prostate cancer (pca)
diagnosis [3]. however, the da scheme received less attention, despite its
potential to leverage the data characteristic and address overfitting as the
root of generalization problems.state-of-the-art approaches still rely on
simplistic spatial transformations, like translation, rotation, cropping, and
scaling by globally augmenting the mri sequences [12,20]. they exclude random
elastic deformations, which can change the lesion outline but might alter the
underlying label and thus produce counterproductive examples for training [22].
however, soft tissue deformations, which are currently missing from the da
schemes, are known to significantly affect the image morphology and therefore
play a critical role in accurate diagnosis [6].both lesion and prostate shape
geometrical appearance influence the clinical assessment of prostate
imaging-reporting and data system (pi-rads) [24]. the prostate constantly
undergoes soft tissue deformation dependent on muscle contractions, respiration,
and more importantly variable filling of the adjacent organs, namely the bladder
and the rectum. among these sources, the rectum has the largest influence on the
prostate and lesion shape variability due to its large motion [4] and the fact
that the majority of the lesions are located in the adjacent peripheral prostate
zone [1]. however, only one snapshot of all these functional states is captured
within each mri examination, and almost never will be exactly the same on any
repeat or subsequent examination. ignoring these deformations in the da scheme
can potentially limit model performance.model-driven transformations attempting
to simulate organ functions -like respiration, urinary excretion,
cardiovascular-and digestion mechanics -offer a high degree of diversity while
also providing realistic transformations. currently, the finite element method
(fem) is the standard for modeling biomechanics [13]. however, their computation
is overly complex [10] and therefore does not scale to on-the-fly da [7]. recent
motion models rely on dnns using either a fem model [15] or complex training
with population-based models [18]. motion models have not been integrated into
any deep learning framework as an online data augmentation yet, thereby leaving
the high potential of inducing applicationspecific knowledge into the training
procedure unexploited.in this work we propose an anatomy-informed spatial
augmentation, which leverages information from adjacent organs to mimic typical
deformations of the prostate. due to its lightweight computational requirements,
it can be easily integrated into common da frameworks. this technique allows us
to simulate different physiological states during the training and enrich our
dataset with a wider range of organ and lesion shapes. inducing this kind of
soft tissue deformation ultimately led to improved model performance in
patient-and lesion-level pca detection on an independent test set.
model-driven spatial transformations simulate realistic soft-tissue
deformations, which are part of the physiology, and can highly affect the shape
of the prostate as well as the lesions in it. as the computation of
state-of-the-art fem models does not scale to on-the-fly da, we introduce
simplifications to be able to integrate such a biomechanical model as an online
da into the model training:-soft tissue deformation of the prostate is mostly
the result of morphological changes in the bladder and rectal space [4,6], -due
to the isotropic mechanical behavior of the rectum and the bladder [19],we apply
isotropic deformation to them, -we assume similar elastic modulus between the
prostate and surrounding muscles [16], allowing us to approximate these tissue
classes as homogeneous, -we introduce a non-linear component into the model by
transforming the surrounding tissue proportionally to the distance from the
rectum and bladder in order to generate realistic deformations [23].based on
them, we define the vector field v for the transformation as the gradient of the
convolution between the gaussian kernel g σ and the indicator function s organ ,
multiplied by a scalar c to control deformation amplitude and direction:the
resulting v serves as the deformation field for an mri sequence i(x, y, z):it
allows us to simulate the distension or evacuation of the bladder or rectal
space. we refer to this transformation as anatomy-informed deformation. we make
it publicly available in batchgenerators [9] and integrate it into a nnu-net
trainer https://github.com/mic-dkfz/anatomy_informed_da.
we evaluate our anatomy-informed da qualitatively as well as
quantitatively.first, we visually inspect whether our assumptions in sect. 2.1
regarding pelvic biomechanics resulted in realistic transformations. we apply
either our proposed transformation to the rectum or the bladder, random
deformable or no transformation in randomly selected exams and conduct a strict
turing test with clinicians having different levels of radiology expertise (a
freshly graduated clinician (c.e.) and resident radiologists (c.m., k.s.z.), 1.5
-3 years of experience in prostate mri) to determine if they can notice the
artificial deformation.finally, we quantify the effect of our proposed
transformation on the clinical task of patient-level pca diagnosis and
lesion-level pca detection. we derive the diagnosis through semantic
segmentation of the malignant lesions following previous studies
[5,11,12,20,21]. semantic segmentation provides interpretable predictions that
are sensitive to spatial transformations, making it appropriate for testing
spatial das. to compare the performance of the trained models to radiologists,
we calculate their performance using the clinical pi-rads scores and
histopathological ground truths. to consider clinically informative results, we
use the partial area under the receiver operating characteristic (pauroc) for
patient-level evaluation with the sensitivity threshold of 78.75%, which is 90%
of the sensitivity of radiologists for pi-rads ≥ 4. additionally, we calculate
the f 1 -score at the sensitivity of pi-rads ≥ 4. afterward, we evaluate model
performances on object-level using the free-response receiver operating
characteristic (froc) and the number of detections at the radiologists' lesion
level performance for pi-rads ≥ 4, at 0.32 average number of false positives per
scan. objects were derived by applying a threshold of 0.5 to the softmax outputs
followed by connected component analysis to identify connected regions in the
segmentation maps. predictions with an intersection over union of 0.1 with a
ground truth object were considered true positives. to systematically compare
the effect of our proposed anatomy-informed da with the commonly used settings,
we create three main da schemes:1. basic da setting of nnu-net [8], which is an
extensive augmentation pipeline containing simple spatial transformations,
namely translation, rotation and scaling. this setting is our reference da
scheme. 2. random deformable transformations as implemented in the nnu-net [8]
da pipeline extending the basic da scheme (1) to test its presence in the
medical domain. our hypothesis is that it will produce counterproductive
examples, resulting in inferior performance compared to our proposed da.3.
proposed anatomy-informed transformation in addition to the simple da scheme
(1). we define two variants of it: (a) deforming only the rectum, as rectal
distension has the highest influence among the organs on the shapes of the
prostate lesions [4]. (b) deforming the bladder in addition to the rectum, as
bladder deformations also have an influence on lesions, although smaller.
774 consecutive bi-parametric prostate mri examinations are included in this
study, which were acquired in-house during the clinical routine. the ethics
committee of the medical faculty heidelberg approved the study (s-164/2019) and
waived informed consent to enable analysis of a consecutive cohort. all
experiments were performed in accordance with the declaration of helsinki [2]
and relevant data privacy regulations. for every exam, pi-rads v2 [24]
interpretation was performed by a board-certified radiologist. every patient
underwent extended systematic and targeted mri trans-rectal ultrasound-fusion
transperineal biopsy. malignancy of the segmented lesions was determined from a
systematic-enhanced lesion ground-truth histopathological assessment, which has
demonstrated reliable ground-truth assessment with sensitivity comparable to
radical prostatectomy [17]. the samples were evaluated according to the
international society of urological pathology (isup) standards under the
supervision of a dedicated uropathologist. clinically significant prostate
cancer (cspca) was defined as isup grade 2 or higher. based on the biopsy
results, every cspca lesion was segmented on the t2-weighted sequences
retrospectively by multiple in-house investigators under the supervision of a
board-certified radiologist. in addition to the lesions, the rectum and the
bladder segmentations were automatically predicted by a model built upon nnu-net
[8] trained iteratively on an in-house cohort initially containing a small
portion of our cohort. multiple radiologists confirmed the quality of the
predicted segmentations.
774 exams were split into 80% training set (619 exams) and 20% test set (155
exams) by stratifying them based on the prevalence of cspca (36.3%). the mri
sequences were registered using b-spline transformation based on mutual
information to match the ground-truth segmentations across all modalities
[12,14].as the limited number of exams with cspca and the small lesion size
compared to the whole image can cause instability during training, we adapted
the cropping strategy from [21] by keeping the organ segmentations to use the
anatomy-informed da (offsets of ±9 mm axial to the prostate and ±11.25 mm in the
axial plane to the rectum and the bladder). the images are preprocessed by the
automated algorithm of nnu-net [8]. we trained 3d nnu-net models in 5-fold
cross-validation with different spatial da schemes, see sect. 2.2. the
hyperparameter c of the anatomy-informed da was optimized using validation
results, sampled during training with uniform distribution constrained by
amplitude values in positive and negative directions of c = {300, 600, 900,
1200, 1500}. c rectum = 1200 and c bladder = 600 were selected for the final
models. compared to the standard nnu-net settings, we implemented balanced
sampling regarding the prevalence of cspca and reduced the number of epochs to
350 to avoid overfitting. we used mish activation function, ranger optimizer,
cosine anneal learning rate scheduler, and initial learning rate of 0.001
following [12]. the final models are ensembled and evaluated on the independent
test set using bootstrapping with 1000 replications to provide standard
deviation and to calculate p-values for the f 1 -score and for the number of
detected lesions using two-sided t-test to determine statistical significance.
the anatomy-informed transformation produced highly realistic soft tissue
deformations. figure 2 shows an example of the transformation simulating rectum
distensions with prostate lesions at different distances from the rectum. 92% of
the rectum and 93% of the bladder deformation from the randomly picked exams
became so realistic that our freshly graduated clinician did not detect them,
but our residents noticed 87.5% of the rectum and 25% of the bladder
deformations based on small transformation artifacts and their expert intuition.
irregularities resulted from the random elastic deformations can be easily
detected, in contrast to our method being challenging to detect its artificial
nature. in table 1 we summarize the patient-level pauroc and f 1 -scores; and
lesion-level froc results on the independent test set showing the advantage of
using anatomy-informed da. to further highlight the practical advantage of the
proposed augmentation, we compare the performance of the trained models to the
radiologists' diagnostic performance for pi-rads ≥ 4, which locate the most
informative performance point clinically on the roc diagram, see fig. 3.
extending the basic da scheme with the proposed anatomy-informed deformation not
only increased the sensitivity closely matching the radiologists' patient-level
diagnostic performance but also improved the detection of pca on a lesion level.
interestingly, while the use of random deformable transformation also improved
lesion-level performance, it did not approach the diagnostic performance of the
radiologists, unlike the anatomy-informed da.at the selected patient-and
object-level working points, the model with the proposed rectum-and
bladder-informed da scheme reached the best results with significant
improvements (p < 0.05) compared to the model with the basic da setting by
increasing the f 1 -score with 5.11% and identifying 4 more lesions (5.3%) from
the 76 lesions in our test set.the time overhead introduced by anatomy-informed
augmentation caused no increase in the training time, the gpu remained the main
bottleneck.
this paper addresses the utilization of anatomy-informed spatial transformations
in the training procedure to increase lesion, prostate, and adjacent organ shape
variability for the task of pca diagnosis. for this purpose, a lightweight
mathematical model is built for simulating organ-specific soft tissue
deformations. the model is integrated into a well-known da framework and used in
model training for enhanced pca detection.towards radiologists' performance.
inducing lesion shape variability via anatomy-informed augmentation to the
training process improved the lesion detection performance and increased the
sensitivity value towards radiologistlevel performance in pca diagnosis in
contrast to the training with the basic da setting. these soft tissue
deformations are part of physiology, but only one snapshot is captured from the
many possible functional states within each individual mr examination. our
proposed da simulates examples of physiologic anatomical changes that may have
occurred in each of the mri training examples at the same exam time points,
thereby aiding the generalization ability as well as the robustness of the
network. we got additional, but slight improvements by extending the da scheme
with bladder distensions. a possible explanation for this result is that less
than 30% of the lesions are located close to the bladder, and our dataset did
not contain enough training examples for more improvements.realistic modeling of
organ deformation. our proposed anatomyinformed transformation was designed to
mimic real-world deformations in order to preserve essential image features.
most of the transformed sequences successfully passed the turing test against a
freshly graduated clinician with prostate mri expertise, and some were even able
to pass against radiology residents with more expertise. to support the
importance of realism in da quantitatively, we compared the performance of the
basic and our anatomy-informed da scheme with that of the random deformable
transformation. the random deformable da scheme generated high lesion shape
variability, but it resulted in lower performance values. this could be due to
the fact that it can also cause implausible or even harmful image warping,
distorting important features, and producing counterproductive training
examples. in comparison, our proposed anatomy-informed da outperformed the basic
and random deformable da, demonstrating the significance of realistic
transformations for achieving superior model performance.high applicability with
limitations. the easy integration into da frameworks and no increase in the
training time make our proposed anatomy-informed da highly applicable. its
limitation is the need for additional organ segmentations, which requires
additional effort from the annotator. however, pre-trained networks for
segmenting anatomical structures like nnu-net [8] have been introduced recently,
which can help to overcome this limitation. additionally, our transformation
computation allows certain errors in the organ segmentations compared to
applications where fully accurate segmentations are needed. the success of
anatomy-informed da opens the research question of whether it enhances
performance across diverse datasets and model backbones.
in this work, we presented a realistic anatomy-informed augmentation, which
mimics typical organ deformations in the pelvis. inducing realistic soft-tissue
deformations in the model training via this kind of organ-dependent
transformation increased the diagnostic accuracy for pca, closely approaching
radiologistlevel performance. due to its simple and fast calculation, it can be
easily integrated into da frameworks and can be applied to any organ with
similar distension properties. due to these advantages, the shown improvements
in the downstream task strongly motivate to utilize this model as a blueprint
for other applications.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43990-2_50.
achieving complete tumor resection in surgical oncology like breast conserving
surgery (bcs) is challenging as boundaries of tumors are not always visible/
palpable [10]. in bcs the surgeon removes breast cancer while attempting to
preserve as much healthy tissue as possible to prevent permanent deformation and
to enhance cosmesis. the current standard of care for evaluating surgical
success is to investigate the resection margins, which refers to the area
surrounding the excised tumor. up to 30% of surgeries result in incomplete tumor
resection and require a revision operation [10]. the intelligent knife (iknife)
is a mass spectrometry device that can address this challenge by analyzing the
biochemical signatures of resected tissue using the smoke that is released
during tissue incineration [3]. each spectrum contains the distribution of
sampled ions with respect to their mass to charge ratio (m/z). previously,
learning models have been used in combination with iknife data for ex-vivo
tissue characterization and real-time margin detection [16,17].the success of
clinical deployment of learning models heavily relies on approaches that are not
only accurate but also interpretable. therefore, it should be clear how models
reach their decisions and the confidence they have in such decision. studies
suggest that one way to improve these factors is through data centric approaches
i.e. to focus on appropriate representation of data. specifically,
representation of data as graphs has been shown to be effective for medical
diagnosis and analysis [1]. it has also been shown that graph neural networks
can accurately capture the biochemical signatures of iknife and determine the
tissue type. particularly, graph transformer networks (gtn) has have shown to
further enhance the transparency of underlying relation between the graph nodes
and decision making via attention mechanism [11].biological data, specially
those acquired intra-opertively, are heterogeneous by nature. while the use of
ex-vivo data collected under specific protocols are beneficial to develop
baseline models, intra-operative deployment of these models is challenging. for
iknife, the ex-vivo data is usually collected from homogeneous regions of
resected specimens under the guidance of a trained pathologist, versus the
intra-operative data is recorded continuously while the surgeon cutting through
tissues with different heterogeneity and pathology. therefore, beyond predictive
power and explainable decision making, intra-operative models must be able to
handle mixed and unseen pathology labels.uncertainty-aware models in
computer-assisted interventions can provide clinicians with feedback on
prediction confidence to increase their reliability during deployment. deep
ensembles [15] and bayesian networks [9] incur high runtime and computational
cost both at training and inference time and thus, less practical for real-time
computer-assisted interventions. evidential deep learning [18] is another
approach that has been proposed based on the evidence framework of
dempster-shafer theory [12]. since the evidential approach jointly generates the
network prediction and uncertainty estimation, it seems more suitable for
computationally efficient intra-operative deployment. in this paper, we propose
evidential graph transformer (egt), a combination of graph-based feature-level
attention mechanism with sample-level uncertainty estimation, to increase the
performance and interpretability of surgical margin assessment. this is done by
implementing the evidential loss and prediction functions within a graph
transformer model to output the uncertainty, intermediate attention, and model
prediction. to demonstrate the state-of-theart performance of the proposed
approach on mass spectrometry data, the model is compared with different
baselines in both cross-validation and prospective schemes on ex-vivo data.
furthermore, the performance of model is also investigated intraoperatively. in
addition to the proposed model, we present a new visualization approach to
better correlate the graph nodes with the spectral content of the data, which
improves interpretability. in addition to the ablation study on the network and
graph strictures, we also investigate the metabolic association of breast cancer
hormone receptor status.
figure 1 presents the overview of the proposed approach. following data
collection and curation, each burn (spectrum) is converted to a single graph
structure. the proposed graph model learns from the biochemical signatures of
the tissue to classify cancer versus normal tissue. the uncertainty and
intermediate attentions generated by the model are visualized and explored for
their association with the biochemical mechanisms of cancer.
ex-vivo: data is collected from fresh breast tissue samples from the patients
referred to bcs at kingston health sciences center over two years. the study is
approved by the institutional research ethics board and patients consent to be
included. peri-operatively, a pathologist guides and annotates the ex-vivo
pointburns, referred to as spectra, from normal or cancerous breast tissue
immediately after excision. in addition to spectral data, clinicopathological
details such as the status of hormone receptors is also provided
post-surgically. in total 51 cancer and 149 normal spectra are collected and
stratified into five folds (4 for cross validation and 1 prospectively) with
each patient restricted to one fold only.
a stream of iknife data is collected during a bcs case (27 min) at kingston
health sciences center. at the sampling rate of 1 hz, a total of 1616 spectra
are recorded. each spectrum is then labeled based both on surgeons comments
during the operation and post-operative pathology report.preprocessing: each
spectrum is converted to a hierarchical graph as illustrated in fig. 2. the
nodes are generated from a specific subband in each spectrum. different subband
widths (50, 100, 300, and 900 m/z) are used to create different levels of
hierarchy (fig. 2). the edges connect nodes with overlapping subbands within and
between levels. as a result, each graph (spectrum) consists of 58 nodes and 135
edges. for details on graph conversion please refer to [2] and [11]. for easier
interpretation of nodes with respect to their corresponding subbands, we
visualize the graph as a piano-key plot in fig. 2, where each key represents a
node with m/z range equal to the angular extent of the key. the dark keys show
the subband overlaps between adjacent nodes.
the gtn consists of a node embedding layer, l graph transformer layers (gtl), a
node aggregation layer, multiple dense layers, and a prediction layer [8].
assume a graph g with n nodes and h i ∈ r d×1 as node features of node i. in
each gtl, the h headed attention mechanism updates the features of node i based
on all neighboring node features h j that are directly connected to node i via e
ij edges. the attention mechanism for node update at layer l + 1 is formulated
as:where q k,l , k k,l , and v k,l are trainable linear weights. the weights w
kl ij defines the k-th attention that is paid by node j to update node i at
layer l. the concatenation of all h attention heads multiplied by trainable
parameters o l generates final attention ĥl+1 i , which is passed through batch
normalization and residual layers to update the node features for the next
layer. after the last gtl, features from all nodes are aggregated, then passed
to the dense layers to construct a final prediction output.
evidential deep learning provides a welldefined theoretical framework to jointly
quantify classification prediction and uncertainty modeling by assuming the
class probability follows a dirichlet distribution [18]. we propose to modify
the loss and prediction layer of gtn, considering the same assumption, to
formulate the evidential graph transformer model. therefore, there are two
mechanisms embedded in egt: i) node-level attention calculation -via aggregation
of neighboring nodes according to their relevance to the predictions, and ii)
graph-level uncertainty estimation -via fitting the dirichlet distribution to
the predictions.in the context of surgical margin assessment, the attentions
reveal the relevant metabolic ranges to cancerous tissue, while uncertainty
helps identify and filter data with unseen pathology. specifically, the
attentions affect the predictions by selectively emphasizing the contributions
of relevant nodes, enabling the model to make more accurate predictions. on the
other hand, the spread of the outcome probabilities as modeled by the dirichlet
distribution represents the confidence in the final predictions. combining the
two provides interpretable predictions along with the uncertainty
estimation.mathematically, the dirichlet distribution is characterized by α = [α
1 , ..., α c ] where c is the number of classes in the classification task. the
parameters can be estimates as α = f (x i |θ) + 1 where f (x i |θ) is the output
of the evidential graph transformer parameterized by θ for each sample(x i ).
then, the expected probability for the c-th class p c and the total uncertainty
u for each sample (x i ) can be calculated as p c = αc s , and u = c s ,
respectively, where s = c c=1 α c . to fit the dirichlet distribution to the
output layer of our network, we use a loss function consisting of the prediction
error l p i and the evidence adjustmentwhere λ is the annealing coefficient to
balance the two terms. l p i can be crossentropy, negative log-likelihood, or
mean square error , while l e i (θ) is kl divergence to the uniform dirichlet
distribution [18].
we explore the hyper-parameters of the proposed model in an extensive ablation
study. the attention parameters include the number of attention heads ( 1-15
with step size of 2) and the number of hidden features
(7)(8)(9)(10)(11)(12)(13)(14). for the evidential loss, we evaluate the choice
of loss function (the 3 previously mentioned), and the annealing coefficient
(5-50 with step size of 5). the number of gtls and dense layers are both fixed
at 3. additionally, we run ablation studies on the graph structure themselves to
show the importance of presenting the data as graphs. we try randomizing the
edge connections and dropping the nodes with overlapping m/z subbands.
the performance of the proposed network is compared with 3 baseline models
including gtn, graph convolution network [14], and non-graph convolution
network. four-fold cross validation is used for comparison of the different
approaches, to increase the generalizability (3 folds for train/validation, test
on remaining unseen fold, report average test performance). separate ablation
studies are performed for the baseline models to fine tune their structural
parameters. all experiments are implemented using pytorch with adam optimizer,
learning rate of 10 -4 , batch size of 32, and early stopping based on
validation loss. to demonstrate the robustness of the model and ensure it is not
overfitting, we also report the performance of the ensemble model from the
4-fold cross validation study on the 5th unseen prospective test fold.clinical
relevance: hormone receptor status plays an important role in determining breast
cancer prognosis and tailoring treatment plans for patients [6]. here, we
explore the correlation of the attention maps generated by egt with the status
of her2 and pr hormones associated with each spectrum. these hormones are
involved in different types of signaling that the cell depends on [5].
to explore the intra-operative capability of the models, we deploy the ensemble
models of the proposed method as well as the baselines from the cross-validation
study to the bcs iknife stream.
ablation study and ex-vivo evaluation: according to our ablation study, hyper
parameters of 11 attention heads, 11 hidden features per attention head, the
cross entropy loss function, and annealing coefficient of 30, result in higher
performances when compared to other configurations (370k learnable parameters).
the performance of egt in comparison with the mentioned baselines are summarized
in table 1. as can be seen, the proposed egt model with average accuracy of
94.1% outperformed all the baselines statistically significantly (maximum
p-values of 0.02 in one-tail paired wilcoxon signed-rank test). the lower
standard deviation of parameters shows the robustness of egt compared to other
baselines. the regularization term in egt loss prevents overconfident estimation
of incorrect predictions [18] that could lead to superior results, compared to
gtn, without overfitting. lastly, when compared to other state-ofthe-art
baselines with uncertainty estimation mechanisms, the proposed evidential graph
transformer network (average balanced accuracy of 91.6 ± 4.3% in table 1)
outperforms mc dropout [9], deep ensembles [15], and masksembles [7] (86.1 ±
5.7%, 88.5 ± 6.8%, and 89.2 ± 5.4% respectively [19]).the estimated
probabilities in evidence based models are directly correlated with model
confidence and therefore more interpretable. to demonstrate this, table 1.
average(standard deviation) of accuracy (acc), balanced accuracy (bac)
sensitivity (sen), specificity (spc), and the area under the curve (auc) for the
proposed evidential graph transformer in comparison with graph transformer
(gtn), graph convolution (gcn), and non-graph convolution (cnn) baselines. the
probability of cancer predictions and uncertainty scores for all test samples
are visualized in the left plot of fig. 3. as seen, the higher the uncertainty
score (bottom bar plot), the closer the estimated cancer probability is to 0.5
(top bar plot). this information can be provided during deployment to further
augment surgical decision making for uncertain data instances. this is
demonstrated in the right plot of fig. 3, where the samples with high
uncertainties are gradually disregarded. it can be seen that by not using the
network prediction for up to 10% of most uncertain test data, the auc increases
to 1. providing surgeons with not only the model decision but also a measure of
model confidence will improve their intervention decisions. for example, if the
model has low confidence in a prediction they can reinforce their decision by
other means. the result of our graph structure ablation shows the drop of
average acc to 85.6% by randomizing the edges in the graph (p-value 0.004).
dropping overlapping nodes further decreased the acc to 82.3% (p-value 0.001).
although the model still trained due to node aggregation, random graph structure
acts as noise and affects the performance. multi-level graphs were shown to
outperform other structures for masspect data [akbarifar 2021] as they preserve
the receptive field in the neighborhood of subbands (metabolites). clinical
relevance: an appropriate visualization of the attention map for samples can be
used to help with this exploration. accumulating the attentions maps from the
cancerous burns based on their hormone receptor status results in the
representative maps demonstrated in fig. 4. the polar bars in this figure show
the attention level paid to the nodes in the associated m/z subband. it can be
seen that more attention is paid to the amino acids range (100-350 m/z) in her2
positive breast cancer in comparison to her2 negative breast cancer, which is in
accordance with previous literature that has found evidence for higher glutamine
metabolism activity in her2+ [13]. we have also found that there's more
attention in this range for pr negative breast cancer in comparison pr positive,
which is in concordance with previous literature demonstrating that these
subtypes have higher glutamine metabolic activity [4,5].
the raw intra-operative iknife data (y-axis is m/z spectral range and x-axis is
the surgery timeline) along with the temporal reference labels extracted from
surgeon's call-outs and pathology report are shown in fig. 5, top. as seen, the
iknife stream contains spectra from skin cuts, which is considered as an unseen
label for the ex-vivo models. the results of deploying the proposed models and
baselines are presented in fig. 5, bottom. when a spectrum is classified as
cancer, a red line is overlaid on the timeline. previous studies showed the
similarity between skin and breast cancer mass spectrum that can confuse the
binary models. since our proposed egt is equipped with uncertainty estimation,
this information can be used to eliminate skin spectra from being wrongly
detected as cancer. by integrating uncertainty, predictions for such burns are
flagged as uncertain so clinicians can compensate for surgical decision making
with other sources of information.
intra-operative deployment of deep learning solutions requires a measure of
interpretability as well as predictive confidence. these two factors are
particularly importance to deal with heterogeneity of tissues which represented
as mixed or unseen labels for the retrospective models. in this paper, we
propose an evidential graph transformer for margin detection in breast cancer
surgery using mass spectrometry with these benefits in mind. this structure
combines the attention mechanisms of graph transformer with predictive
uncertainty. we demonstrate the significance of this model in different
experiments. it has been shown that the proposed architecture can provide
additional insight and consequently clearer interpretation of surgical margin
characterization and clinical features like status of hormone receptors. in the
future, we plan to work on other uncertainty estimation approaches and further
investigate the graph conversion technique to be more targeted on the metabolic
pathways, rather than regular conversion.
atrial fibrillation (af) is a cardiac disease characterized by rapid, irregular
heartbeats [4]. the disease can lead to stroke and heart failure, and has a
mortal-ity rate of almost 20% [5,10,13]. af is classified as either persistent
atrial fibrillation (peaf), where abnormal heart rhythms occur continuously for
more than seven days, or paroxysmal atrial fibrillation (paaf), where the heart
rhythm returns to normal within seven days. although af can be treated through a
procedure called catheter ablation, peaf cases have high recurrence rates and
often require re-intervention [8]. accurate knowledge of the disease type is
therefore highly valuable for treatment planning and has high prognostic value
[22].clinical studies have discovered a strong relationship between af and
epicardial adipose tissue (eat), a fat depot layer on the surface of the
myocardium that can cause inflammation and disrupt cardiac function [3,15].
recent works have shown that automatic classification of af sub-types can be
done using ct volumes of the left atrium and surrounding eat, which can be used
to screen for patients with high risk of peaf. huber et al. [7] showed that eat
volume, approximated from left-atrium ct images, can be used as a predictor for
af recurrence. yang et al. [22] trained a random forest model to classify af
subtype based on radiomic features and volume measurements, achieving 85.3% auc.
although these methods demonstrate the usefulness of radiomic features for af
sub-type classification, such features are generic and not specific to the task,
which can limit model performance [12]. radiomic features also rely on summary
statistics such as entropy or homogeneity to obtain global descriptors, and
these have limited effectiveness when capturing local feature variations
[16].deep learning has achieved outstanding results on medical imaging analysis
tasks, largely due to its ability to learn task-specific features and complex
relations between them [17]. naïvely using deep neural networks (dnns) to
predict af sub-types from ct volumes yields poor results however due to
over-fitting on high-dimensional volume inputs (see results for dnn in table 1).
existing works have attempted to combine deep and radiomic features through
methods such as direct concatenation [2,19], attention modules [14], or
contrastive learning between feature types [24]. although these methods propose
different ways of using both approaches, they do not explicitly address the
limitations of either approach or explore ways to combine their complementary
advantages.in this work, we propose a novel approach to atrial fibrillation
sub-type classification from ct volumes by integrating radiomic and deep
learning methods. we note that textural radiomic features identified by feature
selection methods can serve as an information prior to supplement low-level
features from dnns, since they are designed to capture low-level context and
have predictive power [23]. to this end, we locally calculate radiomic features
based on patches surrounding each voxel, and perform feature fusion with
low-level dnn features. this provides the dnn with pre-defined features known to
be relevant to the task to reduce over-fitting, and also allows spatial
relations between radiomic features to be learned. furthermore, we encourage the
dnn to learn features complementary to radiomic features to obtain more
comprehensive signals and design a novel feature de-correlation loss. the
overall framework, which we term radiomics-informed deep learning (ridl), is
illustrated in fig. 1. unlike existing works, our method is designed to directly
addresses the limitations of both deep learning and radiomic approaches and
achieves state-of-the-art performance on af sub-type classification. to
summarize our key contributions: -we propose a novel radiomics-informed deep
learning (ridl) method for af sub-type classification from ct volumes, which
achieves state-of-the-art results and can be used to screen for patients with
high risk of peaf. -our method uses a novel approach of fusing locally computed
radiomic features with low-level dnn features to improve capturing of local
context. -furthermore, we enforce feature de-correlation using a novel
feature-bank design to ensure complementary deep and radiomic features are
extracted.
we combine radiomic and deep learning approaches using two novel components: 1)
feature fusion of local radiomic features and low-level dnn features to improve
local context, 2) encouraging complementary deep and radiomic features through
feature de-correlation. these are illustrated in fig. 2 and explained in detail
below. our datasetincludes n samples of input x i and binary label y i , where 0
indicates paaf and 1 indicates peaf. x i has two channels, one consisting of the
3d ct volume centered around the left atrium and the other the binary
region-of-interest (roi) mask indicating eat. the roi is obtained through
hounsfield value thresholding such that all voxels valued between -250 and 0 are
identified as eat [7,22].
under the radiomics pipeline, a large set of features, typically more than a
thousand, is first extracted by performing calculations over the volume and roi
input x i . feature selection methodologies such as mutual information (mi),
principal component analysis (pca), or lasso regularization, are then used to
identify predictive features for classification [23]. radiomic features are
classified into shape, first-order statistical features, and texture features.
texture features are designed to capture local variations and use measures such
as gray-level co-occurrence matrices (glcm) to reflect second-order textural
distributions. conventional statistics such as entropy and correlation are then
used to summarize these measures [25], but these tend to be limited in their
ability to capture local heterogeneity, such as the varying textures on the
surface of a cancer tumor. although dnn's are more effective at capturing local
variations, they can overfit without sufficient data for training [17]. unlike
existing works that naïvely concatenate radiomic and deep features before the
classification layer [2,19], we observe that textural features selected through
radiomics feature selection algorithms are known to be predictive and can be
used as prior knowledge to improve low-level dnn features. given radiomic
feature extractor f r , the global radiomic feature, r g i ∈ r, for input x i is
represented by:our method applies feature calculations locally to cubic patches
centered around each voxel, such that features are obtained on a voxel basis and
reflect the statistics of the neighbouring region. for a cubic patch with radius
p and input x i , the local feature at location (h, w, d), denoted by r p
i,(h,w,d) , is obtained by performing r on the cubic patch in x i centered
around (h, w, d):where the input of f r is the cubic sub-volume. this process is
illustrated in fig. 2a. local features can be calculated for multiple texture
features and patch size p, which are then concatenated to obtain r l i ∈ r l×h×w
×d , where l is the total number of features used and h, w , and d are original
input dimensions. we note that only texture radiomic features are used for local
calculation since they are specifically intended to capture local context. r l i
is then concatenated with low-level dnn features, z i ∈ r c×h×w ×d , to
supplement the dnn with local radiomic features. to effectively fuse the
features, we apply a channel attention module, a, following the design in
[20]:where z i is the fused feature, ⊕ is channel concatenation, and ⊗ is
element-wise multiplication. the learned attention tensor a(r l i ⊕ z i ) has
dimensions (c + l) × 1 × 1 × 1 and is broadcasted along the volume dimension,
such that attention is applied channel-wise and spatial feature distributions
are preserved.
global radiomic features are also included in our model by concatenation with
high-level dnn features before the classification layer. unlike existing
approaches however, we encourage our dnn to learn features complementary to
radiomic features by enforcing de-correlation between the two. this ensures that
different variations are captured, which provides a more comprehensive signal to
the classification layer. accurate approximation of correlation requires large
batches sizes however, which requires large gpu memory and can affect model
convergence [9]. we instead propose a novel feature-bank implementation with
exponential weighting to estimate sample statistics. every iteration, we save
dnn and global radiomic features in feature-bank k, which holds up to n k
features in a first-in first-out queue. after a warm-up period, we calculate the
sample correlation using an exponential weighting scheme. given weight parameter
w < 1, and the normalized deep feature z i and radiomic feature r i from k, we
calculate feature de-correlation loss l corr as:the first b samples, where b is
the batch size, belong to the training sample of the current iteration, and
their losses are back-propagated to encourage deep features to have zero
correlation with radiomic features. this process is illustrated in fig. 2b.
although feature banks have been used in techniques such as contrastive learning
to address batch size limitations [6,21], we are the first to formulate this
technique for feature de-correlation.
the dnn model uses raw ct volumes concatenated with roi masks as input.global
and local radiomic features are pre-computed for input into the feature layer.
binary cross-entropy is used for af sub-type classification loss l cls :where ŷi
is the model prediction for sample y i . the model is trained together with
feature de-correlation loss l corr and its loss weighting, w corr . to provide
further regularization and prevent over-fitting, we perform an additional
selfreconstruction task, using loss l rec , which we describe in more detail in
the supplementary materials. the overall loss function is then:3 experiments
dataset. we use a dataset of 172 patients containing 94 paaf and 78 peaf cases
collected from the sun yat-sen memorial hospital in china. ct volumes are
centered on the left atrium and normalized to between -1 and 1. roi masks for
eat are obtained through hounsfield value thresholding between -250 and 0.
volumes are resized to the same aspect ratio to ensure consistent dimensions
across samples. we use an input size of 96 × 128 × 128 voxels and apply zero
padding for smaller volumes. we use five-fold cross-validation and report
average test performance across folds. cross-validation is implemented by
splitting the dataset into five equal subsets and using three subsets for
training, one subset for validation, and one subset for testing. a rolling
scheme is used such that different validation and test subsets are used for each
of the five folds. data acquisition procedures and statistics are given in the
supplementary materials.setup. we use the pyradiomic package [18] to extract
radiomic features from the input volumes and masks. using the cross-validation
splits, we perform feature selection and classification using lasso regularized
logistic regression. lasso regularization consistently selects four radiomic
features as the ones with the most significant predictive power: maximum 3d
diameter, maximum 2d diameter, maximum voxel value, and normalized inverse
difference of glcm (glcm idn). the texture feature glcm idn is calculated
locally for p ∈ {1, 2, 5, 10} to obtain local radiomic features r l i ∈ r
4×96×128×128 . for our dnn network, we use a modified 3d u-net [1] (abbreviated
as m3dunet) with skip connections between the encoder and decoder removed to
enhance bottle-neck feature compression. bottle-neck features are averaged
across spatial dimensions for classification, whilst decoder outputs are used
for self-reconstruction regularization. the model is trained using the adam
optimizer with learning rate 10 -4 for 100 epochs and 0.1 decay at 30 epochs. we
use batch size b = 1, feature bank size n k = 25, and warm-up period of one
epoch. we use w corr = 2 for de-correlation loss weighting, which was chosen
based on the validation splits. mean and standard deviation of ten runs are
reported. additional experiments and details are included in the supplementary
materials.
we compare our method with alternative state-of-the-art approaches based on
radiomics, deep learning, and hybrid techniques. deep and hybrid volume-based
classification methods [11,14,24] are adapted to our task since there are no
existing works for af sub-type classification. we use the same encoder for all
deep architectures for fair comparison, except for methods that are architecture
specific. a naïve feature concatenation method is used as our baseline for the
hybrid approach. radiomic features for the hybrid approach are selected through
lasso regularization as it is the most effective. results are shown in table
1.table 1. comparison with state-of-the-art methods for radiomic, deep learning,
and hybrid approaches. selector * refers to the feature selection method. hybrid
# methods use radiomic features selected by lasso regularization, which is the
most effective. dnn is a naïve implementation using the m3dunet model. baseline
† is a naïve hybrid implementation using simple feature concatenation.
selector we can see that hybrid methods outperform radiomic and deep methods in
general. our method, ridl, achieves the best results across all metrics however
and improves auc by 1.1% over the baseline method (86.9% v.s. 85.8%) and 3.5%
over the best radiomics approach (86.9% v.s. 83.4%).
component analysis. we perform ablation experiments to demonstrate improvements
from using local radiomic features, global radiomic features, and feature
de-correlation loss. results are shown in table 2.we can see that including
local radiomic features, improving auc by up to 1.6% when included with a
standard dnn (78.8% v.s. 77.2%). using feature de-correlation further boosts
performance and leads to the best overall results. effectiveness of radiomic
feature selection. to demonstrate the effectiveness of radiomic feature
selection as prior knowledge for feature fusion, we compare with results from
using features discarded by radiomics feature selection. we randomly select
three discarded features to generate local features r l i as input whilst
keeping other components constant. results are shown in table 3.table 3. results
using different texture radiomic features for input r l i as displayed by their
pyradiomics key [18]. "selected" indicates whether the feature was selected or
discarded by the radiomic feature selection algorithm. we can see that using
discarded features leads to worse performance in general. given the large set of
radiomic features, it is possible some discarded features may outperform
selected features due to differences in global and local computation.
nevertheless, our results indicate that the radiomic feature selection process
serves as an reasonable information prior. our work is the first to propose
fusing locally computed radiomic features with low-level dnn features, and we
leave detailed local feature selection methods to future works.
in this work, we propose a new approach to atrial fibrillation sub-type
classification from ct volumes by integrating radiomic and deep learning
approaches through a radiomics-informed deep learning method, ridl. our method
is based on two key ideas: feature fusion of locally computed radiomic features
with lowlevel dnn features to improve local context, and encouraging
complementary deep and radiomic features through feature de-correlation. unlike
existing hybrid approaches, our method specifically addresses the advantages and
limitations of both techniques to improve feature extraction. we achieve
state-of-the-art results on af sub-type classification and outperform existing
radiomic, deep learning, and hybrid methods.future improvements to ridl can be
made by introducing more sophisticated local radiomic features selection
methods, given the large set features to choose from. experiments on larger
datasets or alternative tasks can also be done to provide more empirical
support, since current results show only slight improvements over baseline.
these issues may be addressed in future works. overall, our method is a novel
way of combining radiomic and deep learning approaches, and can be used to
improve accuracy of peaf screening from ct volumes for better preventive care of
high-risk patients.
*
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43990-2 15.
breast cancer is the most common cancer and the leading cause of cancer death in
women [18]. early detection of breast cancer allows patients to receive timely
treatment, which may have less burden and a higher probability of survival [6].
among current clinical imaging modalities, magnetic resonance imaging (mri) has
the highest sensitivity for breast cancer detection [12]. especially,
contrastenhanced mri (ce-mri) can identify tumors well and has become an
indispensable technique for detecting and defining cancer [13]. however, the use
of gadolinium-based contrast agents (gbca) requires iv-cannulation, which is a
burden to patients, time consuming and cumbersome in a screening situation.
moreover, contrast administration can lead to allergic reactions and finaly
ce-mri may be associated with nephrogenic systemic fibrosis and lead to
bioaccumulation in the brain, posing a potential risk to human health
[4,9,[14][15][16]. in 2017, the european medicines agency concluded its review
of gbca, confirming recommendations to restrict the use of certain linear gbca
used in mri body scans and to suspend the authorization of other contrast
agents, albeit macrocyclic agents can still be freely used [10].with the
development of computer technology, artificial intelligence-based methods have
shown potential in image generation and have received extensive attention. some
studies have shown that some generative models can effectively perform mutual
synthesis between mr, ct, and pet [19]. among them, synthesis of ce-mri is very
important as mentioned above, but few studies have been done by researchers in
this area due to its challenging nature. li et al. analyzed and studied the
feasibility of using t1-weighted mri and t2-weighted mri to synthesize ce-mri
based on deep learning model [11]. their results showed that the model they
developed could potentially synthesize ce-mri and outperform other cohort
models. however, mri source data of too few sequences (only t1 and t2) may not
provide enough valuable informative to effectively synthesize ce-mri. in another
study, chung et al. investigated the feasibility of using deep learning (a
simple u-net structure) to simulate contrast-enhanced breast mri of invasive
breast cancer, using source data including t1-weighted non-fatsuppressed mri,
t1-weighted fat-suppressed mri, t2-weighted fat-suppressed mri, dwi, and
apparent diffusion coefficient [5]. however, obtaining a complete mri sequence
makes the examination costly and time-consuming. on the other hand, the
information provided by multi-sequences may be redundant and may not contain the
relevant information of ce-mri. therefore, it is necessary to focus on the most
promising sequences to synthesize ce-mri.diffusion-weighted imaging (dwi) is
emerging as a key imaging technique to complement breast ce-mri [3]. dwi can
provide information on cell density and tissue microstructure based on the
diffusion of tissue water. studies have shown that dwi could be used to detect
lesions, distinguish malignant from benign breast lesions, predict patient
prognosis, etc [1,3,7,8,17]. in particular, dwi can capture the dynamic
diffusion state of water molecules to estimate the vascular distribution in
tissues, which is closely related to the contrast-enhanced regions in ce-mri.
dwi may be a valuable alternative in breast cancer detection in patients with
contraindications to gbca [3]. inspired by this, we develop a multi-sequence
fusion network based on t1-weighted mri and multi-b-value dwi to synthesize
ce-mri. our contributions are as follows:i from the perspective of method, we
innovatively proposed a multi-sequence fusion model, designed for combining
t1-weighted imaging and multi-b-value dwi to synthesize ce-mri for the first
time. ii we invented hierarchical fusion module, weighted difference module and
multi-sequence attention module to enhance the fusion at different scale, to
control the contribution of different sequence and maximising the usage of the
information within and across sequences. iii from the perspective of clinical
application, our proposed model can be used to synthesize ce-mri, which is
expected to reduce the use of gbca.
this study was approved by institutional review board of our cancer institute
with a waiver of informed consent. we retrospectively collected 765 patients
with breast cancer presenting at our cancer institute from january 2015 to
november 2020, all patients had biopsy-proven breast cancers (all cancers
included in this study were invasive breast cancers, and ductal carcinoma in
situ had been excluded). the mris were acquired with philips ingenia all mris
were resampled to 1 mm isotropic voxels and uniformly sized, resulting in
volumes of 352 × 352 pixel images with 176 slices per mri, and subsequent
registration was performed based on advanced normalization tools (ants) [2].
figure 1 illustrates the structure of the proposed model. first, the
reconstruction module is used to automatically encode and decode each input mri
sequence information to obtain the latent representation of different mri
sequences at multi-scale levels. then, the hierarchical fusion module is used to
extract the hierarchical representation information and fuse them at different
scales. in each convolutional layer group of the reconstruction module, we use
two 3 × 3 filters (same padding) with strides 1 and 2, respectively. the filters
are followed by batch normalization, and after batch normalization, the
activation functions leakyrelu (with a slope of 0.2) and relu are used in the
encoder and decoder, respectively. the l 1 -norm is used as a reconstruction
loss to measure the difference between the reconstructed image and the ground
truth.figure 2 shows the detailed structure of the hierarchical fusion module,
which includes two sub-modules, a weighted difference module and a
multi-sequence attention module. the calculation of the apparent diffusion
coefficient (adc) map is shown in eq. 1, which provides a quantitative measure
of observed diffusion restriction in dwis. inspired by adc, a weighted
difference module is designed, in which the neural network is used to simulate
the dynamic analysis of the ln function, and the element-wise subtraction
algorithm is used to extract the differentiation features between dwis with
different b-values, and finally the features are weighted to obtain weighted
feature maps (f dwi , eq. 2).
where s l and s h represent the image signals obtained from lower b value b l
and higher b h , f θ l and f θ h represent the corresponding neural networks for
dwi with a lower and higher b value.in the multi-sequence attention module, a
channel-based attention mechanism is designed to automatically apply weights (a
s ) to feature maps (f concat ) from different sequences to obtain a refined
feature map (f concat ), as shown in eq. 3. the input feature maps (f concat )
go through the maximum pooling layer and the average pooling layer respectively,
and then are added element-wise after passing through the shared fully connected
neural network, and finally the weight map a s is generated after passing
through the activation function, as shown in eq. 4.where ⊗ represents
element-wise multiplication, ⊕ represents element-wise summation, σ represents
the sigmoid function, θ fc represents the corresponding network parameters of
the shared fully-connected neural network, and avgp ool and m axp ool represent
average pooling and maximum pooling operations, respectively.in the synthesis
process, the generator g tries to generate an image according to the input
multi-sequence mri (d 1 , d 2 , d 3 , d 4 , t 1 ), and the discriminator d tries
to distinguish the generated image g(d 1 , d 2 , d 3 , d 4 , t 1 ) from the real
image y, and at the same time, the generator tries to generate a realistic image
to mislead the discriminator. the generator's objective function is as
follows:and the discriminator's objective function is as follows:where pro data
(d 1 , d 2 , d 3 , d 4 , t 1 ) represents the empirical joint distribution of
inputs d 1 (dw i b0 ), d 2 (dw i b150 ), d 3 (dw i b800 ), d 4 (dw i b1500 ) and
t 1 (t1weighted mri), λ 1 is a non-negative trade-off parameter, and l 1 -norm
is used to measure the difference between the generated image and the
corresponding ground truth. the architecture of the discriminator includes five
convolutional layers, and in each convolutional layer, 3 × 3 filters with stride
2 are used. each filter is followed by batch normalization, and after batch
normalization, the activation function leakyrelu (with a slope of 0.2) is used.
the numbers of filters are 32, 64, 128, 256 and 512, respectively.
the t1-weighted images and the contrast-enhanced images were subtracted to
obtain a difference mri to clearly reveal the enhanced regions in the ce-mri. if
the ce-mri was successfully synthesized, the enhanced region would be
highlighted in the difference mri, otherwise it would not.
based on the ratio of 8:2, the training set and independent test set of the
in-house dataset have 612 and 153 cases, respectively. the trade-off parameter λ
1 was set to 100 during training, and the trade-off parameter of the
reconstruction loss in the reconstruction module is set to 5. masks for all
breasts were used (weighted by a factor of 100 during the calculation of the
loss between generated and real ce-mri) to reduce the influence of signals in
the thoracic area. the batch was set to 8 for 100 epochs, the initial learning
rate was 1e-3 with a decay factor of 0.8 every 5 epochs (total run time is about
60 h). adam optimizer was applied to update the model parameters. mmgsn-net [11]
and the method of chung et al. [5] were used as cohort models, and all models
were trained on nvidia rtx a6000 48 gb gpu.
results analysis was performed by python 3.7. structural similarity index
measurement (ssim), peak signal-to-noise ratio (psnr) and normalized mean
squared error (nmse) were used as metrics, all formulas as follows:p snr = 10
log 10 max 2 (y(x), g(x)) where g(x) represents a generated image, y(x)
represents a ground-truth image, μ y(x) and μ g(x) represent the mean of y(x)
and g(x), respectively, σ y(x) and σ g(x) represent the variance of y(x) and
g(x), respectively, σ y(x)g(x) represents the covariance of y(x) and g(x), and c
1 and c 2 represent positive constants used to avoid null denominators.
first, we compare the performance of different existing methods on synthetic
ce-mri using our source data, the quantitative indicators used include psnr,
ssim and nmse. as shown in table 1, the ssim of mmgsn-net [11] and the method of
chung et al. [5] in synthesizing cet1 mri is 86.61 ± 2.52 and 87.58 ± 2.68,
respectively, the psnr is 26.39 ± 1.38 and 27.80 ± 1.56, respectively, and the
nmse is 0.0982 ± 0.038 and 0.0692 ± 0.035, respectively. in contrast, our
proposed multi-sequence fusion model achieves better ssim of 89.93 ± 2.91,
better psnr of 28.92 ± 1.63 and better nmse of 0.0585 ± 0.026 in synthesizing
cet1 mri, outperforming existing cohort models.mmgsn-net [11] combined
t1-weighted and t2-weighted mri in their work to synthesize ce-mri. here we
combined t1-weighted mri and dwi with a bvalue of 0 according to their method,
but the model did not perform well. it may be because their model can only
combine bi-modality and cannot integrate the features of all sequences, so it
cannot mine the difference features between multiple b-values, which limits the
performance of the model. in addition, although the method of chung et al. [5]
used full-sequence mri to synthesize ce-mri, it would be advantageous to obtain
synthetic ce-mri images using as little data as possible, taking advantage of
the most contributing sequences. they did not take advantage of multi-b-value
dwi, nor did they use the hierarchical fusion module to fully fuse the
hierarchical features of multi-sequence mri. the visualization results of random
samples are shown in fig. 3. it can be seen from the visualization results that
after the difference between the generated ce-mri and the original t1-weighted
mri, the lesion position of the breast is highlighted, the red circle represents
the highlighted area, which proves that our method can effectively synthesize
contrast-enhanced images, highlighting the same parts as the real enhanced
position. see supplementary material for more visualization results, including
visualizations of breast ce-mri synthesized in axial, coronal, and sagittal
planes.
we have developed a multi-sequence fusion network based on multi-b-value dwi to
synthesize ce-mri, using source data including dwis and t1-weighted
fatsuppressed mri. compared to existing methods, we avoid the challenges of
using full-sequence mri and aim to be selective on valuable source data dwi.
hierarchical fusion generation module, weighted difference module, and
multisequence attention module have all been shown to improve the performance of
synthesizing target images by addressing the problems of synthesis at different
scales, leveraging differentiable information within and across sequences. given
that current research on synthetic ce-mri is relatively sparse and challenging,
our study provides a novel approach that may be instructive for future research
based on dwis. our further work will be to conduct reader studies to verify the
clinical value of our research in downstream applications, such as helping
radiologists on detecting tumors. in addition, synthesizing dynamic
contrastenhanced mri at multiple time points will also be our future research
direction. our proposed model can potentially be used to synthesize ce-mri,
which is expected to reduce or avoid the use of gbca, thereby optimizing
logistics and minimizing potential risks to patients.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43990-2 8.
nasopharyngeal carcinoma (npc), also known as lymphoepithelioma, is a highly
aggressive malignancy that originates in nasopharynx [1]. npc is characterized
by a distinct geographical distribution in southeast asia, north africa, and
arctic [2]. in china, npc accounts for up to 50% of all head and neck cancers,
while in southeast asia, npc accounts for more than 70% of all head and neck
cancers [3]. radiotherapy (rt) is currently the main treatment remedy, which
needs precise tumor delineation to ensure a satisfactory rt outcome. however,
accurately delineating the npc tumor is challenging due to the highly
infiltrative nature of npc and its complex location, which is surrounded by
critical organs such as brainstem, spinal cord, temporal lobes, etc. to improve
the visibility of npc tumor for precise gross-tumor-volume (gtv) delineation,
contrastenhanced mri (ce-mri) is administrated through injection of
gadolinium-based contrast agents (gbcas) during mri scanning. despite the
superior tumor-to-normal tissue contrast of ce-mri, the use of gbcas during mri
scanning can result in a fatal systemic disease known as nephrogenic systemic
fibrosis (nsf) in patients with renal insufficiency [4]. nsf can cause severe
physical impairment, such as joint contractures of fingers, elbows, and knees,
and can progress to involve critical organs such as the heart, diaphragm,
pleura, pericardium, kidney, liver, and lung [5]. it was reported that the
incidence rate of nsf is around 4% after gbca administration in patients with
severe renal insufficiency, and the mortality rate can reach 31% [6]. currently,
there is no effective treatment for nsf, making it crucial to find a ce-mri
alternative for patients at risk of nsf.in recent years, artificial intelligence
(ai), especially deep learning, plays a gamechanging role in medical imaging
[7,8], which showed great potential to eliminate the use of the toxic gbcas
through synthesizing virtual contrast-enhanced mri (vce-mri) from
gadolinium-free sequences, such as t1-weighted (t1w) and t2-weighted (t2w) mri
[9][10][11][12]. in 2018, gong et al. [11] utilized pre-contrast and 10%
low-dose t1w mri to synthesize the vce-mri for brain disease diagnosis using a
u-shape model, they found that gadolinium dose is able to be reduced by 10-fold
by deep learning while the contrast information could be preserved. followed by
their work, kleesiek et al. [10] proposed a bayesian model to explore the
feasibility of synthesizing vce-mri from contrast-free sequences, their study
demonstrated that deep learning is highly feasible to totally eliminate the use
of gbcas. in the area of rt, li et al. [9] developed a multi-input model to
synthesize vce-mri for npc rt. in addition to the advantage of eliminating the
use of gbca, vce-mri synthesis can also speed up the clinical workflow by
eliminating the need for acquiring ce-mri scan, which saves time for both
clinical staff and patients. however, current studies mostly focus on algorithms
development while lack comprehensive clinical evaluations to demonstrate the
efficacy of the synthetic vce-mri in clinical settings.the clinical evaluation
of ai-based techniques is of paramount importance in healthcare. rigorous
clinical evaluations can establish the safety and efficacy of ai-based
techniques, identify potential biases and limitations, and facilitate the
integration of clinical expertise to ensure accurate and meaningful results
[13]. furthermore, the clinical evaluation of ai-based techniques can help
identify areas for improvement and optimization, leading to development of more
effective algorithms.to bridge this bench-to-bedside research gap, in this
study, we conducted a series of clinical evaluations to assess the effectiveness
of synthetic vce-mri in npc delineation, with a particular focus on assessment
in vce-mri image quality and primary gtv delineation. this study has two main
novelties: (i) to the best of our knowledge, this is the first clinical
evaluation study of the vce-mri technique in rt; and (ii) multiinstitutional mri
data were included in this study to obtain more reliable results. the success of
this study would fill the current knowledge gap and provide the medical
community with a clinical reference prior to clinical application of the novel
vce-mri technique in npc rt.
patient data was retrospectively collected from three oncology centers in hong
kong. this dataset included 303 biopsy-proven (stage i-ivb) npc patients who
received radiation treatment during 2012-2016. the three hospitals were labelled
as institution-1 (110 patients), institution-2 (58 patients), and institution-3
(135 patients), respectively. for each patient, t1w mri, t2w mri,
gadolinium-based ce-mri, and planning ct were retrieved. mri images were
automatically registered as mri images for each patient were scanned in the same
position. the use of this dataset was approved by the institutional review board
of the university of hong kong/hospital authority hong kong west cluster (hku/ha
hkw irb) with reference number uw21-412, and the research ethics committee
(kowloon central/kowloon east) with reference number kc/ke-18-0085/er-1. due to
the retrospective nature of this study, patient consent was waived. for model
development, 288 patients were used for model development and 15 patients were
used to synthesize vce-mri for clinical evaluation. the details of patient
characteristics and the number split for training and testing of each dataset
were illustrated in table 1. prior to model training, mri images were resampled
to 256*224 by bilinear interpolation [14] due to the inconsistent matrix sizes
of the three datasets.
the multimodality-guided synergistic neural network (mmgsn-net) was applied to
learn the mapping from t1w mri and t2w mri to ce-mri. the mmgsn-net was a 2d
network. the effectiveness of this network in vce-mri synthesis for npc patients
has been demonstrated by li et al. in [9]. t1w mri and t2w mri were used as
input and corresponding ce-mri was used as learning target. in this work, we
obtained 12806 image pairs for model training. different from the original
study, which used single institutional data for model development and utilized
min-max value of the whole dataset for data normalization, in this work, we used
mean and standard deviation of each individual patient to normalize mri
intensities due to the heterogeneity of the mri intensities across institutions
[15].
the evaluation methods used in this study included image quality assessment of
vce-mri and primary gtv delineation. two board-certified radiation oncologists
(with 8 years' and 6 years' clinical experience, respectively) were invited to
perform the vce-mri quality assessment and gtv delineation according to their
clinical experience. considering the clinical burden of oncologists, 15 patients
were included for clinical evaluations. all clinical evaluations were performed
on an eclipse workstation (v5.0.10411.00, varian medical systems, usa) with the
same monitor, and the window/level can be adjusted freely by the oncologists.
the results were obtained under the consensus of the two oncologists. (i)
distinguishability between ce-mri and vce-mri. to evaluate the reality of
vce-mri, oncologists were invited to differentiate the synthetic patients (i.e.,
image volumes that generated from synthetic vce-mri) from real patients (i.e.,
image volumes that generated from real ce-mri). different from the previous
studies that utilized limited number (20-50 slices, axial view) of 2d image
slices for reality evaluation [9,10], we used 3d volumes in this study to help
oncologists visualize the inter-slice adjacent information. the judgement
results were recorded, and the accuracy of each institution and the overall
accuracy were calculated.
(ii) clarity of tumor-to-normal tissue interface. the clarity of tumor-normal
tissue interface is critical for tumor delineation, which directly affects the
delineation outcomes. oncologists were asked to use a 5-point likert scale
ranging from 1 (poor) to 5 (excellent) to evaluate the clarity of
tumor-to-normal tissue interface. paired two-tailed t-test (with a significance
level of p = 0.05) was applied to analyses if the scores obtained from real
patients and synthetic patients are significantly different. (iii) veracity of
contrast enhancement in tumor invasion risk areas. in addition to the critical
tumor-normal tissue interface, the areas surrounding the npc tumor will also be
considered during delineation. to better evaluate the veracity of contrast
enhancement in vce-mri, we selected 25 tumor invasion risk areas according to
[16], including 13 high-risk areas and 12 medium-risk areas, and asked
oncologists to determine whether these areas were at risk of being invaded
according to the contrast-enhanced tumor regions. the 13 high-risk areas
include: retropharyngeal space, parapharyngeal space, levator veli palatine
muscle, prestyloid compartment, tensor veli palatine muscle, poststyloid
compartment, nasal cavity, pterygoid process, basis of sphenoid bone, petrous
apex, prevertebral muscle, clivus, and foramen lacerum. the 12 medium-risk areas
include foramen ovale, great wing of sphenoid bone, medial pterygoid muscle,
oropharynx, cavernous sinus, sphenoidal sinus, pterygopalatine fossa, lateral
pterygoid muscle, hypoglossal canal, foramen rotundum, ethmoid sinus, and
jugular foramen. the areas considered at risk of tumor invasion were
recorded.the jaccard index (ji) [17] was utilized to quantitatively evaluate the
results of recorded risk areas from ce-mri and vce-mri. the ji could be
calculated by:where r ce and r vce represents the set of risk areas that
recorded from ce-mri and corresponding vce-mri, respectively. ji measures
similarity of two datasets, which ranges from 0% to 100%. higher ji indicates
more similar of the two sets.(iv) efficacy in primary tumor staging. a critical
rt-related application of ce-mri is tumor staging, which plays a critical role
in treatment planning and prognosis prediction [18]. to assess the efficacy of
vce-mri in npc tumor staging, oncologists were asked to determine the stage of
the primary tumor shown in ce-mri and vce-mri. the staging results from ce-mri
were taken as the ground truth and the staging accuracy of vce-mri was
calculated.primary gtv delineation. gtv delineation is the foremost prerequisite
for a successful rt treatment of npc tumor, which demands excellent precision
[19]. an accurate tumor delineation improves local control and reduce toxicity
to surrounding normal tissues, thus potentially improving patient survival [20].
to evaluate the feasibility of eliminating the use of gbca by replacing ce-mri
with vce-mri in tumor delineation, oncologists were asked to contour the primary
gtv under assistance of vce-mri. for comparison, ce-mri was also imported to
eclipse for tumor delineation but assigned as a different patient, which were
shown to oncologists in a random and blind manner.to mimic the real clinical
setting, contrast-free t1w, t2w mri and corresponding ct of each patient were
imported into the eclipse system since sometimes t1w and t2w mri will also be
referenced during tumor delineation. due to both real patients and synthetic
patients were involved in delineation, to erase the delineation memory of the
same patient, we separated the patients to two datasets, each with the same
number of patients, both two datasets with mixed real patients and synthetic
patients without overlaps (i.e., the ce-mri and vce-mri from the same patient
are not in the same dataset).when finished the first dataset delineation, there
was a one-month interval before the delineation of the second dataset. after the
delineation of all patients, the dice similarity coefficient (dsc) [21] and
hausdorff distance (hd) [22] of the gtvs delineated from real patients and
corresponding synthetic patients were calculated to evaluate the accuracy of
delineated contours.dice similarity coefficient (dsc). dsc is a broadly used
metric to compare the agreement between two segmentations [23]. it measures the
spatial overlap between two segmentations, which ranges from 0 (no spatial
overlap) to 1 (complete overlap). the dsc can be expressed as:where c ce and c
vce represent the contours delineated from real patients and synthetic patients,
respectively.
even though dsc is a well-accepted segmentation comparison metric, it is easily
influenced by the size of contours. small contours typically receive lower dsc
than larger contours [24].therefore, hd was applied as a supplementary to make a
more thorough comparison. hd is a metric to measure the maximum distance between
two contours. given two contours c ce and c vce , the hd could be calculated
as:where d(x, c vce ) and d(y, c ce ) represent the distance from point x in
contour c ce to contour c vce and the distance from point y in contour c vce to
contour c ce . (i) distinguishability between ce-mri and vce-mri. the overall
judgement accuracy for the mri volumes was 53.33%, which is close to a random
guess accuracy (i.e., 50%). for institution-1, 2 real patients were judged as
synthetic and 1 synthetic patient was considered as real. for institution-2, 2
real patients were determined as synthetic and 4 synthetic patients were
determined as real. for institution-3, 2 real patients were judged as synthetic
and 3 synthetic patients were considered as real. in total, 6 real patients were
judged as synthetic and 8 synthetic patients were judged as real. (ii) clarity
of tumor-to-normal tissue interface. the overall clarity scores of
tumorto-normal tissue interface for real and synthetic patients were 3.67 with a
median of 4 and 3.47 with a median of 4, respectively. no significant difference
was observed between these two scores (p = 0.38). the average scores for real
and synthetic patients were 3.6 and 3, 3.6 and 3.8, 3.8 and 3.6 for
institution-1, institution-2, and institution-3, respectively. 5 real patients
got a higher score than synthetic patients and 3 synthetic patients obtained a
higher score than real patients. the scores of the other 7 patient pairs were
the same. (iii) veracity of contrast enhancement in tumor invasion risk areas.
the overall ji score between the recorded tumor invasion risk areas from ce-mri
and vce-mri was 74.06%. the average ji obtained from institution-1,
institution-2, and institution-3 dataset were similar with a result of 71.54%,
74.78% and 75.85%, respectively. in total, 126 risk areas were recorded from the
ce-mri for all of the evaluation patients, while 10 (7.94%) false positive high
risk invasion areas and 9 (7.14%) false negative high risk invasion areas were
recorded from vce-mri. (iv) efficacy in primary tumor staging. a t-staging
accuracy of 86.67% was obtained using vce-mri. 13 patient pairs obtained the
same staging results. for the institution-2 data, all synthetic patients
observed the same stages as real patients.
for the two t-stage disagreement patients, one synthetic patient was staged as
phase iv while the corresponding real patient was staged as phase iii, the other
synthetic patient was staged as i while corresponding real patient was staged as
phase iii.
the average dsc and hd between the c ce and c vce was 0.762 (0.673-0.859) with a
median of 0.774, and 1.932 mm (0.763 mm-2.974 mm) with a median of 1.913 mm,
respectively. for institution-1, institution-2, and institution-3, the average
dsc were 0.741, 0.794 and 0.751 respectively, while the average hd were 2.303
mm, 1.456 mm, and 2.037 mm respectively. figure 2 illustrated the delineated
primary gtv contours from an average patient with the dsc of 0.765 and hd of
1.938 mm. the green contour shows the primary gtv that delineated form the
synthetic patient, while the red contour was delineated from corresponding real
gbca-based patient.
in this study, we conducted a series of clinical evaluations to validate the
clinical efficacy of vce-mri in rt of npc patients. results showed the vce-mri
has great potential to provide an alternative to gbca-based ce-mri for npc
delineation.
quality evaluation results of vce-mri: (a) distinguishability between ce-mri and
vce-mri; (b) clarity of tumor-to-normal tissue interface; (c) veracity of
contrast enhancement in risk areas; and (d) t-staging. abbreviations: inst:
institution; c.a.: center-based average; o.a.: overall average; syn: synthetic.
lung cancer is the leading cause of cancer death in the united states, and early
detection is key to improving survival rates. ct lung cancer screening is a
lowdose ct (ldct) scan of the chest that can detect lung cancer at an early
stage, when it is most treatable. however, the current workflow for performing
ct lung scans still requires an experienced technician to manually perform
pre-scanning steps, which greatly decreases the throughput of this high volume
procedure. while recent advances in human body modeling [4,5,12,13,15] have
allowed for automation of patient positioning, scout scans are still required as
they are used by automatic exposure control system in the ct scanners to compute
the dose to be delivered in order to maintain constant image quality [3].since
ldct scans are obtained in a single breath-hold and do not require any contrast
medium to be injected, the scout scan consumes a significant portion of the
scanning workflow time. it is further increased by the fact that tube rotation
has to be adjusted between the scout and actual ct scan. furthermore, any
patient movement during the time between the two scans may cause misalignment
and incorrect dose profile, which could ultimately result in a repeat of the
entire process. finally, while minimal, the radiation dose administered to the
patient is further increased by a scout scan.we introduce a novel method for
estimating patient scanning parameters from non-ionizing 3d camera images to
eliminate the need for scout scans during pre-scanning. for ldct lung cancer
screening, our framework automatically estimates the patient's lung position
(which serves as a reference point to start the scan), the patient's isocenter
(which is used to determine the table height for scanning), and an estimate of
patient's water equivalent diameter (wed) profiles along the craniocaudal
direction which is a well established method for defining size specific dose
estimate (ssde) in ct imaging [8,9,11,18]. additionally, we introduce a novel
approach for updating the estimated wed in real-time, which allows for
refinement of the scan parameters during acquisition, thus increasing accuracy.
we present a method for automatically aborting the scan if the predicted wed
deviates from real-time acquired data beyond the clinical limit. we trained our
models on a large collection of ct scans acquired from over 60, 000 patients
from over 15 sites across north america, europe and asia. the contributions of
this work can be summarized as follows:-a novel workflow for automated ct lung
cancer screening without the need for scout scan -a clinically relevant method
meeting iec 62985:2019 requirements on wed estimation. -a generative model of
patient wed trained on over 60, 000 patients.-a novel method for real-time
refinement of wed, which can be used for dose modulation
water equivalent diameter (wed) is a robust patient-size descriptor [17] used
for ct dose planning. it represents the diameter of a cylinder of water having
the same averaged absorbed dose as the material contained in an axial plane at a
given craniocaudal position z [2]. the wed of a patient is thus a function
taking as input a craniocaudal coordinate and outputting the wed of the patient
at that given position. as wed is defined in an axial plane, the diameter needs
to be known on both the anterior-posterior (ap) and lateral (left-right) axes
noted respectively w ed ap (z) and w ed l (z). as our focus here is on lung
cancer screening, we define 'wed profile' to be the 1d curve obtained by
uniformly sampling the wed function along the craniocaudal axis within the lung
region.our method jointly predicts the ap and lateral wed profiles. while wed
can be derived from ct images, paired ct scans and camera images are rarely
available, making direct regression through supervised learning challenging. we
propose a semi-supervised approach to estimate wed from depth images. first, we
train a wed generative model on a large collection of ct scans. we then train an
encoder network to map the patient depth image to the wed manifold. finally, we
propose a novel method to refine the prediction using real-time scan data.
we use an autodecoder [10] to learn the wed latent space. our model is a fully
connected network with 8 layers of 128 neurons each. we used layer normalization
and relu activation after each layer except the last one. our network takes as
input a latent vector together with a craniocaudal coordinate z and outputs w ed
ap (z) and w ed l (z), the values of the ap and lateral wed at the given
coordinate. in this approach, our latent vector represents the encoding of a
patient in the latent space. this way, a single autodecoder can learn
patient-specific continuous wed functions. since our network only takes the
craniocaudal coordinate and the latent vector as input, it can be trained on
partial scans of different sizes. the training consists of a joint optimization
of the autodecoder and the latent vector: the autodecoder is learning a
realistic representation of the wed function while the latent vector is updated
to fit the data.during training, we initialize our latent space to a unit
gaussian distribution as we want it to be compact and continuous. we then
randomly sample points along the craniocaudal axis and minimize the l1 loss
between the prediction and the ground truth wed. we also apply l2-regularization
on the latent vector as part of the optimization process.
after training our generative model on a large collection of unpaired ct scans,
we train our encoder network on a smaller collection of paired depth images and
ct scans. we represent our encoder as a densenet [1] taking as input the depth
image and outputting a latent vector in the previously learned latent space. our
model has 3 dense blocks of 3 convolutional layers. each convolutional layer
(except the last one) is followed by a spectral normalization layer and a relu
activation. the predicted latent vector is then used as input to the frozen
autodecoder to generate the predicted wed profiles. we here again apply
l2regularization on the latent vector during training.
while the depth image provides critical information on the patient anatomy, it
may not always be sufficient to accurately predict the wed profiles. for
example, some patients may have implants or other medical devices that cannot be
guessed solely from the depth image. additionally, since the encoder is trained
on a smaller data collection, it may not be able to perfectly project the depth
image to the wed manifold. to meet the strict safety criteria defined by the
iec, we propose to dynamically update the predicted wed profiles at inference
time using real-time scan data. first, we use our encoder network to initialize
the latent vector to a point in the manifold that is close to the current
patient. then, we use our autodecoder to generate initial wed profiles. as the
table moves and the patient gets scanned, ct data is being acquired and ground
truth wed can be computed for portion of the body that has been scanned, along
with the corresponding craniocaudal coordinate. we can then use this data to
optimize the latent vector by freezing the autodecoder and minimizing the l1
loss between the predicted and ground truth wed profiles through gradient
descent. we can then feed the updated latent vector to our autodecoder to
estimate the wed for the remaining portions of the body that have not yet been
scanned and repeat the process.in addition to improving the accuracy of the wed
profiles prediction, this approach can also help detect deviation from real
data. after the latent vector has been optimized to fit the previously scanned
data, a large deviation between the optimized prediction and the ground truth
profiles may indicate that our approach is not able to find a point in the
manifold that is close to the data. in this case, we may abort the scan, which
further reduces safety risks. overall flowchart of the proposed approach is
shown in fig. 1.
our ct scan dataset consists of 62, 420 patients from 16 different sites across
north america, asia and europe. our 3d camera dataset consists of 2, 742 pairs
of depth image and ct scan from 2, 742 patients from 6 different sites across
north america and europe acquired using a ceiling-mounted kinect 2 camera. our
evaluation set consists of 110 pairs of depth image and ct scan from 110
patients from a separate site in europe.
patient positioning is the first step in lung cancer screening workflow. we
first need to estimate the table position and the starting point of the scan. we
propose to estimate the table position by regressing the patient isocenter and
the starting point of the scan by estimating the location of the patient's lung
top.starting position. we define the starting position of the scan as the
location of the patient's lung top. we trained a denseunet [7] taking the camera
depth image as input and outputting a gaussian heatmap centered at the patient's
lung top location. we used 4 dense blocks of 4 convolutional layers for the
encoder and 4 dense blocks of 4 convolutional layers for the decoder. each
convolutional layer (except the last one) is followed by a batch normalization
layer and a relu activation. we trained our model on 2, 742 patients using
adaloss [14] and the adam [6] optimizer with a learning rate of 0.001 and a
batch size of 32 for 400 epochs. our model achieves a mean error of 12.74 mm and
a 95 th percentile error of 28.32 mm. to ensure the lung is fully visible in the
ct image, we added a 2 cm offset on our prediction towards the outside of the
lung. we then defined the accuracy as whether the lung is fully visible in the
ct image when using the offset prediction. we report an accuracy of 100% on our
evaluation set of 110 patients. third and fourth columns show the performance of
our model with real-time refinement every 5 cm and 2 cm respectively. ground
truth is depicted in green and our prediction is depicted in red. while the
original prediction was off towards the center of the lung, the real-time
refinement was able to correct the error.isocenter. the patient isocenter is
defined as the centerline of the patient's body. we trained a densenet [1]
taking the camera depth image as input and outputting the patient isocenter. our
model is made of 4 dense blocks of 3 convolutional layers. each convolutional
layer (except the last one) is followed by a batch normalization layer and a
relu activation. we trained our model on 2, 742 patients using adadelta [16]
with a batch size of 64 for 300 epochs. on our evaluation set, our model
outperforms the technician's estimates with a mean error of 5.42 mm and a 95 th
percentile error of 8.56 mm compared to 6.75 mm and 27.17 mm respectively.
results can be seen in fig. 2.
we trained our autodecoder model on our unpaired ct scan dataset of 62, 420
patients with a latent vector of size 32. the encoder was trained on our paired
ct scan and depth image dataset of 2, 742 patients. we first compared our method
against a simple direct regression model. we trained a denseunet [7] taking the
camera depth image as input and outputting the water equivalent diameter
profile. we trained this baseline model on 2, 742 patients using the adadelta
[6] optimizer with a learning rate of 0.001 and a batch size of 32. we table 1.
wed profile errors on our testing set (in mm). 'w' corresponds to the portion
size of the body that gets scanned before updating the prediction (in cm). top
of the table corresponds to lateral wed profile, bottom corresponds to ap wed
profile. updating the prediction every 20 mm produces the best results.
mean then measured the performance of our model before and after different
degrees of real-time refinement, using the same optimizer and learning rate. we
report the comparative results in table 1.we observed that our method largely
outperforms the direct regression baseline with a mean lateral error 40% lower
and a 90 th percentile lateral error over 30% lower. bringing in real-time
refinement greatly improves the results with a mean lateral error over 40% and a
90 th percentile lateral error over 20% lower than before refinement. ap
profiles show similar results with a mean ap error improvement of nearly 40% and
a 90 th percentile ap error improvement close to 30%. when using our proposed
method with a 20 mm window refinement, our proposed approach outperforms the
direct regression baseline by over 60% for lateral profile and nearly 80% for
ap.figures 3 highlights the benefits of using real-time refinement. overall, our
approach shows best results with an update frequency of 20 mm, with a mean
lateral error of 15.93 mm and a mean ap error of 10.40 mm. figure 4 presents a
qualitative evaluation on patients with different body morphology.finally, we
evaluated the clinical relevancy of our approach by computing the relative error
as described in the international electrotechnical commission (iec) standard iec
62985:2019 on methods for calculating size specific dose estimates (ssde) for
computed tomography [2]. the δ rel metric is defined as:where:-ŵ ed(z) is the
predicted water equivalent diameter -w ed(z) is the ground truth water
equivalent diameter z is the position along the craniocaudal axis of the
patient. iec standard states the median value of the set of δ rel (z) along the
craniocaudal axis (noted δ rel ) should be below 0.1. our method achieved a mean
lateral δ rel error of 0.0426 and a mean ap δ rel error of 0.0428, falling well
within the acceptance criteria.
we presented a novel 3d camera based approach for automating ct lung cancer
screening workflow without the need for a scout scan. our approach effectively
estimates start of scan, isocenter and water equivalent diameter from depth
images and meets the iec acceptance criteria of relative wed error. while this
approach can be used for other thorax scan protocols, it may not be applicable
to trauma (e.g. with large lung resections) and inpatient settings, as the
deviation in predicted and actual wed would likely be much higher. in future, we
plan to establish the feasibility as well as the utility of this approach for
other scan protocols and body regions.1
age-related macular degeneration (amd) is the leading cause of blindness in the
elderly, affecting nearly 200 million people worldwide [24]. patients with early
stages of the disease exhibit few symptoms until suddenly converting to the late
stage, at which point their central vision rapidly deteriorates [12]. clinicians
currently diagnose amd, and stratify patients, using biomarkers derived from
optical coherence tomography (oct), which provides high-resolution images of
fig. 1. our method finds common patterns of disease progression in datasets of
longitudinal images. we partition time series into sub-trajectories before
introducing a clinically motivated distance function to cluster the
sub-trajectories in feature space. the clusters are then assessed by
ophthalmologists on their interpretability and ability to capture the
progression of amd.the retina. however, the widely adopted amd grading system
[7,13], which coarsely groups patients into broad categories for early and
intermediate amd, only has limited prognostic value for late amd. clinicians
suspect that this is due to the grading system's reliance on static biomarkers
that are unable to capture temporal dynamics which contain critical information
for assessing progression risk.in their search for new biomarkers, clinicians
have annotated known biomarkers in longitudinal datasets that monitor patients
over time and mapped them against disease progression [2,16,19]. this approach
is resource-intensive and requires biomarkers to be known a priori. others have
proposed deep-learningbased methods to discover new biomarkers at scale by
clustering oct images or detecting anomalous features [17,18,23]. however, these
approaches neglect temporal relationships between images and the obtained
biomarkers are by definition static and cannot capture the dynamic nature of the
disease.our contribution: in this work, we present a method to automatically
propose biomarkers that capture temporal dynamics of disease progression in
longitudinal datasets (see fig. 1). at the core of our method is the novel
strategy to represent patient time series as trajectories in a latent feature
space. individual progression trajectories are partitioned into atomic
sub-sequences that encode transitions between disease states. then, we identify
population-level patterns of amd progression by clustering these
sub-trajectories using a newly introduced distance metric that encodes three
distinct temporal criteria. in experiments involving 160,558 retinal scans, four
ophthalmologists verified that our method identified several candidates for
temporal biomarkers of amd. moreover, our clusters demonstrated greater
prognostic value for late-stage amd when compared to the widely adopted amd
grading system.
current amd grading systems: ophthalmologists' current understanding of
progression from early to late amd largely involves drusen, which are subretinal
lipid deposits. drusen volume increases until suddenly stagnating and
regressing, which often precedes the onset of late amd [16]. the established amd
grading system stratifies early and intermediate stages solely by the size of
drusen in a single oct image [1,6,7,10]. late amd is classified into either
choroidal neovascularisation (cnv), identified by subretinal fluid, or
geographic atrophy, signalled by progressive loss of photoreceptors and retinal
thinning. the degree of atrophy can be staged using crora (complete retinal
pigment epithelium and outer retinal atrophy), which measures the width in μm of
focal atrophy in oct [13]. grading systems derived from these biomarkers offer
limited diagnostic value and little to no prognostic capability.tracking
evolution of known biomarkers: few research efforts have aimed at quantifying
and tracking known amd biomarkers, mostly drusen, over time [16,19]. more work
has explored the disease progression of alzheimer's disease (ad), which offers a
greater array of quantitative imaging biomarkers, such as levels of tau protein
and hippocampal volume. young et al. [25] fit an event-based model that
rediscovers the order in which these biomarkers become anomalous as ad
progresses. vogel et al. [21] find four distinct spatiotemporal trajectories for
tau pathology in the brain. however, this only works if biomarkers are known a
priori and requires manual annotation of entire time series.automated discovery
of unknown biomarkers: prior work for automated biomarker discovery in amd
explores the latent feature space of encoders trained for image reconstruction
[18,23], segmentation [27] or generative adversarial networks [17]. however,
these neural networks are prone to overfit to their specific task and lose
semantic information regarding the disease. contrastive methods [3,8,26] encode
invariance to a set of image transformations, which are uncorrelated with
disease features, resulting in a more expressive feature space. however, all
aforementioned methods group single images acquired at one point in time, and in
doing so neglect temporal dynamics. the one work that tackles this challenge,
and the most related to ours, categorises the time-dependent response of cancer
cells to different drugs, measured by the changing distance in contrastive
feature space from healthy controls [5].
we use two retinal oct datasets curated in the scope of the pinnacle study [20].
we first design and test our method on a development dataset, which was
collected from the southampton eye unit. afterwards, we test our method on a
second independent unseen dataset, which was obtained from moorfields eye
hospital. all images were acquired using topcon 3d oct devices (topcon
corporation, tokyo, japan). after strict quality control, the development
dataset consists of 46,496 scans of 6,236 eyes from 3,456 patients. eyes were
scanned 7.7 times over 1.9 years on average at irregular time intervals. the
unseen dataset is larger, containing 114,062 scans of 7,253 eyes from 3,819
patients. eyes were scanned 16.6 times over 3.5 years on average. a subset of
1,031 longitudes was labelled using the established amd grading protocols
derived from known imaging biomarkers. early amd was characterised by small
drusen between 63-125µm in diameter. we also recorded cnv, crora (≥ 250µm and
<1000µm), crora (≥ 1000µm) [13] and healthy cases with no visible biomarkers.
visual acuity scores, which measured the patient's functional quality of vision
using a logmar chart, are available at 83,964 time points.
we use the byol contrastive loss [8] in eq. 1 to train a resnet50 (4x) model f
over each batch of twice transformed images xwhere the output of the momentum
updated 'teacher' network f is passed through a stop-gradient, so that only the
student network f is updated. as several of the contrastive transformations
designed for natural images are inapplicable to medical images, such as
solarisation, colour shift and greyscale, we use the set tailored for retinal
oct images by holland et al. [9]. models were trained on the entire dataset for
120,000 steps using the adam optimiser with a momentum of 0.9, weight decay of
1.5 • 10 -6 and a learning rate of 5 • 10 -4 . after training f , we first
remove the final linear layer before projecting all labelled images to the
feature space of 2048 dimensions.fig. 2. illustration of sub-trajectory distance
functions which each encode temporal criteria for similarity (see eq. 4). we
illustrate clusters assignments, denoted by colour, resulting from three
combinations of φ and λ.
naively clustering whole time series of patients ignores two characteristics of
longitudinal data. firstly, individual time series are not directly comparable
as patients enter and leave the study at different stages of their overall
progression.secondly, longer time series can record multiple successive
transitions in disease stage. inspired by traclus [11], the state of the art in
trajectory clustering, we adapt their partition-and-group framework by assuming
that trajectories can be partitioned into a common set of sub-trajectories that
capture singular transitions between progressive states of the disease.for each
eye, we first form piecewise-linear trajectories by linking points in feature
space that were derived from consecutively acquired oct images. we then extract
sub-trajectories by finding all sequences of images spanning 1.0 ± 0.5 years of
elapsed time within each trajectory. next, to avoid oversampling trajectories
with a shorter time interval between images, we randomly sample at most one
sub-trajectory in every 0.5-year time interval.
in order to find common patterns of disease progression among sub-trajectories
we cluster them. to this end we introduce a new distance function between
subtrajectories that incorporates three distinct temporal criteria (see fig. 2).
the first, formulated in eq. 2, matches two sub-trajectories, u and v , of
patients who progress between the same start and end states:since all
sub-trajectories cover a similar temporal duration, d transition also
differentiates between fast and slow progressors and stable periods of no
progression. however, by ignoring intermediary images, this metric does not
respect the disease pathway along which patients progress. to incorporate this,
we include a second metric that measures path dissimilarity, calculated using
dynamic time warping (dtw) [4,14,15]. dtw finds the optimal temporal alignment
between two time series before computing their distance. this re-alignment
allows us to match sub-trajectories that traverse the same disease states in the
same order, irrespective of the rate of change between states. we combine d
transition with dtw using a λ ∈ r, 0 ≤ λ ≤ 1 coefficient so the overall distance
between u and v isthe third and final temporal criteria is to match time series
that progress in the same relative direction, regardless of absolute disease
states. we weight the contribution of this with φ ∈ r, 0 ≤ φ ≤ 1 in eq.
4:spectral clustering: as the non-linearity of d subtraj prohibits the use of
k-means for clustering, we instead use spectral clustering [22] to group similar
sub-trajectories. hereby, we construct an affinity matrix a encoding the
negative of the distance d subtraj between all pairs of sub-trajectories. using
a, we group sub-trajectories into k clusters.
initially, we tune the hyperparameters, λ, φ and k, on the development dataset
by heuristically selecting values that result in higher uniformity between
subtrajectories within each cluster. two teams of two ophthalmologists then
review 20 sub-trajectories from distinct patients in each cluster, interpreting
and summarising any consistently observed temporal dynamics. next, using the
same hyperparameters we apply the method directly to the unseen dataset. the
ophthalmologists then review these clusters and confirm whether they capture the
same temporal biomarkers observed in the development dataset.in addition to the
qualitative evaluation, we also validate the utility of our clusters as
biomarkers that stratify risk of disease progression. we test this by predicting
the time until conversion to late amd and its subtypes, cnv and crora.
additionally, we predict current visual acuity. for prediction, each
sub-trajectory is characterised by a vector of size k that encodes proportional
similarity to each cluster. this vector is then used by a lasso linear
regression model. similarly, we fit an equivalent linear regression model to the
static biomarkers from the established grading system detailed in sect. 3.1. we
also include a demographic baseline using age and sex. we also add a temporally
agnostic baseline that clusters only single time points. finally, to demonstrate
the performance gap between our interpretable approach and black-box supervised
learning algorithms, we include a fully supervised deep learning baseline by
fitting an svr directly to the feature space. each experiment uses 10-fold cross
validation on random 80/20 partitions, while ensuring a patient-wise split.
finally, we repeat the entire method, starting from sub-trajectory extraction,
followed by clustering and then regression experiments, using 7 random seeds and
report the means and standard deviations.
sub-trajectory clusters are candidate temporal biomarkers: by first applying our
method to the development dataset we found that using λ = 0.75, φ = 0.75 and k =
30 resulted in the most uniform and homogeneous clusters while still limiting
the total number of clusters to a reasonable amount. achieving the same cluster
quality with smaller values of φ required many more clusters in order to encode
all combinations of possible start and end disease states. the expert
ophthalmologists remarked that many of the identified clusters capture dynamics
that have already been linked to the progression of amd, even though they are
not currently included in any clinical grading system. using the same
hyperparameters our method generalised to the unseen dataset which yielded
clusters with equivalent dynamics and quality (see fig. 3). ophthalmologists
identified clusters capturing the same variants of temporal progression in both
datasets. they named these as 'rapid growth of drusen pigment epithelial
detachments (ped)', 'regression of drusen ped', 'development of subretinal fluid
', 'development of intraretinal fluid ', 'development of hypertransmission' and
'stable state' (no signs of progression at each disease state).sub-trajectory
clusters predict conversion to late amd: next, we validated that our clusters
are predictive of progression to late amd. our subtrajectory clusters were
comparable to, and in some cases outperformed, the current widely adopted
grading system in predicting risk of conversion (see table 1). in all tasks the
standard biomarkers are only marginally more indicative of risk than the
patient's age and sex. this experiment confirms that our clusters are related to
disease progression.
motivated to improve inadequate grading systems for amd that do not incorporate
temporal dynamics we developed a method to automatically propose biomarkers that
are time-dependent, interpretable, and predictive of conversion to late-stage
amd. we applied our method to two large longitudinal datasets, cataloguing 3,218
total years of disease progression. the found time-dependent clusters were
subsequently interpreted by four ophthalmologists. they found them to capture
distinct patterns of disease progression that have been previously linked to
amd, but are not currently included in clinical grading systems. furthermore, we
experimentally demonstrated that the found clusters predict conversion to
late-stage amd on par with the established grading system.in the future,
biomarkers identified by our method can be further refined by clinicians. we
will also use the full volumetric image to model progression dynamics outside
the macular. as late stage patients were overrepresented in our datasets, we
also intend to apply our method to datasets with greater numbers of patients
progressing from earlier disease stages. ultimately, we envision that proposals
from our method may inform the next generation of grading systems for amd that
incorporate the temporal dimension intrinsic to this dynamic disease.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43990-2_68.
diffuse glioma is the most common and aggressive primary brain tumors in adults,
accounting for more deaths than any other type [7]. pathology diagnosis is the
gold standard for diffuse glioma but is usually time-consuming and highly
depends on the expertise of senior pathologists [13]. hence, automatic
algorithms based on histology whole slide images (wsis) [15], namely digital
pathology, promise to offer rapid diagnosis and aid precise treatment.recently,
deep learning has achieved success in diagnosing various tumors [2,21]. most
methods are mainly predicting histology based on wsi, less concerning molecular
markers. however, the paradigm of pathological diagnosis of glioma has shifted
to molecular pathology, reflected by the 2021 who classification of tumors of
the central nervous system [14]. the role of key molecular markers, i.e.,
isocitrate dehydrogenas (idh) mutations, co-deletion of chromosome 1p/19q and
homozygous deletion (homdel) of cyclin-dependent kinase inhibitor 2a/b (cdkn),
have been highlighted as major diagnostic markers for glioma, while histology
features that are traditionally emphasized are now considered as reference,
although still relevant in many cases. for instance, in the new pathology
scheme, glioblastoma is increasingly diagnosed according to idh mutations, while
previously its diagnosis mostly relies on histology features, including necrosis
and microvascular proliferation (nmp). 1 however, the primary approaches to
assess molecular markers include gene sequencing and immuno-staining, which are
time-consuming and expensive than histology assessment. as histology features
are closely associated with molecular alterations, algorithm predicting
molecular markers based on histology wsis is feasible and have clinical
significance. moreover, under the new paradigm of integrating molecular markers
with histological features into tumor classification, it is helpful to model the
interaction of histology and molecular makers for a more accurate diagnosis.
therefore, there is an urgent need for developing novel digital pathology
methods based on wsi to predict molecular markers and histology jointly and
modeling their interactions for final tumor classification, which could be
valuable for the clinically relevant diagnosis of diffuse glioma.this paper
proposes a deep learning model (deepmo-glioma) for glioma classification based
on wsis, aiming to reflect the molecular pathology paradigm. previous methods
are proposed to integrate histology and genomics for tumour diagnosis [3,10,20].
for instance, chen et al. [3] proposed a multimodal fusion strategy to integrate
wsis and genomics for survival prediction. xing et al. [20] devised a
self-normalizing network to encode genomics. nevertheless, most existing
approaches of tumor classification only treat molecular markers as additional
input, incapable to simultaneously predict the status of molecular markers, thus
clinically less relevant under the current clinical diagnosis scheme. to jointly
predict histology and molecular markers following clinical diagnostic pathway,
we propose a novel hierarchical multi-task multi-instance learning (hmt-mil)
framework based on vision transformer [4], with two partially weight-sharing
parts to jointly predict molecular markers and histology.moreover, multiple
molecular markers are needed for classifying cancers, due to complex tumor
biology. to reflect real-world clinical scenarios, we formulate predicting
multiple molecular markers as a multi-label classification (mlc) task. previous
mlc methods have successfully modeled the correlation among labels [12,22]. for
example, yazici et al. [22] proposed an orderless recurrent method, while li et
al. designed a label attention transformer network with graph embedding. in
medical domain, zhang et al. [25] devised a dual-pool contrastive learning for
classifying fundus and x-ray images. despite success, when applied to predicting
multiple molecular markers, most existing methods may ignore the co-occurrence
of molecular markers, which have intrinsic associations [23]. hence, we propose
a co-occurrence probability-based, label-correlation graph (cplc-graph) network
to model the co-occurrence of molecular markers, i.e., intra-omic
relationship.lastly, we focus on modeling the interaction between molecular
markers and histology. specifically, we devise a novel inter-omic interaction
strategy to model the interaction between the predictions of molecular markers
and histology, e.g., idh mutation and nmp, both of which are relevant in
diagnosing glioblastoma. particularly, we design a dynamical confidence
constraint (dcc) loss that constrains the model to focus on similar areas of
wsis for both tasks. to the best of our knowledge, this is the first attempt to
classify diffuse gliomas via modeling the interaction of histology and molecular
markers.our main contributions are: (1) we propose a multi-task multi-instance
learning framework to jointly predict molecular markers and histology and
finally classify diffuse glioma, reflecting the new paradigm of pathology
diagnosis. (2) we design a cplc-graph network to model the intra-omic
relationship of multiple molecular markers. (3) we design a dcc learning
strategy to model the inter-omic interaction between histology and molecular
markers for glioma classification.
database: we use publicly available tcga gbm-lgg dataset [6]. following [15], we
remove the wsis of low quality or lack of labels. totally, we include training
labels: original lables for genomic markers and histology of wsis are obtained
from tcga database [6]. according to the up-to-date who criteria [14], we
generate the classification labels for each case as grade 4 glioblastoma
(defined as idh widetype), oligodendroglioma (defined as idh mutant and 1p/19q
co-deletion), grade 4 astrocytoma (defined as idh mutant, 1p/19q non co-deletion
with cdkn homdel or nmp), or low-grade astrocytoma (other cases).
figure 1 illustrates the proposed deepmo-glioma. as shown above, the upto-date
who criteria incorporates molecular markers and histology features. therefore,
our model is designed to jointly learn the tasks of predicting molecular markers
and histology features in a unified framework. deepmo-glioma consists four
modules, i.e., stem, genomic marker prediction, histology prediction and
cross-omics interaction. given the cropped patches {x i } n 1 as the input,
deepmo-glioma outputs 1) the status of molecular markers, including idh mutation
lidh ∈ r 2 , 1p/19q co-deletion l1p/19q ∈ r 2 and cdkn homdel lcdkn ∈ r 2 , 2)
existence of nmp lnmp ∈ r 2 and 3) final diagnosis of diffuse gliomas lglio ∈ r
4 . of note, the final diagnosis of diffuse gliomas is generated via a decision
tree-based logical function with the input of predicted molecular markers and
histology, consistent with the up-to-date who criteria.
to extract global information from input {x i } n 1 , we propose a hierarchical
multi-task multi-instance learning (hmt-mil) framework for both histology and
molecular marker predictions. different from methods using one [24] or several
[3,20] representative patches per slide, hmt-mil framework can extract
information from n = 2,500 patches per wsi via utilizing the mil learning
paradigm with transformer blocks [4] embedded. note for wsis with patch number<
n, we adopt a biological repeat strategy for dimension alignment.
in predicting molecular markers, i.e., idh, 1p/19q and cdkn, existing mlc
methods based on label correlation may ignore the co-occurrence of the labels.
in the genomic marker prediction module, we proposed a co-occurrence
probabilitybased, label-correlation graph (cplc-graph) network and a label
correlation (lc) loss for intra-omic modeling of the co-occurrence probability
of the three markers.1) cplc-graph network: cplc-graph (fig. 2) is defined as g
= (v, e), where v indicates the nodes, while e represents the edges. given the
intermediate features in predicting the three molecular markers subnetsas input
nodes, we construct a co-occurrence probability based correlation matrix a ∈ r
3×3 to reflect the relationships among each node feature, with a weight matrix w
g ∈ r c×c to update the value of f in . formally, the output nodes f mid ∈ r 3×c
are formulated by a single graph convolutional network layer asin (1), δ(•) is
an activation function and p(f in i |f in j ) denote the probability of the
status of i-th marker given the status of j-th marker. besides, residual
structure is utilized to generate the final output f out of cplc-graph network,
defined aswhere α is a graph balancing hyper-parameter.
in order to fully exploit the co-occurrence probability of different molecular
markers, we further devise the lc loss that constrains the similarity between
any two output molecular markers f out i and f out j to approach their
correspondent co-occurrence probability a j i . formally, the lc loss is defined
asin (2), mse denotes the function of mean square error, while d i,j cos is the
cosine similarity of features f out i and f out j .
in the cross-omics interaction module, we design a dynamical confidence
constraint (dcc) strategy to model the interaction between molecular markers and
histological features. taking idh and nmp as an example, the final outputs for
idh widetype2 and nmp predictions can be defined as lwt = n n=1 ω n wt f n wt
and lnmp = n n=1 ω n nmp f n nmp , respectively. note that f n wt and ω n wt are
values of the extracted feature and the corresponding decision weight of n-th
patch, respectively. we then reorder [ω n wt ] n n=1 to [ω n wt ] n n=1 based on
their values. similarly, we obtain [ω n nmp ] n n=1 for nmp confidence weights.
based on ordered confidence weights, we constrain the prediction networks of
histology and molecular markers to focus on the wsi areas important for both
predictions, thus modeling inter-omic interactions. specifically, we achieve the
confidence constraint through a novel dcc loss focusing on top k important
patches for both prediction. formally, the dcc loss in m-th training epoch is
defined as:where s(ω k wt , ωnmp ) is the indicator function taking the value 1
when the k-th important patch of idh widetype is in the set of top k m important
patches for nmp, and vice versa. in addition, to facilitate the learning process
with dcc loss, we adopt a curriculum-learning based training strategy
dynamically focusing on hard-to-learn patches, regarded as the patches with
higher decision importance weight, as patches with lower confidence weight,
e.g., patches with fewer nuclei, are usually easier to learn in both tasks.
hence, k m is further defined asin ( 4), k 0 and m 0 are hyper-parameters to
adjust l dcc in training process.
the proposed deepmo-glioma is trained on the training set for 70 epochs, with
batch size of 8 and initial learning rate of 0.003 with adam optimizer [11]
together with weight decay. key hyper-parameters are listed in table 1 of
supplementary material. all hyper-parameters are tuned to achieve the best
performance over the validation set. all experiments are conducted on a computer
with an intel(r) xeon(r) e5-2698 cpu @2.20 ghz, 256 gb ram and 4 nvidia tesla
v100 gpus. additionally, our method is implemented on pytorch with python
environment. * we modified baselines to the mil setting, since they are not
originally designed for mil.
1) glioma classification. we compare our model with five other state-of-theart
methods: clam [15], transmil [16], resnet-18 [5], densenet-121 [8] and vgg-16
[17]. note clam [15] and transmil [16] are mil framework, while others are
commonly-used image classification methods, set as our baselines.the left panel
of table 1 shows that deepmo-glioma performs the best, achieving at least 6.1%,
13.1%, 3.1% and 11.0% improvement over other models in accuracy, sensitivity,
specificity and auc, respectively, indicating that our model could effectively
integrate molecular markers and histology in classifying diffuse gliomas.2)
predictions of genomic markers and histology features. from the left panel of
table 2, we observe that deepmo-glioma achieves the auc of 92.0%, 88.1%, 77.2%
and 94.5% for idh mutation, 1p/19q co-deletion, cdkn homdel and nmp prediction,
respectively, considerably better than all the comparison models. figure 3 plots
the rocs of all models, demonstrating the superior performance of our model over
other comparison models.3) network interpretability. an additional visualization
experiment is conducted based on patch decision scores to test the
interpretability of our method. due to the page limit, the results are presented
in supplementary fig. 1.
1) cplc-graph network. the right panels of table 1 shows that, by setting graph
balancing weight α to 0 for the proposed cplc-graph, the accuracy, sensitivity,
specificity and f 1 -score decreases by 7.8%, 29.0%, 3.6% and 21.6%,
respectively. similar results are observed for the prediction tasks of molecular
markers and histology (table 2). also, the roc of removing the cplc-graph
network is shown in fig. 3. these results indicate the utility of the proposed
cplc-graph network.2) lc loss. the right panels of table 1 shows that the
performance after removing lc loss decreases in all metrics, causing a reduction
of 6.1%, 15.0%, 4.3% and 9.8%, in accuracy, sensitivity, specificity and f 1
-score, respectively. similar results for the tasks of molecular marker and
histology prediction are observed in the right panel of table 2 with rocs in
fig. 3, indicating the effectiveness of the proposed lc loss.3) dcc loss. from
table 1, we observe that the proposed dcc loss improves the performance in terms
of accuracy by 9.1%. similar results can be found for sensitivity, specificity
and f 1 -score. from table 2, we observe that the auc decreases 2.9%, 2.9%, 0.5%
and 2.8% for the prediction of idh, 1p/19q, cdkn and nmp, respectively, when
removing the dcc loss. such performance is also found in comparing the rocs in
fig. 3, suggesting the importance of the dcc loss for all the tasks.
the paradigm of pathology diagnosis has shifted to integrating molecular makers
with histology features. in this paper, we aim to classify diffuse gliomas under
up-to-date diagnosis criteria, via jointly learning the tasks of molecular
marker prediction and histology classification. inputting histology wsis, our
model incorporates a novel hmt-mil framework to extract global information for
both predicting both molecular markers and histology. we also design a
cplc-graph network and a dcc loss to model both intra-omic and inter-omic
interactions. our experiments demonstrate that our model has achieved superior
performance over other state-of-the-art methods, serving as a potentially useful
tool for digital pathology based on wsis in the era of molecular pathology.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43990-2_52.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43990-2_48.
esophageal cancer is a significant contributor to cancer-related deaths globally
[3,15]. one effective treatment option is radiotherapy (rt), which utilizes
high-energy radiation to target cancerous cells [4]. to ensure optimal treatment
outcomes, both the cancerous region and the adjacent organ-at-risk (oar) must be
accurately delineated, to focus the high-energy radiation solely on the
cancerous area while protecting the oars from any harm. gross tumor volume (gtv)
represents the area of the tumor that can be identified with a high degree of
certainty and is of paramount importance in clinical practice.in the clinical
setting, patients may undergo a second round of rt treatment to achieve complete
tumor control when initial treatment fails to completely eradicate cancer [16].
however, the precise delineation of the gtv is laborintensive, and is restricted
to specialized hospitals with highly skilled rt experts. the automatic
identification of the esophagus presents inherent challenges due to its
elongated soft structure and ambiguous boundaries between it and adjacent organs
[12]. moreover, the automatic delineation of the gtv in the esophagus poses a
significant difficulty, primarily attributable to the low contrast between the
esophageal gtv and the neighboring tissue, as well as the limited
datasets.recently, advances in deep learning [21] have promoted research in
automatic esophageal gtv segmentation from computed tomography (ct) [18,19].
since the task is challenging, jin et al. [9,10] improve the segmentation
accuracy by incorporating additional information from paired positron emission
tomography (pet). nevertheless, such approaches require several imaging
modalities, which can be both costly and time-consuming, while disregarding any
knowledge from previous treatment or anatomical understanding. moreover, the
correlation between the first and second courses of rt is rarely investigated,
where detailed prior tumor information naturally exists in the previous rt
planning.in this paper, we present a comprehensive study on accurate gtv
delineation for the second course rt. we proposed a novel prior anatomy and rt
information enhanced second-course esophageal gtv segmentation network (artseg).
a region-preserving attention module (ram) is designed to effectively capture
the long-range prior knowledge in the esophageal structure, while preserving
regional tumor patterns. to the best of our knowledge, we are the first to
reveal the domain gap between the first and second courses for gtv segmentation,
and explicitly leverage prior information from the first course to improve gtv
segmentation performance in the second course.the medical images are labeled
sparsely, which are isolated by different tasks [20]. meanwhile, an ideal method
for automatic esophageal gtv segmentation in the second course of rt should
consider three key aspects: 1) changes in tumor volume after the first course of
rt, 2) the proliferation of cancerous cells from a tumor to neighboring healthy
cells, and 3) the anatomical-dependent our training approach leverages
multi-center datasets containing relevant annotations, that challenges the
network to retrieve information from e1 using the features from e2. the decoder
d utilizes the prior knowledge obtained from i1 and g1 to generate the mask
prediction. our training strategy leverages three datasets that introduce prior
knowledge to the network of the following three key aspects: 1) tumor volume
variation, 2) cancer cell proliferation, and 3) reliance of gtv on esophageal
anatomy.nature of gtv on esophageal locations. to achieve this, we efficiently
exploit knowledge from multi-center datasets that are not tailored for
second-course gtv segmentation. our training strategy does not specific to any
tasks but challenges the network to retrieve information from another encoder
with augmented inputs, which enables the network to learn from the above three
aspects. extensive quantitative and qualitative experiments validate our
designs.
in the first course of rt, a ct image denoted as i 1 is utilized to manually
delineate the esophageal gtv, g 1 . during the second course of rt, a ct image i
2 of the same patient is acquired. however, i 2 is not aligned with i 1 due to
soft tissue movement and changes in tumor volume that occurred during the first
course of treatment. both images i 1/2 have the spatial shape of h × w × d.our
objective is to predict the esophageal gtv g 2 of the second course. it would be
advantageous to leverage insights from the first course, as it comprises
comprehensive information pertaining to the tumor in its preceding phase.
therefore, the input to encoder e 1 consists of the concatenation of i 1 and g 1
to encode the prior information (features f d 1 ) from the first course, while
encoder e 2 embeds both low-and high-level features f d 2 of the local pattern
of i 2 (fig. 1),where the spatial shape of, with 2 d+4 channels.
region-preserving attention module. to effectively learn the prior knowledge in
the elongated esophagus, we design a region-preserving attention module (ram),
as shown in fig. 1. the multi-head attention (mha) [17] is employed to gather
long-range informative values in f d 1 with f d 2 as queries and f d 1 as keys.
the features f d 1/2 are reshaped to hw d 2 3d × c before passed to the mha,
where c is the channel dimension. the attentive features f d a can be formulated
as:since mha perturbs the positional information, we preserve the tumor local
patterns by concatenating original features to the attentive features at the
channel dimension, followed by a 1 × 1 × 1 bottleneck convolution ξ 1×1×1 to
squeeze the channel features (named as ram), as shown in the following
equations,where the lower-level features from both encoders are fused by
concatenation.the decoder d generates a probabilistic prediction) with skip
connections (fig. 1). we utilize the 3d dice [14] loss function,
the network should learn from three aspects: 1) tumor volume variation: the
structural changes of the tumor from the first to the second course; 2) cancer
cell proliferation: the tumor in esophageal cancer tends to infiltrate into the
adjacent tissue; 3) reliance of gtv on esophageal anatomy: the anatomical
dependency between esophageal gtv and the position of the esophagus. medical
images are sparsely labeled which are isolated by different tasks [20], and are
often inadequate. in this study, we use a paired first-second course gtv dataset
s p , an unpaired gtv dataset s v , and a public esophagus dataset s e .in order
to fully leverage both public and private datasets, the training objective
should not be specific to any tasks. here, we denote g 1 /g 2 as prior/target
annotations respectively, which are not limited only to the gtv areas. as shown
in fig. 1, our strategy is to challenge the network to retrieve information from
augmented inputs in e 1 using the features from e 2 , which can incorporate a
wide range of datasets that are not tailored for second-course gtv segmentation.
the differences in tumor volume between the first and second courses following
an rt treatment can have a negative impact on the state-of-the-art (sota)
learning-based techniques, which will be discussed in sect. 4.2. to adequately
monitor changes in tumor volume and integrate information from the initial
course into the subsequent course, a paired first-second courses dataset s p =
{i 1 p , i 2 p , g 1 p ; g 2 p } is necessary for training. in s p , i 1 p and i
2 p are the first and second course ct images, while g 1 p and g 2 p are the
corresponding gtv annotations.
the paired dataset s p for the first and second courses is limited, whereas an
unpaired gtv dataset s v = {i v ; g v } can be easily obtained in a standard
clinical workflow with a substantial amount. s v lacks its counterpart for the
second course, in which i v /g v are the ct image and the corresponding
annotation for gtv. to address this, we apply two distinct randomized
augmentations, p 1 , p 2 , to mimic the unregistered issue of the first and
second course ct. the transformed data is feed into the encoders e 1/2 as shown
in the following equations:, p 1 (g e ), p 2 (i e ), p 2 (g e ), when i e , g e
∈ s e .(4)the esophageal tumor can proliferate with varying morphologies into
the surrounding tissues. although not paired, s v contains valuable information
about the tumor. challenging the network to query information within gtv will
enhance the capacity to retrieve pertinent information for the tumor positions.
to make full use of the datasets of relevant tasks, we incorporate a public
esophagus segmentation dataset, denoted as s e = {i e ; g e }, where i e /g e
represent the ct images and corresponding annotations of the esophagus
structure. by augmenting the data as described in eq. ( 4), s e challenges the
network to extract information from the entire esophagus, which enhances the
network's embedding space with anatomical prior knowledge of the esophagus.
similarly, data from the paired s p is also augmented by p 1/2 to increase the
network's robustness.in summary, our training strategy is not dataset-specific
or target-specific, thus allowing the integration of prior knowledge from
multi-center esophageal gtv-related datasets, which effectively improves the
network's ability to retrieve information for the second course from the three
key aspects stated in sect. 3.
datasets. the paired first-second course dataset, s p , is collected from sun
yat-sen university cancer center (ethics approval number: b2023-107-01),
comprising paired ct scans of 69 distinct patients from south china. we
collected the gtv dataset s v from medmind technology co., ltd., which has ct
scans from 179 patients. for both s p and s v , physicians annotated the
esophageal cancer gtv in each ct. the gtv volume statistics (cm 3 , mean ± std.)
in s v is 40.60 ± 29.75, and is 83.70 ± 55.97/71.66 ± 49.36 for the first/second
course rt in s p respectively. additionally, we collect s e from segthor [12],
consisting of ct scans and esophagus annotations from 40 patients who did not
implementation details. the ct volumes from the first and second course in s p
are aligned based on the center of the lung mask [8]. the ct volumes are applied
with a windowing of [-100, 300] hu, and resampled to 128 3 , with a voxel size
of 1.2 × 1.2 × 3 mm 3 . the augmentations p 1/2 involve a combination of random
3d resized cropping, flipping, rotation in the transverse plane, and gaussian
noise. we employ the adam [11] optimizer with (β 1 , β 2 , lr) = (0.9, 0.999,
0.001) for training for 500 epoches. the network is implemented using pytorch
[2] and monai [1], and detailed configurations are in the supplementary
material. experiments are performed on an nvidia rtx 3090 gpu with 24gb
memory.performance metrics. dice score (dsc), averaged surface distance (asd)
and hausdorff distance (hsd) are used as metrics for evaluation. the wilcoxon
signed-rank test is used to compare the performance of different methods.
as previously mentioned, the volume of the tumors changes after the first course
of rt. to demonstrate the presence of a domain gap between the first and second
courses, we train sota methods with datasets s train p and s v , by feeding the
data sequentially into the network. we then evaluate the models on s test p .
the results presented in table 1 indicate a performance gap between gtv
segmentation in the first and second courses, with the latter being more
challenging. notably, the paired first-second course dataset s test p pertains
to the same group of patients, thereby ensuring that any performance drop can be
attributed solely to differences in courses of rt, rather than variations across
different patients.figure 2 illustrates the reduction in the gtv area after the
initial course of rt, where the transverse plane is taken from the same location
relative to the vertebrae (yellow lines). the blue arrows indicate that the
networks failed to track these changes and produced false predictions in the
second course of rt. this suggests that deep learning-based approaches may not
rely solely on the identification of malignant tissue patterns, as doctors do,
but rather predict highrisk areas statistically. therefore, for accurate
second-course gtv segmentation, we need to explicitly propagate prior
information from the first course using dual encoders in artseg, and incorporate
learning about tumor changes.
combination of various datasets. table 2 presents the information gain derived
from multi-center datasets using quantified metrics for segmentation
performance. we first utilize a standard artseg (w/o ram) as an ablation
network. when prior information from the first course is explicitly introduced
using s p , artseg outperforms other baselines for gtv segmentation in the
second course, which reaches a dsc of 66.73%. however, in fig. 3, it can be
observed that the model failed to accurately track the gtv area along the
esophagus (orange arrows) due to the soft and elongated nature of the esophageal
tissue, which deforms easily during ct scans performed at different times.by
subsequently incorporating s e for structural esophagus prior knowledge, the dsc
improved to 69.42%. meanwhile, the esophageal tumor comprises two primary
regions, the original part located in the esophagus and the extended part that
has invaded the surrounding tissue. as shown in fig. 3, identifying the tumor
proliferation into the surrounding tissue without comprehensive knowledge of
tumor morphology can be challenging (blue arrows). to address this,
incorporating s v to comprehensively learn the tumor morphology is required.when
s v is incorporated for learning tumor proliferation, the dsc improved to
72.64%. we can observe from case 2 in fig. 3 that the network has a better
understanding of the tumor proliferation with s v , while it still fails to
track the gtv area along the esophagus as pointed by the orange arrow.
therefore, s v and s e improve the network from two distinct aspects and are
both valuable. our proposed training strategy fully exploits the datasets s p ,
s v , and s e , and further improve the dsc to 74.54% by utilizing comprehensive
knowledge of both the tumor morphology and esophageal
structures.region-preserving attention module. although introducing the
esophageal structural prior knowledge using s e can improve the performance in
dsc and asd (table 2), the increase in hsd (38.22 to 47.89 mm; 21.71 to 27.00
mm) indicates that there are outliers far from the ground truth boundaries. this
may be attributed to the convolution that cannot effectively handle the
long-range knowledge of the esophagus structure. the attention mechanism can
effectively capture the long-range relationship as shown recently in
[13].however, there is no performance gain with mha as shown in table 2, and the
hsd further increased to 27.33 mm. we attribute the drawback is due to the
location-agnostic nature of the operations in mha, where the local regional
correlations are perturbed.to tackle the aforementioned problem, we propose ram
which involves the concatenation of the original features with attention
outputs, allowing for the preservation of convolution-generated regional tumor
patterns while effectively comprehending long-range prior knowledge specific to
the esophagus. finally, our proposed artseg with ram achieves the best dsc/hsd
of 75.26%/19.75 mm, and outperforms its ablations as well as other baselines, as
shown in table 2.limitations. for the method's generalizability, analysis of
diverse imaging protocols and segmentation backbones are inadequate. besides,
artseg requires more computational resources due to its dual-encoder and
attention design.
in this paper, we reveal the domain gap between the first and second courses of
rt for esophageal gtv segmentation. to improve the accuracy of gtv declination
in the second course, we explicitly incorporated the naturally existing prior
information from the first course. besides, to efficiently leverage prior
knowledge contained in various medical ct datasets, we train the network in an
information-querying manner. we proposed ram to capture long-range prior
knowledge in the esophageal structure, while preserving the regional tumor
patterns. our proposed artseg incorporates prior knowledge of the tumor volume
variation, cancer cell proliferation, and reliance of gtv on esophageal anatomy,
which enhances the gtv segmentation accuracy in the second course rt. our future
research includes accurate delineation for multiple targets in the second course
and knowledge transferring through the time series of multiple courses.
asd (mm) ↓ dsc (%) ↑ asd (mm) ↓ mean ± std. med. mean ± std. med. mean ± std.
med. mean ± std. med. unetr [7] 59.77 ± 20.24 62.90 10.57 ± 14.66 7.06 53.03 ±
17.62* 55.17 11.29 ± 11.44 8.42 swin unetr [6] 60.84 ± 19.74 64.07 10.29 ± 17.78
6.67 57.04 ± 20.16* 60.73 9.76 ± 15.43 6.21 denseunet [19] 63.95 ± 18.23 68.11
8.94 ± 13.82 6.04 55.35 ± 18.59* 58.54 9.84 ± 6.91* 8.54 3d u-net [5] 66.73 ±
17.21 69.86 8.04 ± 16.83 4.19 57.50 ± 19.49* 62.62 9.14 ± 12.03* 6.09
lung cancer is one of the most fatal diseases worldwide, and early diagnosis of
the pulmonary nodule has been identified as an effective measure to prevent lung
cancer. deep learning-based methods for lung nodule classification have been
widely studied in recent years [9,12]. usually, the malignancy prediction is
often formulated as benign-malignant binary classification [9,10,19], and the
higher classification performance and explainable attention maps are impressive.
most previous works employ a learning paradigm that utilizes cross-entropy loss
between predicted probability distributions and ground-truth one-hot labels.
furthermore, inspired by ordered labels of nodule progression, researchers have
turned their attention to ordinal regression methods to evaluate the
benignunsure-malignant classification task [2,11,13,18,21], where the training
set additionally includes nodules with uncertain labels. indeed, the ordinal
regressionbased methods are able to learn ordered manifolds and to further
enhance the prediction accuracy.however, the aforementioned methods still face
challenges in distinguishing visually similar samples with adjacent rank labels.
for example, in fig. 1(a), since we conduct unimodal contrastive learning and
map the samples onto a spherical space, the false positive nodule with a
malignancy score of 2.75 has a closer distance to that with a score of 4.75, and
the false negative one should not be closer to that of score 2.5. to address
this issue, we found that the text attributes, such as "subtlety", "sphericity",
"margin", and "lobulation", annotated by radiologists, can exhibit the
differences between these hard samples. therefore, we propose leveraging text
annotations to guide the learning of visual features. in practice, this also
aligns with the fact that the annotated text information represents the direct
justification for identifying lesion regions in the clinic. as shown in fig. 1,
this text information is beneficial for distinguishing visually similar pairs,
while we conduct this behavior by applying contrastive learning that pulls
semantic-closer samples and pushes away semantic-farther ones.to integrate text
annotations into the image-domain learning process, an effective text encoder
providing accurate textual features is required. fortunately, recent advances in
vision-language models, such as contrastive languageimage pre-training (clip)
[16], provide us with a powerful text encoder pre-trained with text-based
supervisions and have shown impressive results in downstream vision tasks.
nevertheless, it is ineffective to directly transfer clip to medical tasks due
to the data covariate shift. therefore, in this paper, we pro- pose clip-lung, a
framework to classify lung nodules using image-text pairs. specifically,
clip-lung constructs learnable text descriptions for each nodule from both class
and attribute perspectives. inspired by cocoop [20], we propose a channel-wise
conditional prompt (ccp) module to allow nodule descriptions to guide the
generation of informative feature maps. different from cocoop, ccp constructs
specific learnable prompts conditioned on grouped feature maps and triggers more
explainable attention maps such as grad-cam [17], whereas cocoop provides only
the common condition for all the prompt tokens. then, we design a textual
knowledge-guided contrastive learning based on obtained image features and
textual features involving classes and attributes. experimental results on
lidc-idri [1] dataset demonstrate the effectiveness of learning with textual
knowledge for improving lung nodule malignancy prediction.the contributions of
this paper are summarized as follows.1) we propose clip-lung for lung nodule
malignancy prediction, which leverages clinical textual knowledge to enhance the
image encoder and classifier. 2) we design a channel-wise conditional prompt
module to establish consistent relationships among the correlated text tokens
and feature maps. 3) we simultaneously align the image features with class and
attribute features through contrastive learning while generating more
explainable attention maps.
m=1 is the set of attribute embeddings, where each element a m ∈ r d×1 is a
vector representing the embedding of an attribute word such as "spiculation".
then, for a given sample {i i , y i }, our aim is to learn a mapping f θ : i i →
y i , where f is a deep neural network parameterized by θ. clip-lung. in fig.
2(a), the training framework contains an image encoder f θ and a text encoder g
φ . first, the input image i i is fed into f θ and then generates the feature
maps. according to fig. 2(b), the feature maps are converted to channel-wise
feature vectors f θ (i i ) = f t,: and then to learnable tokens l t . second, we
initialize the context tokens l t and add them with l t to construct the
learnable prompts, where t is the number of context words. next, the
concatenation of the class token and l t + l t is used as input of text encoder
yielding the class features g φ (c k ) = c k,: , note that c k,: is conditioned
on channel-wise feature vectors f t,: . finally, the attribute tokens a m are
also fed into the text encoder to yield corresponding attribute features g φ (a
m ) = a m,: . note that the vectors f t,: , l t,: , l t,: , and c k,: are with
the same dimension d = 512 in this paper. consequently, we have image feature f
∈ r t ×d , class feature c ∈ r k×d , and attribute feature a ∈ r m ×d to conduct
the textual knowledge-guided contrastive learning.
for the attribute annotations, all the lung nodules in the lidc-idri dataset are
annotated with the same eight attributes: "subtlety", "internal structure",
"calcification", "sphericity", "margin", "lobulation", "spiculation", and
"texture" [4,8], and the annotated value for each attribute ranges from 1 to 5
except for "calcification" that is ranged from 1 to 6. in this paper, we fix the
parameters of a pre-trained text encoder so that the generated eight text
feature vectors are the same for all the nodules. therefore, we propose an
instance-specific attribute weighting scheme to distinguish different nodules.
for the i-th sample, the weight for each a m is calculated through normalizing
the annotated values:where v m denotes the annotated value for a m . then the
weight vectors of the i-th sample is represented as. hence, the elementwise
multiplication w i • a i is unique to i i .
cocoop [20] firstly proposed to learn language contexts for vision-language
models conditioned on visual features. however, it is inferior to align context
words with partial regions of the lesion. therefore, we propose a channel-wise
conditional prompt (ccp) module, in fig. 2(b), to split latent feature maps into
t groups and then flatten them into vectors f t,: . next, we denote h(•) as a
context network that is composed of a multi-layer perceptron (mlp) with one
hidden layer, and each learnable context token is now obtained by l t = h(f t,:
). hence, the conditional prompt for the t-th token is l t +l t . in addition,
ccp also outputs the f t,: for image-class and image-attribute contrastive
learning.
recall that our aim is to enable the visual features to be similar to the
textual features of the annotated classes or attributes and be dissimilar to
those of irrelevant text annotations. consequently, we accomplish this goal
through contrastive learning [3,5,7]. in this paper, we conduct such image-text
contrastive learning by utilizing pre-trained clip text encoder [16]. in fig. 2,
we align f ∈ r t ×d with c ∈ r k×d and a ∈ r m ×d , i.e., using class and
attribute knowledge to regularize the feature maps.image-class alignment. first,
the same to clip, we align the image and class information by minimizing the
cross-entropy (ce) loss for the sample {i i , y i }:whereand " " denotes
concatenation, i.e., c k,: is conditioned on learnable prompts l t + l t . σ(•,
•) calculates the cosine similarity and τ is the temperature term. therefore, l
ic implements the contrastive learning between channel-wise features and
corresponding class features, i.e., the ensemble of grouped image-class
alignment results. image-attribute alignment. in addition to image-class
alignment, we further expect the image features to correlate with specific
attributes. so we conduct image-attribute alignment by minimizing the infonce
loss [5,16]:hence, l ia indicates which attribute the f t,: is closest to since
each vector f t,: is mapped from the t-th group of feature maps through the
context network h(•). therefore, certain feature maps can be guided by specific
annotated attributes. class-attribute alignment. although the image features
have been aligned with classes and attributes, the class embeddings obtained by
the pre-trained clip encoder may shift in the latent space, which may result in
inconsistent class space and attribute space, i.e., annotated attributes do not
match the corresponding classes, which is contradictory to the actual clinical
diagnosis. to avoid this weakness, we further align the class and attribute
features:and this loss implies semantic consistency between classes and
attributes.finally, the total loss function is defined as follows:where α and β
are hyperparameters for adjusting the losses and are set as 1 and 0.5,
respectively. l ce denotes the cross-entropy loss between predicted
probabilities obtained by the classifier and the ground-truth labels. note that
during the inference phase, test images are only fed into the trained image
encoder and classifier. as a result, clip-lung does not introduce any additional
computational overhead in inference.
dataset. lidc-idri [1] is a dataset for pulmonary nodule classification or
detection based on low-dose ct, which involves 1,010 patients. according to the
annotations, we extracted 2, 026 nodules, and all of them were labeled with
scores from 1 to 5, indicating the malignancy progression. we cropped all the
nodules with a square shape of a doubled equivalent diameter at the annotated
center, then resized them to the volume of 32 × 32 × 32. following [9,11], we
modified the first layer of the image encoder to be with 32 channels. according
to existing works [11,18], we regard a nodule with an average score between 2.5
and 3.5 as unsure nodules, benign and malignant categories are those with scores
lower than 2.5 and larger than 3.5, respectively. in this paper, we construct
three sub-datasets: lidc-a contains three classes of nodules both in training
and test sets; according to [11], we construct the lidc-b, which contains three
classes of nodules only in the training set, and the test set contains benign
and malignant nodules; lidc-c includes benign and malignant nodules both in
training and test sets.experimental settings. in this paper, we apply the clip
pre-trained vit-b/16 as the text encoder for clip-lung, and the image encoder we
used is resnet-18 [6] due to the relatively smaller scale of training data. the
image encoder is initialized randomly. note that for the text branch, we froze
the parameters of the text encoder and updated the learnable tokens l and l
during training. the learning rate is 0.001 following the cosine decay, while
the optimizer is stochastic gradient descent with momentum 0.9 and weight decay
0.00005. the temperature τ is initialized as 0.07 and updated during training.
all of our experiments are implemented with pytorch [15] and trained with nvidia
a100 gpus. the experimental results are reported with average values through
five-fold cross-validation. we report the recall and f1-score values for
different classes and use "±" to indicate standard deviation.
performance comparisons. in table 1, we compare the classification performances
on the lidc-a dataset, where we regard the benign-unsure-malignant we argue that
this is due to the indistinguishable textual annotations, such as similar
attributes of different nodules. in addition, we verify the effect of textual
branch of clip-lung using mv-dar [12] on lidc-a dataset. the obtained accuracy
values with and without the textual branch are 58.9% and 57.3%, respectively,
demonstrating the effectiveness of integrating textual knowledge. table 2
presents a performance comparison of clip-lung on the lidc-b and lidc-c
datasets. notably, clip-lung obtains higher evaluation values other than recalls
of benign class. this disparity is likely attributed to the similarity in
appearances and subtle variations in text attributes among the benign nodules.
consequently, aligning these distinct feature types becomes challenging,
resulting in a bias towards the text features associated with malignant nodules.
visual features and attention maps. to illustrate the influence of incorporating
class and attribute knowledge, we provide the t-sne [14] and grad-cam [17]
results obtained by clip, cocoop, and clip-lung. in fig. 3, we can see that clip
yields a non-compact latent space for two kinds of nodules.cocoop and clip-lung
alleviate this phenomenon, which demonstrates that the learnable prompts guided
by nodule classes are more effective than fixed prompt engineering. unlike
clip-lung, cocoop does not incorporate attribute information in prompt learning,
leading to increased false negatives in the latent space. from the attention
maps, we can observe that clip cannot precisely capture spiculation and
lobulation regions that are highly correlated with malignancy. simultaneously,
our clip-lung performs better than cocoop, which demonstrates the guidance from
textual descriptions such as "spiculation".ablation studies. in table 3, we
verify the effectiveness of different loss components on the three constructed
datasets. based on l ic , l ia and l ca improve the performances on lidc-a,
indicating the effectiveness of capturing fine-grained features of ordinal ranks
using class and attribute texts. however, they perform relatively worse on
lidc-b and lidc-c, especially the l ic + l ca . that is to say, l ia is more
important in latent space rectification, i.e., image-attribute consistency. in
addition, we observe that l ic +l ia performs better than l ia +l ca , which is
attributed to that l ca regularizes the image features indirectly.
in this paper, we proposed a textual knowledge-guided framework for pulmonary
nodule classification, named clip-lung. we explored the utilization of clinical
textual annotations based on large-scale pre-trained text encoders. clip-lung
aligned the different modalities of features generated from nodule classes,
attributes, and images through contrastive learning. most importantly, clip-lung
establishes correlations between learnable prompt tokens and feature maps using
the proposed ccp module, and this guarantees explainable attention maps
localizing fine-grained clinical features. finally, clip-lung outperforms
compared methods, including clip on lidc-idri benchmark. future work will focus
on extending clip-lung with more diverse textual knowledge.
i=1 is the corresponding class label set and y i ∈ {1, 2, . . . , k}, and k is
the number of
lic lia lca lidc-a lidc-b lidc-c 56.8 ± 0.6 86.8 ± 0.7 88.2 ± 0.6 59.4 ± 0.4
86.8 ± 0.6 86.7 ± 0.4 58.1 ± 0.2 85.7 ± 0.6 87.5 ± 0.5 56.9 ± 0.3 84.7 ± 0.4
84.0 ± 0.7 60.9 ± 0.4 87.5 ± 0.5 89.5 ± 0.4
natural science foundation of shanghai (no. 21zr1403600), national natural
science foundation of china (nos. 62101136 and 62176059), china postdoctoral
science foundation (no. 2022tq0069), shanghai municipal of science and
technology project (no. 20jc1419500), and shanghai center for brain science and
brain-inspired technology.
skin cancer is a serious and widespread form of cancer that requires early
detection for successful treatment. computer-aided diagnosis systems (cad) using
deep learning models have shown promise in accurate and efficient skin lesion
diagnosis. however, recent research has revealed that the success of these
models may be a result of overly relying on "spurious cues" in dermoscopic
images, such as rulers, gel bubbles, dark corners, and hairs [3][4][5]29], which
leads to unreliable diagnoses. when a deep learning model overfits specific
artifacts instead of learning the correct dermoscopic patterns, it may fail to
identify skin lesions in real-world environments where the artifacts are absent
or inconsistent.to alleviate the artifact bias and enhance the model's
generalization ability, we rethink the problem from the domain generalization
(dg) perspective, where a model trained within multiple different but related
domains are expected to perform well in unseen test domains. as illustrated in
fig. 1, we define the domain labels based on the types of artifacts present in
the training images, which can provide environment-aware prior knowledge
reflecting a range of noisy contexts. by doing this, we can develop a dg
algorithm to learn the generalized and robust features from diverse
domains.previous dg algorithms learning domain-invariant features from source
domains have succeeded in natural image tasks [2,17,19], but cannot directly
apply to medical images, in particular skin images, due to the vast cross-domain
diversity of skin lesions in terms of shapes, colors, textures, etc. as each
domain contains ad hoc intrinsic knowledge, learning domain-invariant features
is highly challenging. one promising way is, as suggested in some recent works
[7,24,32], exploiting multiple learnable domain experts (e.g., batch norm
statistic, auxiliary classifiers, etc.) to capture domain-specific knowledge
from different source domains individually. still, two significant challenges
remain. first, previous work only exploits some weak experts, like the batch
norm, to capture knowledge, which naturally hampers the capability of capturing
essential domain-specific knowledge. second, previous methods such as [30]
focused on learning domain knowledge independently while overlooking the rich
cross-domain information that all domain experts can contribute collectively for
the target domain prediction.to overcome the above problems, we propose an
environment-aware prompt vision transformer (epvt) for domain generalization of
skin lesion recognition. on the one hand, inspired by the emerging prompt
learning techniques that embed prompts into a model for adaptation to diverse
downstream tasks [12,26,31], we construct different prompt vectors to strengthen
the learning of domainspecific knowledge for adaptation to diverse domains.
then, the self-attention mechanism of the vision transformer (vit) [8] is
adopted to fully model the relationship between image tokens and prompt vectors.
on the other hand, to encourage cross-domain information sharing while
preserving the domain-specific knowledge of each domain prompt, we propose a
domain prompt generator based on low-rank weights updating. the prompt generator
enables multiple domain prompts to work collaboratively and benefit from each
other for generalization to unknown domains. additionally, we devise a domain
mixup strategy to resolve the problem of co-occurring artifacts in dermoscopic
images and mitigate the resulting noisy domain label assignments.our
contributions can be summarized as: (1) we resolve an artifacts-derived biasing
problem in skin cancer diagnosis using a novel environment-aware prompt
learning-based dg algorithm, epvt; (2) epvt takes advantage of a vitbased
domain-aware prompt learning and a novel domain prompt generator to improve
domain-specific and cross-domain knowledge learning simultaneously;(3) a domain
mixup strategy is devised to reduce the co-artifacts specific to dermoscopic
images; (4) extensive experiments on four out-of-distribution skin datasets and
six biased isic datasets demonstrate the outperforming generalization ability
and robustness of epvt under heterogeneous distribution shifts.
in domain generalization (dg), the training dataset d train consists of m source
domains, denoted as d train = {d k |k = 1, ..., m }. here, each source domain d
k is represented by n labeled instances {(x k j , y k j )} n j=1 . the goal of
dg is to learn a model g : x → y from the m source domains so that it can
generalize well in unseen target domains d test . the overall architecture of
our proposed model, epvt, is shown in fig. 2a. we will illustrate its details in
the following sections.
to enable the pre-trained vision transformer (vit) to capture knowledge from
different domains, as shown in fig. 2a, we define a set of m learnable domain
prompts produced by a domain prompt generator (introduced in sect. 2.2), denoted
as, where d is the same size as the feature embedding of the vit and each prompt
p m corresponds to one domain (i.e. dark corners). to incorporate these prompts
into the model, we follow the conventional practice of visual prompt tuning
[12], which prepends the prompts p d into the first layer of the transformer.
particularly, for each prompt p m in p d , we extract the domain-specific
features as:where f is the feature encoder of the vit, x 0 denotes the class
token, e 0 is the image patch embedding, f m is the feature extracted by vit
with the m-th prompt, and 0 is the index of the first layer. domain prompts p d
are a set of learnable tokens, with each prompt p m being fed into the vision
transformer along with the image and corresponding class tokens from a specific
domain.through optimizing, each prompt becomes a domain expert only responsible
for the images from its own domain. by the self-attention mechanism of vit, the
model can effectively capture domain-specific knowledge from the domain prompt
tokens.
to facilitate effective knowledge sharing across different domains while
maintaining its own parameters of each domain prompt, we propose a domain prompt
generator, as depicted in fig. 2b. our approach is inspired by model adaptation
and multi-task learning techniques used in natural language processing [13,26].
aghajanyan et al. [1] have shown that when adapting a model to a specific task,
the updates to weights possess a low intrinsic rank. similarly, each domain
prompt p m should also have a unique low intrinsic rank when learning knowledge
from its own domain. to this end, we decompose each p m into a hadamard product
between a randomly initialized shared prompt p * and a rank-one matrix p k
obtained from two randomly initialized learnable vectors u k and v k , which
is:where p m represents the domain-specific prompt, computed by hadamard product
of p * and p k . here, p * ∈ r s×d is utilized to learn general knowledge, with
s and d representing the dimensions of the prompt vector and feature embedding
respectively. on the other hand, p k is computed using domain-specific trainable
vectors: u k ∈ r s and v k ∈ r d . these vectors capture domain-specific
information in a low-rank space. the decomposition of domain prompts into
rank-one subspaces ensures that the model effectively encodes domain-specific
information. by using the hadamard product, the model can efficiently leverage
cross-domain knowledge for target domain prediction.
the artifacts-based domain labels can provide domain information for dermoscopic
images. however, a non-trivial issue arises due to the possible cooccurrence of
different artifacts from other domains within each domain. to address this
issue, we employ a domain mixup strategy [27,28]. instead of assigning a hard
prediction label ("0" or "1") to each image, in each batch, we mix every image
using two randomly selected images from two different domains. this allows us to
learn a flexible margin relative to both domains. we then apply the
cross-entropy loss to the corresponding labels of bot images, as shown in fig.
2c and can be represented by the following equation:where x mix = λx k i +(1-λ)x
q j ; x k i and x q j are samples from two different domains k and q, and y k i
and y q j are the corresponding labels. this strategy can overcome the challenge
of ambiguous domain labels in dermoscopic images and improve the performance of
our model.
so far, we have introduced l mixup in eq. 3 for optimizing our model. however,
since our goal is to generalize the model to unseen environments, we also need
to take advantage of each domain prompt. instead of assigning equal weights to
each domain prompt, we employ an adapter [30] that learns the linear correlation
between the domain prompts and the target image prediction. to obtain the
adapted prompt for inference in the target domain, we define it as a linear
combination of the source domain prompts:where a represents an adapter
containing a two-layer mlp with a softmax layer, and w m denotes the learned
weights.to train the adapter a, we simulate the inference process for each image
in the source domain by treating it as an image from the pseudo-target
domain.specifically, we first extract features from the vit: fm (x) = f ([x 0 ,
e 0 ]). then we calculated the adapted prompt p adapted for the pseudo-target
environment image x using the adapter a: p adapted = a( fm (x)). next, we
extract features from vit using the adapted prompt: fm (x) = f ([ fm (x), p
adapted , e 0 ]). finally, the classification head h is applied to predict the
label y: y = h( fm (x)). additionally, the inferece process is the same as the
simulated inference process and our final prediction will be conditioned on the
adapted prompt p adapted .to ensure that the adapter learns the correct linear
correlation between the domain prompts and the target image, we use the domain
label from source domains to directly supervise the weights w m . we also use
the cross-entropy loss to maintain the model performance with the adapted
prompt:(5) where fm (x) is the obtained feature map conditioned on the adapted
prompt p adapted , and h is the classification head. the total loss is then
defined as l total = l mixup + l adapted .
experimental setup: we consider two challenging melanoma-benign classification
settings that can effectively evaluate the generalization ability of our model
in different environments and closely mimic real-world scenarios. (1)
outof-distribution evaluation: the task is to evaluate the model on test sets
that contain different artifacts or attributes compared to the training set. we
train and validate all algorithms on isic2019 [6] dataset, following the split
of [3]. we use the artifacts annotations from [3] and divide the training set of
isic2019 into five groups: dark corner, hair, gel bubble, ruler, and clean, with
2351, 4884, 1640, 672, and 2796 images, respectively. we evaluate models on four
out-of-distribution (ood) datasets, including derm7pt-dermoscopic [14],
derm7pt-clinical [14], ph2 [18], and pad-ufes-20 [21]. it's worth noting that
isic2019, derm7pt-dermoscopic, and ph2 are dermoscopic images, while
derm7pt-clinical and pad are clinical images. (2) trap set debiasing: we train
and test our epvt with its baseline on six trap sets [3] with increasing bias
levels, ranging from 0 (randomly split training and testing sets from the
isic2019 dataset) to 1 (the highest bias level where the correlation between
artifacts and class label is in the opposite direction in the dataset splits).
more details about these datasets and splits are provided in the complementary
material.implementation details: for a fair comparison, we train all models
using vit-base/16 [8] backbone pre-trained on imagenet and report the roc-auc
with five random seeds. hyperparameter and model selection methods are crucial
for domain generalization algorithms. we conduct a grid search over learning
rate (from 3e -4 to 5e -6 ), weight decay (from 1e -2 to 1e -5 ), and the length
of the prompt (from 4 to 16, when available) and report the best performance of
all models. we employ the training-domain validation set method [11] for model
selection. after the grid search, we use the adamw optimizer with a learning
rate of 5e -6 and a weight decay of 1e -2 . the batch size is 130, and the
length of the prompt is 10. we resize the input image to a size of 224 × 224 and
adopt the standard data augmentation like random flip, crop, rotation, and color
jitter. an early stopping with the patience of 22 is set and with a total of 60
epochs for ood evaluation and 100 epochs for trap set debiasing. all experiments
are conducted on a single nvidia rtx 3090 gpu.out-of-distribution evaluation:
table 1 presents a comprehensive comparison of our epvt algorithm with existing
domain generalization methods. the results clearly demonstrate the superiority
of our approach, with the best performance on three out of four ood datasets and
remarkable improvements over the erm algorithm, especially achieving 4.1% and
8.9% improvement on the pad and ph2 datasets, respectively. although some
algorithms may perform similarly to our model on one of the four datasets, none
can consistently match the performance of our method across all four datasets.
particularly, our approach showcases the highest average performance, with a
2.05% improvement over the second-best algorithm across all four datasets. these
findings highlight the effectiveness of our algorithm in learning robust
features and its strong generalization abilities across diverse environments.
we perform ablation studies to analyze each component of our model, as shown in
table 2. we set our baseline as the empirical risk minimization (erm) algorithm,
and we gradually add p (prompt [12]), a (adapter), m (mixup), and g (domain
prompt generator) into the model. firstly, we observe that the baseline model
with prompt only improves the average performance by 0.1%, showing that simply
combining prompt does not very helpful for domain generalization. when we
combine the adapter, the model's average performance improves by 1.37%, but it
performs worse than erm on pad dataset. subsequently, we added domain mixup and
domain prompt generator to the model, resulting in significant further
improvements in the model's average performance by 1.32% and 1.69%,
respectively. the consistently better performance than the baseline on all four
datasets also highlights the importance of addressing coartifacts and
cross-domain learning for dg in skin lesion recognition.trap set debiasing: in
fig. 3a, we present the performance of the erm baseline and our epvt on six
biased isic2019 datasets. each point on the graph represents an algorithm that
is trained and tested on a specific bias degree split.the graph shows that the
erm baseline performs better than our epvt when the bias is low (0 and 0.3).
however, this is because erm relies heavily on spurious correlations between
artifacts and class labels, leading to overfitting on the training set. as the
bias degree increases, the correlation between artifacts and class labels
decreases, and overfitting the train set causes the performance of erm to drop
dramatically on the test set with a significant distribution difference. in
contrast, our epvt exhibits greater robustness to different bias levels.
notably, our epvt outperforms the erm baseline by 9.4% on the bias 1
dataset.prompt weights analysis: to verify whether our model has learned the
correct domain prompts for target domain prediction, we analyze and plot the
results in fig. 3b and3c. firstly, we extract the features of each domain from
our training set and extract the feature from one target dataset,
derm7pt-clin.we then calculate the frechet distance [9] between each domain and
the target dataset using the extracted feature, representing the domain distance
between them. the results are recorded in fig. 3b. next, we record the learned
weights of each domain prompt in fig. 3c; it shows that our model assigns the
highest weight to the "dark corner" group, as the domain distance between "dark
corner" and derm7pt-clin is the closest, as shown in fig. 3b. this suggests that
they share the most similar domain information. further, the "clean" group is
assigned the smallest weight as the domain distance between them is the largest,
indicating that their domains are significantly different and contain less
useful information for target domain prediction. in summary, we observe a
negative correlation between domain distance and the prompt's weights,
indicating that our model can learn the correct knowledge from different domains
precisely.
in this paper, we propose a novel dg algorithm called epvt for robust skin
lesion recognition. our approach addresses the co-artifacts problem using a
domain mixup strategy and cross-domain learning problems using a domain prompt
generator. compared to other competitive domain generalization algorithms, our
method achieves outstanding results on three out of four ood datasets and the
second-best on the remaining one. additionally, we conducted a debiasing
experiment that highlights the shortcomings of conventional training using
empirical risk minimization, which leads to overfitting in dermoscopic images
due to artifacts. in contrast, our epvt model effectively reduces overfitting
and consistently performs better in different biased environments.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43990-2_24.
computer-aided diagnosis (cad) systems have achieved success in many clinical
tasks [5,6,12,17]. most cad studies were developed on regular and selected
datasets in the laboratory environment, which avoided the problems (data noise,
missing data, etc.) in the clinical scenarios [3,6,9,13,18]. in a real clinical
scenario, the clinicians generally synthesize all aspects of information, and
conduct consultations with multidisciplinary team (mdt), to accurately diagnose
and plan the treatment [9,10,13]. real-world studies have received increasing
attention [11,16], and it is challenging for the cad in the real-world scenarios
as: 1) consistent with the clinical workflow, cad needs to consider
multidisciplinary information to obtain multidimensional diagnosis; 2) due to
information collection, storage and manual evaluation, there are missing and
noisy medical data. this phenomenon is especially common in rare tumors like
pancreatic neuroendocrine neoplasms (pnens).in order to overcome above
challenges, some studies [3,9,13,18] used multilabel method because of the
following advantages: 1) the input of the model is only a single modality such
as images, which is easy to apply clinically; 2) the model learns multi-label
and multi-disciplinary knowledge, which is consistent with clinical logic; 3)
multi-label simultaneous prediction, which meets the need of clinical
multi-dimensional description of patients. for the above advantages, multi-label
technology is suitable for real-world cad. the previous multi-label cad studies
were designed based on simple parameter sharing methods [9,15,20] or graph
neural network (gnn) method [2]. the former implicitly interacts with
multi-label information, making it difficult to fully utilize the correlation
among labels; and the latter requires the use of word embeddings pre-trained on
public databases, which is not friendly to many medical domain proper nouns. the
generalizability of previous multi-label cad studies is poor due to these
disadvantages. in addition, none of the current multi-label cad studies have
considered the problem of missing labels and noisy labels.considering these
real-world challenges, we propose a multi-label model named self-feedback
transformer (sft), and validate our method on a realworld pnens dataset. the
main contributions of this work are listed: 1) a transformer multi-label model
based on self-feedback mechanism was proposed, which provided a novel method for
multi-label tasks in real-world medical application; 2) the structure is
flexibility and interactivity to meet the needs of realworld clinical
application by using four inference modes, such as expert-machine combination
mode, etc.; 3) sft has good noise resistance, and can maintain good performance
under noisy label input in expert-assisted mode.
transformer has achieved success in many fields [4,19]. inspired by detr [1] and
c-tran [8], we propose a multi-label model based on transformer and selffeedback
mechanism. as shown in fig. 1,1) image is embedded by convolutional neural
network (cnn) firstly; 2) then all labels are embedded and combined with their
state embeddings; 3) finally, all embeddings are fed into a transformer, and the
output label tokens are fed into fully connection (fc) layers for final
predictions. based on this network, we further introduce a self-feedback
strategy, which allows the label information (including the missing labels) to
be reused iteratively for enhancing the utilization of labels.
image embeddings f . given input image x ∈ r l×w ×h , the feature vector k ∈ r c
is extracted by a cnn after global average pooling (gap), where the output
channel c = 256. then k is split along the channel dimension into n (n = 8)
sub-feature vectors f = {f 1 , f 2 , . . . , f n }, f i ∈ r d , d = c/n for
tokenization. we choose 3d vgg8, a simple cnn with 8 convolution layers.label
embeddings l. in order to realize the information interaction among labels, and
between labels and image features, we embed labels by an embedding layer. each
image x has m labels, and all labels are embedded into a vector set l = {l 1 , l
2 , . . . , l m }, l i ∈ r d by the learnable embedding layer of size d × m .
there is a correlation between labels, e.g. the lesions with indistinct borders
tend to be malignant. therefore, we hypothesize that the states (gt values) of
some labels can be a context for helping predict the remaining labels. we use a
soft state embedding method. specifically, we first embed the positive and
negative states into s p and s n , both ∈ r d , and then the final state
embedding s i is the weighted sum of s p and s n as shown in equation (1). the
state weight w p i and w n i is the true label value (eg. w p i = 1.0 when label
is positive), where w p i + w n i = 1. for labels with continuous values such as
age, the value normalized to 0 ∼ 1 is w p i . the s i is set as a zero vector
for unknown label. l i = s i + l i is the final label embedding.multi-label
inference with transformer encoder. in a transformer, each output token is the
integration of all input tokens. taking advantage of this structure, we use a
transformer encoder to integrate image embeddings and label embeddings, and used
the output label tokens to predict label value. specifically, embedding setare
the input tokens, the attention value α and output token e are computed as
follows:where e i is from e, w q , w k and w v are weight matrices of query, key
and value, respectively, w r and w o are transformation matrices, and b 1 and b
2 are bias vectors. this update procedure is repeated for l layers, where the e
i are fed to the successive transformer layer. finally, all e i which are label
output tokens are fed into m independent fc layers for predicting value of each
label. the states of unknown labels cannot provide context, thus, the
information interaction between known labels and unknown labels may be weaken.
to overcome this problem, we propose a self-feedback strategy (sfs) inspired by
recurrent neural networks (rnn) to enhance the interaction of labels.
training progress and loss function. as shown in fig. 2, at time point t = 0,
the state embedding is initialized to s gt i by ground truth (gt) value, and the
initial label embedding l initial i is computed byis combined with f i as the
initial input, and then the output predicted value is converted into state
embedding s 0 i by equation (1). when t>0, the label embedding l t i is updated
iteratively by l t i = s t-1 i + l i , and then fed into the transformer t
times. for classification and regression labels, we use focal crossentropy loss
and l2 loss respectively, and use the method in [7] to auto-weight the loss
value of each label. the backpropagation of gradients and parameter updates are
performed immediately after calculating the loss at each time point t. in the
regular inference phase, the state of all labels is initialized as unknown.label
mask strategy. to avoid predicting with labels' own input state, we use a label
mask strategy (lms) during training phase to randomly mask a certain proportion
a of known labels, which causes the labels' states to be embedded as zero
vectors. meanwhile, only the loss of the masked known label is calculated.
real-world pnens dataset. we validated our method on a real-world pnens dataset
from two centers. all patients with arterial phase computed tomography (ct)
images were included. the dataset contained 264 and 28 patients in center 1 and
center 2, and a senior radiologist annotated the bounding boxes for all 408 and
28 lesions. we extracted 37 labels from clinical reports, including survival,
immunohistochemical (ihc), ct findings, etc. among them, 1)recist drug response
(rs), 2)tumor shrink (ts), 3)durable clinical benefit (dcb), 4)progression-free
survival (pfs), 5)overall survival (os), 6)grade (gd), 7)somatostatin receptor
subtype 2(sstr2), 8)vascular endothelial growth factor receptor 2 (vefgr2),
9)o6-methylguanine methyltransferase (mgmt), 10)metastatic foci (mtf), and
11)surgical recurrence (rt) are main tasks, and the remaining are auxiliary
tasks. 143 and 28 lesions were segmented by radiologists, and the radiomics
features of them were extracted, of which 162 features were selected and
binarized as auxiliary tasks because of its statistically significant
correlation with the main labels. the label distribution and the overlap ratio
(jaccard index) of lesions between pairs of labels are shown in fig. 3. it is
obvious that the real-world dataset has a large number of labels with randomly
missing data, thus, we used an adjusted 5-fold cross-validation. taking a
patient as a sample, we chose the dataset from center 1 as the internal dataset,
of which the samples with most of the main labels were used as dataset 1 (219
lesions) and was split into 5 folds, and the remaining samples are randomly
divided into the training set dataset 2 (138 lesions) and the validation set
dataset 3 (51 lesions), the training set and the validation set of the
corresponding folds were added during cross-validation, respectively. all
samples in center 2 left as external test set. details of each dataset are in
the supplementary material. dataset evaluation metrics. we evaluate the
performance of our method on the 10 main tasks for internal dataset, and due to
missing labels and too few sstr2 labels, only the performance of predicting rt,
pfs, os, gd, mtf are evaluated for external dataset. we employ accuracy (acc),
sensitivity (sen), specificity (spc), f1-score (f1) and area under the receiver
operating characteristic (auc) for each task, and compute the mean value of them
(e.g. mauc).
the ct window width and window level were adjusted to 310 and 130 hu refer to
[17], and the image inside the bounding box was cropped and scaled to 128×128×64
pixels. the numbers of convolutional kernels of vgg8 are
[3,32,32,64,64,128,128,256]. transformer encoder contained 2 layers and 8 heads.
layer normalization was used in transformer. the lms a was set as 0.5, and the
training feedback times t was 4. we used adam optimiser, and used cosine
annealing to reduce the learning rate from 1e-4 to 1e-12 over all 200 epochs.
our method was implemented in pytorch, using an nvidia rtx titan gpu.
we compared our method with single-task(st), parameters sharing (ps), ml-decoder
[14] and c-tran [8]. specifically, a st model is trained using single label, and
ps model uses a fc layer followed by the cnn to predict all labels. it should be
noted that the cnn backbone of each method was replaced as 3d vgg8 to ensure
fair comparison. in the ablation experiment, we removed the lms and sfs to
analyze their impact. the auc of each main label is shown in fig. 4, and the
average performance is shown in table 1. it can be seen that multi-label models
is better than that of st due to using the relationship among labels, and sft
outperform other methods on most tasks and the average performance. the ablation
experiments results showed that removing the lms and sfs components causes
performance degradation, indicating the necessity of them.
as shown in the fig. 5, we designed 4 inference modes: 1) regular, only input
images; 2) expert-assisted (ea), certain information is provided by clinicians;
3) self-feedback (sf), iteratively inference t times by using prediction; 4)
expertmachine combination (emc), expert-assisted and self-feedback inference are
both performed. only the auxiliary labels states were input in ea mode. the
results is shown in table 2. both sf and ea perform better than regular mode,
and emc outperforms other modes with a mauc of 0.72 (0.82 on external dataset).
we tested the sft with and without sfs training under different feedback times t
in sf mode, and results (fig. 6.) showed that the performance of sft with sfs
training increases gradually with the increase of t , while the sft without sfs
training has a general trend of decreasing performance after continuous
iteration.
to explore the noise resistance of sft in ea mode, we selected 20, 40, 60, 80,
and 100 percent of the known labels respectively, and further negated 0, 20, 40,
60, 80, 100 percent of the selected labels to simulate noisy labels. the way to
negate a label is to change the label value x to 1x. as shown in fig. 7, as the
noise ratio increases, the performance shows a decreasing trend, and the
performance decreases slightly when the noise ratio ≤ 40 %. finally, it can be
observed that the sft using the sfs training strategy is relatively less
affected by noise. when using 100 percent labels, the mauc of the internal
dataset decreased from 0.71 (noise ratio = 0.0) to 0.53 (noise ratio = 1.0), a
decrease of 0.
we proposed a novel model sft for multi-label prediction on real-world pnens
data. the model integrates label and image informations based on a transformer
encoder, and iteratively uses its own prediction based on a self-feedback
mechanism to improve the utilization of missing labels and correlation among
labels. experiment results demonstrated our proposed model outperformed other
multilabel models, showed flexibility by multiple inference modes, and had a
certain ability to maintain performance when the input context noise was less
than 40%.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43990-2_49.
breast cancer is a serious health problem with high incidence and wide
prevalence for women throughout the world [1,2]. regular screening and early
detection are crucial for effective diagnosis and treatment, and hence for
improved prognosis and survival rate [3,4]. clinical researches have shown that
ultrasound imaging is an effective tool for screening breast cancer, due to its
critical characteristics of non-invasiveness, non-radiation and inexpensiveness
[5][6][7]. in clinical diagnosis, delineating tumor regions from background is a
crucial step for quantitative analysis [8]. manual delineation always depends on
the experience of radiologists, which tends to be subjective and time-consuming
[9,10].therefore, there is a high demand for automatic and robust methods to
achieve accurate breast tumor segmentation. however, due to speckle noise and
shadows in ultrasound images, breast tumor boundaries tend to be blurry and are
difficult to be distinguished from background. furthermore, the boundary and
size of breast tumors are always variable and irregular [11,12]. these issues
pose challenges and difficulties for accurate breast tumor segmentation in
ultrasound images.various approaches based on deep learning have been developed
for tumor segmentation with promising results [13][14][15][16][17][18][19]. su
et al. [13] designed a multi-scale u-net to extract more semantic and diverse
features for medical image segmentation, using multiple convolution sequences
and convolution kernels with different receptive fields. zhou et al. [14] raised
a deeply-supervised encoder-decoder network, which is connected through a series
of nested and dense skip pathways to reduce semantic gap between feature maps.
in [15], a multi-scale selection and multi-channel fusion segmentation model was
built, which gathers global information from multiple receptive fields and
integrates multi-level features from different network positions for accurate
pancreas segmentation. oktay et al. [16] proposed an attention gate model, which
is capable of suppressing irrelevant regions while highlighting useful features
for a specific task. huang et al. [17] introduced a unet 3+ for medical image
segmentation, which incorporates low-level and high-level feature maps in
different scales and learns full-scale aggregated feature representations. liu
et al. [18] established a convolution neural network optimized by super-pixel
and support vector machine, segmenting multiple organs from ct scans to assist
physicians diagnosis. pei et al. [19] introduced channel and position attention
module into deep learning neural network to obtain contextual information for
colorectal tumors segmentation in ct scans. however, although these proposed
models have achieved satisfactory results in different medical segmentation
tasks, their performances are limited for breast tumor segmentation in
ultrasound images due to the low image contrast and blurry tissue boundary.to
address these challenges, we present, to the best of our knowledge, the first
work to adopt multi-scale features collected from large set of clinical
ultrasound images for breast tumor segmentation. the main contributions of our
work are as follows: (1) we propose a well-pruned simple but effective network
for breast tumor segmentation, which shows remarkable and solid performance on
large clinical dataset; (2) our large pretrained model is evaluated on two
additional public datasets without fine-tuning and shows extremely stabilized
improvement, indicating that our model has outstanding generalizability and good
robustness against multi-site data data.
we demonstrate the architecture of our proposed network in fig. 1. it is based
on the unet [20] backbone. in order to collect multi-scale rich information for
tumor tissues, we propose to use gan [21]-based multi-scale deep supervision. in
particular, we apply similarity constraint for each stage of the unet decoder to
obtain consistent and stable segmentation maps. instead of using dice score in
the final layer of unet, we also use dice loss on each of the decoder stages.
besides, we integrate an adversarial loss as additional constraint to penalize
the distribution dissimilarity between the predicted segmentation map and the
ground truth. in the framework of gan, we take our segmentation network as the
generator and a convolutional neural network as the discriminator. the
discriminator consists of five convolution layers with the kernel sizes of 7 ×
7, 5 × 5, 4 × 4, 4 × 4 and 4 × 4. therefore, we formulate the overall loss for
the generator, namely the segmentation network, aswhere sn represents the
segmentation network and cn represents the involved convolutional network. θ sn
and θ cn refer to the parameters in the segmentation and convolutional network,
respectively. p (n) represents the segmentation maps obtained from the n-th
stage in the segmentation network, and g refers to the corresponding ground
truth. p sn (proba) denotes the distribution of probability maps. cn θcn (sn θsn
(p (2) )) represents the probability for the input of cn coming from the
predicted maps rather than the real ones. the parameters α 1 is empirically set
as 1, α 2 , α 3 , α 4 are set as 0.1, and β 1 , β 2 , β 3 and β 4 are set as
0.05. it should be noted that, in unet, there are 4 stages and hence we employ 4
cnns for each of them without sharing their weights. meanwhile, the adversarial
loss for each of the cnn is defined as:where p cn (truth) denotes the
distribution of original samples. cn θcn (g) represents the probability for the
input of cn coming from the original dataset.in the implementation, we update
the segmentation network and all the discriminators alternatingly in each
iteration until both the generator and discriminators are converged.
we collected 10927 cases for this research from yunnan cancer hospital. each
scan is with resolution of 1 × 1 mm 2 and size of 512 × 480. the breast tumors
of each case are delineated by three experienced experts. five-fold cross
validation is performed on the dataset in all experiments to verify our proposed
network. for external validation, we further test our model on two independent
publicly-available datasets collected by stu-hospital (dataset 1) [22] and
syu-university (dataset 2) [23]. in order to comprehensively evaluate
segmentation efficiency of our model, dice similarity coefficient (dsc),
precision, recall, jaccard, and root mean squared error (rmse) are used as
evaluation metrics in this work. our proposed algorithm is conducted on pytorch,
and all experiments are performed on nvidia tesla a100 gpu. we use adam
optimizer to train the framework with an initial learning rate of 10 -4 . the
epochs to train all models are 100 and the batch size in training process is set
as 4.
to verify the advantages of our proposed model for breast tumor segmentation in
ultrasound images, we compare our deep-supervised convolutional network with the
state-ofthe-art tumor segmentation methods, including deepres [24], msunet [13],
unet++ [14], segnet [25], attu-net [16], u 2 -net [26] and unet 3+ [17]. the
comparison experiments are carried on a large-scale clinical breast ultrasound
dataset, and the quantitative results are reported in table 1. it is obvious
that our proposed model achieves the optimal performance compared with other
segmentation models. for example, our method obtains a dsc score of 80.97%,
which is 5.81%, 9.13%, 7.77%, 4.13%, 7.51%, 2.19%, and 3.83% higher than other
seven models. these results indicate the effectiveness of the proposed model in
delineating breast tumors in ultrasound images.representative segmentation
results using different methods are provided in fig. 2. the probability maps
predicted by our model are more consistent with the ground truth, especially in
the tiny structures which are difficult to capture. this verifies the superior
ability of the proposed model in maintaining detailed edge information compared
with state-of-the-art methods.
we demonstrate the efficacy of the deep supervision strategy using ablation
studies. four groups of frameworks (stage i, stage ii, stage iii and stage iv)
are designed, with the numerals denoting the level of deep supervision counting
from the last deconvolutional layer.we test these four frameworks on the
in-house breast ultrasound dataset, and verify their segmentation performance
using the same five evaluation criteria. the evaluation metrics from all cases
are presented by the scatter plots in fig. 3. the obtained quantitative results
are shown in table 2, where stage iv model achieves the optimal dsc, precision,
recall, and jaccard. all these results draw a unanimous conclusion on the
relationship between these four frameworks. that is, the segmentation ability of
the proposed stage iv is ameliorated from every possible perspective. moreover,
stage iv obtains minimal rmse compared with other three models (0.68 mm vs 0.84
mm, 0.82 mm, 0.75 mm), which means better matching of the predicted maps from
stage iv with the corresponding ground truth. all these comparison results
verify the superiority of deep supervision for breast tumor segmentation in
ultrasound images.
in this paper, we have developed a large pre-trained model for breast tumor
segmentation from ultrasound images. in particular, two constraints are proposed
to exploit both image similarity and space correlation information for refining
the prediction maps. moreover, our proposed deep supervision strategy is used
for quality control at each decoding stage, optimizing prediction maps
layer-by-layer for overall performance improvement. using a large clinical
dataset, our proposed model demonstrates not only state-of-the-art segmentation
performance, but also the outstanding generalizability to new ultrasound data
from different sites. besides, our large pre-trained model is general and robust
in handling various tumor types and shadow noises in our acquired clinical
ultrasound images. this also shows the potential of directly applying our model
in real clinical applications.
mitochondria are membrane-bound organelles that generate the primary energy
required to power the cell activities, thereby crucial for metabolism.
mitochondrial dysfunction, which occurs when mitochondria are not functioning
properly has been witnessed as a major factor in numerous diseases, including
noncommunicable chronic diseases (e.g, cardiovascular and cancer), metabolic
(e.g, obesity) and neurodegenerative (e.g, alzheimer and parkinson) disorders
[23,25]. electron microscopy (em) images are typically utilized to reveal the
corresponding 3d geometry and size of mitochondria at a nanometer scale, thereby
facilitating basic biological research at finer scales. therefore, automatic
instance segmentation of mitochondria is desired, since manually segmenting from
a large amount of data is particularly laborious and demanding. however,
automatic 3d mitochondria instance segmentation is a challenging task, since
complete shape of mitochondria can be sophisticated and multiple instances can
also experience entanglement with each other resulting in unclear boundaries.
here, we look into the problem of accurate 3d mitochondria instance
segmentation.earlier works on mitochondria segmentation employ standard image
processing and machine learning methods [20,21,33]. recent approaches address
[4,15,26] this problem by leveraging either 2d or 3d deep convolutional neural
network (cnns) architectures. these existing cnn-based approaches can be roughly
categorized [36] into bottom-up [3,4,14,15,28] and top-down [12]. in case of
bottom-up mitochondria instance segmentation approaches, a binary segmentation
mask, an affinity map or a binary mask with boundary instances is computed
typically using a 3d u-net [5], followed by a post-processing step to
distinguish the different instances. on the other hand, top-down methods
typically rely on techniques such as mask r-cnn [7] for segmentation. however,
mask r-cnn based approaches struggle due to undefined bounding-box scale in em
data volume.when designing a attention-based framework for 3d mitochondria
instance segmentation, a straightforward way is to compute joint spatio-temporal
selfattention where all pairwise interactions are modelled between all
spatiotemporal tokens. however, such a joint spatio-temporal attention
computation is computation and memory intensive as the number of tokens
increases linearly with the number of input slices in the volume. in this work,
we look into an alternative way to compute spatio-temporal attention that
captures long-range global contextual relationships without significantly
increasing the computational complexity. our contributions are as follows:-we
propose a hybrid cnn-transformers based encoder-decoder framework, named
stt-unet. the focus of our design is the introduction of a split spatio-temporal
attention (sst) module that captures long-range dependencies within the cubic
volume of human and rat mitochondria samples. the sst module independently
computes spatial and temporal self-attentions in parallel, which are then later
fused through a deformable convolution. -to accurately delineate the region of
mitochondria instances from the cluttered background, we further introduce a
semantic foreground-background (fg-bg) adversarial loss during the training that
aids in learning improved instance-level features. -we conduct experiments on
three commonly used benchmarks: lucchi [20],mitoem-r [36] and mitoem-h [36]. our
stt-unet achieves state-of-the-art fig. 1. qualitative 3d instance segmentation
comparison between the recent res-unet [16] and our proposed stt-unet approach
on the example input regions from mitoem-h and mitoem-r validation sets. here,
we present the corresponding segmentation predictions of the baseline and our
approach along with the ground truth.our stt-unet approach achieves superior
segmentation performance by accurately segmenting 16% more cell instances in
these examples, compared to res-unet-r.segmentation performance on all three
datasets. on lucchi test set, our stt-unet outperforms the recent [4] with an
absolute gain of 3.0% in terms of jaccard-index coefficient. on mitoem-h val.
set, stt-unet achieves ap-75 score of 0.842 and outperforms the recent 3d
res-unet [16] by 3.0%.figure 1 shows a qualitative comparison between our
stt-unet and 3d res-unet [16] on examples from mitoem-r and mitoem-h datasets.
most recent approaches for 3d mitochondria instance segmentation utilize
convolution based designs within the "u-shaped" 3d encoder-decoder
architecture.in such an architecture, the encoder aims to generate a
low-dimensional representation of the 3d data by gradually performing the
downsampling of the extracted features. on the other hand, the decoder performs
upsampling of these extracted feature representations to the input resolution
for segmentation prediction. although such a cnn-based designs [11,16,34] has
achieved promising segmentation results compared to traditional methods, they
struggle to effectively capture long-range dependencies due to their limited
local receptive field. inspired from success in natural language processing
[32], recently vision transformers (vits) [6,13,19,30,31] have been successfully
utilized in different computer vision problems due to their capabilities at
modelling long-range dependencies and enabling the model to attend to all the
elements in the input sequence. the core component in vits is the self-attention
mechanism that that learns the relationships between sequence elements by
performing relevance estimation of one item to other items. the other attention
such as [1,8,10,29,35] have demonstrated remarkable efficacy in effectively
managing volumetric data.inspired by vits [10,19] and based on the observation
that attention-based vision transformers architectures are an intuitive design
choice for modelling long-range global contextual relationships in volume data,
we investigate designing a cnntransformers based framework for the task of 3d
mitochondria instance segmentation.3 method
we base our approach on the recent res-unet [16], which utilizes encoderdecoder
structure of 3d unet [34] with skip-connections between encoder and decoder.
here, 3d input patch of mitochondria volume (32 × 320 × 320) is taken from the
entire volume of (400 × 4096 × 4096). the input volume is denoised using an
interpolation network adapted for medical images [9]. the denoised volume is
then processed utilizing an encoder-decoder structure containing residual
anisotropic convolution blocks (acb). the acb contains three layers of 3d
convolutions with kernels (1having skip connections between first and third
layers. the decoder outputs semantic mask and instance boundary, which are then
post-processed using connected component labelling to generate final instance
masks. we refer to [16] for more details.limitations: as discussed above, the
recent res-unet approach utilizes 3d convolutions to handle the volumetric input
data. however, 3d convolutions are designed to encode short-range
spatio-temporal feature information and struggle to model global contextual
dependencies that extend beyond the designated receptive field. in contrast, the
self-attention mechanism within the vision transformers possesses the
capabilities to effectively encode both local and global long-range dependencies
by directly performing a comparison of feature activations at all the space-time
locations. in this way, self-attention mechanism goes much beyond the receptive
field of the conventional convolutional filters. while self-attention has been
shown to be beneficial when combined with convolutional layers for different
medical imaging tasks, to the best of our knowledge, no previous attempt to
design spatio-temporal self-attention as an exclusive building block for the
problem of 3d mitochondria instance segmentation exists in literature. next, we
present our approach that effectively utilizes an efficient spatiotemporal
attention mechanism for 3d mitochondria instance segmentation. with split
spatio-temporal attention and an instance segmentation block. the denoising
module alleviates the segmentation faults caused by anomalies in the em images,
as in the baseline. the denoising is performed by convolving the current frame
with two adjacent frames using predicted kernels, thereby generating the
resultant frame by adding the convolution outputs. the resulting denoised output
is then processed by our transformer based encoder-decoder with split
spatio-temporal attention to generate the semantic masks. consequently, these
semantic masks are post-processed by an instance segmentation module using a
connected component labelling scheme, thereby generating the final instancelevel
segmentation output prediction. to further enhance the semantic segmentation
quality with cluttered background we introduced semantic adversarial loss which
leads to improved semantic segmentation in noisy background.split
spatio-temporal attention based encoder-decoder: our stt-unet framework
comprises four encoder and three decoder layers. within each layer, we introduce
a split spatio-temporal attention-based (sst) module, fig. 2(b), that strives to
capture long-range dependencies within the cubic volume of human and rat
samples. instead of the memory expensive joint spatiotemporal representation,
our sst module splits the attention computation into a spatial and a temporal
parallel stream. the spatial attention refines the instance level features from
input features along the spatial dimensions, whereas the temporal attention
effectively learns the inter-dependencies between the input volume. where, x s
is spatial attention map, x t is temporal attention map and d k is dimension of
q s and k s . to fuse spatial and temporal attention maps, x s and x t , we
employ deformable convolution. the deformable convolution generates offsets
according to temporal attention map x t and by using these offsets the spatial
attention map x s is aligned. the deformable fusion is given as,where, c is no
of channels, x is spatially aligned attention map with respect to x t . w is the
weight matrix of kernels, x s is spatial attention map, k 0 is starting position
of kernel, k n is enumerating along all the positions in kernel size of r and δk
n is the offset sampled from temporal attention map x t . we empirically observe
that fusing spatial and temporal features through a deformable convolution,
instead of concatenation through a conv. layer or addition, leads to better
performance. the resulting spatio-temporal features of decoder are then input to
instance segmentation block to generate final instance masks, as in
baseline.semantic fg-bg adversarial loss: as discussed earlier, a common
challenge in mitochondria instance segmentation is to accurately delineate the
region of mitochondria instances from the cluttered background. to address this,
we introduce a semantic foreground-background (fg-bg) adversarial loss during
the training to enhance the fg-bg separability. here, we introduce the auxiliary
discriminator network d with two layers of 3d convolutions with stride 2 during
the training as shown in fig. 2(c). the discriminator takes the input volume i
along with the corresponding mask as an input. here, the mask m is obtained
either from the ground truth or predictions, such that all mitochondria
instances within a frame are marked as foreground. while the discriminator d
attempts to distinguish between ground truth and predicted masks (m gt and m
pred , respectively), the model ψ learns to output semantic mask such that the
predicted masks m pred are close to ground truth m gt . let f gt = concat(i, m
gt ) and f pr = concat(i, m pred ) denote the real and fake input, respectively,
to the discriminator d. similar to [11], the adversarial loss is then given
by,consequently, the overall loss for training is:where, l bce is bce loss, λ =
0.5 and l fg-bg is semantic adversarial loss.
dataset: we evaluate our approach on three datasets: mitoem-r [36], mitoem-h
[36] and lucchi [22]. the mitoem [36] is a dense mitochondria instance
segmentation dataset from isbi 2021 challenge. the dataset consists of 2 em
image volumes (30 μm 3 ) of resolution of 8 × 8 × 30 nm, from rat tissues
(mitoem-r) and human tissue (mitoem-h) samples, respectively. each volume has
1000 grayscale images of resolution (4096 × 4096) of mitochondria, out of which
train set has 400, validation set contains 100 and test set has 500 images.
lucchi [22] is a sparse mitochondria semantic segmentation dataset with training
and test volume size of 165 × 1024 × 768.implementation details: we implement
our approach using pytorch1.9 [27] (rcom env) and models are trained using 2 amd
mi250x gpus. during training of mitoem, for the fair comparison, we adopt same
data augmentation technique from [36]. the 3d patch of size (32×320×320) is
input to the model and trained using batch size of 2. the model is optimized by
adam optimizer with learning rate of 1e -4 . unlike baseline [16], we do not
follow multi-scale training and perform single stage training for 200k
iterations. for lucchi, we follow training details of [16,36] for semantic
segmentation. for fair comparison with previous works, we use the same
evaluation metrics as in the literature for both datasets. we use 3d ap-75
metric [36] for mitoem-r and mitoem-h datasets. for lucchi, we use jaccard-index
coefficient (jaccard) and dice similarity coefficient (dsc).
state-of-the-art comparison: table 1 shows the comparison on mitoem-r and
mitoem-h validation sets. our stt-unet achieves state-of-the-art performance on
both sets. compared to the recent [16], our stt-unet achieves an absolute gains
of 4.1% and 2.9% on mitoem-r and mitoem-h validation sets, respectively. note
that [16] employs two decoders for mitoem-h. in contrast, we utilize only a
single decoder for both mitoem-h and mitoem-r sets, while still achieving
improved segmentation performance. fig 3 presents the segmentation predictions
of our approach on example input regions from the validation set.our approach
achieves promising segmentation results despite the noise in the input samples.
ablation study: table 3 shows a baseline comparison when progressively
integrating our contributions: sst module and semantic foreground-background
adversarial loss. the introduction of sst module improves performance from 0.921
to 0.941 with a gain of 2.7%. the performance is further improved by 1%, when
introducing our semantic foreground-background adversarial loss. our final
approach achieves absolute gains of 3.7% and 2.6% over the baseline on mitoem-r
and mitoem-h, respectively. we also compare our approach with other attention
mechanism in literature such as divided space-time attention [1] and axial
attention [35] with our method achieving favorable results with gain of 0.9% and
1.1%, respectively likely due to computing spatial and temporal in parallel and
later fusing them through a deformable convolution. further, we compare our
approach with [16] on mitoem-v2 test set achieving a gain of 4% on mitoem-r,
where the postprocessing from [18] is used to differentiate the mitochondria
instances for both methods. table 4 shows ablation study with feature fusion
strategies in our sst module: addition, concat and deformable-conv. the best
results are obtained with deformable-conv on both datasets. for encoding spatial
and temporal information, we analyze two design choices with sst module:
cascaded and split, as shown in table 5. the best results are obtained using our
split design choice (row 3) with spatial and temporal information encoded in
parallel and later combined. we also evaluate with different input volumes:
4,8,16,32. we observe best results are obtained when using 32 input volume.
we propose a hybrid cnn-transformers based encoder-decoder approach for 3d
mitochorndia instance segmentation. we introduce a split spatio-temporal
attention (sst) module to capture long-range dependencies within the cubic
volume of human and rat mitochondria samples. the sst module computes spatial
and temporal attention in parallel, which are later fused. further, we introduce
a semantic adversarial loss for better delineation of mitochondria instances
from background. experiments on three datasets demonstrate the effectiveness of
our approach, leading to state-of-the-art segmentation performance.
presents the comparison on lucchi test set. our method sets a new
state-of-the-art on this dataset in terms of both jaccard and dsc.
chemical exchange saturation transfer (cest) is a novel metabolic magnetic
resonance imaging (mri) method that allows to detect molecules in tissue based
on chemical exchange of their mobile protons with water protons [18]. cest works
by selectively saturating the magnetization of a specific pool of protons, such
as those in metabolites or proteins, by applying narrow-band radiofrequency (rf)
pulses at their respective larmor frequency. due to chemical exchange this
saturation state is transferred to the water pool and a decrease in the detected
water signal provides information about the concentration and exchange rate of
the underlying molecules. this procedure is repeated for several rf frequencies
to acquire the so-called cest-spectrum in each voxel. cest-mri offers several
promising contrasts that correlate with the diagnosis of diseases such as
ischemic stroke [16], brain tumors [1], and neurodegenerative diseases [2,5].
the cestspectrum contains effects of proton pools of various chemical components
in the tissue, typically isolated by a lorentzian model [14] that is derived
form the underlying physics of the bloch-mcconnell equations [8]. in this
conventional method, several lorentzian distributions are fitted to the
cest-spectrum using nonlinear least squares method [10], and the amplitude of
each fitted distribution represents a particular metabolic map. the number of
lorentzian functions utilized in this process depends on the expected number of
exchanging proton pools present in the spectrum. figure 1a shows an example of
an acquired cest-spectrum and the corresponding 5-pool lorentzian fit.
increasing the static magnetic field b 0 (e.g., with b 0 = 7t), enhances
spectral resolution, but leads to significant variations in the b 1 amplitude of
the saturating rf field across the field of view (cf. fig. 1c). this b 1
inhomogeneity is corrected by acquiring cest-spectra at various rf field
strengths (cf. fig. 1b) and then interpolating between them at fixed b 1 to
produce the b 1 -robust metabolic cest contrast maps [14]. this b 1 correction
increases the acquisition time at least twofold.hunger et al. shown that
supervised learning can be used to generate b 1robust cest maps, coining the
deepcest approach [3,4]. however, the previous work on generating the b 1
-robust cest contrasts rely on valid target data and the underlying assumptions
to generate it, and can only create cest maps at one particular b 1 level.in
this work, we developed a conditional autoencoder (cae) [13] to generate b 1
-homogeneous cest-spectra at arbitrary b 1 levels, and a physics-informed
autoencoder (piae) to fit the 5-pool lorentzian model to the b 1 corrected
cest-spectra. this inclusion of physical knowledge in the form of known
operators in neural nets (nn) is expected to reduce the absolute error margin of
the model [6,7] and to increase its interpretability. both cae and piae are
trained in an unsupervised end-to-end method that eliminates the shortcomings of
conventional lorentzian curve fitting and produces robust cest contrast at
arbitrary b 1 levels without the need for an additional acquisition scan. we
called the proposed method physics-informed conditional autoencoder (picae).
data measurements. cest imaging was performed in seven subjects, including two
glioblastoma patients, after written informed consent was obtained to
investigate the dependence of cest effects on b 1 in brain tissue. the local
ethics committee approved the study. all volunteers were measured at three b 1
field strengths 0.72 μt, 1.0 μt, and 1.5 μt. a method as described by mennecke
et al. [9] was used to acquire cest data on a 7t whole-body mri system
(magne-tom terra, siemens healthcare gmbh, erlangen, germany). saturated images
were obtained for 54 non-equidistant frequency offsets ranging from -100 ppm to
+100 ppm. the acquisition time per b 1 level was 6:42 min. the acquisition of
the b 1 map required an additional 1:06 min.conditional autoencoder. we
developed a conditional autoencoder (cae) to solve the b 1 inhomogeneity
problem, which is essential for the generation of metabolic cest contrast maps
at 7t. the left part of fig. 2 describes the cae. the encoding network of cae
took the raw cest-spectrum and the corresponding effective b 1 value as input
and generate a latent space that was concatenated once with the same b 1 input
value and passed to the decoder that reconstruct the uncorrected b 1
cest-spectrum, and another time the latent space was concatenated with the
desired/specific effective b 1 value to reconstruct the cest-spectrum at a
specific b 1 saturation amplitude. both decoders shared the weights (cf. fig.
2). for the development of the cae networks, we used the well-known fully
concatenated (fc) layers with leaky relu activations except for the last layer
of decoder, which had a linear activation. the encoder and decoder both
consisted of 4 layers, where the layers of the encoder successively contain 128,
128, 64, 32 neurons, while the layers of the decoder successively contain 32,
64, 128, 128 neurons. the input, latent space and output layers had 55, 17 and
54 neurons respectively.physics-informed autoencoder. the lorentzian model and
its b 1dispersion can be derived from the underlying spin physics described by
the bloch-mcconnell equation system [8]. the physics-informed autoencoder (piae)
utilized fully connected nn as encoder and lorentzian distribution generator as
a decoder to perform the pixel-wise 5-pool lorentzian curve fit to the
cest-spectrum (water, amide, amine, noe, mt) [14]. the 5-pool model was
described aswhere l denotes the lorentz function. the direct saturation pool
(water) was defined as(the remaining other four pools were defined as) 2 , i ∈
amide, amine, rn oe, ssm t .(the right part of fig. 2 describes the piae. the
encoder of piae mapped the cest-spectrum to the amplitudes a i , the full width
half maximum (fwhm) τ i , and the water peak position δ ds of the 5-pool
lorentzian model. its encoder consisted of four fc layers, each with 128 neurons
with leaky relu activations. it had three so-called fc latent space layers with
linear activation for position and exponential activations for fwhm and
amplitudes of 5-pool lorentzian model. the positions of amide, rnoe, ssmt, and
amine were fixed at 3.5 ppm, -3.5 ppm, -3 ppm, and 2 ppm, respectively, and
shifted with respect to the predicted position of the water peak. the decoder of
piae consisted of a lorentzian distribution generator (cf. fig. 2). it generated
samples of the 5-pool distributions exactly at the offsets δω (i.e. between -100
ppm and 100 ppm) where the input cest-spectrum was sampled, and combined them
according to eq. 1 to generate the input cest spectrum with or without b 0
correction.bound loss. the peak positions δ i and widths τ i of the pools had to
be within certain bounds so that certain neurons in the latent space layer of
piae would not be exchanged and provide the same pool parameters for all
samples. we developed a simple cost function along the lines of the hinge loss
[12], called the bound loss. mathematically, it is defined as followsthe bound
loss increases linearly as the output of the latent space neurons of piae
exceeds or recede from the boundaries. the lower and upper limits for positions
and widths are given in table 1 of the supplementary material.training and
evaluation. four healthy volunteers formed the training and validation sets. the
test set consisted of the two tumor patients and one healthy subject. to ensure
that the outcomes were exclusively based on the cest-spectrum and not influenced
by spatial position, the training was carried out voxel-by-voxel. consequently,
there were approximately one million cestspectra for the training process. cae
was first trained with mse loss. in this step, the cae encoder was fed with the
cest-spectrum of a specific b 1 saturation amplitude, and it generated two
cest-spectra, one for the input b 1 saturation level and the other for the b 1
level injected into the latent space (cf. fig. 2). later, it was trained with a
combination of mse loss and perception loss (mse loss between the latent space
of the cest-spectra at two different b 1 levels). to incorporate perception
loss, we used two forward passes with two different b 1 cest-spectra and used
perception loss to generate a latent space that is independent of b 1 saturation
amplitude. the following equation describes the loss of the second step.piae, on
the other hand, was trained with a combination of mse loss and bound loss. the
piae loss was described as followsfor evaluation we input the uncorrected
cest-spectrum acquired at 1μt and generated corrected cest-spectra at b 1 0.5,
0.72, 1.0, 1.3, 1.5 μt. piae encoder yielded the amplitudes of 5-pool for b 1
corrected cest-spectrum. its decoder reconstructed the b 1 b 0 fitted
cest-spectrum. the b 0 correction simply refers to the shift of the position of
the water peak to 0 ppm.cest quantification. the multi-b 1 cest-spectra allow
quantification of cest effects (amide, rnoe, amine) [14,15] down to the exchange
rate and concentration. the amplitudes of the cest contrasts were expressed
according to the definition in [15] as followswhere f i , k i , and r 2i express
the concentrations, exchange rates, and relaxation rates of the pools. z ref
defines the sum of all 5 distributions at the resonance frequency of the
specific pool in b 1 b 0 corrected cest-spectrum and w 1 is the frequency of the
oscillating field.the amplitudes of cest contrasts in the lorentzian function
have the b 1 dispersion function given by the labeling efficiency α (eq. 7). the
exchange rate occurs here separately from the concentration, which allows their
quantification via the b 1 dispersion. concentration and exchange rate were
fitted as a product and denoted as z 1 (quantified maps), and k(k+r 2 ) was also
fitted with the single term z 2 using trust-region reflective least squares
[10].
the comparison of picae with the conventional method [9,14] is shown in columns
1 and 2 of fig. 3. the top image in column 3 shows the t 1 -weighted reference
image enhanced with the exogenous contrast agent gadolinium (gd-t 1 w), and the
bottom image shows the b 1 -map. the tumor shows a typical so called gadolinium
ring enhancement indicated by the arrow (a 15 ), which is also visible in the
non-invasive and gadolinium-free cest contrast maps (columns 1 and 2). the
picae-cest maps showed better visualization of this tumor feature compared to
the conventional method. the proposed method yielded at least 25% increase in
the structural similarity index (ssim) with the gd-t 1 w image for the ring
enhancement region. the contrast maps also appear less noisy and more
homogeneous over the whole brain compared to the lorentzian fit on the
interpolated-corrected b 1 cest-spectra [14]. to further evaluate the
performance of piae and cae, we b1-corrected the data using cae and fitted it
with the least squares method (cae-lorentzian fit). the comparison of the cest
maps produced by the conventional lorentzian fit, the cae-lorentzian fit, and
picae is shown in table 1 using ssim and gradient cross correlation (gcc) [11]
for the tumor ring region. both the cae-lorentzian fit and picae were better
than the conventional method. cae-lorentzian fit even outperformed picae for
rnoe metabolic map and has similar performance for amide, but it has much lower
performance for amine. the ability of picae to produce b 1 -robust cest maps at
arbitrary levels is shown in fig. 4, where different b 1 levels reveal different
features of the heterogenous tumor. quantification of chemical exchange rates
and concentration, i.e., z 1 = f•k, is shown in column 4. z 1 (quantified maps)
further improve the visualization of the ring enhancement area. column 5 shows
the z 2 maps, which are combination of the exchange rate k and the relaxation
rate r 2 . quantified maps of amide, rnoe and amine for another tumor patient is
shown in supplementary fig. 1. the accuracy of the cae to generate particular b
1 cestspectra is depicted using absolute error for acquisition at different b 1
levels (see supplementary fig. 2). the performance was lower for b 1 0.72 μt,
and 1.5 μt compared to 1 μt.
in this work, we analyzed the use of an autoencoder approach to generate b
1robust cest contrast maps at arbitrary b 1 levels, which requires multiple
acquisitions in conventional methods [14]. the proposed method reduces the
acquisition time by at least half when only two acquisitions are performed for b
1 correction. supervised learning (deepcest) can generate cest maps that are not
susceptible to b 1 inhomogeneity at a particular b 1 , which already reduces
acquisition time. however, deepcest was trained on data fitted using a
conventional pipeline [9,14] which has suboptimal b 1 correction (cf. fig. 3).
moreover, the different pools in the cest spectrum are highlighted at different
b 1 levels (cf. fig. 4). an approach that can generate a b 1 -robust
cest-spectrum at multiple b 1 levels allows quantifying the exchange rate and
concentration of the cest pools [15]. the optimal b 1 can often only be selected
at post-processing during the analysis of clinical data, as some clinically
important features appear better at certain b 1 levels (cf. fig. 4). the
proposed picae approach combines b 1 correction and lorentzian curve fitting in
a single step. the b 1 correction was performed with a cae, while the lorentzian
line fitting was performed with a piae using nn as the encoder and lorentzian
distribution generator as the decoder. this allows interpretation of the model
while overcoming the drawback of curve fitting, such as being prone to noise
(cf. fig. 3).the bound loss ensured that the positions of the pools were not
interchanged. quantification was still performed using the nonlinear least
squares fit according to eq. 7. the main reason for this was that it does not
affect the acquisition time and it is affected by the z ref .the training was
performed voxel-wise to ensure that the results are based only on the
cest-spectrum. this also results in about 1 million cest-spectra for the
training. figure 3 shows the superiority of picae over the standard method, and
fig. 4 shows that the results produced by picae are authentic because the
quantification column z 1 matches the amplitude images and follows eq. 7.
cae-lorentzian fitting showed comparable performance for amide and rnoe maps,
but significantly lower performance for amine because it was still fitted using
the least squares method, which is susceptible to noise in the input and takes
up to 5 min to evaluate, compared to picae, which takes only a few seconds. the
1 μt acquisition performs better than 0.72 μt and 1.5 μt because it was trained
for both lower and higher b 1 values compared to the other two acquisitions. the
robustness of method for cyclic consistency [17] is displayed in supplementary
fig. 2, which also shows the interpretability of the method. the results of fig.
3 and fig. 4 also show the generalization capability of picae as it was trained
without the tumor data.
in this work, we propose a picae method for evaluating 7t-cest mri that accounts
for b 1 inhomogeneity in the input and predicts homogeneous metabolic cest
contrasts at arbitrary b 1 levels. the proposed generative and interpretable
method enables (i) a reduction of scan time by at least 50%, (ii) the generation
of reliable 7t-cest contrast maps robust to b 1 inhomogeneity at multiple b 1
levels, (iii) a clear physical interpretation of the b 1 correction of the
cestspectra and the fitting of the lorentzian model to it, and (iv) the
quantification of the cest contrast maps.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3_44.
nuclei segmentation is a fundamental step in medical image analysis. accurately
segmenting nuclei helps analyze histopathology images to facilitate clinical
diagnosis and prognosis. in recent years, many deep learning based nuclei
segmentation methods have been proposed [5,18,19,23]. most of these methods are
fully-supervised so the great segmentation performance usually relies on a large
number of labeled images. however, manually labeling the pixels belonging to all
nucleus boundaries in an image is time-consuming and requires domain knowledge.
in practice, it is hard to obtain an amount of histopathology images with dense
pixel-wise annotations but feasible to collect a few labeled images. a question
is raised naturally: can we expand the training dataset with a small proportion
of images labeled to reach or even exceed the segmentation performance of the
fully-supervised baseline? intuitively, since the labeled images are samples
from the population of histopathology images, if the underlying distribution of
histopathology images is learned, one can generate infinite images and their
pixel-level labels to augment the original dataset. therefore, it is demanded to
develop a tool that is capable of learning distributions and generating new
paired samples for segmentation.generative adversarial network (gans)
[2,4,12,16,20] have been widely used in data augmentation [11,22,27,31].
specially, a newly proposed gan-based method can synthesize labeled
histopathology image for nuclei segmentation [21]. while gans are able to
generate high quality images, they are known for unstable training and lack of
diversity in generation due to the adversarial training strategy. recently,
diffusion models represented by denoising diffusion probabilistic model (ddpm)
[8] tend to overshadow gans. due to the theoretical basis and impressive
performance of diffusion models, they were soon applied to a variety of vision
tasks, such as inpainting, superresolution [30], text-to-image translation,
anomaly detection and segmentation [1,9,24,26]. as likelihood-based models,
diffusion models do not require adversarial training and outperform gans on the
diversity of generated images [3], which are naturally more suitable for data
augmentation. in this paper, we propose a novel diffusionbased augmentation
framework for nuclei segmentation. the proposed method consists of two steps:
unconditional nuclei structure synthesis and conditional histopathology image
synthesis. we develop an unconditional diffusion model and a nuclei-structure
conditioned diffusion model (fig. 1) for the first and second step,
respectively. on the training stage, we train the unconditional diffusion model
using nuclei structures calculated from instance maps and the conditional
diffusion model using paired images and nuclei structures. on the testing stage,
the nuclei structures and the corresponding images are generated successively by
the two models. as far as our knowledge, we are the first to apply diffusion
models on histopathology image augmentation for nuclei segmentation.our
contributions are: (1) a diffusion-based data augmentation framework that can
generate histopathology images and their segmentation labels from scratch; (2)
an unconditional nuclei structure synthesis model and a conditional
histopathology image synthesis model; (3) experiments show that with our method,
by augmenting only 10% labeled training data, one can obtain segmentation
results comparable to the fully-supervised baseline.
our goal is to augment a dataset containing a limited number of labeled images
with more samples to improve the segmentation performance. to increase the
diversity of labeled images, it is preferred to synthesize both images and their
corresponding instance maps. we propose a two-step strategy for generating new
labeled images. both steps are based on diffusion models. the overview of the
proposed framework is shown in fig. 2. in this section, we introduce the two
steps in detail.
in the first step, we aim to synthesize more instance maps. since it is not
viable to directly generate an instance map, we instead choose to generate its
surrogate nuclei structure, which is defined as the concatenation of pixel-level
semantic and distance transform. pixel-level semantic is a binary map where 1 or
0 indicates whether a pixel belongs to a nucleus or not. the distance transform
consists of the horizontal and the vertical distance transform, which are
obtained by calculating the normalized distance of each pixel in a nucleus to
the horizontal and the vertical line passing through the nucleus center [5].
clearly, the nuclei structure is a 3-channel map with the same size as the
image. as nuclei instances can be identified from the nuclei structure, we can
easily construct the corresponding instance map by performance marker-controlled
watershed algorithm on the nuclei structure [29]. therefore, the problem of
synthesizing instance map transfers to synthesizing nuclei structures. we deploy
an unconditional diffusion model to learn the distribution of nuclei
structures.denote a true nuclei structure as y 0 , which is sampled from real
distribution q(y). to maximize data likelihood, the diffusion model defines a
forward and a reverse process. in the forward process, small amount of gaussian
noise are successively added to the sample y 0 in t steps by:where t ∼ n (0, i)
and {β t ∈ (0, 1)} t t=1 is a variance schedule. the resulting sequence {y 0 ,
..., y t } forms a markov chain. the conditional probability of y t given y t-1
follows a gaussian distribution:in the reverse process, since q(y t-1 |y t )
cannot be easily estimated, a model p θ (y t-1 |y t ) (typically a neural
network) will be learned to approximate q(y t-1 |y t ). specifically, p θ (y t-1
|y t ) is a also gaussian distribution:the objective function is the variational
lower bound loss: l = l t + l t -1 + ... + l 0 , where every term except l 0 is
a kl divergence between two gaussian distributions. in practice, a simplified
version of l t is commonly used [8]:where α t = 1β t and ᾱt = t i=1 α i .
clearly, the optimization objective of the neural network parameterized by θ is
to predict the gaussian noise t from the input y t at time t.after the network
is trained, one can progressively denoise a random point from n (0, i) by t
steps to produce a new sample:for synthesizing nuclei structures, we train an
unconditional ddpm on nuclei structures calculated from real instance maps.
following [8], the network of this unconditional ddpm has a u-net architecture.
in the second step, we synthesize histopathology images conditioned on nuclei
structures. without any constraint, an unconditional diffusion model will
generate diverse samples. there are usually two ways to synthesize images
constrained by certain conditions: classifier-guided diffusion [3] and
classifier-free guidance [10]. since classifier-guided diffusion requires
training a separate classifier which is an extra cost, we choose classifier-free
guidance to control sampling process.let θ (x t , t) and θ (x t , t, y) be the
noise predictor of unconditional diffusion model p θ (x|y) and conditional
diffusion model p θ (x), respectively. the two models can be learned with one
neural network. specifically, p θ (x|y) is trained on paired data (x 0 , y 0 )
and p θ (x) can be trained by randomly discarding y (i.e. y = ∅) with a certain
drop rate ∈ (0, 1) so that the model learns unconditional and conditional
generation simultaneously. the noise predictor θ (x t , t, y) of classifier-free
guidance is a combination of the above two predictors:where θ (x t , t) = θ (x t
, t, y = ∅), w is a scalar controlling the strength of classifier-free
guidance.unlike the network of unconditional nuclei structure synthesis which
inputs the noisy nuclei structure y t and outputs the prediction of t (y t , t),
the network of conditional nuclei image synthesis takes the noisy nuclei image x
t and the corresponding nuclei structure y as inputs and the prediction of t (x
t , t, y) as output. therefore, the conditional network should be equipped with
the ability to well align the paired histopathology image and nuclei structure.
since nuclei structures and histopathology images have different feature spaces,
simply concatenating or passing them through a cross-attention module [7,15,17]
before entering the u-net will degrade image fidelity and yield unclear
correspondence between synthetic nuclei image and its nuclei structure. inspired
by [28], we embed information of the nuclei structure into feature maps of
nuclei image by the spatially-adaptive normalization (spade) module [25]. in
other words, the spatial and morphological information of nuclei modulates the
normalized feature maps such that the nuclei are generated in the right places
while the background is left to be created freely. we include the spade module
in different levels of the network to utilize the multi-scale information of
nuclei structure. the network of conditional nuclei image synthesis also applies
a u-net architecture. the encoder is a stack of resblocks and attention blocks
(attnblocks). each resblock consists of 2 groupnorm-silu-conv and each
attnblocks calculates the self-attention of the input feature map. the decoder
is a stack of condresblocks and attention blocks. each condresblock consists of
spade-silu-conv which takes both feature map and nuclei structure as inputs.
datasets. we conduct experiments on two datasets: monuseg [13] and kumar [14].
the monuseg dataset has 44 labeled images of size 1000 × 1000, 30 for training
and 14 for testing. the kumar dataset consists of 30 1000×1000 labeled images
from seven organs of the cancer genome atlas (tcga) database. the dataset is
splited into 16 training images and 14 testing images. paired sample synthesis.
to validate the effectiveness of the proposed augmentation method, we create 4
subsets of each training dataset with 10%, 20%, 50% and 100% nuclei instance
labels. precisely, we first crop all images of each dataset into 256 × 256
patches with stride 128, then obtain the features of all patches with pretrained
resnet50 [6] and cluster the patches into 6 classes by kmeans. patches close to
the cluster centers are selected. the encoder and decoder of the two networks
have 6 layers with channels 256, 256, 512, 512, 1024 and 1024. for the
unconditional nuclei structure synthesis network, each layer of the encoder and
decoder has 2 resblocks and last 3 layers contain attnblocks. the network is
trained using the adamw optimizer with a learning rate of 10 -4 and a batch size
of 4. for the conditional histopathology image synthesis network, each layer of
the encoder and the decoder has 2 resblocks and 2 condresblocks respectively,
and last 3 layers contain attnblocks. the network is first trained in a
fully-conditional style (drop rate = 0) and then finetuned in a classifier free
style (drop rate = 0.2). we use adamw optimizer with learning rates of 10 -4 and
2 × 10 -5 for the two training stages, respectively. the batch size is set to be
1. for the diffusion process of both steps, we set the total diffusion timestep
t to 1000 with a linear variance schedule {β 1 , ..., β t } following [8].for
monuseg dataset, we generate 512/512/512/1024 synthetic samples for
10%/20%/50%/100% labeled subsets; for kumar dataset, 256/256/256/512 synthetic
samples are generated for 10%/20%/50%/100% labeled subsets. the synthetic nuclei
structures are generate by the nuclei structure synthesis network and the
corresponding images are generated by the histopathology image synthesis network
with the classifier-free guidance scale w = 2. each follows the reverse
diffusion process with 1000 timesteps [8]. we then obtain the augmented subsets
by adding the synthetic paired images to the corresponding labeled
subsets.nuclei segmentation. the effectiveness of the proposed augmentation
method can be evaluated by comparing the segmentation performance of using the
four labeled subsets and using the corresponding augmented subsets to train a
segmentation model. we choose to train two nuclei segmentation models -hover-net
[5] and pff-net [18]. to quantify the segmentation performance, we use two
metrics: dice coefficient and aggregated jaccard index (aji) [14].
fig. 3 shows the synthetic samples from the models trained on the subset with
10% labeled images. we have the following observations. first, the synthetic
samples look realistic: the patterns of synthetic nuclei structures and textures
of synthetic images are close to the real samples. second, due to the
conditional mechanism of the image synthesis network and the classifier-guidance
sampling, the synthetic images are well aligned with the corresponding nuclei
structures, which is the prerequisite to be additional segmentation training
samples. third, the synthetic nuclei structures and images show great diversity:
the synthetic samples resemble different styles of the real ones but with
apparent differences.we then train segmentation models on the four labeled
subsets of monuseg and kumar dataset and corresponding augmented subsets with
both real and synthetic labeled images. with a specific labeling proportion, say
10%, we name the original subset as 10% labeled subset and the augmented on as
10% augmented subset. specially, 100% labeled subset is the fully-supervised
baseline. table 1 show the segmentation performances with hover-net. for monuseg
dataset, it is clear that the segmentation metrics drop with fewer labeled
images. for example, with only 10% labeled images, dice and aji reduce by 2.4%
and 3.1%, respectively. however, by augmenting the 10% labeled subset, dice and
aji exceed the fully-supervised baseline by 0.9% and 1.3%. for the 20% and 50%
case, the two metrics obtained by augmented subset are of the same level as
using all labeled images. note that the metrics of 10% augmented subset are
higher than those of 20% augmented subset, which might be attributed to the
indetermination of the diffusion model training and sampling. interestingly,
augmenting the full dataset also helps: dice increases by 1.3% and aji increases
by 1.6% compared with the original full dataset. therefore, the proposed
augmentation method consistently improves segmentation performance of different
labeling proportion. for kumar dataset, by augmenting 10% labeled subset, aji
increases to a level comparable with that using 100% labeled images; by
augmenting 20% and 50% labeled subset, ajis exceed the fully-supervised
baseline. these results demonstrate the effectiveness of the proposed
augmentation method that we can achieve the same or higher level segmentation
performance of the fully-supervised baseline by augmenting a dataset with a
small amount of labeled images.generalization of the proposed data augmentation.
moreover, we have similar observations when using pff-net as the segmentation
model. table 2 shows the segmentation results with pff-net. for both monuseg and
kumar datasets, all the four labeling proportions metrics notably improve with
synthetic samples. this indicates the generalization of our proposed
augmentation method.
in this paper, we propose a novel diffusion-based data augmentation method for
nuclei segmentation in histopathology images. the proposed unconditional nuclei
structure synthesis model can generate nuclei structures with realistic nuclei
shapes and spatial distribution. the proposed conditional histopathology image
synthesis model can generate images of close resemblance to real histopathology
images and high diversity. great alignments between synthetic images and
corresponding nuclei structures are ensured by the special design of the
conditional diffusion model and classifier-free guidance. by augmenting datasets
with a small amount of labeled images, we achieved even better segmentation
results than the fully-supervised baseline on some benchmarks. our work points
out the great potential of diffusion models in paired sample synthesis for
histopathology images.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3 57.
machine learning (ml), specifically deep learning (dl), algorithms have shown
exceptional performance on numerous medical image analysis tasks [2].
never-theless, comprehensive reviews highlight major issues of generalizability,
robustness, and reproducibility in medical imaging ai/ml [9,15]. for a
generalizability assessment, reporting only aggregate performance measures is
not sufficient. due to model complexity and limited training data, ml
performance often varies across data subgroups or domains, such as different
patient subpopulations or varied data acquisition scenarios. aggregate
performance measures (e.g., sensitivity, specificity, roc auc) can be dominated
by the larger subgroups, masking the poor ml model performance on smaller but
clinically important subgroups [11]. thus, achieving (through training) and
demonstrating (as part of testing) satisfactory ml model performance across
relevant subgroups is crucial before the real-world clinical deployment of a
medical ml system [13].however, a challenging situation arises when relevant
subgroups are unrecognized. one solution to this issue is to apply a clustering
algorithm to the data, with the goal of identifying the unannotated subgroups.
the main objective of unsupervised clustering is to group data points into
distinct classes of similar traits. however, due to the complexity and high
dimensionality of the medical imaging data and the resulting difficulty in
establishing a concrete notion of similarity, extracting low-dimensional
characteristics becomes the key to establishing the best criteria for grouping.
unsupervised generative clustering aims to simultaneously address both domain
identification and dimensionality reduction. deep unsupervised clustering
algorithms could map the medical imaging data back to their causal factors or
underlying domains, such as image acquisition equipment, patient subpopulations,
or other meaningful data subgroups. however, there is a practical need to be
able to guide the deep clustering model towards the identification of grouping
structures in a given dataset that have not been already annotated. to that end,
we propose a mechanism that is intended to constrain the model towards
identifying clusters in the data that are not associated with given variables of
choice (already known class labels or subgroup structures). the resulting
algorithmic cluster assignments could then be used to improve ml algorithm
training, or for generalizability and robustness evaluation.
we provide a pytorch-based implementation of all deep clustering algorithms
described below (vade, cdvade, and dec) in the open source python package domid
that is publicly available under https://github.com/didsr/domid.
variational deep embedding (vade) [6] is an unsupervised generative clustering
approach based on variational autoencoders [10]. in our study, vade is deployed
as a deep clustering model using convolutional neural network (cnn)
architectures for the encoder g(x; φ) and the decoder f (z; θ). the encoder
learns to compress the high-dimensional input images x into lower-dimensional
latent representations z. using a mixture-of-gaussians (mog) prior distribution
for the latent representations z, we examine subgroups or domains within the
dataset, revealed by the individual gaussians within the learned latent space,
and how z affects the generation of x. the model can be used to perform
inference, where observed images x are mapped to corresponding latent variables
z and their cluster/domain assignments c. we denote the latent space
dimensionality by d (i.e., z ∈ r d ), and the number of clusters by d (i.e., c ∈
{1, 2, . . . , d}). the trained decoder cnn can also be used to generate
synthetic images from the algorithmically identified subgroups.vade is optimized
using stochastic gradient variational bayes [10] to maximize a statistical
measure called the evidence lower bound (elbo). we denote the true data
distribution by p(z, x, c) and the variational posterior distribution by q(z,
c|x). the elbo of vade can be written aswhere p(x|z) is modeled by the decoder
cnn, and q(z|x) is modeled by the encoder cnn g(x; φ) asfinally, the cluster
assignments can be determined viawhere the probability distributions p(c) and
p(z|c) come from the mog prior of the latent space, with the respective
distributional parameters π, μ c , σ 2 c (for c ∈ {1, 2, . . . , d}) optimized
by maximizing the elbo of eq. ( 1). note that eq. ( 2) follows from the
observation that in order to maximize the elbo in eq. 1, the kl divergence
between q(c|x) and p(c|z) needs to be equal to 0. we refer to [6] for details.in
all our experiments, we apply vade with cnn architectures for the encoder and
decoder. the cnn encoder consists of convolution layers with 32, 64, 128
filters, respectively, followed by a fully-connected layer. respectively, the
cnn decoder consists of a fully-connected layer followed by transposed
convolution layers with the number of input/output channels decreasing as 128,
64, 32, 3. batch normalization and the leaky relu activation functions are used.
we propose the conditionally decoded variational deep embedding (cdvade) model
as an extension to vade as shown in fig. 1. the generative process of cdvade
differs from vade in that it concatenates additional variables y to the latent
representation z. for example, y may contain the available class labels or
already known subgroup structures, which do not need to be discovered. it is
assumed that these additional variables y are available at training and test
time. specifically, the generative process of cdvade takes the formsince our
goal is to find clusters c that are unassociated with the available variables y
of choice and to learn latent representations z that do not contain information
about y, the generative process of cdvade assumes that z, c are jointly
independent of y.the changes compared to the generative process of vade can also
be regarded as imposing a structure on the model, where the encoder learns
hidden representations of the image x conditioned to the additional variables y
(i.e., q(z|x, y)), but acts as an identity function with respect to y (i.e., y
can be regarded as being simply concatenated to the latent space representations
z). the decoder then translates this data representation in the form of (z, y)
to the input space (i.e., p(x|z, y)). given that the underlying vae architecture
seeks to efficiently compress the input data x into a learned representation,
this incentivizes the model to exclude information about y from the learned
variables z and c.the elbo of cdvade can be derived as follows,where we use the
fact that by the generative process of cdvade it holds that p(x, z, c|y) =
p(x|z, y)p(z|c, y)p(c|y) = p(x|z, y)p(z|c)p(c),and we adopt from vade the
assumption that q(z, c|x) = q(z|x)q(c|x) holds. hence, once the base vade
decoder cnn is replaced by its modified version f (z, y; θ) in cdvade, there are
no further differences between the elbo loss function of eq. ( 8) compared to
eq. ( 1).while in this work we present our conditioning mechanism as an
extension to vade, it can be combined with any deep clustering algorithm that
follows an encoder-decoder architecture. in all our experiments, we use the same
cnn architectures for the encoder and decoder as in vade (see sect. 2.1).
deep embedding clustering (dec) [14] is a popular state-of-the-art clustering
approach that combines a deep embedding model with k-means clustering. in this
study, we include comparisons of vade and the proposed cdvade to dec, because it
is a model that belongs to a different family of deep clustering algorithms
which are not based on variational inference. in our dec experiments, we use the
same autoencoder architecture and the same initialization as for the vade.
a number of studies have been conducted with several approaches of deep
clustering for medical imaging data. typically, clustering is performed on top
of features extracted with the use of an encoder neural network, and the cluster
assignments are determined by using conventional clustering algorithms, such as
k-means, on top of the learned latent representations [1,5,7,12]. in contrast,
this work investigates models which enforce a clustering structure in the latent
space through the use of a mog prior distribution, as well as guidance of the
clustering model via the proposed conditioning mechanism.
the colored mnist is an extension to the classic mnist dataset [3], which
contains binary images of handwritten digits. the colored mnist includes colored
images of the same digits, where each number and background have a color
assignment. we present results of the experiments with five distinct colors and
five digits of mnist (0-4). to enhance computational efficiency and expedite
experiments, we utilized only 1% of the mnist images, which were sampled at
random. this simple dataset can be used to investigate whether a given
clustering algorithm will categorize the images by color or by the digit label
and whether the proposed conditioning mechanism of cdvade can successfully guide
the clustering away from the categorization we want to avoid (e.g., condition
the model to avoid clustering by color, in order to distinguish the digits in an
unsupervised fashion). we compare cdvade to the deep clustering models vade and
dec that do not incorporate such conditioning. we use latent space
dimensionality d = 20 for all models. in fig. 2 a summary of the results for the
experiments on the colored mnist dataset is presented. the results demonstrate
that by allowing for the incorporation of additional information, particularly
color labels, the proposed cdvade model is more sensitized to learning other
underlying features, which allows for distinguishing between the different
digits in this particular example. notably, both vade and dec end up clustering
the data by color, as it is the most striking distinguishing characteristic of
these images. on the other hand, the predicted domains of cdvade have no
association with color, and the data are separated by the shapes in the images,
distinguishing some of the digit labels (albeit imperfectly). this example
serves as a proof of concept for the proposed conditioning mechanism of cdvade.
her2 dataset. human epidermal growth factor receptor 2 (her2 or her2/neu) is a
protein involved in normal cell growth, which plays an important role in the
diagnosis and treatment of breast cancer [8]. the dataset consists of 241
patches extracted from 64 digitized slides of breast cancer tissue which were
stained with her2 antibody. each tissue slide has been digitized at three
different sites using three different whole slide imaging systems, evaluated by
7 pathologists on a 0-100 scale, and following clinical practice labeled as her2
class 1, 2, or 3 (based on mean pathologists' scores with cut-points at 33 and
66). we use a subset of this dataset consisting of 672 images (the remainder is
held out for future research). because the intended purpose is finding subgroups
in the given dataset only, a separate test set is not used. the dimensions of
the images vary from 600 to 826 pixels, and we scale all data to a uniform size
of 128 × 128 pixels before further processing. we refer to [4,8] for more
details about this dataset. this retrospective human subject dataset has been
made available to us by the authors of the prior studies [4,8], who are not
associated with this paper. appropriate ethical approval for the use of this
material in research has been obtained.deep clustering models applied to the
her2 dataset. we evaluate the performance and behavior of the dec, vade, and
cdvade models on the her2 dataset. we investigate whether the models will learn
to distinguish the her2 class labels, the scanner labels, or other potentially
meaningful data subgroups in a fully unsupervised fashion. to investigate the
clustering abilities of cdvade on the her2 dataset, we inject the her2 class
labels into the latent embedding space. we hypothesize that this will
disincentivize the encoder network from including information related to the
her2 class labels in the latent representations z. thus, with cdvade we aim to
guide the clustering towards identifying subgroup structures that are not
associated with the her2 classes, and potentially were not previously
recognized. the dimensionality of the latent embedding space was set to d = 500
for all three models. as illustrated by the bar graphs in fig. 3, there is an
association between her2 class 2 and predicted domain 2, as well as between her2
class 3 and predicted domain 3. similarly to the vade model, the dec model has
also shown the ability to separate between her2 class 2 and her2 class 3. to
investigate these observations further, we look at the distribution of the
ground truth her2/neu scores within each of the predicted domains. the boxplots
in fig. 4 show that both the vade and dec models tend to separate high her2/neu
scores from the lower ones. the pearson's correlation coefficient between the
clustering assignments c of vade and the her2/neu scores is 0.46. the
correlation coefficient between the dec clusters and the her2/neu scores is
0.71. however, neither vade nor dec clusters are associated to the scanner
labels. we investigate the proposed cdvade model with the goal of identifying
meaningful data subgroups which are not associated with the already known her2
class labels. as visualized in fig. 3, the predicted domains are again clearly
visually disparate. however, as intended, there is a weaker association with the
her2 class labels and a stronger association with the scanner labels, compared
to the results of vade and dec. in fig. 4, her2/neu median scores of the three
clusters move closer together, illustrating the decrease of association with
her2 class labels, as intended by the formulation of the cdvade model. the
correlation coefficient between the cdvade cluster assignments and the her2/neu
scores is 0.39. while the cdvade model does not achieve full independence
between the identified clusters and the her2 labels, it decreases this
association compared to vade and dec. moreover, the clusters identified by
cdvade are distinctly different from those of vade, with a 0.43 proportion of
agreement between the two algorithms (after matching the two sets of cluster
assignments using the hungarian algorithm).
we investigated deep clustering models for the identification of meaningful
subgroups within medical image datasets. the proposed cdvade model incorporates
a conditioning mechanism that is capable of guiding the clustering model away
from subgroup structures that have already been annotated and towards the
identification of yet unrecognized image subgroups/domains. our experimental
findings on the her2 digital pathology dataset surmise that vade and dec are
capable of finding, in an unsupervised fashion, image subgroups related to the
her2 class labels, while cdvade (conditioned on the her2 labels) identifies
visually distinct subgroups that have a weaker association to the her2 labels.
because the cdvade clusters do not clearly correspond to the scanner labels
either, future work involves a review by a pathologist to see whether these
subgroups capture meaningful but unannotated characteristics in the images.
while cdvade can be used as an exploratory tool to unveil unknown subgroups in a
given dataset, developing specialized quantitative evaluation metrics for this
unsupervised task is inherently difficult and will also be a focus in our future
work.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3 64.
brachial plexopathy is a form of peripheral neuropathy [1]. it occurs when there
is damage to the brachial plexus (bp) which is a complex nerve network under the
skin of the shoulder. there is a wide range of disease that may cause a brachial
plexopathy.radiation fibrosis, primary and metastatic lung cancer, and
metastatic breast cancer account for almost three-fourths of causes [2].
brachial plexus syndrome occurs not infrequently in patients with malignant
disease. it is due to compression or direct invasion of the nerves by tumor
which will bring many serious symptoms [3]. our research focuses on the brachial
plexopathy caused by metastatic breast cancers.magnetic resonance imaging (mri)
and ultrasound of the brachial plexus have become two reliable diagnostic tools
for brachial plexopathy [4]. automatic identification of the bp in mri and
ultrasound images has become a hot topic. currently, most of relevant research
in this field are focusing on ultrasound modality [5][6][7][8]. compared with
ultrasound, mri has become the primary imaging technique in the evaluation of
brachial plexus pathology [9]. however, to our knowledge, radiomics related bp
studies utilizing mri have not been reported previously.many radiomics studies
have experimentally demonstrated that image texture has great potential for
differentiation of different tissue types and pathologies [10]. in the past
several decades, many state-of-the-art methods have been proposed to extract
texture patterns [11,12]. however, how to most effectively combine texture
features with deep learning, called deep texture, is still an open area of
research. one prior approach, termed glcm-cnn, was proposed to carry out a polyp
differentiation task [13]. however, how to arrange these glcms to form the 3d
volume to optimize the performance is a major challenge.with the goal of
classifying normal from abnormal bp, we explored the approach of deep texture
learning. this paper constructed a bp dataset with the most commonly used bp
mris in our clinical practice. considering the shortcoming of traditional
patterns, triple point pattern (tpp) is proposed for the quantitative
representation of the heterogeneity of abnormal bp's. in contrast to glcm-cnn,
tppnet is designed to train models by feeding tpp matrices as the input with a
huge number of channels. finally, we analyze the model's performance in the
experimental section. the major contributions of this study include 1) directed
triangle construction idea for tpp, 2) huge number of tpp matrices as the
heterogeneity representations of bp, 3) tppnet with 15 layers and huge number of
channels, 4) the bp dataset containing mr images and their corresponding roi
masks.
following irb approval for this study, we search for patients with metastatic
breast cancer who had a breast cancer mri performed between 2010 and 2020 and
had morphologically positive bp on the mri report from our electronic medical
records (emr) in * hospital. totally, we collect approximate 807 series which
include 274 t2, 254 t1 and 279 post-gadolinium. since some scans are seriously
degraded due to motion artifacts. therefore, each case underwent several
essential image adjustments such as multi-series splitting, two-series merging,
slice swapping, artifact checking and boundary corrections. to yield the roi,
firstly, we randomly sampled -40% of the sequences including both normal and
abnormal ones that were manually segmented with itk-snap by two skilled trainees
[14,15]. then, the manual segmentations were utilized to train a 3d nnunet model
which was utilized to train the model which was used to predict rois for the
rest series [16]. the predicted segmentations were manually divided into three
groups, i.e. good, fair and poor. good cases were added to the training set.
this process was repeated until no improvements in the predictions for the
remaining sequences was seen. the final dataset for radiomic analysis was
constructed by merging the datasets for each sequence type. only patients that
had all three sequences segmented (t2, t1 and post-gadolinium) were included in
the dataset. table 1 shows a breakdown of the final dataset.
theoretically, some texture pattern methods such as lbp, ltp, and gldm, are
based on single-variance pixel functions [17][18][19]. therefore, they extract
local texture features coded by the difference or difference counts between the
concerned pixel and its neighboring ones. one obvious shortcoming is the absence
of global properties which need other statistical methods as the aid to yield,
such as histogram and invariants [20,21]. meanwhile, some other texture methods
are generally defined by two-variance functions that only focus on two-variance
patterns, such as (pixel, pixel), (pixel, neighbor count) [22][23][24]. in
general, image textures extracted by these methods contain both local texture
properties and global texture information. their shortcomings might come from
pattern shapes which might lead to the overfitting risk while combining with
deep learning since the yielded texture matrix might have slim shapes or
adaptive columns. in summary, as the requirement of the image texture and deep
learning, an excellent image texture pattern should have some essential features
including 1) local properties to characterize the micro-unit of the image
texture, 2) global properties to represent the macro-structure of the image
texture, 3) uniform shapes under nonuniform-shape images, 4) invariant or
robustness under some common geometric transforms such as rotation, scaling and
so on.according above requirements, we developed a method to produce a serial of
novel texture patterns by introducing a directed triangle idea with an adjacent
triple pixel as a ternary group, called triple point pattern (tpp), to extract
the local texture information. then, a statistical method like histogram is
employed to count the number of the same type of pixel-triplets within the roi
or throughout the whole image. finally, a threedimensional (3d) tpp matrix is
formed to characterize the image texture globally as the
following:two-dimensional image:where i is a mxn image, x, y, and z is the pixel
triplet, x,y,z ∈ [0,l), l is its gray level, p c = (0,0) denotes the concerned
pixel such as p 0 in fig. 1, p i and p j are p c 's two adjacent pixels, i,j ∈
[1,h], h is the number of p c 's adjacent points. three-dimensional image:where
i is a three dimensional image with the shape of mxnxk, p c = (0,0,0), other
parameters are similar to the two-dimensional image.in statistics, a tpp matrix
should a 3d distribution of directed triangle. as shown in fig. 1, the tpp is
formed by the concerned pixel and its two adjacent pixels in two-dimensional(2d)
images. similarly, the tpp in 3d images is constructed by one concerned voxel
and its two neighboring voxels. more details could be found in the supplementary
material. as the construction idea of tpp, there are four independent modes
categorized by the concerned angle, i.e. 45°, 90°, 135°and 180°in 2d images,
which produce 8 tpps, 8 tpps, 8 tpps and 8 tpps respectively. analogously, the
3d image has twelve independent angle modes, i.e. 35.26°, 45°, 54.74°, 60°,
70.53°, 90°, 109.47°, 120°, 125.26°, 135°, 144.74°, and 180°. these angle modes
could generate 24 tpps, 24 tpps, 24 tpps, 24 tpps, 12 tpps, 96 tpps, 12 tpps, 24
tpps, 24 tpps, 24 tpps, 24 tpps and 13 tpps respectively. totally there are 32
tpps in 2d images and 325 tpps in 3d images and every tpp could produce one
corresponding tpp matrix.by further analysis, we could find some tpp pairs have
an isomorphism relationship since its tpp matrix could be generated by
transposing or flipping another tpp matrix on some certain conditions when two
triangles formed by the pixel triplet have the relevance of shifting or scaling.
for an instance, the tpp matrix by pixel-triplet (p 1 , p 0 , p 2 ) in fig. 1
(1), the tpp matrix by (p 4 , p 0 , p 5 ) in fig. 1 (1) and the tpp matrix by (p
6 , p 0 , p 8 ) in fig. 1(3) have the following relevance:where t denotes matrix
transposing, f represents matrix flipping, * is the product operator. since both
t and f are continuous bijective mappings, these three tpps are isomorphic.in
our study, these isomorphic tpp matrices are not dropped from the tpp matrix set
because they are equivalent to image rotations and re-scaling. image scaling can
result in the image pixels increasing. the normalization could almost remove the
effect of pixel increase caused by scaling transformations. moreover, tpps
generated by 135°c ould also be treated as affine transformations. therefore,
data augmentation could be omitted when we combine tpp with deep learning for
this study. as its definition, the tpp matrix should be a cubic array with the
shape of lxlxl where l is the gray level of the image.
the pipeline of the proposed method tppnet is illustrated in fig. 2. the tpp
matrix calculation is the preprocessing module which feeds mri and its roi and
yield tpp matrix set. the following step is the tppnet architecture to yield
training models. based on the construction idea of tpp, the size of the tpp
matrix depends on the gray level of the image. for the same image or roi, the
larger the gray level, the sparser the matrix will be. the sparse matrix would
lead to overfitting while training the model. therefore, the image requires a
re-scaling step to lower its gray level to avoid the sparsity of the tpp matrix.
consequently, our proposed tppnet only contains three convolution blocks
consisting of 15 layers. each block has two convolution, one normalization, one
max-pooling and one dropout layer. it has four particular features as follows:1)
avoidance of image augmentation. due to the stability of tpp matrix under
rotation, scale and affine transformations, image augmentation could be omitted
in the preprocessing step which can lead to image deformation. 2) huge number of
channels. tppnet treats each tpp as an independent channel. for 2d images, there
are at least 32 channels if more displacements of tpp is considered. similarly,
we could generate no less than 325 tpps in 3d images. 3) simple end-to-end
architecture. we integrate the k-fold cross-validation, tpp generation and model
training into one framework. since the tpp matrix is always small, there are
only 15 layers in tpp which could reduce the risk of overfitting issue met in
deeper neural networks. 4) free from the interference of multi-texture-pattern
arrangements. since each channel is corresponding with one tpp, it can solve the
pattern arrangement issue occurred in glcm-cnn. 3 experiments
some important specificities of our computing platform contain: one amd epyc
7352 24-core processor, 1 tb memory and four nivida a100-sxm gpus with 320 gb
gpu memory. the whole dataset is divided into three subsets according to the mr
sequence, i.e. t2, t1 and post-gad. for each subset, both normal cases and
abnormal cases were randomly and evenly split into five subgroups. a five-fold
cross-validation scheme was employed to generate five cohorts. totally, 15
cohorts were produced. each cohort consists of training set, validation set and
testing set by the ratio of 6:2:2.
since all images in our dataset are 3d images, therefore, the initial channel is
set 325 which is equal to the tpp number. the loss functions in the following
experiments shared categorical_crossentropy. nadam is adopted as the optimizer
with the learning rate of 0.0001 and batch size of 8 for 200 epochs. all
performances listed in this section are the average of performances with 5-fold
cross-validation.
the image gray level determines the shape of the tpp matrix. to avoid its
sparsity, we compute the tpp matrix set via eq. ( 2) with gray levels of 8, 12,
16, 20, 24. while rescaling the image intensity, an arc tangent approach is
utilized to yield the new image.the models are trained and tested over 15
cohorts. their performances evaluated by accuracies are listed in table2 which
tells us that t2 sequence yields the highest accuracy of 96.1% when the gray
level is 12. t1 and post-gadolinium also get acceptable results with the
accuracies of 93.5% and 93.6% respectively. other performances could be read in
supplementary materials.
loss acc loss acc loss 325 0.948±0.026 0.342±0.126 0.922±0.035 0.363±0.098
0.934±0.034 0.302±0. 124 1 0.863±0.027 0.274±0.091 0.810±0.145 0.458±0.105
0.811±0.047 0.590±0.107input channel t2 t1 pg
rescaling approaches of image intensity could also bring impacts on the bp's
differentiation while producing the tpp matrix. the commonly used methods
include minmax-linear approach [25], arc tangent approach [26], and adaptive
rescaling approach [27]. to test the performances fairly, we test above
rescaling methods at the same gray level 12. the yielded performances evaluated
by accuracies are shown in table 3 where arc tangent method achieves the highest
accuracy of 96.1% over the t2 sequence. other performances are shown in
supplementary materials.
we carry out experiments to train the tppnet model and make tests with arc
tangent rescaling approach under gray level 16. as a comparison, we test single
channel mode as well. by sharing every tpp matrix's label with the original case
label, our tppnet works well by assigning one channel for the initial input.
once the trained model with solo channel is generated, all tpp matrices of the
testing set could be tested. hereafter, we adopted a voting method to determine
if the prediction is normal, if the predicted probability is less than 0.5,
otherwise, it is considered abnormal. performances with accuracies and loss are
listed in table 4. other performances are listed in supplementary materials.
we evaluated our proposed tppnet by comparing it to the recent state-of-the-art
approaches over our bp dataset including vgg16 [28], inceptionnet [29],
mobilenet [30], glcm-cnn [13], vit [31]. the glcm size of 32 × 32 is used in
glcm_cnn. all approaches shared the same image shape of 128 × 128 × 64 with 1
channel. the patch shape of vit is 8 × 8 × 8, projection dim is 64, attention
head number is set 4 with 8 transformer layers. the intensity rescaling step
adopts the arc tangent approach. other parameters are similar to the ablation
study. their performances are
in this paper, we develop an approach to carry out the pioneer study of
differentiating abnormal bp from normal ones relevant to breast cancer. in
particular, tpp is proposed to extract texture features as the representation of
bp's heterogeneity from mris. moreover, a tppnet with huge number of initial
channels is designed to train the model. to testify our proposed tppnet, a bp
dataset is constructed with 452 series including three most commonly used mr
sequences in clinical practice, i.e. t2, t1 and post-gadolinium. the best result
is yielded when the gray level is 12, intensity rescaling method adopts arc
tangent approach. experimental outcomes also demonstrate that the proposed
tppnet not only exhibit more stable performances but also outperform six famous
state-of-the-art approaches over three most commonly used bp mr sequences.
nuclei detection is a highly challenging task and plays an important role in
many biological applications such as cancer diagnosis and drug discovery.
rectangle object detection approaches that use cnn have made great progress in
the last decade [4,7,12,14,18]. these popular cnn models use boxes to represent
objects that are not optimized for circular medical objects, such as detection
of glomeruli in renal pathology. to address the problem, an anchor-free cnnbased
circular object detection method circlenet [16] is proposed for glomeruli
detection. different from centernet [18], circlenet estimates the radius rather
than the box size for circular objects. but it also suffers poor detection
accuracy for overlapping objects and requires additional post-processing steps
to obtain the final detection results.recently, detr [1], a transformer-based
object detection method reformulates object detection as a set-to-set prediction
problem, and it removes both the hand-crafted anchors and the non-maximum
suppression (nms) postprocessing. its variants ( [3,10,11,15,19]) demonstrate
promising results compared with cnn-based methods and detr by improving the
design of queries for faster training convergence. built upon conditional-detr,
dab-detr [10] introduces an analytic study of how query design affects rectangle
object detection. specifically, it models object query as 4d dynamic anchor
boxes (x, y, w, h) and iteratively refine them by a sequence of transformer
decoders. however, recent studies on transformer-based detection methods are
designed for rectangle object detection in computer vision, which are not
specifically designed for circular objects in medical images.in this paper, we
introduce circleformer, a transformer-based circular object detection for
medical image analysis. inspired by dab-detr, we propose to use an anchor circle
(x, y, r) as the query for circular object detection, where (x, y) is the center
of the circle and r is the radius. we propose a novel circle cross attention
module which enables us to apply circle center (x, y) to extract image features
around a circle and make use of circle radius to modulate the cross attention
map. in addition, a circle matching loss is adopted in the set-to-set prediction
part to process circular predictions. in this way, our design of circle-former
lends itself to circular object detection. we evaluate our circleformer on the
public monuseg dataset for nuclei detection in whole slide images. experimental
results show that our method outperforms both cnn-based methods for box
detection and circular object detection. it also achieves superior results
compared with recently transformer-based box detection approaches. meanwhile, we
carry out ablation studies to demonstrate the effectiveness of each proposed
component. to further study the generalization ability of our approach, we add a
simple segmentation branch to circleformer following the recent query based
instance segmentation models [2,17] and verify its performance on monuseg as
well.
our circleformer (fig. 1) consists of a cnn backbone, a transformer encoder
module, a transformer decoder and a prediction head to generate circular object
results. the detail of the transformer decoder is illustrated in fig. 2.
inspired by dab-detr, we represent queries in transformer-based circular object
detection with anchor circles. we denote c i = (x i , y i , r i ) as the i-th
anchor, x i , y i , r i ∈ r. its corresponding content part and positional part
are z i ∈ r d and p i ∈ r d , respectively. the positional query p i is
calculated by:where positional encoding (pe) generates embeddings from floating
point numbers, and the parameters of the mlp are shared among all layers.in
transformer decoder, the self-attention and cross-attention are written
as:self-attn :cross-attn :where f x,y ∈ r d denote the image feature at position
(x, y) and an mlp (csq) : r d → r d is used to obtain a scaled vector
conditioned on content information for a query. by representing a circle query
as (x, y, r), we can refine the circle query layer-by-layer in the transformer
decoder. specifically, each transformer decoder estimates relative circle
information (δx, δy, δr). in this way, the circle query representation is
suitable for circular object detection and is able to accelerate the learning
convergence via layer-by-layer refinement scheme.
we propose circle-modulated attention and deformable circle cross attention to
consider size information of circular object detection in cross attention
module.circle-modulated attention. the circle radius modulated positional
attention map provides benefits to extract image features of objects with
different scales.
where r i is the radius of the circle anchor a i , and r i,ref is the reference
radius calculated bydeformable circle cross attention. we modify standard
deformable attention to deformable circle cross attention by applying radius
information as constraint. given an input feature map f ∈ r c×h×w , let i index
a query element with content feature z i and a reference point p i , the
deformable circle cross attention feature is calculated by:where m indexes the
attention head, k indexes the sampled keys. m and k are the number of
multi-heads and the total sampled key number. w m ∈ r d×d , w m ∈ r d×d are the
learnable weights and d = d/m . attn mik denotes attention weight of the k th
sampling point in the m th attention head. δr mik and δθ mik are radius offset
and angle offset, r i,ref is the reference radius. in circle deformable
attention, we transform the offset in polar coordinates to cartesian coordinates
so that the reference point ends up in the circle anchor. rather than initialize
the reference points by uniformly sampling within the rectangle as does
deformable detr, we explore two ways to initialize the reference points within a
circle, random sampling (cda-r) and uniform sampling (cda-c) (as in fig. 3).
experiments show that cda-c initialization of reference points outperforms
others.
a circle is predicted from a decoder embedding as ĉi = sigmoid(ffn(f i ) + [a i
]), where f is the decoder embedding. ĉi = (x, ŷ, r) consists of the circle
center and circle radius. sigmoid is used to normalize the prediction ĉ to the
range [0, 1]. ffn aims to predict the unnormalized box, a i is a circle anchor.
a mask is predicted from a decoder embedding by mi = ffn(ffn(f i ) + f i ),
where f is the decoder embedding. mi ∈ r 28×28 is the predicted mask. we use
dice and bce as the segmentation loss:) between prediction mi and the
groundtruth m i .
circlenet extends intersection over union (iou) of bounding boxes to circle iou
(ciou) and shows that the ciou is a valid overlap metric for detection of
circular objects in medical images. to address the difficulty optimizing
non-overlapping bounding boxes, generalized iou (giou) [13] is introduced as a
loss for rectangle object detection tasks. we propose a generalized circle iou
(gciou) to compute the similarity between two circles: gciou
, where c a and c b denotes two circles, and c c is the smallest circle
containing these two circles. we show that gciou can bring consistent
improvement on circular object detection. figure 4 shows the different
measurements between two rectangles and circles. different from circlenet that
only uses ciou in the evaluation, we incorporate gciou in the training step.
then, we define the circle loss as: l circle (c, ĉ) = λ gciou l gciou (c, ĉ) + λ
c cĉ 1 , while l gciou is generalized circle iou loss, • 1 is 1 loss, and λ
gciou , λ c ∈ r are hyperparameters.circle training loss. following detr, i-th
each element of the groundtruth set is y i = (l i , c i ), where l i is the
target class label (which may be ∅) and c i = (x, y, r). we define the matching
cost between the predictions and the groundtruth set as: (6) where σ ∈ s n is a
permutation of all prediction elements, ŷσ(i) = ( lσ(i) , ĉσ(i) ) is the
prediction, λ focal ∈ r are hyperparameters, and l focal is focal loss
[8].finally, the overall loss is:where σ(i) is the index of prediction ŷ
corresponding to the i-th ground truth y after completing the match. m i is the
ground truth obtained by roi align [5] corresponding to mi .
monuseg dataset. monuseg dataset is a public dataset from the 2018 multi-organ
nuclei segmentation challenge [6]. it contains 30 training/validataion tissue
images sampled from a separate whole slide image of h&e stained tissue and 14
testing images of lung and brain tissue images. following [16], we randomly
sample 10 patches with size 512 ×512 from each image and create 200 training
images, 100 validation images and 140 testing images.evaluation metrics. we use
ap for nuclei detection evaluation metrics as in circlenet [16], and ap m for
the instance segmentation evaluation metrics. s and m are used to measure the
performance of small scale with area less than 32 2 and median scale with area
between 32 2 and 96 2 .
two variants of our proposed method for nuclei detection, circleformer and
circleformer-d are built with a circle cross attention module and a deformable
circle cross attention module, respectively. circleformer-d-joint (ours) extends
circleformer-d to include instance segmentation as additional output. all the
models are with resnet50 as backbone and the number of transformer encoders and
decoders is set to 6. the mlps of the prediction heads share the same
parameters. since the maximum number of objects per image in the dataset is
close to 1000, we set the number of queries to 1000. the parameter of focal loss
for classification is set to α = 0.25, γ = 0.1. λ focal is set to 2.0 in the
matching step and λ focal = 1.0 in the final circle loss. we use λ iou = 2.0, λ
c = 5.0, λ dice = 8.0 and λ bce = 2.0 in the experiments. all the models are
initialized with the coco pre-trained model [9].
in table jointly outputs detection and segmentation results additionally boosts
the detection results of circleformer-d. experiments of joint nuclei detection
and segmentation are listed in table 2. our method outperforms queryinst [2], a
cnn-based instance segmentation method and soit [17], an transformer-based
instance segmentation approach. we extend transformer-based box detection method
to provide additional segmentation output inside the detection region, denoted
as deformable-detr-joint. our method with circular query representation largely
improves both detection and segmentation results.to summarize, our method with
only detection head outperforms both cnnbased methods and transformer based
approaches in most evaluation metrics for circular nuclei detection task. our
circleformer-d-joint provides superior results compared to cnn-based and
transformer-based instance segmentation methods. also, our method with joint
detection and segmentation outputs also improves the detection-only setting. we
have provided additional visual analysis in the open source code repository.
we conduct ablation studies with circleformer on the nuclei detection task
(table 5). effects of the proposed components. for simplicity, we denote the two
parts of table 3 as p1 and p2.in circleformer, the proposed circle-modulated
attention (c-ma) improves the performance of box ap from 45.7% to 48.6% box ap
(row 1 and row 2 in p1). we replaced circle iou (ciou) loss with generalized
circle iou (gciou) loss, the performance is further boosted by 2.2% (row 2 and
row 3 in p1).we obtain similar observations of circleformer-d. when using
standard deformable attention (sda), learning ciou loss gives a 1.2% improvement
on box ap compared to using box iou (row 1 and row 2 in p2). replacing ciou with
gciou, the performances of sda (row 2 and row 5 in p2), cda-r (row 3 and row 6
in p2) and cda-c (row 4 and row 7 in p2) are boostd by 0.3% box ap, 0.7% box ap
and 1.8% box ap, respectively. results show that the proposed gciou is a
favorable loss for circular object detection.two multi-head initialization
methods, random sampling (cda-r) and uniform sampling (cda-c), achieve similar
results (row 3 and row 4 in p2) and both surpass sda by 0.3% box ap (row 2 and
row 3 in p2). by using gciou, cda-r and cda-c initialization methods surpasses
sda 1.1% box ap (row 5 and row 6 in p2), and 1.8% box ap (row 5 and row 7 in
p2), respectively.numbers of multi-head and reference points. we discuss how the
number of multi-heads in the decoder affects the circleformer-d-detr model. we
vary the number of heads for multi-head attention and the performance of the
model is shown in the table 4. we find that the performance increases gradually
as the number of heads increases up to 8. however, the performance drops when
the number of head is 16. we assume increasing the number of heads brings too
many parameters and makes the model difficult to converge. similarly, we study
the impact of the number of reference points in the cross attention module. we
find that 4 reference points give the best performance. therefore, we choose to
use 8 attention heads of decoder and use 4 reference points in the cross
attention module through all the experiments.
in this paper, we introduce circleformer, a transformer-based circular medical
object detection method. it formulates object queries as anchor circles and
refines them layer-by-layer in transformer decoders. in addition, we also
present a circle cross attention module to compute the key-to-image similarity
which can not only pool image features at the circle center but also leverage
scale information of a circle object. we also extend circleformer to achieve
instance segmentation with circle detection results. to this end, our
circleformer is specifically designed for circular object analysis with detr
scheme.
# ap↑ ap (50) ↑ ap (75) ↑ ap (s) ↑ ap (m ) ↑
# ap↑ ap (50) ↑ ap (75) ↑ ap (s) ↑ ap (m ) ↑
live-cell microscopy is a fundamental tool to study the spatio-temporal dynamics
of biological systems [4,24,26]. the resulting datasets can consist of terabytes
of raw videos that require automatic methods for downstream tasks such as
classification, segmentation, and tracking of objects (e.g. cells or nuclei).
current state-of-the-art methods rely on supervised learning using deep neural
networks that are trained on large amounts of ground truth annotations
[6,25,31]. the manual creation of these annotations, however, is laborious and
often constitutes a practical bottleneck in the analysis of microscopy
experiments [6]. recently, self-supervised representation learning (ssl) has
emerged as a promising approach to alleviate this problem [1,3]. in ssl one
first defines a pretext task which can be formulated solely based on unlabeled
images (e.g. inpainting [8], or rotation prediction [5]) and tasks a neural
network to solve it, with the aim of generating latent representations that
capture high-level image semantics. in a second step, these representations can
then be either finetuned or used directly (e.g. via linear probing) for a
downstream task (e.g. image classification) with available ground truth
[7,10,18]. importantly, a proper choice of the pretext task is crucial for the
resulting representations to be beneficial for a specific downstream task.in
this paper we investigate whether time arrow prediction, i.e. the prediction of
the correct order of temporally shuffled image frames extracted from live-cell
microscopy videos, can serve as a suitable pretext task to generate meaningful
representations of microscopy images. we are motivated by the observation that
for most biological systems the temporal dynamics of local image features are
closely related to their semantic content: whereas static background regions are
time-symmetric, processes such as cell divisions or cell death are inherently
timeasymmetric (cf. fig. 1a). importantly, we are interested in dense
representations of individual images as they are useful for both image-level
(e.g. classification) or pixel-level (e.g. segmentation) downstream tasks. to
that end, we propose a time arrow prediction pre-training scheme, which we call
tap, that uses a feature extractor operating on single images followed by a time
arrow prediction head operating on the fused representations of consecutive time
points. the use of time arrow prediction as a pretext task for natural (e.g.
youtube) videos was introduced by pickup et al . [19] and has since then seen
numerous applications for image-level tasks, such as action recognition, video
retrieval, and motion classification [2,11,14,15,22,30]. however, to the best of
our knowledge, ssl via time arrow prediction has not yet been studied in the
context of live-cell microscopy. concretely our contributions are: i) we
introduce the time arrow prediction pretext task to the domain of live-cell
microscopy and propose the tap pre-training scheme, which learns dense
representations (in contrast to only image-level representations) from raw,
unlabeled live-cell microscopy videos, ii) we propose a custom
(permutation-equivariant) time arrow prediction head that enables robust
training, iii) we show via attribution maps that the representations learned by
tap capture biologically relevant processes such as cell divisions, and finally
iv) we demonstrate that tap representations are beneficial for common
image-level and pixel-level downstream tasks in live-cell microscopy, especially
in the low training data regime.
our proposed tap pre-training takes as input a set {i} of live-cell microscopy
image sequences i ∈ r t ×h×w with the goal to produce a feature extractor f that
generates c-dimensional dense representations z = f (x) ∈ r c×h×w from single
images x ∈ r h×w (cf. fig. 1b for an overview of tap). to that end, we randomly
sample from each sequence i pairs of smaller patches x 1 , x 2 ∈ r h×w from the
same spatial location but consecutive time points x 1 ⊂ i t , x 2 ⊂ i t+1 . we
next flip the order of each pair with equal probability p = 0.5, assign it the
corresponding label y (forward or backward ) and compute dense representations z
1 = f (x 1 ) and z 2 = f (x 2 ) with z 1 , z 2 ∈ r c×h×w via a fully
convolutional feature extractor f . the stacked representations z = [z 1 , z 2 ]
∈ r 2×c×h×w are fed to a time arrow prediction head h, which produces the
classification logitsboth f and h are trained jointly to minimize the losswhere
l bce denotes the standard softmax + binary cross-entropy loss between the
ground truth label y and the logits ŷ = h(z), and l decorr is a loss term that
promotes z to be decorrelated across feature channels [12,33] via maximizing the
diagonal of the softmax-normalized correlation matrix a ij :here z ∈ r c×2hw
denotes the stacked features z flattened across the non-channel dimensions, and
τ is a temperature parameter. throughout the experiments we use λ = 0.01 and τ =
0.2. note that instead of creating image pairs from consecutive video frames we
can as well choose a custom time step δt ∈ n and sample x 1 ⊂ i t and x 2 ⊂ i
t+δt , which we empirically found to work better for datasets with high frame
rate.
the time arrow prediction task has an inherent symmetry:in other words, h should
be equivariant wrt. to permutations of the input. in contrast to common models
(e.g. resnet [9]) that lack this symmetry, we here directly incorporate this
inductive bias via a permutation-equivariant head h that is a generalization of
the set permutation-equivariant layer proposed in [32] to dense inputs.
specifically, we choose h = h 1 • . . . • h l as a chain of
permutation-equivariant layers h l :with weight matrices l, g ∈ r c×c and a
non-linear activation function σ. note that l operates independently on each
temporal axis and thus is trivially permutation equivariant, while g operates on
the temporal sum and thus is permutation invariant. the last layer h l includes
an additional global average pooling along the spatial dimensions to yield the
final logits ŷ ∈ r 2 .augmentations: to avoid overfitting on artificial image
cues that could be discriminative of the temporal order (such as a globally
consistent cell drift, or decay of image intensity due to photo-bleaching) we
apply the following augmentations (with probability 0.5) to each image patch
pair x 1 , x 2 : flips, arbitrary rotations and elastic transformations (jointly
for x 1 and x 2 ), translations for x 1 and x 2 (independently), spatial
scaling, additive gaussian noise, and intensity shifting and scaling
(jointly+independently).
to demonstrate the utility of tap for a diverse set of specimen and microscopy
modalities we use the following four different datasets:hela. human cervical
cancer cells expressing histone 2b-gfp imaged by fluorescence microscopy every
30 min [29] . the dataset consists of four videos with overall 368 frames of
size 1100 × 700. we use δt = 1 for tap training.mdck. madin-darby canine kidney
epithelial cells expressing histone 2b-gfp (cf. fig. 3b), imaged by fluorescence
microscopy every 4 min [27,28]. the dataset consists of a single video with 1200
frames of size 1600×1200. we use δt ∈ {4, 8}.flywing. drosphila melanogaster
pupal wing expressing ecad::gfp (cf. fig. 3a), imaged by spinning disk confocal
microscopy every 5 min [4,20]. the dataset consists of three videos with overall
410 frames of size 3900 × 1900.we use δt = 1.yeast. s. cerevisiae cells (cf.
fig. 3c) imaged by phase-contrast microscopy every 3 min [16,17]. the dataset
consists of five videos with overall 600 frames of size 1024 × 1024. we use δt ∈
{1, 2, 3}.for each dataset we heuristically choose δt to roughly correspond to
the time scale of observable biological processes (i.e. larger δt for higher
frame rates).
for the feature extractor f we use a 2d u-net [21] with depth 3 and c = 32
output features, batch normalization and leaky relu activation (approx. 2m
params). the time arrow prediction head h consists of two permutationequivariant
layers with batch normalization and leaky relu activation, followed by global
average pooling and a final permutation-equivariant layer (approx. 5k params).
we train all tap models for 200 epochs and 10 5 samples per epoch, using the
adam optimizer [13] with a learning rate of 4 × 10 -4 with cyclic schedule, and
batch size 256. total training time for a single tap model is roughly 8h on a
single gpu. tap is implemented in pytorch.
we first study how well the time arrow prediction pretext task can be solved
depending on different image structures and used data augmentations. to that
end, we train tap networks with an increasing number of augmentations on hela
and compute the tap classification accuracy for consecutive image patches x 1 ,
x 2 that contain either background, interphase (non-dividing) cells, or mitotic
(dividing) cells. as shown in fig. 2a, the accuracy on background regions is
approx. 50% irrespective of the used augmentations, suggesting the absence of
predictive cues in the background for this dataset. in contrast, on regions with
cell divisions the accuracy reaches almost 100%, confirming that tap is able to
pick up on strong time-asymmetric image features. interestingly, the accuracy
for regions with non-dividing cells ranges from 68% to 80%, indicating the
presence of weak visual cues such as global drift or cell growth. when using
more data augmentations the accuracy decreases by roughly 12% points, suggesting
that data augmentation is key to avoid overfitting on confounding cues.next we
investigate which regions in full-sized videos are most discriminative for tap.
to that end, we apply a trained tap network on consecutive fullsized frames x 1
, x 2 and compute the dense attribution map of the classification logits y wrt.
to the tap representations z via grad-cam [23]. in fig. 3 we show example
attribution maps on top of single raw frames for three different datasets.
strikingly, the attribution maps highlight only a few distributed, yet highly
localized image regions. when inspecting the top six most discriminative regions
and their temporal context for a single image frame, we find that virtually all
of them contain cell divisions (cf. fig. 3). moreover, when examining the
attribution maps for full videos, we find that indeed most highlighted regions
correspond to mitotic cells, underlining the strong potential of tap to reveal
time-asymmetric biological phenomena from raw microscopy videos alone (cf.
supplementary video 1). finally, we emphasize the positive effect of the
permutation-equivariant time arrow prediction head on the training process. when
we originally used a regular cnn-based head, we consistently observed that the
tap loss stagnated during the initial training epochs and decreased only slowly
thereafter (cf. fig. 2b). using the permutation-equivariant head alleviated this
problem and enabled a consistent loss decrease already from the beginning of
training.
we next investigate whether the learned tap representations are useful for
common supervised downstream tasks, where we especially focus on their utility
in the low training data regime. first we test the learned representations on
two image-level classification tasks, and later on two dense segmentation tasks.
mitosis classification on flywing: since tap attribution maps strongly highlight
cell divisions, we consider predicting mitotic events an appropriate first
downstream task to evaluate tap. to that end, we generate a dataset of 97k crops
of size 2 × 96 × 96 from flywing and label them as mitotic/nonmitotic (16k/81k)
based on available tracking data [20]. we train tap networks on flywing and use
a small resnet architecture (≈ 5m params) that is trained from scratch as a
supervised baseline. in fig. 4a mitosis segmentation on flywing: we now apply
tap on a pixel-level downstream task to fully exploit that the learned tap
representations are dense. we use the same dataset as for flywing mitosis
classification, but now densely label post-mitotic cells. we predict a
pixel-wise probability map, threshold it at 0.5 and extract connected components
as objects. to evaluate performance, we match a predicted/ground truth object if
their intersection over union (iou) is greater than 0.5, and report the f1 score
after matching. the baseline model is a u-net trained from scratch. training a
u-net on fixed tap representations always outperforms the baseline, and when
only using 3% of the training data it reaches similar performance as the
baseline trained on all available labels (0.67 vs. 0.68, fig. 5a).
interestingly, fine-tuning tap only slightly outperforms the supervised baseline
for this task even for moderate amounts of training data, suggesting that fixed
tap representations generalize better for limited-size datasets.emerging bud
detection on yeast: finally, we test tap on the challenging task of segmenting
emerging buds in phase contrast images of yeast colonies. we train tap networks
on yeast and generate a dataset of 1205 crops of size 5 × 192 × 192 where we
densely label yeast buds in the central frame (defined as buds that appeared
less than 13 frames ago) based on available segmentation data [17]. we evaluate
all methods on held out test videos by interpreting the resulting 2d+time
segmentations as 3d objects and computing the f1 score using an iou threshold of
0.25. the baseline model is again a u-net trained from scratch. surprisingly,
training with fixed tap representations performs slightly worse than the
baseline for this dataset (fig. 5b), possibly due to cell density differences
between tap training and test videos. however, fine-tuning tap features
outperforms the baseline by a large margin (e.g. 0.64 vs. 0.39 for 120 frames)
across the full training data regime, yielding already with 15% labels the same
f1 score as the baseline using all labels.
we have presented tap, a self-supervised pretraining scheme that learns
biologically meaningful representations from live-cell microscopy videos. we
show that tap uncovers sparse time-asymmetric biological processes and events in
raw unlabeled recordings without any human supervision. furthermore, we
demonstrate on a variety of datasets that the learned features can substantially
reduce the required amount of annotations for downstream tasks. although in this
work we focus on 2d+t image sequences, the principle of tap should generalize to
3d+t datasets, for which dense ground truth creation is often prohibitively
expensive and therefore the benefits of modern deep learning are not fully
tapped into. we leave this to future work, together with the application of tap
to cell tracking algorithms, in which accurate mitosis detection is a crucial
component.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3_52.
whole slide image (wsi) classification is a critical task in computational
pathology enabling disease diagnosis and subtyping using automatic tools. owing
to the paucity of patch-level annotations, multiple instance learning (mil)
[9,18,24] techniques have become a staple in wsi classification. under an mil
scheme, wsis are divided into tissue patches or instances, and a feature
extractor is used to generate features for each instance. these features are
then aggregated using different pooling or attention-based operators to provide
a wsi-level prediction. imagenet pretrained networks have been widely used as
mil feature extractors. more recently, self-supervised learning (ssl), using a
large amount of unlabeled histopathology data, has become quite popular for wsi
classification [5,13] as it outperforms imagenet feature encoders.most existing
mil methods do not fine-tune their feature extractor together with their
classification task; this stems from the requirement for far larger gpu memory
than is available currently due to the gigapixel nature of wsis, e.g. training a
wsi at 10x magnification may require more than 300 gb of gpu memory. recently,
researchers have started to explore optimization methods to enable end-to-end
training of the entire network and entire wsi within gpu memory [21,25,29].
these methods show better performance compared to conventional mil; they suffer,
however, from two limitations. first, they are imagenet-pretrained and do not
leverage the powerful learning capabilities of histology-trained ssl models.
second, these are mostly limited to convolutional architectures rather than more
effective attention-based architectures such as vision transformers [7].
to improve wsi-level analysis, we explore end-to-end training of the entire
network using ssl pretrained vits. to achieve this, we use the patch batching
and gradient retaining techniques in [25]. however, we find that conventional
fine-tuning approaches, where the entire network is fine-tuned, achieve low
performance. for example, on the bright dataset [2], the accuracy drops more
than 5% compared to the conventional mil approaches. the poor performance is
probably caused by the large network over-fitted to the limited downstream
training data, leading to suboptimal feature representation. indeed, especially
for weakly supervised wsi classification, where annotated data for downstream
tasks is significantly less compared to natural image datasets, conventional
finetuning schemes can prove to be quite challenging.to address the subpar
performance of ssl-pretrained vision transformers, we utilize the prompt tuning
techniques. initially proposed in natural language processing, a prompt is a
trainable or a pre-defined natural language statement that is provided as
additional input to a transformer to guide the neural network towards learning a
specific task or objective [3,12]. using prompt tuning we fine-tune only the
prompt and downstream network without re-training the large backbone (e.g. gpt-3
with 17b parameters). this approach is parameter efficient [12,15] and has been
shown to better inject task-specific information and reduce the overfitting in
downstream tasks, particularly in limited data scenarios [8,23]. recently,
prompts have also been adopted in computer vision and demonstrated superior
performance compared to conventional fine-tuning methods [10]. prompt tuning
performs well even when only limited labeled data is available for training,
making it particularly attractive in computational pathology. the process of
prompt tuning thus involves providing a form of limited guidance during the
training of downstream tasks, with the goal of minimizing the discrepancy
between feature representations that are fully tuned to the task and those that
are not task-specific.in this paper, we propose a novel framework, prompt-mil,
which uses prompts for wsi-level classification tasks within an mil paradigm.
our contributions are:-fine-tuning: unlike existing works in histopathology
image analysis, prompt-mil is fine-tuned using prompts rather than conventional
full finetuning methods. -task-specific representation learning: our framework
employs an ssl pretrained vit feature extractor with a trainable prompt that
calibrates the representations making them task-specific. by doing so, only the
prompt parameters together with the classifier, are optimized. this avoids
potential overfitting while still injecting task-specific knowledge into the
learned representations.extensive experiments on three public wsi datasets,
tcga-brca, tcga-crc, and bright demonstrate the superiority of prompt-mil over
conventional mil methods, achieving a relative improvement of 1.49%-4.03% in
accuracy and 0.25%-8.97% in auroc by using only less than 0.3% additional
parameters. compared to the conventional full fine-tuning approach, we finetune
less than 1.3% of the parameters, yet achieve a relative improvement of
1.29%-13.61% in accuracy and 3.22%-27.18% in auroc. moreover, compared to the
full fine-tuning approach, our method reduces gpu memory consumption by 38%-45%
and trains 21%-27% faster. to the best of our knowledge, this is the first work
where prompts are explored for wsi classification. while our method is quite
simple, it is versatile as it is agnostic to the mil scheme and can be easily
applied to different mil methods. our code is available at
https://github.com/cvlab-stonybrook/promptmil.
our prompt-mil framework consists of three components: a frozen feature model to
extract features of tissue patches, a classifier that performs an mil scheme of
feature aggregation and classification of the wsis, and a trainable prompt.
given a wsi and its label y, the image is tiled into n tissue patches/instances
{x 1 , x 2 , . . . , x n } at a predefined magnification. as shown in fig. 1,
the feature model f (•) computes n feature representations from the
corresponding n patches:where h i denotes the feature of the i th patch, h is
the concatenation of all h i , and p = {p i , i = 1, 2, . . . , k} is the
trainable prompt consisting of k trainable tokens.the classifier g(•) applies an
mil scheme to predict the label ŷ and calculate the loss l as:where the l cls is
a classification loss.
the visual prompt tuning is the key component of our framework. as shown in fig.
where t 0 i is the embedding token of z i and t 0 z is the collection of such
tokens. these tokens t 0 z are concatenated with a class token t 0 cls and a
prompt p: the class token is used to aggregate information from all other
tokens. the prompt consists of k trainable tokens p = {p i |i = 1, 2, . . . ,
k}. the concatenation is fed into l layers of the transformer encoders:where p i
j is the j th output prompt token of the i th transformer encoder and t i p is
the collection of all k such output prompt tokens, which are not trainable. the
output feature of x i is defined as the last class token:
our overall loss function is defined aswhere only the parameters of the g(•) and
the prompt p are optimized, while the feature extractor model f (•) is
frozen.training the entire pipeline in an end-to-end fashion on gigapixel images
is infeasible using the current hardware. to address this issue, we utilize the
patch batching and gradient retaining techniques from [25]. as shown in fig.
1(a), to reduce the gpu memory consumption, the n tissue patches {x 1 , x 2 , .
. . , x n } are grouped into m batches. the first step (step① in the figure) of
our optimization is to sequentially feed m batches of tissue patches forward to
the feature model to compute its respective features which are subsequently
concatenated into the h matrix. in this step, we just conduct a forward pass
like the inference stage, without storing the memory-intensive computational
graph for back-propagation.in the second step (step②), we feed h into the
classifier g(•) to calculate the loss l and update the parameters of g(•) by
back-propagate the loss. the back-propagated gradients g = ∂l/∂h on h are
retained for the next step.finally (step③), we feed the input batches into the
feature model f (•) again and use the output h and the retained gradients g from
the last step to update the trainable prompt tokens. in particular, the
gradients on the j th prompt token p j are calculated as:where g i is the
gradient calculated with respect to h i .to sum up, in each step, we only update
either f or g given the current batch, which avoid storing the gradients of the
whole framework for all the input patches. this patch batching and gradient
retaining techniques make the end-to-end training feasible.in this study, we use
dsmil [13] as the classifier and binary cross entropy as the classification loss
l cls when the task is a tumor sub-type classification or cross entropy
otherwise.
we assessed prompt-mil using three histopathological wsi datasets: tcga-brca
[14], tcga-crc [19], and bright [2]. these datasets were utilized for both the
self-supervised feature extractor pretraining and the end-to-end finetuning
(with or without prompts), including the mil component. note that the testing
data were not used in the ssl pretraining. tcga-brca contains 1034 diagnostic
digital slides of two breast cancer subtypes: invasive ductal carcinoma (idc)
and invasive lobular carcinoma (ilc). we used the same training, validation, and
test split as that in the first fold cross validation in [5]. the cropped
patches (790k training, 90k test) were extracted at 5× magnification. tcga-crc
contains 430 diagnostic digital slides of colorectal cancer for a binary
classification task: chromosomal instability (cin) or genome stable (gs).
following the common 4-fold data split [1,16], we used the first three folds for
training (236 gs, 89 cin), and the fourth for testing (77 gs, 28 cin). we
further split 20% (65 slides) training data as a validation set. the cropped
patches (1.07m training, 370k test) were extracted at 10× magnification. bright
contains 503 diagnostic slides of breast tissues. we used the official training
(423 wsis) and test (80 wsis) splits.
we cropped non-overlapping 224 × 224 sized patches in all our experiments and
used vit-tiny (vit-t/16) [7] for feature extraction. for ssl pretraining, we
leveraged the dino framework [4] with the default hyperparameters, but adjusted
the batch size to 256 and employed the global average pooling for token
aggregation. we pretrained separate vit models on the tcga-crc datasets for 50
epochs, on the bright dataset for 50 epochs, and on the brca dataset for 30
epochs. for tcga-brca, we used the adamw [17] optimizer with a learning rate of
1e -4, 1e -2 weight decay, and trained for 40 epochs. for tcga-crc, we also used
the adamw optimizer with a learning rate of 5e -4 and trained for 40 epochs. for
bright, we used the adam [11] optimizer with a learning rate of 1e -4, 5e -2
weight decay and trained for 40 epochs. we applied a cosine annealing learning
rate decay policy in all our experiments. for the mil baselines, we employed the
same hyperparameters as above. for all full fine-tuning experiments, we used the
learning rate in the corresponding prompt experiment as the base learning rate.
for parameters in the feature model f (•), which are ssl pretrained, we use 1/10
of the base learning rate. for parameters in the classifier g(•), which are
randomly initialized, we use the base learning rate. we train the full tuning
model for 10 more epochs than our prompt training to allow full convergence.
this training strategy is optimized using the validation datasets. all model
implementations were in pytorch [20] on a nvidia tesla v100 or a nvidia quadro
rtx 8000.
we chose overall accuracy and area under receiver operating characteristic curve
(auroc) as the evaluation metrics.evaluation of prompt tuning performance: we
compared the proposed prompt-mil with two baselines: 1) a conventional mil model
with a frozen feature extractor [13], 2) fine-tuning all parameters in the
feature model (full fine-tuning). table 1 highlights that our prompt-mil
consistently outperformed both. compared to the conventional mil method,
prompt-mil added negligible parameters (192, less than 0.3% of the total
parameters), achieving a relative improvement of 1.49% in accuracy and 0.25% in
auroc on tcga-brca, 3.36% in accuracy and 8.97% in auroc on tcga-crc, and 4.03%
in accuracy and 0.43% in auroc on bright. the observed improvement can be
attributed to a more optimal alignment between the feature representation
learned during the ssl pretraining and the downstream task, i.e., the prompt
explicitly calibrated the features toward the downstream task.the
computationally intensive full fine-tuning method under-performed conventional
mil and prompt-mil. compared to the full fine-tuning method, our method achieved
a relative improvement of 1.29% to 13.61% in accuracy and 3.22% to 27.18% in
auroc on the three datasets. due to the relatively small amount of slide-level
labels (few hundred to a few thousands) fully fine tuning 5m parameters in the
feature model might suffer from overfitting. in contrast, our method contained
less than 1.3% of parameters compared to full fine-tuning, leading to robust
training. evaluation of time and gpu memory efficiency: prompt-mil is an
efficient method requiring less gpu memory to train and running much faster than
full fine-tuning methods. we evaluated the training speed and memory consumption
of our method and compared to the full fine-tuning baseline on four different
sized wsis in the bright dataset. as shown in table 2, our method consumed
around 38% to 45% less gpu memory compared to full finetuning and was 21% to 27%
faster. as we scaled up the wsi size (i.e. wsis with more number of patches),
the memory cost difference between prompt-mil and full fine-tuning further
widened.evaluation on the pathological foundation models: we demonstrated our
prompt-mil also had a better performance when used with the pathological
foundation model. foundational models refer to those trained on large-scale
pathology datasets (e.g. the entire tcga pan-cancer dataset [28]). we utilized
the publicly available [26,27] vit-small network pretrained using moco v3 [6] on
all the slides from tcga [28] and paip [22]. in table 3, we showed that our
method robustly boosted the performance on both tcga (the same domain as the
foundation model trained on) and bright (a different domain). the improvement is
more prominent in bright, which further confirmed that prompt-mil aligns the
feature extractor to be more task-specific. ablation study: an ablation was
performed to study the effect of the number of trainable prompt tokens on
downstream tasks. table 4 shows the accuracy and auroc of our prompt-mil model
with 1, 2 and 3 trainable prompt tokens (k = 1, 2, 3) on the tcga-brca and the
bright datasets. on the tcga-brca dataset, our prompt-mil model with 1 to 3
prompt tokens reported similar performance. on the bright dataset, the
performance of our model dropped with the increased number of prompt tokens.
empirically, this ablation study shows that for classification tasks, one prompt
token is sufficient to boost the performance of conventional mil methods.
in this work, we introduced a new framework, prompt-mil, which combines the use
of multiple instance learning (mil) with prompts to improve the performance of
wsi classification. prompt-mil adopts a prompt tuning mechanism rather than a
conventional full fine-tuning of the entire feature representation.in such a
scheme, only a small fraction of parameters calibrates the pretrained
representations to encode task-specific information, so the entire training can
be performed in an end-to-end manner. we applied our proposed method to three
publicly available datasets. extensive experiments demonstrated the superiority
of prompt-mil over the conventional mil as well as the conventional fully
finetuning methods. moreover, by fine-tuning much fewer parameters compared to
fully fine-tuning, our method is gpu memory efficient and fast. our proposed
approach also showed promising potentials in transferring foundation models. we
will further explore the task-specific features that are captured by our prompt
toward explainability of these models.
nucleus classification is to identify the cell types from digital pathology
image, assisting pathologists in cancer diagnosis and prognosis [3,30]. for
example, the involvement of tumor-infiltrating lymphocytes (tils) is a critical
prognostic variable for the evaluation of breast/lung cancer [4,29]. it is a
challenge to infer the nucleus types due to the diversity and unbalanced
distribution of nuclei. thus, we aim to automatically classify cell nuclei in
pathological images.a number of methods [7,10,14,[23][24][25]33,34] have been
proposed for automatic nuclei segmentation and classification. most of them use
a u-shape model [28] for training to produce dense predictions with expensive
pixel-level labels. in this paper, we aim to obtain the location and category of
cells, which only needs affordable labels of centroids or bounding boxes. the
task can be solved by generic object detector [17,26,27], but they are usually
built for everyday objects whose positions and combinations are quite random.
differently, in pathological images, experts often identify nuclear communities
via their relationships and spatial distribution. some recent methods resort to
the spatial contexts among nuclei. abousamra et al. [1] adopt a spatial
statistical function to model the local density of cells. hassan et al. [11]
build a location-based graph for nuclei classification. however, the semantics
similarity and dissimilarity between nucleus instances as well as the category
representations have not been fully exploited.based on these observations, we
develop a learnable grouping transformer based classifier (gtc) that leverages
the similarity between nuclei and their cluster representations to infer their
types. specifically, we define a number of nucleus clusters with learnable
initial embeddings, and assign nucleus instances to their most correlated
clusters by computing the correlations between clusters and nuclei. next, the
cluster embeddings are updated with their affiliated instances, and are further
grouped into the categorical representations. then, the cell types can be well
estimated using the correlations between the nuclei and the categorical
embeddings. we propose a novel fully transformer-based framework for nuclei
detection and classification, by integrating a backbone, a centroid detector,
and the grouping-based classifier. however, the transformer framework has a
relatively large number of parameters, which could cause high costs in
fine-tuning the whole model on large datasets. on the other hand, there exist
domain gaps in the pathological images of different organs, staining, and
institutions, which makes it necessary to fine-tune models to new applications.
thus, it is of great significance to tune our proposed transformer framework
efficiently.inspired by the prompt tuning methods [13,16,20] which train
continuous prompts with frozen pretrained models for natural language processing
tasks, we propose a grouping prompt based learning strategy for efficient
tuning. we prepend the embeddings of nucleus clusters to the input space and
freeze the entire pre-trained transformer backbone so that these group
embeddings act as prompt information to help the backbone extract grouping-aware
features. our contributions are: (1) a prompt-based grouping transformer
framework for end-to-end detection and classification of nuclei; (2) a novel
grouping prompt learning mechanism that exploits nucleus clusters to guide
feature learning with low tuning costs; (3) experimental results show that our
method achieves the state-of-the-art on three public benchmarks.
as shown in fig. 1, we propose a novel framework, prompt-based grouping
transformer (pgt), which directly outputs the coordinates of nuclei centroids
and leverages grouping prompts for cell-type prediction. in the architecture,
the detection and classification parts are interdependent and can be trained
together. the proposed framework consists of a transformer-based nucleus
detector, a grouping transformer-based classifier, and a grouping prompt
learning strategy, which are presented in the following.
backbone. we adopt swin transformer [21] as the backbone to learn deep features.
the pixel-level feature maps output from stage 2 to stage 4 of the backbone are
extracted. then the stage-4 feature map is downsampled with a 3 × 3 convolution
of stride 2 to yield another lower-resolution feature map. we obtain four
feature maps in total. the channel number of each feature map is aligned via a 1
× 1 convolution layer and a group normalization operator.encoder and decoder.
the encoder and decoder have 3 deformable attention layers [35], respectively.
the multi-scale feature maps output by the backbone are fed into the encoder in
which the pixel-level feature vectors in all these feature maps are updated via
deformable self-attention. after the attention layers, we send each feature
vector into 2 fully connected (fc) layers separately to obtain the fine-grained
categorical scores of each pixel. only the q feature vectors with the highest
confidence are preserved as object embeddings and their position coordinates are
recorded as reference points. each decoder layer utilizes crossattention to
enhance the object embeddings by taking them as queries/values and the updated
feature maps as keys. the enhanced query embeddings are fed into 2 fc layers to
regress position offsets which are added to and refine the reference points. the
reference points output by the last decoder layer are the finally detected
nucleus centroids. the last query embeddings from the decoder are sent to the
proposed classifier for cell type prediction.
in fig. 2, we develop a grouping transformer based classifier (gtc) that takes
grouping prompts g ∈ r g×d and query embeddings q ∈ r q×d as inputs, and yields
categorical scores for each nucleus query. to divide the queries into primary
groups, the similarity matrix s ∈ r g×q between the query embeddings and the
grouping prompts is built via inner product and gumbel-softmax [12] operation as
eq. ( 1):where w 1 q and w 1 k are the weights of learnable linear projections,
γ ∈ r g×q are i.i.d random samples drawn from the distribution gumbel(0, 1) and
τ denotes the softmax temperature. then we utilize the hard assignment strategy
[31,32] and assign the query embedding to different groups as eq. ( 2):where
argmax(s) returns a 1 × q vector, and one-hot(•) converts the vector to a binary
g × q matrix. sg is the stop gradient operator for better training of the
one-hot function [31,32]. then we merge the embeddings belonging to the same
group into a primary group via eq. (3):where g p denotes the embeddings of
primary groups, w 1 v and w 1 o are learnable linear weights. to separate the
primary groups into the cell categories, we measure the similar matrix between
the primary groups g p and learnable class embeddings c e ∈ r c×d to yield
advanced class embeddings c a ∈ r c×d , in the same way as eq.( 1)-(3). to
classify each centroid query, we measure the similarity between each query
embedding and the advanced class embeddings. the category whose advanced
embedding is most similar to a query, is assigned to the centroid query. the
classification results c ∈ r c×q are computed as:
the proposed method outputs a set of centroid proposals {(x q , y q )|q ∈ {1, •
• • , q}} with a decoder layer, and their corresponding cell-type scores {c q |q
∈ {1, • • • , q}} with our proposed classifier. to compute the loss with
detected centroids, we use the hungarian algorithm [15] to assign k target
centroids (ground truth) to proposal centroids and get p positive (matched)
samples and q -p negative (unmatched) samples. the overall loss is defined as
eq. ( 4):where ω 1 , ω 2 , ω 3 are weight terms, (x i , y i ) is the i th
matched centroid coordinates, (x i , ŷi ) is the target coordinates. c i and c j
denote the categorical scores of matched and unmatched samples, respectively. as
the target of unmatched samples, ĉj is set to an empty category. fl(•) is the
focal loss [18] for training the proposed classifier. we adopt the deep
supervision strategy [35]. in the training, each decoder layer produces the side
outputs of centroids and query embeddings that are fed into a gtc for
classifying nuclei. for the 3 decoder layers, they yield 3 sets of detection and
classification results for the loss in eq. (4).
to avoid the inefficient fine-tuning of the backbone, we propose a new and
simple learning strategy based on grouping prompts, as shown in fig. 1. we
inject a set of prompt embeddings as extra input of the swin-transformer [21],
and only tune the prompts instead of the backbone. to learn group-aware
representations, we further propose to share the embeddings of prompts with
those of initial groups in the proposed gtc. such prompt embeddings are define
as grouping prompts.for a typical swin-transformer backbone, an input
pathological image i ∈ r h×w ×3 is divided into hw e 2 image patches of size e ×
e. we first embed each image patch into a d-dimensional latent space via a
linear projection. then we randomly initialize the grouping prompts g ∈ r g×d as
learnable parameters, and concatenate them with the patch embeddings as input.
note that in the backbone, input patch embeddings are separated into different
local windows and the grouping prompts are also inserted into each window, as
shown in fig. 3. our proposed grouping prompt based learning consists of two
phases, pre-tuning and prompt-tuning. in the pre-tuning phase, we adopt the
swin-b backbone pre-trained on imagenet, replace the gtc head in our model (fig.
1) with 2 fc layers, and train the overall framework without prompts and gtc. in
the prompt-tuning phase, grouping prompts are added to the input of the backbone
and gtc, while the backbone parameters are frozen.3 experiments and results
consep 1 [10] is a colorectal nuclear dataset with three types, consisting of 41
h&e stained image tiles from 16 colorectal adenocarcinoma whole-slide images
(wsis). the wsis are at 20× magnification and the size of the slides is 500 ×
500. we split them following the official partition [1,10].is a breast cancer
dataset with three types and consists of 120 image tiles from 113 patients. the
wsis are at 20× magnification and the size of the slides ranges from 465 × 465
to 504 × 504. we follow the work [1] to apply the slic [2] algorithm to generate
superpixels as instances and split them into 80/10/30 slides for
training/validation/testing.lizard 3 [9] has 291 histology images of colon
tissue from six datasets, containing nearly half a million labeled nuclei in h&e
stained colon tissue. the wsis are at 20× magnification with an average size of
1,016 × 917 pixels. our implementation and the setting of hyper-parameters are
based on mmdetection [5]. the number of grouping prompts g is 64. random crop,
flipping, and scaling are used for data augmentation. our method is trained with
pytorch on a 48 gb gpu (nvidia a100) for 12-24 h (depending on the dataset
size). more details are listed in the supplementary material.
the proposed method is compared with the state-of-the-art models: the existing
methods for detecting and classifying cells in pathological images, i.e.,
hover-net [10], mcspatnet [1], sonnet [7], and the sate-of-the-art methods for
object detection in natural images, i.e., ddod [6], tood [8], dab-detr [19] and
uper-net with convnext backbone [22]. as shown in table 1, our method exceeds
all 1 https://warwick.ac.uk/fac/cross_fac/tia/data/hovernet/. 2
https://github.com/topoxlab/dataset-brca-m2c/. the other methods on three
benchmarks with both detection and classification metrics. specifically, on the
consep dataset, our approach achieves 1.6% higher f-score on the detection (f d
) and 1.8% higher f-score on the classification (f c ) than the second best
methods mcspatnet [1] and upernet [22]. on brca-m2c dataset, our method has 0.5%
higher f d and 3.9% higher f c , compared with the second best models mcspatnet
[1] and dab-detr [19]. besides, on lizard dataset, our method outperforms
upernet [22] by more than 1.5% and 6.4% on f d and f c , respectively.
meanwhile, we conduct t-test on consep dataset for statistical significance
test. the details are listed in the supplementary material. the visual
comparisons are shown in fig. 4. with the context information from surrounding
nuclei, our method effectively reduces the misclassification rate of the
lymphocytes and neutrophil categories (blue and red).
the strengths of the grouping transformer based classifier and the grouping
prompts are verified on consep dataset, as shown in table 2.prompt-based
grouping transformer (pgt) is our proposed detection and classification
architecture with grouping prompts and the gtc (in fig. 1), while the 'baseline'
has no these two settings. pt means using naive prompt tuning. gtc means
classifying nuclei with the grouping transformer. our method achieves comparable
results to the fully fine-tuning pgt with tuning only 15% parameters. compared
to the baseline, our method yields 2.4% higher f d and 2.3% higher f c ,
respectively, which shows the effective combination of the grouping classifier
and prompts. 'detached gtc & pt' means that group features and prompts are
independent. our method surpasses the detached setting by 2.4% in f d and 3.1%
in f c , which suggests that sharing embeddings of groups and prompts is
effective. with a frozen backbone, the performances of 'w/o pt' and 'w/o gtc'
are both dropping, which verifies the strength of the prompt tuning and the gtc
module, respectively. table 3 shows the effect of different numbers of grouping
prompts on consep dataset. when the number of groups is small, the
classification result is inferior. when the group number is large than 64, the
groups may contain too few nuclei to capture their common patterns. it is
suggested to set the group number to a moderate value such as 64.
we propose a new prompt-based grouping transformer framework that is fully
transformer-based, and can achieve end-to-end nuclei detection and
classification. in our framework, a grouping-based classifier groups nucleus
features into cluster and category embeddings whose correlations with nuclei are
used for identifying cell types. we further propose a novel learning scheme,
which shares group embeddings with prompt tokens and extracts features guided by
nuclei groups with less tuning costs. the results not only suggest that our
method can obtain competitive performance on nuclei classification, but also
indicate that the proposed prompt learning strategy can enhance the tuning
efficiency.
c
c f epi. c f stro. c
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3_55.
diffusion magnetic resonance imaging (dmri) is sensitive to water diffusion in
biological tissue. analytical models of dmri signals have played an essential
role in quantifying tissue microstructure in clinical studies. several methods
have been developed to use dmri signals measured with a single or multiple
b-values to estimate compartment-specific diffusivity or diffusion propagators
[1,20,24]. but these methods are developed using dmri data acquired with a fixed
echo time (te). several studies have shown that joint modeling of dmri with
multiple te can probe te-dependent diffusivity and [23], tissue-specific t 2
relaxation rate [14] and the joint relaxation diffusion distribution (rdd) in
each voxel [2,3,12,19]. more specifically, rdd functions describe the
multidimensional distribution of t 2 relaxation rate and diffusivity in each
voxel, providing a general framework to characterize heterogeneous tissue
microstructure. the rdd functions were first applied to measure the structure of
porous media [4,6,8,11]. it was generalized in [2,3,12] to probe the
microstructure of biological tissue using rdmri. a standard approach for
estimating rdd functions is to represent the measurement signal using basis
functions of different diffusivity and relaxation rates which may lead to biased
estimation results because of the strong coupling between basis signals.this
work introduces the maximum entropy (me) estimation method for more accurate
estimation of rdd functions by adapting theories and techniques developed for
the classical hausdorff moment problems [10,13,15,18]. me estimation is also a
standard approach for high-resolution power spectral estimation of time series
data which involves a similar trigonometric moment problem [7,21]. the me power
spectral estimation usually performs better than fourier transform-based methods
[21], which motivates this work to derive me methods for estimating rdd
functions and compare the results with basis function-based methods. to this
end, we first show that the problem of estimation rdd function is equivalent to
the multivariate hausdorff moment problem by applying a change of variables.
three formulations of maximum entropy (me) estimation problems are proposed to
estimate me-rdd functions in different parameter spaces. the performance of
these methods is compared with results based on basis functions using
simulations and in vivo data.
where x • θ denotes the inner product between x and θ. represents the set of all
feasible indices. next, let s k := s(x k ) which satisfies thatwhere p(θ) = e
-xmin•θ p(θ) is a scaled rdd function adjusted based on the non-zero minimum
te.the hausdorff moment problem focuses on the existence of distribution
functions that satisfy a sequence of power moments [10,15]. to change s k to
power moments as in the hausdorff moment problem, we define γ := [e -δ b θ1 , e
-δtθ2 ], which takes value in the interval γ := [e -δ b d0 , 1] × [e -δtr0 , 1].
for a vector k, we define γ k = i γ ki i = e -x k •θ following the convention in
[17]. then eq. ( 4) can be expressed using the new variables aswith). thus, s k
can be considered as the power moments of the density function f (γ) on the
interval γ. therefore the problem of estimating rdd functions using finite rdmri
measurements is equivalent to a multivariate hausdorff moment problem. we note
that the unit interval is usually considered in hausdorff moment problems. this
can be obtained by changing the variable]. thus γ takes the value on the unit
interval i 2 whose moments can be computed using linear transforms of s k . to
simplify notations, the analysis in the following subsection will be based on s
k and the distribution of γ.
the rdd functions that satisfy the rdmri data may not be unique. in moment
problems, the maximum entropy (me) method is a standard approach to estimate
probability distributions and power spectral density functions [15]. based on
the three representations of rdmri data in eqs. (3), ( 4) and ( 5), three
optimization problems are introduced to estimate me-rdd functions below.the
first me problem is developed based on eq. (3) as below:where the objective
function is the shannon differential entropy of p(θ). the solutions to me
problems have been extensively investigated in moment problems [15]. using the
lagrangian method, one can derive that the optimal solution has the following
formfor some coefficients λ k with k ∈ k. the optimal parameters λ k need to be
solved to satisfy the constraints in eq. ( 6).based on eq. ( 4), the second me
problem is introduced as below:where p(θ) is the pre-scaled rdd to adjust for
the nonzero t min . the optimal solution to eq. ( 8) has the following formit is
noted that eq. ( 9) does not include the constant -1 as in eq. ( 7) since it can
be absorbed by the variable λ 0 . then, pme2 (θ) can be scaled back to the
original rdd function by, which has a different form than the solution in eq.
(7).the third me-rdd is estimated based on the new variable γ as in eq. ( 5) by
solving the following problem:γby changing the variable γ back to θ, eq. ( 10)
is transformed toit is interesting to note that the above objective function is
equal to minimizing the kullback-leibler divergence, i.e., the relative entropy,
between p(θ) and δ b δ t e -δ •θ . the optimal solution has the following
formthen, pme3 λ (θ) is scaled back to obtainit is noted the difference between
p me1 λ (θ) and p me2 λ (θ) is related to the nonzero offset x min . the two
solutions are equal if the shortest te is zero, but it is impossible in
practice. the difference between p me2 λ (θ) and p me3 λ (θ) is related to the
sampling rate δ b and δ t . the difference is less significant, with a higher
sampling rate in the b-value and tes.
the optimal values of λ k in eqs. ( 7), ( 9) and ( 12) can be obtained by
solving the dual formulation of the me problems, which can be expressed as
energy minimization problems based on the dual formulations.for the solution in
eq. ( 7), the corresponding energy function, i.e., the dual objective function,
is given byit can be shown thatthus, the hessian matrix of δ 1 (λ) positive
definite, indicating that δ 1 (λ) is a convex function. if δ 1 (λ) has a finite
minimizer, then the minimizer satisfies thattherefore, the optimal parameters
for p me1 λ (θ) can be obtained from the minimizer of δ 1 (λ).the optimal λ for
eq. ( 9) and eq. ( 12) can be obtained by minimizing the following to convex
energy functions:in this paper, the energy minimization problem was solved using
a customized newton algorithm with the armijo line-search method [5]. the code
and data used in this work are available at
https://github.com/lipengning/me-rdd.
the proposed algorithms were examined using synthetic rdmri data with an rdd
function consisting of three gaussian components with the mean at (1.5 µm 2 /ms,
10 ms -1 ), (0.5 µm 2 /ms, 40 ms -1 ), and (1.5 µm 2 /ms, 40 ms -1 ) with the
volume fraction being 0.2, 0.5 and 0.3, respectively. d and r were uncorrelated
in each component, with the standard deviation being 0.01 µm 2 /ms and 5ms -1 .
simulated rdmri signals had b-values at b = 0, 0.5 ms/μm 2 , . . . , 5 ms/µm 2
and tes at t = 50 ms, 75 ms, . . . , 200 ms, which can be achieved using an
advanced mri scanner for in vivo human brains such as the connectom scanner [9].
then, independently and identically distributed gaussian noise was added to
simulated rdmri signals with an average signal-to-noise ratio (snr) from 100 to
600, similar to the range of the snr of direction-averaged dmri signals of in
vivo human brains that scale according to the square root of the number of
directions and voxel size.
for comparison, we applied the basis-function representation method, similar to
the methods in [12], by representing the signals aswhere θ n are a set of
predefined points on a discrete grid in θ. we solved a constrained l 2
minimization problem to find the optimal non-negative coefficient c n with
minimum l 2 norm. to evaluate the performances, we computed the error of the
center of mass (ce) of the estimated rdd functions in three regions defined
using a watershed clustering approach; see the top left figure in fig. 2.the ce
in diffusivity and re moreover, we also computed the volume-fraction error
(vfe), i.e., vfe = k k=1 |f est (k)-f true (k)|, of the estimated rdd, where f
est (k) and f true (k) denote the true and estimated volume fraction for each
component.
the proposed me algorithms were applied to an in vivo rdmri dataset acquired in
our previous work [16]. the data was acquired from a healthy volunteer on a 3t
siemens prisma scanner with the following parameters: voxel size = 2.5×2.5×
2.5mm 3 , matrix size = 96×96×54, te = 71, 101, 131, 161 and 191 ms, tr=5.9 s, b
= 700, 1400, 2100, 2800, 3500 s/mm 2 along 30 gradient directions together with
6 volumes at b = 0, simultaneous multi-slice (sms) factor = 2, and ipat = 2. the
pulse width of the diffusion gradients and the diffusion time were fixed across
scans. an additional pair of b=0 images with anterior-posterior (ap), and
posterior-anterior (pa) phase encoding directions were acquired for distortion
correction using fsl topup/eddy. then the data were further processed using the
unring [22] tool.
the first two figures in fig. 1 show the error in the center of mass of d and r.
the l2 approach based on basis functions had significantly overestimated d and
r, whereas the three me methods had much lower estimation error and similar
performance to each other. the l2 method also had a biased estimation of the
volume fraction, as shown in the third figure. figure 2 shows the true and
estimated rdd functions. the l2-rdd has more spread and biased distributions
compared to the true distribution. the three me-rdd functions were more focal
and less biased.
in summary, this work introduced a maximum-entropy framework for estimating the
relaxation-diffusion distribution functions using rdmri. to our knowledge, this
is the first work showing that the estimation of multidimensional rdd functions
is equivalent to the classical multivariate hausdorff moment problem. although
this work focuses on the two dimensional rdd functions, the results generalize
to the special cases for one dimensional relaxation or diffusion distribution
functions. the contributions of this work also include the development of three
algorithms to estimate rdd functions and the comparisons with the standard
basis-function approach. the me-rdd functions can be estimated using convex
optimization algorithms. experimental results have shown that the proposed
methods provide more accurate parameters for each component and more accurate
volume fractions compared to the standard basis function methods. moreover,
results based on in vivo data have shown that the proposed me-rdd can resolve
multiple components that cannot be distinguished by the basis function approach.
the better performance me-rdd functions compared to basis-function methods may
relate to the superior performance of me spectral estimation methods compared to
fourier transform-based methods [21]. we expect the improved spectral resolution
will be useful in several clinical applications such as lesion and tumor
detection. but further theoretical analysis on the performance of the me methods
is needed in future work. moreover, further histological validations are needed
to examine the biological basis of the rdd functions. finally, we note a
limitation of the proposed method is that the optimization algorithm may have a
slow convergence speed because the hessian matrices may not be well conditioned.
moreover, the results may be sensitive to measurement noise. thus faster and
more reliable computation algorithms will be developed in future work.
gadolinium-based contrast agents (gbcas) are widely used in mri scans owing to
their capability of improving the border delineation and internal morphology of
different pathologies and have extensive clinical applications [1]. however,
gbcas have several disadvantages like contraindications in patients with reduced
renal function [2], patient inconvenience, high operation costs and
environmental side effects [3]. therefore, there is an increased emphasis on the
paradigm of "as low as reasonably achievable" (alara) [4]. to tackle these
concerns of gbcas, several dose reduction [5,6] and elimination approaches [7]
have been proposed. however, these deep learning(dl)-based dose reduction
approaches require high quality low-dose contrast-enhanced (ce) images paired
with pre-contrast and full-dose ce images. acquiring such a dataset requires
modification of the standard imaging protocol and involves additional training
of the mr technicians. therefore, it is important to simulate the process of t1w
low-dose image acquisition, using images from the standard protocol. moreover,
it is crucial for these dose reduction approaches to establish the minimum dose
level required for different pathologies as these are dependent on the scanning
protocol and the gbca compound injected. therefore the simulation tool should
also have the ability to synthesize images with multiple contrast enhancement
levels, that correspond to multiple arbitrary dose levels.currently mri dose
simulation is done using physics-based models [8]. however, these physics-based
methods are dependent on the protocol parameters and the type of gbca and their
relaxation parameters. deep learning (dl) models have been widely used in
medical imaging application due to their high capacity, generazibility, and
transferability [9,10]. the performance of these dl models heavily depend on the
availability of high quality data. there is a dearth of datadriven approaches to
mri dose-simulation given the lack of diverse ground truth data of the different
dose levels. to this effect, we introduce a vision transformer based dl model1
that can synthesize brain 2 mri images that correspond to arbitrary dose levels,
by training on a highly imbalanced dataset with only t1w pre-contrast, t1w 10%
low-dose, and t1w ce standard dose images. the model backbone consists of a
novel global transformer (gformer) with subsampling attention that can learn
long-range dependencies of contrast uptake features. the proposed method also
consists of a rotational shift operation that can further capture the shape
irregularity of the contrast uptake regions. we performed extensive quantitative
evaluation in comparison to other state-of-the art methods. additionally, we
show the clinical utility of the simulated t1w low-dose images using downstream
tasks. to the best of our knowledge, this is the first dl based mri dose
simulation approach.
iterative learning design: dl based models tend to perform poorly when the
training data is highly imbalanced [11]. furthermore, the problem of arbitrary
dose simulation requires the interpolation of intermediate dose-levels using a
minimum number of data points. iterative models [12,13] applications as they
work on the terminal images to generate step-wise intermediate solutions. we
first utilize this design paradigm for the dose simulation task and train an
end-to-end model on a highly imbalanced dataset where only t1w pre-contrast, t1w
low-dose, and t1w post-contrast are available.as shown in fig. 1 where p pre , p
post , and p low denote the pre-contrast, post-contrast, and predicted low-dose
images, respectively and p i-1 denotes the image with a higher enhancement than
p i . this way, the intermediate outputs { p i } k i=1 having different
enhancement levels, correspond to images with different contrast dose level with
a uniform interval. this iterative model essentially learns a gradual dose
reduction process, in which each iteration step removes a certain amount of
contrast enhancement from the full-dose image.
the proposed iterative model aims to learn a mapping from the post-contrast &
pre-contrast images to the synthesized low-dose images p low and is trained with
the true 10% low-dose image p low as the ground truth. we used the l 1 and
structural similarity index measure (ssim) losses. to tackle the problem of
gradient explosion or vanishing, "soft labels" are generated using linear
scaling. these "soft labels" serve as a reference to the intermediate outputs
during the iterative training process and also aid model convergence, without
which the model has to directly learn from post-contrast to low-dose. given k
iterations, the soft label {s i } k-1 i=1 for iteration i is calculated as
follows:where p post and p pre denote the skull-stripped post-contrast and
pre-contrast images. γ = 0.1 represents the dose level of the final prediction,
and τ = 0.1 denotes the threshold to extract the estimated contrast uptake u =
relu( p post -p preτ ). finally, the total losses are calculated aswhere l e = l
l1 + l ssim and α = 0.1 and β = 1. the "soft labels" are assigned a small loss
weight so that they do not overshadow the contribution of the real low-dose
image. additionally, in order to recover the high frequency texture information
and to improve the overall perceptual quality, adversarial [14] and perceptual
losses [15] are applied on ( p low , p low ) with a weight of 0.1.global
transformer (gformer): transformer models have risen to prominence in a wide
range of computer vision applications [10,16]. traditional swin transformers
compute attention on non-overlapping local window patches. to further exploit
the global contrast information, we propose a hybrid global transformer
(gformer) as a backbone for the dose simulation task. as illustrated in fig.
1(b), the proposed model design includes six sequential gformer blocks as the
backbone module with shortcuts. as shown in fig. 1(c), the gformer block
contains a convolution block, a rotational shift module, a sub-sampling process,
and a typical transformer module. the convolution layer extracts granular local
information of the contrast uptake while the self-attention emphasizes more on
the coarse global context, thereby paying attention to the overall contrast
uptake structure.subsampling attention: the sub-sampling is a key element in the
gformer block which generates a number of sub-images from the whole image as
attention windows as shown in fig. 2. gformer performs self-attention on the
sub-sampled images, which encompasses global contextual information with minimal
selfattention overhead on small feature maps. given the entire feature map m e ∈
r b×c×h×w , where b, c, h, and w are the batch size, channel dimension, height,
and width, respectively, the subsampling process aggregates the strided
positions to the sub-feature maps as follows, where d denotes sampling a
position every d pixels, andd is the subsampled feature map. we set h, d = 0 to
avoid any information loss during subsampling. these d 2 sub-feature maps are
stacked onto the batch dimension as the attention windows for the transformer
block shown in fig. 1
rotational shift: image rotation has been widely used as a data augmentation
technique in preprocessing and model training. here, to further capture the
heterogeneous nature of the contrast uptake areas, we employ the rotational
shift as a module to facilitate the representation power of the gformer. to
prevent information loss on the edges due to rotation, only small angles (e.g.,
10 • , 20 • ) are used for rotation and residual shortcuts are also applied.
specifically, given the feature map m o ∈ r b×c×h×w , rotational shift is
performed around the vertical axis of height/width. the rotated feature map m r
∈ r b×c×h×w is obtained by the following equation:where λ is the rotation angle.
(p, q, x, y) and (p , q , x , y ) denote the pixel index in the feature map
tensor before and after rotational shift, respectively.
dataset: with irb approval and informed consent, we retrospectively used 126
clinical cases (113 training, 13 testing) from a internal private dataset3 using
gadoterate meglumine contrast agent (site a). for downstream task assessment we
used 159 patient studies from another site (site b) using gadobenate
dimeglumine. the detailed cohort description is given in table 1. the clinical
indications for both sites included suspected tumor, post-op tumor follow-up and
routine brain. for each patient, 3d t1w mprage scans were acquired for the
pre-contrast, low-dose, and post-contrast images. these paired images were then
mean normalized and affine co-registered (pre-contrast as the fixed image) using
simpleelastix [17]. the images were also skull-stripped, to account for
differences in fat suppression, using the hd-bet brain extraction tool [18] for
generating the "soft labels". evaluation settings: we quantitatively evaluated
the proposed model using psnr, ssim, rmse, and lpips perceptual metrics [19],
between the synthesized and true low-dose images. we replaced the gformer
backbone with other state-of-the-art methods to compare the efficacy of the
different methods. particularly, the following backbone networks were studied:
simple linear scaling ("scaling") approach, rednet [20], mapnn [13], restormer
[21], and swinir [22]. unet [23] and swin-unet [24] models were not assessed due
to their tendency to synthesize blurry artifacts in the iterative modelling.
throughput metric (number of images generated per second) was also calculated to
assess the inference efficiency.evaluation results: figure 4(a) shows that the
proposed model is able to generate images that correspond to different dose
levels. as shown in the zoomed inset, the hyperintensity of the contrast uptake
in these images gradually reduces at each iteration. figure 4(b) shows that the
pathological structure in the synthesized low-dose image is similar to that of
the ground truth. figure 4(c) also shows that the model is robust to
hyperintensities that are not related to contrast uptake. figure 3 and table 2
show that proposed model can synthesize enhancement patterns that look close to
the true low-dose and that it performs better than the other competing methods
with a reasonable inference throughput. quantitative assessment of contrast
uptake: the above pixel-based metrics do not specifically focus on the contrast
uptake region. in order to assess the contrast uptake patterns of the
intermediate images, we used the following metrics as described in [25]:
contrast to noise ratio(cnr), contrast to background ratio(cbr), and contrast
enhancement percentage(cep). the roi for the contrast uptake was computed as the
binary mask of the corresponding "soft labels" in eq. 2. as shown in fig. 5, the
value of the contrast specific metrics increases in a non-linear fashion as the
iteration step increases. downstream tasks: in order to demonstrate the clinical
utility of the synthesized low-dose images, we performed two downstream tasks:
1) low-dose to full-dose synthesis using the dl-based algorithm to predict
full-dose image from pre-contrast and low-dose images described in [5], we
synthesized t1ce volumes using true low-dose (t1ce-real-ldose) and gformer (rot)
synthesized low-dose (t1ce-synth-ldose). we computed the psnr and ssim metrics
of t1ce vs t1ce-synth/t1ce vs t1ce-synth-sim which are 29.82 ± 3.90 db/28.10 ±
3.20 db and 0.908 ± 0.031/0.892 ± 0.026 respectively. this shows that the
synthesized low-dose images perform similar4 to that of the low-dose image in
the dose reduction task. for this analysis we used the data from site b. 2)
tumor segmentation using the t1ce volumes synthesized in the above step, we
perform tumor segmentation using the winning solution of brats 2018 challenge
[26]. let m true , m ldose and m ldose-sim be the whole tumor (wt) masks
generated using t1ce, t1ce-real-ldose and t1ce-synth-ldose (+ t1, t2 and flair
images) respectively. the mean dice scores dice(m true , m ldose ) and dice(m
true , m ldose-sim ) on the test set were 0.889±0.099 and 0.876±0.092
respectively. figure 6 shows visual examples of tumor segmentation performance.
this shows that the clinical utility provided by the synthesized low-dose is
similar5 to that of the actual low-dose image.
we have proposed a gformer-based iterative model to simulate low-dose ce images.
extensive experiments and downstream task performance have verified the efficacy
and clinical performance of the proposed model compared to other state-of-the
art methods. in the future, further reader studies are required to assess the
diagnostic equivalence of the simulated low-dose images. the model can be guided
using physics-based models [27] that estimate contrast enhancement level using
signal intensity. this simulation technique can easily be extended to other
anatomies and contrast agents.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3_9.
diffusion mri (dmri) tractography is the only non-invasive method capable of
mapping the complex white matter (wm) connections within the brain [2].
tractography parcellation [12,28,42] classifies the vast numbers of streamlines
resulting from whole-brain tractography to enable visualization and
quantification of the brain's wm connections. (here a streamline is defined as a
set of ordered points in 3d space resulting from tractography [45]). in recent
years, deep-learning-based methods have been proposed for tractography
parcellation [5,7,14,15,17,20,33,34,36,37,39,44], of which many methods are
designed to classify streamlines [7,[15][16][17]37,39,44]. however, multiple
challenges exist when using streamline data as deep network input. one
well-known challenge is that streamlines can be equivalently represented in
forward or reverse order [11,39], complicating their direct representation as
vectors [7] or images [44]. another challenge is that the geometric
relationships between the streamlines in the brain have previously been ignored:
existing parcellation methods [7,[15][16][17]37,39,44] train and classify each
streamline independently. finally, computational cost can pose a challenge for
the parcellation of large tractography datasets that can include thousands of
subjects with millions of streamlines per subject.in this work, we propose a
novel point-cloud-based strategy that leverages neighboring and whole-brain
streamline information to learn local-global streamline representations. point
clouds have been shown to be efficient and effective representations for
streamlines [1,4,6,15,18,39] in applications such as tractography filtering [1],
clustering [7], and parcellation [15,18,38,39]. one benefit of using point
clouds is that streamlines with equivalent forward and reverse point orders
(e.g., from cortex to brainstem or vice versa) can be represented equally.
however, these existing methods focus on a single streamline (one point cloud)
and ignore other streamlines (other point clouds) in the same brain that may
provide important complementary information useful for tractography
parcellation. in computer vision, point clouds are commonly used to describe
scenes and objects (e.g., cars, tables, airplanes, etc.). however, point cloud
segmentation methods from computer vision, which assign labels to points, cannot
translate directly to the tractography field, where the task of interest is to
label entire streamlines. computer vision studies [21,26,32,35,40,41,46] have
shown that point interactions within one point cloud can yield more effective
features for downstream tasks. however, in tractography parcellation we are
interested in the relationship between multiple point clouds (streamlines) in
the brain. these other streamlines can provide detailed information about the
local wm geometry surrounding the streamline to be classified, as well as global
information about the location and pose of the brain that can reduce the need
for image registration.affine or even nonrigid registration is needed for
current tractography parcellation methods [13,28,42]. recently,
registration-free techniques have been proposed for tractography parcellation to
handle computational challenges resulting from large inter-subject variability
and to increase robustness to image registration inaccuracies [19,29]. avoiding
image registration can also reduce computational time and cost when processing
very large tractography datasets with thousands of subjects. while other
registration-free tractography parcellation techniques require freesurfer input
[29] or work with rigidly mni-aligned human connectome project data [19], our
method can directly parcellate tractography in individual subject space.in this
study, we propose tractcloud, a registration-free tractography parcellation
framework, as illustrated in fig. 1. this paper has three main contributions.
first, we propose a novel, learnable, local-global streamline representation
that leverages information from neighboring and whole-brain streamlines to
describe the local anatomy and global pose of the brain. second, we leverage a
training strategy using synthetic transformations of labeled tractography data
to enable registration-free parcellation at the inference stage. third, we
implement our framework using two compared point cloud networks and demonstrate
fast, registration-free, whole-brain tractography parcellation across the
lifespan.
we utilized a high-quality and large-scale dataset of 1 million labeled
streamlines for model training and validation. the dataset was obtained from a
wm tractography atlas [42] that was curated and annotated by a neuroanatomist.
the atlas was derived from 100 registered tractography of young healthy adults
in the human connectome project (hcp) [30]. the training data includes 43 tract
classes: 42 anatomically meaningful tracts from the whole brain and one tract
category of "other streamlines," including, most importantly, anatomically
implausible outlier streamlines. on average, the 42 anatomical tracts have 2539
streamlines with a standard deviation of 2693 streamlines.for evaluation, we
used a total of 120 subjects from four public datasets and one private dataset.
these five datasets were independently acquired with different imaging protocols
across ages and health conditions. (1) developing hcp (dhcp) [9] s1. the
twotensor unscented kalman filter (ukf) [22,25,27] method, which is consistent
across ages, health conditions, and image acquisitions [42], was utilized to
create whole-brain tractography for all subjects across the datasets mentioned
above.
synthetic transform data augmentation. to enable tractography parcellation
without registration, we augmented the training data by applying synthetic
transform-based augmentation (sta) including rotation, scaling, and
translations. these transformations have been used in voxel-based wm
segmentation [34], but no study has applied these transformations to study
tractography, to our knowledge. in detail, we applied 30 random transformations
to each subject tractography in the training dataset to obtain 3000 transformed
subjects and 30 million streamlines. transformations included: rotation from -45
to 45 • c along the left-right axis, from -10 to 10 • c along the
anterior-posterior axis, and from -10 to 10 • c along the superior-inferior
axis; translation from -50 to 50 mm along all three axes; scaling from -45% to
5% along all three axes. these transformations were selected based on typical
differences between subjects due to variability in brain anatomy and volume,
head position, and image acquisition protocol. many methods are capable of
tractography parcellation after affine registration [12,42]; therefore, with sta
applied to the training dataset, our framework has the potential for
registration-free parcellation.module for local-global streamline representation
learning. we propose a module (fig. 2) to learn the proposed local-global
representation, which benefits from information about the anatomy of the
neighboring wm and the overall pose of the brain. we construct the input for the
learning module by concatenating the coordinates of the original streamline (the
one to be classified), its local neighbor streamlines, and global whole-brain
streamlines. in detail, assume a brain has n streamlines, denoted by s = {s 1 ,
s 2 , . . . , s n }, s i ∈ r m×3 , where 3 is the dimensionality of the point
coordinates and m is the number of points for each streamline (m = 15 as in
[42,44]). for streamline s i , we obtain a set of k nearest streamlines, local(s
i ) = {s j1 , s j2 , . . . , s jk }, using a pairwise streamline distance [11].
from the whole brain, we also randomly select a set of w streamlines, global(s i
) = {s q1 , s q2 , . . . , s qw }. then s i , local(s i ), and global(s i ) are
concatenated as shown in fig. 2 to obtain the input of the module, t i ∈ r m×6×
(k+w) . the proposed module begins with a shared fully connected (fc) layer with
relu activation function (h θ ): (k+w) , where h is the output dimension of h θ
(h=64 [3,26,32]). finally, the local-global representation r i is obtained
through max-pooling [3,26,32,46]. here, we explore two widely used networks:
pointnet [3] and dynamic graph convolutional neural network (dgcnn) [32].
pointnet (see fig. s1 for network details) encodes point-wise features
individually, but dgcnn (see fig. s2 for network details) encodes point-wise
features by interacting with other points on a streamline. both pointnet and
dgcnn then aggregate features of all points through pooling to get a single
streamline descriptor, which is input into fully connected layers for
classification.
to learn r i , we used 20 local streamlines (selected from 10, 20, 50, 100) and
500 global streamlines (selected from 100, 300, 500, 1000). our framework was
trained with the adam optimizer with a learning rate of 0.001 using
cross-entropy loss. the epoch was 20, and the batch size was 1024. training of
our registrationfree framework (tractcloud reg-free ) with the large sta dataset
took about 22 h and 10.9 gb gpu memory with pytorch (v1.13) on an nvidia rtx
a5000 3 experiments and results
we evaluated our method on the original labeled training dataset (registered and
aligned) and its synthetic transform augmented (sta) data (unregistered and
unaligned). we divided both the original and sta data into train/validation/test
sets with the distribution of 70%/10%/20% by subjects (such that all streamlines
from an individual subject were placed into only one set, either train or
validation or test). for experimental comparison, we included two
deep-learning-based state-of-the-art (sota) tractography parcellation methods:
dcnn++ [37] and deepwma [44]. they were both designed to perform deep wm
parcellation using cnns, with streamline spatial coordinate features as input.
we trained the networks based on the recommended settings in their papers and
code. two widely used point-cloud-based networks (pointnet [3] and dgcnn [32]),
with a single streamline as input, were included as baseline methods. to
evaluate the effectiveness of the local-global representation in tractcloud, we
performed experiments using only local neighbor features (pointnet +loc and
dgcnn +loc ) and both local neighbor and whole-brain global features (pointnet
+loc+glo and dgcnn +loc+glo ). for all methods, we report two metrics (accuracy
and macro f1) that are widely used for tractography parcellation
[19,24,37,39,44]. the accuracy is reported as the overall accuracy of streamline
classification, and the macro f1 score is reported as the mean across 43 tract
classes (table 1). table 1 shows that the tractcloud framework achieves the best
performance on data with and without synthetic transformations (sta). especially
on sta data, tractcloud yields a large improvement in accuracy (up to 9.9%) and
f1 (up to 13.8%), compared to pointnet and dgcnn baselines as well as sota
methods. in addition, including local (pointnet +loc and dgcnn +loc ) and global
(pointnet +loc+glo and dgcnn +loc+glo ) features both improve the performance
compared to baselines (pointnet and dgcnn) with a single streamline as input.
this demonstrates the effectiveness of our local-global representation.
we performed experiments on five independently acquired, unlabeled testing
datasets (dhcp, abcd, hcp, ppmi, btp) to evaluate the robustness and
generalization ability of our tractcloud reg-free framework on unseen and
unregistered data. all compared sota methods (deepwma, dcnn++) and tractcloud
regist were tested on registered tractography, and only tractcloud reg-free was
tested on unregistered tractography. tractography was registered to the space of
the training atlas using an affine transform produced by registering the
baseline (b = 0) image of each subject to the atlas population mean t2 image
using 3d slicer [10]. for each method, we quantified the tract identification
rate (tir) and calculated the tract-to-atlas distance (tad), and statistical
significance tests were performed for results of tir and tad (table 2). tir
measures if the tract is identified successfully when labels are not available
[7,42,44]. here, we chose 50 as the minimum number of streamlines for a tract to
be considered as identified (the threshold of 50 is more strict than 10 or 20 in
[7,42,44]). as a complementary metric for tir, tda measures the geometric
similarity between identified tracts and corresponding tracts from the training
atlas. for each testing subject's tract, we calculated the streamline-specific
minimum average direct-flip distance [7,11,42] to the atlas tract and then
computed the average across subjects and tracts to obtain tda. we also recorded
the computation time for tractography parcellation for every method (table 2).
the computation time was tested on a linux workstation with an nvidia rtx a4000
gpu using tractography (0.28 million streamlines) from a randomly selected
subject. to evaluate if differences in result values between our
registration-free method (tractcloud reg-free ) and other methods are
significant, we implemented a repeated measure anova test for all methods across
subjects, and then we performed multiple paired student's t-tests between
tractcloud reg-free method and each compared method. in addition, in order to
evaluate how well our framework can perform without registration, we converted
identified tracts into volume space and calculated the spatial overlap (weighted
dice) [8,43] between results of tractcloud regist and tractcloud reg-free (table
3). furthermore, we also provide a visualization of identified tracts in an
example individual subject for every dataset across methods (fig. 2).as shown in
table 2, all methods achieve high tirs on all datasets, and the tir metric does
not have significant differences across methods. this demonstrates that most
tracts can be identified by all methods robustly. however, our registration-free
framework (tractcloud reg-free ) obtains significantly lower tda values (better
quality of identified tracts) than all compared methods on abcd, hcp, and ppmi
datasets, where ages of test subjects are from 9 to 75 years old. on the very
challenging dhcp (baby brain) dataset, tractcloud reg-free still significantly
outperforms two sota methods. note that tractcloud reg-free directly table 2.
results of tract identification rate (tir) and tract distance to atlas (tda) on
five independently acquired testing datasets as well as computation time on a
randomly selected subject. tir results show no significant differences across
methods (anova p > 0.05), while tda results do (anova p < 1 × 10 -10 ).
asterisks show that the difference between tractcloud reg-free and other methods
is significant using a paired student's t-test. ( * p < 0.05, * * p < 0.001).
abbreviations: tc -tractcloud. the tract spatial overlap (wdice) is over 0.965
on all datasets, except for the challenging dhcp (wdice is 0.932) (table 3).
overall, our registration-free framework is comparable to (or better than) our
framework with registration.figure 3 shows visualization results of example
tracts. all methods can successfully identify these tracts across datasets. it
is visually apparent that the tractcloud reg-free framework obtains results with
fewer outlier streamlines, especially on the challenging dhcp dataset.
we have demonstrated tractcloud, a registration-free tractography parcellation
framework with a novel, learnable, local-global representation of streamlines.
experimental results show that tractcloud can achieve efficient and consistent
tractography parcellation results across populations and dmri acquisitions, with
and without registration. the fast inference speed and robust ability to
parcellate data in original subject space will allow tractcloud to be useful for
analysis of large-scale tractography datasets. future work can investigate
additional data augmentation using local deformations to potentially increase
robustness to pathology. overall, tractcloud demonstrates the feasibility of
registration-free tractography parcellation across the lifespan.
works on unregistered tractography from neonate brains (much smaller than adult
brains). in the challenging btp (tumor patients) dataset, tractcloud reg-free
obtains significantly lower tda values than sota methods and comparable
performance to tractcloud regist . as shown in table2, our registration-free
framework is much faster than other compared methods.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3_40.
recent advances in diffusion mri (dmri) and diffusion signal modeling equip
brain researchers with an in vivo probe into microscopic tissue compositions
[15,21]. signal differences between water molecules in restricted, hindered, and
free compartments can be characterized by higher-order diffusion models for
estimating the relative proportions of cell bodies, axonal fibers, and
interstitial fluids within an imaging voxel. this allows for the detection of
tissue compositional changes driven by development, degeneration, and disorders
[13,22]. however, accurate characterization of tissue composition is not only
affected by compartment-specific diffusivities but also transverse relaxation
rates [4,27]. several studies have shown that explicit consideration of the
relaxation-diffusion coupling may improve the characterization of tissue
microstructure [6,16,25].multi-compartment models are typically used to
characterize signals from, for example, intra-and extra-neurite compartments
[18,29]. however, due to the multitude of possible compartments and fiber
configurations, solving for these models can be challenging. the problem can be
simplified by considering per-axon diffusion models [8,10,28], which typically
factor out orientation information and hence involve less parameters. however,
existing models are typically constrained to data acquired with a single te
(ste) and do not account for compartment-specific t 2 relaxation. several
studies have shown that multi-te (mte) data can account better for intravoxel
architectures and fiber orientation distribution functions (fodfs)
[1,6,16,17,19].here, we propose a unified strategy to estimate using mte
diffusion data (i) compartment specific t 2 relaxation times; (ii) non-t 2
-weighted (non-t 2 w) parameters of multi-scale microstructure; and (iii) non-t
2 w multi-scale fodfs. our method, called relaxation-diffusion spectrum imaging
(rdsi), allows for the direct estimation of non-t 2 w volume fractions and t 2
relaxation times of tissue compartments. we evaluate rdsi using both ex vivo
monkey and in vivo human brain mte data, acquired with fixed diffusion times
across multiple b-values. using rdsi, we demonstrate the te dependence of t 2 w
fodfs. furthermore, we show the diagnostic potential of rdsi in differentiating
tumors and normal tissues.
the diffusion-attenuated signal s(τ, b, g) acquired with te τ , diffusion
gradient vector g, and gradient strength b can be modeled aswhich can be
expanded to a multi-compartment model:to account for signals s r (b, g), s h (b,
g), and s f (b) and t 2 values of restricted, hindered, and free compartments.
the apparent relaxation rates at different b-values, r(b) = 1/t 2 (b), can be
estimated using single-shell data acquired with two or more tes [14]. this model
can be expressed using spherical deconvolution [9]:where the
compartment-specific response functions r(b, g, d r ), r(b, g, d h ), and r(b, d
f ) are associated with apparent diffusion coefficients d r , d h , and d f ,
yielding compartment-specific multi-scale fodfs f (d r ), f (d h ), and f (d f
). operator a relates the spherical harmonics coefficients to fodf amplitudes.
the spherical mean technique (smt) [10] focuses on the direction-averaged signal
to factor out the effects of the fiber orientation distribution. taking the
spherical mean, (3) can be written aswhere w(d r ), w(d h ), and w(d f ) are
volume fractions and k(b, d r ), k(b, d h ), and k(b, d f ) are spherical means
of response functions r(b, g, d r ), r(b, g, d h ), and r(b, d f ),
respectively. based on [8,10], spherical means can be written as:where d r , d h
, and d f are parameterized by parallel diffusivity λ and perpendicular
diffusivity λ ⊥ for the restricted (λ r ), hindered (λ h ) and free (λ f )
compartments. φ is the geometric tortuosity [28]. the spherical mean signal can
thus be seen as the weighted combination of the spherical mean signals of spin
packets. similar to [8,28], (4) allows us to probe the relaxation-diffusion
coupling across a spectrum of diffusion scales. anisotropic diffusion can be
further separated as restricted or hindered.
we first solve for the relaxation modulated spherical mean coefficients in (4).
next, we disentangle the relaxation terms from the spherical mean coefficients
and solve for the relaxation rates. finally, we estimate the fodfs using (3).
details are provided below:(i) relaxation modulated spherical mean coefficients.
we rewrite (4) in matrix form aswhere the mean signal s is expressed as the
product of the response function spherical mean matrix k and the kronecker
product (•) of relaxation matrix e and volume fraction matrix w. we can solve
for x in ( 6) via an augmented problem with the osqp solver 1 :(ii) relaxation
times. with x solved, e and w can be determined by minimizing a constrained
non-linear multivariate problem:which can be solved using a gradient based
optimizer. relaxation times can be determined based on e.(iii) fodfs. with e
determined, (3) can be rewritten as a strictly convex quadratic programming (qp)
problem:which can be solved using the osqp solver.
based on (3) and ( 4), various microstructure indices can be
derived:-microscopic fractional anisotropy [20], per-axon axial and radial
diffusivity [2], and free and restricted isotropic diffusivity. -axonal
morphology indices derived based on [17,26] to compute the mean neurite radius
(mean nr), its internal deviation (std. nr), and relative neurite radius (cov.
nr):, where 0 is a pulse scale that only depends on the pulse width δ and
diffusion time δ of the diffusion gradients.), which is independent on .
ex vivo data. we used an ex vivo monkey dmri dataset2 collected with a 7t mri
scanner [19]. implementation. to cover the whole diffusion spectrum, we set the
diffusivity from 0 s/mm 2 (no diffusion) to 3 × 10 -3 s/mm 2 (free diffusion).
for the anisotropic compartment, λ was set from 1.5 × 10 -3 mm 2 /s to 2 × 10 -3
mm 2 /s. radial diffusivity λ ⊥ was set to satisfy λ /λ ⊥ ≥ 1.1 as in [8,28].
for the isotropic compartment, we set the diffusivity λ = λ ⊥ from 0 mm 2 /s to
3×10 -3 mm 2 /s with step size 0.1×10 -3 mm 2 /s.
figure 1(a) shows the estimated maps of t 2 -independent parameters given by a
baseline comparison method, called redim [16], and rdsi. we observe that the two
methods yield similar intracellular volume fraction (icvf) estimates. however,
redim overestimates the anisotropic volume fraction (avf) compared to rdsi,
resulting in blurred boundaries between the gray matter and superficial white
matter. rdsi yields consistent distribution between icvf and μfa maps. figure
1(b) shows the rdsi t 2 relaxation maps of restricted, hindered, and free
diffusion across b-values. as the b-value increases, the relaxation time
increases for the restricted component but decreases for the hindered and free
components. at lower b-values, the relaxation time for the extra-neurite
compartment is substantially higher than that of the intra-neurite compartment.
figure 2 shows the rdsi t 2 relaxation maps of restricted, hindered, and free
diffusion across b-values. the values are consistent between healthy and glioma
subjects. the estimated relaxation times are in general in line with previous
reports [6,11]. rdsi shows substantial differences between tumor and normal
tissues in the relaxation maps (fig. 2(b)).figure 3 shows the voxel
distributions with respect to relaxation times and b-values. it is apparent that
at higher b-values, a greater fraction of voxels in the restricted compartment
have relaxation times within 100 to 200 ms, particularly for higher-grade
gliomas. this might be related to prolonged transverse relaxation time due to
increased water content within the tumor [5,7,24]. this property is useful in
the visualization of peritumoral edema, an area containing infiltrating tumor
cells and increased extracellular water due to plasma fluid leakage from
aberrant tumor capillaries that surrounds the tumor core in higher-grade
gliomas.
figure 4(a) shows the relaxation times of the restricted compartment in white
matter lesions, indicating that relaxation times are longer in gliomas than
normal white matter tissue. the higher t 2 in grade 4 glioma is associated with
changes in metabolite compositions, resulting in remarkable changes in neurite
morphology in lesioned tissues (fig. 4(c-d)), consistent with previous
observations [12,23]. the rate of longitudinal relaxation time has been shown to
be positively correlated with myelin content. our results indicate that mte dmri
is more sensitive to neurite morphology than ste dmri (fig. 4(b)).figures 4(c-d)
show that the estimated mean nr in the gray matter is approximately in the range
of 10 µm, which is in good agreement with the sizes of somas in human brains,
i.e., 11 ± 7 µm [26]. rdsi improves the detection of small metastases,
delineation of tumor extent, and characterization of the intratumoral
microenvironment when compared to conventional microstructure models (fig.
4(c)). our studies suggest that rdsi provides useful information on
microvascularity and necrosis helpful for facilitating early stratification of
patients with gliomas (fig. 4(d)).
figure 5 shows the relaxation-diffusivity distributions of white matter (wm),
cortical gray matter (gm), and subcortical gray matter (sgm). the 2d plots show
the contours of the joint distributions of the relaxation and diffusivity values
across all voxels. the average diffusivity and relaxation in these regions
indicate the existence of a single homogeneous region in wm and sgm. for gm,
however, we observe a small peak for the relaxation rate arange 1e-3 to 1.5e-3.
figure 6 shows that the reconstructed fodfs are consistent with the expected wm
arrangement of the healthy human brain. we provide a visual comparison of the
fodfs estimated with and without the explicit consideration of relaxation. the
two cases yield different fodfs. as expected, fiber populations are associated
with different relaxation times, in line with [3,16]. our studies suggest that
this difference could be caused by the spatially heterogeneous tissue
microstructure, since fiber bundles with slower relaxation times contribute less
to diffusion signals acquired with a longer te. explicitly taking into account
relaxation in our model results in noteworthy contrast improvement in spatially
heterogeneous superficial wm.
rdsi provides a unified strategy for direct estimation of relaxation-independent
volume fractions and compartment-specific relaxation times. using mte data, we
demonstrated that rdsi can delineate heterogeneous tissue microstructure elusive
to ste data. we also showed that rdsi provides information that is conducive to
characterizing tissue abnormalities.
cell recognition serves a key role in exploiting pathological images for disease
diagnosis. clear and accurate cell shapes provide rich details: nucleus
structure, cell counts, and cell density of distribution. hence, pathologists
are able to conduct a reliable diagnosis according to the information from the
segmented cell, which also improves their experience of routine pathology
workflow [5,14].in recent years, the advancement of deep learning has
facilitated significant success in medical images [17,18,20]. however, the
supervised training requires massive manual labels, especially when labeling
cells in histopathology images. a large number of cells are required to be
labeled, which results in inefficient and expensive annotating processes. it is
also difficult to achieve accurate labeling because of the large variations
among different cells and the variability of reading experiences among
pathologists.work has been devoted to reducing dependency on manual annotations
recently. qu et al. use points as supervision [19]. it is still a
labor-intensive task due to the large number of objects contained in a
pathological image. with regard to unsupervised cell recognition, traditional
methods can segment the nuclei by clustering or morphological processing. but
these methods suffer from worse performance than deep learning methods. among
ai-based methods, some works use domain adaptation to realize unsupervised
instance segmentation [2,9,12], which transfers the source domain containing
annotations to the unlabeled target. however, their satisfactory performance
depends on the appropriate annotated source dataset. hou et al. [10] proposed to
synthesize training samples with gan. it relies on predefined nuclei texture and
color. feng et al. [6] achieved unsupervised detection and segmentation by a
mutual-complementing network. it combines the advantage of correlation filters
and deep learning but needs iterative training and finetuning.cnns with
inductive biases have priority over local features of the nuclei with dense
distribution and semi-regular shape. in this paper, we proposed a simple but
effective framework for unsupervised cell recognition. inspired by the strong
representation capability of self-supervised learning, we devised the prior
self-activation maps (psm) as the supervision for downstream cell recognition
tasks. firstly, the activation network is initially trained with self-supervised
learning like predicting instance-level contrastiveness. gradient information
accumulated in the shallow layers of the activation network is then calculated
and aggregated with the raw input information. these features extracted from the
activation network are then clustered to generate pseudo masks which are used
for downstream cell recognition tasks. in the inferring stage, the networks
which are supervised by pseudo masks are directly applied for cell detection or
segmentation. to evaluate the effectiveness of psm, we evaluated our method on
two datasets. our framework achieved comparable performance on cell detection
and segmentation on par with supervised methods. code is available at
https://github.com/cpystan/psm.
the structure of our proposed method is demonstrated in fig. 1. firstly, an
activation network u ss is trained with self-supervised learning. after the
backpropagation of gradients, gradient-weighted features are exploited to
generate the self-activation maps (psm). next is semantic clustering where the
psm is combined with the raw input to generate pseudo masks. these pseudo masks
can be used as supervision for downstream tasks. related details are discussed
in the following.
we introduce self-supervised learning to encourage the network to focus on the
local features in the image. and our experiments show that neural networks are
capable of adaptively recognizing nuclei with dense distribution and
semi-regular shape. here, we have experimented with several basic proxy tasks
below.
it is straightforward to exploit the models pre-trained on natural images. in
this strategy, we directly extract the gradient-weighted feature map in the
imagenet pre-trained network and generate prior self-activation
maps.contrastiveness: following the contrastive learning [3] methods, the
network is encouraged to distinguish between different patches. for each image,
its augmented view will be regarded as the positive sample, and the other image
sampled in the training set is defined as the negative sample. the network is
trained to minimize the distance between positive samples. it also maximizes the
distance between the negative sample and the input image. the optimization goal
can be denoted as:where l dis is the loss function. z l , z r , and z n are
representations of the input sample, the positive sample, and the negative
sample, respectively. in addition, dif f (•) is a function that measures the
difference of embeddings.similarity: lecun et al. [4] proposed a siamese network
to train the model with a similarity metric. we also adopted a weight-shared
network to learn the similarity discrimination task. in specific, the pair of
samples (each input and its augmented view) will be fed to the network, and then
embedded as highdimensional vectors z l and z r in the high-dimensional space,
respectively. based on the similarity measure sim(•), l dis is introduced to
reduce the distance, which is denoted as,here, maximizing the similarity of two
embeddings is equal to minimizing their difference.
the self-supervised model u ss is constructed by sequential blocks which contain
several convolutional layers, batch normalization layers, and activation
layers.the self-activation map of a certain block can be obtained by nonlinearly
mapping the weighted feature maps a k :where i am is the prior self-activation
map. a k indicates the k-th feature map in the selected layer. α k is the weight
of each feature map, which is defined by global-average-pooling the gradients of
output z with regard to a k :where i, j denote the height and width of output,
respectively, and n indicates the input size. the obtained features are
visualized in the format of the heat map which is later transformed to pseudo
masks by clustering.semantic clustering. we construct a semantic clustering
module (scm) which converts prior self-activation maps to pseudo masks. in scm,
the original information is included to strengthen the detailed features. it is
defined as:where i f denotes the fused semantic map, i raw is the raw input, β
is the weight of i raw .to generate semantic labels, an unsupervised clustering
method k-means is selected to directly split all pixels into several clusters
and obtain foreground and background pixels. given the semantic map i f and its
n pixel featuresthe goal is to find s to reach the minimization of withinclass
variances as follows:where c i denotes the centroid of each cluster s i . after
clustering, the pseudo mask i sg can be obtained.
in this section, we introduce the training and inferring of cell recognition
models.cell detection. for the task of cell detection, a detection network is
trained under the supervision of pseudo mask i sg . in the inferring stage, the
output of the detection network is a score map. then, it is post-processed to
obtain the detection result.the coordinates of cells can be got by searching
local extremums in the score map, which is described below:where t (m,n) denotes
the predicted label at location of (m, n), p is the value of the score map and d
(m,n) indicates the neighborhood of point (m, n). t (m,n) is exactly the
detection result.cell segmentation. due to the lack of instance-level
supervision, the model does not perform well in distinguishing adjacent objects
in the segmentation.to further reduce errors and uncertainties, the voronoi map
i vor which can be transformed from i sg is utilized to encourage the model to
focus on instance-wise features. in the voronoi map, the edges are labeled as
background and the seed points are denoted as foreground. other pixels are
ignored.we train the segmentation model with these two types of labels. the
training loss function can be formulated as below,where λ is the partition
enhancement coefficient. in our experiment, we discovered that false positives
hamper the effectiveness of segmentation due to the ambiguity of cell
boundaries. since that, only the background of i sg will be concerned to
eliminate the influence of false positives in instance identification.
dataset. we validated the proposed method on the public dataset of multi-organ
nuclei segmentation (monuseg) [13] and breast tumor cell dataset (bcdata) [11].
monuseg consists of 44 images of size 1000 × 1000 with around 29,000 nuclei
boundary annotations. bcdata is a public large-scale breast tumor dataset
containing 1338 immunohistochemically ki-67 stained images of size 640 × 640.
evaluation metrics. in our experiments on monuseg, f1-score and iou are employed
to evaluate the segmentation performance. denote t p , f p , and f n as the
number of true positives, false positives, and false negatives. then f1score and
iou can be defined as: f 1 = 2t p/(2t p + f p + f n), iou = t p/(t p + f p + f
n). in addition, common object-level indicators such as dice coefficient and
aggregated jaccard index (aji) [13] are also considered to assess the
segmentation performance.in the experiment on bcdata, precision (p), recall (r),
and f1-score are used to evaluate the detection performance. predicted points
will be matched to ground-truth points one by one. and those unmatched points
are regarded as false positives. precision and recall are: p = t p/(t p + f p ),
and r = t p/(t p + f n). in addition, we introduce mp and mn to evaluate the
cell counting results. 'mp' and 'mn' denote the mean average error of positive
and negative cell numbers.hyperparameters. res2net101 [7] is adopted as the
activation network u ss with random initialization of parameters. the positive
sample is augmented by rotation. the weights β are set to 2.5 and 4 for training
in monuseg and bcdata, respectively. the weight λ is 0.5. the analysis for β and
λ is included in the supplementary. pixels of the fused semantic map will be
decoupled into three piles by k-means. the following segmentation and detection
are constructed with resnet-34. they are optimized using crossentropy loss by
the adam optimizer for 100 epochs with the initial learning rate of 1e -4 . the
function dif f (•) is instantiated as the measurement of manhattan distance.
this section includes the discussion of results which are visualized in fig. 2
segmentation. in monuseg dataset, four fully-supervised methods unet [20], medt
[24], cdnet [8], and the competition winner [13] are adopted to estimate the
upper limit as shown in the first four rows of table 1. two weakly-supervised
models trained with only point annotations are also adopted as the
comparison.compared with the method [22] fully exploiting localization
information, ours can achieve better performance without any annotations in
object-level metrics (aji). in addition, two unsupervised methods using
traditional image processing tools [1,21] and two unsupervised methods [9,10]
with deep learning are compared. our framework has achieved promising
performance because robust low-level features are exploited to generate
high-quality pseudo masks.detection. following the benchmark of bcdata, metrics
of detection and counting are adopted to evaluate the performance as shown in
table 2. the first three methods are fully supervised methods which predict
probability maps to achieve detection. furthermore, transcrowd [16] with the
backbone of swin-transformer is employed as the weaker supervision trained by
cell counts regression. by con- trast, even without any annotation supervision,
compared to csrnet [15], np-cnn [23] and u-csrnet [11], our proposed method
still achieved comparable performance. especially in terms of mp, our model
surpasses all the baselines. it is challenging to realize multi-class
recognition in an unsupervised framework. our method still achieves not bad
counting results on negative cells.ablation study. ablation experiments are
built on monuseg. in our pipeline, the activation network can be divided into
four layers which consists of multiple basic units including relu, bacthnorm,
and convolution. we exploit the prior self-activation maps generated from
different depths in the model after training with the same proxy tasks. as shown
in fig. 3, the performance goes down and up with we extracting features from
deeper layers. due to the relatively small receptive field, the shallowest layer
in the activation network is the most capable to translate local descriptions we
have also experimented with different types of proxy tasks in a selfsupervised
manner, as shown in table 3. we can see that relying on the pretrained models
with external data can not improve the results of subsequent segmentation. the
model achieves similar pixel-level performance (f1) when learning similarity or
contrastiveness. but similarity learning makes the model performs better in
object-level metrics (aji) than contrastive learning. the high intra-domain
similarity hinders the comparison between constructed image pairs. unlike
natural image datasets containing diverse samples, the minor inter class
differences in biomedical images may not fully exploit the superiority of
contrastive learning.
in this paper, we proposed the prior self-activation map (psm) based framework
for unsupervised cell segmentation and multi-class detection. the framework is
composed of an activation network, a semantic clustering module (scm), and the
networks for cell recognition. the proposed psm has a strong capability of
learning low-level representations to highlight the area of interest without the
need for manual labels. scm is designed to serve as a pipeline between
representations from the activation network and the downstream task. and our
segmentation and detection network are supervised by the pseudo masks. in the
whole training process, no manual annotation is needed. our unsupervised method
was evaluated on two publicly available datasets and obtained competitive
results compared to the methods with annotations. in the future, we will apply
our psm to other types of medical images to further release the dependency on
annotations.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3 54.
brain magnetic resonance imaging (mri) has been increasingly used to assess
future progression of cognitive impairment (ci) in various clinical and research
fields by providing structural brain anatomy [1][2][3][4][5][6]. many
learning-based methods have been developed for automated mri analysis and brain
disorder prognosis, which usually heavily rely on labeled training data
[7][8][9][10]. however, it is generally time-consuming and tedious to collect
category labels for brain mris in practice, resulting in a limited number of
labeled mris [11].fig. 1. illustration of brain anatomy-guided representation
(bar) learning framework for assessing the clinical progression of cognitive
impairment. the bar consists of a pretext model and a downstream model, with a
shared brain anatomy-guided encoder for mri feature extraction. the pretext
model also contains a decoder for brain tissue segmentation, while the
downstream model relies on a predictor for prediction. the pretext model is
trained on the large-scale adni [12] with 9,544 t1-weighted mris, yielding a
generalizable encoder. with this learned encoder frozen, the downstream model is
then fine-tuned on target mris for prediction tasks.even without task-specific
category label information, brain anatomical structures provided by auxiliary
mris can be employed as a prior to boost disease progression prediction
performance. considering that there are a large number of unlabeled mris in
existing large-scale datasets [12,13], several deep learning methods propose to
extract brain anatomical features from mri without requiring specific category
labels. for instance, song et al. [14] suggest that the anatomy prior can be
utilized to segment brain tumors, while yamanakkanavar et al. [15] discuss how
brain mri segmentation improves disease diagnosis. unfortunately, there are few
studies that try to utilize such brain anatomy prior for assessing the clinical
progression of cognitive impairment with structural mris.to this end, we propose
a brain anatomy-guided representation (bar) learning framework for cognitive
impairment prognosis with t1-weighted mris, incorporated with brain anatomy
prior provided by a brain tissue segmentation task. as shown in fig. 1, the bar
consists of a pretext model and a downstream model, with a shared anatomy-guided
encoder for mri feature extraction. these two models also use a decoder and a
predictor for brain tissue segmentation and disease progression prediction,
respectively. the pretext model is trained on 9,544 mri scans from the public
alzheimer's disease neuroimaging initiative (adni) [12] without any category
label information, yielding a generalizable encoder. the downstream model is
further fine-tuned on target mris for ci progression prediction. experiments are
performed on two ci-related studies with 391 subjects, with results suggesting
the efficacy of bar compared with state-ofthe-art (sota) methods. the pretext
model can also be used for brain tissue segmentation in other mri-based studies.
to the best of our knowledge, this is the first work that utilizes anatomy prior
derived from large-scale t1-weighted mris for automated cognitive decline
analysis. to promote reproducible research, the source code and trained models
have been made publicly available to the research community (see
https://github.com/goodaycoder/bar).
data and preprocessing. the pretext model is trained via a tissue segmentation
task on auxiliary mris (without category label) from adni. a total of 9,544
t1-weighted mris from 2,370 adni subjects with multiple scans are used in this
work. to provide accurate brain anatomy, we perform image preprocessing and
brain tissue segmentation for these mris to generate ground-truth segmentation
of three tissues, i.e., white matter (wm), gray matter (gm) and cerebrospinal
fluid (csf), using an in-house toolbox ibeat [16] with manual verification.the
downstream model is trained on 1) a late-life depression (lld) study with 309
subjects from two sites [17,18], and 2) a type 2 diabetes mellitus (dm) study
with 82 subjects from the first affiliated hospital of guangzhou university of
chinese medicine. subjects in lld are categorized into three groups: 1) 89
non-depressed cognitively normal (cn), 2) 179 depressed but cognitively normal
(cnd), 3) 41 depressed subjects (called ci) who developed cognitive impairment
or even dementia in the follow-up years. category labels in the lld study are
determined based on subjects' 5-year follow-up diagnostic information, while
mris are acquired at baseline time. the dm contains 1) 45 health control (hc)
subjects and 2) 37 diabetes mellitus patients with mild ci (mci). detailed image
acquisition protocols are given in table si of supplementary materials. all mris
are preprocessed via the following pipeline: 1) bias field correction, 2) skull
stripping, 3) affine registration to the mni space, 4) resampling to 1 × 1 × 1
mm 3 , 5) deformable registration to aal3 [19] with syn [20], and 6) warping 166
regions-of-interest (rois) of aal3 back to mri volumes.proposed method. while it
is often challenging to annotate mris in practice, there are a large number of
mris (without task-specific category labels) in existing large-scale datasets.
even without category labels, previous studies propose to extract anatomical
features (e.g., roi volumes of gm segmentation maps) to characterize brain
anatomy [21,22]. such brain anatomy prior learned via tissue segmentation can be
employed to boost learning performance intuitively. accordingly, we propose a
brain anatomy-guided representation (bar) learning framework for progression
prediction of cognitive impairment, incorporated with brain anatomy prior
provided by brain tissue segmentation. as shown in fig. 1, the bar consists of a
pretext model for brain tissue segmentation and a downstream model for disease
progression prediction, both equipped with brain anatomy-guided encoders (shared
weights) for mri feature learning.(1) pretext model for segmentation. to learn
brain anatomical features from mris in a data-driven manner, we propose to
employ a segmentation task for pretext model training. as shown in the top of
fig. 1, the pretext model consists of 1) a brain anatomy-guided encoder for mri
feature extraction and 2) a decoder for segmentation. the brain anatomy-guided
encoder takes large-scale auxiliary 3d mris without category labels as input,
and outputs 512 feature maps. it contains 8 convolution blocks, with each block
containing a convolution layer (kernel size: 3 × 3 × 3), followed by instance
normalization and parametric rectified linear unit (prelu) activation. the first
4 blocks downsample the input with a stride of 2 × 2 × 2. the channel numbers of
the eight blocks are [64, 128, 256, 512, 512, 512, 512, 512], respectively. a
skip connection is applied to sum the input and output of every two of the last
4 blocks for residual learning.the decoder takes the 512 feature maps as input
and outputs segmentation maps of three tissues (i.e., wm, gm, and csf), thus
guiding the encoder to learn brain anatomical features. the decoder contains
four deconvolution blocks with 256, 128, 64 and 4 channels, respectively. each
deconvolution block shares the same architecture as the convolution block in the
encoder. the output of the decoder is then fed into a softmax layer to get four
probability maps that indicate the probability of a voxel belonging to a
specific tissue (i.e., background, wm, gm, and csf). besides, the reconstruction
task can be used to train the pretext model instead of segmentation when lacking
ground-truth segmentation maps. for problems without ground-truth segmentation
maps, we can resort to an mri reconstruction task to train the pretext model in
an unsupervised manner.(2) downstream model for prediction. as shown in the
bottom panel of fig. 1, the downstream model takes target mris as input and
outputs probabilities of category labels. it consists of 1) a brain
anatomy-guided encoder and 2) a predictor for prognosis. this encoder shares the
same architecture and parameters as that of the pre-trained pretext model. with
the encoder frozen, predictor parameters will be updated when the downstream
model is trained on target mris. the predictor has two convolution blocks
(kernel size: 3 × 3 × 3, stride: 2 × 2 × 2, channel: 256) with a skip
connection, followed by a flatten layer, an fc layer, and a softmax layer. the
architecture of the predictor can be flexibly adjusted according to the
requirements of different downstream tasks.(3) implementation. the proposed bar
is trained via two steps. 1) the pretext model is first trained on 9,544 mris
from adni, with ground-truth segmentation as supervision. the adam optimizer
[23] with a learning rate of 10 -4 and dice loss are used for training (batch
size: 4, epoch: 30). 2) we then share sii of supplementary materials. such
partition is repeated five times independently to avoid any bias introduced by
random partition, and the mean and standard deviation results are recorded. the
training data is duplicated and augmented using random affine transform. five
evaluation metrics are used, including area under roc curve (auc), accuracy
(acc), sensitivity (sen), specificity (spe), and f1-score (f1s). besides, we
perform tissue segmentation by directly applying the trained pretext model to
target mris from lld and dm studies and visually compare the results of our bar
with those of fsl [24].competing methods. we compare our bar with two classic
machine learning methods and five sota deep learning approaches, including (1)
support vector machine (svm) [25] with a radial basis function kernel
(regularization: 1.0), ( 2) xgboost (xgb) [26] (estimators: 300, tree depth: 4,
learning rate: 0.2), (3) resnetx with x convolution layers, (4) med3dx [27] with
x convolution layers, (5) seresnet [28] that is an improved model by adding
squeeze and excitation blocks to resnet, (6) efficientnet [29], and (7)
mobilenet [30] that is an efficient lightweight cnn model. for svm and xgb, we
extract roibased wm and gm volumes of each mri as input. all competing deep
learning methods (with default architectures) take whole 3d mris as input and
share the same training strategy as that used in the downstream model of the
bar. an early-stop training strategy (epoch: 90) is used in all deep learning
models. results of depression and ci identification. in this task, we aim to
recognize cognitively normal subjects with depression with a higher risk of
progressing to ci than healthy subjects. the results of fourteen methods on the
lld study are reported in table 1, where '*' denotes that the results of bar and
a competing method are statistically significantly different (p < 0.05 via
paired t-test).from the left of table 1, we have the following observations on
cnd vs. cn classification. first, our bar model generally outperforms thirteen
competing methods in most cases. for instance, the bar yields the results of auc
= 70.5% and sen = 77.3%, which are 4.1% and 10.0% higher than those of the
secondbest methods (i.e., seresnet and resnet50), respectively. this implies
that the brain anatomical mri features learned by our pretext model on
large-scale datasets would be more discriminative, compared with those used in
the competing methods. second, among 10 deep models, our bar produces the lowest
standard deviation in most cases (especially on sen and spe), suggesting its
robustness to bias introduced by random data partition in the downstream task.
this could be due to the strong generalizability of the feature encoder guided
by brain anatomy prior (derived from the auxiliary tissue segmentation task). in
addition, our bar significantly outperforms four machine learning methods and
two lightweight deep models (i.e., efficientnet and mobilenet) with p <
0.05.from the right of table 1, we can see that the overall results of fourteen
methods in ci vs. cnd classification are usually worse than cnd vs. cn
classification. this suggests that the task of ci vs. cnd classification is more
challenging, which could be due to the more imbalanced training data in this
task (as shown in table sii of supplementary materials). on the other hand, the
proposed bar still performs best in terms of auc=64.5% and spe=67.0%, which are
2.8% and 2.0% higher than those of the second-best competing methods (i.e.,
xgb-wm and resnet34), respectively. these results further demonstrate the
superiority of the bar in mri-based depression recognition.results of mci
detection. the results of different methods in mci detection (i.e., mci vs. hc
classification) on the dm study are reported in table 2. there are a total of 42
subjects (i.e., 17 mci and 25 hc) used for training in this task, which are
fewer but more balanced than the two tasks in the lld study (see table sii). it
can be observed from tables 1 and2 that the proposed bar yields relatively lower
standard deviations in terms of auc and acc in mci vs. hc classification,
compared with the two tasks on the lld study. these results imply that data
imbalance may be an important issue affecting the performance of deep learning
models when the number of training samples is limited.segmentation results. the
pre-trained pretext model can also be used for brain tissue segmentation in
downstream studies. thus, we visualize brain segmentation maps generated by fsl
and our bar for target mris in both lld and dm studies in fig. 2. note that
t1-weighted mris in the lld study are collected from 2 sites and have more
inconsistent image quality when compared to those from dm. from fig. 2, we have
several interesting observations.first, the segmentation results generated by
the proposed bar are generally better than those of fsl in most cases,
especially for those cortical surface areas on the two studies. for instance,
the wm region in segmentation maps generated by our bar is much cleaner than
that of fsl, indicating that our model is not sensitive to noise in mri. even
for the lld study with significant inter-site data heterogeneity, the boundary
of wm and gm produced by bar is more continuous and smoother, which is in line
with the brain anatomy prior. second, for mris with severe motion artifacts in
the lld study (ids: 1240, 1334, and 1653), our method can produce high-quality
segmentation maps, and the results are even comparable to those of mris without
motion artifacts. this demonstrates that our model is robust to motion
artifacts. the underlying reason could be that the pretext model is trained on
large-scale mris, and thus, has good generalization ability when applied to mris
with different image quality. in addition, both bar and fsl often achieve better
results in the dm study, since dm has relatively higher image quality than lld.
still, the proposed bar can achieve better segmentation results in many
fine-grained brain regions, such as the putamen region (see hc001 and mci003)
and the vermis region (see hc004). these results demonstrate that our method has
good adaptability when applied to classification and segmentation tasks in
mri-based studies.ablation study. to validate the effectiveness of the learned
brain anatomical mri features, we further compare the bar with its two variants
(called bar-b and bar-r) that use anatomy prior derived from different pretext
tasks in cnd vs. cn classification on lld. specifically, the bar-b is trained
from scratch as a baseline on target data without any pre-trained encoder. the
bar-r trains the pretext model through an mri reconstruction task in an
unsupervised learning manner. as shown in fig. 3(a), the bar consistently
performs better than its variants in terms of all five metrics. this implies
that the learned mri features guided by the segmentation task help promote
prediction performance. also, bar and bar-r outperform bar-b in most cases,
implying that brain anatomy prior derived from tissue segmentation or mri
reconstruction can help improve discriminative ability of mri features and boost
prediction performance.influence of training data size. we also study the
influence of training data size on bar in cnd vs. cn classification on lld. with
fixed test data, we randomly select a part of mris (i.e., [20%, 40%, • • • ,
100%]) from target training data to fine-tune the downstream prediction model.
it can be observed from fig. 3(b) that the overall performance in terms of auc
and acc of our bar increases with the increase of training data, and it produces
the best results when using all training data for model fine-tuning. this
suggests that using more data for downstream model fine-tuning helps promote
learning performance.
in this paper, we develop a brain anatomy-guided representation (bar) learning
framework for mri-based progression prediction of cognitive impairment,
incorporated by brain anatomy prior (derived from an auxiliary tissue
segmentation task). we validate the proposed bar on two ci-related studies with
t1-weighted mris, and the experimental results demonstrate its effectiveness
compared with sota methods. besides, the pretext model trained on 9,544 mris
from adni can be well adapted to tissue segmentation in the two ci-related
studies. there is significant intra-and inter-site data heterogeneity in lld
with two sites. it is interesting to reduce such heterogeneity using domain
adaptation [31,32], which will be our future work. aside from tissue
segmentation, one can use other auxiliary tasks to model brain anatomy, such as
brain parcellation and brain mri to ct translation. besides, it is meaningful to
compare our method with other model pre-training strategies [33,34], which will
also be our future work.
experimental setting. three classification tasks are performed: (1) depression
recognition (i.e., cnd vs. cn classification) on lld, (2) ci identification
(i.e., ci vs. cnd classification) on lld, and (3) mci detection (i.e., mci vs.
hc classification) on dm. the partition of training/test set is given in table
diffusion-weighted mri enables visualization of brain white matter structures.
it can be used to generate tractography data consisting of millions of synthetic
fibers or streamlines for a single subject stored in a tractogram that
approximate groups of biological axons [1]. many applications require
streamlines to be segmented into individual tracts corresponding to known
anatomy. tract segmentations are used for a variety of tasks, including surgery
planning or tract-specific analysis of psychiatric and neurodegenerative
diseases [2,11,12,17].automated methods built on supervised machine learning
algorithms have attained the current state-of-the-art in segmenting tracts
[3,18,21]. those are trained using various features, either directly from
diffusion data in voxel space or from tractography data. models may output
binary masks containing the target white matter tract, or perform a
classification on streamline level. however, such algorithms are commonly
trained on healthy subjects and have shown issues in processing cases with
anatomical abnormalities, e.g. brain tumors [20]. consequently, they are
unsuitable for tasks such as preoperative planning of neurosurgical patients, as
they may produce incomplete or false segmentations, which could have harmful
consequences during surgery [19]. additionally, supervised techniques are
restricted to fixed sets of predetermined tracts and are trained on substantial
volumes of hard-to-generate pre-annotated reference data.manual methods are
still frequently used for all cases not yet covered by automatic methods, such
as certain populations like children, animal species, new acquisition schemes or
special tracts of interests. experts determine regions of interest (roi) in
areas where a particular tract is supposed to traverse or through which it must
not pass, and segmentations can be accomplished either (1) by virtually
excluding and maintaining streamlines from tractography according to the defined
roi or (2) by using these regions for tract-specific roi-based tractography.
both approaches require a similar effort, although the latter is more commonly
used. the correct definition of rois can be time-consuming and challenging,
especially for inexperienced users. despite these limitations, roi-based
techniques are currently without vivid alternatives for segmenting tracts that
automated methods cannot handle.methods to simplify tract segmentation have been
proposed before. clustering approaches were developed to reduce complexity of
large amounts of streamlines in the input data [4,6]. tractome is a tool that
allows interactive segmentation of such clusters by representing them as single
streamlines that can interactively be included or excluded from the target tract
[14]. although the approach has shown promise, it has not yet supplanted
conventional roibased techniques.we propose a novel semi-automated tract
segmentation method for efficient and intuitive identification of arbitrary
white matter tracts. the method employs entropy-based active learning of a
random forest classifier trained on features of the dissimilarity representation
[13]. active learning has been utilized for several cases in the medical domain,
while it has never been applied in the context of tract segmentation [7,9,16].
it reduces manual efforts by iteratively identifying the most informative or
ambiguous samples, here, streamlines, during classifier training, to be
annotated by a human expert. the method is implemented as the tool attractive in
mitk diffusion1 , enabling researchers to quickly and intuitively segment tracts
in pathological datasets or other situations not covered by automatic
techniques, simply by annotating a few but informative streamlines.
to create a segmentation of a white matter tract from an individual wholebrain
tractogram t , streamlines which not belong to this tract must be excluded from
the tractography data. this is formulated as a binary classification of a
streamline s ∈ t , depending on whether it belongs to the target tract t (see
fig. 1 for a brief summary of the nomenclature of this work)to perform the
classification, supervised models have been trained on various features
representing the data. we choose the dissimilarity representation proposed by
olivetti to classify streamlines, which has shown well performance and can be
computed quickly for arbitrary data [3,13]. a number of n streamlines, in this
case, n = 100, are used as prototypes forming a reference system of the entire
tractogram. a streamline is expressed through a feature vector relative to this
reference system. briefly, a single streamline s = [x 1 , ..., x m ], i.e. a
polyline containing varying numbers of ordered 3d points..m, is described by its
minimum average direct flip distance d mdf to each prototype [5]. the d mdf of a
streamline s a to a prototype p a is defined aswherewith m being the number of
3d points of the streamlines andadditionally to d mdf , the endpoint distance d
end between a streamline and a prototype is calculated, which is equal to d mdf
, besides, only the start points x 1 and endpoints x m of the streamline and
prototype are respected for the calculation [3]. hence, features for a single
streamline are represented by a vector twice the number of prototypes. in order
to calculate these, all streamlines must have the same number of 3d points and
are thus resampled to m = 40 points.
commonly, for training classifiers, large amounts of annotated and potentially
redundant data are used, leading to high annotation efforts and long training
times. active learning reduces both by training machine learning models with
only small and iteratively updated labeled subsets of the originally unlabeled
data. the proposed workflow is initialized, as shown in fig. 2(a), by presenting
a randomly selected subset s rand = [s 1 , ..., s n ] of n = streamlines from an
individual whole-brain tractogram to an expert for annotation, where s rand ⊂ t
. subsequently, the dissimilarity representation is calculated using initially
100 prototypes (fig. 2(b)), and a classifier is trained, in this case, a random
forest. within completing the training, which takes only a few seconds, the
classifier predicts whether the remaining unlabeled streamlines belong to the
target tract. based on the predicted class labels, the target tract is presented
(fig. 2(c)). furthermore, the class probabilities p(s) determined by the random
forest are used to estimate its uncertainty with respect to each sample by
calculating the entropy enext, a subset s emax of streamlines with the highest
entropy or uncertainty is selected to be labeled by the expert and is added to
the training data (fig. 2 (c)) [8]. additionally, these streamlines are utilized
as adaptive prototypes until a threshold of n = 100 adaptive prototype
streamlines is reached.since the model selects ambiguous streamlines in the
target tract region, utilizing them as supplementary prototypes improves feature
expressiveness in this region of interest. this process is repeated iteratively
until the expert accepts the prediction.
the proposed technique was tested on a healthy-subject dataset and on a dataset
containing tumor cases. the first comprises 21 subjects of the human connectome
project (hcp) that were used for testing the automated methods tractseg and
classifyber [3,18]. visual examinations revealed false-negatives in the
reference tracts, meaning that some streamlines that belong to the target tract
were not included in the reference. these false-negatives did not affect the
generation of accurate segmentation masks, since most false-negatives are
occupied by true-positive streamlines, but negatively influenced our
experiments. to reduce false-negatives, the reference segmentation mask as well
as start-and end-region segmentations were used to reassign streamlines from the
tractogram using two criteria: streamlines must be inside the binary reference
segmentation (1) and start and end in the assigned regions (2). as the initial
size of ten million streamlines is computationally challenging and unsuitable
for most tools, all tractograms were randomly down-sampled to one million
streamlines. we focused on the left optic radiation (or), the left
cortico-spinal tract (cst), and the left arcuate-fasciculus (af), representing a
variety of established tracts.to test the proposed method on pathological data,
we used an in-house dataset containing ten presurgical scans of patients with
brain tumors. tractography was performed using probabilistic streamline
tractography in mitk diffusion. to reduce computational costs, we retained one
million streamlines that passed through a manually inserted roi located in an
area traversed by the or [15]. subjects have tumor appearance with varying sizes
((17.87±12.73 cm 3 )) in temporoloccipital, temporal, and occipital regions,
that cause deformations around the or and lead to deviations of the tract from
the normative model.
to evaluate the proposed method, we conducted two types of experiments. manual
segmentation experiments using an interactive prototype of attractive were
initiated on the tumor data (holistic evaluation). additionally, reproducible
simulations on the freely available hcp and the internal tumor dataset were
created (algorithmic evaluation). in order to mimic expert annotation during
algorithmic evaluation, class labels were assigned to streamlines using
previously generated references. the quality of the predictions was measured by
calculating the dice score of the binary mask. the code used for these
experiments is publicly available2 .for the algorithmic evaluation, the initial
training dataset was created with 20 randomly selected streamlines from the
whole-brain tractogram, which have been shown to be a decent number to start
training. since some tracts contain only a fraction of streamlines from the
entire tractogram, it might be unlikely that the training dataset will contain
any streamline belonging to the target tract. therefore, two streamlines of the
specific tract were further added to the training dataset, and class weights
were used to compensate for the class unbalance. according to fig. 2, the
dissimilarity representation was determined, the random forest classifier was
trained and the converged model was used to predict on the unlabeled streamlines
and to calculate the entropy. in each iteration, the ten streamlines with the
highest entropy are added to the training dataset, which has been determined to
be a good trade-off between annotation effort and prediction improvement. the
process was terminated after 20 iterations, increasing the size of the training
data from 22 to 222 out of one million streamlines.the holistic evaluation was
conducted with equal settings, except that the workflow was terminated when the
prediction matched the expectation of the expert. to ensure that the initial
dataset s rand contained streamlines from the target tract, the expert initiated
the active learning workflow by defining a small roi that included fibers of the
tract. s rand was created by randomly sampling only those streamlines that pass
through this roi. to allow comparison between the proposed and traditional
roi-based techniques, the or of subjects from the tumor dataset were segmented
using both approaches by an expert familiar with the respective tool, and the
time required was reported to measure efficiency.note, in all experiments, the
classifier is trained from scratch every iteration, prototypes are generated for
each subject individually, and the classifier predicts on data from the same
subject it is trained with, as it performs subject-individual tract segmentation
and is not used as a fully automated method. to ensure a stable active learning
setup that generalizes across different datasets, the whole method was developed
on the hcp and applied with fixed settings to the tumor data [10].
in table 1, the dice score of the active learning simulation on the hcp and
tumor data after the fifth, tenth, and twentieth iterations are shown and
compared with outcomes of classifyber and tractseg. results for the hcp data
were already on par with the benchmark of automatic methods between the fifth
and tenth iterations. on the tumor data, the performance of the proposed method
remains above 0.7 while the performance of tractseg drops substantially.
furthermore, classifyber does not support the or and is therefore not listed in
table 1. figure 3 depicts the quantitative gain of active learning on the three
tracts of the hcp data and compares it to pure random sampling by displaying the
dice score depending on annotated streamlines. while active learning leads to an
increase in the metric until the predictions at around five to ten iterations
show no meaningful improvements, the random selection does not improve overall.
qualitative results of the algorithmic evaluation of the af of a randomly chosen
subject of the hcp dataset are shown in fig. 4(a). initially, the randomly
sampled streamlines in the training data are distributed throughout the brain,
while entropy-based selected streamlines from subsequent iterations cluster
around the af. the prediction improves iteratively, as indicated by a rising
dice score. when accessing qualitative results of the pathological dataset
visual inspection revealed particularly poorly performance of tractseg in cases
where or fibers were in close proximity to tumor tissue, leading to fragmented
segmentations, while complete segmentations were reached with active learning
even for these challenging tracts after a few iterations, as shown in fig.
4(b).the initial manual experiments with attractive were consistent with the
simulations. the prediction aligned with the expectations of the expert at
around five to seven iterations taking a mean of 4,5 min, while it took seven
minutes on average to delineate the tract with roi-based segmentation. during
the iterations, streamlines around the target tract were suggested for labeling,
and the prediction improved. visual comparison yielded more false-positive
streamlines with the roi-based approach while attractive created more compact
tracts.
active learning-based white matter tract segmentation enables the identification
of arbitrary pathways and can be applied to cases where fully automated methods
are unfeasible. in this work, algorithmic evaluation as well as the
implementation of the technique into the gui-based tool attractive including
further holistic manual experiments were conducted. the algorithmic evaluation
yielded consistent results from the fifth to the tenth iterations on both the
hcp and tumor datasets. as expected, outcomes obtained from the tumor dataset
were not quite as good as those of the hcp dataset. this trend is generally
observed in clinical datasets, which tend to exhibit lower performance levels
compared to high-quality datasets, which could be responsible for the decline in
the results. preliminary manual experiments with attractive indicated active
learning to have shorter segmentation times compared to traditional roi-based
techniques. these experiments are in line with the simulations as the generated
tracts matched the expectations of the expert after around five to seven
iterations, meaning that less than a hundred out of million annotated
streamlines are required to train the model. enhancements to the usability of
the prototype are expected to further improve efficiency. a current limitation
of attractive is the selection of the initial subset, based on randomly sampling
streamlines passing through a manually inserted roi. this approach does not
guarantee that streamlines of the target tract are included in the subset. in
that case, the roi has to be replaced or s rand needs to be regenerated.future
analyses, evaluating the inter-and intra-rater variability compared to other
interactive approaches, will be conducted on further tracts. for selected
scenarios, the ability of the classifier to generalize by learning from
previously annotated subjects will be investigated, which may even allow to
train a fully automatic classifier for new tracts once enough data is annotated.
to further optimize the method, the feature representation or sampling procedure
could be improved. uncertainty sampling may select redundant streamlines due to
similar high entropy values. instead, annotating samples with high entropy
values being highly diverse or correcting false classifications could convey
more information.by introducing active learning into tract segmentation, we
provide an efficient and intuitive alternative compared to traditional roi-based
approaches.attractive has the potential to interactively assist researchers in
identifying arbitrary white matter tracts not captured by existing automated
approaches.
making artificial neural networks more interpretable, transparent, and
trustworthy remains one of the biggest challenges in deep learning. they are
often still considered black boxes, limiting their application in
safety-critical domains such as healthcare. histopathology is a prime example of
this. for years, the number of pathologists has been decreasing while their
workload has been increasing [23]. consequently, the need for explainable
computer-aided diagnostic tools has become more urgent.as a result, research in
explainable artificial intelligence is thriving [20]. much of it focuses on
convolutional neural networks (cnns) [13]. however, with the rise of
transformers [31] in computational pathology, and their increasing application
to cancer classification, segmentation, survival prediction, and mutation
detection tasks [26,32,33], the old tools need to be reconsidered. visualizing
filter maps does not work for transformers, and grad-cam [30] has known
limitations for both cnns and transformers.the usual way to interpret
transformer-based models is to plot their multihead self-attention scores [8].
but these often lead to fragmented and unsatisfactory explanations [10]. in
addition, there is an ongoing controversy about their trustworthiness [5]. to
address these issues, we propose a novel family of transformer architectures
based on the b-cos transform originally developed for cnns [7]. by aligning the
inputs and weights during training, the models are implicitly forced to learn
more biomedically relevant and meaningful features (fig. 1). overall, our
contributions are as follows:• we propose the b-cos vision transformer (bvt) as
a more explainable alternative to the vision transformer (vit) [12]. • we
extensively evaluate both models on three public datasets: nct-crc-he-100k [18],
tcga-coad-20x [19], munich-aml-morphology [25]. • we apply various post-hoc
visualization techniques and conduct a blind study with domain experts to assess
model interpretability. • we derive the b-cos swin transformer (bwin) based on
the swin transformer [21] (swin) in a generalization study.
explainability, interpretability, and relevancy are terms used to describe the
ability of machine learning models to provide insight into their decision-making
process. although these terms have subtle differences, they are often used
interchangeably in the literature [15]. recent research on understanding vision
models has mostly focused on attribution methods [13,20], which aim to identify
important parts of an image and highlight them in a saliency map. gradient-based
approaches like grad-cam [30] or attribution propagation strategies such as deep
taylor decomposition [27] and lrp [6] are commonly used methods.
perturbation-based techniques, such as shap [22], are another way to extract
salient features from images. besides saliency maps, one can also visualize the
activations of the model using activation maximization [14].however, it is still
controversial whether the above methods can correctly reflect the behavior of
the model and accurately explain the learned function (model-faithfulness [17]).
for example, it has been shown that some saliency maps are independent of both
the data on which the model was trained and the model parameters [2]. in
addition, they are often considered unreliable for medical applications [4]. as
a result, inherently interpretable models have been proposed as a more reliable
and transparent solution. the most recent contribution are b-cos cnns [7], which
use a novel nonlinear transformation (the b-cos transformation) instead of the
traditional linear transformation.compared to cnns, there is limited research on
understanding transformers beyond attention visualization [10]. post-hoc methods
such as grad-cam [30] fig. 3. rollout, attention-last (attn-last), grad-cam,
lrp, lrp of the second layer (lrp-second), lrp of the last layer (lrp-last), and
transformer attribution (ta) applied on the test set of munich-aml-morphology.
the image shows an eosinophil, which is characterized by its split, but
connected nucleus, large specific granules (pink structures in the cytoplasm),
and dense chromatin (dark spots inside the nuclei) [29]. across all
visualization techniques, bvt focuses on these exact features unlike vit.and
activation maximization [14] used for cnns can also be applied to transformers.
but in practice, the focus is on visualizing the raw attention values (see
attention-last [16], integrated attention maps [12], rollout [1], or attention
flow [1]). more recent approaches such as generic attention [9], transformer
attribution [10], and conservative propagation [3] go a step further and
introduce novel visualization techniques that better integrate the attention
modules with contributions from different parts of the network. note that these
methods are all post-hoc methods applied after training to visualize the model's
reasoning.on the other hand, the concepttransformer [28], achieves better
explainability by cross-attending user-defined concept tokens in the classifier
head during training. more recently, hipt [11] combines multi-scale images and
dino [8] pre-training to learn hierarchical visual concepts in a self-supervised
fashion. unlike all of these methods, interpretability is already an integral
part of our architecture. therefore, these methods can be easily applied to our
models. in fig. 3 and fig. 6, we show that the b-cos transformer produces
superior feature maps over various post-hoc approaches -suggesting that our
architecture does indeed learn human-plausible features that are independent of
the specific visualization technique used.
we focus on the original vision transformer [12]: the input image is divided
into non-overlapping patches, flattened, and projected into a latent space of
dimension d. class tokens [cls] are then prepended to these patch embeddings. in
addition, positional encodings [pos] are added to preserve topological
information. in the scaled dot-product attention [31], the model learns
different features (query q, key k, and value v ) from the input vectors through
a linear transformation. both query and key are then correlated with a scaled
dot-product and normalized with a softmax. these self-attention scores are then
used to weight the value by importance:to extract more information, this process
is repeated h times in parallel (multi-headed self-attention). each
self-attention layer is followed by a fullyconnected layer consisting of two
linear transformations and a relu activation.we propose to replace all linear
transforms in the original vit (fig. 2)c(x, w) = cos(∠(x, w)), ∠...angle between
vectorswith the b-cos* transform [7] b-cos*(x;where b ∈ n. similar to [7], an
additional nonlinearity is applied after each b-cos* transform. specifically,
each input is processed by two b-cos* transforms, and the subsequent maxout
activation passes only the larger output. this ensures that only weight vectors
with higher cosine similarity to the inputs are selected, which further
increases the alignment pressure during optimization. thus, the final b-cos
transform is given byto see the significance of these changes, we look at eq. 4
and derivefig. 4. we compute the central kernel alignment (cka), which measures
the representation similarity between each hidden layer. since the b-cos
transform aligns the weights with the inputs, bvt (ours) achieves a more uniform
representation structure compared to vit (values closer to 1). when trained with
the binary cross-entropy loss (bce) instead of the categorical cross-entropy
loss (cce), the alignment is higher.since |c(x, ŵ)| ≤ 1, equality is only
achieved if x and w are collinear, i.e., if they are aligned. intuitively, this
forces the weight vector to be more similar to the input. query, key, and value
thus capture more patterns in an image -which the attention mechanism can then
attend to. this can be shown visually by plotting the centered kernel alignment
(cka). it measures the similarity between layers by comparing their internal
representation structure. compared to vits, bvts achieve a highly uniform
representation across all layers (fig. 4).
task-based evaluation: cancer classification and segmentation is an important
first step for many downstream tasks such as grading or staging. therefore, we
choose this problem as our target. we classify image patches from the public
colorectal cancer dataset nct-crc-he-100k [18]. we then apply our method to
tcga-coad-20x [19], which consists of 38 annotated slides from the tcga
colorectal cancer cohort, to evaluate the effectiveness of transfer learning.
this dataset is highly unbalanced and not color normalized compared fig. 5. in a
blinded study, domain experts ranked models (lower is better) based on whether
the models focus on biomedically relevant features that are known in the
literature to be important for diagnosis. we then performed the conover post-hoc
test after friedman with adjusted p-values according to the two-stage
benjamini-hochberg procedure. bvt ranks above vit with p < 0.1 (underlined) and
p < 0.05 (bold). to the first dataset. additionally, we demonstrate that the
b-cos vision transformer is adaptable to domains beyond histopathology by
training the model on the single white blood cell dataset munich-aml-morphology
[25], which is also highly unbalanced and also publicly available.domain-expert
evaluation: our primary objective is to develop an extension of the vision
transformer that is more transparent and trusted by medical professionals. to
assess this, we propose a blinded study with four steps: (i) randomly selecting
images from the test set of tcga-coad-20x (32 samples) and munich-aml-morphology
(56 samples), (ii) plotting the last-layer attention and transformer
attributions for each image, (iii) anonymizing and randomly shuffling the
outputs, (iv) submitting them to two domain experts in histology and cytology
for evaluation. most importantly, we show them all the available saliency maps
without pre-selecting them to get their unbiased opinion.implementation details:
in our experiments, we compare different variants of the b-cos vision
transformer and the vision transformer. specifically, we implement two versions
of vit: vit-t/8 and vit-s/8. they only differ in parameter size (5m for t models
and 22m for s models) and use the same patch size of 8. all bvt models (bvt-t/8
and bvt-s/8) are derivatives of the corresponding vit models. the b-cos
transform used in the bvt models has an exponent of b = 2. we use adamw with a
cosine learning rate scheduler for optimization and a separate validation set
for hyperparameter selection. following the findings of [7], we add [1 -r, 1 -g,
1 -b] to the rgb channels [r, g, b] of bvt. this allows us to encode each pixel
with the direction of the color channel vector, forcing the model to capture
more color information. furthermore, we train models with two different loss
functions: the standard categorical cross-entropy loss (cce) and the binary
cross-entropy loss (bce) with one-hot encoded entries. it was suggested in [7]
that bce is a more appropriate loss for b-cos cnns. we explore whether this is
also true for transformers in our experiments. additional details on training,
optimization, and datasets can be found in the appendix.
task-based evaluation: when trained from scratch, all bvt models underperform
their vit counterparts by about 2% on nct-crc-he-100k and 3% on munich
aml-morphology (table 1). however, when we use the pretrained weights from
nct-crc-he-100k and transfer them to tcga-coad-20x for fine-tuning, bvt
outperforms vit by up to 5% (table 1). we believe this is due to the
simultaneous optimization of two objectives: classification loss and
weight-input alignment. with a pre-trained model, bvt is likely to focus more on
the former. in addition, we observe that models trained with bce tend to perform
worse than those trained with cce. however, their saliency maps seem to be more
interpretable (see fig. 3).
the results show that bvts are significantly more trustworthy than vits (p <
0.05). this indicates that bvt consistently attends to biomedically relevant
features such as cancer cells, nuclei, cytoplasm, or membrane [24] (fig. 5). in
many visualization techniques, we see that bvt, unlike vit, focuses exclusively
on these structures (fig. 3). in contrast, vit attributes high attention to
seemingly irrelevant features, such as the edges of the cells. a third expert
points out that vit might overfit certain patterns in this dataset, which could
aid the model in improving its performance.
we aim to explore whether the b-cos transform can enhance the interpretability
of other transformer-based architectures. the swin transformer (swin) [21] is a
popular alternative to vit (e.g., it is currently the sota feature extractor for
histopathological images [33]). swin utilizes window attention and feed-forward
layers. in this study, we replace all its linear transforms with the b-cos
transform, resulting in the b-cos swin transformer (bwin). however, unlike bvt
and vit, it is not obvious how to visualize the window attention. therefore, we
introduce a modified variant here that has a regular vit/bvt block in the last
layer.in our experiments (table 2), we observe that bwin outperforms swin by up
to 2.7% and 4.8% in f1-score on nct-crc-he-100k and munich-aml-morphology,
respectively. this is consistent with the observations made in sect. 5: when bvt
is trained from scratch, the model faces a trade-off between learning the weight
and input alignment and finding the appropriate inductive bias to solve the
classification task. by reintroducing many of the inductive biases of cnns
through the window attention in the case of swin or transfer learning in the
case of bvt, the model likely overcomes this initial problem.moreover, we would
like to emphasize that the modified models have no negative impact on the
model's performance. in fact, all metrics remain similar or even improve. the
accumulated attention heads (we keep 50% of the mass) demonstrate that bwin
solely focuses on nuclei and other cellular features (fig. 6). conversely, swin
has very sparse attention heads, pointing to a few spots. consistent with the
bvt vs vit blind study, our pathologists also agree that bwin is more plausible
than swin (p < 0.05).
we have introduced the b-cos vision transformer (bvt) and the b-cos swin
transformer (bwin) as two alternatives to the vision transformer (vit) and the
swin transformer (swin) that are more interpretable and explainable. these
models use the b-cos transform to enforce similarity between weights and inputs.
in a blinded study, domain experts clearly preferred both bvt and bwin over vit
and swin. we have also shown that bvt is competitive with vit in terms of
quantitative performance. moreover, using bwin or transfer learning for bvt, we
can even outperform the original models.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3_50.
in clinical practice, magnetic resonance imaging (mri) provides important
information for diagnosing and monitoring patient conditions [4,16]. to capture
the complex pathophysiological aspects during disease progression,
multiparametric mri (such as t1w, t2w, dir, flair) is routinely acquired. image
acquisition inherently poses a trade-off between scan time, resolution, and
signalto-noise ratio (snr) [19]. to maximize the source of information within a
reasonable time budget, clinical protocol often combines anisotropic 2d scans of
different contrasts in complementary viewing directions. although acquired 2d
scans offer an excellent in-plane resolution, they lack important details in the
orthogonal out-of-plane. for a reliable pathological assessment, radiologists
often resort to a second scan of a different contrast in the orthogonal viewing
direction. furthermore, poor out-of-plane resolution significantly affects the
accuracy of volumetric downstream image analysis, such as radiomics and lesion
volume estimation, which usually require isotropic 3d scans. as multi-parametric
isotropic 3d scans are not always feasible to acquire due to time-constraints
[19], motion [9], and patient's condition [10], super-resolution offers a
convenient alternative to obtain the same from anisotropic 2d scans. recently,
it has been shown that acquiring three complementary 2d views of the same
contrast may yield higher snr at reduced scan time [19,29]. however, it remains
under-explored if orthogonal anisotropic 2d views of different contrasts can
benefit from each other based on the underlying anatomical consistency.
additionally, whether such strategies can further decrease scan times while
preserving similar resolution and snr remains unanswered. moreover, unlike
conventional super-resolution models trained on a cohort, a personalized model
is of clinical relevance to avoid the danger of potential misdiagnosis caused by
cohort-learned biases. in this work, we mitigate these gaps by proposing a novel
multi-contrast super-resolution framework that only requires the
patient-specific low-resolution mr scans of different sequences (and views) as
supervision. as shown in various settings, our approach is not limited to
specific contrasts or views but provides a generic framework for
super-resolution. the contributions in this paper are three-fold: 1. to the best
of our knowledge, our work is the first to enable subject-specific
multi-contrast super-resolution from low-resolution scans without needing any
high-resolution training data. we demonstrate that implicit neural
representations (inr) are good candidates to learn from complementary views of
multi-parametric sequences and can efficiently fuse low-resolution images into
anatomically faithful super-resolution. 2. we introduce mutual information (mi)
[26] as an evaluation metric and find that our method preserves the mi between
high-resolution ground truths in its predictions. further observation of its
convergence to the ground truth value during training motivates us to use mi as
an early stopping criterion. 3. we extensively evaluate our method on multiple
brain mri datasets and show that it achieves high visual quality for different
contrasts and views and preserves pathological details, highlighting its
potential clinical usage.related work. single-image super-resolution (sisr) aims
at restoring a highresolution (hr) image from a low-resolution (lr) input from a
single sequence and targets applications such as low-field mr upsampling or
optimization of mri acquisition [3]. recent methods [3,8] incorporate priors
learned from a training set [3], which is later combined with generative models
[2]. on the other hand, multi-image super-resolution (misr) relies on the
information from complementary views of the same sequence [29] and is especially
relevant to capturing temporal redundancy in motion-corrupted low-resolution mri
[9,27]. multi-contrast super-resolution (mcsr) targets using inter-contrast
priors [20]. in conventional settings [15], an isotropic hr image of another
contrast is used to guide the reconstruction of an anisotropic lr image. zeng et
al. [30] use a two-stage architecture for both sisr and mcsr. utilizing a
feature extraction network, lyu et al. [14] learn multi-contrast information in
a joint feature space. later, multi-stage integration networks [6], separatable
attention [7] and transformers [13] have been used to enhance joint feature
space learning. however, all current mcsr approaches are limited by their need
for a large training dataset. consequently, this constrains their usage to
specific resolutions and further harbors the danger of hallucination of features
(e.g., lesions, artifacts) present in the training set and does not generalize
well to unseen data.originating from shape reconstruction [18] and multi-view
scene representations [17], implicit neural representations (inr) have achieved
state-of-the-art results by modeling a continuous function on a space from
discrete measurements. key reasons behind inr's success can be attributed to
overcoming the low-frequency bias of multi-layer perceptrons (mlp) [21,24,25].
although mri is a discrete measurement, the underlying anatomy is a continuous
space. we find inr to be a good fit to model a continuous intensity function on
the anatomical space. once learned, it can be sampled at an arbitrary resolution
to obtain the super-resolved mri. following this spirit, inrs have recently been
successfully employed in medical imaging applications ranging from k-space
reconstruction [11] to sisr [29]. unlike [22,29], which learn anatomical priors
in single contrasts, and [1,28], which leverage inr with latent embeddings
learned over a cohort, we focus on employing inr in subject-specific,
multi-contrast settings.
in this section, we first formally introduce the problem of joint
super-resolution of multi-contrast mri from only one image per contrast per
patient. next, we describe strategies for embedding information from two
contrasts in a shared space. subsequently, we detail our model architecture and
training configuration.problem statement. we denote the collection of all 3d
coordinates of interest in this anatomical space as ω = {(x, y, z)} with
anatomical function q : ω → a. the image intensities are a function of the
underlying anatomical properties a. two contrasts c 1 and c 2 can be scanned in
a low-resolution subspace ω 1 , ω 2 ⊂ ω. let us consider g 1 , g 2 : a → r that
map from anatomical properties to contrast intensities c 1 and c 2 ,
respectively. we obtain sparse observations, where f i is composition of g i and
q. however, one can easily obtain the global anatomical space ω by knowing ω 1
and ω 2 , e.g., by rigid registration between the two images. in this paper, we
aim to estimate f 1 , f 2 : ω → r given i 1 and i 2 .joint multi-contrast
modelling. since both component-functions f 1 and f 2 operate on a subset of the
same input space, we argue that it is beneficial to model them jointly as a
single function f : ω → r 2 and optimize it based on their estimation error
incurred in their respective subsets. this will enable information transfer from
one contrast to another, thus improving the estimation and preventing
over-fitting in single contrasts, bringing consistency to the prediction.to this
end, we propose to leverage inr to model a continuous multi-contrast function f
from discretely sampled sparse observations i 1 and i 2 .mcsr setup. without
loss of generalization, let us consider two lr input contrasts scanned in two
orthogonal planes p 1 and p 2 , where p 1 , p 2 ∈ {axial, sagittal, coronal}. we
assume they are aligned by rigid registration requiring no coordinate
transformation. their corresponding in-plane resolutions are (s 1 ×s 1 ) and (s
2 × s 2 ) and slice thickness is t 1 and t 2 , respectively. note that s 1 < t 1
and s 2 < t 2 imply high in-plane and low out-of-plane resolution. in the end,
we aim to sample an isotropic (s × s × s) grid for both contrasts where s ≤ s 1
, s 2 .implicit neural representations for mcsr. we intend to project the
information available in one contrast into another by embedding both in the
shared weight space of a neural network. however, a high degree of weight
sharing could hinder contrast-specific feature learning. based on this
reasoning, we aim to hit the sweet spot where maximum information exchange can
be encouraged without impeding contrast-specific expressiveness. we propose a
split-head architecture, as shown in fig. 1, where the initial layers jointly
learn the common anatomical features, and subsequently, two heads specialize in
contrast-specific information. the model takes fourier [25] features v =
[cos(2πbx), sin(2πbx)] t as input and predicts [ î1 , î2 ] = f (v), where x =
(x, y, z) and b is sampled from a gaussian distribution n (μ, σ 2 ). we use
mean-squared error loss, l mse , for training.where α and β are coefficients for
the reconstruction loss of two contrasts. note that for points {(x, y, z)} ∈ ω 2
\ ω 1 , there is no explicit supervision coming from low resolution c 1 . for
these points, one can interpret learning c 1 from the loss in c 2 , and vice
versa, to be a weakly supervised task.
given the rigidly registered lr images, we compute ω 1 , ω 2 ∈ ω in the scanner
reference space using their affine matrices. subsequently, we normalize ω to the
interval [-1, 1] 3 and independently normalize each contrast's intensities to
[0, 1]. we use 512-dimensional fourier features in the input. our model consists
of a four-layer mlp with a hidden dimension of 1024 for the shared layers and
two layers with a hidden dimension of 512 for the heads. we use adam optimizer
with a learning rate of 4e-4 and a cosine annealing rate scheduler with a batch
size of 1000. for the multi-contrast inr models, we use mi as in eq. 2 for early
stopping. implemented in pytorch, we train our model on a single a6000 gpu.
please refer to table 3 in supplementary for an exhaustive hyper-parameter
search.model selection and inference. since our model is trained on sparse sets
of coordinates, it is prone to overfitting them and has little incentive to
generalize in out-of-plane predictions for single contrast settings. a remedy to
this is to hold random points as a validation set. however, this will reduce the
number of training samples and hinder the reconstruction of fine details. for
multi-contrast settings, one can exploit the agreement between the two predicted
contrasts. ideally, the network should reach an equilibrium between the
contrasts over the training period, where both contrasts optimally benefit from
each other. we empirically show that mutual information (mi) [26] is a good
candidate to capture such an equilibrium point without the need for ground truth
data in its computation. for two predicted contrasts î1 and î2 , mi can be
expressed as:compared to image registration, we do not use mi as a loss for
aligning two images; instead, we use it as a quantitative assessment metric.
given two ground truth hr images for a subject, one can compute the optimum
state of mi. we observe that the mi between our model predictions converges
close to such an optimum state over the training period without any explicit
knowledge about it, c.f. fig. 3 in the supplementary. this observation motivates
us to detect a plateau in mi between the predicted contrasts and use it as a
stopping criterion for model selection in multi-contrast inr.
datasets. to enable fair evaluation between our predictions and the reference hr
ground truths, the in-plane snr between the lr input scan and corresponding
ground truth has to match. to synthetically create 2d lr images, it is necessary
to downsample out-of-plane in the image domain anisotropically [32] while
preserving in-plane resolution. consequently, to mimic realistic 2d clinical
protocol, which often has higher in-plane details than that of 3d scans, we use
spline interpolation to model partial volume and downsampling. we demonstrate
our network's modeling capabilities for different contrasts (t1w, t2w, flair,
dir), views (axial, coronal, sagittal), and pathologies (ms, brain tumor). we
conduct experiments on two public datasets, brats [16], and msseg [4], and an
in-house clinical ms dataset (cms). in each dataset, we select 25 patients that
fulfill the isotropic acquisition criteria for both ground truth hr scans. note
that we only use the ground truth hr for evaluation, not anywhere in training.
we optimize separate inrs for each subject with supervision from only its two lr
scans. if required, we employ skull-stripping [12] and rigid registration to the
mni152 (msseg, cms) or sri24 (brats) templates. for details, we refer to table 2
in the supplementary.metrics. we evaluate our results by employing common sr
[5,14,29] quality metrics, namely psnr and ssim. to showcase perceptual image
quality, we additionally compute the learned perceptual image patch similarity
(lpips) [31] and measure the absolute error mi in mutual information of two
upsampled images to their ground truth counterparts as follows:baselines and
ablation. to the best of our knowledge, there are no prior data-driven methods
that can perform mcsr on a single-subject basis. hence, we provide
single-subject baselines that operate solely on single contrast and demonstrate
the benefit of information transfer from other contrasts with our proposed
models. quantitative analysis. table 1 demonstrates that our proposed framework
poses a trustworthy candidate for the task of mcsr. as observed in [32], lrtv
struggles for anisotropic up-sampling while smore's overall performance is
better than cubic-spline, but slightly worse to single-contrast inr. however,
the benefit of single-contrast inr may be limited if not complemented by
additional views as in [29]. for mcsr from single-subject scans, we achieve
encouraging results across all metrics for all datasets, contrasts, and views.
since t1w and t2w both encode anatomical structures, the consistent improvement
in brats for both sequences serves as a proof-of-concept for our approach. as
flair is the go-to-sequence for ms lesions, and t1w does not encode such
information, the results are in line with the expectation that there could be a
relatively higher transfer of anatomical information to pathologically more
relevant flair than vice-versa. lastly, given their similar physical acquisition
and lesion sensitivity, we note that dir/flair benefit to the same degree in the
cms dataset.qualitative analysis. figure 2 shows the typical behavior of our
models on cms dataset, where one can qualitatively observe that the split-head
inr pre-serves the lesions and anatomical structures shown in the yellow boxes,
which other models fail to capture. while our reconstruction is not identical to
the gt hr, the coronal view confirms anatomically faithful reconstructions
despite not receiving any in-plane supervision from any contrast during
training. we refer to fig. 4 in the supplementary for similar observations on
brats and msseg.
given the importance and abundance of large multi-parametric retrospective
cohorts [4,16], our proposed approach will allow the upscaling of lr scans with
the help of other sequences. deployment of such a model in clinical routine
would likely reduce acquisition time for multi-parametric mri protocols
maintaining an acceptable level of image fidelity. importantly, our model
exhibits trustworthiness in its clinical applicability being 1)
subject-specific, and 2) as its gain in information via super-resolution is
validated by mi preservation and is not prone to hallucinations that often occur
in a typical generative model.in conclusion, we propose the first
subject-specific deep learning solution for isotropic 3d super-resolution from
anisotropic 2d scans of two different contrasts of complementary views. our
experiments provide evidence of inter-contrast information transfer with the
help of inr. given the supervision of only single subject data and trained
within minutes on a single gpu, we believe our framework to be potentially
suited for broad clinical applications. future research will focus on
prospectively acquired data, including other anatomies.
our proposed split-head inr: single inr with two separate heads that jointly
predicts the two contrast intensities (cf. fig.1).
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43993-3_17.
glioblastomas (gbms, known as grade iv gliomas) are the most common primary
malignant brain tumors with high spatial heterogeneity and varying degrees of
aggressiveness [22]. patients with gbm generally have a very poor survival rate;
the median overall survival time is about 14 months [17]; and the overall
survival time is affected by many factors, including patient characteristics
(e.g., age and physical status), tissue histopathology (e.g., cellular density
and nuclear atypia), and molecular pathology (e.g., mutations and gene
expression levels) [1,14,15]. although these factors, particularly molecular
information, have usually proved to be strong predictors of survival in gbm,
there remain substantial challenges and unmet clinical needs to exploit easily
accessible, noninvasive neuroimaging data acquired preoperatively to predict
overall survival time of gbm patients, which can benefit treatment planning.to
do so, magnetic resonance imaging (mri) and its derived radiomics have been
widely used to study gbm preoperative prognosis over the last few decades. for
example, anand et al. [2] first applied a forest of trees to assign an
importance value to each of the 1022 radiomic features extracted from t1 mri,
and then the 32 most important features were fed to the random forest regressor
for predicting overall survival time of a gbm patient. based on patches from
multi-modal mri images, nie et al. [19] trained a 3d convolutional neural
network (cnn) to learn the high-level semantic features, which were eventually
input to a support vector machine (svm) for classifying long-and short-term gbm
survivors. in addition, an integrated model by fusing radiomics features,
mri-based cnn features, and clinical features, was presented for gbm survival
group classification, resulting in better performance than using any single type
of features [12].although both mri and its derived radiomics features have been
demonstrated to have predictive power for survival analysis in the
aforementioned literature, they do not account for brain's functional
alternations caused by tumors, which are clinically significant as
biologically-interpretable biomarkers of recovery and therapy. these
alternations can be reflected by changes in resting-state functional mri
(fmri)-derived functional connectivities/connections (fcs) between the blood
oxygenation level-dependence (bold) time series of paired brain regions.
therefore, the use of fcs to predict overall survival time for gbm has recently
attracted increasing attention [7,16,24], and more importantly, survival-related
fc patterns or brain regions were found to guide therapeutic solutions aimed at
inhibiting tumor-brain communication.nevertheless, current fc-based survival
prediction still suffers from two main deficiencies when applied to gbm
prognosis. first, due to mass effect and physical infiltration of gbm in the
brain, fcs estimated directly from gbm patients' resting-state fmri might be
inaccurate, especially when the tumors are near or in the regions of interest.
second, resting-state fmri data are not routinely collected for gbm clinical
practices, which restricts the size of annotated datasets such that it is
infeasible to train a reliable prediction model based on deep learning for
survival prediction. in order to circumvent these issues, in this paper we
introduce a novel neuroimaging feature family, namely functional lesion network
(fln) maps that are generated by our augmented lesion network mapping (a-lnm),
for overall survival time prediction of gbm patients. our a-lnm is motivated by
lesion network mapping (lnm) [8] which can localize neurological deficits to
functional brain networks and identify regions relate to a clinical syndrome. by
embedding the lesion into a normative functional connectome and computing
functional connectivity between the lesion and the rest of the brain using fmri
of all healthy subjects in the normative cohort, lnm has been successfully
employed to the identification of the brain network underlying particular
symptoms or behavioral deficits in stoke [4,13].the details of our workflow are
described as follows.1) we first manually segment the whole tumor (regarded as
lesion in this paper) on structural mri for all gbm patients, and the resulting
lesion masks are mapped onto a reference brain template, e.g., the mni152 2mm 3
template.2) the proposed a-lnm is next used to generate fln maps for each gbm
patient by using resting-state fmri from a large cohort of healthy subjects.
specifically, for each patient, we correlate the mean bold time series of all
voxels within the lesion with the bold time series of every voxel in the whole
brain for all n subjects in the normative cohort, producing n functional
disconnection (fdc) maps of voxel-wise correlation values (transformed to
zscores). these resulting n fdc maps are partitioned into m disjoint subsets of
equal size, and m fln maps are separately obtained by averaging the fdc maps in
each of the m subsets. similar to data augmentation schemes, we can artificially
boost data volume (i.e., fln maps) up to m times through producing m fln maps
for each patient in the a-lnm, which helps to mitigate the risk of over-fitting
and improve the performance of overall survival time prediction when learning a
deep neural network from a small sized dataset.for this reason, we propose the
name "augmented lnm (a-lnm)", compared to the traditional lnm where only one fln
map is generated per patient by averaging all the n fdc maps. 3) finally, these
augmented fln maps are fed to a 3d resnet-based backbone network followed by the
average pooling operation and fully-connected layers for gbm survival
prediction.to our knowledge, this paper is the first to demonstrate a successful
extension of lnm for survival prediction in gbm. to evaluate the predictive
power of the fln maps generated by our a-lnm, we conduct extensive experiments
on 235 gbm patients in the training dataset of brats 2020 [18] to classify the
patients into three overall survival time groups viz. long, mid, and short.
experimental results show that our a-lnm based survival prediction framework
outperforms previous state-of-the-art methods. in addition, an explainable
analysis driven by the gradient-weighted class activation mapping (grad-cam)
[10] for survivalrelated brain regions is fulfilled.
2.1 materials gsp1000 processed connectome. it publicly released preprocessed
restingstate fmri data of 1000 healthy right-handed subjects with an average age
21.5 ± 2.9 years and approximately equal numbers of males and females from the
brain genomics superstruct project (gsp) [5], where the concrete image
acquisition parameters and preprocessing procedures can be found as well.
specifically, a slightly modified version of yeo's computational brain imaging
group (cbig) fmri preprocessing pipeline (https://github.com/bchcohenlab/cbig)
was employed to obtain either one or two preprocessed resting-state fmri runs of
each subject that had 120 time points per run and were spatially normalized into
the mni152 template with 2mm 3 voxel size. we downloaded and used the first-run
preprocessed resting-state fmri of each subject for the following analysis.brats
2020. it provided an open-access pre-operative imaging training dataset to
segment brain tumors of glioblastoma (gbm, belonging to high grade glioma) and
low grade glioma (lgg) patients, as well as to predict overall survival time of
gbm patients [18]. this training dataset contained 133 lgg and 236 gbm patients,
and each patient had four mri modalities, including t1, post-contrast
t1-weighted, t2-weighted, and t2 fluid attenuated inversion recovery. manual
expert segmentation delineated three tumor sub-regions, i.e., the gd-enhancing
tumor, the peritumoral edema, and the necrotic and non-enhancing tumor core.the
union of all the three tumor sub-regions was considered as the whole tumor,
which is regarded as the lesion in this paper.
in this paper, we propose to investigate the feasibility of the novel
neuroimaging features, i.e., fln maps, for overall survival time prediction of
gbm patients in the training dataset of the brats 2020, in which one patient
alive was excluded, and the remaining 235 patients consisted of 89 short-term
survivors (less than 10 months), 59 mid-term survivors (between 10 and 15
months), and 87 long-term survivors (more than 15 months). to this end, our
framework for the three-class survival classification is shown in fig. 1, and
the details are described as follows.lesion mapping procedures. as stated above,
the whole tumor is referred to as a lesion for each gbm patient. from the manual
expert segmentation labels of lesions in the 235 gbm patients of the brats 2020,
we co-register the lesion masks to the mni152 2mm 3 template by employing a
symmetric normalization algorithm in antspy [3].
after lesion mapping, we introduce a modified lnm (called augmented lnm (a-lnm)
in this paper) to generate fln maps for each gbm patient by using resting-state
fmri of all 1000 gsp healthy subjects, as described below. i) for each patient,
the lesion is viewed as a seed region to calculate fdc in the healthy subjects
with restingstate fmri. specifically, to compute fdc, the mean bold time series
of voxels within each lesion is correlated with the bold time series of every
voxel in the whole brain for all the 1000 healthy subjects, yielding 1000 fdc
maps of voxelwise correlation values (transformed to z-scores), where an fdc map
is actually a three-dimensional voxel-wise matrix of size 91 × 109 × 91 (spatial
resolution: 2mm 3 voxel size). ii) different from the commonly used lnm where
the resulting 1000 fdc maps are thresholded or averaged to obtain a single fln
map for each patient, the a-lnm generates many fln maps for each patient in a
manner that partitions all the 1000 fdc maps into disjoint subsets of equal size
and averages each subset to produce one fln map. one can clearly see that
similar to data augmentation schemes, we artificially boost the number of
training samples (i.e., fln maps) by our a-lnm, which helps to mitigate the risk
of over-fitting and improve the performance of overall survival time prediction
when learning a deep neural network from such a small sized training set used in
this paper. note that in sect. 3 of this paper, according to experimental
results, we divided the 1000 fdc maps into 100 subsets, and randomly chose 10
out of the resulting 100 fln maps for each patient as input to the downstream
prediction model.deep neural network for overall survival time prediction. by
taking the obtained fln maps as input, we apply a 3d resnet-based backbone
network transferred from the encoder of medicalnet [6] to extract cnn features
from each fln map. the features are then combined using the average pooling
operation and fed to a fully-connected layer with kernel size (1, 1, 1) to
classify each gbm patient into one of the three overall survival time groups
(i.e., short-term survival, mid-term survival, and long-term survival).
implementation details. our proposed method was implemented in pytorch 1.13.1 on
nvidia a100 tensor core gpus. the loss function was the standard cross-entropy
loss. the adam optimizer with the weight decay of 10 -5 was adopted. three 3d
resnet-based backbones with different numbers of layers (10, 50, and 101) were
performed, where the initial learning rates were set as 10 -4 , 10 -4 , and 10
-5 , respectively, and would decrease by a factor of 5 if the classification
performance is not improved within 5 epochs. the number of epochs for training
was 50, and the batch size was fixed as 64.performance evaluation. we evaluated
the classification performance of our proposed method using 235 gbm patients in
the brats 2020 training dataset, because only these 235 patients had both
overall survival time and manual expert segmentation labels of lesions. in all
experiments, we conducted five-fold crossvalidation ten times in order to reduce
the effect of sampling bias. moreover, the a-lnm was performed ten times
randomly to avoid particular data distribution and obtain more reliable results.
the classification results were reported in terms of accuracy, macro precision
(macro-p), macro recall (macro-r), and macro f1 score (macro-f1), respectively.
quantitative comparison of different prediction models. as this paper is the
first application of fln maps in the overall survival time prediction for gbm,
comparison among the classification performance of different models using the
same type of features, i.e., the a-lnm or the lnm derived fln maps, is demanded
for model selection. to validate the effectiveness of the 3d resnetbased
backbones for gbm survival prediction, we made quantitative comparison of a
ridge classifier (rc) with pca [23], a support vector classifier (svc) with pca,
a logistic regression (lr) with pca, and three 3d resnet-based backbones with
different numbers of layers (10, 50, and 101). as presented in table 1, all the
3d resnet-based backbones outperformed the other three machine learning-based
models. in addition, all the 3d resnet-based backbones achieved better
classification results by using the a-lnm derived fln maps than the lnm derived
fln maps, which implies that the a-lnm derived fln maps have stronger predictive
power than the lnm derived fln maps. quantitative comparison of different types
of features. subsequently, we compared the predictive power of fln maps and the
other four widely used types of features, i.e., clinical features with an lr
[27], biophysics features with an svc [9], radiomics features with a gradient
boosting classifier (gbc) [21], and mri-based features with a 3d cnn model [9].
these competing models were executed following the instructions in their
respective papers to achieve the best performance. quantitative results of
different types of features are displayed in table 2. one can see that fln maps
showed the strongest predictive power on all the four metrics. specifically, the
3d resnet-10 backbone with the a-lnm derived fln maps improved the
classification performance by 10.5% to 18.5% in terms of accuracy, which again
demonstrates the superiority of the a-lnm derived fln maps for gbm survival
prediction.
to identify the most discriminative brain regions associated with overall
survival time in gbm, we estimated the relative contribution of each voxel to
the classification performance in our proposed method by using the grad-cam
[10]. to obtain steady results, as shown in fig. 2(a), the voxels with top 5%
weights in the class activation maps (cams) of all candidate models were
overlapped by class, and the position covered by more than half of the models is
displayed. the cams of three classes of survivors overlapped in fig. 2(b) where
both coincident and non-coincident areas exist.the association of an increased
degree of invasion within the frontal lobe with decreased survival time can be
observed, which is in concordance with a previous study [20]. patients whose
frontal lobe is affected by tumors showed more executive dysfunction, apathy,
and disinhibition [11]. on the dominant left hemisphere, the cams of long-term
survivors and mid-term survivors overlapped at the superior temporal gyrus and
wernicke's area which are involved in the sensation of sound and language
comprehension respectively, and have been associated with decreased survival in
patients with high-grade glioma [26]. in addition, the cam of mid-term survivors
covered more areas of the middle and inferior temporal gyri which were
considered as one of the higher level ventral streams of visual processing
linked to facial recognition [25].
in this paper, we introduce a novel neuroimaging feature family, called a-lnm
derived fln maps, for overall survival time prediction of gbm patients. a-lnm
was presented to generate plenty of fln maps for each gbm patient by
partitioning the fdc maps obtained from resting-state fmri of 1000 gsp healthy
subjects into disjoint subsets of equal size and averaging each subset. we
applied a 3d resnet-based backbone network to extract features from the
generated fln maps and classify gbm patients into three overall survival time
groups. experimental results on the brats 2020 training dataset validated the
effectiveness of the a-lnm derived fln maps for gbm survival prediction.
moreover, a visualization analysis implemented by the grad-cam revealed the
brain regions associated with gbm survival. in future work, we will try to fuse
the fln maps and mri-based radiomics features to study their combined predictive
power for gbm survival analysis.
a difficulty faced by surgeons performing endoscopic pituitary surgery is
identifying the areas of the bone which are safe to open. this is of particular
importance during the sellar phase as there are several critical anatomical
structures within close proximity of each other [9]. the sella, behind which the
pituitary tumour is located, is safe to open. however, the smaller structures
surrounding the sella, behind which the optic nerves and internal carotid
arteries are located, carry greater risk. failure to appreciate these critical
parasellar neurovascular structures can lead to their injury, and adverse
outcomes for the patient [9,11]. the human identification of these structures
relies on visual clues, inferred from the impressions made on the bone, rather
than direct visualisations of the structures [11]. this is especially
challenging as the pituitary tumour often compresses; distorts; or encases the
surrounding structures [11]. neurosurgeons utilise identification instruments,
such as a stealth pointer or micro-doppler, to aid in this task [9]. however,
once an identification instrument is removed, identification is lost upon
re-entry with a different instrument, and so the identification can only be used
in referenced to the more visible anatomical landmarks. automatic identification
from endoscopic vision may therefore aid surgeons in this effort while
minimising disruption to the surgical workflow [11]. this is a challenging
computer vision task due to the narrow camera angles enforced by minimally
invasive surgery, which lead to: (i) structure occlusions by instruments and
biological factors (e.g., blood); and (ii) image blurring caused by rapid camera
movements. additionally, in this specific task there are: (iii) numerous small
structures; (iv) visually similar structures; and (v) unclear structure
boundaries. hence, the task can be split into two sub-tasks to account for these
difficulties in identification: (1) the semantic segmentation of the two larger,
visually distinct, and frequently occurring structures (sella and clival
recess); and (2) the centroid detection of the eight smaller structures (fig.
1).to solve both tasks simultaneously, painet (pituitary anatomy identification
network) is proposed. this paper's contribution is therefore:1. the automated
identification of the ten critical anatomical structures in the sellar phase of
endoscopic pituitary surgery. to the best of the authors' knowledge, this is the
first work addressing the problem at this granularity. 2. the creation of
painet, a multi-task neural network capable of simultaneously semantic
segmentation and centroid detection of numerous anatomical structures within
minimally invasive surgery. painet uniquely utilises two loss functions for
improved performance over single-task neural networks due to the increased
information gain from the complementary task.
encoder-decoder architectures are the leading models in semantic segmentation
and landmark detection [4], with common architectures for anatomy identification
including the u-net and deeplab families [6]. improvements to these models
include: adversarial training to limit biologically implausible predictions
[14]; spatial-temporal transformers for scene understanding across consecutive
frames [5]; transfer learning from similar anatomical structures [3]; and graph
neural networks for global image understanding [2]. multi-task networks improve
on the baseline models by leveraging common characteristics between sub-tasks,
increasing the total information provided to the network [15], and are effective
at instrument segmentation in minimally invasive surgery [10].the most
clinically similar works to this paper are: (1) the semantic segmentation of
3-anatomical-structures in the nasal phase of endoscopic pituitary surgery [12].
here, u-net was weakly-supervised on centroids, outputting segmentation masks
for each structure. training on 18-videos (367-images), the model achieved
statistically significant results (p < 0.001) on the hold-out testing dataset of
5-videos (182-images) when compared to a location prior baseline model [12]. (2)
the semantic segmentation of: 2-zones (safe or dangerous); and
3-anatomical-structures in laparoscopic cholecystectomy [7]. here, two pspnets
were fully-supervised on 290-videos (2627 images) using 10-fold
cross-validation, achieving 62% mean intersection over union (miou) for the
2-zones; and 74% miou for the 3-structures [7]. these works are extended in this
paper by increasing the number of anatomical structures and the identification
granularity.
painet: a multi-task encoder-decoder network is proposed to improve performance
by exchanging information between the semantic segmentation and centroid
detection tasks. efficientnetb3, pre-trained on imagenet, is used as the encoder
because of its accuracy, computational efficiency and proven generalisation
capabilities [13]. the decoder is based on u-net++, a state-of-the-art
segmentation network widely used in medical applications [16]. the
encoderdecoder architecture is modified to output both segmentation and centroid
predictions by sending the decoder output into two separate layers: (1) a
convolution for segmentation prediction; and (2) an average pooling layer for
centroid prediction. different loss functions were minimised for each sub-task
(fig. 2). ablation studies and granular details are provided below. the priority
was to find the optimal sella segmentation model, as it is required to be opened
to access the pituitary tumour behind it, indicating the surgical
"safe-zone".semantic segmentation: first, single-class sella segmentation models
were trialed. 8-encoders (pre-trained convolution neural networks) and
15-decoders were used, with their selection based off architecture variety. two
loss functions were also used: (1) distribution-based logits cross-entropy; and
(2) region-based jaccard loss. boundary-based loss functions were not trialed
as: (1) the boundary of the segmentation masks are not well-defined; and (2) in
the cases of split structures (fig. 1c), boundary-based loss functions are not
appropriate [8]. the decoder output is passed through a convolution layer and
sigmoid activation.for multi-class sella and clival recess segmentation, the
optimal single-class model was extended by: (1) sending through each class to
the loss function separately (multi-class separate); and (2) sending both
classes through together (multi-class together). an extension of logits
cross-entropy, logits focal loss, was used instead as it accounts for data
imbalance between classes.centroid detection: 5-models were trialed: 3-models
consisted of encoders with a convolution layer and linear activation; and
2-models consisted of encoderdecoders with an average pooling layer and sigmoid
activation with 0.3 dropout. two distance-based loss functions were trialed: (1)
mean squared error (mse); and (2) mean absolute error (mae). loss was calculated
for all structures simultaneously as a 16 dimensional output (8 centroids × 2
coordinates) and set to 0 for a structure if ground-truth centroids of that
structure was not present.
evaluation metrics: for sella segmentation the evaluation metric is intersection
over union (iou), as commonly used in the field [4,8]. for multi-class
segmentation, the model that optimises clival recess iou without reducing the
previously established sella iou is chosen. precision and recall are also
given.for centroid detection, the evaluation metric is mean percentage of
correct keypoints (mpck) with the threshold set to 20%, indicating the mean
number of predicted centroids falling within 144 pixels of the ground-truth
centroid. this is commonly used in anatomical detection tasks as it ensures the
predictions are close to the ground-truth while limiting overfitting [1].
mpck-40% and mpck-10%, along with the mean percentage of centroids that fall
within their corresponding segmentation mask (mean percentage of centroid masks
(mpcm)) are given as secondary metrics. for multi-task detection, mpck-20% is
optimised such that sella iou does not drop from the previously established
optimal iou.network parameters: 5-fold cross-validation was implemented with no
holdout testing. to account for structure data imbalance, images were randomly
split such that the number of structures in each fold is approximately even.
images from a singular video were present in either the training or validation
dataset.each model was run for with a batch size of 5 for 20 epochs, where the
epoch with the best primary evaluation metric on the validation dataset was
kept. the optimising method was adam with varying initial learning rates, with a
separate optimiser for each loss function during multi-task training.all images
were scaled to 736 × 1280 pixels for model compatibility, and training images
were randomly augmented within the following parameters: shift in any direction
by up to 10%; zooming in or out about the image center by up to 10%; rotation
about the image center clockwise or anticlockwise by up to π/6; increasing or
decreasing brightness, contrast, saturation, and hue by up to 10%.the code is
written in python 3.8 using pytorch 1.8.1, run on a single nvidia tesla v100
tensor core 32-gb gpu using cuda 11.2, and is available at
https://github.com/dreets/pitnet-anat-public. for painet, a batch size of 5
utilised 29-gb and the runtime was approximately 5-min per epoch. valuation
runtime is under 0.1-s per image and therefore a real-time overlay on-top of the
endoscope video feed is feasible intra-operatively.
images: images come from 64-videos of endoscopic pituitary surgery where the
sellar phase is present [9], recorded between 30 aug 2018 and 20 feb 2021 from
the national hospital of neurology and neurosurgery, london, united kingdom. all
patients have provided informed consent, and the study was registered with the
local governance committee. a high-definition endoscope (hopkins telescope, karl
storz endoscopy) was used to record the surgeries at 24 frames per second (fps),
with at least 720p resolution, and stored as mp4 files. 10-images corresponding
to 10-s of the sellar phase immediately preceding sellotomy were extracted from
each video at 1 fps, and stored as 720p png files. video upload and annotation
was performed using touch surgery tm enterprise. annotations: expert
neurosurgeons identified 10-anatomical-structures as critical based on the
literature (fig. 1a) [9,11]. 640-images were manually segmented to obtain
ground-truth segmentations. a two-stage process was used: (1) two neurosurgeons
segmented each image, with any differences settled through discussion; (2) two
consultant neurosurgeons independently peer-reviewed the segmentations. only
visible structures were annotated (fig. 1b); if the structures were occluded,
the segmentation boundaries were drawn around these occlusions (fig. 1c); and if
an image is too blurry to see the structures no segmentation boundaries were
drawn -this excluded 5 images (fig. 1d). the center of mass of each segmentation
mask was defined as the centroid.the sella is present in all 635-images (fig.
3). other than the clival recess, the remaining 8-structures are found in less
than 65% of images, with planum sphenoidal found in less than 25% of images.
moreover, the area covered by these 8-structures are small, with several
covering less than 10% of the total area covered by all structures in a given
image. furthermore, most smaller structures boundaries are ambiguous as they are
hard to define even by expert neurosurgeons. this emphasizes the challenge of
identifying smaller structure in computer vision, and supports the need for
detection and multi-task solutions.
quantitative evaluation is calculated for: single-class sella segmentation
(table 1); single-class, multi-class, and painet 2-structures segmentation
(table 2); multi-class, and painet 8-structures centroid detection (tables 3
and4).the optimal model for single-class sella segmentation achieved 65.4% iou,
utilising an efficientnetb3 encoder; u-net++ decoder; jaccard loss; and a 0.001
initial learning rate. reductions in iou are seen when alternative parameters
are used, highlighting their impact on model performance. using the optimal
sella model configuration, 53.4% iou is achieved for singleclass clival recess
segmentation. extending this to multi-class and painet training improves both
sella and clival recess iou to 66.1% and 54.1% respectively.the optimal model
for centroid detection achieves 51.7% mpck-20%, with minor deviations during
model parameter changes. this model, resnet18 with mse loss, outperforms the
more sophisticated models, as these models over-learn image features in the
training dataset. however, painet leverages the additional information from
segmentation masks to achieve an improved 53.2%.the per structure pck-20%
indicate performance is positively correlated with the number of images where
the structure is present. this implies the limiting factor is the number of
images rather than architectural design. qualitative predictions of the best
performing model, painet, are displayed in fig. 4. the segmentation predictions
look strong, with small gaps from the ground-truth. however, this is expected as
structure boundaries are not welldefined. the centroid predictions are weaker:
in (a) the planum sphenoidale (grey) is predicted within the segmentation mask;
in (b) three structures are within their segmentation mask, but the left
optic-carotid recess (orange) is predicted in a biologically implausible
location; and in (c) this is repeated for the right carotid (pink) and no
structures are within their segmentation masks.
identification of critical anatomical structures by neurosurgeons during
endoscopic pituitary surgery remains a challenging task. in this paper, the
potential of automating anatomical structure identification during surgery was
shown. the proposed multi-task network, painet, designed to incorporate
identification of both large prominent structures and numerous smaller less
prominent structures, was trained on images of the sellar phase of endoscopic
pituitary surgery. using 635-images from 64-surgeries annotated by expert
neurosurgeons and various model configurations, the robustness of the painet was
shown over single task networks. painet achieved 66.1% (+0.7%) and 54.1% iou
(+0.7%) for sella and clival recess segmentation respectively, a higher
performance than other minimally invasive surgeries [7]. painet also achieved
53.2% mpck-20% (+1.5%) for detection of the remaining 8-structures. the most
important structures to identify and avoid, the carotids and optic
protuberances, have high performance, and therefore demonstrate the success of
painet. this performance is greater than similar studies in endoscopic pituitary
surgery for different structures [12] but lower than anatomical detection in
other surgeries [1]. collecting data from more pituitary surgeries will support
incorporating anatomy variations and achieving generalisability. furthermore,
introducing modifications to the model architecture, such as the use of temporal
networks [5], will further boost performance required for real-time video
clinical translation.
1 ± 3.7 56.2 ± 4.7 26.4 ± 4.8 06.8 ± 1.6painetu-net++ efficientnetb3 mse 53.2 ±
5.9 58.0 ± 6.9 39.6 ± 3.2 13.4 ± 2.9
pck-20% 68.3 ± 9.2 37.6 ± 4.8 53.9 ± 7.3 27.6 ± 2.4 76.1 ± 2.8 72.0 ± 9.1 34.8 ±
3.0 55.4 ± 8.5
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43996-4_45.
needle-based liver tumor ablation techniques (e.g., radiofrequency, microwave,
laser, cryoablation) have a great potential for local curative tumor control
[1], with comparable results to surgery in the early stages for both primary and
secondary cancers. furthermore, as it is minimally invasive, it has a low rate
of major complications and procedure-specific mortality, and is tissue-sparing,
thus, its indications are growing exponentially and extending the limits to more
advanced tumors [3]. ct-guidance is a widely used imaging modality for placing
the needles, monitoring the treatment, and following up patients. however, it is
limited by the exposure to ionizing radiation and the need for intravenous
injection of contrast agents to visualize the intrahepatic vessels and the
target tumor(s).in standard clinical settings, the insertion of each needle
requires multiple check points during its progression, fine-tune maneuvers, and
eventual repositioning. this leads to multiple ct acquisitions to control the
progression of the needle with respect to the vessels, the target, and other
sensible structures [26]. however, intrahepatic vessels (and some tumors) are
only visible after contrast-enhancement, which has a short lifespan and
dose-related deleterious kidney effects. it makes it impossible to perform each
of the control ct acquisitions under contrast injection. a workaround to
shortcut these limitations is to perform an image fusion between previous
contrasted and intraoperative noncontrasted images. however, such a solution is
only available in a limited number of clinical settings, and the registration is
only rigid, usually deriving into bad results. in this work, we propose a method
for visualizing intrahepatic structures after organ motion and needle-induced
deformations, in non-injected images, by exploiting image features that are
generally not perceivable by the human eye in common clinical workflows.to
address this challenge, two main strategies could be considered: image fusion
and image processing techniques. image fusion typically relies on the estimation
of rigid or non-rigid transformations between 2 images, to bring into the
intraoperative image structures of interest only visible in the preoperative
data. this process is often described as an optimization problem [9,10] which
can be computationally expensive when dealing with non-linear deformations,
making their use in a clinical workflow limited. recent deep learning approaches
[11,12,14] have proved to be a successful alternative to solve image fusion
problems, even when a large non-linear mapping is required. when ground-truth
displacement fields are not known, state-of-the-art methods use unsupervised
techniques, usually an encoder-decoder architecture [7,13], to learn the unknown
displacement field between the 2 images. however, such unsupervised methods fail
at solving our problem due to lack of similar image features between the
contrasted (cct) and non-contrasted (ncct) image in the vascular tree region
(see sect. 3.3).on the other hand, deep learning techniques have proven to be
very efficient at solving image processing challenges [15]. for instance, image
segmentation [16], image style transfer [17], or contrast-enhancement to cite a
few. yet, segmenting vessels from non-contrasted images remains a challenge for
the medical imaging community [16]. style transfer aims to transfer the style of
one image to another while preserving its content [17][18][19]. however,
applying such methods to generate a contrasted intraoperative ct is not a
sufficiently accurate solution for the problem that we address.
contrast-enhancement methods could be an alternative. in the method proposed by
seo et al. [20], a deep neural network synthesizes contrast-enhanced ct from non
contrast-enhanced ct. nevertheless, results obtained by this method are not
sufficiently robust and accurate to provide an augmented intraoperative ct on
which needle-based procedures can be guided.in this paper we propose an
alternative approach, where a neural network learns local image features in a
ncct image by leveraging the known preoperative vessel tree geometry and
topology extracted from a matching (undeformed) cct. then, the augmented ct is
generated by fusing the deformed vascular tree with the non-contrasted
intraoperative ct. section 2 presents the method and its integration in the
medical workflow. section 3 presents and discusses the results, and finally we
conclude in sect. 4 and highlight some perspectives.
in this section, we present our method and its compatibility with current
clinical workflows. a few days or a week before the intervention, a preoperative
diagnostic multiphase contrast-enhanced image (mpcect) is acquired (fig. 1,
yellow box). the day of the intervention, a second mpcect image is acquired
before starting the needle insertion, followed by a series of standard,
non-injected acquisitions to guide the needle insertion (fig. 1, blue box).
using such a noncontrasted intraoperative image as input, our method performs a
combined non-rigid registration and augmentation of the intraoperative ct by
adding anatomical features (mainly intrahepatic vessels and tumors) from the
preoperative image to the current image. to achieve this result, our method only
requires to process and train on the baseline mpcect image (fig. 1,red box). an
overview of the method is shown in the fig 2 and the following sections describe
its main steps.
we call vessel map (vm) the region of interest defining the vascular tree in the
ncct. since vascular structures are not visible in non-contrasted images, the
extraction of this map is done by segmenting the cct and then using this
segmentation as a mask in the ncct. mathematical morphology operators, in
particular a dilation operation [23], are performed on the segmented region of
interest to slightly increase its dimensions. this is needed to compensate for
segmentation errors and the slight anatomical motion that may exist between the
contrasted and non-contrasted image acquisitions. in practice, the acquisition
protocols limit the shift between the ncct and cct acquisitions, and only a few
sequential dilation operations are needed to ensure we capture the true vessel
fingerprint in the ncct image. note that the resulting vessel map is not a
binary mask, but a subset of the image limited to the volume covered by the
vessels.
the preoperative mpcect provides a couple of registered ncct and cct images.
this is obviously not sufficient for training purposes, as they do not represent
the possible soft tissue deformation that may occur during the procedure.
therefore, we augment the data set by applying multiple random deformations to
the original images. random deformations are created by considering a predefined
set of control points for which we define a displacement field with a random
normal distribution. the displacement field of the full volume is then obtained
by linearly interpolating the control points' displacement field to the rest of
the volume. all the deformations are created using the same number of control
points and characteristics of the normal distributions.
predicting the vascular tree location in the deformed intraoperative ncct is
done using a u-net [5] architecture. the neural network takes as input the
preoperative vessel map and the intraoperative ncct, and outputs the
intraoperative vessel map. our network learns to find the image features (or
vessel fingerprint) present in the vessel map, in a given ncct assuming the
knowledge of its geometry, topology, and the distribution of contrast from the
preoperative mpcect. the architecture of our network is illustrated in fig. 3.
it consists of a four layers analysis (left side) and synthesis (right side)
paths that provide a non-linear mapping between low resolution input and output
images. both paths include four 3 × 3 × 3 unpadded convolutions, each followed
by a leaky rectified linear unit (leakyrelu) activation function. the analysis
includes a 2 × 2 × 2 max pooling with a stride of 1, while the synthesis follows
each convolution by a 2 × 2 × 2 up-convolution with a stride of 1. shortcut
connections from layers of equal resolution in the analysis path provide the
essential high-resolution features to the synthesis path. in the last layer, a 1
× 1 × 1 convolution reduces the number of output channels to one, yielding the
vessel map in the intraoperative image. our network is trained by minimizing the
mean square error between the predicted and ground truth vessel map. training
details are presented in sect. 3.
once the network has been trained on the patient-specific preoperative data, the
next step is to augment and visualize the intraoperative ncct. this is done in 3
steps:-the dilatation operations introduced in sect. 2.1 are not reversible
(i.e. the segmented vessel tree cannot be recovered from the vm by applying the
same number of erosion operations). also, neighboring branches in the vessel
tree could end up being fused, thus changing the topology of the vessel map.
therefore, to retrieve the correct segmented (yet deformed) vascular tree, we
compute a displacement field between the pre-and intraoperative vms. this is
done with the elastix library [21,22]. the resulting displacement field is
applied on the preoperative segmentation to retrieve the intraoperative vessel
tree segmentation. this is illustrated in fig. 4. -the augmented image is
obtained by fusing the predicted intraoperative segmentation with the
intraoperative ncct image. the augmented vessels are displayed in green to
ensure the clinician is aware this is not a true cct image (see fig. 5). -it is
also possible to add anatomical labels to the intraoperative augmented ct to
further assist the clinician. to achieve this objective, we compute a graph data
structure from the preoperative segmentation. we first extract the vessel
centerlines as described in [4]. to define the associated graph structure, we
start by selecting all branches with either no parent or no children. the branch
with the highest radius is then selected as the root edge. an oriented graph is
created using a breadth first search algorithm starting from the root edge.
nodes and edges correspond respectively to vessel tree bifurcations and
branches. we use the graph structure to associate each anatomical label
(manually defined) with a strahler [6] graph ordering. the same process is
applied to the predicted intraoperative segmentation. this makes it possible to
correctly map the preoperative anatomical labels (e.g. vessel name) and display
them on the augmented image.fig. 4. this figure illustrates the different stages
of the pipeline adopted to generate the vm and show how the vessel tree topology
is retrieved from the predicted intraoperative vm by computing a displacement
field between the preoperative vm and the predicted vm. this field is applied to
the preoperative segmentation to get the intraoperative one.
to validate our approach, 4 couples of mpcect abdominal porcine images were
acquired from 4 different subjects. for a given subject, each couple corresponds
to a preoperative and an intraoperative mpcect. we recall that an mpcect
contains a set of registered ncct and cct images. these images are then cropped
and down-sampled to 256 × 256 × 256, and the voxels intensities are scaled
between 0 and 255. finally, we extract the vm from each mpcect sample and apply
3 dilation operations, which demonstrated the best performance in terms of
prediction accuracy and robustness on our data. we note that public data sets
such as deeplesion [24], 3dircadb-01 [25] and others do not fit our problem
since they do not include the ncct images. aiming at a patientspecific
prediction, we only train on a "subject" at a time. for a given subject, we
generate 100 displacement fields using the data augmentation strategy explained
above with 50 voxels for the control points spacing in the three spatial
directions and a standard deviation of 5 voxels for the normal distributions.
the resulting deformation is applied to the preoperative mpcect and its
corresponding vm. thus, we end up with a set of 100 triplets (ncct, cct and vm).
two out of the 100 triplets are used for each training batch, where one is
considered as the pre-operative mpcect and the other as the intraoperative one.
this makes it possible to generate up to 4950 training and validation samples.
the intraoperative mpcect of the same subject is used to test the network. our
method is implemented in tensorflow 2.4, on a geforce rtx 3090. we use an adam
optimizer (β 1 = 0.001, β 2 = 0.999) with a learning rate of 10 -4 . the
training process converges in about 1,000 epochs with a batch size of 1 and 200
steps per epoch.
to assess our method, we use a dice score to measure the overlap between our
predicted segmentation and the ground truth. being a commonly used metric for
segmentation problems, dice aligns the nature of our problem as well as the
clinical impact of our solution. we ha performed tests on 4 different (porcine)
data sets. results are reported in table 1. the method achieved a mean dice
score of 0.81. an example of a subject intraoperative augmented ct is
illustrated in fig. 5, where the three images correspond respectively to the
initial non injected ct, the augmented ct without and with labels. figure 6
illustrates the results of our method for subject 1. the green vessels
correspond to the ground truth intraoperative segmentation, the orange ones to
the predicted intraoperative segmentation and finally the gray vessel tree
corresponds to the preoperative cct vessel tree. such results demonstrate the
ability of our method to perform very well even in the presence of large
deformations. qualitative assessment: to further demonstrate the value of our
method, we have asked two clinicians to manually segment the ncct images in the
intraoperative mpcect data. their results (mean and standard deviation) are
reported in table 1. our method outperforms the results of both clinicians, with
an average dice score of 0.81 against 0.51 as a mean for the clinical experts.
vessel map: we have removed the vm from the network input to demonstrate its
impact on our results. using the data of the subject 1, a u-net was trained to
segment the vessel tree of the intraoperative ncct image. the network only
managed to segment a small portion of the main portal vein branch. thus,
achieving a dice score of 0.16 vs 0.79 when adding the preoperative vm as
additional input. we also studied the influence of the diffusion kernel applied
to the initial segmentation. we have seen, on our experimental data, that 3
dilation operations were sufficient to compensate for the possible motion
between ncct and cct acquisitions.
the problem that we address can be seen from different angles. in particular, we
could attempt to solve it by registering the preoperative ncct to the
intraoperative one and then applying the resulting displacement field to the
known preoperative segmentation. however, state-of-the-art registration methods
such as voxelmorph [7] and others do not necessarily guarantee a diffeomorphic
[8] displacement field that ensures the continuity of the displacement field
inside the parenchyma where the intensity is quite homogeneous on the ncct. to
assess this assumption, a voxelmorph1 network was trained on the subject 1 of
our porcine data sets. we trained the network with both mse and smoothness
losses during 100 epochs and given a batch of size 4. results are illustrated
below in fig. 7. while the voxelmorph network accurately registers the liver
shape, the displacement field is almost null in the region of vessels inside the
parenchyma. therefore, the preoperative vessel segmentation is not correctly
transferred into the intraoperative image.
in this paper, we proposed a method for augmenting intra-operative ncct images
as a means to improve needle ct-guided techniques while reducing the need for
contrast agent injection during tumor ablation procedures, or other needle-based
procedures. our method uses a u-net architecture to learn local vessel tree
image features in the ncct by leveraging the known vessel tree geometry and
topology extracted from a matching cct image. the augmented ct is generated by
fusing the predicted vessel tree with the ncct. our method is validated on
several porcine images, achieving an average dice score of 0.81 on the predicted
vessel tree location. in addition, it demonstrates robustness even in the
presence of large deformations between the preoperative and intraoperative
images. our future steps will essentially involve applying this method to
patient data and perform a small user study to evaluate the usefulness and
limitations of our approach.aknowledgments. this work was partially supported by
french state funds managed by the anr under reference anr-10-iahu-02 (ihu
strasbourg). the authors would like to thank paul baksic and robin enjalbert for
proofreading the manuscript.
healthy and cancerous soft tissue display different elastic properties, e.g. for
breast [19], colorectal [7] and prostate cancer [4]. different imaging
modalities can be used to detect the biomechanical response to an external load
for the characterization of cancerous tissue, e.g., ultrasound, magnetic
resonance and optical coherence elastography (oce). the latter is based on
optical coherence tomography (oct), which provides excellent visualization of
microstructures and superior spatial and temporal resolution in comparison to
ultrasound or magnetic resonance elastography [8]. one common approach for
quantitative oce is to determine the elastic properties from the deformation of
the sample and the magnitude of a quasi-static, compressive load [10]. however,
due to the attenuation and scattering of the near-infrared light, imaging depth
is generally limited to approximately 1 mm in soft tissue. therefore, oce is
well suited for sampling surface tissue and commonly involves bench-top imaging
systems [26], e.g. in ophthalmology [21,22] or as an alternative to
histopathological slice examination [1,16]. handheld oce systems for
intraoperative assessment [2,23] have also been proposed. while conventional oce
probes have been demonstrated at the surface, regions of interest often lie deep
within the soft tissue, e.g., cancerous tissue in percutaneous biopsy.taking
prostate cancer as an example, biomechanical characterization could guide needle
placement for improved cancer detection rates while reducing complications
associated with increased core counts, e.g. pain and erectile dysfunction
[14,18]. however, the measurement of both the applied load and the local sample
compression is challenging. friction forces superimpose with tip forces as the
needle passes through tissue, e.g., the perineum. furthermore, the prostate is
known to display large bulk displacement caused by patient movement and needle
insertions [20,24] in addition to actual sample compression (fig. 1, left). tip
force sensing for estimating elastic properties has been proposed [5] but bulk
tissue displacement of deep tissue was not considered. in principle, compression
and tip force could be estimated by oct. yet, conventional oce probes typically
feature flat tip geometry [13,17].to perform oce in deep tissue structures, we
propose a novel bevel tip oce needle design for the biomechanical
characterization during needle insertions. we consider a dual-fiber setup with
temporal multiplexing for the combined load and compression sensing at the
needle tip. we design an experimental setup that can simulate friction forces
and bulk displacement occurring during needle biopsy (fig. 1). we consider
tissue-mimicking phantoms for surface and deep tissue indentation experiments
and compare our results with force-position curves externally measured at the
needle shaft. finally, we consider how the obtained elasticity estimates can be
used for the classification of both materials.
in the following, we first present our oce needle probe and outline data
processing for elasticity estimates. we then present an experimental setup for
simulating friction and bulk displacement and describe the conducted surface and
deep tissue indentation experiments.
our oce needle approach is illustrated in fig. 2. it consists of an oct imaging
system, a time-division multiplexer and our oce needle probe. the needle
features two single-mode glass fibers (smf-28, thorlabs gmbh, ger) embedded into
a bevel tip needle. the forward viewing fiber (fiber 1) images sample
compression while the load sensing fiber (fiber 2) visualizes the displacement
of a reference epoxy layer that is deformed under load. we cleave the distal
ends of both fibers to enable common path interference imaging. the outer
diameter of the oce needle prototype is 2.0 mm. we use a spectral domain oct
imaging system (telesto i, thorlabs gmbh, ger) with a center wavelength λ 0 of
1325 nm to acquire axial scans (a-scans) at a sampling rate of 91.3 khz. a solid
state optical switch (nssw 1x2 nanospeed tm , agiltron, usa), a 100 khz switch
driver (swdr dc-100khz ns driver, agiltron, usa) and a microcontroller (arduino
tm mega 2560, arduino, usa) alternate between the two fibers every second
a-scan. compared to spatial multiplexing [17], our temporal multiplexing
maximizes the field-of-view and signal strength while effectively halving the
acquisition frequency. with the force f , the area a, initial sample length l 0
and assuming incompressibility, quasi-static loading and neglecting
viscoelasticity. however, the indentation with our bevel tipped needle will not
result in uniform stress and we hypothesize instead that the elasticity is only
relative to the applied tip force f t and the resulting local strain l . to
obtain a single parameter for comparing two measurements, we assume a linear
relationin the context of this work. to detect strain (fiber 1) and applied
force (fiber 2), we consider the phase φ of the complex oct signals for fiber i
at time t and depth z. the phase shift between two a-scans is proportional to
the depth dependent displacement δu i (z, t)assuming a refractive index n of
1.45 and 1.5 for tissue (fiber 1) and epoxy (fiber 2), respectively. we obtain
the deformation u i (z, t) from the unwrapped phase and perform spatial
averaging to reduce noise. for fiber 1, we employ a moving average with a window
size of 0.1 mm. we estimate local strain based on the finite difference along
the spatial dimension over an axial depth δz of 1 mm.for fiber 2, we calculate
the mean ū2 (t) over the entire depth of the epoxy. we assume a linear
coefficient a f to model the relation between the applied tip force f t and the
mean deformation ū2 of the reference epoxy layer.f t (t) = a f * ū2 (t).(5)
we build an experimental setup for surface and deep tissue indentations with
simulated force and bulk displacement (fig. 1). for deep tissue indentations,
different tissue phantoms are stacked on a sample holder with springs in
between.for surface measurements, we position the tissue phantoms separately
without additional springs or tissue around the needle shaft. we use a motorized
linear stage (zfs25b, thorlabs gmbh, ger) to drive the needle while
simultaneously logging motor positions. an external force sensor (kd24s 20n,
me-meßsysteme gmbh, ger) measures combined axial forces. we consider two gelatin
gels as tissue mimicking materials for healthy and cancerous tissue. the two
materials (mat. a and mat. b) display a young's modulus of 53.4 kpa and 112.3
kpa, respectively. reference elasticity is determined by unconfined compression
experiments of three cylindrical samples for each material according to eq. 1,
using force and position sensor data (see supplementary material). the young's
modulus is obtained by linear regression for the combined measurements of each
material. we calibrate tip force estimation (fiber 2) by indentation of silicone
samples with higher tear resistance to ensure that no partial rupture has taken
place. we then determine a linear fit according to eq. 5 and obtain a f = 174.4
mn mm -1 from external force sensor and motor position measurements (see
supplementary material).
in total, we conduct ten oce indentation measurements for each material. three
surface measurements with fixed samples and seven deep tissue indentations with
simulated friction and bulk displacement. for each indentation, we place the
needle in front of the surface or deep tissue interface and acquire oct data
while driving the needle for 3 mm (fig. 1). as the beginning of the needle
movement might not directly correspond to the beginning of sample indentation,
we evaluate oce measurements only if the estimated tip force is larger than 50
mn.to further ensure that measurements occur within the pre-rupture deformation
phase [6,15], only samples below 20 % local strain are considered. a
visualization of the oce acquisition window from an example insertion with
surface rupture and post-rupture cutting phase [6,15] is shown in fig. 3. we
evaluate external needle shaft measurements of relative axial force and relative
motor position with the same endpoint obtained from local strain estimates. we
perform linear regression to determine the slopes e oce [mn % -1 ] and e ext [mn
mm% -1 ] from tip-force-strain and axial-force-position curves, respectively. as
we can consider surface measurements as equivalents to the known elasticity, we
regard the relative error (re) of the mean value obtained for deep indentations,
with respect to the average estimate during surface indentations. we report the
re for both oce and external measurements and material a and b, respectively.
finally, we consider the measured elasticities for the biomechanical
classification of the material. we report the area under the receiver operating
characteristic (auroc) and area under the precision recall curve (auprc) for
both external and oce sensing.
the oce measurements for surface and deep tissue indentations are displayed in
fig. 4a. in comparison, external force-position curves are shown in fig. 5a. the
resulting estimates e oce and e ext are shown in fig. 4b and fig. 5b,
respectively. it can be seen that oce measurements result in separation of both
materials while an overlap is visible for external sensors. the sample
elasticities and relative error are also accumulated in table 1. biomechanical
characterization based on the oce estimates allows complete separation between
materials, with auroc and auprc scores of 1.00 (see supplementary material).
external measurements do not enable robust discrimination of materials and
yielded auroc and auprc scores of only 0.85 and 0.861, respectively.
we demonstrate our approach on two tissue mimicking materials that have similar
elastic properties as healthy and cancerous prostate tissue [5,11]. the con- 4b
and the auroc and auprc scores of 1. note that the high errors for external
measurements at the needle shaft are systematic, as friction and bulk
displacement are unknown. in contrast, our probe does not suffer from these
systematic errors. moreover, considering the standard deviation for oce
estimates, improved calibration of our dual-fiber needle probe is expected to
further improve performance. deep learning-based approaches for tip force
estimation could provide increased accuracy and sensitivity compared to the
assumed linear model [3]. weighted strain estimation based on oct signal
intensity [26] could address the underestimation of local strain during segments
of low signal-to-noise-ratio (see supplementary material). we are also currently
only considering the loading cycle and linear elastic models for our approach.
however, soft-tissue displays strong non-linearity in contrast to the mostly
linear behavior of gelatin gels. compression oce theoretically enables the
analysis of non-linear elastic behavior [26] and future experiments will
consider non-linear models and unloading cycles better befitting
needletissue-interaction [15,25]. interestingly, our needle works with a beveled
tip geometry that allows insertion into deep tissue structures. during
insertion, tip force estimation can be used to detect interfaces and select the
pre-rupture deformation phase for oce estimates (fig. 3). this was previously
not possible with flat tip needle probes [9,13,17]. while the cylindrical tip is
advantageous for calculating the young's modulus, it has been shown that the
calculation of an equivalent young's modulus is rarely comparable across
different techniques and samples [4,12]. instead, it is important to provide
high contrast and high reproducibility to reliably distinguish samples with
different elastic properties. we show that our dual-fiber oce needle probe
enables biomechanical characterization by deriving quantitative biomechanical
parameters as demonstrated on tissue mimicking phantoms. further experiments
need to include biological soft tissue to validate the approach for clinical
application, as our evaluation is currently limited to homogeneous gelatin. this
needle probe could also be very useful when considering robotic needle
insertions, e.g., to implement feedback control based on elasticity estimates.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43996-4 58.
pelvic fracture is a severe type of high-energy injury, with a fatality rate
greater than 50%, ranking the first among all complex fractures [8,16]. surgical
planning and reduction tasks are challenged by the complex pelvic structure, as
well as the surrounding muscle groups, ligaments, neurovascular and other
tissues. robotic fracture reduction surgery has been studied and put into
clinical use in recent years, and has successfully increased reduction precision
and reduced radiation exposure [2]. accurate segmentation of pelvic fracture is
required in both manual and automatic reduction planning, which aim to find the
optimal target location to restore the healthy morphology of pelvic bones.
segmenting pelvic fragments from ct is challenging due to the uncertain shape
and irregular position of the bone fragments and the complex collision fracture
surface. therefore, surgeons typically annotate the anatomy of pelvic fractures
in a semi-automatic way. first, by tuning thresholds and selecting seed points,
adaptive thresholding and region-growing methods are used to extract bone
regions [1,11,15]. then, the fracture surfaces are manually delineated by
outlining the fragments in 3d view or even modifying the masks in a
slice-by-slice fashion. usually, this tedious process can take more than 30 min,
especially when the fractured fragments are collided or not completely
separated.several studies have been proposed to provide more efficient tools for
operators. a semi-automatic graph-cut method based on continuous max-flow has
been proposed for pelvic fracture segmentation, but it still requires the manual
selection of seed points and trail-and-error [4,22]. fully automatic max-flow
segmentation based on graph cut and boundary enhancement filter is useful when
fragments are separated, but it often fails on fragments that are collided or
compressed [9,19]. learning-based bone segmentation has been successfully
applied to various anatomy, including the pelvis, rib, skull, etc. [12,13]. some
deep learning methods have been proposed to detect fractures [17,18,21], but the
output from these methods cannot provide a fully automated solution for
subsequent operations. in fracnet, rib fracture detection was formulated as a
segmentation problem, but with a resultant dice of 71.5%, it merely outlined the
fracture site coarsely without delineating the fracture surface [7].
learning-based methods that directly deal with fracture segmentation have rarely
been studied.fracture segmentation is still a challenging task for the
learning-based method because (1) compared to the more common organ/tumor
segmentation tasks where the model can implicitly learn the shape prior of an
object, it is difficult to learn the shape information of a bone fragment due to
the large variations in fracture types and shapes [10]; (2) the fracture surface
itself can take various forms including large space (fragments isolated and
moved), small gap (fragments isolated but not moved), crease (fragments not
completely isolated), compression (fragments collided), and their combinations,
resulting in quite different image intensity profiles around the fracture site;
and (3) the variable number of bone fragments in pelvic fracture makes it
difficult to prescribe a consistent labeling strategy that applies to every type
and case.this paper proposes a deep learning-based method to segment pelvic
fracture fragments from preoperative ct images automatically. our major
contribution includes three aspects. (1) we proposed a complete automatic
pipeline for pelvic fractures segmentation, which is the first learning-based
pelvic fracture segmentation method to the best of our knowledge. (2) we
designed a novel multi-scale distance-weighted loss and integrated it into the
deeply supervised training of the fracture segmentation network to boost
accuracy near the fracture site. (3) we established a comprehensive pelvic
fracture ct dataset and provided ground-truth annotations. our dataset and
source code are publicly available at https://github.com/yzzliu/fracsegnet. we
expect them to facilitate further pelvis-related research, including but not
limited to fracture identification, segmentation, and subsequent reduction
planning.
our study aims to automatically segment the major and minor fragments of target
bones (left and right ilia and sacrum) from ct scans. as illustrated in fig. 1,
our method consists of three steps. in the first step, an anatomical
segmentation network is used to extract the pelvic bones from the ct scan. with
a cascaded 3d nn-unet architecture, the network is pre-trained on a set of
healthy pelvic ct images [5,13] and further refined on our fractured dataset. in
the second step, a fracture segmentation network is used to separate the bone
fragments from each iliac and sacral region extracted from the first step. to
define a consistent labeling rule that is applicable to all fracture types, we
prescribe three labels for each bone, namely the background, the main fragment,
and other fragments. the main fragment is the largest fragment at the center. in
the third step, isolated components are further separated and labeled, and small
isolated bone fragments are removed to form the final output.
the contact fracture surface (cfs) is the part where the bones collide and
overlap due to compression and impact, and is the most challenging part for
human operators to draw. we are particularly concerned about the segmentation
performance in this region. therefore, we introduce guidance into the network
training using fracture distance map (fdm). a 3d unet is selected as the base
model [6]. the model learns a non-linear mapping relationship m : x → y , where
x and y are the input and ground truth of a training sample,
respectively.fracture distance map. the fdm is computed on the ground-truth
segmentation of each data sample before training. this representation provides
information about the boundary, shape, and position of the object to be
segmented. first, cfs regions are identified by comparing the labels within each
voxel's neighbourhood. then, we calculate the distance of each foreground voxel
to the nearest cfs as its distance value d v , and divide it by the maximum.is
the indicator function for foreground, and d is the normalized distance. the
distance is then used to calculate the fdm weight ŵ using the following
formula:to ensure the equivalence of the loss among different samples, the
weights are normalized so that the average is always 1.fdm-weighted loss. the
fdm weight ŵ is then used to calculate the weighted dice and cross-entropy
losses to emphasize the performance near cfs by assigning larger weights to
those pixels.where l is the number of labels, p l v , y l v are the network
output prediction and the one-hot encoding form of the ground truth for the l th
label of the v th voxel. the overall loss is their weighted sum:where λ ce is a
balancing weight.multi-scale deep supervision. we use a multi-scale deep
supervision strategy in model training to learn different features more
effectively [20]. the deep layers mainly capture the global features with
shape/structural information, whereas the shallow layers focus more on local
features that help delineate fracture surfaces. we add auxiliary losses to the
decoder at different resolution levels (except the lowest resolution level). the
losses are calculated using the corresponding down-sampled fdm ŵ n v , and
down-sampled ground truth y n v . we calculate the output of the n th level l n
by changing λ f dm in eq. ( 3). the λ f dm of each layer decreases by a factor
of 2 as the depth increases, i.e., λ n+1 = λ n /2. in this way, the local cfs
information are assigned more attention in the shallow layers, while the weights
become more uniform in the deep layers.smooth transition. to stabilize network
training, we use a smooth transition strategy to maintain the model's attention
on global features at the early stage of training and gradually shift the
attention towards the fracture site as the model evolves [14]. the smooth
transition dynamically adjusts the proportion of the fdm in the overall weight
matrix based on the number of training iterations. the dynamic weight is
calculated using the following formula:where δ = -ln(1 -t τ ), j is an all-ones
matrix with the same size as the input volume, t is the current iteration
number, and τ is a hyper-parameter. the dynamic weight w st is adjusted by
controlling the relative proportion of j and ŵ . the transition terminates when
the epoch reaches τ .
connected component analysis (cca) has been widely used in segmentation [3], but
is usually unsuitable for fracture segmentation because of the collision between
fragments. however, after identifying and removing the main central fragment
from the previous step, other fragments are naturally separated. therefore, in
the post-processing step, we further isolate the remaining other fragments by
cca. the isolated components are then assigned different labels. in addition, we
remove fragments smaller than a certain threshold, which has no significant
impact on planning and robotic surgery.
although large-scale datasets on pelvic segmentation have been studied in some
research [13], to the best of our knowledge, currently there is no
well-annotated fractured pelvic dataset publicly available. therefore, we
curated a dataset of 100 preoperative ct scans covering all common types of
pelvic fractures. these data is collected from 100 patients (aged 18-74 years,
41 females) who were to undergo pelvic reduction surgery at beijing jishuitan
hospital between 2018 and 2022, under irb approval (202009-04). the ct scans
were acquired on a toshiba aquilion scanner. the average voxel spacing is 0.82 ×
0.82 × 0.94 mm 3 . the average image shape is 480 × 397 × 310.to generate
ground-truth labels for bone fragments, a pre-trained segmentation network was
used to create initial segmentations for the ilium and sacrum [13]. then, these
labels were further modified and annotated by two annotators and checked by a
senior expert.
we compared the proposed method (fdmss-unet) against the network without smooth
transition and deep supervision (fdm-unet) and the network without distance
weighting (unet) in an ablation study. in addition, we also compared the
traditional max-flow segmentation method. in the five-fold cross-validation,
each model was trained for 2000 epochs per fold. the network input was augmented
eight times by mirror flip. the learning rate in adam optimizer was set to
0.0001. λ back was set to 0.2. λ ce was set to 1. the initial λ f dm was set to
16. the termination number for smooth transition τ was set to 500 epochs.the
models were implemented in pytorch 1.12. the experiments were performed on an
intel xeon cpu with 40 cores, a 256 gb memory, and a quadro rtx 5000 gpu. the
comprehensive code and pertinent details are provided at
https://github.com/yzzliu/fracsegnet.
we calculate the dice similarity coefficient (dsc) and the 95th percentile of
hausdorff distance (hd) to evaluation the performance. in addition, we evaluated
the local dice (ldsc) within the 10 mm range near the cfs to assess the
performance in the critical areas. we reported the performance on iliac main
fragment (i-main), iliac other fragment(s) (i-other), sacral main fragment
(smain), sacral other fragment(s) (s-other), and all together.
figure 2 shows a qualitative comparison among different methods. max-flow was
able to generate reasonable segmentation only when the cfs is clear and mostly
non-contact. unet correctly identified the fracture fragments, but was often
confused by the complicated cfs regions, resulting in imprecise fracture lines.
the introduction of fdm weighting and deep supervision with smooth transition
successfully improved the performance near the cfs, and achieved the overall
best result.table 1 shows the quantitative results. the results on the main
fragments were better than other fragments due to the larger proportion. the
deep learning methods had much higher success rates in identifying the
fragments, resulting in significantly better results in minor fragments than
max-flow, with p < 0.05 in paired t-test. introducing the fdm significantly
improved the prediction accuracy in the cfs region (p < 0.05). although fdm-unet
achieved the best ldsc results in several parts, it compromised the global
performance of dsc and hd significantly, compared to fdmss-unet. the deep
supervision and smooth transition strategies stabilized the training, and
achieved the overall best results, with balanced local and global
performance.the average inference time for the fracture segmentation network was
12 s. the overall running time for processing a pelvic ct was 0.5 to 2 min,
depending on the image size and the number of fractured bones.
we have introduced a pelvic fracture ct segmentation method based on deep
convolutional networks. a fracture segmentation network was trained with a
distance-weighted loss and multi-scale deep supervision to improve fracture
surface delineation. we evaluated our method on 100 pelvic fracture ct scans and
made our dataset and ground truth publicly available. the experiments
demonstrated the method's effectiveness on various types of pelvic fractures.
the fdm weighted loss, along with multi-scale deep supervision and smooth
transition, improved the segmentation performance significantly, especially in
the areas near fracture lines. our method provides a convenient tool for
pelvis-related research and clinical applications, and has the potential to
support subsequent automatic fracture reduction planning. one obstacle for deep
learning-based fracture segmentation is the variable number of bone fragments in
different cases. the ultimate goal of this study is to perform automatic
fracture reduction planning for robotic surgery, where the main bone fragment is
held and moved to the planned location by a robotic arm, whereas minor fragments
are either moved by the surgeons' hands or simply ignored. in such a scenario,
we found isolating the minor fragments usually unnecessary. therefore, to define
a consistent labeling strategy in annotation, we restrict the number of
fragments of each bone to three. this rule of labelling applies to all 100 cases
we encountered. minor fragments within each label can be further isolated by cca
or handcrafting when needed by other tasks.we utilize a multi-scale
distance-weighted loss to guide the network to learn features near the fracture
site more effectively, boosting the local accuracy without compromising the
overall performance. in semi-automatic pipelines where human operators are
allowed to modify and refine the network predictions, an accurate initial
segmentation near the fracture site is highly desirable because the fracture
surface itself is much more complicated, often intertwined and hard to draw by
manual operations. therefore, with the emphasis on fracture surface, even when
the prediction from the network is inaccurate, manual operations on 3d view can
suffice for most modifications, eliminating the need for the inefficient
slice-by-slice handcrafting. in future studies, we plan to integrate the
proposed method into an interactive segmentation and reduction planning software
and evaluate the overall performance.
gliomas are the most common central nervous system (cns) tumors in adults,
accounting for 80% of primary malignant brain tumors [1]. early surgical
treatment to remove the maximum amount of cancerous tissues while preserving the
eloquent brain regions can improve the patient's survival rate and functional
outcomes of the procedure [2]. although the latest multi-modal medical imaging
(e.g., pet, diffusion/functional mri) allows more precise pre-surigcal planning,
during surgery, brain tissues can deform under multiple factors, such as
gravity, intracranial pressure change, and drug administration. the phenomenon
is referred to as brain shift, and often invalidates the pre-surgical plan by
displacing surgical targets and other vital anatomies. with high flexibility,
portability, and cost-effectiveness, intra-operative ultrasound (us) is a
popular choice to track and monitor brain shift. in conjunction with effective
mri-us registration algorithms, the tool can help update the pre-surgical plan
during surgery to ensure the accuracy and safety of the intervention.as the true
underlying deformation from brain shift is impossible to obtain and the
differences of image features between mri and us are large, quantitative
validation of automatic mri-us registration algorithms often rely on homologous
anatomical landmarks that are manually labeled between corresponding mri and
intra-operative us scans [3]. however, manual landmark identification requires
strong expertise in anatomy and is costly in labor and time. moreover, inter-and
intra-rater variability still exists. these factors make quality assessment of
brain shift correction for us-guided brain tumor resection challenging. in
addition, due to the time constraints, similar evaluation of inter-modal
registration quality during surgery is nearly impossible, but still highly
desirable. to address these needs, deep learning (dl) holds the promise to
perform efficient and automatic inter-modal anatomical landmark
detection.previously, many groups have proposed algorithms to label landmarks in
anatomical scans [4][5][6][7][8][9]. however, almost all earlier techniques were
designed for mono-modal applications, and inter-modal landmark detection, such
as for usguided brain tumor resection, has rarely been attempted. in addition,
unlike other applications, where the full anatomy is visible in the scan and all
landmarks have consistent spatial arrangements across subjects, intra-operative
us of brain tumor resection only contains local regions of the pathology with
noncanonical orientations. this results in anatomical landmarks with different
spatial distributions across cases. to address these unique challenges, we
proposed a new contrastive learning (cl) framework to detect matching landmarks
in intra-operative us with those from mri as references. specifically, the
technique leverages two convolutional neural networks (cnns) to learn features
between mri and us that distinguish the inter-modal image patches which are
centered at the matching landmarks from those that are not. our approach has two
major novel contributions to the field. first, we proposed a multi-modal
landmark detection algorithm for us-guided brain tumor resection for the first
time. second, cl is employed for the first time in inter-modal anatomical
landmark detection. we developed and validated the proposed technique with the
public resect database [10] and compared its landmark detection accuracy against
the popular scale-invariant feature transformation (sift) algorithm in 3d [11].
contrastive learning has recently shown great results in a wide range of medical
image analysis tasks [12][13][14][15][16][17][18]. in short, it seeks to boost
the similarity of feature representations between counterpart samples and
decrease those between mismatched pairs. often, these similarities are
calculated based on deep feature representations obtained from dl models in the
feature embedding space. this self-supervised learning set-up allows robust
feature learning and embedding without explicit guidance from fine-grained image
annotations, and the encoded features can be adopted in various downstream
tasks, such as segmentation. a few recent works [19][20][21] explored the
potential of cl in anatomical landmark annotation in head x-ray images for 2d
skull landmarks. quan et al. [19,20] attempted to leverage cl for more efficient
and robust learning. yao et al. [21] used multiscale pixel-wise contrastive
proxy tasks for skull landmark detection in x-ray images. with a consistent
protocol for landmark identification, they trained the network to learn
signature features within local patches centered at the landmarks. these prior
works with cl focus on single-modal 2d landmark identification with systematic
landmark localization protocols and sharp image contrast (i.e., skull in x-ray).
in contrast, our described application is more challenging due to the 3d nature,
difficulty in inter-modal feature learning, weaker anatomical contrast (i.e.,
mri vs us), and variable landmark locations. in cl, many works have employed the
infonce loss function [22,23] in attaining good outcomes. inspired by yao et al.
[21], we aimed to use infonce as our loss function with a patch-based approach.
to date, cl has not been explored in multi-modal landmark detection, a unique
problem in clinical applications. in this paper, to bridge this knowledge gap,
we proposed a novel cl-based framework for mri-us anatomical landmark detection.
we employed the publicly available easy-resect (retrospective evaluation of
cerebral tumors) dataset [10] (https://archive.sigma2.no/pages/ public/dataset
detail.jsf?id=10.11582/2020.00025) to train and evaluate our proposed method.
this dataset is a deep-learning-ready version of the original resect database,
and was released as part of the 2020 learn2reg challenge [24]. specifically,
easy-resect contains mri and intra-operative us scans (before resection) of 22
subjects who have undergone low-grade glioma resection surgeries. all images
were resampled to a unified dimension of 256 × 256 × 288 voxels, with an
isotropic resolution of ∼0.5mm. between mri and the corresponding us images,
matching anatomical landmarks were manually labeled by experts and 15∼16
landmarks were available per case. a sample illustration of corresponding
inter-modal scans and landmarks is shown in fig. 1. for the target application,
we employed the t2flair mri to pair with intra-operative us since low-grade
gliomas are usually more discernible in t2flair than in t1-weighted mri [10].
we used two cnns with identical architectures in parallel to extract robust
image features from mri and us scans. specifically, these cnns are designed to
acquire relevant features from mri and us patches, and maximize the similarity
between features of corresponding patches while minimizing those between
mismatched patches. each cnn network contains six successive blocks, and each
block consists of one convolution layer and one group norm, with leaky relu as
the activation function. also, the convolution layer of the first and last three
blocks of the network has 64 and 32 convolutional filters, respectively, and a
kernel size of 3 is used across all blocks. after the convolution layers, the
proposed network has two multi-layer perceptron (mlp) layers with 64 and 32
neurons and leaky relu as the activation function. these mlp layers compress the
extracted features from convolutional layers and produce the final feature
vectors. the resulting cnn network is depicted in fig. 2.
working with 3d images is computationally expensive and can make the model
training unstable and prone to overfitting, especially when the size of the
database is limited. therefore, instead of a full 3d processing, we decided to
implement a 2.5d approach [25] to leverage the efficiency of 2d cnn in the cl
framework for the task. in this case, we extracted a series of three adjacent 2d
image patches in one canonical direction (x-, y-, or z-direction), with the
middle slice centred at the true or candidate landmarks in a 3d scan to provide
slight spatial context for the middle slice of interest. to construct the full
2.5d formulation, we performed the same image patch series extraction in all x-,
y-, and z-directions for a landmark, and this 2.5d patch forms the basis to
compute the similarity between the queried us and reference mri patches. note
that the setup of cl requires three types of samples, anchor, positive sample
pairs, and negative sample pairs. specifically, the anchor is defined as the
2.5d mri patch centred at a predefined landmark, a positive pair is represented
by an anchor and a 2.5d us patch at the corresponding landmark, and finally, a
negative pair means an anchor and a mismatched 2.5d us patch. note that during
network training, instead of 2.5d patches, we compared the 2d image patch series
in one canonical direction between mri and us, and 2d patch series in all three
directions were used. during the inference stage, the similarity between mri and
us 2.5d patches was obtained by summing the similarities of corresponding 2d
image patch series in each direction, and a match was determined with the
highest similarity from all queried us patches. with the assumption that the
brain shift moves the anatomy within a limited range, during the inference time,
we searched within a range of [-5,5] mm in each direction in the us around the
reference mri landmark location to find the best match. note that this search
range is an adjustable parameter by the user (e.g., surgeons/clinicians), and
when no match is found in the search range, an extended search range can be
used. the general overview of the utilized framework for 2d image patch
extraction is shown in fig. 3.
the sift algorithm [11] is a well-known tool for keypoint detection and image
registration. it has been widely used in multi-modal medical registration, such
as landmark matching for brain shift correction in image-guided neurosurgery
[8,26]. to further validate the proposed cl-based method for multi-modal
anatomical landmark detection in us scans, we replicated the procedure using the
3d sift algorithm as follows. first, we calculated the sift features at the
reference landmark's location in mri. then, we acquired a set of candidate sift
points in the corresponding us scan. finally, we identified the matching us
landmark by selecting the top ranking candidate based on sift feature similarity
measured with cosine similarity. note that, for sift-based landmark matching, we
have attempted to impose a similar spatial constraint like in the cl-based
approach. however, as the sift algorithm pre-selects keypoint candidates based
on their feature strengths, with this in consideration, we saw no major benefits
by imposing the spatial constraint.
for cl training, both positive and negative sample pairs need to be created. all
2d patch series were extracted according to sect. 3.3 with a size of 42 × 42 × 3
voxels. these sample pairs were used to train two cnns to extract relevant image
features across mri and us leveraging the infonce loss.
we used the infonce loss [23] for our cl framework. the loss function has been
widely used and demonstrated great performance in many vision tasks. like other
contrastive loss functions, infonce requires a similarity function, and we chose
commonly used cosine similarity. the formulas for infonce (l inf once ) and
cosine similarity (cossim) are as follows:where f θ and g β are the cnn feature
extractors for mr and us patches. x a i and x p i are the cropped image patches
around the corresponding landmarks in mr and us scans, respectively, and x n i
is a mismatched patch in the us image to that cropped around the mri reference
landmark. here, f θ • x a i , g β • x p i , and g β • x n j give the extracted
feature vectors for mr and us patches.
to train our dl model, we made subject-wise division of the entire dataset into
70%:15%:15% as the training, validation, and testing sets, respectively. also,
to improve the robustness of the network, we used data augmentation for the
training data by random rotation, random horizontal flip, and random vertical
flip. furthermore, an adamw optimizer with a learning rate of 0.00001 was used,
and we trained our model for 50 epochs with a batch size of 256.in order to
evaluate the performance of our technique, we used the provided ground truth
landmarks from the database and calculated the euclidean distance between the
ground truths and predictions. the utilized metric is as follows: where x i and
x i , and n are the ground truth landmark location, model prediction, and the
total number of landmarks per subject, respectively.
table 1 lists the mean and standard deviation of landmark identification errors
(in mm) between the predicted position and the ground truth in intra-operative
us for each patient of the resect dataset. in the table, we also provide the
severity of brain shift for each patient. here, tissue deformation measured as
mean target registration errors (mtres) with the ground truth anatomical
landmarks is classified as small (mtre below 3 mm), median (3-6 mm), or large
(above 6 mm). the results show that our cl-based landmark selection technique
can locate the corresponding us landmarks with a mean landmark identification
error of 5.88±4.79 mm across all cases while the sift algorithm has an error
18.78±4.77 mm. with a two-sided paired-samples t-test, our method outperformed
the sift approach with statistical significance (p <1e-4). when reviewing the
mean landmark identification error using our proposed technique, we also found
that the magnitude is associated with the level of brain shift. however, no such
trend is observed when using sift features for landmark identification. when
inspecting landmark identification errors across all subjects between the cl and
sift techniques, we also noticed that our cl framework has significantly lower
standard deviations (p <1e-4), implying that our technique has a better
performance consistency.
inter-modal anatomical landmark localization is still a difficult task,
especially for the described application, where landmarks have no consistent
spatial arrangement across different cases and image features in us are rough.
we tackled the challenge with the cl framework for the first time. as the first
step towards more accurate inter-modal landmark localization, there are still
aspects to be improved. first, while the 2.5d approach is memory efficient and
quick, 3d approaches may better capture the full corresponding image features.
this is partially reflected by the observation that the quality of landmark
localization is associated with the level of tissue shift. however, due to
limited clinical data, 3d approaches caused overfitting in our network training.
second, in the current setup, we employed landmarks in pre-operative mris as
references since its contrast is easier to understand and it allows sufficient
time for clinicians to annotate the landmarks before surgery. future exploration
will also seek techniques to automatically tag mri reference landmarks. finally,
we only employed us scans before resection since tissue removal can further
complicate feature matching between mri and us, and requires more elaborate
strategies, such as those involving segmentation of resected regions [27]. we
will explore suitable solutions to extend the application scenarios of our
proposed framework as part of the future investigation. as a baseline
comparison, we employed the sift algorithm, which has demonstrated excellent
performance in a large variety of computer vision problems for keypoint
matching. however, in the described inter-modal landmark identification for
us-guided brain tumor resection, the sift algorithm didn't offer satisfactory
results. this could be due to the coarse image features and textures of
intra-operative us and the differences in the physical resolution between mri
and us. one major critique for using the sift algorithm is that it intends to
find geometrically interesting keypoints, which may not have good anatomical
significance. in the resect dataset, eligible anatomical landmarks were defined
as deep grooves and corners of sulci, convex points of gyri, and vanishing
points of sulci. the relevant local features may be hard to capture with the
sift algorithm. in this sense, dl-based approaches may be a better choice for
the task. with the cl framework, our method learns the common features between
two different modalities via the training process.besides better landmark
identification accuracy, the tighter standard deviations also imply that our dl
approach serves a better role in grasping the local image features within the
image patches.
in this project, we proposed a cl framework for mri-us landmark detection for
neurosurgery for the first time by leveraging real clinical data, and achieved
state-of-the-art results. the algorithm represents the first step towards
efficient and accurate inter-modal landmark identification that has the
potential to allow intra-operative assessment of registration quality. future
extension of the method in other inter-modal applications can further confirm
its robustness and accuracy.
traditional optical imaging samples the visual spectrum in three diffuse
spectral bands (rgb), while hyperspectral imaging (hsi) provides much more
detailed spectral information. this information is potentially valuable for
making intraoperative decisions, particularly in cases where tissue
differentiation is critical but challenging to perform using traditional
visualisation techniques. in the case of brain tumour excision,
fluorescence-guided resection is commonly used to minimize damage to healthy
tissue [2] but is limited to high-grade gliomas, and results in added cost and
workflow disruptions. thanks to a more detailed definition between tissue types
[5], hsi is seen as a promising alternative with wider applicability and
smoother integration into the workflow.while hsi has been integrated into
surgical microscope systems [11], it is suggested that handheld systems are
better suited to translational research [4]. such handheld systems consist of an
exoscope coupled to a draped optical stack, as shown in fig. 1. the optics in
the exoscope typically result in a short focal depth, making manual focusing
tricky, particularly as the tuning must be performed through the drape. as such,
these systems are commonly left at a fixed focal power and the surgeon must keep
the working distance fixed to keep the subject in focus. furthermore, the narrow
spectral bands of hsi sensors reduce the amount of light collected [12]. to
avoid increasing exposure time, a large aperture size is needed, at a cost of
further reducing focal depth. this exacerbates the focusing issues, making
current real-time handheld hsi imaging systems particularly challenging to
focus, posing significant usability issues. figure 1 highlights the limited
focal depth of our system, and shows a typical target that the surgeon must
manually bring into focus during surgery.the issue of reduced focal depth in
real-time hsi systems could be mitigated by the introduction of a video
autofocus system. autofocus methods are divided into active methods, which use
transmission to probe the scene, and passive methods, which rely only on
incoming light. passive methods are further split into phase-based, which
require specialised hardware, and contrast-based, which compare images captured
at different focal powers. our investigation focuses on contrast-based methods,
which require minimal hardware development.
while autofocusing systems are prevalent in consumer device, the scientific
literature is sparse, especially for dynamic video autofocusing. many
publications in the field are concerned with benchtop microscope autofocus
systems [7,8,15]. this environment is conducive to autofocus as the scene is
typically static with a single focal plane across the whole image. additionally,
the focus can be adjusted easily by moving the stage vertically. [8] take a
traditional approach, making use of a laplacian focal metric combined with a
modified hill-climber optimisation scheme. [15] input a stack of sequential
images to a 3d convolutional neural network (cnn) trained as a deep
reinforcement agent trained to output changes in stage height. [7] train a cnn
to regress the optimal focal power from just two images taken at different focal
powers. beyond benchtop microscopy, [6] also use a cnn to directly regress
optimal focal powers, this time from varying number of samples from the full
focal stacks. [1] take a novel approach by using pre-trained object detection
models to generate latent vector representations of images and using these as
inputs to a deep reinforcement agent. [14] train two cnns, one to regress focal
steps from a single image, the other to determine if the current image is in
focus.
this work aims to improve intraoperative handheld hsi systems by alleviating one
of their main usability drawbacks, that of shortened focal depth. we introduce
an autofocus system to an existing handheld intraoperative real-time hsi system
[4]. the focus adjustments are handled by a focus tunable liquid lens which is
integrated into the setup. we propose autofocusing policies based on deep
reinforcement learning and compare these to traditional heuristic approaches.
our final model is similar to that presented in [15] but differs in its use of a
weight shared image encoder, software simulated defocusing for training data,
and small input patch size. in addition, our method is designed and trained to
handle dynamic environments, something entirely missing in the literature. we
performed a robotic focal-time scan to create a reproducible testing benchmark
and allow quantitative comparison of autofocus policies. finally, we demonstrate
the utility of our approach in a blinded user study involving two neurosurgeons.
our intraoperative hsi system, shown in fig. 2, builds on our existing system
[4] by integrating an optotune el-10-30-ci focus-tunable liquid lens to allow
electrical control of the focal length. the hyperspectral camera is based on an
imec 2/3" snapshot mosaic cmv2k-ssm4x4-vis sensor, which acquires 16 spectral
bands in a 4 × 4 mosaic between the spectral range of 460 nm and 600 nm. with a
sensor resolution of 2048 × 1088 pixels, hyperspectral data is acquired with a
spatial resolution of 512 × 272 pixels per spectral band. video-rate imaging of
snapshot data is achieved with a speed of up to 50 fps depending on acquisition
parameters.
software simulated focal-time scans. we define a focal-time scan as a time
series of focal stacks, with a focal stack being a single image captured at
multiple focal lengths. in order to assemble a large and diverse focal-time scan
dataset, we choose to simulate focal-time scans using existing in-focus video
data. to ensure the resulting focal-time scan features diverse camera motion, we
implement a smooth random walk to step a cropping rectangle across the video
after each frame. this also allows for the construction of plausible focaltime
scans from single images, although features such as dynamic subjects or imaging
noise will be missing. in order to simulate defocus, we implement another random
walk to simulate a dynamic optimal focal power. when an agent is interacting
with the simulated scan, a gaussian filter is used to approximate focal blurring
with σ = σ 0 |f *f | where f and f * are the current and optimal focal powers
and σ 0 is chosen randomly from the range 2-8 for each scan. we use this
technique to create a training and testing dataset consisting of 1000 and 200
simulated focal-time scans based on 200 10-second video clips sampled from
cholec80 [13], a popular endoscopic dataset. in addition, we created simulated
focal-time scans from 200 in focus images taken of a brain phantom with our hsi
system. these act as a validation dataset to help prevent over fitting and aid
generalisation. while gaussian blur is a reasonable approximation, we note that
more rigours methods exist to simulate defocus blur that may produce better
simulated data [9].robotic focal-time scan. as a testing dataset similar to our
intended use case, we chose to approximate a real focal-time scan by controlling
conditions during capture of the individual focal stacks. our optical system was
fixed to a robotic arm, which was then used in a compliant control mode to
record a natural hand-guided trajectory whilst imaging a brain phantom. the
motion was performed to try to emulate typical usage during a surgery, whilst
also trying to cover the range of plausible working distances. the focal range
of the liquid lens is discretised into a set of focal powers, and the recorded
trajectory is discretised into a sequence of 1184 poses. for each discrete pose,
the robotic arm is fixed, and an image captured for each focal power. we
randomise the order of the focal powers to reduce systematic bias caused by the
response of the liquid lens. auto-exposure was implemented in order to ensure
good exposure across all working distances. to ensure consistency within a given
focal stack, autoexposure was only stepped in-between discrete poses. the
robotic arm holding our optical system and a sample of the resulting focal-time
scan can be seen in fig. 3. the optimal focus for all focal stacks was computed
via global search of a traditional focal metric (mean gradient magnitude) as
detailed below. this was then validated visually and corrected where
appropriate.integration and usability trial. to ensure the validity of our
quantitative evaluation, and to get feedback on the system in general, a blinded
trial was set up with two practising neurosurgeons. a set was made containing
two repeats of three selected autofocus policies. this set was then shuffled,
and the surgeons remained blinded to the autofocus policy until after the trial.
each surgeon used our optical system to inspect a brain phantom with each policy
in the set. the surgeon was made aware when the policy was changed and prompted
to make comments throughout the trial, which were recorded.
as seen in fig. 1, the area of surgical interest can make up a rather small
amount of the overall image, as such, we limit ourselves to a patch size of just
32 × 32 pixels. the positioning of the patch could be dictated by a second
algorithm or user input, but this is outside the scope of this work. here, we
simply position the patch at the centre of the circular content area, which is
detected using the method presented in [3]. all of our autofocus policies deal
with the grayscale reconstruction of the hsi images. throughout this work, we
further deal with a normalised focal power range (0-1).traditional approach. we
implement two traditional autofocus policy based on different focal metrics
combined with a simple hill-climber optimisation policy. we choose mean gradient
magnitude (mgm) and mean local ratio (mlr). two focal metrics which are
conceptually simple but competitive [6] and implemented in quite different ways.
they are defined aswhere p is the set of all pixels in the image, i x and i y
are defined as the x and y responses of a sobel filter, and g σ is a gaussian
blur. the kernel size is chosen as σ = 4 for all our experiments. our
hill-climber optimisation policy o hc sets the focal power f at time t + 1 based
on information at time t and is defined aswhere) is the direction of the
previous step and h is a step size which we set to h = 0.05 for all our
experiments. we note that our definition is different from standard
hill-climber. a normal hill-climber will repeat a step while the focal metric is
increasing, and either stop or change direction with a smaller step size when
the focal metric decreases, but this does not translate to a continuous and
dynamic environment.learned optimisation policy. due to our dynamic environment,
it seems likely that considering a sequence of the n last focal metrics, rather
than the last two, would help to build a strong optimisation policy. however, as
n increases, it quickly becomes unclear how to incorporate this information
effectively. it is likely that a learning based solution would uncover a better
strategy than heuristic approaches. while regression based approcahes may work,
reinforcement learning provides a natural framework for this problem by allowing
the policy to model the trade-off between maximisation and exploration. by
modelling the autofocus task as a markov process, we can define a q-function
q(s, a) which maps state-action pairs to expected future rewards. we define our
state, actions, and reward function aswhere f * t is the optimal focal power at
t which can only be known in controlled environment. as before, we take h =
0.05. our learned optimisation policy o rl can then be defined asto model q(s,
a), we use an mlp consisting of 2 hidden layers of 256 relus each and a third
layer with 3 outputs corresponding to the 3 possible actions. the mlp takes as
input the state vector s containing the n most recent focal metrics and focal
powers, we take n = 8 for all our experiments. to train the model, we use deep q
learning following the recommendations set out by the dqn method [10] to improve
training stability. we use an experience memory with size 2.5 × 10 6 , and an
-greedy exploration policy where exponentially decays from 1.0 to 0.1 over the
first 2 × 10 6 experiences. our target model is updated with exponential moving
average (ema) weight updates with a β = 0.005, and we use γ = 0.99 in our
bellman equation. finally, we use a smoothed l1 loss function and optimise with
rmsprop with learning rate 1×10 -5 and momentum 0.95. we trained on our software
simulated focal-time scans created from real endoscopy videos and validated
against our simulated focal-time scans created with hsi images taken with our
optical system mounted on a robotic arm.end-to-end model. in addition to
learning the optimisation policy, we can also learn the focal metric. by
learning the two together, we are no longer constrained to a scalar metric and
can instead learn a latent vector encoding of the image patches. to do this, we
construct a cnn consisting of 4 convolutions with 8 filters each and a stride of
2, outputting a vector of 8 logits for our patch size of 32 × 32. the cnn is run
on each of the n most recent image patches as a batch during training, but only
the most recent during inference, with the previous encodings stored between
steps. the encodings are concatenated with the n most recent focal powers and
fed into an mlp. the mlp and training procedure are the same as before.
we evaluated each autofocus policies on both our simulated focal-time scan test
set, and the robotically recorded focal-time scan. the mean focal errors are
shown in table 1. the scores show an improvement in almost all cases by the
introduction of a learned optimiser. the paths taken and the focal error over
time for the robotic focal-time scan for a selection of policies are plotted in
fig. 4.during the usability trial, the surgeon participants were positive about
all presented policies. in line with our quantitative results, the participants
both showed preference for the cnn-based policy. it was thought by both to be
smoother and more deliberate in its adjustments, and felt more robust to minor
accidental motions inherent to hand-operated system. one commented that it felt
slower to focus but more stable, going on to state that this was desirable
behaviour. all algorithms handled the brain fissure well, this is likely due to
the small patch size used, allowing for precise targeting. overall, the surgeons
were very positive about the integration of autofocus into optical imaging
systems.
we have successfully designed a handheld intraoperative hsi imaging system with
autofocusing capability. we developed a novel cnn-based autofocus policy
suitable for video data. in addition, we performed a robotic focal-time scan to
evaluate our methods. our novel method significantly outperforms a traditional
baseline on our robotic focal-time scan, and performs preferably in a usability
trial by two neurosurgeons. the comments from the usability trial also suggest
that the dynamic video autofocusing systems will be well received among
surgeons.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43996-4 63.
automatic surgical video captioning is critical to understanding the surgery
with complicated operations, and can produce the natural language description
with given surgical videos [24,26]. in this way, these techniques can reduce the
workload of surgeons with multiple applications, such as providing the
intraoperative surgical guidance [17], generating the post-operative surgical
report [4], and even training junior surgeons [8].to generate text descriptions
from input videos, existing captioning works [5,9,21,24,26] mostly consist of a
visual encoder for visual representations and a text decoder for text
generation. some early works [9,11,21,23] adopted a fixed object detector as the
visual encoder to capture object representations for text decoding. this
paradigm in fig. 1(a) requires auxiliary annotations (e.g., bounding box) to
pre-train the visual encoder, and cannot adequately train the entire network for
captioning. to improve performance with high efficiency in practice, recent
works [13,24,25] followed the detector-free strategy, and opened up the joint
optimization of visual encoder and text decoder towards captioning, as shown in
fig. 1(b). despite great progress in this field, these works can be further
improved with two limitations of surgical video captioning.first, existing
surgical captioning works [23,24,26] did not fully consider the inherent
patterns of surgery to facilitate captioning. due to the variability of lesions
and surgical operations, surgical videos contain complex visual contents, and
thus it is difficult to directly learn the mapping from the visual input to the
text output. in fact, the same type of surgery has relatively fixed semantic
patterns, such as using specific surgical instruments for a certain surgical
action. therefore, we introduce the surgical concepts (e.g., surgical
instruments, operated targets and surgical actions) from a semantic perspective,
and guide the surgical captioning network to perceive these surgical concepts in
the input video to generate more accurate surgical descriptions. second,
existing studies [9,24,26] simply processed visual and text modalities in
sequential, while ignoring the semantic gap between these two modalities. this
restricts the integration of visual and text modality knowledge, thereby
damaging the captioning performance. considering that both visual and text
modalities revolve around the same set of surgical concepts, we aim to align the
features in the visual and text modalities with each other through surgical
concepts, and achieve more efficient multi-modal fusion for accurate text
predictions.to address these two limitations in surgical video captioning, we
propose the surgical concept alignment network (sca-net) to bridge the visual
and text modalities through the surgical concepts, as illustrated in fig. 1(c).
specifically, to enable the sca-net to accurately perceive surgical concepts, we
first devise the surgical concept learning (scl) to predict the presence of
surgical concepts with the representations of visual and text modalities,
respectively. moreover, to mitigate the semantic gap between visual and text
modalities of captioning, we propose the mutual-modality concept alignment
(mc-align) to mutually coordinate the encoded features with surgical concept
representations of the other modality. in this way, the proposed sca-net
achieves the surgical concept alignment between visual and text modalities,
thereby producing more accurate captions with aligned multi-modal knowledge. to
the best of our knowledge, this work represents the first effort to introduce
the surgical concepts for the surgical video captioning. extensive experiments
are performed on neurosurgery video and nephrectomy image datasets, and
demonstrate the effectiveness of our sca-net by remarkably outperforming the
state-of-the-art captioning works.
as illustrated in fig. 2, the surgical concept alignment network (sca-net)
follows the advanced captioning architecture [25], and consists of visual and
text encoders, and a multi-modal decoder. we implement the visual encoder with
videoswin [15] to capture the discriminative spatial and temporal
representations from input videos, and utilize the vision transformer (vit) [7]
with causal mask [6] as the text encoder to exploit text semantics with merely
previous text tokens. the multi-modal decoder with vit structure takes both
visual and text tokens as input, and finally generates the caption of the input
video. moreover, to accurately perceive surgical concepts in scl (sect. 2.2),
the sca-net learns from surgical concept labels using separate projection heads
after the visual and text encoders. in the mc-align (sect. 2.3), the visual and
text tokens from two encoders are mutually aligned with the concept
representations of the other modality for better multi-modal decoding.
previous surgical captioning works [23,24,26] generated surgical descriptions
directly from input surgical videos. considering the variability of lesions and
surgical operations, these methods may struggle to understand complex visual
contents and generate erroneous surgical descriptions, thereby hindering
performance to meet clinical requirements. in fact, both the surgical video and
surgical caption represent the same surgical semantics in different modalities.
therefore, we decompose surgical operations into surgical concepts, and guide
these two modalities to accurately perceive the presence of surgical concepts,
so as to better complete this cross-modal task. given a type of surgery, we
regard the surgical instruments, surgical actions and the operated targets used
in surgical videos as surgical concepts. considering that both the visual input
and the shifted text input contain the same set of surgical concepts, we find
out which surgical concepts appear in the input surgical video by parsing the
caption label. in this way, the presence of surgical concepts can be represented
in a multi-hot surgical concept label y cpt ∈ {0, 1} c , where the surgical
concepts that appear in the video are marked as 1 and the rest are marked as 0,
and c is the number of possible surgical concepts. for example, the surgical
video in fig. 2 contains the instrument cutting forcep, the action removing and
the target bone, and thus the surgical concept label y cpt represents these
surgical concepts in corresponding dimensions.to guide the visual modality to
perceive surgical concepts, we aggregate visual tokens generated by the visual
encoder in average, and add a linear layer to predict the surgical concepts of
input videos, where the normalized output p v ∈ [0, 1] c estimates the
probability of each surgical concept. we perform the multi-label classification
using binary sigmoid cross-entropy loss, as follows:in this way, the visual
tokens are supervised to contain discriminative semantics related to valid
surgical concepts, which can reduce prediction errors in surgical descriptions.
for the text modality, we also perform scl for surgical concept prediction p t c
∈ [0, 1] c and calculate the loss l t scl in the same way. by optimizing l scl =
l v scl + l t scl , the scl enables visual and text encoders to exploit
multi-modal features with the perception of surgical concepts, thereby
facilitating the sca-net towards the captioning task.
with the help of scl in sect. 2.2, our sca-net can perceive the shared set of
surgical concepts in both visual and text modalities. however, given the
differences between two modalities with separate encoders, it is inappropriate
for the decoder to directly explore the cross-modal relationship between visual
and text tokens [24,26]. to mitigate the semantic gap of two modalities, we
devise the mc-align to bridge these tokens in different modalities through
surgical concept representations for better multi-modal decoding, as shown in
fig. 2.to align these two modalities, we first collect surgical concept
representations for each modality. note that text tokens are separable for
surgical concepts, while visual tokens are part of the input video containing
multiple surgical concepts. for text modality, we parse the label of each text
token and average text tokens of each surgical concept as t c , and update the
historical text concept representations { tc } c c=1 using exponential moving
average (ema), as tc ← γ tc +(1-γ)t c , where the coefficient γ controls the
updating for stable training and is empirically set as 0.9. for visual modality,
we average visual tokens as the representation of each surgical concept present
in the input video (i.e., v c if surgical concept label y cpt c = 1), and update
the historical visual concept representations {v c } c c=1 with ema, as vc ← γ
vc + (1γ)v c . in this way, we obtain the text and visual concept
representations with tailored strategies for the alignment.then, we mutually
align visual and text concept representations with corresponding historical ones
in another modality. for visual-to-text alignment, visual concept
representations are expected to be similar to corresponding text concept
representations, while differing from other text concept representations as
possible. thus, we calculate the alignment objective l v→t mca with regard to
surgical concepts [10], and the visual encoder can be optimized with the
gradients of visual concept representations in backward, thereby gradually
aligning visual modality to text modality. similarly, text concept
representations are also aligned to the historical visual ones, as
text-to-visual alignment l t→v mca . the mc-align is summarized as follows:where
v and t denote all visual and text representations respectively, and • is the
inner product of vectors. in this way, the mc-align aligns visual and text
representations with each other modality according to the surgical concept, thus
benefiting multi-modal decoding for captioning.
for the surgical captioning task, we adopt standard captioning loss l cap to
optimize the cross-entropy of each predicted word based on previous words y <t
and input video x, as follows:where t is the length of caption prediction.
overall, the final objective of sca-net is summarized as l = l cap + λ 1 l scl +
λ 2 l mca , where loss coefficients λ 1 and λ 2 control the trade-off of scl and
mc-align. by optimizing this final objective l, the proposed sca-net can achieve
multi-modal concept alignment, and generate superior descriptions for the
surgical video captioning.
neurosurgery video captioning dataset. to evaluate the effectiveness of surgical
video captioning, we collect a large-scale dataset with 41 surgical videos of
endonasal skull base neurosurgery. these surgical videos are recorded at the
prince of wales hospital, chinese university of hong kong, where surgeons remove
pituitary tumors through the endonasal corridor to the skull base. after
necessary data cleaning, we divide these surgical videos with resolution of 1,
920× 1, 080 into 11, 004 thirty-second video clips with clear surgical purposes.
these video clips are annotated under tool-tissue interaction (tti) principle
[18], and include a total of 16 instruments, 8 targets, and 10 surgical actions.
the annotation preprocessing follows [26] using nltk [16] toolkit. the
proportion of surgical concepts is illustrated in fig. 3. we split these video
clips at patientlevel, where the video clips of 31 patients are used for
training and the rest of 10 patients are utilized for test.endovis image
captioning dataset. we further compare our method with state-of-the-arts on the
public endovis-2018 image captioning dataset [1,23]. this dataset reveals
robotic nephrectomy procedures acquired by the da vinci x or xi system, and is
annotated with surgical actions between 9 possible tools and surgical targets
[23]. we follow the official split in [24] with 11 sequences for training and 3
sequences for test. in this way, these two datasets can comprehensively evaluate
the captioning tasks under both surgical videos and images.implementation
details. we implement our sca-net and state-of-the-art captioning methods
[5,9,21,24,26] in pytorch [20]. we optimize the sca-net and compared captioning
methods using adam with the batch size of 12 for both captioning datasets. all
models are trained for 20 and 50 epochs in neurosurgery and endovis datasets,
respectively. we adopt the step-wise learning rate decay strategy to facilitate
training convergence, where the learning rate is initialized as 1 × 10 -2 and
halved after every 5 epochs. the loss coefficients λ 1 of l scl and λ 2 of of l
mca are empirically set to 0.1 and 0.01, respectively. all experiments are
performed on a single nvidia a100 gpu.evaluation metrics. to evaluate the
captioning performance, we adopt standard metrics, including bleu@4 [19], meteor
[3], spice [2], rouge [12] and cider [22]. specifically, bleu@4 [19] evaluates
the 4-gram precision of the predicted caption, and cider [22] is based on the
n-gram similarity with tf-idf weights. meteor [3] considers both precision and
recall. rouge [12] and spice [2] measure the matching between predictions and
ground truth. the higher scores of these metrics indicate better performance in
surgical captioning.
to evaluate the performance of our sca-net, we perform a comprehensive
comparison with the state-of-the-art captioning methods, including self-seq
[21], aoanet [9], sig-former [26], m 2 transformer [5], and swinmlp-trancap
[24].as illustrated in table 1, our sca-net achieves the best performance, with
the overwhelming bleu@4 of 48.1%, meteor of 35.1% and cider of 368.4%.
noticeably, our sca-net outperforms the surgical captioning work,
swinmlp-trancap [24], by a large margin, e.g., 16.9% in spice and 13.0% in
rouge. this advantage confirms that the proposed surgical concept alignment can
alleviate the modalities gap in surgical captioning. moreover, compared with the
second-best m 2 transformer [5] with meshed attention between the visual encoder
and the text decoder, our sca-net obtains superior performance with a remarkable
increase of 9.5% in spice and 7.1% in rouge. these experimental results
demonstrate the performance advantage of our sca-net over state-of-the-arts in
the neurosurgery video captioning.ablation study. to further validate the
effectiveness of scl and mc-align, we perform the detailed ablation study in
table 1. specifically, we implement three ablative baselines of the proposed
sca-net, by removing the mc-align (denoted as w/o mc-align) and the scl (denoted
as w/o scl) individually, as well as removing both (denoted as w/o scl,
mc-align). as illustrated in table 1, the proposed scl and mc-align can bring an
individual improvement of 4.0% and 5.5% in bleu@4, respectively, to the baseline
of 40.3%. furthermore, the scl and mc-align can work together to facilitate the
captioning, with a bleu@4 gain of 7.8%. these ablation experiments confirm that
the proposed scl and mc-align play an important role in solving the modality gap
in surgical video captioning, resulting in the performance advantage of our
sca-net.qualitative analysis. we present qualitative results of our sca-net and
state-of-the-arts [5,24] on neurosurgery video captioning. in fig. 4(a),
swinmlp-trancap [24] and m 2 transformer [5] incorrectly predict the operated
targets and ignore important surgical instruments, respectively, and both
methods [5,24] cannot recognize the rare instrument ultrasound probe as well as
the corresponding surgical action in fig. 4(b). with the help of surgical
concept alignment, our sca-net can perceive the surgical concepts present in the
surgical videos and thus generate correct descriptions in these two complex
videos.
to further confirm the effectiveness of surgical captioning, we perform the
comparison on the public endovis image captioning dataset. as shown in table 2,
the end-to-end captioning methods [24,26] outperform the detector-based works
using instrument bounding box as auxiliary annotations [9,21], by optimizing the
visual encoder to meet the requirement of the captioning task. in particular,
our sca-net with swin transformer [14] as visual encoder achieves the best
performance of four metrics (e.g., 47.6% in bleu@4 and 58.4% in spice), and
outperforms the surgical state-of-the-art [24] with the advantage of 7.3% in
bleu@4 and 5.1% in meteor. these comparisons confirm that our sca-net with
surgical concept alignment can produce more accurate surgical captions.
to achieve accurate surgical video captioning, we propose the sca-net to
mitigate the semantic gap of visual and text modalities with surgical concepts.
specifically, we devise the scl to enable the sca-net with the perception of
surgical concepts in visual and text modalities, respectively. moreover, we
propose the mc-align to mutually coordinate visual and text representations with
surgical concept representations of the other modality for multi-modal decoding,
thereby generating more accurate captions with aligned multi-modal
knowledge.extensive experiments on neurosurgery and nephrectomy datasets confirm
the advantage of our sca-net over state-of-the-arts on the surgical captioning.
. this work is supported by national key r&d program of china under grant no.
2021yfe0205700, national natural science foundation of china (no. 62276260,
62076235, 62176254, 61976210, 62002356, 62006230), sponsored by zhejiang lab
(no. 2021kh0ab07) and the innohk program.
despite that deep learning models have shown success in surgical data science to
improve the quality of surgical intervention [20][21][22], such as intelligent
workflow analysis [7,13] and scene understanding [1,28], research on
higher-level cognitive assistance for surgery still remains underexplored. one
essential task is supporting decision-making on dissection trajectories
[9,24,29], which is challenging yet crucial for ensuring surgical safety.
endoscopic submucosal dissection (esd), a surgical procedure for treating early
gastrointestinal cancers [2,30], involves multiple dissection actions that
require considerable experience to determine the optimal dissection trajectory.
informative suggestions for dissection trajectories can provide helpful
cognitive assistance to endoscopists, for mitigation of intraoperative errors,
reducing risks of complications [15], and facilitating surgical skill training
[17]. however, predicting the desired trajectory for future time frames based on
the current endoscopic view is challenging. first, the decision of dissection
trajectories is complicated and depends on numerous factors such as safety
margins surrounding the tumor. second, dynamic scenes and poor visual conditions
may further hamper scene recognition [27]. to date, there is still no work on
data-driven solutions to predict such dissection trajectories, but we argue that
it is possible to reasonably learn this skill from expert demonstrations based
on video data.imitation learning has been widely studied in various domains
[11,16,18] with its good ability to learn complex skills, but it still needs
adaptation and improvement when being applied to learn dissection trajectory
from surgical data. one challenge arises from the inherent uncertainty of future
trajectories. supervised learning such as behavior cloning (bc) [3] tends to
average all possible prediction paths, which leads to inaccurate predictions.
while advanced probabilistic models are employed to capture the complexity and
variability of dissection trajectories [14,19,25], how to ensure reliable
predictions across various surgical scenes still remains a great challenge. to
overcome these issues, implicit models are emerging for policy learning,
inspiring us to rely on implicit behavior cloning (ibc) [5], which can learn
robust representations by capturing the shared features of both visual inputs
and trajectory predictions with a unified implicit function, yielding superior
expressivity and visual generalizability. however, these methods still bear
their limitations. for instance, approaches leveraging energy-based models
(ebms) [4][5][6]12] suffer from intensive computations due to reliance on the
langevin dynamics, which leads to a slow training process. in addition, the
model performance can be sensitive to data distribution and the noise in
training data would result in unstable trajectory predictions.in this paper, we
explore an interesting task of predicting dissection trajectories in esd surgery
via imitation learning on expert video data. we propose implicit diffusion
policy imitation learning (idiff-il), a novel imitation learning approach for
dissection trajectory prediction. to effectively model the surgeon's behaviors
and handle the large variation of surgical scenes, we leverage implicit modeling
to express expert dissection skills. to address the limitations of inefficient
training and unstable performance associated with ebm-based implicit policies,
we formulate the implicit policy using an unconditional diffusion model, which
demonstrates remarkable ability in representing complex high-dimensional data
distribution for videos. subsequently, to obtain predictions from the implicit
policy, we devise a conditional action inference strategy with the guidance of
forward-diffusion, which further improves the prediction accuracy. for
experimental evaluation, we collected a surgical video dataset of esd
procedures, and preprocessed 1032 short clips with dissection trajectories
labelled. results show that our method achieves superior performances in
different contexts of surgical scenarios compared with representative popular
imitation learning methods.
in this section, we describe our approach idiff-il, which learns to predict the
dissection trajectory from expert video data using the implicit diffusion
policy. an overview of our method is shown in fig. 1. we first present the
formulation of the task and the solution with implicit policy for dissection
trajectory learning. next, we present how to train the implicit policy as an
unconditional generative diffusion model. finally, we show the action inference
strategy with forward-diffusion guidance which produces accurate trajectory
predictions with our implicit diffusion policy.
in our approach, we formulate the dissection trajectory prediction to an
imitation learning from expert demonstrations problem, which defines a markov
decision process (mdp) m = (s, a, t , d), comprising of state space s, action
set a, state transition distribution t , and expert demonstrations d. the goal
is to learn a prediction policy π * (a|s) from a set of expert demonstrations d.
the input state of the policy is a clip of video frames s = {i t-l+1 , i t-l+2 ,
. . . , i t }, i t ∈ r h×w ×3 and the output is an action distribution of a
sequence of 2d coordinates a = {y t+1 , y t+2 , ..., y t+n }, y t ∈ r 2
indicating the future dissection trajectory projected to the image space.in
order to obtain the demonstrated dissection trajectories from the expert video
data, we first manually annotate the dissection trajectories on the video frame
according to the moving trend of the instruments observed from future frames,
then create a dataset d = {(s, a) i } m i=0 containing m pairs of video clip
(state) and dissection trajectory (action).to precisely predict the expert
dissection behaviors and effectively learn generalizable features from the
expert demonstrations, we use the implicit model as our imitation policy.
extending the formulation in [5], we model the dissection trajectory prediction
policy to a maximization of the joint state-action probability density function
arg max a∈a p θ (s, a) instead of an explicit mapping f θ (s). the optimal
action is derived from the policy distribution conditioned on the state s, and p
θ (s, a) represents the joint state-action distribution.to learn the implicit
policy from the demonstrations, we adopt the behavior cloning objective which is
to essentially minimize the kullback-leibler (kl) divergence between the
learning policy π θ (a|s) and the demonstration distribution d, also equivalent
to maximize the expected log-likelihood of the joint state-action distribution,
as shown:(in this regard, the imitation of surgical dissection decision-making
is converted to a distribution approximation problem.
approximating the joint state-action distribution in eq. 1 from the video
demonstration data is challenging for previous ebm-based methods. to address the
learning of implicit policy, we rely on recent advances in diffusion models. by
representing the data using a continuous thermodynamics diffusion process, which
can be discretized into a series of gaussian transitions, the diffusion model is
able to express complex high-dimensional distribution with simple parameterized
functions. in addition, the diffusion process also serves as a form of data
augmentation by adding a range of levels of noise to the data, which guarantees
a better generalization in high-dimensional state space.as shown in fig. 1 (a),
the diffusion model comprises a predefined forward diffusion process and a
learnable reverse denoising process. the forward process gradually diffuses the
original data x 0 = (s, a), to a series of noised data {x 0 , x 1 , • • • , x t
} with a gaussian kernel q(x t |x t-1 ), where t denotes the diffusion step. in
the reverse process, the data is recovered via a parameterized gaussian p θ (x
t-1 |x t ) iteratively. with the reverse process, the joint state-action
distribution in the implicit policy can be expressed as:the probability of the
noised data x t in forward diffusion process is a gaussian distribution
expressed as q(x t |x 0 ) = n (x t , √ α t x 0 , (1α t )i ), where α t is a
scheduled variance parameter, which can be referred from [10], and i is an
identity matrix. the trainable reverse transition is a gaussian distribution as
well, whose posterior isand σ θ (x t , t) are the means and the variances
parameterized by a neural network.to train the implicit diffusion policy, we
maximize the log-likelihood of the state-action distribution in eq. 1. using the
evidence lower bound (elbo) as the proxy, the likelihood maximization can be
simplified to a noise prediction problem, more details can be referred to [10].
noise prediction errors for the state and the action are combined using a weight
γ ∈ [0, 1] as the following:where s and a are sampled from n (0, i s ), n (0, i
a ) respectively. to better process features from video frames and trajectories
of coordinates, we employ a variant of the unet as the implicit diffusion policy
network, where the trajectory information is fused into feature channels via mlp
embedding layers. then the trajectory noise is predicted by an mlp branch at the
bottleneck layer.
since the training process introduced in sect. 2.2 is for unconditional
generation, the conventional sampling strategy through the reverse process will
predict random trajectories in expert data. an intuitive way to introduce the
condition into the inference is to input the video clip as the condition state s
* to the implicit diffusion policy directly, then only sample the action part.
but there is a mismatch between the distribution of the state s * and the s t in
the training process, which may lead to inaccurate predictions. hence, we
propose a sampling strategy to correct such distribution mismatch by introducing
the forward-process guidance into the reverse sampling procedure.considering the
reverse process of the diffusion model, the transition probability conditioned
by s * can be decomposed as:where x t = (s t , a t ), p θ (x t-1 |x t ) denotes
the learned denoising function of the implicit diffusion model, and q(s t |s * )
represents a forward diffusion process from the condition state to the t-th
diffused state. therefore, we can attain conditional sampling via the
incorporation of forward-process guidance into the reverse sampling process of
the diffusion model. the schematic illustration of our sampling approach is
shown in fig. 1 (b). at the initial step t = t , action a t is sampled from a
pure gaussian noise, whereas the input state s t is diffused from the input
video clip s * through a forward-diffusion process. at the t-th step of the
denoising loop, the action input a t comes from the denoised action from the
last time step, while the visual inputs s t are still obtained from s * via the
forward diffusion process. the above forward diffusion process and the denoising
step are repeated till t = 0. the final action â0 is the prediction from the
implicit diffusion policy. the deterministic action can be obtained by taking
the most probable samples during the reverse process.
dataset. we evaluated the proposed approach on a dataset assembled from 22
videos of esd surgery cases, which are collected from the endoscopy centre of
the prince of wales hospital in hong kong. all videos were recorded via olympus
microscopes operated by an expert surgeon with over 15 years of experience in
esd. considering the inference speed, we downsampled the original videos to 2fps
frames which are resized to 128 × 128 in resolution. the input state is a 1.5-s
length video clip containing 3 consecutive frames, and the expert dissection
trajectory is represented by a 6-point polyline indicating the tool's movements
in future 3 s. we totally annotated 1032 video clips, which contain 3 frames for
each clip. we randomly selected 742 clips from 20 cases for training, consisting
of 2226 frames, where 10% of these are for validation. the remaining 290 clips
(consisting of 970 frames) were used for testing.experiment setup. first, to
study how the model performs on data within the same surgical context as the
training data, we define a subset, referred as to the "in-the-context" testing
set, which consists of consecutive frames selected from the same cases as
included in the training data. second, to assess the model's ability to
generalize to visually distinct scenes, we created an "out-ofthe-context"
testing set that is composed of video clips sampled from 2 unseen surgical
cases. the sizes of these two subsets are 224 and 66 clips, respectively.
evaluation metrics. to evaluate the performance of the proposed approach, we
adopt several metrics, including commonly used evaluation metrics for trajectory
prediction as used in [23,26], including average displacement error (ade), which
respectively reports the overall deviations between the predictions and the
ground truths, and final displacement error (fde) describing the difference from
the moving target by computing the l2 distance between the last trajectory
points. besides, we also use the fréchet distance (fd) metric, to indicate the
geometrical similarity between two temporal sequences. pixel errors are used as
units for all metrics, while the input images are in 128 × 128 resolution.
to evaluate the proposed approach, we have selected popular baselines and
stateof-the-art methods for comparison. we have chosen the fully supervised
method, behavior cloning, as the baseline, which is implemented using a cnn-mlp
network. in addition, we have included ibc [5], an ebm-based implicit policy
learning method and mid [8], a diffusion-based trajectory prediction approach,
as comparison state-of-the-art approaches.as shown in table 1, our method
outperforms the comparison approaches in both "in-the-context" and
"out-of-the-context" scenarios on all metrics. compared with the diffusion-based
method mid [8], our idiff-il is more effective in predicting long-term goals,
particularly in the "out-of-the-context" scenes, with the evidence of 2.18 error
reduction on fde. for ibc [5], the performance did not meet our expectations and
was even surpassed by the baseline. this exhibits the limitations of ebm-based
methods in learning visual representations from complex endoscopic scenes. the
superior results achieved by our method demonstrate the effectiveness of the
diffusion model in learning the implicit policy from the expert video data. in
addition, our method can learn generalizable dissection skills by exhibiting a
lower standard deviation of the prediction errors compared to the bc, which
severely suffers from over-fitting to the training data. the qualitative results
are presented in fig. 2. we selected three typical scenes in esd surgery (i.e.,
submucosa dissection, mucosa dissection and mucosa incision), and showed the
predictions of idiff-il accompanying the ground truth trajectories. from the
results, our method can generate reasonable visual guidance aligning with the
expert demonstrations on both evaluation sets.
implicit modeling. first, we examined the importance of using implicit modeling
as the policy representation. we simulated the explicit form of the imitation
policy by training a conditional diffusion model whose conditional input is a
video clip. according to the bar charts in fig. 3, the explicit diffusion policy
shows a performance drop for both evaluation sets on ade compared with the
implicit form. the implicit modeling makes a more significant contribution in
predicting within the "in-the-context" scenes, suggesting that the implicit
model excels at capturing subtle changes in surgical scenes. while our method
improves marginally compared with the explicit form on the "out-of-the-context"
data, exhibiting a slighter over-fitting with a lower standard
deviation.forward-diffusion guidance. we also investigated the necessity of the
forward-diffusion guidance in conditional sampling for prediction accuracy. we
remove the forward-diffusion guidance during the action sampling procedure so
that the condition state is directly fed into the policy while sampling actions
through the reverse process. as shown in fig. 3, the implicit diffusion policy
benefits more from the forward-diffusion guidance in the "in-the-context"
scenes, achieving an improvement of 0.33 on ade. when encountered with the
unseen scenarios in "out-of-the-context" data, the performance improvement of
such inference strategy is marginal.value of synthetic data. since the learned
implicit diffusion policy is capable of generating synthetic expert dissection
trajectory data, which can potentially reduce the expensive annotation cost. to
better explore the value of such synthetic expert data for downstream tasks, we
train the baseline model with the generated expert demonstrations. we randomly
generated 9k video-trajectory pairs by unconditional sampling from the implicit
diffusion policy. then, we train the bc model with different data, the pure
expert data (real), synthetic data only (synt) and the mixed data with the real
and the synthetic (mix).the table in fig. 3 shows the synthetic data is useful
as the augmented data for downstream task learning.
this paper presents a novel approach on imitation learning from expert video
data, in order to achieve dissection trajectory prediction in endoscopic
surgical procedure. our idiff-il method utilizes a diffusion model to represent
the implicit policy, which enhances the expressivity and visual generalizability
of the model. experimental results show that our method outperforms
state-of-the-art approaches on the evaluation dataset, demonstrating the
effectiveness of our approach for learning dissection skills in various surgical
scenarios. we hope that our work can pave the way for introducing the concept of
learning from expert demonstrations into surgical skill modelling, and motivate
future exploration on higher-level cognitive assistance in computer-assisted
intervention.
image-to-physical registration is a necessary process for computer-assisted
surgery to align preoperative imaging to the intraoperative physical space of
the patient to in-form surgical decision making. most intraoperatively utilized
image-to-physical regis-trations are rigid transformations calculated using
fiducial landmarks [1]. however, with better computational resources and more
advanced surgical field monitoring sensors, nonrigid registration techniques
have been proposed [2,3]. this has made image-guided surgery more tractable for
soft tissue organ systems like the liver, prostate, and breast [4][5][6]. this
work focuses specifically on nonrigid breast registration, although these
methods could be adapted for other soft tissue organs. current guidance
technologies for breast conserving surgery localize a single tumor-implanted
seed without providing spatial information about the tumor boundary. as a
result, resections can have several centimeters of tissue beyond the cancer
margin. despite seed information and large resections, reoperation rates are
still high (~17%) emphasizing the need for additional guidance technologies such
as computer-assisted surgery systems with nonrigid registration
[7].intraoperative data available for registration is often sparse and subject
to data collection noise. image-to-physical registration methods that accurately
model an elastic soft-tissue environment while also complying with
intraoperative data constraints is an active field of research. determining
correspondences between imaging space and geometric data is required for
image-to-physical registration, but it is often an inexact and ill-posed
problem. establishing point cloud correspondences using machine learning has
been demonstrated on liver and prostate datasets [8,9]. deep learning image
registration methods like voxelmorph have also been used for this purpose [10].
however, these methods require extensive training data and may struggle with
generalizability. other non-learning image-to-physical registration strategies
include [11] which utilized a corotational linear-elastic finite element method
(fem) combined with an iterative closest point algorithm. similarly, the
registration method introduced in [12] iteratively updated the image-to-physical
correspondence between surface point clouds while solving for an optimal
deformation state.in addition to a correspondence algorithm, a technique for
modeling a deformation field is required. both [11] and [12] leverage fem, which
uses a 3d mesh to solve for unique deformation solutions. however, large
deformations can cause mesh distortions with the need for remeshing. mesh-free
methods have been introduced to circumvent this limitation. the element-free
galerkin method is a mesh-free method that requires only nodal point data and
uses a moving least-squares approximation to solve for a solution [13]. other
mesh-free methods are reviewed in [14]. although these methods do not require a
3d mesh, solving for a solution can be costly and boundary condition designation
is often unintuitive. having identified these same shortcomings, [15] proposed
regularized kelvinlet functions for volumetric digital sculpting in computer
animation applications. this sculpting approach provided de-formations
consistent with linear elasticity without large computational overhead.in this
work, we propose an image-to-physical registration method that uses regularized
kelvinlet functions as a novel deformation basis for nonrigid registration.
regularized kelvinlet functions are analytical solutions to the equations for
linear elasticity that we superpose to compute a nonrigid deformation field
nearly instantaneously [15].we utilize "grab" and "twist" regularized kelvinlet
functions with a linearized iterative reconstruction approach (adapted from
[12]) that is well-suited for sparse data registration problems. sensitivity to
regularized kelvinlet function hyperparameters is explored on a supine mr breast
imaging dataset. finally, our approach is validated on an exemplar breast cancer
case with a segmented tumor by comparing performance to previously proposed
registration methods.
in this section, closed-form solutions to linear elastic deformation responses
in an infinite medium are derived to obtain regularized kelvinlet functions.
then, methods for constructing a superposed regularized kelvinlet function
deformation basis for achieving registration within an iterative reconstructive
framework are discussed. equation notation is written such that constants are
italicized, vectors are bolded, and matrices are double-struck letters. linear
elasticity in a homogeneous, isotropic media is governed by the navier-cauchy
equations in eq. ( 1), where e is young's modulus, ν is poisson's ratio, u(x) is
the displacement vector, and f(x) is the forcing function. analytical
displacement solutions to eq. ( 1) that represent elastostatic states in an
infinite solid can be found for specific forcing functions f(x). equation (2)
represents the forcing function for a point source f δ (x), where f is the point
source forcing vector and x 0 is the load location. the closed-form displacement
solution for eq. ( 1) given the forcing function in eq. ( 2) is classically
known as the kelvin state in eq. ( 3), rewritten as a function of r where r = xx
0 and r = r . the coefficients are a
, and i is the identity matrix.we note that the deformation response is linear
with respect to f , which implies that forcing functions can be linearly
superposed. however, practical use of eq. ( 3) becomes numerically problematic
in discretized problems because the displacement and displacement gradient
become indefinite as x approaches x 0 .to address numerical singularity,
regularization is incorporated with a new forcing function eq. ( 4), where r ε =
√ r 2 + ε 2 is the regularized distance, and ε is the regularization radial
scale. solving eq. (1) using eq. ( 4) yields a formula for the first type of
regularized kelvinlet functions used in this work in eq. ( 5), which is the
closed-form, analytical solution for linear elastic translational ("grab")
deformations.the second type of regularized kelvinlet functions represent
"twist" deformations which are derived by expanding the previous formulation to
accommodate locally affine loads instead of displacement point sources. this is
accomplished by associating each component of the forcing function eq. ( 4) with
the directional derivative of each basis g i of the affine transformation,
leading to the regularized forcing matrix in eq. ( 6). an affine loading
configuration consisting of pure rotational ("twist") deformation constrains f
ij ε (x) to a skew-symmetric matrix that simplifies the forcing function to a
cross product about a twisting force vector f in eq. (7). the pure twist
displacement field response u ε,twist (r) to the forcing matrix in eq. ( 7) can
be represented as the second type of regularized kelvinlet functions used in
this work in eq. (8).superpositions of eq. ( 5) and eq. ( 8) are used in a
registration workflow to model linear elastic deformations in the breast. these
deformations are visualized on breast geometry embedded in an infinite medium
with varying ε values in fig. 1.
for registration, x 0 control point positions for k number of total regularized
kelvinlets "grab" and "twist" functions are distributed in a predetermined
configuration. then, the f grab and f twist vectors are optimized to solve for a
displacement field that minimizes distance error between geometric data inputs.
for a predetermined configuration of regularized kelvinlet "grab" and "twist"
functions centered at different x 0 control point locations, an elastically
deformed state can be represented as the summation of all regularized kelvinlet
displacement fields where ∼ u (x) is the superposed displacement vector and k =
k grab + k twist in eq. (9). equation ( 9) can be rewritten in matrix form shown
in eq. (10), where α is a concatenated vector of length 3k such that αthis
formulation decouples the forcing magnitudes from the kelvinlet response matrix
∼ k (x), which is composed of column u ε,grab (x) and u ε,twist (x) vectors
calculated with unit forcing vectors for each k grab (x) and k twist (x)
function. this allows for linear scaling of ∼ k (x) using α. by setting x 0
locations, ε grab , and ε twist as hyperparameters, deformation states can be
represented by various α vectors with the registration task being to solve for
the optimal α vector.an objective function is formulated to minimize
misalignment between the moving space x moving and fixed space x fixed through
geometric data constraints. for the breast imaging datasets in this work, we
used simulated intraoperative data features that realistically could be
collected in a surgical environment visualized in fig. 2. the first data feature
is mr-visible skin fiducial points placed on the breast surface (fig. 2,red).
these fiducials have known point correspondence. the other two data features are
an intra-fiducial point cloud of the skin surface (fig. 2, light blue) and
sparse contour samples of the chest wall surface (fig. 2, yellow). these data
features are surfaces that do not have known correspondence. these data feature
designations are consistent with implementations in previous work [16,17].for a
given deformation state, each data feature contributes to the total error
measure. for the point data, the error e i point for each point i is simply the
distance magnitude between corresponding points in x fixed and x moving space.
for the surface data, the error e i surface is calculated as the distance from
every point i in the x fixed point cloud surface to the closest point in the x
moving surface, projected onto the surface unit normal which allows for sliding
contact between surfaces. the optimization using the objective function in eq. (
11) includes two additions to improve the solution. the first is rigid
parameters, translation τ and rotation θ, that are optimized simultaneously with
the vector α. β represents the deformation state with β = [α, τ , θ ], and this
compensates for rigid deformation between x fixed and x moving . the second is a
strain energy regularization term e se which penalizes deformations with large
strain energy. e se is the average strain energy density within the breast
geometry, and it is computed for each β at every iteration. it is scaled by
weight w se . the optimal state β is iteratively solved using
levenberg-marquardt optimization terminating at | (β)|<10 -12 .
in this section, two experiments are conducted. the first explores sensitivity
to regularized kelvinlet function hyperparameters k grab , k twist , ε grab ,
and ε twist and establishes optimal hyperparameters in a training dataset of 11
breast deformations. the second validates the registration method in a breast
cancer patient and compares registration accuracy and computation time to
previously proposed methods.
this dataset consists of supine breast mr images simulating surgical
deformations of 11 breasts from 7 healthy volunteers. volunteers (ages 23-57)
were enrolled in a study approved by the institutional review board at
vanderbilt university. prior to imaging, 26 skin fiducials were distributed on
the breast surface. mr images (0.391 × 0.391 × 1 mm 3 or 0.357 × 0.357 × 1 mm 3
) were acquired with the volunteers' arms placed by their sides. this image was
used as the x moving space. the volunteers were then instructed to raise one arm
above their heads, causing deformation of the ipsilateral breast. a second mr
image in the deformed state was acquired to create simulated intraoperative
physical data and to use for validation. this second image was used as the x
fixed space. the breast in x moving was segmented at the boundary between the
chest wall and breast parenchyma to create a 3d model. the posterior surface was
labeled to inform x 0 control point locations. the skin fiducials and
intra-fiducial surface point clouds were labeled in both images as data
features. sparse tracked ultrasound data collection patterns were projected on
the posterior surface for use as the third data feature. subsurface anatomical
targets were labeled in both images and used to compute target error after
registration.three configurations were explored to test different distributions
of grab and/or twist regularized kelvinlet functions: grab functions only, twist
functions only, and a combination of grab and twist functions. grab function
control points were distributed evenly on the posterior surface of the breast to
approximate forces from the chest wall. twist function control points were
distributed evenly within the breast to approximate internal body forces. three
hyperparameter sweeps were used:
this dataset consists of supine breast mr images simulating surgical
deformations from one breast cancer patient. a 71-year-old patient with invasive
mammary carcinoma in the left breast was enrolled in a study approved by the
institutional review board at vanderbilt university. skin fiducial placement,
image acquisition, arm placement, and preprocessing steps followed the same
protocol detailed in sect. 3.1. the tumor was segmented in both images by a
subject matter expert, and a 3d tumor model was created to evaluate tumor
overlap metrics after registration.regularized kelvinlet function registration
was compared to 3 other registration methods: rigid registration, an fem-based
image-to-physical registration method, and an image-to-image registration
method. a point-based rigid registration using the skin fiducials provided a
baseline comparator for accuracy without deformable correction. the fem-based
image-to-physical registration method, detailed in [12] and implemented in
breast in [16], utilizes the same optimization scheme as this method but with an
fem-generated basis. k = 40 control points were used for the fem-based
registration. the image-to-image registration method was a symmetric
diffeomorphic method with explicit b-spline regularization publicly available in
the advanced normalization toolkit (ants) repository [19,20]. image-to-image
registration would not be possible for intraoperative registration in most
surgical settings. however, it was included to demonstrate accuracy when
volumetric imaging data is available, as opposed to sparse geometric point data
as in the surgical application case. the rigid and image-to-physical
registrations were performed on a single thread of a 3.6 ghz amd ryzen 7 3700x
cpu. image-to-image registration was multithreaded on 2.3 ghz intel xeon
(e5-4610 v2) cpus.registration results for the 4 methods are shown in table 1.
the regularized kelvinlet method accuracy was comparable (if not slightly
improved) to the fem-based method for this example case. runtime for the
regularized kelvinlet method was improved compared to the fem-based method. as
expected, registration without deformable correction was poor, and
image-to-image registration had the best accuracy. registered tumor geometry
results are shown in fig. 4.
several limitations should be noted. regularized kelvinlet functions describe
solutions that assume a physical embedding within an infinite elastic domain,
which does not account for organ-specific geometry. this approach may not be
well suited for problems where geometry has significant influence. this method
is derived from a linear elastic model, and nonlinear models are known to better
describe soft tissue mechanics. additionally, this method assumes homogeneity
and isotropy -it does not account for different tissue types and directional
structures in the breast. with regards to clinical feasibility, supine mr
imaging with skin fiducials is not the standard-of-care. however, using supine
mr imaging for surgery is becoming increasingly investigated, and previous work
demonstrated the potential of ink-based skin fiducial markings on the breast
[21,22]. despite these limitations, this method's accuracy and speed may be
appropriate for surgical guidance applications.in this work, we demonstrated the
use of regularized kelvinlet functions for imageto-physical registration of the
breast. we achieved near real-time registration with comparable accuracy to
previously proposed methods. we believe that this approach is generalizable to
other soft-tissue organ systems and is well-suited for improving navigation
during image-guided surgeries.
target error (mm)6.1 ± 1.4 3.3 ± 1.1 3.0 ± 1.1 2.3 ± 1.5
we address the important problem of intraoperative patient-to-image registration
in a new way by relying on preoperative data to synthesize plausible
transformations and appearances that are expected to be found intraoperatively.
in particular, we tackle intraoperative 3d/2d registration during neurosurgery,
where preoperative mri scans need to be registered with intraoperative surgical
views of the brain surface to guide neurosurgeons towards achieving a maximal
safe tumor resection [22]. indeed, the extent of tumor removal is highly
correlated with patients' chances of survival and complete resection must be
balanced against the risk of causing new neurological deficits [5] making
accurate intraoperative registration a critical component of
neuronavigation.most existing techniques perform patient-to-image registration
using intraoperative mri [11], cbct [19] or ultrasound [9,17,20]. for 3d-3d
registration, 3d shape recovery of brain surfaces can be achieved using
near-infrared cameras [15], phase-shift 3d shape measurement [10], pattern
projections [17] or stereovision [8]. the 3d shape can subsequently be
registered with the preoperative mri using conventional point-to-point methods
such as iterative closest point (icp) or coherent point drift (cpd). most of
these methods rely on cortical vessels that bring salient information for such
tasks. for instance, in [6], cortical vessels are first segmented using a deep
neural network (dnn) and then used to constrain a 3d/2d non-rigid registration.
the method uses physics-based modeling to resolve depth ambiguities. a manual
rigid alignment is however required to initialize the optimization.
alternatively, cortical vessels have been used in [13] where sparse 3d points,
manually traced along the vessels, are matched with vessels extracted from the
preoperative scans. a model-based inverse minimization problem is solved by
estimating the model's parameters from a set of pre-computed transformations.
the idea of pre-computing data for registration was introduced by [26], who used
an atlas of pre-computed 3d shapes of the brain surface for registration. in
[7], a dnn is trained on a set of pre-generated preoperative to intraoperative
transformations. the registration uses cortical vessels, segmented using another
neural network, to find the best transformation from the pre-generated set.the
main limitation of existing intraoperative registration methods is that they
rely heavily on processing intraoperative images to extract image features (eg.,
3d surfaces, vessels centerlines, contours, or other landmarks) to drive
registration, making them subject to noise and low-resolution images that can
occur in the operating room [2,25]. outside of neurosurgery, the concept of
pregenerating data for optimizing dnns for intraoperative registration has been
investigated for ct to x-ray registration in radiotherapy where x-ray images can
be efficiently simulated from cts as digital radiographic reconstructions
[12,27]. in more general applications, case-centered training of dnns is gaining
in popularity and demonstrates remarkable results [16].
we propose a novel approach for patient-to-image registration that registers the
intraoperative 2d view through the surgical microscope to preoperative mri 3d
images by learning expected appearances. as shown in fig. 1, we formulate the
problem as a camera pose estimation problem that finds the optimal 3d pose
minimizing the dissimilarity between the intraoperative 2d image and its
pre-generated expected appearance. a set of expected appearances are synthesized
from the preoperative scan and for a set of poses covering the range of
plausible 6 degrees-of-freedom (dof) transformations. this set is used to train
a patient-specific pose regressor network to obtain a model that is
texture-invariant and is cross-modality to bridge the mri and rgb camera
modalities. similar to other methods, our approach follows a monocular
singleshot registration, eliminating cumbersome and tedious calibration of
stereo cameras, the laser range finder, or optical trackers. in contrast to
previous methods, our approach does not involve processing intraoperative images
which have several advantages: it is less prone to intraoperative image
acquisition noise; it does not require pose initialization; and is
computationally fast thus supporting real-time use. we present results on both
synthetic and clinical data and show that our approach outperformed
state-of-the-art methods.
pose sampling fig. 1. our approach estimates the 6-dof camera pose that aligns a
preoperative 3d mesh derived from mri scans onto an intraoperative rgb image
acquired from a surgical camera. we optimize a regressor network pω over a set
of expected appearances that are generated by first sampling multiple poses and
appearances from the 3d mesh using neural image analogy through sθ.
as illustrated in fig. 1, given a 3d surface mesh of the cortical vessels m,
derived from a 3d preoperative scan, and a 2d monocular single-shot image of the
brain surface i, acquired intraoperatively by a surgical camera, we seek to
estimate the 6-dof transformation that aligns the mesh m to the image i.
assuming a set of 3d points u = {u j ∈ r 3 } ⊂ m and a set of 2d points in the
image v = {v i ∈ r 2 } ⊂ i, solving for this registration problem can be
formalized as finding the 6-dof camera pose that minimizes the reprojection
error:where r ∈ so(3) and t ∈ r 3 represent a 3d rotation and 3d translation,
respectively, and a is the camera intrinsic matrix composed of the focal length
and the principal points (center of the image) while {c i } i is a
correspondence map and is built so that if a 2d point v i corresponds to a 3d
point u j where c i = j for each point of the two sets. note that the set of 3d
points u is expressed in homogenous coordinates in the minimization of the
reprojection error.in practice, finding the correspondences set {c i } i between
u and v is nontrivial, in particular when dealing with heterogeneous
preoperative and intraoperative modality pairs (mri, rgb cameras, ultrasound,
etc.) which is often the case in surgical guidance. existing methods often rely
on feature descriptors [14], anatomical landmarks [13], or organ's contours and
segmentation [6,18] involving tedious processing of the intraoperative image
that is sensitive to the computational image noise. we alleviate these issues by
directly minimizing the dissimilarity between the image i and its expected
appearance synthesized from m.by defining a synthesize function s θ that
synthesizes a new image i given a projection of a 3d surface mesh for different
camera poses, i.e. i = s θ (a[r|t], m), the optimization problem above can be
rewritten as:argminthis new formulation is correspondence-free, meaning that it
alleviates the requirement of the explicit matching between u and v. this is one
of the major strengths of our approach. it avoids the processing of i at
run-time, which is the main source of registration error. in addition, our
method is patient-specific, centered around m, since each model is trained
specifically for a given patient. these two aspects allow us to transfer the
computational cost from the intraoperative to the preoperative stage thereby
optimizing intraoperative performance.the following describes how we build the
function s θ and how to solve eq. 1.
we define a synthesis network s θ : (a[r|t], m, t) → i, that will generate a new
image resembling a view of the brain surface from the 2d projection of the input
mesh m following [r|t], and a texture t. several methods can be used to optimize
θ. however, they require a large set of annotated data [3,24] or perform only on
modalities with similar sensors [12,27]. generating rgb images from mri scans is
a challenging task because it requires bridging a significant difference in
image modalities. we choose to use a neural image analogy method that combines
the texture of a source image with a high-level content representation of a
target image without the need for a large dataset [1]. this approach transfers
the texture from t to i constrained by the projection of m using a[r|t] by
minimizing the following loss function:where g l ij (t ) is the gram matrix of
texture t at the l-th convolutional layer (pre-trained vgg-19 model), and w l,c
t class are the normalization factors for each gram matrix, normalized by the
number of pixels in a label class c of t class . this allows for the
quantification of the differences between the texture image t and the generated
image i as it is being generated. importantly, computing the inner-most sum over
each label class c allows for texture comparison within each class, for
instance: the background, the parenchyma, and the cortical vessels.in practice,
we assume constant camera parameters a and first sample a set of binary images
by randomly varying the location and orientation of a virtual camera [r|t]
w.r.t. to the 3d mesh m before populating the binary images with the textures
using s θ (see fig. 2). we restrict this sampling to the upper hemisphere of the
3d mesh to remain consistent with the plausible camera positions w.r.t.
patient's head during neurosurgery.we use the l-bfgs optimizer and 5
convolutional layers of vgg-19 to generate each image following [1] to find the
resulting parameters θ. the training to synthesize for a single image typically
takes around 50 iterations to converge.
in order to solve eq. 1, we assume a known focal length that can be obtained
through pre-calibration. to obtain a compact representation of the rotation and
since poses are restricted to the upper hemisphere of the 3d mesh (no gimbal
lock), the euler-rodrigues representation is used. therefore, there are six
parameters to be estimated: rotations r x , r y , r z and translations t x , t y
, t z . we estimate our 6-dof pose with a regression network p ω : i → p and
optimize its weights ω to map each synthetic image i to its corresponding camera
pose p = [r x , r y , r z , t x , t y , t z ] t .the network architecture of p ω
consists of 3 blocks each composed of two convolutional layers and one relu
activation. to decrease the spatial dimension, an average pooling layer with a
stride of 2 follows each block except the last one. at the end of the last
hierarchy, we add three fully-connected layers with 128, 64, and 32 neurons and
relu activation followed by one fully-connected with 6 neurons with a linear
activation. we use the set of generated expected appearances t p = {(i i ; p i
)} i ; and optimize the following loss function over the parameters ω of the
network p ω :where t and r vec are the translation and rotation vector,
respectively. we experimentally noticed that optimizing these entities
separately leads to better results. the model is trained for each case (patient)
for 200 epochs using mini-batches of size 8 with adam optimizer and a learning
rate of 0.001 and decays exponentially to 0.0001 over the course of the
optimization. finally, at run-time, given an image i we directly predict the
corresponding 3d pose p so that: p ← p(i; ω), where ω is the resulting
parameters from the training.
dataset. we tested our method retrospectively on 6 clinical datasets from 6
patients (cases) (see fig. 5). these consisted of preoperative t1 contrast mri
scans and intraoperative images of the brain surface after dura opening.
cortical vessels around the tumors were segmented and triangulated to generate
3d meshes using 3d slicer. we generated 100 poses for each 3d mesh (i.e.: each
case) and used a total of 15 unique textures from human brain surfaces
(different from our 6 clinical datasets) for synthesis using s θ . in order to
account for potential intraoperative brain deformations [4] we augment the
textured projection with elastic deformation [21] resulting in approximately
1500 images per case. the surgical images of the brain (left image of the
stereoscopic camera) were acquired with a carl zeiss surgical microscope. the
ground-truth poses were obtained by manually aligning the 3d meshes on their
corresponding images. we evaluated the pose regressor network on both synthetic
and real data. the model training and validation were performed on the
synthesized images while the model testing was performed on the real images.
because a conventional train/validation/test split would lead to texture
contamination, we created our validation dataset so that at least one texture is
excluded from the training set. on the other hand, the test set consisted of the
real images of the brain surface acquired using the surgical camera and are
never used in the training. accuracy-threshold curves on the validation
set.metrics. we chose the average distance metric (add) as proposed in [23] for
evaluation. given a set of mesh's 3d vertices, the add computes the mean of the
pairwise distance between the 3d model points transformed using the ground truth
and estimated transformation. we also adjusted the default 5 cm-5 deg
translation and rotation error to our neurosurgical application and set the new
threshold to 3 mm-3 deg.accuracy-threshold curves. we calculated the number of
'correct' poses estimated by our model. we varied the distance threshold on the
validation sets (excluding 2 textures) in order to reveal how the model performs
w.r.t. that threshold. we plotted accuracy-threshold curves showing the
percentage of pose accuracy variation with a threshold in a range of 0 mm to 20
mm. we can see in fig. 3 that a 80.23% pose accuracy was reached within the 3
mm-3 deg threshold for all cases. this accuracy increases to 95.45% with a 5
mm-5 deg threshold.
we chose to follow a leave-one-texture-out cross-validation strategy to validate
our model. this strategy seemed the most adequate to prevent over-fitting on the
textures. we measured the add errors of our model for each case and report the
results in we observed a variance in the add error that depends on which texture
is left out. this supports the need for varying textures to improve the pose
estimation. however, the errors remain low, with a 2.01 ± 0.58 mm average add
error, over all cases. the average add error per case (over all left-out
textures) is reported in table 1. we measured the impact of the number of
textures on the pose accuracy by progressively adding new textures to the
training set, starting from 3 to 12 textures, while leaving 3 textures out for
validation. we kept the size of the training set constant to not introduce size
biases. shows that increasing the number and variation of textures improved
model performances. test and comparison on clinical images. we compared our
method (ours) with segmentation-based methods (probseg) and (binseg) [7]. these
methods use learning-based models to extract binary images and probability maps
of cortical vessels to drive the registration. we report in table 1 the
distances between the ground truth and estimated poses. our method outperformed
probseg and binseg with an average adm error of 3.26 ± 1.04 mm compared to 4.13
± 0.70 mm and 8.67 ± 2.84 mm, respectively. our errors remain below clinically
measured neuronavigation errors reported in [4], in which a 5.26 ± 0.75 mm
average initial registration error was measured in 15 craniotomy cases using
intraoperative ultrasound. our method outperformed probseg in 5 cases out of 6
and binseg in all cases and remained within the clinically measured errors
without the need to segment cortical vessels or select landmarks from the
intraoperative image. our method also showed fast intraoperative computation
times. it required an average of only 45 ms to predict the pose (tested on
research code on a laptop with nvidia geforce gtx 1070 8 gb without any specific
optimization), suggesting a potential use for real-time temporal tracking.figure
5 shows our results as augmented reality views with bounding boxes and overlaid
meshes. our method produced visually consistent alignments for all 6 clinical
cases without the need for initial registration. because our current method does
not account for brain-shift deformation, our method produced some misalignment
errors. however, in all cases, our predictions are similar to the ground truth.
clinical feasibility. we have shown that our method is clinically viable. our
experiments using clinical data showed that our method provides accurate
registration without manual intervention, that it is computationally efficient,
and it is invariant to the visual appearance of the cortex. our method does not
require intraoperative 3d imaging such as intraoperative mri or ultrasound,
which require expensive equipment and are disruptive during surgery. training
patient-specific models from preoperative imaging transfers computational tasks
to the preoperative stage so that patient-to-image registration can be performed
in near real-time from live images acquired from a surgical
microscope.limitations. the method presented in this paper is limited to 6-dof
pose estimation and does not account for deformation of the brain due to changes
in head position, fluid loss, or tumor resection and assumes a known focal
length. in the future, we will expand our method to model non-rigid deformations
of the 3d mesh and to accommodate expected changes in zoom and focal depth
during surgery. we will also explore how texture variability can be controlled
and adapted to the observed image to improve model accuracy.
we introduced expected appearances, a novel learning-based method for
intraoperative patient-to-image registration that uses synthesized expected
images of the operative field to register preoperative scans with intraoperative
views through the surgical microscope. we demonstrated state-ofthe-art,
real-time performance on challenging neurosurgical images using our method. our
method could be used to improve accuracy in neuronavigation and in image-guided
surgery in general.
radiotherapy (rt) has proven effective and efficient in treating cancer
patients. however, its application depends on treatment planning involving
target lesion and radiosensitive organs-at-risk (oar) segmentation. this is
performed to guide radiation to the target and to spare oar from inappropriate
irradiation. hence, this manual segmentation step is very time-consuming and
must be performed accurately and, more importantly, must be patient-safe.
studies have shown that the manual segmentation task accounts for over 40% of
the treatment planning duration [7] and, in addition, it is also error-prone due
to expert-dependent variations [2,24]. hence, deep learning-based (dl)
segmentation is essential for reducing time-to-treatment, yielding more
consistent results, and ensuring resource-efficient clinical workflows.nowadays,
training of dl segmentation models is predominantly based on loss functions
defined by geometry-based (e.g., softdice loss [15]), distributionbased
objectives (e.g., cross-entropy), or a combination thereof [13]. the general
strategy has been to design loss functions that match their evaluation
counterpart. nonetheless, recent studies have reported general pitfalls of these
metrics [4,19] as well as a low correlation with end-clinical objectives
[11,18,22,23]. furthermore, from a robustness point of view, models trained with
these loss functions have been shown to be more prone to generalization issues.
specifically, the dice loss, allegedly the most popular segmentation loss
function, has been shown to have a tendency to yield overconfident trained
models and lack robustness in out-of-distribution scenarios [5,14]. these
studies have also reported results favoring distribution-matching losses, such
as the cross-entropy being a strictly proper scoring rule [6], providing
better-calibrated predictions and uncertainty estimates. in the field of rt
planning for brain tumor patients, the recent study of [17] shows that current
dl-based segmentation algorithms for target structures carry a significant
chance of producing false positive outliers, which can have a considerable
negative effect on applied radiation dose, and ultimately, they may impact
treatment effectiveness. in rt planning, the final objective is to produce the
best possible radiation plan that jointly targets the lesion and spares healthy
tissues and oars. therefore, we postulate that training dl-based segmentation
models for rt planning should consider this clinical objective.in this paper, we
propose an end-to-end training loss function for dl-based segmentation models
that considers dosimetric effects as a clinically-driven learning objective. our
contributions are: (i) a dosimetry-aware training loss function for dl
segmentation models, which (ii) yields improved model robustness, and (iii)
leads to improved and safer dosimetry maps. we present results on a clinical
dataset comprising fifty post-operative glioblastoma (gbm) patients. in
addition, we report results comparing the proposed loss function, called
dose-segmentation loss (doselo), with models trained with a combination of
binary cross-entropy (bce) and softdice loss functions.
figure 1 describes the general idea of the proposed doselo. a segmentation model
(u-net [20]) is trained to output target segmentation predictions for the gross
tumor volume (gtv) based on patient mri sequences. predicted segmentations and
their corresponding ground-truth (gt) are fed into a dose predictor model, which
outputs corresponding dose predictions (denoted as d p and d p in fig. 1). a
pixel-wise mean squared error between both dose predictions is then a
segmentation model (u-net [20]) is trained to output target segmentation
predictions ( st ) for the gross tumor volume (gtv) based on patient mri
sequences imr. predicted ( st ) and ground-truth segmentations (st ) are fed
into the dose predictor model along with the ct-image (ict ), and oar
segmentation (sor). the dose predictor outputs corresponding dose predictions dp
and dp . a pixel-wise mean squared error between both dose predictions is
calculated, and combined with the binary crossentropy (bce) loss to form the
final loss, l total = lbce + λldsl. calculated and combined with the bce loss to
form the final loss. in the next sections we describe the adopted dose
prediction model [9,12], and the proposed doselo.
recent dl methods based on cascaded u-nets have demonstrated the feasibility of
generating accurate dose distribution predictions from segmentation masks,
approximating analytical dose maps generated by rt treatment planning systems
[12]. originally proposed for head and neck cancer [12], this approach has been
recently extended for brain tumor patients [9] with levels of prediction error
below 2.5 gy, which is less than 5% of the prescribed dose. this good level of
performance, along with its ability to yield near-instant dose predictions,
enables us to create a training pipeline that guides learned features to be
dose-aware.following [12], the dose predictor model consists of a cascaded u-net
(i.e., the input to the second u-net is the output of the first concatenated
with the input to the first u-net) trained on segmentation masks, ct images, and
reference dose maps. the model's input is a normalized ct volume and
segmentation masks for target volume and oars. as output, it predicts a
continuous-valued dose map of the same dimension as the input. the model is
trained via deep supervision as a linear combination of l2-losses from the
outputs of each u-net in the cascade. we refer the reader to [9,12] for further
implementation details. we remark that the dose predictor model was also trained
with data augmentation, so imperfect segmentation masks and corresponding dose
plans are included. this allows us in this study to use the dose predictor to
model the interplay between segmentation variability and dosimetric
changes.formally, the dose prediction model m d receives as inputs:
segmentations masks for the gtv s t ∈ z w ×h and the oars s or ∈ z w ×h , the ct
image (used for tissue attenuation calculation purposes in rt) i ct ∈ r w ×h ,
and outputs m d (s t , s or , i ct ) → d p ∈ r w ×h , a predicted dose map where
each pixel value in d corresponds to the local predicted dose in gy. due to the
limited data availability, we present results using 2d-based models but remark
that their extension to 3d is straightforward. working in 2d is also feasible
from an rt point of view because the dose predictor is based on co-planar
volumetric modulated arc therapy (vmat) planning, commonly used in this clinical
scenario.
during the training of the segmentation model, we used the dose predictor model
to generate pairs of dose predictions for the model-generated segmentations and
the gt segmentations. the difference between these two predicted dose maps is
used to guide the segmentation model. the intuition behind this is to guide the
segmentation model to yield segmentation results being dosimetrically consistent
with the dose maps generated via the corresponding gt segmentations. to guide
the training process with dosimetry information stemming from segmentation
variations, we propose to use the mean squared error (mse) between dose
predictions for the gt segmentation (s t ) and the predicted segmentation ( s t
), and construct the following dose-segmentation loss,where d i p and d i p
denote pixel-wise dose predictions. the final loss is then,where λ is a
hyperparameter to weigh the contributions of each loss term. we remark that
during training we use standard data augmentations including spatial
transformations, which are also subjected to dose predictions, so the model is
informed about relevant segmentation variations producing dosimetry changes.
we divide the descriptions of the two separate datasets used for the dose
prediction and segmentation models.
the dose prediction model was trained on an in-house dataset comprising a total
of 50 subjects diagnosed with post-operative gbm. this includes ct imaging data,
segmentation masks of 13 oars, and the gtv. gtvs were defined according to the
estro-acrop guidelines [16]. the oars were contoured by one radiotherapist
according to [21] and verified by mutual consensus of three experienced
radiation oncology experts. each subject had a reference dose map, calculated
using a standardized clinical protocol with eclipse (varian medical systems
inc., palo alto, usa). this reference was generated on basis of a double arc
co-planar vmat plan to deliver 30 times 2 gy while maximally sparing oars. we
divided the dataset into training (35 cases), validation (5 cases), and testing
(10 cases). we refer the reader to [9] for further details.segmentation models:
to develop and test the proposed approach, we employed a separate in-house
dataset (i.e., different cases than those used to train the dose predictor
model) of 50 cases from post-operative gmb patients receiving standard rt
treatment. we divided the dataset into training (35 cases), validation (5
cases), and testing (10 cases). all cases comprise a planning ct registered to
the standard mri images (t1-post-contrast (gd), t1-weighted, t2-weighted,
flair), and gt segmentations containing oars as well as the gtv. we note that
for this first study, we decided to keep the dose prediction model fixed during
the training of the segmentation model for a simpler presentation of the concept
and modular pipeline. hence, only the parameters of the segmentation model are
updated.
we employed the same u-net [20] architecture for all trained segmentation
models, with the same training parameters but two different loss functions, to
allow for a fair comparison. as a strong comparison baseline, we used a
combo-loss formed by bce plus softdice, which is also used by nnunet and
recommended by its authors [8]. this combo-loss has also been reported as an
effective one [13]. for each loss function, we computed a five-fold
cross-validation. our method1 was implemented in pytorch 1.13 using adam
optimizer [10] with β 1 = 0.9, β 2 = 0.999, batch normalization, dropout set at
0.2, learning rate set at 10 -4 , 2 • 10 4 update iterations, and a batch size
of 16. the architecture and trained parameters were kept constant across
compared models. training and testing were performed on an nvidia titan x gpu
with 12 gb ram. the input image size is 256 × 256 pixels with an isotropic
spacing of 1 mm.
to evaluate the proposed doselo, we computed dose maps for each test case using
a standardized clinical protocol with eclipse (varian medical systems inc., palo
alto, usa). we calculated dose maps for segmentations using the stateof-the-art
bce+softdice and the proposed doselo. for each obtained dose map, we computed
the dose score [12], which is the mean absolute error between the reference dose
map (d st ) and the dose map derived from the corresponding segmentation result
(d st , where s t ∈ {bce+softdice, doselo}), and set it relative to the
reference dose map (d st ) (see eq. 5).
although it has been shown that geometric-based segmentation metrics poorly
correlate with the clinical end-goal in rt [4,11,18,23], we report in
supplementary material dice and hausdorff summary statistics as well
(supplementary table 3). we nonetheless reemphasize our objective to move away
from such proxy metrics for rt purposes and promote the use of more
clinically-relevant ones.
figure 2 shows results on the test set, sorted by their dosimetric impact. we
found an overall reduction of the relative mean absolute error (rmae) with
respect to the reference dose maps, from 0.449 ± 0.545, obtained via the
bce+softdice combo-loss, to 0.258 ± 0.201 for the proposed doselo (i.e., an
effective 42.5% reduction with λ = 1). this significant dose error reduction
shows the ability of the proposed approach to yield segmentation results in
better agreement with dose maps obtained using gt segmentations than those
obtained using the state-of-the-art bce+softdice combo-loss.table 1 shows
results for the first and most significant four cases from a rt point of view
(due to space limitations, all other cases are shown in supplementary material).
we observe the ability of the proposed approach to significantly reduce
outliers, generating a negative dosimetry impact on the dose fig. 2. relative
mean absolute dose errors/differences (rmae) between the reference dose map and
dose maps obtained using the predicted segmentations. lower is better. across
all tested cases and folds we observe a large rmae reduction for dose maps using
the proposed doselo (average rmae reduction of 42.5%).maps. we analyzed case
number 3, 4, and 5 from fig. 2 for which the standard bce+softdice was slightly
better than the proposed doselo. for case no. 3 the tumor presents a non-convex
shape alongside the skull's parietal lobe, which was not adequately modeled by
the training dataset used to train the segmentation models. indeed, we remark
that both models failed to yield acceptable segmentation quality in this area.
in case no. 4, both models failed to segment the diffuse tumor area alongside
the skull; however, as shown in fig. 2-case no. 4, the standard bce+softdice
model would yield a centrally located radiation dose, with strong negative
clinical impact to the patient. case no. 5 (shown in supplementary material) is
an interesting case called butterfly gbm, which is a rare type of gbm (around 2%
of all gbm cases [3]), characterized by bihemispheric involvement and invasion
of the corpus callosum. in this case, the training data also lacked
characterization for such cases. despite this limitation, we observed favorable
dose distributions with the proposed method.although we are aware that classical
segmentation metrics poorly correlate with dosimetric effects [18], we report
that the proposed method is more robust than the baseline bce+softdice loss
function, which yields outliers with hausdorff distances: 64.06 ± 29.84 mm vs
28.68 ± 22.25 mm (-55.2% reduction) for the proposed approach. as pointed out by
[17], segmentation outliers can have a detrimental effect on rt planning. we
also remark that the range of hd values is in range with values reported by
models trained using much more training data (see [1]), alluding to the
possibility that the problem of robustness might not be directly solvable with
more data. dice coefficients did not deviate significantly between the baseline
and the doselo models (dsc: 0.713 ± 0.203 (baseline) vs. 0.697 ± 0.216
(doselo)).table 1. comparison of dose maps and their absolute differences to the
reference dose maps (bce+softdice (bce+sd), and the proposed doselo). it can be
seen that doselo yields improved dose maps, which are in better agreement with
the reference dose maps (dose map color scale: 0 (blue) -70gy (red)).
the ultimate goal of dl-based segmentation for rt planning is to provide
reliable and patient-safe segmentations for dosimetric planning and optimally
targeting tumor lesions and sparing of healthy tissues. however, current loss
functions used to train models for rt purposes rely solely on geometric
considerations that have been shown to correlate poorly with dosimetric
objectives [11,18,22,23]. in this paper, we propose a novel dosimetry-aware
training loss function, called doselo, to effectively guide the training of
segmentation models toward dosimetric-compliant segmentation results for rt
purposes. the proposed doselo uses a fast-dose map prediction model, enabling
model guidance on how dosimetry is affected by segmentation variations. we merge
this information into a simple yet effective loss function that can be combined
with existing ones. these first results on a dataset of post-operative gbm
patients show the ability of the proposed doselo to deliver improved
dosimetric-compliant segmentation results. future work includes extending our
database of gbm cases and to other anatomies, as well as verifying potential
improvements when cotraining the segmentation and dose predictor models, and
jointly segmenting gtvs and oars. with this study, we hope to promote more
research toward clinically-relevant dl training loss functions.
residual tumor in the cavity after head and neck cancer (hnc) surgery is a
significant concern as it increases the risk of cancer recurrence and can
negatively impact the patient's prognosis [1]. hnc comprises the third highest
positive surgical margins (psm) rate across all oncology fields [2]. achieving
clear margins can be challenging in some cases, particularly in tumors with
involved deep margins [3,4].during transoral robotic surgery (tors), surgeons
may assess the surgical margin via visual inspection, palpation of the excised
specimen and intraoperative frozen sections analysis (ifsa) [5]. in the surgical
cavity, surgeons visually inspect for residual tumors and use specimen driven or
defect-driven frozen section analysis to identify any residual tumor [6,7]. the
latter involves slicing a small portion of the tissue at the edge of the cavity
and performing a frozen section analysis. these approaches are error-prone and
can result in psms and a higher risk of cancer recurrence [7]. in an effort to
improve these results, recent studies reported the use of exogenous fluorescent
markers [8] and wide-field optical coherence tomography [9] to inspect psms in
the excised specimen. while promising, each modality presents certain
limitations (e.g., time-consuming analysis, administration of a contrast agent,
controlled lighting environment), which has limited their clinical adoption
[10,11].label-free mesoscopic fluorescence lifetime imaging (flim) has been
demonstrated as an intraoperative imaging guidance technique with high
classification performance (auc = 0.94) in identifying in vivo tumor margins at
the epithelial surface prior to tumor excision [12]. flim can generate optical
contrast using autofluorescence derived from tissue fluorophores such as
collagen, nadh, and fad. due to the sensitivity of these fluorophores to their
microenvironment, the presence of tumor changes their emission properties (i.e.,
intensity and lifetime characteristics) relative to healthy tissue, thereby
enabling the optical detection of cancer [13].however, ability of label-free
flim to identify residual tumors in vivo in the surgical cavity (deep margins)
has not been reported. one significant challenge in developing a flim-based
classifier to detect tumor in the surgical cavity is the presence of highly
imbalanced labels.surgeons aim to perform an en bloc resection, removing the
entire tumor and a margin of healthy tissue around it to ensure complete
excision. therefore, in most cases, only healthy tissue in left in the cavity.
to address the technical challenge of highly imbalanced label distribution and
the need for intraoperative real-time cavity imaging, we developed an
intraoperative flim guidance model to identify residual tumors by classifying
residual cancer as anomalies. our proposed approach identified all patients with
psm. in contrast, the ifsa reporting a sensitivity of 0.5 [6,7].
as illustrated in fig. 1, the proposed method uses a clinically-compatible flim
system coupled to the da vinci sp transoral robotic surgical platform to scan
the surgical cavity in vivo and acquire flim data. we used the cumulative
distribution transform (cdt) of the fluorescence decay curves extracted from the
flim data as the input feature. the novelty detection model classified flim
points closer to the healthy distribution as healthy and further from the
healthy distribution as a residual tumor. we implemented the image guidance by
augmenting the classification map to the surgical view using the predictor
output and point locations of the scan.
this study used a multispectral fluorescence lifetime imaging (flim) device to
acquire data [14]. the flim device features a 355 nm uv laser for fluorescence
excitation, which is pulsed at a 480 hz repetition rate. a 365 µm multimode
optical fiber (0.22 na) delivers excitation light to tissue and relays the
corresponding fluorescence signal to a set of dichroic mirrors and bandpass
filters to spectrally resolve the autofluorescence. three variable gain uv
enhanced si apd modules with integrated trans-impedance amplifiers receive the
autofluorescence, which is spectrally resolved as follows: (1) 390/40 nm
attributed to collagen autofluorescence, (2) 470/28 nm to nadh, and (3) 542/50
nm to fad. the resulting autofluorescence waveform measurements for each channel
are averaged four times, thus with a 480 hz excitation rate, resulting in 120
averaged measurements per second [15].the flim device includes a 440 nm
continuous wave laser that serves as an aiming beam; this aiming beam enables
real-time visualization of the locations where fluorescence (point measurements)
is collected by generating visible blue illumination at the location where data
is acquired. segmentation of the 'aiming beam' allows for flim data points to be
localized as pixel coordinates within a surgical white light image (see fig. 1).
localization of these coordinates is essential to link the regions where data is
obtained to histopathology, which is used as the ground truth to link flim
optical data to pathology status [16]. flim data was acquired using the da vinci
sp robotic surgical platform. as part of the approved protocol for this study,
the surgeon performed in vivo flim scan on the tumor epithelial surface and the
surrounding uninvolved benign tissue. upon completing the scan, the surgeon
proceeded with en bloc excision of the tissue suspected of cancer. an ex vivo
flim scan was then performed on the surgically excised specimen. finally, the
patient's surgical cavity was scanned to check for residual tumor.
the research was performed under the approval of the uc davis institutional
review board (irb) and with the patient's informed consent. all patients were
anesthetized, intubated, and prepared for surgery as part of the standard of
care. n = 22 patients are represented in this study, comprising hnc in the
palatine tonsil (n = 15) and the base of the tongue (n = 7). for each patient,
the operating surgeon conducted an en bloc surgical tumor resection procedure
(achieved by tors-electrocautery instruments), and the resulting excised
specimen was sent to a surgical pathology room for grossing. the tissue specimen
was serially sectioned to generate tissue slices, which were then
formalin-fixed, paraffin-embedded, sectioned, and stained to create hematoxylin
& eosin (h&e) slides for pathologist interpretation (see fig. 1).after the
surgical excision of the tumor, an in vivo flim scan of approximately 90 s was
conducted within the patient's surgical cavity, where the tumor was excised. to
validate optical measurements to pathology labels (e.g., benign tissue vs.
residual tumor), pathology labels from the excision margins were digitally
annotated by a pathologist on each h&e section. the aggregate of h&e sections
was correspondingly labeled on the ex vivo specimen at the cut lines where the
tissue specimen was serially sectioned.thereafter, the labels were spatially
registered in vivo within the surgical cavity. this process enables the direct
validation of flim measurements to the pathology status of the electrocauterized
surgical margins (see table 1).
the raw flim waveform contains background noise, instrument artifacts, and other
types of interference, which need to be carefully processed and analyzed to
extract meaningful information (i.e., the fluorescence signal decay
characteristics). to account for background noise, the background signal
acquired at the beginning of each clinical case was subtracted from the measured
raw flim waveform. to retrieve the fluorescence function, we used a
non-parametric model based on a laguerre expansion polynomials and a constrained
least-square deconvolution with the instrument impulse response function as
previously described [17]. in addition, an snr threshold of ≥50 db was applied
as a filtering criterion to select flim points with good signal quality.
the state-of-the-art novelty detection models were comprehensively reviewed in
the literature [18,19]. due to its robust performance, we chose the generalized
one-class discriminative subspaces (gods) classification model [20] to classify
healthy flim points from the residual tumor. the model trained only on the
healthy flim points and use a semi-supervised technique to classify residual
tumor from healthy. the gods is a pairwise complimentary classifier defined by
two separating hyperplanes to minimize the distance between the two classifiers,
limiting the healthy flim data within the smallest volume and maximizing the
margin between the hyperplanes and the data, thereby avoiding overfitting while
improving classification robustness. the first hyperplane (w 1 , b 1 ) projects
most of the healthy flim points to the positive half of the space, whereas the
second hyperplane (w 2 , b 2 ) projects most of the flim points in the negative
half.minwhere w 1 , w 2 are the orthonormal frames, minis the stiefel manifold,
η is the sensitivity margin, and was set η = 0.4 for our experiments. ν denote a
penalty factor on these soft constraints, and b is the biases. x i denotes the
training set containing cdt of the concatenated flim decay curve across channels
1-3 along the time axis. the cdt of the concatenated decay curves is computed as
follows: normalize the decay curves to 0-1. compute and normalize the cumulative
distribution function (cdf). transforming the normalized cdf into the cumulative
distribution transform by taking the inverse cumulative distribution function of
the normalized cdf [21].
the novelty detection model used for detecting residual cancer is evaluated at
the pointmeasurement level to assess the diagnostic capability of the method
over an entire tissue surface. the evaluation followed a leave-one-patient-out
cross-validation approach. the study further compared gods with two other
novelty detection models: robust covariance and, one-class support vector
machine (oc-svm) [22]. novelty detection model solely used healthy labels from
the in vivo cavity scan for training. the testing data contained both healthy
and residual cancer labels. we used grid search to optimize the hyper-parameters
and features used in each model and are tabulated in the supplementary section
table s1. the sensitivity, specificity, and accuracy were used as evaluation
metrics to assess the performance of classification models in the context of the
study.results of a binary classification model using svm are also shown in the
supplementary section table s2.
the classifier augmentation depends on three independent processing steps:
aiming beam localization, motion correction, and interpolation of the point
measurements. a detailed description of implementing the augmentation process is
discussed in [23]. the interpolation consists of fitting a disk to the segmented
aiming beam pixel location for each point measurement and applying a color map
(e.g., green: healthy and red: cancer) for each point prediction. individual
pixels from overlapping disks are averaged to produce the overall classification
map and augmented to the surgical field as a transparent overlay.
table 2 tabulates the classification performance comparison of novelty detection
models for classifying residual cancer vs. healthy on in vivo flim scans in the
cavity. three novelty detection models were evaluated, and all three models
could identify the presence of residual tumors in the cavity for the three
patients. however, the extent of the tumor classification over the entire tissue
surface varied among the models. the gods reported the best classification
performance with an average sensitivity of 0.75 ± 0.02 (see fig. 2). the lower
standard deviation indicates that the model generalizes well. the oc-svm and
robust covariance reported a high standard deviation, indicating that the
performance of the classification model is inconsistent across different
patients. the model's ability to correctly identify negative instances is
essential to its reliability. the gods model reported the highest mean
specificity of 0.78 ± 0.14 and the lowest standard deviation. the robust
covariance model reported the lowest specificity, classifying larger portions of
healthy tissue in the cavity as a residual tumor; indicating that the model did
not generalize well to the healthy labels. we also observed that changing the
hyper-parameter, such as the anomaly factor, biased the model toward a single
class indicating overfitting (see supplementary section fig. s1).the gods uses
two separating hyperplanes to minimize the distance between the two classifiers
by learning a low-dimensional subspace containing flim data properties of
healthy labels. residual tumor labels are detected by calculating the distance
between the projected data points and the learned subspace. points that are far
from the subspace are classified as residual tumors. we observed that the gods
with the flim decay curves in the cdt space achieve the best classification
performance compared to other novelty detection models with a mean accuracy of
0.76 ± 0.02. this is mainly due to the robustness of the model, the ability to
handle high-dimensional data, and the contrast in the flim decay curves. the
contrast in the flim decay curves was further improved in the cdt space by
transforming the flim decay curves to a normalized scale and improving linear
separability.
curent study demonstrates that label-free flim parameters-based classification
model, using a novelty detection aproach, enables identification of residual
tumors in the surgical cavity. the proposed model can resolve residual tumor at
the point-measurement level over a tissue surface. the model reported low
point-level false negatives and positives. moreover, the current approach
correctly identified all patients with psms (see fig. 2). this enhances surgical
precision for tors procedures otherwise limited to visual inspection of the
cavity, palpation of the excised specimen, and ifsa. the flimbased
classification model could help guide the surgical team in real-time, providing
information on the location and extent of cancerous tissue.in context to the
standard of care, the proposed residual tumor detection model exhibits high
patient-level sensitivity (sensitivity = 1) in detecting patients with psms. in
contrast, defect-driven ifsa reports a patient-level sensitivity of 0.5 [6,7].
our approach exhibits a low patient-level specificity compared to ifsa. surgeons
aim to achieve negative margins, meaning the absence of cancer cells at the
edges of the tissue removed during surgery. the finding of positive margins from
final histology would result in additional surgical resection, potentially
impacting the quality of life. combining the proposed approach and ifsa could
lead to an image-guided frozen section analysis to help surgeons achieve
negative margins in a more precise manner. therefore, completely resecting
cancerous tissue and improving patient outcomes.the false positive predictions
from the classification model presented two trends: false positives in an
isolated region and false positives spreading across a larger region. isolated
false positives are often caused by the noise of the flim system and are
accounted for by the interpolation approach used for the classifier augmentation
(refer to supplementary section fig. s2). on the other hand, false positives
spreading across a larger region are much more complex to interpret. one insight
is that the electrocautery effects on the tissues in the cavity may have
influenced them [24]. according to jackson's burn wound model, the thermal
effects caused by electrocautery vary with the different burnt zones. we
observed a correlation between a larger spread of false positive predictions
associated with a zone of coagulation to a zone of hyperemia.the novelty
detection model generalizes to the healthy labels and considers data falling off
the healthy distribution as residual cancer. the flim properties associated with
the healthy labels in the cavity are heterogeneous due to the electrocautery
effects. electrocautery effects are mainly thermal and can be observed by the
levels of charring in the tissue. refining the training labels based on the
levels of charring could lead to a more homogeneous representation of the
training set and result in an improved classification model with better
generalization.
this study demonstrates a novel flim-based classification method to identify
residual cancer in the surgical cavity of the oropharynx. the preliminary
results underscore the significance of the proposed method in detecting psms.
the model will be validated on a larger patient cohort in future work and
address the limitations of the point-level false positive and negative
predictions. this work may enhance surgical precision for tors procedures as an
adjunctive technique in combination with ifsa.
resection of early-stage brain tumors can greatly reduce the mortality rate of
patients. during the surgery, brain tissue deformation (called brain shift) can
occur due to various causes, such as gravity, drug administration, and pressure
change after craniotomy. while modern magnetic resonance imaging (mri)
techniques can provide rich anatomical and physiological information with
various contrasts (e.g., fmri) for more elaborate pre-surgical planning,
intra-operative mri that can track brain shift requires a complex setup and is
costly. in contrast, intra-operative ultrasound (ius) has gained popularity for
real-time imaging during surgery to monitor tissue deformation and surgical
tools because of its lower cost, portability, and flexibility [1]. accurate and
robust mri-ius registration techniques [2] can greatly enhance the value of ius
for updating pre-surgical plans and guiding the interpretation of ius, which has
an unintuitive contrast and non-standard orientations. this can greatly enhance
the safety and outcomes of the surgical procedure by allowing maximum brain
tumor removal while avoiding eloquent regions [3]. however, as the true
underlying tissue deformation is unknown due to the 3d nature of the surgical
data and the time constraint, real-time manual inspection of mri-ius
registration results is challenging and error-prone, especially for
precision-sensitive neurosurgery. therefore, algorithms that can detect and
quantify unreliable inter-modal medical image registration results are highly
beneficial.recently, automatic quality assessment for medical image registration
has attracted increasing attention [4] from the domains of big medical data
analysis and surgical interventions. with high efficiency, machine, and deep
learning techniques have been proposed to allow automatic grading and dense
estimation of medical image registration errors. early endeavors on this topic
primarily relied on hand-crafted features, including information theory-based
metrics [5][6][7][8][9][10]. more recently, deep learning (dl) techniques that
learn task-specific features have also been adopted in automatic evaluation of
medical image registration, with a primary focus on intra-contrast/modal
applications, including ct [9,10] and mri [11]. unfortunately, so far, error
grading and estimation in inter-contrast/modal registration have rarely been
explored, despite the particular demand in surgical applications. in this
direction, bierbrier et al. [12] made the first attempt using simulated ius from
mri to train 3d convolutional neural networks (cnns) to perform dense error
regression for mri-ius registration in brain tumor resection. although their
algorithm performed well in simulated cases, the results on real clinical scans
still required improvements. in this paper, we propose a novel 3d cnn to perform
patch-wise error estimation for mri-ius registration in neurosurgery, by using
focal modulation [13], a recent alternative dl technique to self-attention [14]
for encoding contextual information, and uncertainty estimation. we call our
method focalerrornet, which has three main novelties. first, we adapted the
focal modulation network [13] from 2d to 3d and employed the technique in
registration error assessment for the first time. second, we incorporated
uncertainty estimation using monte carlo (mc) dropouts [15] to offer assurance
for error regression. lastly, we developed and thoroughly evaluated our
technique against a recent baseline model [12] using real clinical data and
showed excellent results.
for methodological development and assessment, we used the resect
(retro-spective evaluation of cerebral tumors) dataset [16], which has
pre-operative mri, and ius scans at different surgical stages from 23 subjects
who underwent low-grade glioma resection surgeries. as it is still challenging
to model ius scans with tissue resection, we took 22 cases with t2flair mri that
better depicts tumor boundaries and ius acquired before resection. an example of
an mri-ius pair from a patient is shown in fig. 1. we hypothesized that directly
leveraging clinical ius could help learn more realistic image features with
potentially better outcomes in clinical applications than with simulated
contrasts [9,12]. however, since the true brain shift model is impossible to
obtain, we followed the strategy of creating silver ground truths for image
alignment [9,12], upon which simulated misalignment is augmented in the ius to
build and test our dl model. to create the silver registration ground truths, we
used the homologous landmarks between mri and ius in the resect dataset to
perform landmark-based 3d b-spline nonlinear registration to register ius to the
corresponding mri for all 22 cases. to tackle the limited field of view (fov) in
ius, we cropped the t2flair mri to the same fov of the ius, which was resampled
to a 0.5 × 0.5 × 0.5 mm 3 resolution. to perform spatial misalignment
augmentation, we continued to leverage 3d b-spline transformation, similar to
earlier reports on the same topic [10,12,17]. in short, b-spline transformation
can be modeled by a grid of regularly spaced control points and the associated
parameters to allow various levels of nonlinear deformation. while the spacing
of the control points determines the levels of details in local deformation
fields, the displacement parameters control the magnitude of the deformation. to
ensure that simulated registration errors are of different varieties and sizes,
we randomly selected the number of control points and the associated
displacements (in each 3d axis) with a maximum of 20 points and 30 mm,
respectively. note that the control point grid is isotropic, and the density is
arbitrarily determined per deformation in our case. each coregistered ius scan
was deformed ten times. after misalignment augmentation on the previously
co-registered ius, matching pairs of 3d image patches of size 33 × 33 × 33
voxels were taken from both the ius volume and the corresponding mri. as ius has
limited fov and may contain no anatomical features, to ensure that the patches
we extracted contain useful information (e.g. to avoid the dark background) in
ius, we focused on acquiring patches centered around the anatomical landmark
locations available through the resect database. since b-spline transformation
offers a displacement vector at each voxel of the ius volume, we directly
considered the norm of the vector as the simulated registration error at the
associated voxel. in our design, we determined the registration error of the
image patch pair as the mean of all voxel-wise errors within the ius patch.
finally, the image patch pairs, along with corresponding registration errors
were then fed to the proposed dl algorithm for training and validation.
we proposed a novel 3d neural network, named focalerrornet, based on the recent
focal modulation networks [13] that was originally proposed for 2d vision tasks
to estimate the registration error between mri and ius patches. with a similar
goal as the vision transformer (vit), the focal modulation network was designed
to model contextual information in images. it incorporates three main elements
to achieve the goal: 1) focal contextualization that comprises a stack of
depth-wise convolutional layers to account for long-to short-range dependencies,
2) gated aggregation to collect contexts into a modulator for individual query
tokens, and 3) element-wise affine transformation to inject the modulator into
the query. in the architecture of focalerrornet (see fig. 2), all layers contain
two focal modulator blocks, where two depth-wise convolutional layers focally
extract contexts around each voxel, selectively aggregate and inject them into
the query, and pass the information to the next block. we designed the
focalerrornet as a resnet-like variant of the focal modulation network to better
encode relevant features across the input image and ensure a better gradient
flow. finally, the information from the backbone was propagated to a multi-layer
perceptron (mlp) to regress registration errors, and two mc dropout layers were
added to the mlp to allow uncertainty quantification for the results.
for registration error regression in surgical applications, knowledge regarding
the reliability of the automated results is instrumental for the safety and
wellbeing of the patients. uncertainty estimation has gained popularity in
probing the trustworthiness and credence of dl algorithms. although the concept
has been widely applied in image segmentation and classification, it has not
been employed for registration error estimation, especially in the case of
multi-modal situations, such as mri-ius alignment. therefore, we incorporated
uncertainty estimation in our proposed focalerrornet. for each mri-ius patch
pair, 200 regression samples were collected by random sampling from mc dropouts
[15] at test time. while the final patch registration error was obtained as the
mean of all the samples, the sample standard deviation was used as the
uncertainty metric.
from the transformation augmentation, we acquired 3380 samples of mri-ius pairs.
for our experiments, we arbitrarily split the subjects into training,
validation, and test sets with the proportion of 60%, 20%, and 20%,
respectively. to prevent information leakage, we ensured that each patient was
included in only one of the split sets. for model training, we adopted the adam
optimization with a learning rate of 5 × 10 -5 and a batch size of 64. for the
loss function, we used mean squared error (mse) to minimize the difference
between the predicted mri-ius registration error and the ground truths.
furthermore, in addition to the transformation augmentation, we also included
additional data augmentation, including random noise addition and random image
flipping on training sets to mitigate overfitting and increase the model's
generalizability. to assess our proposed focalerrornet, we compared it against a
3d cnn [9,12] (see fig. 3) that was employed for medical image registration
error regression. the two dl models were trained with the same dataset and
procedure, and their prediction accuracies, measured as the absolute error
between the predicted and ground truths mis-registration on the test set were
compared with two-sided pairedsamples t-tests to confirm the superiority of the
proposed method, in addition to correlations between their estimated and ground
truth errors. to validate the proposed uncertainty estimation method, we
calculated the correlation between the uncertainty measure and absolute error of
focalerrornet, and the correlation between the uncertainty and mutual
information between mri and ius, which is often used to measure the information
overlap in multi-modal registration. finally, to test the robustness of the
focalerrornet, we acquired additional mri-ius patch pairs from the test
subjects, by introducing random linear shifts (the max displacement from
landmark locations is 10 voxels) from the selected locations in the original
set, and evaluated the dl model performance.
the accuracy comparison between the proposed focalerrornet and the baseline 3d
cnn [9,12] is shown in table 1. across all samples in the testing data, we
achieved an accuracy of 0.59 ± 0.57 mm, while the counterpart obtained a
prediction error of 1.69 ± 1.37 mm. with the t-test, our focalerrornet
outperformed the 3d cnn [9,12] (p < 1e-4). in addition, the correlations between
the predicted and ground truths errors are 0.82 (p < 1e-4) and 0.61 (p < 1e-3)
for focalerrornet and 3d cnn, respectively, further confirming the advantage of
the proposed technique. to allow a qualitative comparison, scatter plots for
predicted vs. ground truth errors of the two models are depicted in fig. 4a
and5a. at larger error levels, it is evident that the point clouds exhibit a
wider shape.
we obtained correlations of 0.70 (p < 1e-4) and 0.34 (p < 1e-4) between
estimated uncertainty and prediction error for focalerrornet and the baseline 3d
cnn, respectively. additionally, the uncertainty vs. mutual information
uncertainties was assessed at -0.67 (p < 1e-4) for our proposed method and -0.18
(p < 1e-3) for the baseline. to allow better visual comparisons, the associated
scatter plots are illustrated in fig. 4 and5. these metrics proved the validity
of our uncertainty measure and further confirmed the performance of
focalerrornet. note the scatter plots for uncertainty measure validation were
performed using value binning (with 20 values per bin) for each axis to better
reveal the trends of the metrics.
to examine the performance of our proposed method for image regions that contain
fewer potent anatomical features, we acquired additional image pairs from test
subjects, according to sect. 2.4. with the new test set, the prediction errors
for our method and the baseline model were 1.28 ± 0.99 mm and 2.49 ± 1.87 mm,
respectively. furthermore, the correlations between estimated and true error
were calculated at 0.41 for focalerrornet and 0.20 for the baseline. these
results supported the benefits of focal modulation in registration error
estimation. in this test, patches can contain large areas of zeros (image
content out of the scanning fov of the ius). the main reason for the observed
performance decline is due to the reduction in sufficient image features in ius.
however, despite these challenges, we saw an acceptable outcome from
focaler-rornet (absolute error = 1.28 mm or ∼1 voxel in clinical mris).
in image-guided interventions, there is an urgent need for automatic assessment
of image registration quality. multi-modal registration quality evaluation poses
major challenges due to three main factors. first, dissimilar contrasts between
images require more elaborate strategies to derive relevant features for error
assessment. second, unlike segmentation or classification, the ground truths of
registration errors are difficult to obtain. finally, compared with
classification, regression tasks tend to be more error-prone for deep learning
algorithms. to tackle these challenges, we employed 3d focal modulation with
depth-wise convolution to encode contextual information for the image pair.
compared with the vit and its variants, focal modulation allows a more
lightweight setup, which could be desirable for 3d data. although we admit that
residual errors still remain after landmark-based b-spline nonlinear alignment,
this approach has been adopted in different prior studies, considering the
residual landmark registration error is fairly low (mtre of 0.0008 ± 0.0010mm).
although simulated ultrasound has been used to provide a perfect alignment with
mris, the fidelity of the simulated results is still suboptimal, and this may
explain the underperformance of the previous technique in real clinical data
[12]. to ensure the performance of our focalerrornet, we opted to regress the
mean registration error of image patches than simplistic error grades or
voxel-wise error maps. we believe that this design choice offers a more stable
performance, which is supported by our validation. we adopted uncertainty
estimation in inter-modal registration error assessment for the first time.
while other techniques exist to provide model uncertainty [18], mc dropout is
more flexible for various dl models. furthermore, the use of standard deviation
as an uncertainty measurement maintains the same unit as the regressed errors,
thus making the interpretation more intuitive. from quantitative and qualitative
evaluations using correlation coefficients and scatter plots to assess the
association of uncertainty measures with the prediction errors and image
entropy, we confirmed the validity of the proposed uncertainty estimation
approach. for our focalerrornet, we achieved a prediction error of 0.59 ± 0.57
mm, which is on par with the image resolution (0.5 mm). additionally, the
standard deviation of our results is lower than the baseline model [12]. these
signify a robust performance of the focalerrornet. one limitation of our work
lies in the limited patient data, as public ius datasets are scarce, while the
settings and properties of us scanners can vary, potentially affecting the dl
model designs. therefore, we created random deformations for patch-wise error
estimation, and will further explore data-efficient approaches for registration
error assessment.
we proposed focalerrornet, a novel dl model for uncertainty-aware inter-modal
registration error estimation in ius-guided neurosurgery, leveraging the latest
focal modulation technique and mc dropout. with thorough assessments of the
accuracy and uncertainty measures, we have confirmed the performance of the
proposed method against a baseline model previously adopted for the same task.
as the first to introduce uncertainty measures and 3d focal modulation in
registration error evaluation, our work provides the first step for fast and
reliable feedback in inter-modal medical image registration to guide clinical
decisions in surgery. we plan to adapt the presented framework for other
inter-modal/contrast image registration applications in the future.
cancer remains a significant public health challenge worldwide, with a new
diagnosis occurring every two minutes in the uk (cancer research uk 1 ). surgery
is one of the main curative treatment options for cancer. however, despite
substantial advances in pre-operative imaging such as ct, mri, or pet/spect to
aid diagnosis, surgeons still rely on the sense of touch and naked eye to detect
cancerous tissues and disease metastases intra-operatively due to the lack of
reliable intraoperative visualization tools. in practice, imprecise
intraoperative cancer tissue detection and visualization results in missed
cancer or the unnecessary removal of healthy tissues, which leads to increased
costs and potential harm to the patient. there is a pressing need for more
reliable and accurate intraoperative visualization tools for minimally invasive
surgery (mis) to improve surgical outcomes and enhance patient care. a recent
miniaturized cancer detection probe (i.e., 'sensei r ' developed by lightpoint
medical ltd.) leverages the cancer-targeting ability of nuclear agents typically
used in nuclear imaging to more accurately identify cancer intraoperatively from
the emitted gamma signal (see fig. 1b) [6]. however, the use of this probe
presents a visualization challenge as the probe is non-imaging and is air-gapped
from the tissue, making it challenging for the surgeon to locate the
probe-sensing area on the tissue surface.it is crucial to accurately determine
the sensing area, with positive signal potentially indicating cancer or affected
lymph nodes. geometrically, the sensing area is defined as the intersection
point between the gamma probe axis and the tissue surface in 3d space, but
projected onto the 2d laparoscopic image. however, it is not trivial to
determine this using traditional methods due to poor textural definition of
tissues and lack of per-pixel ground truth depth data. similarly, it is also
challenging to acquire the probe pose during the surgery.problem redefinition.
in this study, in order to provide sensing area visualization ground truth, we
modified a non-functional 'sensei' probe by adding a miniaturized laser module
to clearly optically indicate the sensing area on the laparoscopic images -i.e.
the 'probe axis-surface intersection'. our system consists of four main
components: a customized stereo laparoscope system for capturing stereo images,
a rotation stage for automatic phantom movement, a shutter for illumination
control, and a daq-controlled switchable laser module (see fig. 1a). with this
setup, we aim to transform the sensing area localization problem from a
geometrical issue to a high-level content inference problem in 2d. it is
noteworthy that this remains a challenging task, as ultimately we need to infer
the probe axis-surface intersection without the aid of the laser module to
realistically simulate the use of the 'sensei' probe.
laparoscopic images play an important role in computer-assisted surgery and have
been used in several problems such as object detection [9], image segmentation
[23], depth estimation [20] or 3d reconstruction [13]. recently, supervised or
unsupervised depth estimation methods have been introduced [14]. ye et al. [22]
proposed a deep learning framework for surgical scene depth estimation in
self-supervised mode and achieved scalable data acquisition by incorporating a
differentiable spatial transformer and an autoencoder into their framework. a 3d
displacement module was explored in [21] and 3d geometric consistency was
utilized in [8] for self-supervised monocular depth estimation. tao et al. [19]
presented a spatiotemporal vision transformer-based method and a selfsupervised
generative adversarial network was introduced in [7] for depth estimation of
stereo laparoscopic images. recently, fully supervised methods were summarized
in [1] for depth estimation. however, acquiring per-pixel ground truth depth
data is challenging, especially for laparoscopic images, which makes it
difficult for large-scale supervised training [8].laparoscopic segmentation is
another important task in computer-assisted surgery as it allows for accurate
and efficient identification of instrument position, anatomical structures, and
pathological tissue. for instance, a unified framework for depth estimation and
surgical tool segmentation in laparoscopic images was proposed in [5], with
simultaneous depth estimation and segmentation map generation. in [12],
self-supervised depth estimation was utilized to regularize the semantic
segmentation in knee arthroscopy. marullo et al. [16] introduced a multi-task
convolutional neural network for event detection and semantic segmentation in
laparoscopic surgery. the dual swin transformer u-net was proposed in [11] to
enhance the medical image segmentation performance, which leveraged the
hierarchical swin transformer into both the encoder and the decoder of the
standard u-shaped architecture, benefiting from the self-attention computation
in swin transformer as well as the dual-scale encoding design.although the
intermediate depth information was not our final aim and can be bypassed, the 3d
surface information was necessary in the intersection point inference. resnet
[3] has been commonly used as the encoder to extract the image features and
geometric information of the scene. in particular, in [21], concatenated stereo
image pairs were used as inputs to achieve better results, and such stereo image
types are also typical in robot-assisted minimally invasive surgery with stereo
laparoscopes. hence, stereo image data was also adopted in this paper.if the
problem of inferring the intersection point is treated as a geometric problem,
both data collection and intra-operative registration would be difficult, which
inspired us to approach this problem differently. in practice, we utilize the
laser module to collect the ground truth of the intersection points when the
laser is on. we note that the standard illumination image from the laparoscopic
probe is also captured with the same setup when the laser module is on.
therefore, we can establish a dataset with an image pair (rgb image and laser
image) that shares the same intersection point ground truth with the laser image
(see fig. 2a and fig. 2b). the assumptions made are that the probe's 3d pose
when projected into the two 2d images is the observed 2d pose, and that the
intersection point is located on its axis. hence, we input these axes to the
network as another branch and randomly sampled points along them to represent
the probe.
to validate our proposed solution for the newly formulated problem, we acquired
and publicly released two new datasets. in this section, we introduce the
hardware and software design that was used to achieve our final goal, while fig.
2 shows a sample from our dataset. data collection. two miniaturized,
high-resolution cameras were coupled onto a stereo laparoscope using a
custom-designed connector. the accompanying api allowed for automatic image
acquisition, exposure time adjustment, and white balancing. an electrically
controllable shutter was incorporated into the standard laparoscopic
illumination path. to indicate the probe axis-surface intersection, we
incorporated a daq controlled cylindrical miniature laser module into a 'sensei'
probe shell so that the adapted tool was visually identical to the real probe.
the laser module emitted a red laser beam (wavelength 650 nm) that was visible
as a red spot on the tissue surface. we acquired the dataset on a silicone
tissue phantom which was 30 × 21 × 8 cm and was rendered with tissue color
manually by hand to be visually realistic. the phantom was placed on a rotation
stage that stepped 10 times per revolution to provide views separated by a
36-degree angle. at each position, stereo rgb images were captured i) under
normal laparoscopic illumination with the laser off; ii) with the laparoscopic
light blocked and the laser on; and iii) with the laparoscopic light blocked and
the laser off. subtraction of the images with laser on and off readily allowed
segmentation of the laser area and calculation of its central point, i.e. the
ground truth probe axis-surface intersection.all data acquisition and devices
were controlled by python and labview programs, and complete data sets of the
above images were collected on visually realistic phantoms for multiple probe
and laparoscope positions. this provided 10 tissue surface profiles for a
specific camera-probe pose, repeated for 120 different camera-probe poses,
mimicking how the probe may be used in practice. therefore, our first newly
acquired dataset, named jerry, contains 1200 sets of images. since it is
important to report errors in 3d and in millimeters, we recorded another dataset
similar to jerry but also including ground truth depth map for all frames by
using structured-lighting system [8]-namely the coffbee dataset.these datasets
have multiple uses such as:-intersection point detection: detecting intersection
points is an important problem that can bring accurate surgical cancer
visualization. we believe this is an under-investigated problem in surgical
vision. -depth estimation: corresponding ground truth will be released.-tool
segmentation: corresponding ground truth will be released.
the problem of detecting the intersection point is trivial when the laser is on
and can be solved by training a deep segmentation network. however, segmentation
requires images with a laser spot as input, while the real gamma probe produces
no visible mark and therefore this approach produces inferior results.an
alternative approach to detect the intersection point is to reconstruct the 3d
tissue surface and estimate the pose of the probe in real time. a tracking and
pose estimation method for the gamma probe [6] involved attaching a dualpattern
marker to the probe to improve detection accuracy. this enabled the derivation
of a 6d pose, comprising a rotation matrix and translation matrix with respect
to the laparoscope camera coordinate. to obtain the intersection point, the
authors used the structure from motion (sfm) method to compute the 3d tissue
surface, combining it with the estimated pose of the probe, all within the
laparoscope coordinate system. however, marker-based tracking and pose
estimation methods have sterilization implications for the instrument, and the
sfm method requires the surgeon to constantly move the laparoscope, reducing the
practicality of these methods for surgery.in this work, we propose a simple, yet
effective regression approach to address this problem. our approach relies
solely on the 2d information and works well without the need for the laser
module after training. furthermore, this simple methodology facilitated an
average inference time of 50 frames per second, enabling real-time sensing area
map generation for intraoperative surgery.
we utilized different deep segmentation networks as a first attempt to address
our problem [10,18]. please refer to the supplementary material for the
implementation details of the networks. we observed that when we do not use
images with the laser, the network was not able to make any good predictions.
this is understandable as the red laser spot provides the key information for
the segmentation. therefore the network does not have any visual information to
make predictions from images of the gamma probe. we note that to enable
real-world applications, we need to estimate the intersection point using the
images when the laser module is turned off.
problem formulation. formally, given a pair of stereo images i l , i r , n
points {p l 1 , p l 2 , ..., p l n } were sampled along the principal axis of
the probe, p l i ∈ r 2 from the left image. the same process was repeated for
the right image. the goal was to predict the intersection point p intersect on
the surface of the tissue. during the training, the ground truth intersection
point position was provided by the laser source, while during testing the
intersection was estimated solely based on visual information without laser
guidance (see fig. 3).network architecture. unlike the segmentation approach,
the intersection point was directly predicted using a regression network. the
images fed to the network were 'laser off' stereo rgb, but crucially, the
intersection point for these images was known a priori from the paired 'laser
on' images. the raw image resolution was 4896×3680 but these were binned to
896×896. principal component analysis (pca) [15] was used to extract the central
axis of the probe and 50 points were sampled along this axis as an extra input
dimension. a network was designed with two branches, one branch for extracting
visual features from the image and one branch for learning the features from the
sequence of principal points using resnet [3] and vision transformer (vit) [2]
as two backbones. the principal points were learned through a multi-layer
perception (mlp) or a long short-term memory (lstm) network [4]. the features
from both branches were concatenated and used for regressing the intersection
point (see fig. 4). finally, the whole network is trained end-to-end using the
mean square error loss.
evaluation metrics. to evaluate sensing area location errors, euclidean distance
was adopted to measure the error between the predicted intersection points and
the ground truth laser points. we reported the mean absolute error, the standard
derivation, and the median in pixel units.implementation details. the networks
were implemented in pytorch [17], with an input resolution of 896 × 896 and a
batch size of 12. we partitioned the jerry dataset into three subsets, the
training, validation, and test set, consisting of 800, 200, and 200 images,
respectively, and the same for the coffbee dataset. the learning rate was set to
10 -5 for the first 300 epochs, then halved until epoch 400, and quartered until
the end of the training. the model was trained for 700 epochs using the adam
optimizer on two nvidia 2080 ti gpus, taking approximately 4 h to complete.
quantitative results on the released datasets are shown in table 1 and table 2
with different backbones for extracting image features, resnet and vit. for the
2d error on two datasets, among the different settings, the combination of
resnet and mlp gave the best performance with a mean error of 70.5 pixels and a
standard deviation of 56.8. the median error of this setting was 59.8 pixels
while the r2 score was 0.82 (higher is better for r2 score). comparing the table
1 and table 2, we found that the resnet backbone was better than the vit
backbone in the image processing task, while mlp was better than lstm in probe
pose representation. resnet processed the input images as a whole, which was
better suited for utilizing the global context of a unified scene composed of
the tissue and the probe, compared to the vit scheme, which treated the whole
scene as several patches. similarly, the sampled 50 principal points on the
probe axis were better processed using the simple mlp rather than using a
recurrent procedure lstm. it is worth noting that the results from stereo inputs
exceeded those from mono inputs, which can be attributed to the essential 3d
information included in the stereo image pairs.for the 3d error, the resnet
backbone still gave generally better performance than the vit backbone while
under the resnet backbone, lstm and mlp gave competitive results and they are
all in sub-milimeter level. we note that the 3d error subjected to the quality
of the acquired ground truth depth maps, which had limited resolution and
non-uniformly distributed valid data due to hardware constraints. hence, we used
the median depth value of a square area of 5 pixels around the points where
depth value was not available.figure 5 shows visualization results of our method
using resnet and mlp. this figure illustrates that our proposed method
successfully detected the intersection point using solely standard rgb
laparoscopic images as the input. furthermore, based on the simple design, our
method achieved the inference time of 50 frames per second, making it
well-suitable for intraoperative surgery.
in this work, a new framework for using a laparoscopic drop-in gamma detector in
manual or robotic-assisted minimally invasive cancer surgery was presented,
where a laser module mock probe was utilized to provide training guidance and
the problem of detecting the probe axis-tissue intersection point was
transformed to laser point position inference. both the hardware and software
design of the proposed solution were illustrated and two newly acquired datasets
were publicly released. extensive experiments were conducted on various
backbones and the best results were achieved using a simple network design,
enabling real time inference of the sensing area. we believe that our problem
reformulation and dataset release, together with the initial experimental
results, will establish a new benchmark for the surgical vision community.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43996-4 25.
flexible ureteroscopy (furs) is a routinely performed surgical procedure for
renal lithotripsy. this procedure inserts a flexible ureteroscope through the
blad-der and ureters to get inside the kidneys for diagnosis and treatment of
stones and tumors. unfortunately, such an examination and treatment depends on
skills and experiences of surgeons. on the other hand, surgeons may miss stones
and tumors and unsuccessfully orientate the ureteroscope inside the kidneys due
to limited field of views, just 2d images without depth information, and the
complex anatomical structure of the kidneys. to this end, ureteroscope tracking
and navigation is increasingly developed as a promising tool to solve these
issues.many researchers have developed various methods to boost endoscopic
navigation. these methods generally consist of vision-and sensor-based tracking.
han et al. [3] utilized the porous structures in renal video images to develop a
vision-based navigation method for ureteroscopic holmium laser lithotripsy. zhao
et al. [15] designed a master-slave robotic system to navigate the flexible
ureteroscope. luo et al. [7] reported a discriminative structural similarity
measure driven 2d-3d registration for vision-based bronchoscope tracking. more
recently, huang et al. [4] developed an image-matching navigation system using
shape context for robotic ureteroscopy. additionally, sensor-based methods are
widely sued in surgical navigation [1,6]. zhang et al. [14] employed
electromagnetic sensors to estimate the ureteroscope shape for
navigation.although these methods mentioned above work well, ureteroscopic
navigation is still a challenging problem. compared to other endoscopes such as
colonoscope and bronchoscope, the diameter of the ureteroscope is smaller,
resulting in more limited lighting source and field of view. particularly,
ureteroscopy involves much solids (impurities) and fluids (liquids), making
ureteroscopic video images low-quality, as well as these solids and fluids
inside the kidneys cannot be regularly observed in computed tomography (ct)
images. on the other hand, the complex internal structures such as calyx,
papilla, and pyramids of the kidneys are difficult to be observed in ct images.
these issues introduce a difficulty in directly aligning ureteroscopic video
sequences to ct images, leading to a challenge of image-based continuous
ureteroscopic navigation.this work aims to explore an accurate and robust
vision-based navigation method for furs procedures without using any external
positional sensors. based on ureteroscopic video images and preoperative
computed tomography urogram (ctu) images, we propose a novel video-ctu
registration method to precisely locate the flexible ureteroscope in the ctu
space. several highlights of this work are clarified as follows. to the best of
our knowledge, this work shows the first study to continuously track the
flexible ureteroscope in preoperative data using a vision-based method.
technically, we propose a novel 2d-3d (video-ctu) registration method that
introduces a structural point similarity measure without using image pixel
intensity information to characterize the difference between the structural
regions in real video images and ctu-driven virtual image depth maps.
additionally, our proposed method can successfully deal with solid and fluid
ureteroscopic video images and attains higher navigation accuracy than
intensity-based 2d-3d registration methods.
our proposed video-ctu registration method consists of several steps: (1)
ureteroscopic structure extraction, (2) virtual depth map generation, and (3)
structural point similarity and optimization. figure 1 illustrates the flowchart
of our method.
the internal kidneys consist of complex anatomical structures such as pelvis and
calyx and also contain solid particles (e.g., stones and impurities) floating in
fluids (e.g., water, urine and small blood), resulting in poor image quality
during ureteroscopy. therefore, it is a challenging task to extract meaningful
features from these low-quality images for achieving accurate 2d-3d
registration.our idea is to introduce specific structures inside the kidneys to
boost the video-ctu registration since these structural regions are meaningful
features that can facilitate the similarity computation. during ureteroscopy,
various anatomical structures observed in ureteroscopic video images indicate
different poses of the ureteroscope inside the kidneys. while some structural
features such as capillary texture and striations at the tip of the renal
pyramids are observed ureteroscopic images, they are not discernible in ct or
other preoperative data. typical structural or texture regions (columns 1∼3 in
fig. 2 (b)) observed both in ureteroscopic video and ctu images are the renal
papilla when the ureteroscope gets into the kidneys through the ureter and renal
pelvis to reach the major and minor calyxes. additionally, we also find that the
renal pelvis (dark or ultra-low light) regions (columns 4∼6 in fig. 2 (b)) are
also useful to enhance the registration. hence, this work employs these interior
renal structural characteristics to calculate the similarity between
ureteroscopic images and ctu.
deep learning is widely used for medical image segmentation. lazo et al. [5]
used spatial-temporal ensembles to segment lumen structures in ureteroscopic
images. more recently, vision transformers show the potential to precisely
segment various medical images [8,12]. this work employs the dense prediction
transformer (dpt) [11] to extract these structural regions from ureteroscopic
images. dpt is a general deep learning framework for dense prediction tasks such
as semantic segmentation and has three versions of dpt-base, dpt-large, and
dpt-hybrid. this work use dpt-base since it only requires a small number of
parameters but provides a high inference speed. dpt-base consists of a
transformer encoder and a convolutional decoder. its backbone is vision
transformers [2], where input images are transformed into tokens by
non-overlapping patches extraction, followed by a linear projection of their
flattened representation. the conventional decoder employs a reassemble
operation [11] to assemble a set of tokens into image-like feature
representations at various resolutions:where s is the output size ratio of the
feature representation and d is the output feature dimension. tokens from layers
l = {6, 12, 18, 24} are reassembled in dpt-base. these feature representations
are subsequently fused into the final dense prediction. in the structure
extraction, we define three classes: non-structural regions (background),
structural regions, and stones. we manually select and annotate ureteroscopic
video images for training and testing. vision transformers require large
datasets for training, so we initialize the encoder with weights pretrained on
imagenet and further train it on our in-house database. figure 3 displays some
segmentation results of ureteroscopic video images.
this step is to compute depth maps of 2d virtual images generated from ctu
images by volume rendering [13]. depth maps can represent structural information
of virtual images. note that this work uses ctu to create virtual images since
non-contrast ct images cannot capture certain internal structures of the
kidneys, although these structures can be observed in ureteroscopic video
images. we introduce a ray casting algorithm in volume rendering to generate
depth maps of virtual rending images [13]. the ray casting algorithm is to trace
a ray that starts from the viewpoint and passes through a pixel on the screen.
when the tracing ray intersects with a voxel, the properties of that voxel will
affect the value of corresponding pixel in the final image. for a 3d point (x 0
, y 0 , z 0 ) and a normalized direction vector (r x , r y , r z ) of its
casting ray, a corresponding point (x, y, z) at any distance d on the tracing
ray is: (x, y, z) = (x 0 + dr x , y 0 + dr y , z 0 + dr z ).(the tracing ray
r(x, y, z, r x , r y , r z ) will stop when it encounters opaque voxels. for 3d
point (x,y,z), its depth value v can be calculated by projecting the ray r(x, y,
z, r x , r y , r z ) onto the normal vector n(x, y, z) of the image plane:where
symbol • denotes the dot product.to obtain the depth map with structural
regions, we define two thresholds t u and t v . only ct intensity values within
[t u , t v ] are opaque voxels that the casting rays cannot pass through in the
ray-casting algorithm. according to ctu characteristics [10], this work uses our
excretory-phase data to generate virtual images and set [t u , t v ] to [-1000,
120], where -1000 represents air and 120 was determined by the physician's
experience and characteristics of contrast agents.unfortunately, the accuracy of
thresholded structural regions suffers from inaccurate depth maps caused by
renal stones and contrast agents. stones and agents are usually high intensity
in ctu images, which result in incorrect depth information of structural regions
(e.g., renal papilla). to deal with these issues, we use the segmented stones as
a mask to remove these regions with wrong depth. on the other hand, the
structural regions usually have larger depth values than the agent-contrasted
regions. therefore, we sort the depth values outside the mask and only use the
thresholded structural regions with the largest depth values for the structural
point similarity computation.
we define a point similarity measure between dpt-base segmented structural
regions in ureteroscopic images and thresholded structural regions in virtual
depth maps generated by the volume rendering ray casting algorithm.the
structural point similarity function (cost function) is defined as an
intersection of point sets from the extracted real and virtual structural
regions: where i i is the ureteroscopic video image at frame i, point sets p i
and p v are from the ureteroscopic image extracted structural region e i (a, b)
and the thresholded structural region e v (a, b) ((a, b) denotes a point)from
the depth map d v of the 2d virtual rendering image i v (p i , q i ),
respectively:where (p i , q i ) is the endoscope position and orientation in the
ctu space. eventually, the optimal pose (p i , qi ) of the ureteroscope in the
ctu space can be estimated by maximizing the structural point similarity:where
powell method [9] is used as an optimizer to run this procedure.
we validate our method on clinical ureteroscopic lithotripsy data with video
sequences and ctu volumes. ureteroscopic video images were a size of 400 × 400
pixels, while the space parameters of ctu volumes were 512 × 512 pixels, 361∼665
slices, 0.625∼1.25 mm slice thickness. three ureteroscopic videos more than
30000 frames were acquired from three ureteroscopic procedures for experiments.
while we manually annotated ureteroscopic video images for dpt-base
segmentation, three experts also manually generated ureteroscope pose
groundtruth data by our developed software, which can manually adjust position
and direction parameters of the virtual camera to visually align endoscopic real
images to virtual images, evaluating the navigation accuracy of the different
methods. figure 4 illustrates the navigation results of segmentation, depth
maps, extracted structural regions for similarity calculation, and generated 2d
virtual images corresponding to estimated ureteroscope poses. structural regions
can be extracted from ureteroscopic images and virtual depth maps. particularly,
we can see that our method generated virtual images (row 7 in fig. 4) resemble
real video images (row 1 in fig. 4) much better than luo et al. [7] generated
ones (row 6 in fig. 4). this implies that our method can estimate the
ureteroscope pose much more accurate than luo et al. [7]. table 1 summarizes
quantitative segmentation results and position and orientation errors. dpt-base
can achieve average segmentation iou 88.34%, accuracy 92.26%, and dsc 93.71%.
the average position and orientation errors of our method were 5.39 mm and 8.14
• , which much outperform the compared method. table 2 shows the results of
sensitivity analysis for the threshold values. it can be seen that inappropriate
threshold selection can lead to an increase in errors. figure 5 boxplots
estimated position and orientation errors for a statistical analysis of our
navigation accuracy.
the effectiveness of our proposed method lies in several aspects. first, renal
interior structures are insensitive to solids and fluids inside the kidneys and
can precisely characterize ureteroscopic images. next, we define a structural
point similarity measure as intersection of point sets between real and virtual
structural regions. such a measure does not use any point intensity information
for the similarity calculation, leading to an accurate and robust similarity
characterization under renal floating solids and fluids. additionally, ctu
images can capture more renal anatomical structures inside the kidneys compared
to ct slices, still facilitating an accurate similarity computation.our method
still suffers from certain limitations. figure 6 displays some ureteroscopic
video images our method fails to track. this is because that the segmentation
method cannot successfully extract structural regions, while the ray casting
algorithm cannot correctly generate virtual depth maps with structural regions.
both unsuccessfully extracted real and virtual structural regions collapse the
similarity characterization. we will improve the segmentation of ureteroscopic
video images, while generating more ground-truth data for training and testing.
this paper proposes a new 2d-3d registration approach for vision-based furs
navigation. specifically, such an approach can align 2d ureteroscopic video
sequences to 3d ctu volumes and successfully locate an ureteroscope into ctu
space. different from intensity-based cost function, a novel structural point
similarity measure is proposed to effectively and robustly characterize
ureteroscopic video images. the experimental results demonstrate that our
proposed method can reduce the navigation errors from (11.28 mm, 10.8 • ) to
(5.39 mm, 8.13 • ).
colorectal cancer (crc) is the third most commonly diagnosed cancer but ranks
second in terms of mortality worldwide [11]. intestinal lesions, particularly
polyps and adenomas, are usually developed to crc in many years. therefore,
diagnosis and treatment of colorectal polyps and adenomas at their early stages
are essential to reduce morbidity and mortality of crc. interventional
colonoscopy is routinely performed by surgeons to visually examine colorectal
lesions. however these lesions in colonoscopic images are easily omitted and
wrongly classified due to limited knowledge and experiences of surgeons.
automatic and accurate segmentation is a promising way to improve colorectal
examination.many researchers employ u-shaped network [7,13,18] for colonoscopic
polyp segmentation. resunet++ [7] combines residual blocks and atrous spatial
pyramid pooling and zhao et al. [18] designed a subtraction unit to generate the
difference features at multiple levels and constructed a training-free network
to supervise polyp-aware features. unlike a family of u-net driven segmentation
methods, numerous papers have been worked on boundary constraints to segment
colorectal polyps. fan et al. [2] introduced pranet with reverse attention to
establish the relationship between boundary cues from global feature maps
generated by a parallel partial decoder. both polyp boundary-aware segmentation
methods work well but still introduce much false positive. based on pranet [2]
and hardnet [1], huang et al. [6] removed the attention mechanism and replaced
res2net50 by hardnet to build hardnet-mseg that can achieve faster segmentation.
in addition, kim et al. [9] modified pranet to construct uacanet with parallel
axial attention and uncertainty augmented context attention to compute uncertain
boundary regions. although pranet and uacanet aim to extract ambiguous boundary
regions from both saliency and reverse saliency features, they simply set the
saliency score to 0.5 that cannot sufficiently detect complete boundaries to
separate foreground and background regions. more recently, shen et al. [10]
introduced task-relevant feature replenishment networks for crosscenter polyp
segmentation, while tian et al. [12] combined transformers and multiple instance
learning to detect polyps in a weakly supervised way.unfortunately, limited
field of view and illumination variations usually result in insufficient
boundary contrast between intestinal lesions and their surrounding tissues. on
the other hand, various polyps and adenomas with different pathological features
have similar visual characteristics to intestinal folds. to address these issues
mentioned above, we explore a new deep learning architecture called cascade
transformer encoded boundary-aware multibranch fusion (ctbmf) networks with
cascade transformers and multibranch fusion for polyp and adenoma segmentation
in colonoscopic white-light and narrow-band video images. several technical
highlights of this work are summarized as follows. first, we construct cascade
transformers that can extract global semantic and subtle boundary features at
different resolutions and establish weighted links between global semantic cues
and local spatial ones for intermediate reasoning, providing long-range
dependencies and a global receptive field for pixel-level segmentation. next, a
hybrid spatial-frequency loss function is defined to compensate for loss
features in the spatial domain but available in the frequency domain.
additionally, we built a new colonoscopic lesion image database and will make it
publicly available, while this work also conducts a thorough evaluation and
comparison on our new database and four publicly available ones (fig. 2).
this section details our ctbmf networks that can refine inaccurate lesion
location, rough or blurred boundaries, and unclear textures. figure 1
illustrates the encoder-decoder architecture of ctmbf with three main modules.
this work employs a pyramid transformer [15] to build a transformer cascaded
encoder. let x 0 and x i be input patches and the feature map at stage i,
respectively. overlapping patch embedding (ope) separates an image into
fixed-size patches and linearly embeds them into tokenized images while making
adjacent windows overlap by half of a patch. either key k i or value v i is the
input sequence of linear spatial reduction (lsr) that implements layer
normalization (ln) and average pooling (ap) to reduce the input dimension:where
ω(•) denotes the output parameters of position embedding, ⊕ is the element-wise
addition, w ki indicates the parameters that reduces the dimension of k i or v i
, and r i is the reduction ratio of the attention layers at stage i.as the
output of lsr is fed into multihead attention, we can obtain attention feature
map a j i from head j (j = 1, 2, • • • , n, n is the head number of the
attention layer) at stage i:where attention(•) is calculated as the original
transformer [14]. subsequently, the output lsrawhere is the concatenation and w
ai is the linear projection parameters. then,where dc is a 3 × 3 depth-wise
convolution [5] with padding size of between the fully-connected (fc) layer and
the gaussian error linear unit (gelu) [4] in the feed-forward networks.
eventually, the output feature map x i of the pyramid transformer at stage i can
be represented by
boundary-aware attention module. current methods [2,9] detect ambiguous
boundaries from both saliency and reverse-saliency maps by predefining a
saliency score of 0.5. unfortunately, a predefined score cannot distinguish
foreground and background of different colonoscopic lesions [3]. based on [17],
this work explores an effective boundary-aware attention mechanism to adaptively
extract boundary regions. given the feature map x i with semantic cues and rough
appearance details, we perform convolution (conv) on it and obtain xi = conv(x i
), which is further augmented by channel and spatial attentions. the channel
attention performs channel maxpooling (cmp), multilayer perceptron (mlp), and
sigmoid (sig) to obtain the intermediate feature map y i :where ⊗ indicates the
elementwise product. subsequently, the detail enhanced feature map z i of the
channel-spatial attention iswhere smp indicates spatial maxpooling. we subtract
the feature map xi from the enhanced map z i to obtain the augmented boundary
attention map b i , and also establish the correlation between the neighbor
layers x i+1 and x i to generate multilevel boundary map g i :where and us
indicate subtraction and upsampling.residual multibranch fusion module. to
highlight salient regions and suppress task-independent feature responses (e.g.,
blurring), we linearly aggregate b i and g i to generate discriminative boundary
attention map d i :where i = 1, 2, 3 and relu is the rectified linear unit
function.we obtain the fused feature representation map m i (i = 1, 2, 3, 4)
from the elementwise addition or summation of m i+1 , d i , and the residual
feature xi byeventually, the output m 1 of the boundary-aware multibranch fusion
decoder is represented by the following equation:which precisely combines global
semantic features with boundary or appearance details of colorectal lesions.
this work proposes a hybrid spatial-frequency loss function h l to train our
network architecture for colorectal polyp and adenoma segmentation:where s l and
f l are a spatial-domain loss and a frequency-domain loss to calculate the total
difference between prediction p and ground truth g, respectively. the
spatial-domain loss s l consists of a weighted intersection over union loss and
a weighted binary cross entropy loss [16].the frequency-domain loss f l can be
computed by [8] where w × h is the image size, λ is the coefficient of f l ,
g(u, v) and p(u, v) are a frequency representation of ground truth g and
prediction p using 2-d discrete fourier transform. γ(u, v) is a spectrum weight
matrix that is dynamically determined by a non-uniform distribution on the
current loss of each frequency.
our clinical in-house colonoscopic videos were acquired from various
colonoscopic procedures under a protocol approved by the research ethics
committee of the university. these white-light and narrow-band colonoscopic
images contain four types of colorectal lesions with different pathological
features classified by surgeons: (1) 268 cases of hyperplastic polyp, (2) 815
cases of inflammatory polyp, (3) 1363 cases of tubular adenoma, and (4) 143
cases of tubulovillous adenoma. additionally, four public datasets including
kvasir, etis-laribpolypdb, cvc-colondb, and cvc-clinicdb were also used to
evaluate our network model. we implemented ctbmf on pytorch and trained it with
a single nvidia rtx3090 to accelerate the calculations for 100 epochs at
mini-batch size 16. factors λ (eq. ( 14)) were set to 0.1. we employ the
stochastic gradient descent algorithm to optimize the overall parameters with an
original learning rate of 0.0001 for cascade transformer encoding and 0.05 for
other parts and use warmup and linear decay strategies to adjust it. the
momentum and weight decay were set as 0.9 and 0.0005. further, we resized input
images to 352 × 352 for training and testing and the training time was nearly
1.5 h to achieve the convergence. we employ three metrics to evaluate the
segmentation: dice similarity coefficient (dsc), intersection over union (iou),
and weighted f-measure (f β ).
figure 3 visually compares the segmentation results of the four methods tested
on our in-house and public databases. our method can accurately segment polyps
in white-light and narrow-band colonoscopic images under various scenarios, and
ctbmf can successfully extract small, textureless and weak boundary and
colorectal lesions. the segmented boundaries of our method are sharper and clear
than others especially in textureless lesions that resemble intestinal
lining.figure 4 shows the dsc-boxplots to evaluate the quality of segmented
polyps and adenomas, which still demonstrate that our method works much better
than the others. figure 5 displays the enhanced feature maps using the
boundary-aware attention module. evidently, small and weak-boundary or
textureless lesions can be enhanced with good boundary feature representation.
table 1 summarizes the quantitative results in accordance with the three metrics
and computational time of four methods. evidently, ctbmf generally works better
than the compared methods on the in-house database with four types of colorectal
lesions. furthermore, we also summarizes the average three metrics computed from
all the five databases (the in-house dataset and four public datasets). our
method attains much higher average dsc and iou of (0.870, 0.805) than the others
on the five databases. we performed an ablation study to evaluate the
effectiveness of each module used in ctbmf. the baseline is the standard version
of cascade pyramid transformers. modules d 1 , d 2 , d 3 , residual connections,
and frequency loss f l are gradually added into the baseline, evaluating the
effectiveness of each module and comparing the variants with each other. we
tested these modules on the four public databases. table 2 shows all the
ablation study results. each module can improve the segmentation performance.
particularly, the boundary-aware attention module critically improves the
average dsc, iou, and f β .our method generally works better than the other
three methods. several reasons are behind this. first, the cascade-transformer
encoder can extract local and global semantic features of colorectal lesions
with different pathological characteristics due to its pyramid representation
and linear spatial reduction attention. while the pyramid operation extracts
multiscale local features, the attention mechanism builds global semantic cues.
both pyramid and attention strategies facilitate the representation of small and
textureless intestinal lesions in encoding, enabling to characterize the
difference between intestinal folds (linings) and subtle-texture polyps or small
adenomas. next, the boundary-aware attention mechanism drives the multibranch
fusion, enhancing the representation of intestinal lesions in weak boundary and
nonuniform lighting. such a mechanism first extracts the channel-spatial
attention feature map, from which subtracts the current pyramid transformer's
feature map to enhance the boundary information. also, the multibranch fusion
generates multilevel boundary maps by subtracting the next pyramid transformer's
upsampling output from the current pyramid transformer's output, further
improving the boundary contrast. additionally, the hybrid spatial-frequency loss
was also contributed to the improvement of colorectal lesion segmentation. the
frequency-domain information can compensate loss feature information in the
spatial domain, leading to a better supervision in training.
this work proposes a new deep learning model of cascade pyramid transformer
encoded boundary-aware multibranch fusion networks to automatically segment
different colorectal lesions of polyps and adenomas in colonoscopic imaging.
while such an architecture employs simple and convolution-free cascade
transformers as an encoder to effectively and accurately extract global semantic
features, it introduces a boundary-aware attention multibranch fusion module as
a decoder to preserve local and global features and enhance structural and
boundary information of polyps and adenomas, as well as it uses a hybrid
spatialfrequency loss function for training. the thorough experimental results
show that our method outperforms the current segmentation models without any
preprocessing. in particular, our method attains much higher accuracy on
colonoscopic images with small, illumination changes, weak-boundary,
textureless, and motion blurring lesions, improving the average dice similarity
coefficient and intersection over union from (89.5%, 84.1%) to (90.3%, 84.4%) on
our in-house database, from (78.9%, 72.6%) to (83.4%, 76.5%) on the four public
databases, and from (84.3%, 78.4%) to (87.0%, 80.5%) on the five databases.
ctbmf (ours) 0.870 0.805 0.846 33.1 fps 33.6 fps 33.4 fps
medical imaging is essential during diagnosis, surgical planning, surgical
guidance, and follow-up for treating brain pathology. images from multiple
modalities are typically acquired to distinguish clinical targets from
surrounding tissues. for example, intra-operative ultrasound (ius) imaging and
magnetic resonance imaging (mri) capture complementary characteristics of brain
tissues that can be used to guide brain tumor resection. however, as noted in
[30], multi-modal data is expensive and sparse, typically leading to incomplete
sets of images. for example, the prohibitive cost of intra-operative mri (imri)
scanners often hampers the acquisition of imri during surgical procedures.
conversely, ius is an affordable tool but has been perceived as difficult to
read compared to imri [5]. consequently, there is growing interest in
synthesizing missing images from a subset of available images for enhanced
visualization and clinical training.medical image synthesis aims to predict
missing images given available images. deep-learning based methods have reached
the highest level of performance [29], including conditional generative
adversarial (gan) models [6,14,15,21] and conditional variational auto-encoders
[3]. however, a key limitation of these techniques is that they must be trained
for each subset of available images.to tackle this challenge, unified approaches
have been proposed. these approaches are designed to have the flexibility to
handle incomplete image sets as input, improving practicality as only one
network is used for generating missing images. to handle partial inputs, some
studies proposed to fill missing images with arbitrary values [4,17,18,24].
alternatively, other work aim at creating a common feature space that encodes
shared information from different modalities. feature representations are
extracted independently for each modality. then, arithmetic operations (e.g.,
mean [7,11,28], max [2] or a combination of sum, product and max [32]) are used
to fuse these feature representations. however, these operations do not force
the network to learn a shared latent representation of multi-modal data and lack
theoretical foundations. in contrast, multi-modal variational auto-encoders
(mvaes) provide a principled probabilistic fusion operation to create a common
representation space [8,30]. in mvaes, the common representation space is
low-dimensional (e.g., r 256 ), which usually leads to blurry synthetic images.
in contrast, hierarchical vaes (hvaes) [19,22,26,27] allow for learning complex
latent representations by using a hierarchical latent structure, where the
coarsest latent variable (z l ) represents global features, as in mvaes, while
the finer variables capture local characteristics. however, hvaeshave not yet
been extended to multi-modal settings to synthesize missing images.in this work,
we introduce multi-modal hierarchical latent representation vae (mhvae), the
first multi-modal vae approach with a hierarchical latent representation for
unified medical image synthesis. our contribution is four-fold.first, we
integrate a hierarchical latent representation into the multi-modal variational
setting to improve the expressiveness of the model. second, we propose a
principled fusion operation derived from a probabilistic formulation to support
missing modalities, thereby enabling image synthesis. third, adversarial
learning is employed to generate realistic image synthesis. finally, experiments
on the challenging problem of ius and mr synthesis demonstrate the effectiveness
of the proposed approach, enabling the synthesis of high-quality images while
establishing a mathematically grounded formulation for unified image synthesis
and outperforming non-unified gan-based approaches and the state-of-the-art
method for unified multi-modal medical image synthesis.
variational auto-encoders (vaes). the goal of vaes [16] is to train a generative
model in the form of p(x, z) = p(z)p(x|z) where p(z) is a prior distribution
(e.g. isotropic normal distribution) over latent variables z ∈ r h and where p θ
(x|z) is a decoder parameterized by θ that reconstructs data x ∈ r n given z.
the latent space dimension h is typically much lower than the image space
dimension n , i.e. h n . the training goal with respect to θ is to maximize the
marginal likelihood of the data p θ (x) (the "evidence"); however since the true
posterior p θ (z|x) is in general intractable, the variational evidence lower
bound (elbo) is instead optimized. the elbo l vae (x; θ, φ) is defined by
introducing an approximate posterior q φ (z|x) with parameters φ:where kl[q||p]
is the kullback-leibler divergence between distributions q and p.
multi-modal vaes [8,25,30] introduced a principled probabilistic formulation to
support missing data at training and inference time. multi-modal vaes assume
that m paired images x = (x 1 , ..., x m ) ∈ r m ×n are conditionally
independent given a shared representation z as higlighted in fig. 1, i.e. p θ
(x|z) = m i=1 p(x i |z). instead of training one single variational network q φ
(z|x) that requires all images to be presented at all times, mvaes factorize the
approximate posterior as a combination of unimodal variational posteriors (q φ
(z|x i )) m i=1 . given any subset of modalities π ⊆ {1, ..., m }, mvaes have
the flexibility to approximate the π-marginal posteriors p(z|(x i ) i∈π ) using
the |π| unimodal variational posteriors (q φ (z|x i )) i∈π . mvae [30] and
u-hved [8] factorize the π-marginal variational posterior as a
product-of-experts (poe), i.e.:(2)
in this paper, we propose a deep multi-modal hierarchical vae called mhvae that
synthesizes missing images from available images. mhvae's design focuses on
tackling three challenges: (i) improving expressiveness of vaes and mvaes using
a hierarchical latent representation; (ii) parametrizing the variational
posterior to handle missing modalities; (iii) synthesizing realistic images.
be a complete set of paired (i.e. co-registered) images of different modalities
where m is the total number of image modalities and n the number of pixels (e.g.
m = 2 for t 2 mri and ius synthesis). the images x i are assumed to be
conditionally independent given a latent variable z. then, the conditional
distribution p θ (x|z) parameterized by θ can be written as:given that vaes and
mvaes typically produce blurry images, we propose to use a hierarchical
representation of the latent variable z to increase the expressiveness the model
as in hvaes [19,22,26,27]. specifically, the latent variable z is partitioned
into disjoint groups, as shown in fig. 1 i.e. z = {z 1 , ...z l }, where l is
the number of groups. the prior p(z) is then represented by:where p(z l ) = n (z
l ; 0, i) is an isotropic normal prior distribution and the conditional prior
distributions p θ l (z l |z >l ) are factorized normal distributions with
diagonal covariance parameterized using neural networks, i.e. p θ l (z l |z >l )
= n (z l ; μ θ l (z >l ), d θ l (z >l )). note that the dimension of the finest
latent variable z 1 ∈ r h1 is similar to number of pixels, i.e. h 1 = o(n ) and
the dimension of the latent representation exponentially decreases with the
depth, i.e. h l h 1 . reusing eq. 1, the evidence log (p θ (x)) is lower-bounded
by the tractable variational elbo l elbo mhvae (x; θ, φ):where q φ (z|x) = l l=1
q φ (z l |x, z >l ) is a variational posterior that approximates the intractable
true posterior p θ (z|x).
to synthesize missing images, the variational posterior (q φ (z l |x, z >l )) l
l=1 should handle missing images. we propose to parameterize it as a combination
of unimodal variational posteriors. similarly to mvaes, for any set π ⊆ {1, ...,
m } of images, the conditional posterior distribution at the coarsest level l is
expressedwhere p(z l ) = n (z l ; 0, i) is an isotropic normal prior
distribution and q φl (z|x i ) is a normal distribution with diagonal covariance
parameterized using cnns.for the other levels l ∈ {1, .., l -1}, we similarly
propose to express the conditional variational posterior distributions as a
product-of-experts:where q φ i l (z l |x i , z >l ) is a normal distribution
with diagonal covariance parameterized using cnns, i.e.). this formulation
allows for a principled operation to fuse content information from available
images while having the flexibility to handle missing ones. indeed, at each
level l ∈ {1, ..., l}, the conditional variational distributions q poe φ l ,θ l
(z l |x π , z >l ) are normal distributions with mean μ φ l ,θ l (x π , z >l )
and diagonal covariance d φ l ,θ l (x π , z >l ) expressed in closed-form
solution [12] as:with d θl (z >l ) = i and μ θl (z >l ) = 0.
the joint reconstruction and synthesis optimization goal is to maximize the
expected evidence e x∼p data [log(p(x))]. as the elbo defined in eq. 5 is valid
for any approximate distribution q, the evidence, log(p θ (x)), is in particular
lowerbounded by the following subset-specific elbo for any subset of images
π:hence, the expected evidence e x∼p data [log(p(x))] is lower-bounded by the
average of the subset-specific elbo, i.e.:consequently, we propose to average
all the subset-specific losses at each training iteration. the image decoding
distributions are modelled as normal with variance σ, i.e. p θ (x i |z 1 ) = n
(x i ; μ i (z 1 ), σi), leading to reconstruction losses log(p θ (x i |z 1 )),
which are proportional to ||x iμ i (z 1 )|| 2 . to generate sharper images, the
l 2 loss is replaced by a combination of l 1 loss and gan loss via a patchgan
discriminator [14]. moreover, the expected kl divergences are estimated with one
sample as in [19]. finally, the loss associated with the subsetspecific elbos
eq. ( 9) is:following standard practices [4,14], images are normalized in [-1,
1] and the weights of the l 1 and gan losses are set to λ l1 = 100 and λ gan =
1.
in this section, we report experiments conducted on the challenging problem of
mr and ius image synthesis.data. we evaluated our method on a dataset of 66
consecutive adult patients with brain gliomas who were surgically treated at the
brigham and women's hospital, boston usa, where both pre-operative 3d t2-space
and pre-dural opening intraoperative us (ius) reconstructed from a tracked
handheld 2d probe were acquired. the data will be released on tcia in 2023. 3d
t2-space scans were affinely registered with the pre-dura ius using niftyreg
[20] following the pipeline described in [10]. three neurological experts
manually checked registration outputs. the dataset was randomly split into a
training set (n = 56) and a testing set (n = 10). images were resampled to an
isotropic 0.5 mm resolution, padded for an in-plane matrix of (192,192) using
spade [21], pix2pix [14], mvae [30], resvit [4] and mhvae (ours) without and
with gan loss. as highlighted by the arrows, our approach better preserves
anatomy compared to gan-based approach and produces more realistic approach than
the transformer-based approach (resvit).for the encoder and decoder from
mobilenetv2 [23] are used with squeeze and excitation [13] and swish activation.
the image decoders (μ i ) m i=1 correspond to 5 resnet blocks. following
state-of-the-art bidirectional inference architectures [19,27], the
representations extracted in the contracting path (from x i to (z l ) l ) and
the expansive path (from z l to x i and (z l ) l<l ) are partially shared.
models are trained for 1000 epochs with a batch size of 16. to improve
convergence, λ gan is set to 0 for the first 800 epochs. network architecture is
presented in appendix, and the code is available at https://github.com/reubendo/
mhvae.evaluation. since paired data was available for evaluation, standard
supervised evaluation metrics are employed: psnr (peak signal-to-noise ratio),
ssim (structural similarity), and lpips [31] (learned perceptual image patch
similarity). quantitative results are presented in table 1, and qualitative
results are shown in fig. 2. wilcoxon signed rank tests (p < 0.01) were
performed.ablation study. to quantify the importance of each component of our
approach, we conducted an ablation study. first, our model (mhvae) was compared
with mvae, the non-hierarchical multi-modal vae described in [30]. it can be
observed in table 1 that mhvae (ours) significantly outperformed mvae. this
highlights the benefits of introducing a hierarchy in the latent representation.
as shown in fig. 2, mvae generated blurry images, while our approach produced
sharp and detailed synthetic images. second, the impact of the gan loss was
evaluated by comparing our model with (λ gan = 0) and without (λ gan = 1) the
adversarial loss. both models performed similarly in terms of evaluation
metrics. however, as highlighted in fig. 2, adding the gan loss led to more
realistic textures with characteristic ius speckles on synthetic ius. finally,
the image similarity between the target and reconstructed images (i.e., target
image used as input) was excellent, as highlighted in table 1. this shows that
the learned latent representations preserved the content information from input
modalities. state-of-the-art comparison. to evaluate the performance of our
model (mhvae) against existing image synthesis frameworks, we compared it to two
state-of-the-art gan-based conditional image synthesis methods: pix2pix [14] and
spade [21]. these models have especially been used as synthesis backbones in
previous mr/ius synthesis studies [6,15]. results in table 1 show that our
approach statistically outperformed these gan methods with and without
adversarial learning. as shown in fig. 2, these conditional gans produced
realistic images but did not preserve the brain anatomy. given that these models
are not unified, pix2pix and spade must be trained for each synthesis direction
(t 2 → ius and ius → t 2 ). in contrast, mhvae is a unified approach where one
model is trained for both synthesis directions, improving inference practicality
without a drop in performance. finally, we compared our approach with resvit
[4], a transformer-based method that is the current state-of-the-art for unified
multi-modal medical image synthesis. our approach outperformed or reached
similar performance depending on the metric. in particular, as shown in fig. 2
and in table 1 for the perceptual lpips metric, our gan model synthesizes images
that are visually more similar to the target images. finally, our approach
demonstrates significantly lighter computational demands when compared to the
current sota unified image synthesis framework (resvit), both in terms of time
complexity (8g macs vs. 487g macs) and model size (10m vs. 293m parameters).
compared to mvaes, our hierarchical multi-modal approach only incurs a marginal
increase in time complexity (19%) and model size (4%).overall, this set of
experiments demonstrates that variational auto-encoders with hierarchical latent
representations, which offer a principled formulation for fusing multi-modal
images in a shared latent representation, are effective for image synthesis.
other potential applications. the current framework enables the generation of
ius data using t 2 mri data. since image delineation is much more efficient on
mri than on us, annotations performed on mri could be used to train a
segmentation network on pseudo-ius data, as performed by the top-performing
teams in the crossmoda challenge [9]. for example, synthetic ultrasound images
could be generated from the brats dataset [1], the largest collection of
annotated brain tumor mr scans. qualitative results shown in appendix
demonstrate the ability of our approach to generalize well to t 2 imaging from
brats. finally, the synthetic images could be used for improved ius and t 2
image registration.
we introduced a multi-modal hierarchical variational auto-encoder to perform
unified mr/ius synthesis. by approximating the true posterior using a
combination of unimodal approximates and optimizing the elbo with multi-modal
and uni-modal examples, mhvae demonstrated state-of-the-art performance on the
challenging problem of ius and mr synthesis. future work will investigate
synthesizing additional imaging modalities such as ct and other mr sequences.
l ) were set to 1 × 1 and 256. the spatial and feature dimensions are
respectively doubled and halved after each level to reach a feature
representation of dimension 8 for each pixel, i.e. z 1 ∈ r 196×196×8 and z l ∈ r
1×1×256 . this leads to 7 latent variable levels, i.e. l = 7. following
state-of-the-art nvae architecture[27], residual cells
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 43.
2d-3d registration refers to the highly challenging process of aligning an input
2d image to its corresponding slice inside a given 3d volume [4]. it has
received growing attention in medical imaging due to the various contexts where
it applies, like image fusion between 2d real-time acquisitions and either
pre-operative 3d images for guided interventions or reference planning volumes
for patient positioning in radiation therapy (rt). another important task is the
volumetric reconstruction of a sequence of misaligned slices ex vivo, enabling
multimodal comparison toward improved diagnosis. in this respect, overlaying 3d
radiology and 2d histology could significantly enhance radiologists'
understanding of the links between tissue characteristics and radiologic signals
[9]. indeed, mri or ct scans are the baseline source of information for cancer
treatment but fail to provide an accurate assessment of disease proliferation,
leading to high variability in tumor detection [5,13,17]. on the other hand,
high-resolution digitized histopathology, called whole slide imaging (wsi),
provides cell-level information on the tumor environment from the surgically
resected specimens. however, the registration process is substantially difficult
due to the visual characteristics, resolution scale, and dimensional differences
between the two modalities. in addition, histological preparation involves
tissue fixation and slicing, leading to severe collapse and out-of-plane
deformations. (semi-)automated methods have been developed to avoid
time-consuming and biased manual mapping, including protocols with 3d mold or
landmarks [10,22], volume reconstruction to perform 3d registration
[2,18,19,23], or optimization algorithms for direct multimodal comparison
[3,15]. more recently, deep learning (dl) has been introduced but is limited to
2d/2d and requires prior plane selection [20]. on the other hand, successful dl
methods have been proposed to address the 2d/3d mapping problem for other
medical modalities [6,8,16,21]. however, given the extreme deformation that the
tissue undergoes during the histological process, additional guidance is needed.
one promising solution is to rely on rigid structures that are supposedly more
robust during the preparation. structural information to guide image
registration has been studied with the help of segmentations into the training
loop [11], or by learning new image representations for refined mapping [12].in
this paper, we propose to leverage the structural features of tissue and more
particularly the rigid areas to guide the registration process with two distinct
contributions: (1) a cascaded rigid alignment driven by stiff regions and
coupled with recursive plane selection, and (2) an improved 2d/3d deformable
motion model with distance field regularization to handle out-of-plane
deformation. to our knowledge, no previous study proposed 2d/3d registration
combined with structure awareness. we also use the cyclegan for image
translation and direct monomodal signal comparison [25]. like [14,24], we
combine registration with modality translation and integrate the two
aforementioned components. we demonstrate superior quantitative results for head
and neck (h&n) 3d ct and 2d wsis than traditional approaches failing due to the
histological constraints. in addition, we show that structuregnet performs
better than the state-of-the-art model from [14] on 3d ct/2d mr for the pelvis
in rt.
our three-step structure-aware pipeline is thoroughly detailed in fig. 1. for
clarity, we focus on the radiology-histology application but the pipeline is
versatile to any 2d/3d setting. the modality transfer is a 2d image-to-image
translation problem defined as follows: given a sequence of n slices h = {h 1 ,
..., h n } and a volume considered as a full stack of m axial slices ct = {ct 1
, ..., ct m }, we build a cyclegan with two generators and two discriminators g
h→ct , g ct →h , d h and d ct . with a symmetric situation for g ct →h , g h→ct
outputs a synthetic ct image, which is then processed by d ct along with
randomly sampled original input slices with an associated adversarial loss l adv
. the cyclical pattern lies in the similarity between the original images and
the reconstructed samples g ct →h • g h→ct (h i ) through a pixel-wise cycle
loss l cyc . finally, we employ two additional metrics: an identity loss l id to
encourage modality-specific feature representation when considering h i being
the input for g ct →h with an expected identity synthesis; and a structure
consistency mind loss from [7] to ensure style transfer without content
alteration. these losses are the classical implementations for cyclegan and are
detailed in the supplementary material.
we replace the volume reconstruction step with a recursive dual model. we first
rotate and translate the 3d ct to match the stack of slices h, which is crucial
as an initialization step to help the 2d-3d network to focus on small
out-of-plane deformations and avoid local minima. then, we perform a precise
plane selection and solve the spacing gap by adjusting the z-position of each
slice to its most similar ct section. these two steps are performed iteratively
until convergence, with a recursive algorithm to reduce computational cost (fig.
2).for rigid initialization, the hypothesis is that the histological specimen is
cut with an unknown spacing and angle, but the latter is supposed constant
between wsis. a rigid alignment is thus sufficient to reorient moving ct onto
fixed h. based on a theoretical axial slice sequence z = (z 1 , ..., z m ), we
define fig. 2. cascaded alignment through rigid structure-aware warping followed
by recursive plane selection. the deformed ct from 1. is the input for 2., along
with sct and slice sequence z. the updated z from 2. is applied to m h while the
rigid deformation is applied to mct so that new inputs can feed 1. again as
iterative refining.h as a sparse 3d volume the same size as ct , filled in with
h i at z = z i and zeros elsewhere (the same applies for the corresponding sct
from the previous module). because soft tissues undergo too large out-of-plane
deformations, we leverage the rigid structures which are supposed not to be
distorted or shrunk during the histological process. we extract their
segmentation masks m ct , m h for both modalities (see preprocessing in sect.
3), concatenate and fed them into an encoder followed by a fully connected layer
that outputs six transformation parameters (3 rotations, 3 translations). a
differentiable spatial transform r finally warps m ct for similarity
optimization with m h . similarly to [14], we adopt a loss l rigid masked on
empty slices to avoid the introduction of noise at slices within the gradient
where no data is provided, and directly train on the dice similarity coefficient
(dsc) between rigid areas:additionally, r also warps ct without gradient
backpropagation and is the input with sct for plane selection. we then introduce
a sequence alignment problem, the objective being to update the slice sequence z
of sct by mapping it to a corresponding sequence j of 2d images from ct . we
define s a similarity matrix, where s(i, j) is the total similarity (measured
with mi) when mapping sct z1 , ..., sct zi with ct j1 , ..., ctwhich means that
each row of s will be filled by computing the sum of the mi for the
corresponding column j and the maximum similarity from the last row. like any
dynamic programming method, we want to find the optimal sequence j by following
the backward path of s building. to do so, we retrieve the new index j that
yielded the maximized similarity for each step j = [max i (s(i, j)] j , and we
update z ← j accordingly. in addition, the j sequence cannot be too different
from z as it would induce overlap between ordered wsis. we thus constrained the
possible matching values with k ∈ [z i -2, z i + 2]. based on these rigid
registration and plane selection blocks, we build a cascaded module to
iteratively refine the alignment where the intermediate warping becomes the new
input. we defined the number of iterations as a hyperparameter to reach a good
balance between computational time and similarity maximization. this dual model
is crucial for initialization but does not take into account out-ofplane
deformations and a perfect alignment is not accessible yet. the deformable
framework bridges this gap by focusing on irregular displacements caused by
tissue manipulation and refining the rigid warping.
given one fixed multi-slice sct and a moving rigidly warped r(ct ) from the
previous module, we adopt an architecture close to voxelmorph [1]. still, the
rigidly warped ct = r(ct ) and the plane-adjusted sparse sct are fed through two
different encoders for independent feature extraction. the architecture is
depicted in fig. 3. both latent representations are element-wise subtracted. a
decoder is connected to both encoders and generates a displacement field φ the
same size as input images but with (x, y, z)-channels corresponding to the
displacement in each spatial coordinate. a differentiable sampler d warps ct in
a deformable setting, which is then compared to sct through a masked normalized
cross-correlation (ncc) loss l defo :finally, we add two sources of
regularization. soft tissues away from bones and cartilage are more subject to
shrinkage or disruption, so we harness the information from the cartilage
segmentation mask of ct to generate a distance transform map δ defined as δ(v) =
min m∈mct ||v -m|| 2 . it maps each voxel v of ct to its distance with the
closest point m to the rigid area m ct . we can then control the displacement
field, with close tissue being more highly constrained than isolated areas: φ =
φ (δ + ), where is the hadamard product and is a hyperparameter matrix allowing
small displacement even for cartilage areas for which distance transform is
null. a second regularization takes the form of a loss l regu (φ ) = v∈r 3 ||∇φ
(v)|| 2 on the volume to constrain spatial gradients and thus encourage smooth
deformation, which is essential for empty slices which are excluded from l defo
. the total loss is a weighted sum of l defo and l regu .
dataset and preprocessing. our clinical dataset consists of 108 patients for
whom were acquired both a pre-operative h&n ct scan and 4 to 11 wsis after
laryngectomy (with a total amount of 849 wsis). the theoretical spacing between
each slice is 5 mm, and the typical pixel size before downsampling is 100k ×
100k. two expert radiation oncologists on ct delineated both the thyroid and
cricoid cartilages for structure awareness and the gross tumor volume (gtv) for
clinical validation, while two expert pathologists did the same on wsis. they
then meet and agreed to place 6 landmarks for each slice at important locations
(not used for training). we ended up with images of size 256 × 256 (×64 for 3d
ct) of 1 mm isotropic grid space. we split the dataset patient-wise into three
groups for training (64), validation (20), and testing (24). to demonstrate the
performance of our model on another application, we also retrieved the datasets
from [14] for pelvis 3d ct/2d mr. it is made of 451 pairs between ct and
truefisp sequences, and 217 other pairs between ct and t2 sequences. we guided
the registration thanks to the rigid left/right femoral heads and computed
similarity metrics on the 7 additional organs at risk (anal canal, bladder,
rectum, penile bulb, seminal vesicle, and prostate). all masks were provided by
the authors and were originally segmented by internal experts.hyperparameters.
we drew our code from cyclegan and voxelmorph implementations with modifications
explained above, and we thank the authors of msv-regsynnet for making their code
and data available to us [1,14,25]. a detailed description of architectures and
hyperparameters can be found in the supplementary material. we implemented our
model with pytorch1.13 framework and trained for 600 (800 for mr/ct) epochs with
a batch size of 8 (4 for mr/ct) patients parallelized over 4 nvidia gtx 1080
tis.evaluation. we benchmarked our method against three baselines: first, to
assess the benefit of modality translation over the multimodal loss, we re-used
the original 3d voxelmorph model with mind as a multimodal metric for
optimization. we also modified this approach by masking the loss function to
account for the 2d-3d setting. next, we implemented the modality
translation-based msv-regsyn-net and modified it for our application to measure
the importance of joint structure-aware initialization and regularization.
finally, to differentiate the latter contributions, we tested two ablation
studies: without the cascaded rigid mapping or without the distance field
control. according to the mr/ct application in rt, we compared our model against
the state-of-the-art results of msv-regsynnet which were computed on the same
dataset.
three samples from the test set are displayed in fig. 1. from a qualitative
perspective, the densities of the different tissues are well reconstructed, with
rigid structures like cartilage being lighter than soft tissues or tumors. the
general shape of the larynx also complies with the original radiologic images.
we achieve a mean structural similarity (ssim) index of 0.76/1 between both
modalities, demonstrating the strong synthesis capabilities of our network
compared to msv-regsynnet and our ablative study without initialization process,
with an ssim of 0.72 (respectively 0.69). therefore, the cascaded rigid
initialization is crucial and helps the modality translation module in getting
more similar pairs of images for eased synthesis on the next pass.
we present visual results in fig. 4. the initialization enables an accurate
plane selection as proved by the similar shape of cartilages in (b). even for
some severe difficulties inherent to the histological process like a cut larynx,
the model successfully maps both cartilage and soft tissue without completely
tearing the ct image thanks to regularization (c-d-e). for quantitative
assessment, we computed the dsc as well as the hausdorff distance between
cartilages, and the average distance between characteristic landmarks disposed
before registration(table 1).our method outperforms all baselines, proving the
necessity of a singular approach to handle the specific case of histology. the
popular voxelmorph framework fails, and the 2d-3d adaptation demonstrates the
value of the masked loss function. the superior performance of msv-regsynnet
advocates for a modality translation-based method compared to a direct
multimodal similarity criterion. in addition, the ablation studies prove the
benefit of the distance field regularization and more importantly the cascaded
initialization. concerning the gpu runtime, with a 3-step cascade for
initialization, the inference remains in a similar time scale to baseline
methods and performs mapping in less than 3s. we also compared against
msv-regsynnet on its own validation dataset for generalization assessment: we
yielded comparable results for the first cohort and significantly better ones
for the second, which proves that structuregnet behaves well on other modalities
and that the structure awareness is an essential asset for better registration,
as pelvis is a location where organs are moving. visuals of registration results
are displayed in the supplementary material. eventually, an important clinical
endpoint of our study is to compare the gtv delineated on ct with gold-standard
tumor extent after co-registration to highlight systematic errors and better
understand the biological environment from the radiologic signals. we show in
(f) that the gtv delineated on ct overestimates the true tumor extent of around
31%, but does not always encompass the tumor with a proportion of histological
tumor contained within the ct contour of 0.86. the typical error cases are the
inclusion of cartilage or edema, which highlights the limitations and
variability of radiology-based examinations, leading to increased toxicity or
untreated areas in rt.
we introduced a novel framework for 2d/3d multimodal registration.
struc-turegnet leverages the structure of tissues to guide the registration
through both initial plane selection and deformable regularization; it combines
adversarial training for modality translation with a 2d-3d mapping setting and
does not require any protocol for 3d reconstruction. it is worth noticing that
even if the annotation of cartilage was manual, automating this process is not a
bottleneck as the difference in contrast between soft tissue and stiff areas is
clear enough to leverage any image processing tool for this task. finally, it is
entirely versatile as we designed our experiments for ct-wsi but any 3d
radiological images are suitable. we achieve superior results than
state-of-the-art methods in dl-based registration in a similar time scale,
allowing precise mapping of both modalities and a better understanding of the
tumor microenvironment. the main limitation lies in the handling of organs
without any rigid areas like the prostate. future work also includes a study
with biomarkers from immunohistochemistry mapped onto radiology to go beyond
binary tumor masks and move toward virtual biopsy.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 73.
tomographic imaging estimates body density using hundreds of x-ray projections,
but it's slow and harmful to patients. acquisition time may be too high for
certain applications, and each projection adds dose to the patient. a quick,
low-cost 3d estimation of internal structures using only bi-planar x-rays can
revolutionize radiology, benefiting dental imaging, orthopedics, neurology, and
more. this can improve image-guided therapies and preoperative planning,
especially for radiotherapy, which requires precise patient positioning with
minimal radiation exposure.however, this task is an ill-posed inverse problem:
x-ray measurements are the result of attenuation integration across the body,
which makes them very fig. 1. current methods vs our method. feed-forward
methods do not manage to predict a detailed and matching tomographic volume from
a few projections. iterative methods based on neural radiance fields lack prior
for good reconstruction. by learning an embedding for the possible volumes, we
can recover an accurate volume from very few projections with an optimization
based on a bayesian formulation.ambiguous. traditional reconstruction methods
require hundreds of projections to get sufficient constraints on the internal
structures. with very few projections, it is very difficult to disentangle the
structures for even coarse 3d estimation. in other words, many 3d volumes may
have generated such projections a priori.classical analytical and iterative
methods [8] fail when very few projections are available. several works have
attempted to largely decrease the number of projections needed for an accurate
volumetric reconstruction. some deep learning methods [7,12,24,25,30] predict
directly a 3d volume in a forward way from very few projections. the volume is
however not guaranteed to be consistent with the projections and it is not clear
which solution is retrieved. other recent methods have adapted nerfs [20] to
tomographic reconstruction [23,31]. these non-learning methods show good results
when the number of input projections remains higher than a dozen but fail when
very few projections are provided, as our experiments in sect. 3.3 show.as
illustrated in fig. 1, to be able to reconstruct a volume accurately given as
low as two projections only, we first learn a prior on the volume. to do this,
we leverage the potential of generative models to learn a low-dimensional
manifold of the target body part. given projections, we find by a bayesian
formulation the intermediate latent vectors conditioning the generative model
that minimize the error between synthesized projections of our reconstruction
and these input projections. our work builds on hong et al. [10]'s 3d
style-based generative model, which we extend via a more complex network and
training framework.compared to other 3d gans, it is proven to provide the best
disentanglement of the feature space related to semantic features [2].by
contrast with feed-forward methods, our approach does not require paired
projections-reconstructions, which are very tedious to acquire, and it can be
used with different numbers of projections and different projection geometries
without retraining. compared to nerf-based methods, our method exploits prior
knowledge from many patients to require only two projections. we evaluate our
method on reconstructing cancer patients' head-and-neck cts, which involves
intricate and complicated structures. we perform several experiments to compare
our method with a feed-forward-based method [30] and a recent nerf-based method
[23], which are the previous state-of-the-art methods for the very few or few
projections cases, respectively.we show that our method allows to retrieve
results with the finest reconstructions and better matching structures, for a
variety of number of projections. to summarize, our contributions are two-fold:
(i) a new paradigm for 3d reconstruction with biplanar x-rays: instead of
learning to invert the measurements, we leverage a 3d style-based generative
model to learn deep image priors of anatomic structures and optimize over the
latent space to match the input projections; (ii) a novel unsupervised method,
fast and robust to sampling ratio, source energy, angles and geometry of
projections, all of which making it general for downstream applications and
imaging systems.
figure 2 gives an overview of the pipeline we propose. we first learn the
lowdimensional manifold of ct volumes of a target body region. at inference, we
estimate the maximum a posteriori (map) volume on this manifold given very few
projections: we find the latent vectors that minimize the error between the
synthetic projections from the corresponding volume on the manifold and the real
ones. in this section, we formalize the problem, describe how we learn the
manifold, and detail how we optimize the latent vectors.
given a small set of projections {i i } i , possibly as few as two, we would
like to reconstruct the 3d tomographic volume v that generates these
projections. this is a hard ill-posed problem, and to solve it, we need prior
knowledge about the possible volumes. to do this, we look for the maximum a
posteriori (map) estimate given the projections {i i } i :(1) term l(v, i i ) is
a log-likelihood. we take it as:fig. 2. our pipeline. we first learn the
low-dimensional manifold of 3d structures using a generative model. then, given
projections, we find the latent vectors that minimize the error between the
projections of our generation and the input projections.where a i is an operator
that projects volume v under view i. we provide more details about operator a in
sect. 2.3. l p is the perceptual loss [13] between projection of v and the
observed projectionit is crucial as it is the term that embodies prior knowledge
about the volume to reconstruct. as discussed in the introduction, we rely on a
generative model, which we describe in the next section. then, we describe how
exactly we use this generative model for regularization term r(v) and how this
changes our optimization problem.
to regularize the domain space of solutions, we leverage a style-based
generative model to learn deep priors of anatomic structures. our model relies
on style-gan2 [15] that we extend in 3d by changing the 2d convolutions into 3d
ones as done in 3dstylegan [10] except that we start from the stylegan2
architecture.our generator g generates a volume v given a latent vector w and
gaussian noise vectors n = {n j } j : v = g(w, n). latent vector w ∈ n (w|μ, σ)
is computed from an initial latent vector z ∈ n (0, i ) mapped using a learned
network m: w = m(z). w controls the global structure of the predicted volumes at
different scales by its components w i , while the noise vectors n allow more
fine-grained details. the mean μ and standard deviation σ of the mapped latent
space can be computed by mapping over initial latent space n (0, i ) after
training. the mapping network learns to disentangle the initial latent space
relatively to semantic features which is crucial for the inverse problem. we
train this model using the non-saturating logistic loss [5] and path length
regularization [15]. for the discriminator, we use the non-saturating logistic
loss with r1 regularization [19]. we implement adaptive discriminator
augmentation from stylegan-ada [14] to improve learning of the model's manifold
with limited medical imaging data.
since our generative model provides a volume v as a function of vectors w and n,
we can reparameterize our optimization from eq. ( 1) into:note that by contrast
with [18] for example, we optimize on the noise vectors n as well: as we
discovered in our early experiments, the n are also useful to embed
high-resolution details. we take our regularization term r(w, n) as:term l w (w)
=k log n (w k |μ, σ) ensures that w lies on the same distribution as during
training. n (•|μ, σ) represents the density of the standard normal distribution
of mean μ and standard deviation σ.term l c (w) =i,j log m(θ i,j |0, κ)
encourages the w i vectors to be collinear so to keep the generation of
coarse-to-fine structures coherent. m(•; μ, κ) is the density of the von mises
distribution of mean μ and scale κ, which we take fixed, and θ i,j = arccos(
wi•wj wi wj ) is the angle between vectors w i and w j .term l n (n) =j log n (n
j |0, i ) ensures that the n j lie on the same distribution as during training,
i.e., a multivariate standard normal distribution. the λ * are fixed
weights.projection operator. in practice, we take operator a as a 3d cone beam
projection that simulates x-ray attenuation across the patient, adapted from
[21,27]. we model a realistic x-ray attenuation as a ray tracing projection
using material and spectrum awareness:with μ(m, e) the linear attenuation
coefficient of material m at energy state e that is known [11], t m the material
thickness, i 0 the intensity of the source x-ray.for materials, we consider the
bones and tissues that we separate by threshold on electron density. a inverts
the attenuation intensities i atten to generate an x-ray along few directions
successively. we make a differentiable using [21] to allow end-to-end
optimization for reconstruction.3 experiments and results
manifold learning. we trained our model with a large dataset of 3500 cts of
patients with head-and-neck cancer, more exactly 2297 patients from the publicly
available the cancer imaging archive (tcia) [1,6,16,17,28,32] and 1203 from
private internal data, after obtention of ethical approbations. we split this
data into 3000 cases for training, 250 for validation, and 250 for testing. we
focused ct scans on the head and neck region above shoulders, with a resolution
of 80 × 96 × 112, and centered on the mouth after automatic segmentation using a
pre-trained u-net [22]. the cts were preprocessed by min-max normalization after
clipping between -1024 and 2000 hounsfield units (hu).3d reconstruction. to
evaluate our approach, we used an external private cohort of 80 patients who had
undergone radiotherapy for head-and-neck cancer, with their consent. planning ct
scans were obtained for dose preparation, and cbct scans were obtained at each
treatment fraction for positioning with full gantry acquisition. as can be seen
in fig. 3 and the supplementary material, all these cases are challenging as
there are large changes between the original ct scan and the cbct scans. we
identified these cases automatically by comparing the cbcts with the planning
cts. to compare our reconstruction in the calibrated hu space, we registered the
planning cts on the cbcts by deformable registration with mrf minimization [4].
we hence obtained 3d volumes as virtual cts we considered as ground truths for
our reconstructions after normalization. from these volumes, we generated
projections using the projection module described in sect. 2.3.
manifold learning. we used pytorch to implement our model, based on style-gan2
[15]. it has a starting base layer of 256 × 5 × 6 × 7 and includes four
upsamplings with 3d convolutions and filter maps of 256, 128, 64, 32. we also
used 8 fully-convolutional layers with dimension 512 and an input latent vector
of dimension 512, with tanh function as output activation. to optimize our
model, we used lazy regularization [15] and style mixing [15], and added a 0.2
probability for generating images without gaussian noise to focus on embedding
the most information. we augmented the discriminator with vertical and
depthoriented flips, rotation, scaling, motion blur and gaussian noise at a
probability of 0.2. our training used mixed precision on a single gpu nvidia
geforce gtx 3090 with a batch size of 6, and we optimized the generator,
discriminator, and mapping networks using adam at learning rates 6e-5 and 1e-5
to avoid mode collapse and unstable training. after training for 4 weeks, we
achieved stabilization of the fréchet inception distance (fid) [9] and
multi-scale structural similarity (ms-ssim) [29] on the validation set.3d
reconstruction. for the reconstruction, we performed the optimization on gpu
v100 pci-e using adam, with learning rate of 1e-3. by grid search on the
validation set, we selected the best weights that well balance between structure
and fine-grained details, λ 2 = 10, λ p = 0.1, λ w = 0.1, λ c = 0.05, λ n = 10.
we perform 100 optimization steps starting from the mean of the mapped latent
space, which takes 25 s, enabling clinical use.
manifold learning. we tested our model's ability to learn the low-dimensional
manifold. we used fid [9] to measure the distance between the distribution of
generated volumes and real volumes, and ms-ssim [29] to evaluate volumes'
diversity and quality. we obtained a 3d fid of 46 and a ms-ssim of 0.92. for
reference, compared to 3dstylegan [10], our model achieved half their fid score
on another brain mri dataset, with comparable ms-ssim. this may be due to a more
complex architecture, discriminator augmentation, or simpler anatomy.baselines.
we compared our method against the main feed-forward method x2ct-gan [30] and
the neural radiance fields with prior image embedding method nerp [23] meant for
modest sparsely-sampled reconstruction. recent methods like [24] and [12] were
excluded because they provide only minor improvements compared to x2ct-gan [30]
and have similar constraints to feed-forward methods. additionally, no public
implementation is available. [26] uses a flow-based generative model, but the
results are of lower quality compared to gans and similar to x2ct-gan [30].3d
reconstruction. to evaluate our method's performance with biplanar projections,
we focused on positioning imaging for radiotherapy. figure 3 compares our
reconstruction with those of the baselines from biplanar projections. our method
achieves better fitting of the patient structure, including bones, tissues, and
air separations, almost matching the real ct volume. x2ct-gan [30] produced
realistic structures, but failed to match the actual structures as it does not
enforce consistency with the projections. in some clinical procedures, an
earlier ct volume of the patient may be available and can be used as an
additional input for nerp [23]. without a previous ct volume, nerp lacks the
necessary prior to accurately solve the ill-posed problem. even when initialised
with a previous ct volume, nerp often fails to converge to the correct volume
and introduces many artifacts when few projections are used. in contrast, our
method is more versatile and produces better results. we used quantitative
metrics (psnr and ssim) to evaluate reconstruction error and human perception,
respectively. table 1 shows these metrics for our method and baselines with 1 to
8 cone beam projections. deviation from projections, as in x2ct-gan, leads to
inaccurate reconstruction. however, relying solely on projection consistency is
inadequate for this ill-posed problem. nerp matches projections but cannot
reconstruct the volume correctly. our approach balances between instant and
iterative methods by providing a reconstruction in 25 s with 100 optimization
steps, while ensuring maximal consistency. in contrast, nerp requires 7 min, and
x2ct-gan produces structures instantly but unmatching. clinical cbct acquisition
and reconstruction by fdk [3] take about 1-2 min and 10 s respectively. our
approach significantly reduces clin-ical time and radiation dose by using
instant biplanar projections, making it promising for fast 3d visualization
towards complex positioning.
we proposed a new unsupervised method for 3d reconstruction from biplanar x-rays
using a deep generative model to learn the structure manifold and retrieve the
maximum a posteriori volume with the projections, leading to stateof-the-art
reconstruction. our approach is fast, robust, and applicable to various human
body parts, making it suitable for many clinical applications, including
positioning and visualization with reduced radiation.future hardware
improvements may increase resolution, and our approach could benefit from other
generative models like latent diffusion models. this approach may provide coarse
reconstructions for patients with rare abnormalities, as most learning methods,
but a larger dataset or developing a prior including tissue abnormalities could
improve robustness.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5_66.
magnetic resonance imaging (mri) and computed tomography (ct) are two commonly
used cross-sectional medical imaging techniques. mri and ct produce different
tissue contrast and are often used in tandem to provide complementary
information. while mri is useful for visualizing soft tissues (e.g. muscle, [20]
fails to preserve the smooth anatomy of the mri. (c) attentiongan [12] inflates
the head area in the synthetic ct, which is inconsistent with the original mri.
quantitative evaluations in mae (lower is better) are shown in yellow.fat), ct
is superior for visualizing bony structures. some medical procedures, such as
radiotherapy for brain tumors, craniosynostosis, and spinal surgery, typically
require both mri and ct for planning. unfortunately, ct imaging exposes patients
to ionizing radiation, which can damage dna and increase cancer risk [9],
especially in children and adolescents. given these issues, there are clear
advantages for synthesizing anatomically accurate ct data from mri.most
synthesis methods adopt supervised learning paradigms and train generative
models to synthesize ct [1][2][3]6,17]. despite the superior performance,
supervised methods require a large amount of paired data, which is prohibitively
expensive to acquire. several unsupervised mri-to-ct synthesis methods [4,6,14],
leverage cyclegan with cycle consistency supervision to eliminate the need for
paired data. unfortunately, the performance of unsupervised ct synthesis methods
[4,14,15] is inferior to supervised counterparts. due to the lack of direct
constraints on the synthetic outputs, cyclegan [20] struggles to preserve the
anatomical structure when synthesizing ct images, as shown in fig. 1(b). the
structural distortion in synthetic results exacerbates when data from the two
modalities are heavily misaligned, which usually occurs in pediatric scanning
due to the rapid growth in children.recent unsupervised methods impose
structural constraints on the synthesized ct through pixel-wise or shape-wise
consistency. pixel-wise consistency methods [8,14,15] capture and align
pixel-wise correlations between mri and synthesized ct. however, enforcing
pixel-wise consistency may introduce undesirable artifacts in the synthetic
results. this problem is particularly relevant in brain scanning, where both the
pixel-wise correlation and noise statistics in mr and ct images are different,
as a direct consequence of the signal acquisition technique. the alternative
shape-wise consistency methods [3,4,19] aim to preserve the shapes of major body
parts in the synthetic image. notably, shape-cyclegan [4] segments synthesized
ct and enforces consistency with the ground-truth mri segmentation. however,
these methods rely on segmentation annotations, which are time-consuming,
labor-intensive, and require expert radiological annotators. a recent natural
image synthesis approach, called attention-gan [12], learns attention masks to
identify discriminative structures. atten-tiongan implicitly learns prominent
structures in the image without using the ground-truth shape. unfortunately, the
lack of explicit mask supervision can lead to imprecise attention masks and, in
turn, produce inaccurate mappings of the anatomy, as shown in fig. 1(c). in this
paper, we propose maskgan, a novel unsupervised mri-to-ct synthesis method, that
preserves the anatomy under the explicit supervision of coarse masks without
using costly manual annotations. unlike segmentationbased methods [4,18],
maskgan bypasses the need for precise annotations, replacing them with standard
(unsupervised) image processing techniques, which can produce coarse anatomical
masks. such masks, although imperfect, provide sufficient cues for maskgan to
capture anatomical outlines and produce structurally consistent images. table 1
highlights our differences compared with previous shape-aware methods [4,12].
our major contributions are summarized as follows. 1) we introduce maskgan, a
novel unsupervised mri-to-ct synthesis method. maskgan is the first framework
that maintains shape consistency without relying on human-annotated
segmentation. 2) we present two new structural supervisions to enforce
consistent extraction of anatomical structures across mri and ct domains. 3)
extensive experiments show that our method outperforms state-of-the-art methods
by using automatically extracted coarse masks to effectively enhance structural
consistency.
in this section, we first introduce the maskgan architecture, shown in fig. 2,
and then describe the three supervision losses we use for optimization.
the network comprises two generators, each learning an mri-ct and a ct-mri
translation. our generator design has two branches, one for generating masks and
the other for synthesizing the content in the masked regions. the mask branch
learns n attention masks a i , where the first n -1 masks capture foreground
(fg) structures and the last mask represents the background (bg). the content
branch synthesizes n -1 outputs for the foreground structures, denoted as c.
each output, c i , represents the synthetic content for the corresponding
foreground region that is masked by the attention mask a i .intuitively, each
channel a i in the mask tensor a focuses on different anatomical structures in
the medical image. for instance, one channel emphasizes on synthesizing the
skull, while another focuses on the brain tissue. the last channel a n in a
corresponds to the background and is applied to the original input to preserve
the background contents. the final output is the sum of masked foreground
contents and masked background input. formally, the synthetic ct output
generated from the input mri x is defined asthe synthetic mri output from the ct
scan y is defined similarly based on the attention masks and the contents from
the mr generator. the proposed network is trained using three training
objectives described in the next sections.
the two generators, g mr and g ct , map images from mri domain (x) and ct domain
(y ), respectively. two discriminators, d mr and d ct , are used to distinguish
real from fake images in the mri and ct domains. the adversarial loss for
training the generators to produce synthetic ct images is defined as(2) the
adversarial loss l mr for generating mri images is defined in a similar manner.
for unsupervised training, cyclegan imposes the cycle consistency loss, which is
formulated as follows(3) the cyclegan's objective l gan is the combination of
adversarial and cycle consistency loss.
mask loss. to reduce spurious mappings in the background regions, maskgan
explicitly guides the mask generator to differentiate the foreground objects
from the background using mask supervision. we extract the coarse mask b using
basic image processing operations. specifically, we design a simple but robust
algorithm that works on both mri and ct scans, with a binarization stage
followed by a refinement step. in the binarization stage, we normalize the
intensity to the range [0, 1] and apply a binary threshold of 0.1, selected
based on histogram inspection, to separate the foreground from the background.
in the post-processing stage, we refine the binary image using morphological
operations, specifically employing a binary opening operation to remove small
artifacts. we perform connected component analysis [11] and keep the largest
component as the foreground. column 6 in fig. 3 shows examples of extracted
masks.we introduce a novel mask supervision loss that penalizes the difference
between the background mask a n learned from the input image and the groundtruth
background mask b in both mri and ct domains. the mask loss for the attention
generators is formulated asdiscussion. previous shape-aware methods [4,18] use a
pre-trained u-net [10] segmentation network to enforce shape consistency on the
generator. u-net is pre-trained in a separate stage and frozen when the
generator is trained. hence, any errors produced by the segmentation network
cannot be corrected. in contrast, we jointly train the shape extractor, i.e.,
the mask generator, and the content generator end-to-end. besides mask loss l
mask , the mask generator also receives supervision from adversarial loss l gan
to adjust the extracted shape and optimize the final synthetic results.
moreover, in contrast to previous methods that train a separate shape extractor,
our maskgan uses a shared encoder for mask and content generators, as
illustrated in fig. 2. our design embeds the extracted shape knowledge into the
content generator, thus improving the structural consistency of the synthetic
contents.cycle shape consistency loss. spurious mappings can occur when the
anatomy is shifted during translation. to preserve structural consistency across
domains, we introduce the cycle shape consistency (csc) loss as our secondary
contribution. our loss penalizes the discrepancy between the background
attention mask a mr n learned from the input mri image and the mask ãct n
learned from synthetic ct. enforcing consistency in both domains, we formulate
the shape consistency loss asthe final loss for maskgan is the sum of three loss
objectives weighted by the corresponding loss coefficients: l = l gan + λ mask l
mask + λ shape l shape .
data collection. we collected 270 volumetric t1-weighted mri and 267 thinslice
ct head scans with bony reconstruction performed in pediatric patients under
routine scanning protocols1 . we targeted the age group from 6-24 months since
pediatric patients are more susceptible to ionizing radiation and experience a
greater cancer risk (up to 24% increase) from radiation exposure [7].
furthermore, surgery for craniosynostosis, a birth defect in which the skull
bones fuse too early, typically occurs during this age [5,16]. the scans were
acquired by ingenia 3.0t mri scanners and philips brilliance 64 ct scanners. we
then resampled the volumetric scans to the same resolution of 1.0 × 1.0 × 1.0 mm
3 . the dataset comprises brain mr and ct volumes from 262 subjects. 13 mri-ct
volumes from the same patients that were captured less than three months apart
are registered using rigid registration algorithms. the dataset is divided into
249, 1 and 12 subjects for training, validating and testing set. following [13],
we conducted experiments on sagittal slices. each mr and ct volume consists of
180 to 200 slices, which are resized and padded to the size of 224 × 224. the
intensity range of ct is clipped into [-1000, 2000]. all models are trained
using the adam optimizer for 100 epochs, with a learning rate of 0.0002 which
linearly decays to zero over the last 50 epochs. we use a batch size of 16 and
train on two nvidia rtx 3090 gpus.evaluation metrics. to provide a quantitative
evaluation of methods, we compute the same standard performance metrics as in
previous works [6,14] including mean absolute error (mae), peak signal-to-noise
ratio (psnr), and structural similarity (ssim) between ground-truth and
synthesized ct. the scope of the paper centers on theoretical development;
clinical evaluations such as dose calculation and treatment planning will be
conducted in future work.
comparisons with state-of-the-art. we compare the performance of our proposed
maskgan with existing state-of-the-art image synthesis methods, including
cyclegan [20], attentiongan [12], structure-constrained cyclegan (sc-cyclegan)
[14] and shape-cyclegan [4]. shape-cyclegan requires annotated segmentation to
train a separate u-net. for a fair comparison, we implement shape-cyclegan using
our extracted coarse masks based on the authors' official code. note that
ct-to-mri synthesis is a secondary task supporting the primary mri-to-ct
synthesis task. as better mri synthesis leads to improved ct synthesis, we also
report the model's performance on mri synthesis. table 2. quantitative
comparison of different methods on the primary mri-ct task and the secondary
ct-mri task. the results of an ablated version of our proposed maskgan are also
reported. ± standard deviation is reported over five evaluations. the paired
t-test is conducted between maskgan and a compared method at p = 0.05. the
improvement of maskgan over all compared methods is statistically significant.
primary: mri-to-ct secondary:cyclegan [20] 32.12 ± 0.31 31.57 ± 0.12 46.17 ±
0.20 34.21 ± 0.33 29.88 ± 0.24 45.73 ± 0.17 attentiongan [12] 28.25 ± 0.25 32.88
± 0.09 53.57 ± 0.15 30.47 ± 0.22 30.15 ± 0.10 50.66 ± 0.14 sc-cyclegan [14]
24.55 ± 0.24 32.97 ± 0.07 57.08 ± 0.11 26.13 ± 0.15 31.22 ± 0.07 54.14 ± 0.10
shape-cyclegan [4] 24.30 ± 0.28 33. table 2 demonstrates that our proposed
maskgan outperforms existing methods for statistical significance of p = 0.05 in
both tasks. the method reduces the mae of cyclegan and attentiongan by 29.07%
and 19.36%, respectively. furthermore, maskgan outperforms shape-cyclegan,
reducing its mae by 11.28%. unlike shape-cyclegan, which underperforms when
trained with coarse segmentations, our method obtains consistently higher
results. figure 3 shows the visual results of different methods. sc-cyclegan
produces artifacts (e.g., the eye socket in the first sample and the nasal
cavity in the second sample), as it preserves pixel-wise correlations. in
contrast, our proposed maskgan preserves shape-wise consistency and produces the
smoothest synthetic ct. unlike adult datasets [4,14], pediatric datasets are
easily misaligned due to children's rapid growth between scans. under this
challenging setting, unpaired image synthesis can have non-optimal visual
results and ssim scores. yet, our maskgan achieves the highest quality,
indicating its suitability for pediatric image synthesis.we perform an ablation
study by removing the cycle shape consistency loss (w/o shape). compared with
shape-cyclegan, maskgan using only a mask loss significantly reduces mae by
6.26%. the combination of both mask and cycle shape consistency losses results
in the largest improvement, demonstrating the complementary contributions of our
two losses.robustness to error-prone coarse masks. we compare the performance of
our approach with shape-cyclegan [4] using deformed masks that simulate human
errors during annotation. to alter object shapes, we employ random elastic
deformation, a standard data augmentation technique [10] that applies random
displacement vectors to objects. the level of distortion is controlled by the
standard deviation of the normal distribution from which the vectors are
sampled. figure 4 (left) shows mae of the two methods under increasing levels of
distortion. mae of shape-cyclegan drastically increases as the masks become more
distorted. figure 4 (right) shows that our maskgan (d) better preserves the
anatomy.
this paper proposes maskgan -a novel automated framework that maintains the
shape consistency of prominent anatomical structures without relying on expert
annotated segmentations. our method generates a coarse mask outlining the shape
of the main anatomy and synthesizes the contents for the masked foreground
region. experimental results on a clinical dataset show that maskgan
significantly outperforms existing methods and produces synthetic ct with more
consistent mappings of anatomical structures.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 6.
x-ray computed tomography (ct) is an established diagnostic tool in clinical
practice; however, there is growing concern regarding the increased risk of
cancer induction associated with x-ray radiation exposure [14]. lowering the
dose of ct scans has been widely adopted in clinical practice to address this
issue, following the "as low as reasonably achievable" (alara) principle in the
medical community [9]. sparse-view ct is one of the effective solutions, which
reduces the radiation by only sampling part of the projection data for image
reconstruction. nevertheless, images reconstructed by the conventional filtered
back-projection (fbp) present severe artifacts, thereby compromising their
clinical value.in recent years, the success of deep learning has attracted much
attention in the field of sparse-view ct reconstruction. existing learning-based
approaches mainly include image-domain methods [2,4,18] and dual-domain ones
[7,13,16], both involving image post-processing to restore a clean ct image from
the low-quality one with streak artifacts. for the image post-processing,
residual learning [3] is often employed to encourage learning the artifacts
hidden in the residues, which has become a proven paradigm for enhancing the
performance [2,4,6,16]. unfortunately, existing image post-processing methods
may fail to model the globally distributed artifacts within the image domain.
they can also produce over-smoothed images due to the lack of differentiated
supervision for each pixel. in this paper, we advance image post-processing to
benefit both classical image-domain methods and the dominant dual-domain
ones.motivation. we view the sparse-view ct image reconstruction as a two-step
task: artifact removal and detail recovery. for the former, few work has
investigated the fact that the artifacts exhibit similar pattern across
different sparseview scenarios, which is evident in fourier domain as shown in
fig. 1: they are aggregated mainly in the mid-frequency band and gradually
migrate from low to high frequencies as the number of views increases. inspired
by this, we propose a frequency-band-aware artifact modeling network (freenet)
that learns the artifact-concentrated frequency components to remove the
artifacts efficiently using learnable band-pass attention maps in the fourier
domain.while fourier domain band-pass maps help capture the pattern of the
artifacts, restoring the image detail contaminated by strong artifacts may still
be difficult due to the entanglement of artifacts and details in the residues.
consequently, we propose a self-guided artifact refinement network (seednet)
that provides supervision signals to aid freenet in refining the image details
contaminated by the artifacts. with these novel designs, we introduce a simple
yet effective model termed frequency-band-aware and self-guided network
(freeseed), which enhances the reconstruction by modeling the pattern of
artifacts from a frequency perspective and utilizing the artifact to restore the
details. freeseed achieves promising results with only image data and can be
further enhanced once the sinogram is available.our contributions can be
summarized as follows: 1) a novel frequency-bandaware network is introduced to
efficiently capture the pattern of global artifacts in the fourier domain among
different sparse-view scenarios; 2) to promote the restoration of heavily
corrupted image detail, we propose a self-guided artifact refinement network
that ensures targeted refinement of the reconstructed image and consistently
improves the model performance across different scenarios; and 3) quantitative
and qualitative results demonstrate the superiority of freeseed over the
state-of-the-art sparse-view ct reconstruction methods.
given a sparse-view sinogram with projection views n v , let i s and i f denote
the directly reconstructed sparse-and full-view images by fbp, respectively. in
this paper, we aim to construct an image-domain model to effectively recover i s
with a level of quality close to i f . the proposed framework of freeseed is
depicted in fig. 2, which mainly consists of two designs: freenet that learns to
remove the artifact and is built with band-pass fourier convolution blocks that
better capture the pattern of the artifact in fourier domain; and seednet as a
proxy module that enables freenet to refine the image detail under the guidance
of the predicted artifact. note that seednet is involved only in the training
phase, additional computational cost will not be introduced in the application.
the parameters of freenet and seednet in freeseed are updated in an iterative
fashion.
to learn the globally distributed artifact, freenet uses band-pass fourier
convolution blocks as the basic unit to encode artifacts from both spatial and
frequency aspects. technically, fourier domain knowledge is introduced by fast
fourier convolution (ffc) [1], which benefits from the non-local receptive field
and has shown promising results in various computer vision tasks [12,17]. the
features fed into ffc are split evenly along the channel into a spatial branch
composed of vanilla convolutions and a spectral branch that applies convolution
after real fourier transform, as shown in fig. 2. despite the effectiveness, a
simple fourier unit in ffc could still preserve some low-frequency information
that may interfere with the learning of artifacts, which could fail to
accurately capture the banded pattern of the features of sparse-view artifacts
in the frequency domain. to this end, we propose to incorporate learnable
band-pass attention maps into ffc. given an input spatial-domain feature map x
in ∈ r cin×h×w , the output x out ∈ r cout×h×w through the fourier unit with
learnable band-pass attention maps is obtained as follows:where f real and f -1
real denote the real fourier transform and its inverse version, respectively. f
denotes vanilla convolution. " " is the hadamard product. specifically, for c-th
channel frequency domain feature z (c) in ∈ c u ×v (c = 1, ..., c in ), the
corresponding band-pass attention map h (c) ∈ r u ×v is defined by the following
gaussian transfer function:where d (c) is the c-th channel of the normalized
distance map with entries denoting the distance from any point (u, v) to the
origin. two learnable parameters, w (c) > 0 and d, represent the bandwidth and
the normalized inner radius of the band-pass map, respectively, and are
initialized as 1 and 0, respectively. is set to 1 × 10 -12 to avoid division by
zero. the right half part of the second row of fig. 1 shows some samples of the
band-pass maps.the pixel-wise difference between the predicted artifact a of
freenet and the groundtruth artifact a f = i s -i f is measured by 2 loss:(4)
areas heavily obscured by the artifact should be given more attention, which is
hard to achieve using only freenet. therefore, we propose a proxy network
seednet that provides supervision signals to focus freenet on refining the
clinical detail contaminated by the artifact under the guidance of the artifact
itself. seednet consists of residual fourier convolution blocks. concretely,
given sparseview ct images i s , freenet predicts the artifact a and restored
image i = i s -a; the latter is fed into seednet to produce targeted refined
result i. to guide the network on refining the image detail obscured by heavy
artifacts, we design the transformation t that turns a into a mask m using its
mean value as threshold: m = t ( a), and define the following masked loss for
seednet:(5)
freenet and seednet in our proposed freeseed are trained in an iterative
fashion, where seednet is updated using l mask defined in eq. ( 5), and freenet
is trained under the guidance of the total loss:where α > 0 is empirically set
as 1. the pseudo-code for the training process and the exploration on the
selection of α can be found in our supplementary material.once the training is
complete, seednet can be dropped and the prediction is done by freenet.
dual-domain methods are effective in the task of sparse-view ct reconstruction
when the sinogram data are available. to further enhance the image
reconstruction quality, we extend freeseed to the dominant dual-domain framework
by adding the sinogram-domain sub-network from dudonet [7], where the resulting
dual-domain counterpart shown in fig. 3 is called freeseed dudo . the
sinogramdomain sub-network involves a mask u-net that takes in the linearly
interpolated sparse sinogram s s , where a binary sinogram mask m s that
outlines the unseen part of the sparse-view sinogram is concatenated to each
stage of the u-net encoder. the mask u-net is trained using sinogram loss l sino
and radon consistency loss l rc . we refer the readers to lin et al. [7] for
more information.
we conduct experiments on the dataset of "the 2016 nih-aapm mayo clinic low dose
ct grand challenge" [8], which contains 5,936 ct slices in 1 mm image thickness
from 10 anonymous patients, where a total of 5,410 slices from 9 patients,
resized to 256 × 256 resolution, are randomly selected for training and the 526
slices from the remaining one patient for testing without patient overlap.
fan-beam ct projection under 120 kvp and 500 ma is simulated using torchradon
toolbox [11]. specifying the distance from the x-ray source to the rotation
center as 59.5 cm and the number of detectors as 672, we generate sinograms from
full-dose images with multiple sparse views n v ∈ {18, 36, 72, 144} uniformly
sampled from full 720 views covering [0, 2π].the models are implemented in
pytorch [10] and are trained for 30 epochs with a mini-batch size of 2, using
adam optimizer [5] with (β 1 , β 2 ) = (0.5, 0.999) and a learning rate that
starts from 10 -4 and is halved every 10 epochs. experiments are conducted on a
single nvidia v100 gpu using the same setting. all sparse-view ct reconstruction
methods are evaluated quantitatively in terms of root mean squared error (rmse),
peak signal-to-noise ratio (psnr), and structural similarity (ssim) [15].
we compare our models (freeseed and freeseed dudo ) with the following
reconstruction methods: direct fbp, ddnet [18], fbpconv [4], dudonet [7], and
dudotrans [13]. fbpconv and ddnet are image-domain methods, while dudonet and
dudotrans are state-of-the-art dual-domain methods effective for ct image
reconstruction. table 1 shows the quantitative evaluation.not surprisingly, we
find that the performance of conventional image-domain methods is inferior to
the state-of-the-art dual-domain method, mainly due to the failure of removing
the global artifacts. we notice that dual-domain methods underperform fbpconv
when n v = 18 because of the secondary artifact induced by the inaccurate
sinogram restoration in the ultra-sparse scenario. notably, freeseed outperforms
the dual-domain methods in most scenarios. figure 4 provides the visualization
results for different methods. in general, freeseed successfully restores the
tiny clinical structures (the spines in the first row, and the ribs in the
second row) while achieving more comprehensive artifact removal (see the third
row). note that when the sinogram data are available, dual-domain counterpart
freeseed dudo gains further improvements, showing the great flexibility of our
model. 6) freenet trained with l mask using 1 norm (freeseed 1 ); and (7)
freenet trained with l mask using 2 norm, i.e., the full version of our model
(freeseed).
by comparing the first two rows of table 2, we find that simply applying ffc
provides limited performance gains. interestingly, we observe that the advantage
of band-pass attention becomes more pronounced given more views, which can be
seen in the last row of fig. 1 where the attention maps are visualized by
averaging all inner radii and bandwidths in different stages of freenet and
calculating the map following eq. ( 2). figure 1 shows that these maps
successfully capture the banded pattern of the artifact, especially in the cases
of n v = 36, 72, 144 where artifacts are less entangled with the image content
and present a banded shape in the frequency domain. thus, the band-pass
attention maps lead to better convergence. the effectiveness of seednet can be
seen by comparing rows (1) and (3) and also rows ( 4) and (7). both the baseline
and freenet can benefit from the seednet supervision. visually, clinical details
in the image that are obscured by the heavy artifacts can be further refined by
freenet; please refer to fig. s1 in our supplementary material for more examples
and ablation study. we also find that freenet 1+mask does not provide stable
performance gains, probably because directly applying a mask on the pixel-wise
loss leads to the discontinuous gradient that brings about sub-optimal results,
which, however, can be circumvented with the guidance of seednet. in addition,
we trained freeseed with eq. ( 6) using 1 norm. from the last two rows in table
2 we find that 1 norm does not ensure stable performance gains when ffc is used.
in this paper, we proposed freeseed, a simple yet effective image-domain method
for sparse-view ct reconstruction. freeseed incorporates fourier knowledge into
the reconstruction network with learnable band-pass attention for a better grasp
of the globally distributed artifacts, and is trained using a self-guided
artifact refinement network to further refine the heavily damaged image details.
extensive experiments show that both freeseed and its dual-domain counterpart
outperformed the state-of-the-art methods. in future, we will explore ffc-based
network for sinogram interpolation in sparse-view ct reconstruction.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 24.
image registration has been widely studied in both academia and industry over
the past two decades. in general, the goal of deformable image registration is
to estimate a suitable nonlinear transformation that overlaps the pair of images
with corresponding spatial relationships [4,22]. this goal is usually achieved
by minimizing a well-defined similarity score. however, these methods often
assume that there is no spatial non-correspondence between the two images. in
the field of medical image analysis, this assumption is often not valid,
particularly in cases such as pathology image to atlas registration or
pre-operative and post-operative longitudinal registration. direct registration
of pathology images without taking into account the impact of focal tissue can
result in missed pixel-level correspondence and large registration errors.a
variety of approaches have been proposed to handle the noncorrespondence problem
in medical image registration. these methods can be roughly divided into three
main categories: 1) cost function masking. the authors of [5,17] used the
segmentation of the non-corresponding regions to mask the image similarity
measure in optimization. 2) converting pathological image to normal appearance.
this class of approaches aims to replace or reconstruct the focal area as normal
tissue to guide the registration either through low-rank and sparse image
decomposition [10,11] or generative models [23]. 3) non-correspondence detection
via intensity criteria. this category of methods can be formulated as joint
segmentation and registration to detect non-corresponding regions during the
registration process [6,7]. although these approaches partially handle the issue
of non-correspondence in the registration, they still have some serious
shortcomings. the cost function masking and image conversion approaches require
ground truth or accurate labels during registration and may decrease the
alignment accuracy when the focal area is large. the non-correspondence
detection approach, which typically relies on a sophisticated designed loss
function, is very sensitive to the dataset [1] and difficult to find a set of
unified parameters.therefore, to effectively address the non-correspondence
problem in registering pathology images, it is necessary to incorporate both a
data-independent segmentation module and a modality-adaptive inpainting module
into the registration pipeline. to bridge this gap, we introduce the semantic
information of the category based on [21,24]. it employs the non-correspondence
in registration to achieve accurate segmentation of the lesion region and uses
the segmented mask to reconstruct the lesion area and guide the registration. in
this paper, we address the challenge of large alignment errors due to the loss
of spatial correspondence in processing pathological images. to overcome this
challenge, we propose girnet, a tri-net collaborative learning framework that
simultaneously updates the segmentation, inpainting, and registration networks.
the segmentation network minimizes the mutual information between the lesion and
normal tissue based on the semantic information introduced by the registration
network, allowing for accurate segmentation of regions with missing spatial
correspondence. the registration network, in turn, weakens the adverse effects
of the lesions based on the mask generated by the segmentation network. to the
best of our knowledge, this is the first work to apply an unsupervised
segmentation method based on minimal mutual information (mmi) to pathological
image registration, with simultaneous training of segmentation and registration.
our work makes the following key contributions.-we propose a collaborative
learning method for the simultaneous optimization of registration, segmentation,
and inpainting networks.-we show the effectiveness of using mutual information
minimization in an unsupervised manner for pathological image segmentation and
registration by incorporating semantic information through the registration
process. -we perform a series of experiments to validate our method's
superiority in accurately finding lesions and effectively registering
pathological images.
our proposed framework (fig. 1) involves three modules: a register denoted by ψ,
a segmenter denoted by θ, and an inpainter denoted by φ. the three modules are
trained in a co-learning manner to enable the registration aware of semantic
information. importantly, our proposed training procedure is fully unsupervised
which does not require any labeled data for training the network.
the most critical problem in pathological image registration is identifying and
dealing with the lesion area. if we naively register a source pathological image
s to a template t without caring about the lesion boundary, the deformation
field near the boundary would be uncontrollable because a healthy template does
not have a lesion. a possible approach here is to initialize an inflating
boundary containing the lesion area, followed by calculating the registration
loss either outside of the boundary only or based on a modified s that is
inpainted within the given boundary. however, the registration error has no
sensitivity to the location of the inflated boundary as long as it is larger
than the real one. on the other hand, if we compared the inpainted image and the
pathological image s within the boundary only, we can notice that their
dissimilarity increases when the boundary shrinks as the inpainting algorithm
only generates healthy parts. this mechanism can then induce a segmentation
module that segments the lesion as the foreground and the remaining as the
background, which iteratively serves as the input mask for the inpainting
module. further, as the registration loss is calculated based on the registered
inpainted image and the target image, the registration provides a regularization
for the inpainting module such that the inpainting is specialized to facilitate
the registration. specially for the input and output of the three modules,
regnet takes images s and t as input and generates the deformation field from s
to t and t to s as ϕ st and ϕ t s respectively. inpnet takes the background
(foreground) cropped by segnet and image t •ϕ t s warped by regnet as input and
outputs foreground (background) with a normal appearance. segnet takes the
pathology image s as input and employs the normal foreground and background
inpainted by inpnet to segment the lesion region based on mmi. segnet and inpnet
are actually in an adversarial relationship. through this joint optimization
approach, the three networks collectively work to achieve registration and
segmentation of pathological images under entirely unsupervised conditions,
without being limited by the specific network structure. for the sake of
simplicity, we employ a unet-like [20] basic structure without any normalization
layer.
regnet. the primary objective of registration is to generate a deformation field
that minimizes the dissimilarity between the source image (s) and the template
image (t). the deformation is usually required to satisfy constraints like
smoothness and even diffeomorphism. in terms of pathological image registration,
the deformation field is only valid off the lesion area. thus the registration
loss should be calculated on the normal area only. suppose that the lesion area
is already obtained as θ(s) and inpainted with normal tissue, the registration
loss can then be formulated aswhere ϕ st = ψ(s, t ), ϕ t s = ψ(t, s) are the
deformation fields that warp s → t and t → s respectively. the symbol • denotes
element-wise multiplication. furthermore, l sym denotes the registration loss of
symnet [14], which aims to balance the losses of orientation consistency,
regularization and magnitude.segnet. minimal mutual information (mmi) is a
typically used unsupervised segmentation method that distinguishes foreground
from background. however, for a pathological image, the lesion regions often
have a similar intensity to normal tissues near the boundary, which prevents the
mmi from accurate segmentation without the semantic information. to address this
limitation, we warp a healthy image t onto a pathology image s using a
deformation field ϕ t s = ψ(t, s). this process maximizes the mutual information
between corresponding regions of the two images and minimizes that of
non-corresponding regions, thereby facilitating accessible lesion segmentation
with mmi. let ω ∈ r denote the image domain, m denote the mask, f θ = ω•m and b
θ = ω•m denote the foreground and background, where m = 1 -m, m ∈ {0, 1}.
regarding a pathological image s, when the background (normal) is given, the
inpainted foreground (normal) will be different from the true foreground
(lesion). when the foreground (lesion) is given, the inpainted background will
remain the same as the background (normal). thus we can formulate the
adversarial loss of unsupervised segmentation aswhere d is the distance function
given by localized normalized cross-correlation (lncc) [3]. appendix a provides
a detailed derivation.inpnet. let m denote the mask and ϕ t s denote the
deformation field from t to s. to handle the potential domain differences
between the masked image s •m and the aligned image t •ϕ t s , inpnet employs
two encoders. the adversarial loss function of inpnet is represented as l mi .
to incorporate semantic information, we include an additional similarity term l
sim that prevents inpnet from focusing too heavily on the foreground (lesion)
and encourages it to produce healthy tissue. the proposed loss function l inp is
then formulated as the combination of mutual information loss defined through
the normalized correlation coefficient (ncc) and similarity loss through the
mean squared error (mse):withwhere λ represents the weight that balances the
contributions of mutual information loss and similarity loss, and t m denotes
image t after histogram matching. we modify the histogram of t •ϕ t s to be
similar to that of s in order to mitigate the effects of domain differences.
our experimental design focuses on two common clinical tasks: atlas-based
registration, which involves warping pathology images to a standard atlas
template, and longitudinal registration, which involves registering
pre-operative images to post-operative images for the purpose of tracking
changes over time.dataset and pre-processing. for our study, we selected the
icbm 152 nonlinear symmetric template as our atlas [9]. we reoriented all mri
scans of the t1 sequence to the ras orientation with a resolution of 1 mm × 1 mm
× 1 mm and align the images to atlas using the mri robust register tool in
freesurfer [19]. we then cropped the resulting mri scans to a size of 160 × 192
× 144, without any image augmentation. to evaluate our approach, we employed a
5-fold cross-validation method and divided our data into training and test sets
in an 8:2 ratio. 3d brain mri. oasis-1 [12] includes 416 cross-sectional mri
scans from individuals aged 18 to 96, with 100 of them diagnosed with mild to
moderate alzheimer's disease. brats2020 [13] provides 369 expert-labeled
pre-operative mri scans of glioblastomas and low-grade gliomas, acquired from
multiple institutions for routine clinical use.
to evaluate the performance of atlas-based registration, it is essential to have
the correct mapping of pathological regions to healthy brain regions. to create
such a mapping, we created a pseudo dataset by utilizing images from the oasis-1
and brats2020. from the resulting t1 sequences, a pseudo dataset of 300 images
was randomly selected for further analysis. appendix b provides a detailed
process for creating the pseudo dataset.real data with landmarks. brats-reg 2022
[2] provides extensive annotations of landmarks points within both the
pre-operative and the follow-up scans that have been generated by clinical
experts. a total of 140 images are provided, of which 112 are for training, and
28 for testing. comparison to pathology registration. we compared our method
(gir-net) with competitive algorithms: 1) three cutting-edge deep learning-based
unsupervised deformable registration approaches: voxelmorph [3], voxelmorph-df
[8] and symnet [14]. 2) two unsupervised deformable registration methods for
pathological images: dramms [18] and dirac [16]. dramms is an optimization-based
method that reduces the impact of non-corresponding regions. dirac jointly
estimates regions with absent correspondence and bidirectional deformation
fields and ranked first in the bratsreg2022 challenge.atlas-based registration.
after creating the pseudo dataset, we warped brain mr images without tumors to
the atlas and used the resulting deformation field as the gold standard for
evaluation. we then evaluated the mean deformation error (mde) [10], which is
calculated as the average euclidean distance between the coordinates of the
deformation field and the gold standard within specific regions of interest.
these regions include: 1) the tumor region. 2) the normal region near the tumor
(within 30 voxels). 3) the normal region far from the tumor (over 30 voxels but
within brain tissue). our results, presented in fig. 2, show that our method
with histogram matching (hm) outperforms other methods in all three regions,
particularly in the normal regions (near and far). by utilizing hm, our network
achieves an mde of less than 1 mm compared to the gold standard deformations.
these results demonstrate the effectiveness of our method in differentiating the
impact of pathology in atlas-based registration tasks. specifically, dirac is
unable to eliminate the influence of domain differences and resulting in the
largest registration error among the evaluated methods.longitudinal
registration. to perform the longitudinal registration task, we registered each
pre-operative scan to the corresponding follow-up scan of the same patient and
measured the mean target registration error (tre) of the paired landmarks using
the resulting deformation field. for this purpose, we leveraged segnet, trained
on brats2020, to segment the tumor of brat-sreg2022 and separated the landmarks
into two regions: near tumor and far from tumor. figure 3 shows the mean tre for
the various registration approaches. in our proposed framework, we replaced
regnet with cir-dm [15] (denoted as gir(cirdm)) without the need for supervised
training or pretraining, and achieved comparable performance with the
state-of-the-art method dirac. moreover, our gir approach outperforms other deep
learning-based methods and achieved accurate segmentation of pathological
images.to quantitatively evaluate the segmentation capability of our proposed
framework, we compared its performance with other unsupervised segmentation
techniques methods, including unsupervised clustering toolbox aucseg [25], joint
non-correspondence segmentation and registration method ncrnet [1], and dirac.
we used the mean dice similarity coefficient (dsc) to evaluate the similarity
between predicted masks and the ground truth. as shown in table 1, aucseg fails
to detect the lesion in t1 scans. our proposed framework achieved the highest
dsc result of 0.83, following post-processing.ablation study. we compared the
performance of the inpnet trained with histogram matching (hm) and the segnet
trained with ground truth masks (supervised). the results, shown in table 1 and
fig. 2, demonstrate that domain differences between s and t have a significant
effect on segmentation accuracy (without hm), leading to lower registration
quality overall. additionally, fig. 4 shows an example of a pseudo image. we
reconstructed the spatial correspondence by first using segnet to localize the
lesion and then using inpnet to inpaint it with the normal appearance.
in this paper, we proposed a novel tri-net framework for joint image
registration and unsupervised segmentation in medical imaging based on mutual
information minimization in collaborative learning. our experiments demonstrate
that the proposed framework is effective for both atlas-based and longitudinal
pathology image registration. we also observed that the accuracy of the
segmentation network is significantly influenced by the quality of the
inpainting, which, in turn, affects the registration outcome. in the future, our
research will focus on enhancing the performance of inpnet to address domain
differences better to improve the registration results.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 51.
positron emission tomography (pet) has been widely used in human brain imaging,
thanks to the availability of a vast array of specific radiotracers. these
compounds allow for studying various neurotransmitters and receptor dynamics for
different brain targets [11]. brain pet images are commonly used to diagnose and
monitor neurodegenerative diseases, such as alzheimer's disease, parkinson's
disease, epilepsy, and certain types of brain tumors [3]. head motion in pet
imaging reduces brain image resolution, lowers tracer distribution estimation,
and introduces attenuation correction (ac) mismatch artifacts [12].
consequently, the capability to monitor and correct head motion is of utmost
importance in brain pet studies.the first step of pet head motion correction is
motion tracking. when head motion information is acquired, either frame-based
motion correction or eventby-event (ebe) motion correction methods can be
applied in the reconstruction workflow to derive motion-free pet images. ebe
motion correction provides better results for real-time motion tracking compared
to frame-based methods, as the latter does not allow for correction of motion
that occurs within each dynamic frame [1]. currently, there are two main
categories of head motion tracking methods, hardware-based motion tracking (hmt)
and data-driven methods. for hmt, head motion is obtained from external devices.
generally, hmt systems offer accurate tracking results with high time
resolution. marker-based hmt such as polaris vicra (ndi, canada) use
light-reflecting markers on the patient's head and track the markers for motion
correction [6]. however, vicra is not routinely used in the clinic, as setup and
calibration of the tracking device can be complicated and attaching markers to
each patient increases the logistical burden of the scan. in response, some
researchers began to use markerless motion tracking systems for brain pet
[4,13]. these methods typically rely on the use of cameras and computer vision
algorithms to detect and analyze the movement of a person's head in real-time,
but these methods still require additional hardware setup. in data-driven motion
tracking methods, head motion is estimated from pet reconstructions or raw data.
with the development of commercial pet systems and technological advancements
such as time of flight (tof), data-driven head pet motion tracking has shown
promising results in reducing motion artifacts and improving image quality. for
instance, [12] developed a novel data-driven head motion detection method based
on the centroid of distribution (cod) of pet 3d point cloud image (pci). image
registration methods that seek to align two or more images offer a data-driven
solution for correcting head motion. intensity-based registration methods have
been used to track head motion using good-quality pet reconstruction frames to
achieve stable performance [14]. however, because of the dynamic change in pet
images, current registration-based methods need to split the data into several
discrete time frames, e.g., 5 min. therefore, they will introduce a cumulative
error when dealing with inter-frame motion. finally, inspired by the development
of deep learning-based registration methods, a deep learning head motion
correction (dl-hmc) network using vicra as ground truth was proposed [15]. this
study achieved accurate motion tracking on single subject testing data, but
showed less accurate motion predictions for multi-subject motion studies.
meanwhile, the input images were low-resolution pcis without tof and had large
voxel spacing, which can negatively affect motion tracking accuracy.in this
study, we proposed a new method to perform deep learning-based brain pet motion
prediction across multiple subjects by utilizing high-resolution one-second fast
reconstruction images (fris) with tof. a novel encoder and data augmentation
strategy was also applied to improve model performance. ablation studies were
conducted to assess the individual contributions of key method components.
multi-subject studies were conducted on a dataset of 20 subject and its results
were quantitatively and qualitatively evaluated by molar reconstruction studies
and corresponding brain region of interest (roi) standard uptake values (suv)
evaluation.
we identified 20 18 f-fpeb studies from a database of brain pet scans acquired
on a conventional mct scanner (siemens, germany) at the yale pet center. all
subjects are healthy controls and the mean activity injected was 3.90 ± 1.02
mci. the mean translational and rotational motions across time and scans are
3.75 ± 6.88 mm and 3.30 ± 8.77 • , respectively. pet list-mode data and vicra
motion tracking information are available for each subject, as well as
t1-weighted mr images and mr-space to pet-space transformation matrices. we
consider data acquired between 60 and 90 min post injection (30 min total).
to overcome the challenges when using low-quality, noisy pci for motion
correction, ultra-fast reconstruction techniques [14] that generate one-second
dynamic fast reconstruction images (fris), can be utilized as input for deep
learning motion correction methods. leveraging the availability of cpu-parallel
reconstruction platforms [5], we develop a reconstruction package for one-second
fri. our proposed method employs a tof-based projector and utilizes pure
maximum-likelihood expectation maximization (mlem) for reconstruction [10].
attenuation correction (ac) and scatter correction are turned off to avoid ac
mismatch and expensive computation. normalization correction, random correction
and decay correction are applied and the iteration number was set to two.
standard uptake value (suv) calculation was also conducted to normalize the
activities between different subjects. the final reconstructed image dimension
is 150 × 150 × 111, with voxels spaced at 2.04 × 2.04 × 2.00 mm 3 . for
comparison purposes, we also computed the same resolution pci by tof
back-projection of the pet list-mode data along the line-of-response (lor) with
normalization for scanner sensitivity. both fri and pci were resized to 96 × 96
× 64, with voxel spacing of 3.18 × 3.18 × 3.45 mm 3 . as shown in fig. 1, the
resized fri and pci from the same time pairs are displayed. due to the pet
corrections, the fri quality and noise level is superior to the pci,
particularly in areas outside of the head.
network architecture. we propose a modified version of the dl-hmc framework to
learn rigid head motion in a supervised manner [15] (fig. 1). our proposed
method uses two fris i ref and i mov from two different time points t ref and t
mov to predict the relative rigid motion transformation between the reference
and moving time points with respect to the vicra gold-standard. our encoder
consists of 3 convolution layers with kernel size of 5 3 and an intermediate
convolution layer with convolution size of 3 3 . prelu activation layers follow
each convolution. in addition, we add dropout in the regression layers with rate
0.3. in this new architecture, the embedding space consists of feature maps of
size 16 × 4 × 4 × 2, which preserves the spatial information in the fri. no
padding is applied to the images or feature maps. the extracted features are fed
into the fully connected regression block to predict the six translation and
rotation components of the relative rigid motion. data augmentation. to improve
the performance and generalizability of our network, we use a task-specific data
augmentation strategy to expose it to more varied and diverse training data. as
a rule of thumb, translations of 2-5 mm and rotations of 2 • -3 • are common and
larger magnitudes are expected without restraint or if non-customized supports
are used [9]. due to our sampling strategy during model training, statistically,
most of the sampled pairs will have small relative motion. however, during the
inference, the relative motion between the moving frames at late time points and
the reference frame at the beginning will be large due to the accumulation of
motion. therefore, the model may not be able to make accurate predictions when
facing large relative motions. to take this problem into account, we perform
data augmentation by simulating an additional relative motion that can be
concatenated with the true relative motion. to be specific, the synthetic
translation and rotation are uniformly sampled in the range of [-10, 10] mm and
[-5, 5] • , respectively. the randomly simulated motion t will be applied to the
moving frame to generate a synthetic moving frame t • i ref and be concatenated
with the real relative motion to acquire the synthetic relative motion between
the reference and the synthetic moving frame. the synthetic moving frame and the
synthetic relative motion will be used for training to increase the data
variability.network training and inference. to train the network, we randomly
sampled image pairs (t ref , t mov ) under the condition (t ref < t mov ). the
network was optimized by minimizing the mean square error (mse) between the
predicted motion estimate and vicra parameters. more specifically, the
prediction error for a given pair of reference and moving clouds is defined as
l( θ, θ) = θθ 2 with θ = [t x , t y , t z , r x , r y , r z ] the vicra
information for the three translational and three rotational parameters (t x , t
y , t z ) and (r x , r y , r z ), respectively, and θ the network prediction.
after training the model, we perform motion tracking inference by setting the
image from first time point t ref = 3,600 (60 min post-injection) as the
reference image and predict the motion from this reference image to all
subsequent one-second image frames in the next 30 min (1,800 one-second time
points).
we performed quantitative and qualitative experiments to validate our approach.
we evaluated motion correction performance by comparing our proposed method to
dl-hmc [15], intensity-based registration using the bioimage suite (bis)
software package [7] using the one-second fris, and ablation studies to
demonstrate the effectiveness of our design choices. we qualitatively assessed
motion correction performance by reconstructing the pet images with motion
tracking result and comparing to dl-hmc and the vicra gold-standard
reconstruction results. we split our dataset of 20 subjects into distinct
subsets for training and testing with 14 and 6 subjects, respectively. from the
training cohort, we randomly sampled 10% of the time frames to be used as a
validation set. training the network required 6,000 epochs for convergence using
a minibatch size of 64. adam optimization was used with initial learning rate
set to 5e-4, γ set to 0.98, and exponential decay with a step size of 150. all
computations were performed on a server with intel xeon gold 5218 processors,
256 gb ram, and an nvidia quadro rtx 8000 gpu (48 gb ram). the network was
implemented in python (v3.9) using pytorch (v1.13.1) and monai (v1.0.1).table 1.
quantitative motion correction results. we compared our proposed approach using
fast reconstruction image (fri) as input with dl-hmc and with using point cloud
image (pci) as input. an ablation study quantifies the effect of stochastic data
augmentation (da) and to standard intensity-based registration (bis). reported
values are mse (mean ± sd) comparing motion estimates to vicra gold-standard.
val. loss test set total loss translation (mm) rotation ( quantitative
evaluation. for quantitative comparisons of motion tracking, we compare mse loss
in the validation set and test set. we calculate mse for the 6 parameter rigid
motion as well as the translation and rotational components separately. to
verify feasibility of traditional intensity-based registration method on fris,
we use bis with a multi-resolution hierarchical representation (3 levels) and
minimize the sum of squared differences (ssd) similarity metric (fig. s1 shows a
bis result on a example testing subject). compared to vicra gold-standard, bis
fails to predict the motion. we evaluate the following motion prediction methods
(table 1): (i) dl-hmc with pci as input (dl-hmc pci);(ii) dl-hmc with fri as
input (dl-hmc fri); (iii) proposed network with pci as input (proposed pci);
(iv) proposed network with fri as input (proposed fri); and (v) proposed method
with fri but without the data augmentation module (proposed w/o da); results
demonstrate that the proposed network with fri input provides the best motion
tracking performance in both validation and testing data. we also observes that
using fri yields a lower loss for the proposed network, indicating that high
image quality enhanced the motion correction performance. for testing
translation results, proposed pci outperforms proposed fri and has similar total
motion loss, which indicates that the proposed network can still estimate motion
on testing subjects even with noisy input. figure 2 shows motion prediction
results for different variations of the proposed method in a single test
subject. these results show that the proposed fri method is more similar to
vicra than the other methods, especially for translation in the x and z
directions. however, the proposed fri method exhibits higher variance than other
methods, which may be a result of the data augmentation distribution. overall,
our experiments demonstrate that the strategies in proposed fri method enhance
the motion tracking performance of the network.qualitative reconstruction
evaluation. after inference, the 6 rigid degrees of freedom transformation
estimated from the network were used to reconstruct the pet images using
motion-compensation osem list-mode algorithm for resolution-recovery
reconstruction (molar) algorithm [5]. we applied pet head motion correction
using the proposed fri model and compared with vicra and no motion correction
(nmc) reconstruction results. figure 3 shows the reconstruction results for the
same testing subject in quantitative evaluation.based on the tracer distribution
of 18 f-peb, we selected some frames from reconstructed images to illustrate the
proposed fri motion correction performance. in general, the proposed fri results
in qualitatively enhanced anatomical interpretation of pet images. in fig. 3,
nmc reconstruction has motion blurring on margins of the brain, while proposed
fri reconstruction shows welldefined gyrus and sulcus comparable to vicra
reconstruction. 18 f-fpeb tracer is a metabotropic glutamate 5 receptor
antagonist with moderate to high accumulation in multiple brain regions such as
insula/caudate nucleus, thalamus and temporal lobe. thus, we compared
visualization of these regions among nmc, proposed fri and vicra reconstructions
(fig. 3). specifically, our proposed fri method yields clear delineation of the
insula/caudate nucleus, thalamus, and temporal lobe nearly indistinguishable
from vicra reconstructed images.in addition, brain region of interest (roi)
analyses were also performed for quantitative use. each subject's mr image was
segmented into 74 regions using freesurfer software [2]. these regions were then
merged into twelve large grey matter (gm) rois. for all testing subjects, the
suv difference of 12 rois from dl-hmc, proposed pci, and proposed fri
reconstruction were calculated for comparison with vicra reference (fig. 3
right). the proposed fri method yields the lowest absolute difference (0.8%)
from vicra images in suv, while the absolute suv difference for dl-hmc fri is
1.5% and for nmc is 1.9%. specifically, results of proposed fri method are
closest to vicra results in regions such as thalamus, temporal lobe, insula
(absolute activity difference are 0.5%, 0.7%, 0.9%, respectively), which are the
target areas of 18 f-fpeb tracer with highest empirical tracer accumulation. the
reconstruction and roi evaluation results indicate that the proposed fri method
holds potential to improve clinical applicability through amelioration of pet
motion correction accuracy.
in this work, we propose a new head motion correction approach using fast
reconstructions as input. the proposed method outperforms other methods in a
multi-subject cohort, and ablation studies demonstrate the effectiveness of our
strategies. we apply our proposed fri motion correction to get motion-free
reconstruction using molar. the proposed fri method achieves good image quality
and similar roi evaluation results compared to vicra gold-standard hmt. in this
study, we showed that conventional intensity-based registration fails at
performing motion tracking on fri data. this is likely due to the pet dynamic
changes and non-optimal registration parameters. compared with previous deep
learning motion correction [15], the training speed and gpu memory usage of the
proposed method are much better thanks to the proposed shallower encoder
architecture and our efficient training and testing strategies. though hmt
method such as vicra achieves good accuracy and time resolution for pet head
motion tracking, two common types of vicra failure may occur: slipping and
wobbling. our method would be robust enough to compensate for the vicra failure.
because of the limited vicra data, in the future, we will develop
semi-supervised deep learning methods for pet head motion correction. our study
used tof pet data because it can yield high signal to noise ratio (snr) for both
fri and pci due to the better location identification of photons, thus the
one-second fri still retains some essential brain structures. limitations of
this work include partial limited tracking time and low time resolution compared
to hmt methods mentioned in sect. 1. in the future, with the development of pet
techniques such as depth-of-interaction [8], higher resolution and sensitivity
pet will be available. such pet will give data-driven pet motion correction a
revolutionary opportunity to have more accurate tracking and higher time
resolution. we plan to apply the proposed method to other datasets, developing a
generalized model for multi-tracer and multi-scanner pet data.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 67.
positron emission tomography (pet) is a general nuclear imaging technique, which
has been widely used to characterize tissue metabolism, protein deposition, etc.
[9]. according to the pet imaging principle, radioactive tracers injected into
the body involve in the metabolism and produce γ decay signals externally.
however, due to photoelectric absorption and compton scattering, the decay
signals are attenuated when passing through human tissues to external receivers,
resulting in incorrect tracer distribution reasoning (see non-attenuation
corrected pet (nac-pet) in fig. 1(a)). to obtain correct tracer distribution
(see ac-pet in fig. 1(a)), attenuation correction (ac) on the received signals
is required. traditional ac accompanies additional costs caused by the
simultaneously obtained mr or ct images which are commonly useless for
diagnosis. the additional costs are especially significant for advanced
total-body pet/ct scanners [12,13], which have effective sensitivity and low
radiation dose during pet scanning but accumulative radiation dose during ct
scanning. in another word, ct becomes a non-negligible source of radiation
hazards. to reduce the costs, including expense, time, and radiation hazards,
some studies proposed to conduct ac by exploiting each pet image itself.
researchers have been motivated to generate pseudo ct images from nac-pet images
[2,7], or more directly, to generate ac-pet images from nac-pet images [5,11].
since pseudo ct is convenient to be integrated into conventional ac processes,
generating pseudo ct images is feasible in clinics for ac.the pseudo ct images
should satisfy two-fold requests. firstly, the pseudo ct images should be
visually similar in anatomical structures to corresponding actual ct images.
secondly, pet images corrected by pseudo ct images should be consistent with
that corrected by actual ct images. however, current techniques of image
generation tend to produce statistical average values and patterns, which easily
erase significant tissues (e.g., bones and lungs). as a result, for those
tissues with relatively similar metabolism but large variances in attenuation
coefficient, these methods could cause large errors as they are blind to the
correct tissue distributions. therefore, special techniques should be
investigated to guarantee the fidelity of anatomical structures in these
generated pseudo ct images.in this paper, we propose a deep learning framework,
named anatomical skeleton enhanced generation (aseg), to generate pseudo ct from
nac-pet for attenuation correction. aseg focuses more on the fidelity of tissue
distribution, i.e., anatomical skeleton, in pseudo ct images. as shown in fig.
1(b), this framework contains two sequential modules structure prediction module
g 1 and tissue rendering module g 2 }. g 1 devotes to delineating the anatomical
skeleton from a nac-pet image, thus producing a prior tissue distribution map to
g 2 , while g 2 devotes to rendering the tissue details according to both the
skeleton and nac-pet image. we regard g 1 as a segmentation network that is
trained under the combination of cross-entropy loss and dice loss and outputs
the anatomical skeleton. for training the generative module g 2 , we further
propose the anatomical-consistency constraint to guarantee the fidelity of
tissue distribution besides general constraints in previous studies. experiments
on four publicly collected pet/ct datasets demonstrate that our aseg outperforms
existing methods by preserving better anatomical structures in generated pseudo
ct images and achieving better visual similarity in corrected pet images.
we propose the anatomical skeleton enhanced generation (aseg, as illustrated in
fig. (1) framework that regards the ct generation as two sequential tasks, i.e.,
skeleton prediction and tissue rendering, instead of simply mapping pseudo ct
from nac-pet. aseg composes of two sequential generative modules {g 1 , g 2 } to
deal with them, respectively. g 1 devotes itself to decoupling the anatomical
skeleton from nac-pet to provide rough prior information of attenuation
coefficients to g 2 , particularly for lungs and bones that have the most
influential variances. g 2 then devotes to rendering the tissue details in the
ct pattern exploiting both the skeleton and nac-pet images. in short, the
skeleton decoupled by g 1 is a prior guidance to g 2 , and in turn, g 2 can
serve as a target supervision for g 1 . these two modules are trained with
different constraints according to the corresponding tasks. specially, the
general dice loss and cross-entropy loss [16] are employed to guarantee g 1 for
the fidelity of tissue distributions while general mean absolute error and
feature matching losses are utilized to guarantee g 2 for potential
coarse-to-fine semantic constraint. to improve the fidelity of anatomical
structures, we further propose the anatomical consistency loss to encourage g 2
to generate ct images that are consistent in tissue distributions with actual ct
images in particular.network architecture. as illustrated in fig. 1(b), our aseg
has two generative modules for skeleton prediction and tissue rendering,
respectively, where g 1 and g 2 share the same network structure but g 2 is
accompanied by an adversarial network d (not drawn, same structure in [8]). each
generative network consists of an input convolutional layer, four encoding
blocks, two residual blocks (rbs) [8], four decoding blocks, and an output
convolutional layer. each encoding block contains a rb and a convolutional layer
with strides of 2 × 2 × 2 for downsampling while each decoding block contains an
upsampling operation of 2 × 2 × 2 and a convolutional layer. the kernel size for
the input and output convolutional layers is 7 × 7 × 7 while for others is 3 × 3
× 3. skip connections are further used locally in rbs and globally between
corresponding layers to empower information transmission. meanwhile, the
adversarial network d consists of five 4 × 4 × 4 convolutional layers with
strides of 2 × 2 × 2 for the first four layers and 1 × 1 × 1 for the last
layer.model formulation. let x nac and x ac denote the nac-pet and ac-pet
images, and y be the actual ct image used for ac. since ct image is highly
crucial in conventional ac algorithms, they generally have a relationship
asunder an ac algorithm f. to avoid scanning an additional ct image, we attempt
to predict y from x nac as an alternative in ac algorithm. namely, a mapping g
is required to build the relationship between y and x nac , i.e., ŷ = g(x nac
).then, x ac can be acquired bythis results in a pioneering ac algorithm that
requires only a commonly reusable mapping function g for all pet images rather
than a corresponding ct image y for each pet image. as verified in some previous
studies [1,2,7], g can be assigned by some image generation techniques, e.g.
gans and cnns. however, since these general techniques tend to produce
statistical average values, directly applying them may lead to serious
brightness deviation, for those tissues with large intensity ranges. to overcome
this drawback, we propose aseg as a specialized ac technique, which decouple the
ct generation process g in two sequential parts, i.e., g 1 for skeleton
prediction and g 2 for tissue rendering, as formulated asherein, g 1 devotes to
delineating anatomical skeleton y as from x nac , thus providing a prior tissue
distribution to g 2 while g 2 devotes to rendering the tissue details from x nac
and ŷas = g 1 (x nac ).to avoid annotating the ground truth, y as can be derived
from the actual ct image by a segmentation algorithm (denoted as s : y as = s(y
)). as different tissues have obvious differences in intensity ranges, we define
s as a simple thresholding-based algorithm. herein, we first smooth each
non-normalized ct image with a small recursive gaussian filter to suppress the
impulse noise, and then threshold this ct image to four binary masks according
to the hounsfield scale of tissue density [6], including the air-lung mask
(intensity ranges from -950hu to -125hu), the fluids-fat mask (ranges from
-125hu to 10hu), the soft-tissue mask (ranges from 10hu to 100hu), and the bone
mask (ranges from 100hu to 3000hu), as demonstrated by anatomical skeleton in
fig. 1(b). this binarization trick highlights the difference among different
tissues, and thus is easier perceived.general constraints. as mentioned above,
two generative modules {g 1 , g 2 } work for two tasks, namely the skeleton
prediction and tissue rendering, respectively. thus, they are trained with
different target-oriented constraints. in the training scheme, the loss function
for g 1 is the combination of dice loss l dice and cross-entropy loss l ce [16],
denoted asmeanwhile, the loss function for g 2 combines the mean absolute error
(mae) l mae , perceptual feature matching loss l fm [14], and
anatomical-consistency loss l ac , denoted aswhere the anatomical consistency
loss l st is explained below.anatomical consistency. it is generally known that
ct images can provide anatomical observation because different tissues have a
distinctive appearance in hounsfield scale (linear related to attenuation
coefficients). therefore, it is crucial to ensure the consistency of tissue
distribution in the pseudo ct images, tracking which we propose to use the
tissue distribution consistency to guide the network learning. based on the
segmentation algorithm s, both the actual and generated cts {y, ŷ } can be
segmented to anatomical structure/tissue distribution masks {s(y ), s( ŷ )}, and
their consistency can then be measured by dice coefficient. accordingly, the
anatomical-consistency loss l ac is a dice loss asduring the inference phase,
only the nac-pet image of each input subject is required, where the pseudo ct
image is derived by ŷ ≈ g 2 (g 1 (x nac ), x nac ).
the data used in our experiments are collected from the cancer image archive
(tcia) [4] (https://www.cancerimagingarchive.net/collections/), where a series
of public datasets with different types of lesions, patients, and scanners are
open-access. among them, 401, 108, 46, and 20 samples are extracted from the
head and neck scamorous cell carcinoma (hnscc), non-small cell lung cancer
(nsclc), the cancer genome atlas (tcga) -head-neck squamous cell carcinoma
(tcga-hnsc), and tcga -lung adenocarcinoma (tcga-luad), respectively. we use
these samples in hnscc for training and in other three datasets for
evaluation.each sample contains co-registered (acquired with pet-ct scans) ct,
pet, and nac-pet whole-body scans. in our experiments, we re-sampled all of them
to a voxel spacing of 2×2×2 and re-scaled the intensities of nac-pet/ac-pet
images to a range of [0, 1], of ct images by multiplying 0.001. the input and
output of our aseg framework are cropped patches with the size of 192 × 192 ×
128 voxels. to achieve full-fov output, the consecutive outputs of each sample
are composed into a single volume where the overlapped regions are averaged.
we compared our aseg with three state-of-the-art methods, including (i ) a u-net
based method [3] that directly learns a mapping from nac-pet to ct image with
mae loss (denoted as u-net), (ii ) a conventional gan-based method [1,2] that
uses the u-net as the backbone and employ the style-content loss and adversarial
loss as an extra constraint (denoted as cgan), and (iii ) an auxiliary gan-based
method [10] that uses the ct-based segmentation (i.e., the simple thresholding
s) as an auxiliary task for ct generation (denoted as agan). for a fair
comparison, we implemented these methods by ourselves in a tensorflow platform
with an nvidia 3090 gpu. all methods share the same backbone structure as g * in
fig. 1(b) and follow the same experimental settings. particularly, the
adversarial loss of methods (ii ) and (iii ) are replaced by the perceptual
feature matching loss. these two methods could be considered as variants of our
method without using predicted prior anatomic skeleton.quantitative analysis of
ct. as the most import application of ct that is to display the anatomical
information, we propose to measure the anatomical consistency between the pseudo
ct images and actual ct images, where the dice coefficients on multiple
anatomical regions that extracted from the pseudo/actual ct images are
calculated. to avoid excessive self-referencing in evaluating anatomical
consistency, instead of employing the simple thresholding segmentation (i.e.,
s), we resort to the open-access totalsegmentator [15] to finely segment the
actual and pseudo ct images to multiple anatomical structures, and compose them
to nine independent tissues for simplifying result , where the following
conclusions can be drawn. firstly, u-net and cgan generate ct images with
slightly better global intensity similarity but worse anatomical consistency in
some tissues than agan and aseg. this indicates that the general constraints
(mae and perceptual feature matching) cannot preserve the tissue distribution
since they tend to produce statistical average values or patterns, particularly
in these regions with large intensity variants. secondly, agan achieves the
worst intensity similarity and anatomical consistency for some organs. such
inconsistent metrics suggest that the global intensity similarity may have a
competing relationship with anatomical consistency in the learning procedure,
thus it is not advisable to balance them in a single network. thirdly, cgan
achieves better anatomical consistency than u-net, but worse than aseg. it
implies that the perceptual feature matching loss can also identify the variants
between different tissues implicitly but cannot compare to our strategy to
explicitly enhance the anatomical skeleton. fourthly, our proposed aseg achieves
the best anatomical consistency for all tissues, indicating it is reasonable to
enhance tissue variations. in brief, the above results supports the strategy to
decouple the skeleton prediction as a preceding task is effective for ct
generation.effectiveness in attenuation correction. as the pseudo ct images
generated from nac-pet are expected to be used in ac, it is necessary to further
evaluate the effectiveness of pseudo ct images in pet ac. because we cannot
access the original scatters [5], inspired by [11], we propose to resort cgan to
simulate the ac process, denoted as acgan and trained on hnscc dataset. the
input of acgan is a concatenation of nac-pet and actual ct, while the output is
actual ac-pet. to evaluate the pseudo ct images, we simply use them to take
place of the actual ct. four metrics, including the peak signal to noise ratio
(psnr), mean absolute error (mae), normalized cross correlation (ncc), and ssim,
are used to measure acgan with pseudo ct images on test datasets (nsclc,
tcga-hnsc, and tcga-luda). the results are reported in table 1(b), where the
fourth column list the acgan results with actual ct images. meanwhile, we also
report the results of direct mapping nac-pet to ac-pet without ct images in the
third column ("no ct"), which is trained from scratch and independent from
acgan.it can be observed from table 1(b) that: (1) acgan with actual ct images
can predict images very close to the actual ac-pet images, thus is qualified to
simulate the ac process; (2) with actual or pseudo ct images, acgan can predict
images closer to the actual ac-pet images than without ct, demonstrating the
necessity of ct images in process of pet ac; (3) these pseudo cts cannot compare
to actual cts, reflecting that there exist some relative information that can
hardly be mined from nac-pet; (4) the pseudo cts generated by aseg achieve the
best in three metrics (mae, psnr, ncc) and second in the other metric (ssim),
demonstrating the advance of our aseg. figure 2 displayed the detailed diversity
of the ac-pet corrected by different pseudo cts. it can be found that the
structures of ac-pet are highly dependent on ct, particularly the lung regions.
however, errors in corners and shapes are relatively large (see these locations
marked by red arrows), which indicates there are still some space in designing
more advanced mapping methods. nonetheless, compared to other pseudo cts, these
generated by aseg result in more realistic ac-pet with fewer errors,
demonstrating the ac usability of aseg.
in this paper, we proposed the anatomical skeleton-enhance generation (aseg) to
generate pseudo ct images for pet attenuation correction (ac), with the goal of
avoiding acquiring extra ct or mr images. aseg divided the ct generation into
the skeleton prediction and tissue rendering, two sequential tasks, addressed by
two designed generative modules. the first module delineates the anatomical
skeleton to explicitly enhance the tissue distribution which are vital for ac,
while the second module renders the tissue details based on the anatomical
skeleton and nac-pet. under the collaboration of two modules and specific
anatomical-consistency constraint, our aseg can generate more reasonable pseudo
ct from nac-pet. experiments on a collection of public datasets demonstrate that
our aseg outperforms existing methods by achieving advanced performance in
anatomical consistency. our study support that aseg could be a promising and
lower-cost alternative of ct acquirement for ac. our future work will extend our
study to multiple pet tracers.
report, e.g., lung (dice l ), heart (dice h ), liver (dice li ), kidneys (dice k
), blood vessels (dice k ), digestive system (dice d ), ribs (dice r ),
vertebras (dice v ), and iliac bones (dice ib ). additionally, the structure
similarity index measure (ssim) values are also reported to measure the global
intensity similarity.results of various methods are provided in table1(a)
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5_3.
magnetic resonance imaging (mri) consists of a series of pulse sequences, e.g.
t1-weighted (t1), contrast-enhanced (t1gd), t2-weighted (t2), and
t2-fluidattenuated inversion recovery (flair), each showing various contrast of
water and fat tissues. the intensity contrast combination of multi-sequence mri
provides clinicians with different characteristics of tissues, extensively used
in disease diagnosis [16], lesion segmentation [17], treatment prognosis [7],
etc. however, some acquired sequences are unusable or missing in clinical
settings due to incorrect machine settings, imaging artifacts, high scanning
costs, time constraints, contrast agents allergies, and different acquisition
protocols between hospitals [5]. without rescanning or affecting the downstream
pipelines, the mri synthesis technique can generate missing sequences by
leveraging redundant shared information between multiple sequences [18].many
studies have demonstrated the potential of deep learning methods for
image-to-image synthesis in the field of both nature images [8,11,12] and
medical images [2,13,19]. most of these works introduce an autoencoder-like
architecture for image-to-image translation and employ adversarial loss to
generate more realistic images. unlike these one-to-one approaches, mri
synthesis faces the challenge of fusing complementary information from multiple
input sequences. recent studies about multi-sequence fusion can specifically be
divided into two groups: (1) image fusion and (2) feature fusion. the image
fusion approach is to concatenate sequences as a multi-channel input. sharma et
al. [18] design a network with multi-channel input and output, which combines
all the available sequences and reconstructs the complete sequences at once. li
et al. [14] add an availability condition branch to guide the model to adapt
features for different input combinations. dalmaz et al. [9] equip the synthesis
model with residual transformer blocks to learn contextual features. image-level
fusion is simple and efficient but unstable -zero-padding inputs for missing
sequences lead to training unstable and slight misalignment between images can
easily cause artifacts. in contrast, efforts have been made on feature fusion,
which can alleviate the discrepancy across multiple sequences, as high-level
features focus on the semantic regions and are less affected by input
misalignment compared to images. zhou et al. [23] design operation-based (e.g.
summation, product, maximization) fusion blocks to densely combine the
hierarchical features. and li et al. [15] employ self-attention modules to
integrate multi-level features. the model architectures of these methods are not
flexible and difficult to adapt to various sequence combinations. more
importantly, recent studies only focus on proposing end-to-end models, lacking
quantifying the contributions for different sequences and estimating the
qualities of generated images.in this work, we propose an explainable
task-specific fusion sequence-tosequence (tsf-seq2seq) network, which has
adaptive weights for specific synthesis tasks with different input combinations
and targets. specially, this framework can be easily extended to other tasks,
such as segmentation. our primary contributions are as follows: (1) we propose a
flexible network to synthesize the target mri sequence from an arbitrary
combination of inputs; (2) the network shows interpretability for fusion by
quantifying the contribution of each input sequence; (3) the network provides
reliability for synthesis by highlighting the area the network tried to refine.
figure 1 illustrates the overview of the proposed tsf-seq2seq network. our
network has an autoencoder-like architecture including an encoder e, a
multisequence fusion module, and a decoder g. available mri sequences are first
encoded to features by e, respectively. then features from multiple input
sequences are fused by giving the task-specific code, which identifies sources
and targets with a binary code. finally, the fused features are decoded to the
target sequence by g. furthermore, to explain the mechanism of multi-sequence
fusion, our network can quantify the contributions of different input sequences
with the task-specific weighted average module and visualize the tsem with the
task-specific attention module.to leverage shared information between sequences,
we use e and g from seq2seq [10], which is a one-to-one synthetic model that
integrates arbitrary sequence synthesis into single e and g. they can reduce the
distance between different sequences at the feature level to help more stable
fusion. details of the multi-sequence fusion module and tsem are described in
the following sections.
define a set of n sequences mri: x = {x i |i = 1, ..., n } and corresponding
available indicator a ⊂ {1, ..., n } and a = ∅. our goal is to predict the
target set∈ a} by giving the available set x a = {x i |i ∈ a} and the
corresponding task-specific code c = {c src , c tgt } ∈ z 2n . as shown in fig.
1, c src and c tgt are zero-one codes for the source and the target set,
respectively. to fuse multiple sequences at the feature level, we first encode
images and concatenate the features as f = {e(x i )|i = 1, ..., n }.
specifically, we use zero-filled placeholders with the same shape as e(x i ) to
replace features of i / ∈ a to handle arbitrary input sequence combinations. the
multi-sequence fusion module includes: (1) a task-specific weighted average
module for the linear combination of available features; (2) a task-specific
attention module to refine the fused features.task-specific weighted average.
the weighted average is an intuitive fusion strategy that can quantify the
contribution of different sequences directly. to learn the weight automatically,
we use a trainable fully connected (fc) layer to predict the initial weight ω 0
∈ r n from c.where w and b are weights and bias for the fc layer, = 10 -5 to
avoid dividing 0 in the following equation. to eliminate distractions and
accelerate training, we force the weights of missing sequences in ω 0 to be 0
and guarantee the outputwhere • refers to the element-wise product and •, •
indicates the inner product.with the weights ω, we can fuse multi-sequence
features as f by the linear combination.specially, f ≡ e(x i ) when only one
sequence i is available, i.e. a = {i}. it demonstrates that the designed ω can
help the network excellently inherit the synthesis performance of pre-trained e
and g. in this work, we use ω to quantify the contribution of different input
combinations.task-specific attention. apart from the sequence-level fusion of f
, a taskspecific attention module g a is introduced to refine the fused features
at the pixel level. the weights of g a can adapt to the specific fusion task
with the given target code. to build a conditional attention module, we replace
convolutional layers in convolutional block attention module (cbam) [20] with
hyper-conv [10]. hyperconv is a dynamic filter whose kernel is mapped from a
shared weight bank, and the mapping function is generated by the given target
code.as shown in fig. 1, channel attention and spatial attention can provide
adaptive feature refinement guided by the task-specific code c to generate
residual attentional fused features f a .loss function. to force both f and f +
f a can be reconstructed to the target sequence by the conditional g, a
supervised reconstruction loss is given as,where• 1 refers to a l 1 loss, and l
p indicates the perceptual loss based on pre-trained vgg19. λ r and λ p are
weight terms and are experimentally set to be 10 and 0.01.
as f a is a task-specific contextual refinement for fused features, analyzing it
can help us understand more what the network tried to do. many studies focus on
visualizing the attention maps to interpret the principle of the network,
especially for the transformer modules [1,6]. however, visualization of the
attention map is limited by its low resolution and rough boundary. thus, we
proposed the tsem by subtracting the reconstructed target sequences with and
without f a , which has the same resolution as the original images and clear
interpretation for specific tasks. tsem3 experiments
we use brain mri images of 1,251 subjects from brain tumor segmentation 2021
(brats2021) [3,4,17], which includes four aligned sequences, t1, t1gd, t2, and
flair, for each subject. we select 830 subjects for training, 93 for validation,
and 328 for testing. all the images are intensity normalized to [-1, 1] and
central cropped to 128 × 192 × 192. during training, for each subject, a random
number of sequences are selected as inputs and the rest as targets. for
validation and testing, we fixed the input combinations and the target for each
subject. the synthesis performance is quantified using the metrics of peak
signal noise rate (psnr), structural similarity index measure (ssim), and
learned perceptual image patch similarity (lpips) [21], which evaluate from
intensity, structure, and perceptual aspects.
the models are implemented with pytorch and trained on the nvidia geforce rtx
3090 ti gpu. e comprises three convolutional layers and six residual blocks. the
initial convolutional layer is responsible for encoding intensities to features,
while the second and third convolutional layers downsample images by a factor of
four. the residual blocks then extract the high-level representation. the 28.5 ±
2.5 0.883 ± 0.040 9.65 ± 3.57 diamondgan [14] 28.2 ± 2.5 0.877 ± 0.041 10.20 ±
3.33 resvit [9] 28.3 ± 2.4 0.882 ± 0.039 9.87 ± 3.30 seq2seq [10] (average) 28.5
± 2.3 0.880 ± 0.038 11.61 ± 3.87 tsf-seq2seq (w/o fa) 28.3 ± 2.6 0.876 ± 0.044
9.61 ± 4.00 tsf-seq2seq 28.8 ± 2.6 0.887 ± 0.042 8.89 ± 3.80 channels are 64,
128, 256, and 256, respectively. g has an inverse architecture with e, and all
the convolutional layers are replaced with hyperconv. the e and g from seq2seq
are pre-trained using the adam optimizer with an initial learning rate of 2 × 10
-4 and a batch size of 1 for 1,000,000 steps, taking about 60 h. then we
finetune the tsf-seq2seq with the frozen e using the adam optimizer with an
initial learning rate of 10 -4 and a batch size of 1 for another 300,000 steps,
taking about 40 h.
we compare our method with one-to-one translation, image-level fusion, and
feature-level fusion methods. one-to-one translation methods include pix2pix
[12] and seq2seq [10]. image-level fusion methods consist of mm-gan [18],
diamondgan [14], and resvit [9]. feature-level fusion methods include hi-net
[23] and mmgsn-net [15]. figure 2 shows the examples of synthetic t2 of
comparison methods input with the combinations of t1gd and flair. table 1
reports the sequence synthesis performance for comparison methods organized by
the different numbers of input combinations. note that, for multiple inputs,
one-to-one translation methods synthesize multiple outputs separately and
average them as one. and hi-net [23] and mmgsn-net [15] only test on the subset
with two inputs due to fixed network architectures. as shown in table 1, the
proposed method achieves the best performance in different input combinations.
we compare two components of our method, including (1) task-specific weighted
average and (2) task-specific attention, by conducting an ablation study between
seq2seq, tsf-seq2seq (w/o f a ), and tsf-seq2seq. tsf-seq2seq (w/o f a ) refers
to the model removing the task-specific attention module. as shown in table 1,
when only one sequence is available, our method can inherit the performance of
seq2seq and achieve slight improvements. for multi-input situations, the
task-specific weighted average can decrease lpips to achieve better perceptual
performance. and task-specific attention can refine the fused features to
achieve the best synthesis results.
the proposed method not only achieves superior synthesis performance but also
has good interpretability. in this section, we will visualize the contribution
of different input combinations and tsem. sequence contribution. we use ω in eq.
2 to quantify the contribution of different input combinations for synthesizing
different target sequences. figure 3 shows the bar chart for the sequence
contribution weight ω with different taskspecific code c. as shown in fig. 3,
both t1 and t1gd contribute greatly to the sequence synthesis of each other,
which is expected because t1gd are t1weighted scanning after contrast agent
injection, and the enhancement between these two sequences is indispensable for
cancer detection and diagnosis. the less contribution of t2, when combined with
t1 and/or t1gd, is consistent with the clinical findings [22,23] that t2 can be
well-synthesized by t1 and/or t1gd.tsem vs. attention map. figure 4 shows the
proposed tsem and the attention maps extracted by resvit [9]. as shown in fig.
4, tsem has a higher resolution than the attention maps and can highlight the
tumor area which is hard to be synthesized by the networks. table 2 reports the
results of psnr for regions highlighted or not highlighted by tsem with a
threshold of the 99th percentile. to assist the synthesis models deploying in
clinical settings, tsem can be used as an attention and uncertainty map to
remind clinicians of the possible unreliable synthesized area.
in this work, we introduce an explainable network for multi-to-one synthesis
with extensive experiments and interpretability visualization. experimental
results based on brats2021 demonstrate the superiority of our approach compared
with the state-of-the-art methods. and we will explore the proposed method in
assisting downstream applications for multi-sequence analysis in future works.
number of inputs tsem > 99% tsem < 99% total
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 5.
ultrasound localization microscopy (ulm) has revolutionized medical imaging by
enabling sub-wavelength resolution from images acquired by piezo-electric
transducers and computational beamforming. however, the necessity of beamforming
for ulm remains questionable. our work challenges the conventional assumption
that beamforming is the ideal processing step for ulm and presents an
alternative approach based on geometric reconstruction from time-of-arrival
(toa) information.the discovery of ulm has recently surpassed the
diffraction-limited spatial resolution and enabled highly detailed visualization
of the vascularity [8]. ulm borrows concepts from super-resolution fluorescence
microscopy techniques to precisely locate individual particles with sub-pixel
accuracy over multiple frames. by the accumulation of all localizations over
time, ulm can produce a superresolved image, providing researchers and
clinicians with highly detailed representation of the vascular structure.while
contrast-enhanced ultra-sound (ceus) is used in the identification of
musculoskeletal soft tissue tumours [5], the far higher resolution capability
offered by ulm has great potential for clinical translation to improve the
reliability of cancer diagnosis (i.e., enable differentiation of tumour types in
kidney cancer [7] or detect breast cancer tissue [1]). moreover, ulm has shown
promise in imaging neurovascular activity after visual stimulation (functional
ulm) [14]. the pioneering study by errico et al. [8] initially demonstrated the
potential of ulm by successfully localizing contrast agent particles
(microbubbles) using a 2d point-spread-function model. in general, the accuracy
in microbubble (mb) localization is the key to achieving sub-wavelength
resolution [4], for which classical imaging methods [11,17], as well as deep
neural networks [1,16], have recently been reported. however, the conventional
approach for ulm involves using computational beamformers, which may not be
ideal for mb localization. for example, a recent study has shown that ultrasound
image segmentation can be learned from radiofrequency data and thus without
beamforming [13]. beamforming techniques have been developed to render irregular
topologies, whereas mbs exhibit a uniform geometric structure, for which ulm
only requires information about its spatial position. although the impact of
adaptive beamforming has been studiedfor ulm to investigate its potential to
refine mb localization [3], optimization of the point-spread function (psf)
poses high demands on the transducer array, data storage, and algorithm
complexity.to this end, we propose an alternative approach for ulm, outlined in
fig. 1, that entirely relies on time-difference-of-arrival (tdoa) information,
omitting beamforming from the processing pipeline for the first time. we
demonstrate a novel geometry framework for mb localization through ellipse
intersections to overcome limitations inherent to beamforming. this approach
provides a finer distinction between overlapping and clustered spots, improving
localization precision, reliability, and computation efficiency. in conclusion,
we challenge the conventional wisdom that beamforming is necessary for ulm and
propose a novel approach that entirely relies on tdoa information for mb
localization. our proposed approach demonstrates promising results and indicates
a considerable trade-off between precision, computation, and memory.
geometric modeling is a useful approach for locating landmarks in space. one
common method involves using a time-of-flight (tof) round-trip setup that
includes a transmitter and multiple receivers [10]. this setup is analogous to
the parallax concept in visual imaging, where a triangle is formed between the
target, emitter, and receivers, as illustrated in fig. 2. the target's location
can be accurately estimated using trilateration by analyzing the time delay
between the transmitted and received signals. however, the triangle's side
lengths are unknown in the single receiver case, and all possible travel path
candidates form triangles with equal circumferences fixed at the axis connecting
the receiver and the source. these candidates reside on an elliptical shape. by
adding a second receiver, its respective ellipse intersects with the first one
resolving the target's 2-d position. thus, the localization accuracy depends on
the ellipse model, which is parameterized by the known transducer positions and
the time delays we seek to estimate. this section describes a precise echo
feature extraction, which is essential for building the subsequent ellipse
intersection model. finally, we demonstrate our localization refinement through
clustering.
feature extraction of acoustic signals has been thoroughly researched [9,18]. to
leverage the geometric ulm localization, we wish to extract time-of-arrival
(toa) information (instead of beamforming) at sub-wavelength precision. despite
the popularity of deep neural networks, which have been studied for toa
detection [18], we employ an energy-based model [9] for echo feature extraction
to demonstrate the feasibility of our geometric ulm at the initial stage.
ultimately, future studies can combine our proposed localization with a
supervised network. here, echoes f (m k ; t) are modeled as multimodal
exponentially-modified gaussian oscillators (memgo) [9],where t ∈ r t denotes
the time domain with a total number of t samples andangular frequency ω k and
phase φ k for each echo k. note that erf(•) is the error function. to estimate
these parameters iteratively, the cost function is given by,where y n (t) is the
measured signal from waveform channel n ∈ {1, 2, . . . , n} and the sum over k
accumulates all echo components mn = [m 1 , m 2 , . . . , m k ] . we get the
best echo feature set m n over all iterations j via,for which we use the
levenberg-marquardt solver. model-based optimization requires initial estimates
to be nearby the solution space. for this, we detect initial toas via
gradient-based analysis of the hilbert-transformed signal to set m(1) n as in
[9]. before geometric localization, one must ensure that detected echo
components correspond to the same mb. in this work, echo matching is
accomplished in a heuristic brute-force fashion. given an echo component m n,k
from a reference channel index n, a matching echo component from an adjacent
channel index n ± g with gap g ∈ n is found by k + h in the neighborhood of h ∈
{-1, 0, 1}. a corresponding phase-precise toa t n,k is obtained by t n±g,k = μ
n±g,k+h +φ n,k -δ , which takes μ n,k and φ n,k from m n for phase-precise
alignment across transducer channels after upsampling. here, δ is a fixed offset
to accurately capture the onset of the mb locations [2]. we validate echo
correspondence through a re-projection error in adjacent channels and reject
those with weak alignment.
while ellipse intersections can be approximated iteratively, we employ eberly's
closed-form solution [6] owing to its fast computation property. although one
might expect that the intersection of arbitrarily placed ellipses is
straightforward, it involves advanced mathematical modelling due to the degrees
of freedom in the ellipse positioning. an ellipse is drawn by radii (r a , r b )
of the major and minor axes with,where the virtual transmitter ûs ∈ r 2 and each
receiver u n ∈ r 2 with channel index n represent the focal points of an
ellipse, respectively. for the intersection, we begin with the ellipse standard
equation. let any point s ∈ r 2 located on an ellipse and displaced by its
center c n ∈ r 2 such that, where m contains the ellipse equation with v n and v
⊥ n as a pair of orthogonal ellipse direction vectors, corresponding to their
radial extents (r 0 , r 1 ) as well as the squared norm • 2 2 and vector norm |
• |. for subsequent root-finding, it is the goal to convert the standard eq. (
5) to a quadratic polynomial with coefficients b j given by, b(x, y) = b 0 + b 1
x + b 2 y + b 3 x 2 + b 4 xy = 0, which, when written in vector-matrix form
reads,where b and b carry high-order polynomial coefficients b j found via
matrix factorization [6]. an elaborated version of this is found in the
supplementary material.let two intersecting ellipses be given as quadratic
equations a(x, y) and b(x, y) with coefficients a j and b j , respectively.
their intersection is found via polynomial root-finding of the equation,where
∀j, d j = a jb j . when defining y = w -(a 2 + a 4 x)/2 to substitute y, we get
a(x, w) = w 2 + (a 0 + a 1 x + a 3 x 2 ) -(a 2 + a 4 x) 2 /4 = 0 which after
rearranging is plugged into (7) to yield an intersection point s i = [x i , w i
] . we refer the interested reader to the insightful descriptions in [6] for
further implementation details.
micro bubble reflections are dispersed across multiple waveform channels
yielding groups of location candidates for the same target bubble. localization
deviations result from toa variations, which can occur due to atmospheric
conditions, receiver clock errors, and system noise. due to the random
distribution of corresponding toa errors [8], we regard these candidates as
clusters. thus, we aim to find a centroid p of each cluster using multiple
bi-variate probability density functions of varying sample sizes by,here, the
bandwidth of the kernel is set to λ/4. the mean shift algorithm updates the
estimate p (j) by setting it to the weighted mean density on each iteration j
until convergence. in this way, we obtain the position of the target bubble.
dataset: we demonstrate the feasibility of our geometric ulm and present
benchmark comparison outcomes based on the pala dataset [11]. this dataset is
chosen as it is publicly available, allowing easy access and reproducibility of
our results. to date, it is the only public ulm dataset featuring radio
frequency (rf) data as required by our method. its third-party simulation data
makes it possible to perform a numerical quantification and direct comparison of
different baseline benchmarks for the first time, which is necessary to validate
the effectiveness of our proposed approach.metrics: for mb localization
assessment, the minimum root mean squared error (rmse) between the estimated p
and the nearest ground truth position is computed. to align with the pala study
[11], only rmses less than λ/4 are considered true positives and contribute to
the total rmse of all frames. in cases where the rmse distance is greater than
λ/4, the estimated p is a false positive. consequently, ground truth locations
without an estimate within the λ/4 neighbourhood are false negatives. we use the
jaccard index to measure the mb detection capability, which considers both true
positives and false negatives and provides a robust measure of each algorithm's
performance. the structural similarity index measure (ssim) is used for image
assessment.for a realistic analysis, we employ the noise model used in [11],
which is given by,where σ p = √ b × 10 p/10 and n (0, σ 2 p ) are normal
distributions with mean 0 and variance σ 2 p . here, l c and l a are noise
levels in db, and n(t) is the array of length t containing the random values
drawn from this distribution. the additive noise model is then used to simulate
a waveform channel y n (t) = y n (t)+ n(t) g(t, σ f ) suffering from noise,
where represents the convolution operator, and g(t, σ f ) is the one-dimensional
gaussian kernel with standard deviation σ f = 1.5. to mimic the noise reduction
achieved through the use of sub-aperture beamforming with 16 transducer channels
[11], we multiplied the rf data noise by a factor of 4 for an equitable
comparison.baselines: we compare our approach against state-of-the-art methods
that utilize beamforming together with classical image filterings [8], spline
interpolation [17], radial symmetry (rs) [11] and a deep-learning-based u-net
[16] for mb localization. to only focus on the localization performance of each
algorithm, we conduct the experimental analysis without temporal tracking. we
obtain the results for classical image processing approaches directly from the
open-source code provided by the authors of the pala dataset [11]. as there is
no publicly available implementation of [16] to date, we model and train the
u-net [15] according to the paper description, including loss design, layer
architecture, and the incorporation of dropout. since the u-net-based
localization is a supervised learning approach, we split the pala dataset into
sequences 1-15 for testing and 16-20 for training and validation, with a split
ratio of 0.9, providing a sufficient number of 4500 training frames.results:
table 1 provides the benchmark comparison results with state-of-theart methods.
our proposed geometric inference indicates the best localization performance
represented by an average rmse of around one-tenth of a wavelength. also, the
jaccard index reflects an outperforming balance of true positive and false
negative mb detections by our approach. these results support the hypothesis
that our proposed geometric localization inference is a considerable alternative
to existing beamforming-based methods. upon closer examination of the channels
column in table 1, it becomes apparent that our geometric ulm achieves
reasonable localization performance with only a fraction of the 128 channels
available in the transducer probe. using more than 32 channels improves the
jaccard index but at the expense of computational resources. this finding
confirms the assumption that transducers are redundant for mb tracking. the
slight discrepancy in ssim scores between our 128-channel results and the
64channel example may be attributed to the higher number of false positives in
the former, which decreases the overall ssim value. we provide rendered ulm
image regions for visual inspection in fig. 3 with full frames in the
supplementary material. to enhance visibility, all images are processed with
srgb and additional gamma correction using an exponent of 0.9. the presence of
noisy points in figs. 3b to 3d is attributed to the exces- sive false positive
localizations, resulting in poorer ssim scores. overall, these visual
observations align with the numerical results presented in table 1. an nvidia
rtx2080 gpu was used for all computations and time measurements. to improve
performance, signal processing chains are often pipelined, allowing for the
simultaneous computation of subsequent processes. table 1 lists the most
time-consuming process for each method, which acts as the bottleneck. for our
approach, the memgo feature extraction is the computationally most expensive
process, followed by clustering. however, our method contributes to an overall
efficient computation and acquisition time, as it skips beamforming and coherent
compounding [12] with the latter reducing the capture interval by
two-thirds.table 2 presents the results for the best-of-3 algorithms at various
noise levels l c . as the amount of noise from (9) increases, there is a decline
in the jaccard index, which suggests that each method is more susceptible to
false detections from noise clutter. although our method is exposed to higher
noise in the rf domain, it is seen that l c has a comparable impact on our
method. however, it is important to note that the u-net yields the most steady
and consistent results for different noise levels.
this study explored whether a geometric reconstruction may serve as an
alternative to beamforming in ulm. we employed an energy-based model for feature
extraction in conjunction with ellipse intersections and clustering to pinpoint
contrast agent positions from rf data available in the pala dataset. we carried
out a benchmark comparison with state-of-the-art methods, demonstrating that our
geometric model provides enhanced resolution and detection reliability with
fewer transducers. this capability will be a stepping stone for 3-d ulm
reconstruction where matrix transducer probes typically consist of 32
transducers per row only. it is essential to conduct follow-up studies to
evaluate the high potential of our approach in an extensive manner before
entering a pre-clinical phase. the promising results from this study motivate us
to expand our research to more rf data scenarios. we believe our findings will
inspire further research in this exciting and rapidly evolving field.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5_21.
near-infrared (nir) fluorescence imaging can allow the detection of fluorophores
up to 4 cm depth in tissue [11]. recently, with the availability of clinically
approved nir fluorophores such as indocyanine green or icg, fluorescence imaging
is increasingly being employed for intra-operative guidance during surgically
excision of malignant tumors and lymph nodes [6,15,16]. fluorescence imaging is
also a workhorse for small animal or preclinical research with multiple
commercial devices utilizing sensitive front or back-illuminated and cooled ccd
camera detectors available at prices ranging from 250-600k usd [9,14].a majority
of fluorescence imaging applications including fluorescence guided surgery (fgs)
rely upon visible 2d surface imaging [5,8,17,20] while reconstruction of the
invisible 3d target in tissue is not widely used for reflectance mode imaging
despite a large number of publications in 3d fluorescence diffuse optical
tomography (fdot) since early 1990s [1,4,10,18,19]. the primary cause of this
impasse is the ill-posedness of the mathematical inverse problem underlying the
3d reconstruction of the target in tissue from boundary measurements.the prime
motivation of our work is to enable an efficient 3d tumor shape reconstruction
for fgs in an operating room environment, where we do not have full control of
the ambient light and we cannot rely on sophisticated time or frequency domain
imaging instrumentation and setup. in these situations, one has to use clinical
cameras producing rapid continuous wave (cw) fluorescence boundary measurements
[19] in reflectance mode (i.e., the transmission of the light through the domain
is not measured), and with low signal-to-noise ratio which further exacerbates
the ill-posedness of fdot problem. the standard approach for solving fdot
problem with cw measurements is based on born approximation which works well in
the case of a small compared to the computational domain target and a very large
number of reflectance-transmission type measurements made by "slow in
acquisition" light sources and detector arrays of highly sensitive cooled ccd
cameras or photomultiplier tube arrays collecting both reflected and transmitted
light [18]. none of these is suitable for fgs settings where time is limited,
just a few reflectance mode cw-measurements are available, and the target can be
large compared to the imaged domain.we propose an incremental fluorescent target
reconstruction (iftr) scheme, based on the recent advances in quadratic and
conic convex optimization and sparse regularization, which can recover a
relatively large 3d target in tissuelike media. in our experiments, iftr scheme
demonstrates accurate reconstruction of 3d targets from reflectance mode
cw-measurements collected at the top surface of the domain. to our best
knowledge, this is the first report where the 3d shape of tumor-like target has
been recovered from reflectance mode steady-state cw measurements. previously
such results were reported in fdot literature only for time-consuming
frequency-domain or time-domain measurements [12] where photon path-length
information is available. moreover, the data is acquired almost instantly by an
inexpensive (<100 euros) camera with flexible fiber-optics making it suitable
for endoscopic fgs in contrast to the standard slow in acquisition
frequency-based measurements obtained by expensive (usd100k+ range) stationary
cameras. lastly, iftr scheme is implemented using fenics [3], a highlevel python
package for fem discretization of the physical model, and cvxpy [2,7], a convex
optimization package making this method easy to reuse/adjust for a different
setup. the code and data produced for this work are released as an open source
at https://github.com/ibm/dot.
figure 1 describes the setup representing a typical surgical field while
excising tumors. we simulate the provision of 3d surgical guidance via a
flexible endo- scope type fluorescence imager. for such provision we need to
solve the following fdot problem: estimate the spatial shape χ of the icg tagged
tumor target (the cube in green) within the tissue domain ω ∈ r 3 (the area in
grey) from measurements y. measurements are obtained by illuminating the tissue
domain with nir light at icg peak excitation wavelength via the expanded beam
from endoscope fiber bundle, and then measuring the light emitted by the tumor
like target diffusing to the top face of the phantom surface ∂ω obs , by the
fiber bundle with suitable emission filter which is coupled to a camera at the
backend.in this section we briefly describe the mathematical formulation of the
fdot problem and introduce the iftr scheme for solving it.forward and inverse
problems. photon propagation in tissue-like media is described by a coupled
system of elliptic partial differential equations (pdes) for determining photon
fluence φ (w/cm 2 ) at excitation and fluorescence emission wavelengths through
out the domain. wavelength and space dependent absorption and scattering
coefficients and fluorophore properties comprise the coefficients of this pde
system (see appendix a). the discretization of coupled diffusion pdes is
obtained by applying a standard fem methodology [13]: domain ω is covered by a
uniform grid comprised of n nodes {x i } n i=1 ; each function φ is approximated
by a vector φ ∈ r n with components φ i = φ(x i ); pdes are approximated using
weak formulations incorporating boundary conditions. this results in a system of
algebraic equations:where the first equation describes the excitation photon
fluence φ x ∈ r n , and the second describes photon emission fluence φ m ∈ r n ;
subscripts x and m indicate excitation and emission respectively. vectors f , χ
∈ r n are the source of excitation light and target's shape indicator, i.e., a
binary vector such that χ i = 1 if x i belongs to the target and 0 otherwise. s
x/m (•) ∈ r n ×n are the stiffness matrices obtained by discretizing the
diffusion terms of excitation/emission pdes respectively and additionally s x
depends on χ. m ∈ r n ×n is the mass matrix and denotes hadamard (elementwise)
product such that m χ = m diag(χ) and m χφ x = m φ x χ. finally, vector of
measurements y ∈ r k is related to the emission fluence φ m as followshere t ∈ r
k×n is a binary matrix that selects components of φ m corresponding to the
observed grid nodes and k is a number of observed nodes.in the following if
target indicator χ is given then the system (1) is referred to as the forward
fdot problem to compute unknown excitation and emission fluence φ x , φ m . if
vector χ is unknown but measurements of emission fluence are present then the
system (1)-( 2) is referred to as the fdot inverse problem.
in what follows we propose an algorithm that estimates target's indicator χ from
data y, i.e. solves the inverse fdot problem. to reduce the ill-posedness of the
inverse problem ( 1)-( 2) we introduce several regularization schemes. these
regularizations describe prior knowledge about the desired solution χ and thus
reduce the search space of admissible targets.the first regularization
represents an assumption that the correct χ is a binary vector. since binary
constraints are not convex, we adopt a more relaxed condition on χ referred to
as the box constraints: 0 ≤ χ ≤ 1.the second regularization describes the
piece-wise constant structure of the indicator χ, and is referred to as the
piece-wise total variation (ptv). it is obtained by extending the notion of
total variation which has been successfully applied in optical tomography. to
this end, assume m(j), n(j), and j ∈ i are indices corresponding to the j-th
pair of neighboring nodes. let the domain ω be split into n ptv non-overlapping
subdomains, e.g., cuboids, and the index i is correspondingly split into
non-overlapping sub-indices i i , i = 1, . . . , n ptv of nodes pairs that
belong to ω i . ptv is obtained as a sum of total variations computed using
sub-indices i i :and is also written in a matrix form assuming matrix v encodes
subtraction across node pairs across all sub-indices. the third regularization
aims to reduce a null space of the inverse problem in the boundary layer of a
thickness , reflecting the assumption that the target is under the surface. it
is referred to as the boundary regularization and is defined as w χ = 0 where w
selects components of χ that belong to the boundary layer.finally, the fourth
regularization referred to as the minimum volume regularization requires that χ
has at least m 0 non-zero components:optimization framework. in this subsection
we present an incremental fluorescent target reconstruction (iftr) scheme
solving the inverse problem (1)- (2). noting that the nonlinearity of the
inverse problem stems from the fact that χ φ x is a bi-linear vector function
the iftr scheme employs the following splitting method: (i) for n = 0, 1, . . .
fix χ n and compute φ nx as the unique solution of linear excitation
equation:then (ii) fix the obtained φ n x and compute χ n+1 as the unique
solution of one of the 3 convex optimization problems:
m to find χ n+1 :variant ii. this variant imposes the emission equation as an
inequality constraint:here, depending on the value of p we take x 1 = x 2 or x 2
= 1 t x and e m is a parameter defining emission equation constraint
tolerance.variant iii. this variant uses the emission equation as a term of the
loss function:we note that all the three variants depend on parameter p = 1, 2
which defines the type of optimization problem that should be solved: i) if p =
1 we get conic optimization problems of the loss function in the form • 2 which
would be treated as conic constraints); ii) if p = 2 we get quadratic
optimization problems.to get a good initial guess for χ 0 we borrow from the
born approximation which suggests that excitation field φ x can be approximated
by the background excitation obtained by solving excitation equation with no
icg, i.e., χ 0 = 0. iterating this splitting method for n = 0, 1, 2, . . . we
obtain a sequence of updates χ n that converge into a vicinity of the true χ
provided data is "representative enough". we conclude the presentation of iftr
scheme with stopping criteria of the iterative process. for this we use a
standard dice coefficient d(•, •) and a binary projector b(•): the scheme stops
once the following condition is met
to validate the iftr scheme, we performed an experiment capturing the essential
elements of fgs applications. figure 1 describes the experiment setup. the
tissue phantom was composed of a 13×13×30 mm (inner dimensions) glass box filled
with a 1% liposyn solution, which is a fat emulsion with scattering absorption
properties mimicking human soft tissue [12]. the fluorescent target used was a
8×8×8 mm (inner dimensions) acrylic spectrophotometry cuvette filled with a 5%
bsa, 1% liposyn, 7μm icg solution.figure 2 depicts the imaging system consisting
of relatively inexpensive components: a raspberry pi computer (4b/2gb), 12mp rgb
camera (raspberry pi, sc0261) with ir filter removed, 16 mm telephoto lens
(raspberry pi, sc0123), 700-800 nm band stop filter(midwest optical sytems,
db850-25.4), 785 nm laser (roithner lasertechnik, rltmdl-785-300-5) as the
excitation source, and a polyscope fiber bundle (polydiagnost, pd-ps-0095). the
detector and lens are approximately €100 combined, with just 8 bits of dynamic
range. this is in stark contrast to the ultra-sensitive 16-bit scientific
cameras priced an order of magnitude more used in other studies.we collected 3
sets of experimental measurements (see fig. 3): i) y 6 mm top is emission
fluence collected on the top surface from the target immersed 6 mm under the top
surface of tissue phantom; ii) y 3 mm top is emission fluence from the target
immersed 3 mm under the top surface; and iii) y 3 mm top&side is also emission
performance of the proposed iftr scheme is characterized by a set of numerical
experiments. iftr scheme was implemented using the fenics package for fem
matrices computation and cvxpy for the construction of the loss functions and
constraints. additionally cvxpy provides a common interface to various
state-of-the-art optimization solvers making it very easy to switch between
them. although we tested iftr with 4 commonly used solvers: osqp, scs and ecos
(distributed together with cvxpy) and mosek (required an additional
installation) we report results for the solvers that performed the best. thus,
for the quadratic optimization formulation we selected osqp and for the conic
optimization formulation we selected mosek. the resulting configurations were
compared in terms of dice coefficient d(χ est , χ true ) comparing estimated
target and the true target as well as execution time. the results of the
performed experiments are summarised in fig. 4 where the left column of panels
presents dice coefficients and the right column of panels presents respective
execution time. each row of panels in fig. 4 corresponds (from the top to the
bottom) to the experiment using one of three measurements vectors: 1) y 6 mm top
; 2) y 3 mm top and 3) y 3 mm top&side . the first experiment is the most
challenging as it recovers the target 6 mm deep under the surface. yet, variant
i of iftr obtains good reconstruction with both quadratic osqp and conic mosek
solvers for which dice score reaches value of 0.831. good quality reconstruction
is indeed confirmed on the left panel in fig. 5 depicting target recovered by
iftr variant i with quadratic osqp solver and plotted over the true target. the
second experiment recovers the target 3 mm deep which is easier and thus more
iftr variants are capable of obtaining good reconstructions as suggested on the
left panel in the middle row in fig. 4. additionally, the right panel in fig. 5
depicts the target reconstructed by iftr variant ii with conic mosek solver. we
stress that both of these experiments employ reflectance-mode measurements
suggesting the proposed iftr scheme is promising for adoption in fgs-related
applications.the third experiment demonstrates the consistency of iftr scheme:
adding side measurements allows all variants to obtain good reconstructions and
further increases dice coefficients. this, however, comes at a price of
increased computational demands, particularly for variant i solved with
quadratic osqp solver. the performed experiments reveal that it is difficult to
pick a single winning configuration of iftr scheme but there are several
considerations: i) variant i provides the lowest errors but is the slowest
variant with mosek solver has been consistently faster than osqp; ii) variant ii
is the fastest variant but it is more sensitive to the amount of measurement
compared to others; iii) variant iii is less sensitive to the amount of
measurements compared to variant ii has similar execution time but is less
accurate.we also note that iftr scheme is robust with respect to ptv
regularization parameter. this was achieved by scaling the data misfit and ptv
term to similar magnitude: we normalised the misfit term by the norm of the
observations vector and rescaled ptv term by the number of subdomains and each
local total variation weight by the number of nodes in that subdomain. the
robustness to regularization parameter choice was confirmed by our experiments
with several different values of such parameter. another relevant consideration
is that ptv impacts the loss function in a different way compared to a standard
l1 or l2 regularization: the latter has the unique global minimizer (0-vector)
while the former has many global minimizers and iftr benefits from this.
in this work we proposed novel iftr scheme for solving fdot problem. it performs
a splitting of the bi-linearity of the original non-convex problem into a
sequence of convex ones. additionally, iftr restricts the search space by a set
of regularizers promoting piece-wise constant structure of target's indicator
function which in turn allows to recover fluorescent targets from only the
reflectance mode cw measurements collected by a consumer grade camera.although
the scheme was tested using proof-of-concept experimental data and cubical shape
target the method is general and depending on mesh discretization level,
scalable to arbitrary domain and target shapes. thus, the obtained results
suggest strong potential for adoption of iftr scheme in fgs related
applications.
near-infrared photon propagation in tissue like media is described by the
following coupled system of pdes:here ( 9) is the excitation equation describing
the excitation (at wavelength 785 nm) photon fluence φ x , w/cm 2 , and (10)
-emission equation describing emission (at wavelength 830 nm) photon fluence φ m
, w/cm 2 , subscripts x and m indicate excitation and emission respectively. the
parameters of those equations are taken according to the laboratory experiment
setup. γ = 0.016 is a constant representing the dimensionless quantum efficiency
of icg fluorescence emission. d x/m , cm and k x/m , cm -1 refer to coefficients
in excitation and emission equations, which determine light scattering and
absorption properties of tissues:3 μ ax/mi + μ ax/mf + μ sx/m , k x/m = μ ax/mi
+ μ ax/mf (11) where μ sx = 9.84 cm -1 and μ sm = 9.84 cm -1 are the scattering
coefficients of liposyn at excitation and emission wavelength respectively; μ
axi = 0.023 cm -1 and μ ami = 0.0289 cm -1 are the absorption coefficients of
liposyn at excitation and emission wavelengths; μ axf = μ icg χ, cm -1 is the
absorption coefficient of the unknown icg-tagged target and thus depends on the
target's shape modelled by an indicator function χ and icg absorption
coefficient at excitation wavelength μ icg = 3.5 cm -1 ; μ amf = 0 cm -1 as we
assume there is no selfabsorption of icg fluorescence emission at the
concentration ranges employed in this work and for practical applications
[12].the system ( 9)-( 10) is complemented by robin-type boundary conditions
modelling the excitation source applied at the surface of the domain ω:where γ =
2.5156 -dimensionless constant depending on the optical reflective index
mismatch at the boundary.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 49.
multimodal imaging has become increasingly popular in healthcare due to its
ability to provide complementary anatomical and functional information. however,
to fully exploit its benefits, it is crucial to perform accurate and robust
registration of images acquired from different modalities. multimodal image
registration is a challenging task due to differences in image appearance,
acquisition protocols, and physical properties of the modalities. this holds in
particular if ultrasound (us) is involved, and has not been satisfactorily
solved so far.while simple similarity measures directly based on the images'
intensities such as sum of absolute (l1) or squared (l2) differences and
normalized crosscorrelation (ncc) [16] work well in monomodal settings, a more
sophisticated approach is needed when intensities cannot be directly correlated.
historically, a breakthrough in ct-mri registration was achieved by viola and
wells, who proposed mutual information [19]. essentially, it abstracts the
problem to the statistical concept of information theory and optimizes
image-wide alignment statistics. broken down to patch level and inspired by
ultrasound physics, the linear correlation of linear combination (lc 2 ) measure
has shown to work well for us to mri or ct registration [2,22]. while dealing
well with us specifics, it is not differentiable and expensive to compute.as an
alternative to directly assessing similarity on the original images, various
groups have proposed to first compute intermediate representations, and then
align these with conventional l1 or l2 metrics [5,20]. a prominent example is
the modality-independent neighbourhood descriptor (mind) [5], which is based on
image self-similarity and has with minor adaptations (denoted mind-ssc for
self-similarity context) also been applied to us problems [7]. most recently, it
has been shown that using 2d confidence maps-based weighting and adaptive
normalization may further improve registration accuracy [21]. yet, such feature
descriptors are not expressive enough to cope with complex us artifacts and
exhibit many local optima, therefore requiring closer initialization.more
recently, multimodal registration has been approached using various machine
learning (ml) techniques. some of these methods involve the utilization of
convolutional neural networks (cnn) to extract segmentation volumes from the
source data, transforming the problem into the registration of label maps
[13,24]. although these methods have demonstrated promising results, they are
anatomy-specific and require the identification and labeling of structures that
are visible in both modalities. other approaches are trained using ground truth
registrations to directly predict the pose [9,12] or to establish keypoint
correspondences [1,11]. however, these methods are not generalizable to
different anatomies or modalities. moreover, the paucity of precise and
unambiguous ground truth registration, particularly in abdominal mr-us
registration, exacerbates the overfitting problem, restricting generalization
even within the same modality and anatomy. it has furthermore been proposed in
the past to utilize cnns as a replacement for a similarly metric. in [3,17], the
two images being registered are resampled into the same grid in each optimizer
iteration, concatenated and fed into a network for similarity evaluation. while
such a measure can directly be integrated into existing registration methods, it
still suffers from similar limitations in terms of runtime performance and
modality dependance.in contrast, we propose in this work to use a small cnn to
approximate an expensive similarity metric with a straightforward dot product in
its feature space. crucially, our method does not necessitate to evaluate the
cnn at every optimizer iteration. this approach combines ml and classical
multimodal image registration techniques in a novel way, avoiding the common
limitations of ml approaches: ground truth registration is not required, it is
differentiable and computationally efficient, and generalizes well across
anatomies and imaging modalities.
we formulate image registration as an optimization problem of a similarity
metric s between the moving image m and the fixed image f with respect to the
parameters α of a spatial transformation t α : ω → ω. most multi-modal
similarity metrics are defined as weighted sums of local similarities computed
on patches. denoting m • t α the deformed image, the optimization target can be
expressed in the following way:where w(p) is the weight assigned to the point p,
s(•, •) defines a local similarity and the [•] operator extracts a patch (or a
pixel) at a given spatial location. this definition encompasses ssd but also
other more elaborate metrics like lc 2 or mind. the function w is typically used
to reduce the impact of patches with ambiguous content (e.g. with uniform
intensities), or can be chosen to encode prior information on the target
application.the core idea of our method is to approximate the similarity metric
s(p 1 , p 2 ) of two image patches with a dot product φ(p 1 ), φ(p 2 ) where
φ(•) is a function that extracts a feature vector, for instance in r 16 , from
its input patch. when φ is a fully convolutional neural network (cnn), we can
simply feed it the entire volume in order to pre-compute the feature vectors of
every voxel with a single forward pass. the registration objective (eq. 1) is
then approximated asthus converting the original problem into a registration of
pre-computed feature maps using a simple and differentiable dot product
similarity. this approximation is based on the assumption that the cnn is
approximately equivariant to the transformation, i.e.our experiments show that
this assumption (implicitly made also by other descriptors like mind) does not
present any practical impediment. our method exhibits a large capture range and
can converge over a wide range of rotations and deformations.advantages. in
contrast to many existing methods, our approach doesn't require any ground truth
registration and can be trained using patches from unregistered pairs of images.
this is particularly important for multi-modal deformable registration as ground
truths are harder to define, especially on ultrasound. the simplicity of our
training objective allows the use of a cnn with a limited number of parameters
and a small receptive field. this means that the cnn has a negligible
computational cost and can generalize well across anatomies and modalities: a
single network can be used for all types of images and does not need to be
retrained for a new task. furthermore, the objective function (eq. 2) can be
easily differentiated without backpropagating the gradient through the cnn. this
permits efficient gradient-based optimization, even when the original metric is
either non-differentiable or costly to differentiate. finally, we quantize the
feature vectors to 8-bit precision further increasing the computational speed of
registration without impacting accuracy.
we train our model to approximate the three-dimensional lc 2 similarity, as it
showed good performance on a number of tasks, including ultrasound [2,22]. the
lc 2 similarity quantifies whether a target patch can be approximated by a
linear combination of the intensities and the gradient magnitude of the source
patch. in order to reduce the sensitivity on the scale, our target is actually
the average lc 2 over different radiuses of 3, 5, and 7. in order to be
consistent with the original implementation of lc 2 we use the same weighting
function w based on local patch variance. note that the network will be trained
only once, on a fixed dataset that is fully independent of the datasets that
will be used in the evaluation (see sect. 4).dataset. our neural network is
trained using patches from the "gold atlas -male pelvis -gentle radiotherapy"
[14] dataset, which is comprised of 18 patients each with a ct, mr t1, and mr t2
volumes. we resample each volume to a spacing of 2 mm and normalize the voxel
intensities to have zero mean and standard variation of one. since our approach
is unsupervised, we don't make use of the provided registration but leave the
volumes in their standard dicom orientation. as lc 2 requires the usage of
gradient magnitude in one of the modalities, we randomly pick it from either ct
or mr. we would like to report that, initially, we also made use of a
proprietary dataset including us volumes. however, as our investigation
progressed, we observed that the incorporation of us data did not significantly
contribute to the generalization capabilities of our model. consequently, for
the purpose of ensuring reproducibility, all evaluations presented in this paper
exclusively pertain to the model trained solely on the public mr-ct
dataset.patch sampling from unregistered datasets. for each pair of volumes (m,
f ) we repeat the following procedure 5000 times: (1) select a patch from m with
probability proportional to its weight w; (2) compute the similarity with all
the patches of f ; (3) uniformly sample t ∈ [0, 1]; (4) pick the patch of f with
similarity score closest to t. running this procedure on our training data
results in a total of 510000 pairs of patches.
we use the same feed-forward 3d cnn to process all data modalities. the proposed
model is composed of residual blocks [4], leakyrelu activations [10] and uses
blurpool [25] for downsampling, resulting in a total striding factor of 4. we do
not use any normalization layer, as this resulted in a reduction in performance.
the output of the model is 16-channels volume with the norm of each voxel
descriptor clipped at 1. the architecture consists of ten layers and a total of
90,752 parameters, making it notably smaller than many commonly utilized neural
networks.augmentation on the training data is used to make the model as robust
as possible while leaving the target similarity unchanged. in particular, we
apply the same random rotation to both patches, randomly change the sign and
apply random linear transformation on the intensity values. we train our model
for 35 epochs using the l2 loss and batch size of 256. the training converges to
an average patch-wise l2 error of 0.0076 on the training set and 0.0083 on the
validation set. the total training time on an nvidia rtx4090 gpu is 5 h, and
inference on a 256 3 volume takes 70 ms. we make the training code and
preprocessed data openly available online1 .
we present an evaluation of our approach across tasks involving diverse
modalities and anatomies. notably, the experimental data utilized in our
analysis differs significantly from our model's training data in terms of both
anatomical structures and combination of modalities. to assess the effectiveness
of our method, we compare it against lc 2 , which is the metric we approximate,
and mind-ssc [7]. in all experiments, we use a wilcoxon signed-rank test with
p-value 10 -2 to establish the significance of our results.as will be
demonstrated in the next subsections, our method is capable of achieving
comparable levels of accuracy as lc 2 while retaining the speed and flexibility
of mind-ssc. in particular, on abdominal us registration (sect. 4.3) our method
obtains a significantly larger capture range, opening new possibilities for
tackling this challenging problem.
in this experiment, we evaluate the performance of different methods for
estimating affine registration of the retrospective evaluation of cerebral
tumors (resect) miccai challenge dataset [23]. this dataset consists of 22 pairs
of pre-operative brain mrs and intra-operative ultrasound volumes. the initial
pose of the ultrasound volumes exhibits an orientation close to the ground truth
but can contain a significant translation shift. for both mind-ssc and disa-lc 2
, we resample the input volumes to 0.4 mm spacing and use the bfgs [18]
optimizer with 500 random initializations within a range of ±10 • and ±25 mm. we
report the obtained fiducial registration errors (fre) in table 1. disa-lc 2 is
significantly better than mind-ssc while the difference with lc 2 is not
significant. in conclusion, our experiments demonstrate that the proposed
disa-lc 2 , combined with a simple optimization strategy, is capable of
achieving equivalent performance to manually tuned lc 2 .
our second application is the abdomen mr-ct task of the learn2reg challenge 2021
[8]. the dataset comprises 8 sets of mr and ct volumes, both depicting the
abdominal region of a single patient and exhibiting notable deformations. we
estimate dense deformation fields using the methodology outlined in [6] (without
inverse consistency) which first estimates a discrete displacement using
explicit search and then iteratively enforces global smoothness. segmentation
maps of anatomical structures are used to measure the quality of the
registration. in particular, we compute the 25th, 50th, and 75th quantile of the
dice similarity coefficient (dsc) and the 95th quantile of the hausdorff
distance (hd95) between the registered label maps. we compare mind-scc and
disa-lc 2 used with different strides and followed by a downsampling operation
that brings the spacing of the descriptors volumes to 8 mm. the hyperparameters
of the registration algorithm have been manually optimized for each approach.
table 2 shows that our method obtains significantly better results than mind-scc
on the dsc metrics while being not significantly better on hd95.
as the most challenging experiment, we finally use our method to achieve
deformable registration of abdominal 3d freehand us to a ct or mr volume.we are
using a heterogeneous dataset of 27 cases, comprising liver cancer patients and
healthy volunteers, different ultrasound machines, as well as optical vs.
electro-magnetic external tracking, and sub-costal vs. inter-costal scanning of
the liver. all 3d ultrasound data sets are accurately calibrated, with overall
system errors in the range of commercial ultrasound fusion options. between 4
and 9 landmark pairs (vessel bifurcations, liver gland borders, gall bladder,
kidney) were manually annotated by an expert. in order to measure the capture
range, we start the registration from 50 random rigid poses around the ground
truth and calculate the fiducial registration error (fre) after optimization.
for local optimization, lc 2 is used in conjunction with bobyqa [15] as in the
original paper [22], while mind-scc and disa-lc 2 are instead used with bfgs.
due to an excessive computation time, we don't do global optimization with lc 2
while with other methods we use bfgs with 500 random initializations within a
range of ±40 • and ±150 mm. we use six parameters to define the rigid pose and
two parameters to describe the deformation caused by the ultrasound probe
pressure.from the results shown in table 3 and fig. 2, it can be noticed that
the proposed method obtains a significantly larger capture range than mind-scc
and lc 2 while being more than 300 times faster per evaluation than lc 2 (the
times reported in the table include not just the optimization but also
descriptor extraction). the differentiability of our objective function allows
our method to converge in fewer iterations than derivative-free methods like
bobyqa. furthermore, the evaluation speed of our objective function allows us to
exhaustively search the solution space, escaping local minima and converging to
the correct solution with pose and deformation parameters at once, in less than
two seconds.note that this registration problem is much more challenging than
the prior two due to difficult ultrasonic visibility in the abdomen, strong
deformations, and ambiguous matches of liver vasculature. therefore, to the best
of our knowledge, these results present a significant leap towards reliable and
fully automatic fusion, doing away with cumbersome manual landmark placements.
we have discovered that a complex patch-based similarity metric can be
approximated with feature vectors from a cnn with particularly small
architecture, using the same model for any modality. the training is
unsupervised and merely requires unregistered data. after features are extracted
from the volumes, the actual registration comprises a simple iterative
dot-product computation, allowing for global and derivative-based optimization.
this novel combination of classical image processing and machine learning
elevates multi-modal registration to a new level of performance, generality, but
also algorithm simplicity.we demonstrate the efficiency of our method on three
different use cases with increasing complexity. in the most challenging
scenario, it is possible to perform global optimization within seconds of both
pose and deformation parameters, without any organ-specific distinction or
successive increase of parameter sizes.while we specifically focused on
developing an unsupervised and generic method, a sensible extension would be to
specialize our method by including global information, such as segmentation
maps, into the approximated measure or by making use of ground-truth
registration during training. finally, the cross-modality feature descriptors
produced by our model could be exploited by future research for tasks different
from registration such as modality synthesis or segmentation.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 72.
computed tomography (ct) is one of the most widely used technologies in medical
imaging, which can assist doctors for diagnosing the lesions in human internal
organs. due to harmful radiation exposure of standard-dose ct, the low dose ct
is more preferable in clinical application [4,6,34]. however, when the dose is
low together with the issues like sparse-view or limited angles, it becomes
quite challenging to reconstruct high-quality ct images. the high-quality ct
images are important to improve the performance of diagnosis in clinic [27]. in
mathematics, we model the ct imaging as the following procedure y = t (x r ) +
δ, (1) where x r ∈ r d denotes the unknown ground-truth picture, y ∈ r m denotes
the received measurement, and δ is the noise. the function t represents the
forward operator that is analogous to the radon transform, which is widely used
in medical imaging [23,28]. the problem of ct reconstruction is to recover x r
from the received y.solving the inverse problem of ( 1) is often very
challenging if there is no any additional information. if the forward operator t
is well-posed and δ is neglectable, we know that an approximate x r can be
easily obtained by directly computing t -1 (y). however, t is often ill-posed,
which means the inverse function t -1 does not exist and the inverse problem of
(1) may have multiple solutions. moreover, when the ct imaging is low-dose, the
filter backward projection (fbp) [11] can produce serious detrimental artifact.
therefore, most of existing approaches usually incorporate some prior knowledge
during the reconstruction [14,17,26]. for example, a commonly used method is
based on regularization:where • p denotes the p-norm and r(x) denotes the
penalty item from some prior knowledge.in the past years, a number of methods
have been proposed for designing the regularization r. the traditional
model-based algorithms, e.g., the ones using total variation [3,26], usually
apply the sparse gradient assumptions and run an iterative algorithm to learn
the regularizers [12,18,24,29]. another popular line for learning the
regularizers comes from deep learning [13,17]; the advantage of the deep
learning methods is that they can achieve an end-to-end recovery of the true
image x r from the measurement y [1,21]. recent researches reveal that
convolutional neural networks (cnns) are quite effective for image denoising,
e.g., the cnn based algorithms [10,34] can directly learn the reconstructed
mapping from initial measurement reconstructions (e.g., fbp) to the ground-truth
images. the dual-domain network that combines the sinograms with reconstructed
low-dose ct images were also proposed to enhance the generalizability [15,30].a
major drawback of the aforementioned reconstruction methods is that they deal
with the input ct 2d slices independently (note that the goal of ct
reconstruction is to build the 3d model of the organ). namely, the neighborhood
correlations among the 2d slices are often ignored, which may affect the
reconstruction performance in practice. in the field of computer vision,
"optical flow" is a common technique for tracking the motion of object between
consecutive frames, which has been applied to many different tasks like video
generation [35], prediction of next frames [22] and super resolution synthesis
[5,31]. to estimate the optical flow field, existing approaches include the
traditional brightness gradient methods [2] and the deep networks [7]. the idea
of optical flow has also been used for tracking the organs movement in medical
imaging [16,20,33]. however, to the best of our knowledge, there is no work
considering gans with using optical flow to capture neighbor slices coherence
for low dose 3d ct reconstruction.in this paper, we propose a novel optical flow
based generative adversarial network for 3d ct reconstruction. our intuition is
as follows. when a patient is located in a ct equipment, a set of consecutive
cross-sectional images are generated. if the vertical axial sampling space of
transverse planes is small, the corresponding ct slices should be highly
similar. so we apply optical flow, though there exist several technical issues
waiting to solve for the design and implementation, to capture the local
coherence of adjacent ct images for reducing the artifacts in low-dose ct
reconstruction. our contributions are summarized below:1. we introduce the
"local coherence" by characterizing the correlation of consecutive ct images,
which plays a key role for suppressing the artifacts. 2. together with the local
coherence, our proposed generative adversarial networks (gans) can yield
significant improvement for texture quality and stability of the reconstructed
images. 3. to illustrate the efficiency of our proposed approach, we conduct
rigorous experiments on several real clinical datasets; the experimental results
reveal the advantages of our approach over several state-of-the-art ct
reconstruction methods.
in this section, we briefly review the framework of the ordinary generative
adversarial network, and also introduce the local coherence of ct slices.
traditional generative adversarial network [8] consists of two main modules, a
generator and a discriminator. the generator g is a mapping from a latent-space
gaussian distribution p z to the synthetic sample distribution p xg , which is
expected to be close to the real sample distribution p x . on the other hand,
the discriminator d aims to maximize the distance between the distributions p xg
and p x . the game between the generator and discriminator actually is an
adversarial process, where the overall optimization objective follows a min-max
principle:local coherence. as mentioned in sect. 1, optical flow can capture the
temporal coherence of object movements, which plays a crucial role in many
videorelated tasks. more specifically, the optical flow refers to the
instantaneous velocity of pixels of moving objects on consecutive frames over a
short period of time [2]. the main idea relies on the practical assumptions that
the brightness of the object more likely remains stable across consecutive
frames, and the brightness of the pixels in a local region are usually changed
consistently [9].based on these assumptions, the brightness of optical flow can
be described by the following equation:where v = (v w , v h ) represents the
optical flow of the position (w, h) in the image. ∇i = (∇i w , ∇i h ) denotes
spatial gradients of image brightness, and ∇i t denotes the temporal partial
derivative of the corresponding region. following the eq. ( 4), we consider the
question that whether the optical flow idea can be applied to 3d ct
reconstruction. in practice, the brightness of adjacent ct images often has very
tiny difference, due to the inherent continuity and structural integrity of
human body. therefore, we introduce the "local coherence" that indicates the
correlation between adjacent images of a tissue. namely, adjacent ct images
often exhibit significant similarities within a certain local range along the
vertical axis of the human body. due to the local coherence, the noticeable
variations observed in ct slices within the local range often occur at the edges
of organs. we can substitute the temporal partial derivative ∇i t by the
vertical axial partial derivative ∇i z in the eq. ( 4), where "z" indicates the
index of the vertical axis. as illustrated in fig. 1, the local coherence can be
captured by the optical flow between adjacent ct slices.
in this section, we introduce our low-dose ct image generation framework with
local coherence in detail.the framework of our network. the proposed framework
comprises three components, including a generator g, a discriminator d and an
optical flow estimator f. the generator is the core component, and the flow
estimator provides auxiliary warping images for the generation process.suppose
we have a sequence of measurements y 1 , y 2 , • • • , y n ; for each y i , 1 ≤
i ≤ n, we want to reconstruct its ground truth image x r i as the eq. ( 1).
before performing the reconstruction in the generator g, we apply some prior
knowledge in physics and run filter backward projection on the measurement y i
in eq. ( 1) to obtain an initial recovery solution s i . usually s i contains
significant noise comparing with the ground truth x r i . then the network has
two input components, i.e., the initial backward projected image s i that serves
as an approximation of the ground truth x r i , and a set of neighbor ct slices
n (s i ) = {s i-1 , s i+1 }1 for preserving the local coherence. the overall
structure of our framework is shown in fig. 2. below, we introduce the three key
parts of our framework separately. optical flow estimator. the optical flow f(n
(s i ), s i ) denotes the brightness changes of pixels from n (s i ) to s i ,
where it captures their local coherence. the estimator is derived by the network
architecture of flownet [7]. the flownet is an autoencoder architecture with
extraction of features of two input frames to learn the corresponding flow,
which is consist of 6 (de)convolutional layers for both encoder and
decoder.discriminator. the discriminator d assigns the label "1" to real
standarddose ct images and "0" to generated images. the goal of d is to maximize
the separation between the distributions of real images and generated
images:where x g i is the image generated by g (the formal definition for x g i
will be introduced below). the discriminator includes 3 residual blocks, with 4
convolutional layers in each residual block.generator. we use the generator g to
reconstruct the high-quality ct image for the ground truth x r i from the
low-dose image s i . the generated image is obtained bywhere w(•) is the warping
operator. before generating x g i , n (x g i ) is reconstructed from n (s i ) by
the generator without considering local coherence. subsequently, according to
the optical flow f(n (s i ), s i ), we warp the reconstructed images n (x g i )
to align with the current slice by adjusting the brightness values. the warping
operator w utilizes bi-linear interpolation to obtain w(n (x g i )), which
enables the model to capture subtle variations in the tissue from the generated
n (x g i ); also, the warping operator can reduce the influence of artifacts for
the reconstruction. finally, x g i is generated by combining s i and w(n (x g i
)). since x r i is our target for reconstruction in the i-th batch, we consider
the difference between x g i and x r i in the loss. our generator is mainly
based on the network architecture of unet [25]. partly inspired by the loss in
[5], the optimization objective of the generator g comprises three items with
the coefficients λ pix , λ adv , λ per ∈ (0, 1):in (7), "l pixel " is the loss
measuring the pixel-wise mean square error of the generated image x g i with
respect to the ground-truth x r i . "l adv " represents the adversarial loss of
the discriminator d, which is designed to minimize the distance between the
generated standard-dose ct image distribution p xg and the real standard-dose ct
image distribution p x . "l percept " denotes the perceptual loss, which
quantifies the dissimilarity between the feature maps of x r i and x g i ; the
feature maps denote the feature representation extracted from the hidden layers
in the discriminator d (suppose there are t hidden layers):where d j (•) refers
to the feature extraction performed on the j-th hidden layer. through capturing
the high frequency differences in ct images, l percept can enhance the sharpness
for edges and increase the contrast for the reconstructed images. l pixel and l
adv are designed to recover global structure, and l percept is utilized to
incorporate additional texture details into the reconstruction process.
datasets. first, our proposed approaches are evaluated on the "mayo-clinic
low-dose ct grand challenge" (mayo-clinic) dataset of lung ct images [19].the
dataset contains 2250 two dimensional slices from 9 patients for training, and
the remaining 128 slices from 1 patient are reserved for testing. the lowdose
measurements are simulated by parallel-beam x-ray with 200 (or 150) uniform
views, i.e., n v = 200 (or n v = 150), and 400 (or 300) detectors, i.e., n d =
400 (or n d = 300). in order to further verify the denoising ability of our
approaches, we add the gaussian noise with standard deviation σ = 2.0 to the
sinograms after x-ray projection in 50% of the experiments. to evaluate the
generalization of our model, we also consider another dataset rider with
nonsmall cell lung cancer under two ct scans [36] for testing. we randomly
select 4 patients with 1827 slices from the dataset. the simulation process is
identical to that of mayo-clinic. the proposed networks were implemented in the
pytorch framework and trained on nvidia 3090 gpu with 100 epochs.baselines and
evaluation metrics. we consider several existing popular algorithms for
comparison. ( 1) fbp [11]: the classical filter backward projection on low-dose
sinograms. ( 2) fbpconvnet [10]: a direct inversion network followed by the cnn
after initial fbp reconstruction. ( 3) lpd [1]: a deep learning method based on
proximal primal-dual optimization. ( 4) uar [21]: an end-toend reconstruction
method based on learning unrolled reconstruction operators and adversarial
regularizers. our proposed method is denoted by gan-lc.we set λ pix = 1.0, λ adv
= 0.01 and λ per = 1.0 for the optimization objective in eq. ( 7) during our
training process. following most of the previous articles on 3d ct
reconstruction, we evaluate the experimental performance by two metrics: the
peak signal-to-noise ratio (psnr) and the structural similarity index (ssim)
[32]. psnr measures the pixel differences of two images, which is negatively
correlated with mean square error. ssim measures the structure similarity
between two images, which is related to the variances of the input images. for
both two measures, the higher the better.results. a similar increasing trend
with our approach across different settings but has worse reconstruction
quality. to evaluate the stability and generalization of our model and the
baselines trained on mayo-clinic dataset, we also test them on the rider
dataset. the results are shown in table 2. due to the bias in the datasets
collected from different facilities, the performances of all the models are
declined to some extents. but our proposed approach still outperforms the other
models for most testing cases.to illustrate the reconstruction performances more
clearly, we also show the reconstruction results for testing images in fig. 3.
we can see that our network can reconstruct the ct image with higher quality.
due to the space limit, the experimental results of different views n v and more
visualized results are placed in our supplementary material.
in this paper, we propose a novel approach for low-dose ct reconstruction using
generative adversarial networks with local coherence. by considering the
inherent continuity of human body, local coherence can be captured through
optical flow, which is small deformations and structural differences between
consecutive ct slices. the experimental results on real datasets demonstrate the
advantages of our proposed network over several popular approaches. in future,
we will evaluate our network on real-world ct images from local hospital and use
the reconstructed images to support doctors for the diagnosis and recognition of
lung nodules. our code is publicly available at
https://github.com/lwjie595/ganlc.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 50.
radiotherapy (rt) is one of the cornerstones of cancer patients. it utilizes
ionizing radiation to eradicate all cells of a tumor. the total radiation dose
is typically divided over 3-30 daily fractions to optimize its effect. as the
surrounding normal tissue is also sensitive to radiation, highly accurate
delivery is vital. image guided rt (igrt) is a technique to capture the anatomy
of the day using in room imaging in order to align the treatment beam with the
tumor location [1]. cone beam ct (cbct) is the most widely used imaging modality
for igrt.a major challenge especially for cbct imaging of the thorax and
upperabdomen is the respiratory motion that introduces blurring of the anatomy,
reducing the localization accuracy and the sharpness of the image.a technique
used to alleviate motion artifacts is respiratory correlated cbct (4dcbct) [16].
from the projections, it is possible to extract a respiratory signal [12], which
indicates the position of the organs within the patient during breathing. with
this, subsets of the projections can be defined to create reconstructions that
resolve the motion. however, only 20 to 60 respiratory periods are imaged. this
limits the number of projections available and results in view-aliasing [16].
additionally, the projections are affected by stochastic measurement noise
caused by the finite imaging dose used, which further degrades the quality of
the reconstruction even when all projections are used.several traditional
methods based on iterative reconstruction algorithms and motion compensation
techniques are used to reduce view-aliasing in 4dcbcts [7,10,11,14,15]. although
effective, these methods suffer from motion modeling uncertainty and prolonged
reconstruction times.deep learning has been proposed as a way to address
view-aliasing with accelerated reconstruction [6]. however, the method cannot
reduce measurement noise because it is still present in the images used as
targets during training.a different method, called noise2inverse, uses an
unsupervised approach to reduce measurement noise in the traditional ct setting
[4]. there are two ways to apply it to 4dcbct and both fail to reduce stochastic
noise effectively. the first is to apply noise2inverse to each
respiratory-correlated reconstruction. in this case, the method will struggle
because of the very low number of projections that are available. the second is
to apply noise2inverse directly to all the projections. in this case, the motion
artifacts that blur the image will appear again, as noise2inverse requires
averaging the sub-reconstructions to obtain a clean reconstruction.we propose
noise2aliasing to address these limitations. the method can be used to provably
train models to reduce both view-aliasing artifacts and stochastic noise from
4dcbcts in an unsupervised way. training deep learning models for medical
applications often needs new data. this was not the case for noise2aliasing, and
historical clinical data sufficed for training.we validated our method on
publicly available data [15] against a supervised approach [6] and applied it to
an internal clinical dataset of 30 lung cancer patients. we explore different
dataset sizes to understand their effects on the reconstructed images.
in this section, we will introduce the concepts and the notation necessary to
understand the method and the choices made during implementation.unsupervised
noise reduction with noise2noise. given input-target pairs x, y ∈ r we can
define the regression problem in the one-dimensional setting as finding f * : r
→ r which satisfies the following:which can be minimized point-wise [3],
yielding:in noise2noise [5], input-target pairs are two samples of the same
image that only differ because of some independent mean-zero noise (x + δ 1 , x
+ δ 2 ) withthen f * will recover the input image without any noise:denoising
for tomography with noise2inverse. during a ct scan, a volume x is imaged by
acquiring projections y = ax using an x-ray source and a detector placed on the
opposite side of the volume. the projections can then be used by an algorithm
that computes a linear operator r to obtain an approximation of the original
distribution of x-ray attenuation coefficients x = ry. the algorithm can also
operate on a subset of the projections. let j = {1, 2, . . . } be the set of all
projections and j ⊂ j , then xj = r j y j is the reconstruction obtained using
only projections y j . let us now assume that the projections have some meanzero
noise ỹi = y i + with e (ỹ i ) = y i . then, in noise2inverse [4] the results
from noise2noise are extended to find a function f * which removes projection
noise when trained using noisy reconstructions xj = r j ỹj = r j y j + r j = xj
+ r j and the expected mse as loss function. in particular, they find that the
loss function can be decomposed in the following way:where j is a random
variable that picks subsets of projections at random and j is its complementary.
given eq. 2, we observe that function f * which minimizes l is:when using
reconstructions from a subset of noisy projections as input and reconstructions
from their complementary as its output, a neural network will learn to predict
the expected reconstruction without the noise.property of expectation over
subsets of projections using fdk. now let j be a random variable that selects
subsets of projections j ⊂ j at random such that each projection is selected at
least once. define r j : r d d ×|j| → r dv to be the fdk reconstruction
algorithm [2] that reconstructs a volume of dimensionality d v from projections
j each with dimensionality d d (geometrical details on the exact setup are not
relevant). the fdk uses, as its fundamental step, the dual radon transform [9],
which is a weighted summation that can be written as an expectation. then, the
following holds:
here, we propose noise2aliasing, an unsupervised method capable of reducing both
view-aliasing and projection noise in 4dcbcts. at the core of this method is the
following proposition.proposition. given the projection set j = {1, 2, . . . },
the fdk reconstruction algorithm r, and the noisy projections ỹ = ax+ with
mean-zero element-wise independent noise. let j 1 , j 2 be two random variables
that pick different subsets at random belonging to a partition of j , andbe the
input-target pairs in dataset d of reconstructions using disjoint subsets of
noisy projections. let l be the expected mse over d with respect to a function f
: r dv → r dv and the previously-described input-target pairs. then, we find
that the function f * that minimizes l for any given j ∈ j will reconstruct the
volume using all the projections and remove the noise :proof. the loss function
l is defined in the following way:additionally, j 1 , j 2 are disjoint, the
noise is mean-zero element-wise, and we are using the fdk reconstruction
algorithm which defines a linear operator r. these allow us to use eq. 5 to find
that the function f * that minimizes l is the following:this is sufficient to
reduce stochastic noise but we need to further manipulate this expression to
address view aliasing. simplifying notation and using the properties of
conditional expectations, we can write:now assume that xj1 is the clean
reconstruction that is consistent with the observed noisy reconstruction z
obtained from each disjoint subset j 2 , then:finally, we use the property of
the fdk from eq. 6:
the proposition guided the choice of reconstruction method to be fdk and the
design of the subset selection method from considerations that are now
explained. equation 12 holds true only when the same underlying clean
reconstruction x can be determined from the noisy reconstruction using any
subset from a partition of the projections j . this means that, in our dataset,
we should have at our disposal reconstructions of the same underlying volume x
using disjoint subsets of projections. in 4dcbcts this is not the case, as
separate respiratory phases are being reconstructed, where the organs are in
different positions. we can address this problem by carefully choosing subsets
of projections that result in respiratory-uncorrelated reconstructions. the
reconstructions will display organs in their average position and, therefore,
have the same underlying structure. when the projections are selected with the
same sampling pattern as the one used in respiratory-correlated reconstructions,
then the view-aliasing artifacts display will have the same pattern as the ones
present in the 4dcbcts.compared to previous work, to obtain the additional
effect of reducing projection noise, the respiratory-uncorrelated
reconstructions must use non-overlapping subsets of projections. coincidentally,
a previously proposed subset selection method utilized for supervised aliasing
reduction fits all these requirements and will, therefore, be used in this work
[4].
first, we used the spare varian dataset to study whether noise2aliasing can
match the performance of the supervised baseline and if it can outperform it
when adding noise to the projections. then, we use the internal dataset to
explore the requirements for the method to be applied to an existing clinical
dataset. these required around 64 gpu days on nvidia a100 gpus.training of the
model is done on 2d slices. the projections obtained during a scan are
sub-sampled according to the pseudo-average subset selection method described in
[6] and then used to obtain 3d reconstructions. in noise2aliasing these are used
for both input and target during training. given two volumes (x, y), the
training pairs (x i (k) , y i (k) ) are the same i-th slice along the k-th
dimension of each volume chosen to be the axial plane.the datasets used in this
study are two:1. the spare varian dataset was used to provide performance
results on publicly available patient data. to more closely resemble normal
respiratory motion per projection image, the 8 min scan has been used from each
patient (five such scans are available in the dataset). training is performed
over 4 patients while 1 patient is used as a test set. the hyperparameters are
optimized over the training dataset.2. an internal dataset (irb approved) of 30
lung cancer patients' 4dcbcts from 2020 to 2022, originally used for igrt, with
25 patients for training and 5 patients for testing. the scans are 4 min 205 •
scans with 120kev source and 512 × 512 sized detector, using elekta linacs. the
data were anonymized prior to analysis.projection noise was added using the
poisson distribution to the spare varian dataset to evaluate the ability of the
unsupervised method to reduce it. given a projected value of p and a photon
count π (chosen to be 2500), the rate of the poisson distribution is defined as
πe -p and given a sample q from this distribution, then the new projected value
is p =log q π .the architecture used in this work is the mixed scale dense cnn
(msd) [8], the most successful architecture from noise2inverse [4]. the msd
makes use of dilated convolutions to process features at all scales of the
image. we use the msd with depth 200 and width 1, adam optimizer, mse loss, a
batch size of 16, and a learning rate of 0.0001.the baselines we compare against
are two. the first is the traditional fdk obtained using rtk [13]. the second is
the supervised approach proposed by [6], where we replace the model with the
msd, for a fair comparison. in the supervised approach, the model is trained by
using as input reconstructions obtained from subsets defined with pseudo-average
subset selection while the targets use all of the projections available.the
metrics used in this work are the root mean squared error (rmse), peak
signal-to-noise ratio (psnr), and structural similarity index measure (ssim)
[17] all the metrics are defined between the output of the neural network and a
3d (cb)ct scan. for the spare varian dataset, we use the rois defined provided
[15] and used the 3d reconstruction using all the projections available as a
ground truth. for the internal dataset, we deformed the planning ct to each of
the phases reconstructed using the fdk algorithm and evaluate the metric over
only the 4dcbct volume boundaries.
spare varian. inference speed with the nvidia a100 gpu averages 600ms per volume
made of 220 slices. from the qualitative evaluation of the methods in fig. 1,
noise2aliasing matches the visual quality of the supervised approach on the
low-noise dataset on both soft tissue and bones. the metrics in table 1 show
mean and standard deviation across all phases for a single patient. in the
lownoise setting, both supervised and noise2aliasing outperform fdk with very
similar results, often within a single standard deviation. noise2aliasing
successfully matches the performance of the supervised baseline. noisy spare
varian. from fig. 1 and table 1, the supervised approach reproduces the noise
that was seen during training, while noise2aliasing manages to remove it
consistently, outperforming the supervised approach, especially in the soft
tissue area around the lungs, where the noise affects attenuation coefficients
the most.noise2aliasing is capable of reducing the artifacts present in
reconstructions caused by stochastic noise in the projections used,
outperforming the supervised baseline.internal dataset. noise2alisting trained
on 25 patients and tested on 5 achieved mean psnr of 35.24 and ssim of 0.91,
while the clinical method achieved mean psnr of 29.97 and 0.74 ssim with p-value
of 0.048 for the psnr and 0.0015 for the ssim, so noise2aliasing was
significantly better according to both metrics. additionally, from fig. 3 we can
see how the breathing extent is matched with sharp reconstruction of the
diaphragm. overall, using more patients results in better noise reduction and
sharper reconstructions (see fig. 2), fig. 2. reconstruction using
noise2aliasing with different-sized datasets. with fewer patients, the model is
more conservative and tends to keep more noise, but also smudges the interface
between tissues and bones. with more patients, more of the view-aliasing is
addressed, and the reconstruction is sharper, however, a few small anatomical
structures tend to be suppressed by the model.especially between fat tissue and
skin and around the bones. however, the model also tends to remove small
anatomical structures as high-frequency objects that cannot be distinguished
from the noise.when applied to a clinical dataset, noise2aliasing benefits from
more patients being included in the dataset, however, qualitatively good
performance is already achieved with 5 patients. no additional data collection
was required and the method can be applied without major changes to the current
clinical practice.
we have presented noise2aliasing, a method to provably remove both viewaliasing
and stochastic projection noise from 4dcbcts using an unsupervised deep learning
method. we have empirically demonstrated its performance on a publicly available
dataset and on an internal clinical dataset. noise2aliasing outperforms a
supervised approach when stochastic noise is present in the projections and
matches its performance on a popular benchmark. noise2aliasing can be trained on
existing historical datasets and does not require changing current clinical
practices. the method removes noise more reliably when the dataset size is
increased, however further analysis is required to establish a good quantitative
measurement of this phenomenon. as future work, we plan to study noise2aliasing
in the presence of changes in the breathing frequency and amplitude between
patients and during a scan.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 46.
positron emission tomography (pet) is a widely used modality in functional
imaging for oncology, cardiology, neurology, and medical research [1]. however,
pet images often suffer from a high level of noise due to several physical
degradation factors as well as the ill-conditioning of the pet reconstruction
problem.as a result, the quality of pet images can be compromised, leading to
difficulties in accurate diagnosis. deep learning (dl) techniques, especially
supervised learning, have recently garnered considerable attention and show
great promise in pet image reconstruction compared with traditional analytical
methods and iterative methods. among them, four primary approaches have emerged:
dl-based postdenoising [2,3], end-to-end direct learning [4][5][6], deep
learning regularized iterative reconstruction [7][8][9][10] and deep unrolled
methods [11][12][13].dl-based post denoising methods are relatively
straightforward to implement but can not reduce the lengthy reconstruction time
and its results are significantly affected by the pre-reconstruction algorithm.
end-to-end direct learning methods utilize deep neural networks to learn the
directing mapping from measurement sinogram to pet image. without any physical
constraints, these methods can be unstable and extremely data-hungry. deep
learning regularized iterative reconstruction methods utilize a deep neural
network as a regularization term within the iterative reconstruction process to
regularize the image estimate and guide the reconstruction process towards a
more accurate and stable solution. despite the incorporation of deep learning,
the underlying mathematical framework and assumptions of deep learning
regularized iterative methods still rely on the conventional iterative
reconstruction methods. deep unrolled methods utilize a dnn to unroll the
iterative reconstruction process and to learn the mapping from sinogram to the
reconstructed pet images, which potentially result in more accurate and
explainable image reconstruction. deep unrolled methods have demonstrated
improved interpretabillity and yielded inspiring outcomes.however, the
aforementioned approaches for pet image reconstruction depend on high quality
ground truths as training labels, which can be diffi-cult and expensive to
obtain. this challenge is further compounded by the high dose exposure
associated with pet imaging. unsupervised/self supervised learning has gained
considerable interest in medical imaging, owing to its ability to mitigate the
need for high-quality training labels. gong et al. proposed a pet image
reconstruction approach using the deep image prior (dip) framework [15], which
employed a randomly initialized unet as a prior. in another study, fumio et al.
proposed a simplified dip reconstruction framework with a forward projection
model, which reduced the network parameters [16]. shen et al. proposed a deepred
framework with an approximate bayesian framework for unsupervised pet image
reconstruction [17]. these methods all utilize generative models to generate pet
images from random noise or mri prior images and use sinogram to design loss
functions. however, these generative models tend to favor low frequencies and
sometimes lack of mathematical interpretability. in the absence of anatomic
priors, the network convergence can take a considerable amount of time,
resulting in prolonged reconstruction times. recently, equivariant property [18]
of medical imaging system is proposed to train the network without labels, which
shows the potential for the designing of pet reconstruction algorithms.in this
paper, we propose a dual-domain unsupervised learned descent algorithm for pet
image reconstruction, which is the first attempt to combine unsupervised
learning and deep unrolled method for pet image reconstruction. the main
contributions of this work are summarized as follows: 1) a novel model based
deep learning method for pet image reconstruction is proposed with a learnable l
2,1 norm for more general and robust feature sparsity extraction of pet images;
2) a dual domain unsupervised training strategy is proposed, which is
plug-and-play and does not need paired training samples; 3) without any anatomic
priors, the proposed method shows superior performance both quantitatively and
visually.
as a typical inverse problem, pet image reconstruction can be modeled in a
variational form and cast as an optimization task, as follows:where y is the
measured sinogram data, y is the mean of the measured sinogram.x is the pet
activity image to be reconstructed, l(y|x) is the poisson loglikelihood of
measured sinogram data. p (x; θ) is the penalty term with learnable parameter θ.
a ∈ r i×j is the system response matrix, with a ij representing the
probabilities of detecting an emission from voxel j at detector i.we expect that
the parameter θ in penalty term p can be learned from the training data like
many other deep unrolling methods. however, most of these methods directly
replace the penalty term [14] or its gradient [11,13] with a network, which
loses some mathematical rigor and interpretablities.
we choose to parameterize p as the l 2,1 norm with a feature extraction operator
g(x) to be learned in the training data. the smooth nonlinear mapping g is used
to extract sparse features and the l 2,1 norm is used as a robust and effective
sparse feature regularization. specifically, we formulate p as follows
[19]:where g i,θ (x) is i-th feature vector. we choose g as a multi-layered cnn
with nonlinear activation function σ, and σ is a smoothed relu:in this case, the
gradient ∇g can be computed directly. the nesterov's smoothing technique is used
in p for the derivative calculation of the l 2,1 norm through smooth
approximation:where parameter ε controls how close the approximation p ε to the
original p .
input: image initialization x0, ρ, γ ∈ (0, 1), ε0, σ, τ > 0, maximum number of
iteration i, total phase numbers k and measured sinogram y 1:
with the parametric form of learnable regularization given above, we rewrite eq.
1 as the objective function: min φ(x; y, θ) = -l(y|x) + p ε (x; θ) (8) we
unrolled the learned descent algorithm in several phases as shown in fig. 1.in
each phase k -1, we apply the proximal gradient step in eq. 8:where the proximal
operator is defined as:in order to have a close form solution of the proximal
operator, we perform a taylor approximation of p ε k-1 :after discarding
higher-order constant terms, we can simplify the eq. 10 as:where α k-1 and β k-1
are two parameters greater than 0 andwe also calculate a close-form safeguard v
k as:the line search strategy is used by shrinking α k-1 to ensure objective
function decay. we choose the u k or v k with smaller objection function value φ
ε k-1 to be the next x k . the smoothing parameter ε k-1 is shrinkage by γ ∈ (0,
1) if the ||∇φ ε k-1 (x k )|| < σγε k-1 is satisfied. the whole flow is shown in
algorithm 1.
the whole reconstruction network is indicated by f θ with learned parameter
θ.inspired by deep image prior [20] and equivariance [18] of pet imaging system,
the proposed dual-domain unsupervised training loss function is formulated
as:where λ is the parameter that controls the ratio of different domain loss
function, which was set to 0.1 in the experiments. for image domain loss l image
, the equivariance constraint is used. for example, if the test sample x t first
undergoes an equivariant transformation, such as rotation, we obtain x tr .
subsequently, we perform a pet scan to obtain the sinogram data of x tr and x t
. the image reconstructed by the f θ of these two sinogram should also keep this
rotation properties. the l image is formulate as: where t r denotes the rotation
operator, a is the forward projection which also can be seen as a measurement
operator. for sinogram domain loss l measure , the data argumentation with
random noise ξ is performed on y:
we implemented dulda using pytorch 1.7 on a nvidia geforce gtx titan x. the adam
optimizer with a learning rate of 10 -4 was used and trained for 100 epochs with
batch size of 8. the total unrolled phase was 4. the image x 0 was initialized
with the values of one. the smoothing parameter ε 0 and δ were initialized to be
0.001 and 0.002. the step-size α 0 and β 0 were initialized to be 0.01 and 0.02.
the system matrix was computed by using michigan image reconstruction toolbox
(mirt) with a strip-integral model [21]. the proposed dulda was compared with
mlem [22], total variation regularized em (em-tv) [23] and deep image prior
method (dip) [16]. for both mlem and em-tv, 25 iterations were adopted. the
penalty parameter for em-tv was 2e -5 .for dip, we used random noise as input
and trained 14000 epochs with the same training settings as dulda to get the
best results before over-fitting.the proposed method can also be trained in a
fully supervised manner (we call it slda). the loss is the mean square error
between the output and the label image. to further demonstrate the
effectiveness, we compared slda with deeppet [5] and fbsem [11], the training
settings remained the same.
forty 128 × 128 × 40 3d zubal brain phantoms [24] were used in the simulation
study as ground truth, and one clinical patient brain images with different dose
level were used for the robust analysis. two tumors with different size were
added in each zubal brain phantom. the ground truth images were firstly
forward-projected to generate the noise-free sinogram with count of 10 6 for
each transverse slice and then poisson noise were introduced. 20 percent of
uniform random events were simulated. in total, 1600 (40 × 40) 2d sinograms were
generated. among them, 1320 (33 samples) were used in training, 200 (5 samples)
for testing, and 80 (2 samples) for validation. a total of 5 realizations were
simulated and each was trained/tested independently for bias and variance
calculation [15]. we used peak signal to noise ratio (psnr), structural
similarity index (ssim) and root mean square error (rmse) for overall
quantitative analysis. the contrast recovery coefficient (crc) [25] was used for
the comparison of reconstruction results in the tumor region of interest (roi)
area.
figure 2 shows three different slices of the reconstructed brain pet images
using different methods. the dip method and proposed dulda have lower noise
compared with mlem and em-tv visually. however, the dip method shows unstable
results cross different slices and fails in the recovery of the small cortex
region. the proposed dulda can recover more structural details and the white
matter appears to be more sharpen. the quantitative and bias-variance results
are shown in table 1. we noticed that dip method performs even worse than mlem
without anatomic priors. the dip method demonstrates a certain ability to reduce
noise by smoothing the image, but this leads to losses in important structural
information, which explains the lower psnr and ssim. both dip method and dulda
have a better crc and bias performance compared with mlem and em-tv. in terms of
supervised training, slda also performs best.
to test the robustness of proposed dulda, we forward-project one patient brain
image data with different dose level and reconstructed it with the trained dulda
model. the results compared with mlem are shown in fig. 3. the patient is
scanned with a ge discovery mi 5-ring pet/ct system. the real image has very
different cortex structure and some deflection compared with the training data.
it can be observed that dulda achieves excellent reconstruction results in both
details and edges across different dose level and different slices.table 2 shows
the ablation study on phase numbers and loss function for dulda. it can be
observed that the dual domain loss helps improve the performance and when the
phase number is 4, dulda achieves the best performance.
in this work, we proposed a dual-domain unsupervised model-based deep learning
method (dulda) for pet image reconstruction by unrolling the learned descent
algorithm. both quantitative and visual results show the superior performance of
dulda when compared to mlem, em-tv and dip based method. future work will focus
more on clinical aspects.
following the "as low as reasonably achievable" (alara) principle [22], lowdose
computer tomography (ldct) has been widely used in various medical applications,
for example, clinical diagnosis [18] and cancer screening [28]. to balance the
high image quality and low radiation damage compared to normaldose ct (ndct),
numerous algorithms have been proposed for ldct superresolution [3,4].in the
past decades, image post-processing techniques attracted much attention from
researchers because they did not rely on the vendor-specific parameters [2] like
iterative reconstruction algorithms [1,23] and could be easily applied to
current ct workflows [29]. image post-processing super-resolution (sr) methods
could be divided into 3 categories: interpolated-based methods [16,25],
modelbased methods [13,14,24,26] and learning-based methods [7][8][9]17].
interpolatedbased methods could recover clear results in those flattened regions
but failed to reconstruct detailed textures because they equally recover
information with different frequencies. and model-based methods often involved
time-consuming optimization processes and degraded quickly when image statistics
were biased from the image prior [6].with the development of deep learning (dl),
various learning-based methods have been proposed, such as edsr [20], rcan [31],
and swinir [19]. those methods optimized their trainable parameters by
pre-degraded low-resolution (lr) and high-resolution (hr) pairs to build a
robust model with generalization and finally reconstruct sr images. however,
they were designed for known degradation (for example bicubic degradation) and
failed to deal with more complex and unknown degradation processes (such as ldct
degradation). facing more complex degradation processes, blind sr methods have
attracted attention. huang et al. [11] introduced a deep alternating network
(dan) which estimated the degradation kernels and corrected those kernels
iteratively and reconstructed results following the inverse process of the
estimated degradation. more recently, aiming at improving the quality of medical
images further, huang et al. [12] first composited degradation model proposed
for radiographs and proposed attention denoising super-resolution generative
adversarial network (aid-srgan) which could denoise and super-resolve
radiographs simultaneously. to accurately reconstruct hr ct images from lr ct
images, hou et al. [10] proposed a dual-channel joint learning framework which
could process the denoising reconstruction and sr reconstruction in parallel.the
aforementioned methods still have drawbacks: (1) they treated the regions of
interest (roi) and regions of uninterest equally, resulting in the extra cost in
computing source and inefficient use for hierarchical features. (2) most of them
extracted the features with a fixed resolution, failing to effectively leverage
multi-scale features which are essential to image restoration task [27,32].(3)
they connected the sr task and the ldct denoising task stiffly, leading to
smooth texture, residual artifacts and unclear edges.to deal with those issues,
as shown in fig. 1(a), we propose an ldct image sr network with dual-guidance
feature distillation and dual-path content com-fig. 1. architecture of our
proposed method. sam is sampling attention module. cam is channel attention
module. avg ct is the average image among adjacent ct slices of each patient.
munication. our contributions are as follows: (1) we design a dual-guidance
fusion module (dgfm) which could fuse the 3d ct information and roi guidance by
mutual attention to make full use of ct features and reconstruct clearer
textures and sharper edges. (2) we propose a sampling attention block (sab)
which consists of sampling attention module (sam), channel attention module
(cam) and elaborate multi-depth residual connection aiming at the essential
multi-scale features by up-sampling and down-sampling to leverage the features
in ct images. (3) we design a multi-supervised mechanism based on shared task
heads, which introducing the denoising head into sr task to concentrate on the
connection between the sr task and the denoising task. such design could
suppress more artifacts while decreasing the number of parameters.
the pipeline of our proposed method is shown in fig. 1(a). we first calculate
the average ct image of adjacent ct slices of each patient to provide the 3d
spatial structure information of ct volume. meanwhile, the roi mask is obtained
by a pre-trained segmentation network to guide the network to concentrate on the
focus area or tissue area. then those guidance images and the input ldct image
are fed to the dual-guidance feature distillation backbone to extract the deep
features. finally, the proposed dual-path architecture consisting of
parametershared sr heads and denoising heads leverages the deep visual features
obtained by our backbone to build the connection between the sr task and the
denoising task, resulting in noise-free and detail-clear reconstructed
results.dual-guidance feature distillation backbone. to decrease the redundant
computation and make full use of the above-mentioned extra information, we
design a dual-guidance feature distillation backbone consisting of a
dual-guidance fusion module (dgfm) and sampling attention block(sab).firstly, we
use a 3 × 3 convolutional layer to extract the shallow features of the three
input images. then, those features are fed into 10 dgfm-sab blocks to obtain the
deep visual features.especially, the dgfm-sab block is composed of dgfm
concatenated with sab. considering the indicative function of roi, we calculate
the correlation matrix between ldct and its mask and then acquire the response
matrix between the correlation matrix and the average ct image by multi-heads
attention mechanism:where, f sab i are the output of i-th sab. f mask and f av g
represent the shallow features of the input roi mask and the average ct image
respectively. meanwhile, p rj(•) is the projection function, sof tmax[•] means
the softmax function and f i are the output features of the i-th dgfm. the dgfm
helps the backbone to focus on the roi and tiny structural information by
continuously introducing additional guidance information.furthermore, to take
advantage of the multi-scale information which is essential for obtaining the
response matrix containing the connections between different levels of features,
as shown in fig. 1(b), we design the sampling attention block (sab) which
introduces the resampling features into middle connection to fuse the
multi-scale information. in the sab, the input features are up-sampled and
down-sampled simultaneously and then down-sampled and up-sampled to recover the
spatial resolution, which can effectively extract multi-scale features. in
addition, as shown in fig. 1(c), we introduce the channel attention module (cam)
to focus on those channels with high response values, leading to detailed
features with high differentiation to different regions. shared heads mechanism.
singly using the sr head that consists of pixel shuffle layer and convolution
layer fails to suppress the residual artifacts because of its poor noise removal
ability. to deal with this problem, we develop a dualpath architecture by
introducing the shared denoising head into sr task where the parameters of sr
heads and denoising heads in different paths are shared respectively. two paths
are designed to process the deep features extracted from our backbone: (1) the
sr path transfers the deep features to those with highfrequency information and
reconstructs the sr result, and (2) the denoising path migrates the deep
features to those without noise and recovers the clean result secondly.
especially, the parameters of those two paths are shared and optimized by
multiple supervised strategy simultaneously. this process could be formulated
as:where,
following the multiple supervision strategy, the target function l total is
calculated as:where, i gt is the ground truth, bi(•) means bicubic
interpolation, • 1 represents the l1 norm and λ 1 , λ 2 are the weight
parameters for adjusting the losses.
datasets. two widely-used public ct image datasets, 3d-ircadb [5] and pancreas
[5] we augment the data by rotation and flipping first and then randomly crop
them to 128 × 128 patches. adam optimizer with β 1 = 0.9 and β 2 = 0.99 is used
to minimize the target function. λ 1 and λ 2 of our target function are set as
0.2. the batch size is set to 16 and the learning rate is set to 10 -4 which
decreases to 5×10 -5 at 200k iterations. peak signal-to-noise (psnr) and
structural similarity (ssim) are used as the quantitative indexes to evaluate
the performance.
table 1a shows the experimental result of the dual-guidance ablation
study.introducing the average ct image guidance alone degrades performance
compared with the model without guidance for both the scale factor of 2 and 4.
and introducing mask guidance alone could improve the reconstruction effect.
when the average ct image guidance and the mask guidance are both embedded, the
performance will be promoted further. table 1b presents the result of the shared
heads mechanism ablation study. the experimental result proves that introducing
the proposed dual-path architecture could promote the reconstruction performance
and the model with shared heads is superior than that without them in both
reconstruction ability and parameter amount.
we compare the performance of our proposed method with other state-of-theart
methods, including bicubic interpolation [16], dan [11], realsr [15], spsr [21],
aid-srgan [12] and jdnsr [10].figure 2 shows the qualitative comparison results
on the 3d-ircadb dataset with the scale factor of 2. all methods enhance the
image quality to different extents compared with bicubic interpolation. however,
for the calcifications within the liver which are indicated by the blue arrows,
our method recovers the clearest edges. the results of dan, spsr and aid-srgan
suffers from the artifacts. jdnsr blurs the issue structural information, e.g.
the edges of liver and bone. for the inferior vena cava, portal vein, and
gallbladder within the kidney, realsr restores blurred details and textures
though it could recover clear edges of calcifications. figure 3 shows the
qualitative comparison results on the pancreas dataset with the scale factor of
4. figure 3 has similar observation as fig. 2, that is, our method could
suppress more artifacts than other methods, especially at the edges of the
pancreas and the texture and structure of the issues with in the kidney.
therefore, our method reconstructs more detailed results than other
methods.table 2 shows the quantitative comparison results of different
state-of-theart methods with two scale factors on two datasets. for the
3d-ircadb and pancreas datasets, our method outperforms the second-best methods
1.6896/0.0157 and 1.7325/0.0187 on psnr/ssim with the scale factor of 2
respectively. similarly, our method outperforms the second-best methods
in this paper, we propose an ldct image sr network with dual-guidance feature
distillation and dual-path content communication. facing the existing problem
that reconstructed results suffer from residual artifacts, we design a
dualguidance feature distillation backbone which consists of dgfm and sab to
extract deep visual information. especially, the dgfm could fuse the average ct
image to take the advantage of the 3d spatial information of ct volume and the
segmentation mask to focus on the roi, which provides pixel-wise shallow
information and deep semantic features for our backbone. the sab leverages the
essential multi-scale features to enhance the ability for feature extraction.
then, our shared heads mechanism reconstructs the deep features obtained by our
backbone to satisfactory results. the experiments compared with 6
state-ofthe-art methods on 2 public datasets demonstrate the superiority of our
method.
experiment setup. all experiments are implemented on ubuntu 16.04.12 with an
nvidia rtx 3090 24g gpu using pytorch 1.8.0 and cuda 11.1.74.
image registration is a fundamental requirement for medical image analysis and
has been an active research focus for decades [1]. it aims to find a spatial
transformation between a pair of fixed and moving images, through which the
moving image can be warped to spatially align with the fixed image. similar to
natural image registration [2], medical image registration usually requires
affine registration to eliminate rigid misalignments and then performs
additional deformable registration to address non-rigid deformations.
traditional methods usually formulate medical image registration as a
time-consuming iterative optimization problem [3,4]. recently, deep registration
methods based on deep learning have been widely adopted to perform end-to-end
registration [5,6]. deep registration methods learn a mapping from image pairs
to spatial transformations based on training data in an unsupervised manner,
which have shown advantages in registration accuracy and computational
efficiency [7][8][9][10][11][12][13][14][15][16][17][18].many deep registration
methods perform coarse-to-fine registration to improve registration accuracy,
where the registration is decoupled into multiple coarse-to-fine registration
steps that are iteratively performed by using multiple cascaded networks
[10][11][12][13] or repeatedly running a single network for multiple iterations
[14,15]. mok et al. [13] proposed a laplacian pyramid image registration network
(lapirn), where multiple networks at different pyramid levels were cascaded. shu
et al. [14] proposed to use a single network (ulae-net) to perform
coarse-to-fine registration with multiple iterations. these methods perform
iterative coarse-to-fine registration and extract image features repeatedly in
each iteration, which inevitably increases computational loads and prolongs the
registration runtime. recently, non-iterative coarse-to-fine (nice) registration
methods have been proposed to perform coarse-to-fine registration with a single
network in a single iteration [16][17][18]. for example, we previously proposed
a nice registration network (nice-net) [18,19], where multiple coarse-to-fine
registration steps are performed with a single network in a single iteration.
these nice registration methods show advantages in both registration accuracy
and runtime on the benchmark task of intra-patient brain mri registration.
nevertheless, we identified that existing nice registration methods still have
two main limitations.firstly, existing nice registration methods merely focus on
deformable coarseto-fine registration, while affine registration, a common
prerequisite, is still reliant on traditional registration methods [16,18] or
extra affine registration networks [17]. using traditional registration methods
incurs time-consuming iterative optimization, while cascading extra networks
consumes additional computational resources (e.g., extra gpu memory and
runtime). secondly, existing nice registration methods are based on convolution
neural networks (cnn) and thus are limited by the intrinsic locality (i.e.,
limited receptive field) of convolution operations. transformers have been
widely adopted in many medical applications for their capabilities to capture
long-range dependency [20]. recently, transformers have also been shown to
improve registration with conventional voxelmorph [7]-like architecture
[21][22][23]. however, the benefits of using transformers for nice registration
have not been explored.in this study, we propose a non-iterative coarse-to-fine
transformer network (nice-trans) for joint affine and deformable registration.
our technical contributions are two folds: (i) we extend the existing nice
registration framework to affine registration, where multiple steps of both
affine and deformable coarse-to-fine registration are performed with a single
network in a single iteration. (ii) we explore the benefits of transformers for
nice registration, where swin transformer [24] is embedded into the nice-trans
to model long-range relevance between fixed and moving images. this is the first
deep registration method that integrates previously separated affine and
deformable coarse-to-fine registration into a single network, and this is also
the first deep registration method that exploits transformers for nice
registration. extensive experiments with seven public datasets show that our
nice-trans outperforms state-of-the-art registration methods on both
registration accuracy and runtime.
image registration aims to find a spatial transformation φ that warps a moving
image i m to a fixed image i f , so that the warped image i m•φ = i m • φ is
spatially aligned with the i f . in this study, we assume the i m and i f are
two single-channel, grayscale volumes defined in a 3d spatial domain ⊂ r 3 ,
which is consistent with common medical image registration studies
[7][8][9][10][11][12][13][14][15][16][17][18]. the φ is parameterized as a
displacement field, and we parametrized the image registration problem as a
function r θ (i f , i m ) = φ using nice-trans. as shown in fig. 1, our
nice-trans consists of an intra-image feature learning encoder and an
inter-image relevance modeling decoder (refer to sect. 2.1). multiple steps of
affine and deformable registration are performed within a single network
iteration (refer to sect. 2.2). the θ is a set of learnable parameters that are
optimized through unsupervised learning (refer to sect. 2.3).
the architecture of the proposed nice-trans is presented in fig. 1, which
consists of a dual-path encoder to learn image features from i m and i f
separately and a single-path decoder to model the spatial relevance between i m
and i f . skip connections are used at multiple scales to propagate features
from the encoder to the decoder. here, we assume the nice-trans performs l a and
l d steps of affine and deformable registration, resulting in a total of l = l a
+ l d steps of coarse-to-fine registration.the encoder has two identical,
weight-shared paths p m and p f that take i m and i f as input, respectively.
each path consists of l successive conv modules with 2 × 2 × 2 max pooling
applied between two adjacent modules, which produces two l-level feature
pyramidswhere the f f i and f m i are the output of the i th conv module in the
p f and p m . each conv module consists of two 3 × 3 × 3 convolutional layers
followed by leakyrelu activation with parameter 0.2. this dual-path design can
learn uncoupled image features of i m and i f , which enables the nice-trans to
reuse the learned features at multiple registration steps, thereby discarding
the requirement for repeated feature learning.the decoder consists of l-1
swintrans modules and a conv module, with a patch expanding layer [23] applied
between two adjacent modules to double the feature resolution and halve the
feature dimension. each swintrans module consists of one 1 × 1 × 1 convolutional
layer for feature dimension reduction and four successive swin transformer
blocks [24] including layer normalization, window/shifted window-based
multi-head self-attention (w/sw-msa), multilayer perceptron (mlp), and residual
connections. the output of each decoder module is fed into an affine or
deformable registration head that maps the input features into a displacement
field, which produces l displacement fields φ i ∈ {φ 1 , φ 2 , . . . , φ l } for
l steps of coarse-to-fine registration (detailed in sect. 2.2). the output of
each patch expanding layer is concatenated with, which is then fed into its
later decoder module. the decoder performs finer registration after each decoder
module, where the φ l is the final output φ. detailed architecture settings
(e.g., feature dimensions, head numbers of self-attention) are presented in the
supplementary materials.our nice-trans differs from the existing nice-net [18]
mainly in two aspects: (i) our nice-trans integrates affine and deformable
registration into a unified network, and (ii) our nice-trans leverages swin
transformer to model long-range spatial relevance between i m and i f . in
addition, the existing nice-net extracts features from the intermediately warped
image at each registration step, while our nice-trans directly warps the f m to
avoid this process and achieves similar performance.
the output features of the first l a decoder modules are fed into l a affine
registration heads, where the features are mapped to a 3 × 4 affine matrix
through global average pooling and two fully-connected layers, which are then
sampled as a dense displacement field. after the first l a steps of affine
registration, the output features of the last l d decoder modules are fed into l
d deformable registration heads, where the features are directly mapped to a
dense displacement field via a 3 × 3 × 3 convolutional layer.at the beginning of
coarse-to-fine registration, the φ 1 is the output of the first registration
head. then, the φ 1 is upsampled (×2) and voxel-wisely added to the output of
the second registration head to derive φ 2 . this process is repeated until the
φ l is derived, which realizes joint affine and deformable coarse-to-fine
registration. in our experiments, we set l a and l d as 1 and 4 (illustrated in
fig. 1) as this setting achieved the best validation results (refer to the
supplementary materials). figure 2 exemplifies a registration result of the
nice-trans with five steps of coarse-to-fine registration.
the learnable parameters θ are optimized using an unsupervised loss l that does
not require labels. the l is defined as l = l sim + σ l reg , where the l sim is
an image similarity term that penalizes the differences between the warped image
i m•φ and the fixed image i f , the l reg is a regularization term that
encourages smooth and invertible transformations φ, and the σ is a
regularization parameter.we adopt negative local normalized cross-correlation
(ncc) as the l sim , which is a widely used similarity metric in image
registration methods [7][8][9][10][12][13][14][15][16][17][18]. for the l reg ,
we impose a diffusion regularizer on the φ to encourage its smoothness and also
adopt a jacobian determinant (jd) loss [25] to enhance its invertibility. as the
φ is not invertible at voxel p where the jacobian determinant is negative (|j
φ(p)| ≤ 0) [26], the jd loss explicitly penalizes the negative jacobian
determinants of φ. finally, the l reg is defined as , where the λ is a
regularization parameter balancing registration accuracy and transformation
invertibility.
we evaluated the proposed nice-trans on the task of inter-patient brain mri
registration, which is a common benchmark task in medical image registration
studies [7][8][9][12][13][14][15][16][17][18]. we followed the dataset settings
in [18]: 2,656 brain mri images acquired from four public datasets (adni [27],
abide [28], adhd [29], and ixi [30]) were used for training; two public brain
mri datasets with anatomical segmentation (mindboggle [31] and buckner [32])
were used for validation and testing. the mindboggle dataset contains 100 mri
images and were randomly split into 50/50 images for validation/testing. the
buckner dataset contains 40 mri images and were used for testing only. in
addition to the original settings of [18], we adopted an additional public brain
mri dataset (lpba [33]) for testing, which contains 40 mri images.we performed
brain extraction and intensity normalization for each mri image with freesurfer
[32]. each image was placed at the same position via center of mass (com)
initialization [34], and then was cropped into 144 × 192 × 160 voxels.
we implemented our nice-trans using pytorch on a nvidia titan v gpu with 12 gb
memory. we used an adam optimizer with a learning rate of 0.0001 and a batch
size of 1 to train the nice-trans for 100,000 iterations. at each iteration, two
images were randomly picked from the training data as the fixed and moving
images. a total of 100 image pairs, randomly picked from the validation data,
were used to monitor the training process and to optimize hyper-parameters. we
set σ as 1 to ensure that the l sim and σ l reg have close values, while the λ
was set as 10 -4 to ensure that the percentage of voxels with negative jacobian
determinants is less than 0.05% (refer to the supplementary materials for
detailed regularization analysis). our code will be available in
https://github.com/ mungomeng/registration-nice-trans.
our nice-trans was compared with nine image registration methods, including two
traditional methods and seven deep registration methods. the compared
traditional methods are syn [3] and niftyreg [4]. for these methods, we used
cross-correlation as the similarity measure and adopted flirt [35] for affine
registration. the compared deep registration methods are voxelmorph (vm) [7],
diffeomorphic voxelmorph (difvm) [8], transmorph [21], swin-voxelmorph (swin-vm)
[22], lapirn [13], ulae-net [14], and nice-net [18]. the vm and difvm are two
commonly benchmarked registration methods in the literature
[12][13][14][15][16][17][18][21][22][23]. the transmorph and swin-vm are two
state-of-the-art methods that embed swin transformer into vm-like architecture.
the lapirn, ulae-net, and nice-net are three state-of-the-art coarse-to-fine
registration methods. for the compared deep registration methods, we adopted ncc
as the similarity loss and followed [17,36] to cascade a cnn-based registration
network (affinenet) for affine registration.
we compared the nice-net to the nine comparison methods for subject-to-subject
registration. for testing, we randomly picked 100 image pairs from each of the
mindboggle, buckner, and lpba testing sets. we used standard evaluation metrics
for medical image registration [7][8][9][10][11][12][13][14][15][16][17][18].
the registration accuracy was evaluated using the dice similarity coefficients
(dsc) of segmentation labels, while the smoothness and invertibility of spatial
transformations were evaluated using the percentage of negative jacobian
determinants (njd). generally, a higher dsc and a lower njd indicate better
registration performance. a two-sided p value less than 0.05 is considered to
indicate a statistically significant difference between two dscs.we also
performed an ablation study to explore the benefits of transformers. we built a
baseline method that has the same architecture as the nice-trans but only uses
conv modules. after that, we embedded swin transformer into the baseline method,
where swintrans modules replaced the conv modules in the encoder
(trans-encoder), decoder (trans-decoder), or both (trans-all).
table 1 presents the registration performance of our nice-trans and all
comparison methods. the registration accuracy of all methods degraded by 1-3% in
dsc when affine registration was not performed, which demonstrates the
importance of affine registration. however, using flirt or affinenet for affine
registration incurred extra computational loads and increased the registration
runtime. our nice-trans performed joint affine and deformable registration,
which enabled it to realize affine registration with negligible additional
runtime. moreover, we suggest that integrating affine and deformable
registration into a single network also brings convenience for network training.
training two separate affine and deformable registration networks will prolong
the whole training time, while joint training will consume more gpu memory. as
for registration accuracy, the transmorph and swin-vm achieved higher dscs than
the conventional vm and difvm, but still cannot outperform the existing
cnn-based coarse-to-fine registration methods (lapirn, ulae-net, and nice-net).
our nice-trans leverages swin transformer to perform coarse-to-fine
registration, which enabled it to achieve the highest dscs among all methods.
this means that our nice-trans also has advantages on registration accuracy. we
present a qualitative comparison in the supplementary materials, which shows
that the registration result produced by our nice-trans is more consistent with
the fixed image. in addition, there usually exists a trade-off between dsc and
njd as imposing constraints on the spatial transformations limits their
flexibility, which results in degraded registration accuracy [13,18]. for
example, compared with vm, the difvm with diffeomorphic constraints achieved
better njds and worse dscs. nevertheless, our nice-trans achieved both the best
dscs and njds. we suggest that, if we set λ as 0 to maximize the registration
accuracy with the cost of transformation invertibility, our nice-trans can
achieve higher dscs and outperform the comparison methods by a larger margin
(refer to the regularization analysis in the supplementary materials).table 2
shows the results of our ablation study. swin transformer improved the
registration performance when embedded into the decoder, but had limited
benefits in the encoder. this suggests that swin transformer can benefit
registration in modeling inter-image spatial relevance while having limited
benefits in learning intra-image representations. this finding is intuitive as
image registration aims to find spatial relevance between images, instead of
finding the internal relevance within an image. under this aim, embedding
transformers in the decoder helps to capture long-range relevance between images
and improves registration performance. we noticed that previous studies gained
improvements by embedding swin transformer in the encoder [21] or leveraging a
full transformer network [22]. this is attributed to the fact that they used a
vm-like architecture that entangles image representation learning and spatial
relevance modeling throughout the whole network. our nice-trans decouples these
two parts and provides further insight on using transformers for registration:
leveraging transformers to learn intra-image relevance might not be beneficial
but merely incurs extra computational loads. it should be acknowledged that
there are a few limitations in our study. first, the experiment (table 1)
demonstrated that our nice-trans can well address the inherent misalignments
among inter-patient brain mri images, but the sensitivity of affine registration
to different degrees of misalignments is still awaiting further exploration.
second, in this study, we evaluated the nice-trans on the benchmark task of
inter-patient brain mri registration, while we believe that our nice-trans also
could apply to other image registration applications (e.g., brain tumor
registration [37]).
we have outlined a non-iterative coarse-to-fine transformer network (nice-trans)
for medical image registration. unlike the existing image registration methods,
our nice-trans performs joint affine and deformable coarse-to-fine registration
with a single network in a single iteration. the experimental results show that
our nice-trans can outperform the state-of-the-art coarse-to-fine or
transformer-based deep registration methods on both registration accuracy and
runtime. our study also suggests that transformers benefit registration in
modeling inter-image spatial relevance while having limited benefits in learning
intra-image representations.
bold: the best dsc and njd in each testing dataset and the shortest runtime of
completing both affine and deformable registration. *: <0.05, in comparison to
nice-trans (ours).
bold: the best dsc and njd in each testing dataset.
liver cancer is the most prevalent indication for liver surgery, and although
there have been notable advancements in oncologic therapies, surgery remains as
the only curative approach overall [20].liver laparoscopic resection has
demonstrated fewer complications compared to open surgery [21], however, its
adoption has been hindered by several reasons, such as the risk of unintentional
vessel damage, as well as oncologic concerns such as tumor detection and margin
assessment. hence, the identification of intrahepatic landmarks, such as
vessels, and target lesions is crucial for successful and safe surgery, and
intraoperative ultrasound (ious) is the preferred technique to accomplish this
task. despite the increasing use of ious in surgery, its integration into
laparoscopic workflows (i.e., laparoscopic intraoperative ultrasound) remains
challenging due to combined problems.performing ious during laparoscopic liver
surgery poses significant challenges, as laparoscopy has poor ergonomics and
narrow fields of view, and on the other hand, ious demands skills to manipulate
the probe and analyze images. at the end, and despite its real-time
capabilities, ious images are intermittent and asynchronous to the surgery,
requiring multiple iterations and repetitive steps (probe-in -→ instruments-out
-→ probe-out -→ instruments-in). therefore, any method enabling a continuous and
synchronous us assessment throughout the surgery, with minimal iterations
required would significantly improve the surgical workflow, as well as its
efficiency and safety.to overcome these limitations, the use of intravascular
ultrasound (ivus) images has been proposed, enabling continuous and synchronous
inside-out imaging during liver surgery [19]. with an intravascular approach, an
overall view and full-thickness view of the liver can quickly and easily be
obtained through mostly rotational movements of the catheter, while this is
constrained to the lumen of the inferior vena cava, and with no interaction with
the tissue (contactless, a.k.a. standoff technique) as illustrated in fig. 1.
however, to benefit from such a technology in a computer-guided solution, the
different us images would need to be tracked and possibly integrated into a
volume for further processing. external us probes are often equipped with an
electromagnetic tracking system to track its position and orientation in
realtime. this information is then used to register the 3d ultrasound image with
the patient's anatomy. the use of such an electromagnetic tracking system in
laparoscopic surgery is more limited due to size reduction. the tracking system
may add additional complexity and cost to the surgical setup, and the tracking
accuracy may be affected by metallic devices in the surgical field [22].several
approaches have been proposed to address this limitation by proposing a
trackerless ultrasound volume reconstruction. physics-based methods have
exploited speckle correlation models between different adjacent frames [6][7][8]
to estimate their relative position. with the recent advances in deep learning,
recent works have proposed to learn a higher order nonlinear mapping between
adjacent frames and their relative spatial transformation. prevost et al. [9]
first demonstrated the effectiveness of a convolution neural network to learn
the relative motion between a pair of us images. xie et al. [10] proposed a
pyramid warping layer that exploits the optical flow features in addition to the
ultrasound features in order to reconstruct the volume. to enable a smooth 3d
reconstruction, a case-wise correlation loss based on 3d cnn and pearson
correlation coefficient was proposed in [10,12]. qi et al. [13] leverages past
and future frames to estimate the relative transformation between each pair of
the sequence; they used the consistency loss proposed in [14]. despite the
success of these approaches, they still suffer significant cumulative drift
errors and mainly focus on linear probe motions. recent work [15,16] proposed to
exploit the acceleration and orientation of an inertial measurement unit (imu)
to improve the reconstruction performance and reduce the drift error. motivated
by the weakness of the state-of-the-art methods when it comes to large
non-linear probe motions, and the difficulty of integrating imu sensors in the
case of minimally invasive procedures, we introduce a new method for pose
estimation and volume reconstruction in the context of minimally invasive
trackerless ultrasound imaging. we use a siamese architecture based on a
sequence to vector(seq2vec) neural network that leverages image and optical flow
features to learn relative transformation between a pair of images.our method
improves upon previous solutions in terms of robustness and accuracy,
particularly in the presence of rotational motion. such motion is predominant in
the context highlighted above and is the source of additional nonlinearity in
the pose estimation problem. to the best of our knowledge, this is the first
work that provides a clinically sound and efficient 3d us volume reconstruction
during minimally invasive procedures. the paper is organized as follows: sect. 2
details the method and its novelty, sect. 3 presents our current results on ex
vivo porcine data, and finally, we conclude in sect. 4 and discuss future work.
in this work, we make the assumption that the organ of interest does not undergo
deformation during the volume acquisition. this assumption is realistic due to
the small size of the probe. let i 0 , i 1 ...i n -1 be a sequence of n frames.
our aim is to find the relative spatial transformation between each pair of
frames i i and i j with 0 ≤ i ≤ j ≤ n -1. this transformation is denoted t (i,j)
and is a six degrees of freedom vector representing three translations and three
euler angles. to achieve this goal, we propose a siamese architecture that
leverages the optical flow in the sequences in addition to the frames of
interest in order to provide a mapping with the relative frames spatial
transformation. the overview of our method is presented in fig. 2.we consider a
window of 2k + 3 frames from the complete sequence of length n , where 0 ≤ k ≤ n
-3 2 is a hyper-parameter that denotes the number of frames between two frames
of interest. our method predicts two relative transformations between the pairs
of frames (i 1 , i k+2 ) and (i k+2 , i 2k+3 ). the input window is divided into
two equal sequences of length k + 2 sharing a common frame. both deduced
sequences are used to compute a sparse optical flow allowing to track the
trajectory of m points. then, gaussian heatmaps are used to describe the motion
of the m points in an image-like format(see sect. 2.2). finaly, a siamese
architecture based on two shared weights sequence to vector (seq2vec) network
takes as input the gaussian heatmaps in addition to the first and last frames
and predicts the relative transformations. in the following we detail our
pipeline.
given a sequence of frames i i and i i+k+1 , we aim at finding the trajectory of
a set of points throughout the sequence. we choose the m most prominent points
from the first frame using the feature selection algorithm proposed in [3].
points are then tracked throughout each pair of adjacent frames in the sequence
by solving eq. 1 which is known as the optical flow equation. we use the
pyramidal implementation of lucas-kanade method proposed in [4] to solve the
equation. thus, yielding a trajectory matrix a ∈ r m ×(k+2)×2 that contains the
position of each point throughout the sequence. figure 3 illustrates an example
where we track two points in a sequence of frames.
after obtaining the trajectory of m points in the sequencewe only keep the first
and last position of each point, which corresponds to the positions in our
frames of interest. we use gaussian heatmaps h ∈ r h×w with the same dimension
as the ultrasound frames to encode these points, they are more suitable as input
for the convolutional networks. for a point with a position (x 0 , y 0 ), the
corresponding heatmap is defined in the eq. 2.thus, each of our m points are
converted to a pair of heatmaps that represent the position in the first and
last frames of the ultrasound sequence. these pairs concatenated with the
ultrasound first and last frames form the recurrent neural network sequential
input of size (m + 1, h, w, 2), where m + 1 is the number of channels (m
heatmaps and one ultrasound frame), h and w are the height and width of the
frames and finally 2 represents the temporal dimension.
the siamese architecture is based on a sequence to vector network. our network
maps a sequence of two images having m + 1 channel each to a six degrees of
freedrom vector (three translations and three rotation angles). the architecture
of seq2vec is illustrated in the fig. 4. it contains five times the same block
composed of two convolutional lstms (convlstm) [5] followed by a batch
normalisation. their output is then flattened and mapped to a six degrees of
freedom vector through linear layers; relu is the chosen activation function for
the first linear layer. we use an architecture similar to the one proposed in
[5] for the convlstm layers. seq2vec networks share the same weights.
in the training phase, given a sequence of 2k + 3 frames in addition to their
ground truth transformations t(1,k+2) , t(k+2,2k+3) and t(1,2k+3) , the
seq2vec's weights are optimized by minimising the loss function given in the eq.
3. the loss contains two terms. the first represents the mean square error (mse)
between the estimated transformations (t (1,k+2) , t (k+2,2k+3) ) at each corner
point of the frames and their respective ground truth. the second term
represents the accumulation loss that aims at reducing the error of the volume
reconstruction, the effectiveness of the accumulation loss have been proven in
the literature [13]. it is written as the mse between the estimated t (1,2k+3) =
t (k+2,2k+3) ×t (1,k+2) at the corner points of the frames and the ground truth
t (1,2k+3) .3 results and discussion
to validate our method, six tracked sequences were acquired from an ex vivo
swine liver. a manually manipulated ivus catheter was used (8 fr lateral firing
acunav tm 4-10 mhz) connected to an ultrasound system (acuson s3000 helx touch,
siemens healthineers, germany), both commercially available. an electromagnetic
tracking system (trakstar tm , ndi, canada) was used along with a 6 dof sensor
(model 130) embedded close to the tip of the catheter, and the plus toolkit [17]
along with 3d slicer [18] were used to record the sequences. the frame size was
initially 480 × 640. frames were cropped to remove the patient and probe
characteristics, then down-sampled to a size of 128 × 128 with an image spacing
of 0.22 mm per pixel. first and end stages of the sequences were removed from
the six acquired sequences, as they were considered to be largely stationary,
and aiming to avoid training bias. clips were created by sliding a window of 7
frames (corresponding to a value of k = 2) with a stride of 1 over each
continuous sequence, yielding a data set that contains a total of 13734 clips.
the tracking was provided for each frame as a 4×4 transformation matrix.we have
converted each to a vector of six degrees of freedom that corresponds to three
translations in mm and three euler angles in degrees. for each clip, relative
frame to frame transformations were computed for the frames number 0, 3 and 6.
the distribution of the relative transformation between the frames in our clips
is illustrated in the fig. 5. it is clear that our data mostly contains
rotations, in particular over the axis x. heatmaps were calculated for two
points (m = 2) and with a quality level of 0.1, a minimum distance of 7 and a
block size of 7 for the optical flow algorithm (see [4] for more details). the
number of heatmaps m and the frame jump k were experimentally chosen among 0, 2,
4, 6.the data was split into train, validation and test sets by a ratio of
7:1.5:1.5. our method is implemented in pytorch1 1.8.2, trained and evaluated on
a geforce rtx 3090. we use an adam optimizer with a learning rate of 10 -4 . the
training process converges in 40 epochs with a batch size of 16. the model with
the best performance on the validation data was selected and used for the
testing.
the test data was used to evaluate our method, it contains 2060 clips over which
our method achieved a translation error of translation of 0.449 ± 0.189 mm, and
an orientation error of orientation 1.3 ± 1.5 • . we have evaluated our
reconstruction with a commonly used in state-of-the-art metric called final
drift error, which measures the distance between the center point of the final
frame according to the real relative position and the estimated one in the
sequence. on this basis, each of the following metrics was reported over the
reconstructions of our method. final drift rate (fdr): the final drift divided
by the sequence length. average drift rate (adr): the average cumulative drift
of all frames divided by the length from the frame to the starting point of the
sequence. table 1 shows the evaluation of our method over these metrics compared
to the state-of-the-art methods monet [15] and cnn [9]. both state-of-the-art
methods use imu sensor data as additional input to estimate the relative
transformation between two relative frames. due to the difficulty of including
an imu sensor in our ivus catheter, the results of both methods were reported
from the monet paper where the models have been trained on arm scans, see [15]
for more details. as the table 1 shows, our method is comparable with
state-of-the-art methods in terms of drift errors without using any imu and with
non-linear probe motion as one may notice in our data distribution in the fig.
5. figure 6 shows the volume reconstruction of two sequences of different sizes
with our method in red against the ground truth slices. despite the
non-linearity of the probe motion, the relative pose estimation results obtained
by our method remains very accurate. however, one may notice that the drift
error increases with respect to the sequence length. this remains a challenge
for the community even in the case of linear probe motions.
in this paper, we proposed the first method for trackerless ultrasound volume
reconstruction in the context of minimally invasive surgery. our method does not
use any additional sensor data and is based on a siamese architecture that
leverages the ultrasound image features and the optical flow to estimate
relative transformations. our method was evaluated on ex vivo porcine data and
achieved translation and orientation errors of 0.449±0.189 mm and 1.3±1.5 •
respectively with a fair drift error. in the future work, we will extend our
work to further improve the volume reconstruction and use it to register a
pre-operative ct image in order to provide guidance during
interventions.aknowledgments. this work was partially supported by french state
funds managed by the anr under reference anr-10-iahu-02 (ihu strasbourg).
magnetic resonance imaging (mri) is critical to the diagnosis, treatment, and
follow-up of brain tumour patients [26]. multiple mri modalities offer
complementary information for characterizing brain tumours and enhancing patient
l. jiang and y. mao-contribute equally in this work.management [4,27]. however,
acquiring multi-modality mri is time-consuming, expensive and sometimes
infeasible in specific modalities, e.g., due to the hazard of contrast agent
[15]. trans-modal mri synthesis can establish the mapping from the known domain
of available mri modalities to the target domain of missing modalities,
promising to generate missing mri modalities effectively. the synthetic methods
leveraging multi-modal mri, i.e., many-to-one translation, have outperformed
single-modality models generating a missing modality from another available
modality, i.e., one-to-one translation [23,33]. traditional multi-modal methods
[21,22], e.g., sparse encoding-based, patch-based and atlasbased methods, rely
on the alignment accuracy of source and target domains and are poorly scalable.
recent generative adversarial networks (gans) and variants, e.g., mm-gan [23],
diamondgan [13] and provogan [30], have been successful based on multi-modal
mri, further improved by introducing multi-modal coding [31], enhanced
architecture [7], and novel learning strategies [29].despite the success,
gan-based models are challenged by the limited capability of adversarial
learning in modelling complex multi-modal data distributions [25] recent studies
have demonstrated that gans' performance can be limited to processing and
generating data with less variability [1]. in addition, gans' hyperparameters
and regularization terms typically require fine-tuning, which otherwise often
results in gradient vanish and mode collapse [2].diffusion model (dm) has
achieved state-of-the-art performance in synthesizing natural images, promising
to improve mri synthesis models. it shows superiority in model training [16],
producing complex and diverse images [9,17], while reducing risk of modality
collapse [12].for instance, lyu et al. [14] used diffusion and score-marching
models to quantify model uncertainty from monte-carlo sampling and average the
output using different sampling methods for ct-to-mri generation; özbey et al.
[19] leveraged adversarial training to increase the step size of the inverse
diffusion process and further designed a cycle-consistent architecture for
unpaired mri translation.however, current dm-based methods focus on one-to-one
mri translation, promising to be improved by many-to-one methods, which requires
dedicated design to balance the multiple conditions introduced by multi-modal
mri. moreover, as most dms operate in original image domain, all markov states
are kept in memory [9], resulting in excessive burden. although latent diffusion
model (ldm) [20] is proposed to reduce memory consumption, it is less feasible
for many-to-one mri translation with multi-condition introduced. further,
diffusion denoising processes tend to change the original distribution structure
of the target image due to noise randomness [14], rending dms often ignore the
consistency of anatomical structures embedded in medical images, leading to
clinically less relevant results. lastly, dms are known for their slow speed of
diffusion sampling [9,11,17], challenging its wide clinical application.we
propose a dm-based multi-modal mri synthesis model, cola-diff, which facilitates
many-to-one mri translation in latent space, and preserve anatomical structure
with accelerated sampling. our main contributions include: -present a denoising
diffusion probabilistic model based on multi-modal mri.as far as we know, this
is the first dm-based many-to-one mri synthesis model. -design a bespoke
architecture, e.g., similar cooperative filtering, to better facilitate
diffusion operations in the latent space, reducing the risks of excessive
information compression and high-dimensional noise. -introduce structural
guidance of brain regions in each step of the diffusion process, preserving
anatomical structure and enhancing synthesis quality. -propose an auto-weight
adaptation to balance multi-conditions and maximise the chance of leveraging
relevant multi-modal information.
figure 1 illustrates the model design. as a latent diffusion model, cola-diff
integrates multi-condition b from available mri contrasts in a compact and
low-dimensional latent space to guide the generation of missing modality x ∈ r
h×w ×1 . precisely, b constitutes available contrasts and anatomical structure
masks generated from the available contrasts. similar to [9,20], cola-diff
invovles a forward and a reverse diffusion process. during forward diffusion, x
0 is encoded by e to produce κ 0 , then subjected to t diffusion steps to
gradually add noise and generate a sequence of intermediate representations: {κ
0 , . . . , κ t }.the t-th intermediate representation is denoted as κ t ,
expressed as:where ᾱt = t i=1 α i , α i denotes hyper-parameters related to
variance. the reverse diffusion is modelled by a latent space network with
parameters θ, inputting intermediate perturbed feature maps κ t and y
(compressed b) to predict noise level θ (κ t , t, y) for recovering feature maps
κt-1 from previous,to enable effective learning of the underlying distribution
of κ 0 , the noise level needs to be accurately estimated. to achieve this, the
network employs similar cooperative filtering and auto-weight adaptation
strategies. κ0 is recovered by repeating eq. 2 process for t times, and decoding
the final feature map to generate synthesis images x0 .
we map multi-condition to the latent space network for guiding noise prediction
at each step t. the mapping is implemented by n transformer blocks (fig. 1 (d)),
including global self-attentive layers, layer-normalization and position-wise
mlp. following the latent diffusion model (ldm) [20], the network θ (κ t , t, y)
is trained to predict the noise added at each step using(3)to mitigate the
excessive information losses that latent spaces are prone to, we replace the
simple convolution operation with a residual-based block (three sequential
convolutions with kernels 1 * 1, 3 * 3, 1 * 1 and residual joins [8]), and
enlarge the receptive field by fusion (5 * 5 and 7 * 7 convolutions followed by
aff [6]) in the down-sampling section. moreover, to reduce high-dimensional
noise generated in the latent space, which can significantly corrupt the quality
of multi-modal generation. we design a similar cooperative filtering detailed
below.similar cooperative filtering. the approach has been devised to filter the
downsampled features, with each filtered feature connected to its respective
upsampling component (shown in fig. 1 (f)). given f , which is the downsampled
feature of κ t , suppose the 2d discrete wavelet transform φ [24] decomposes the
features into low frequency component f (i)a and high frequency componentsa ,
where i is the number of wavelet transform layers. previous work [5] has shown
to effectively utilize global information by considering similar patches.
however, due to its excessive compression, it is less suitable for ldm. here, we
group the components and further filter by similar block matching δ [18] or
thresholding γ, use the inverse wavelet transform φ -1 (•) to reconstruct the
denoising results, given f * .
unlike natural images, medical images encompass rich anatomical information.
therefore, preserving anatomical structure is crucial for mri generation.
however, dms often corrupt anatomical structure, and this limitation could be
due to the learning and sampling processes of dms that highly rely on the
probability density function [9], while brain structures by nature are
overlapping in mri density distribution and even more complicated by
pathological changes. previous studies show that introducing geometric priors
can significantly improve the robustness of medical image generation. [3,28].
therefore, we hypothesize that incorporating structural prior could enhance the
generation quality with preserved anatomy. specifically, we exploit fsl-fast
[32] tool to segment four types of brain tissue: white matter, grey matter,
cerebrospinal fluid, and tumour. the generated tissue masks and inherent density
distributions (fig. 1 (e)) are then used as a condition y i to guide the reverse
diffusion.the combined loss function for our multi-conditioned latent diffusion
is defined aswhere kl is the kl divergence loss to measure similarity between
real q and predicted p θ distributions of encoded images.where d kl is the kl
divergence function.
it is critical to balance multiple conditions, maximizing relevant information
and minimising redundant information. for encoded conditions y ∈ r h×w×c , c is
the number of condition channels. set the value after auto-weight adaptation to
ỹ, the operation of this module is expressed as (shown in fig. 1 (e))the
embedding outputs are adjusted by embedding weight μ. the autoactivation is
governed by the learnable weight ν and bias o. where is a small constant added
to the equation to avoid the issue of derivation at the zero point. the
normalization method can establish stable competition between channels, g = {g c
} s c=1 . we use l 2 normalization for cross-channel operations:where s denotes
the scale. we use an activation mechanism for updating each channel to
facilitate the maximum utilization of each condition during diffusion model
training, and further enhance the synthesis performance. given the learnable
weightwhich gives new representations ỹc of each compressed conditions after the
automatic weighting. s(•) denotes the sigmoid activation function.
datasets and baselines. we evaluated cola-diff on two multi-contrast brain mri
datasets: brats 2018 and ixi datasets. the brats 2018 contains mri scans from
285 glioma patients. each includes four modalities: t1, t2, t1ce, and flair. we
split them into (190:40:55) for training/validation/testing. for each subject,
we automatically selected axial cross-sections based on the perceptible
effective area of the slices, and then cropped the selected slices to a size of
224 × 224. the ixi1 dataset consists of 200 multi-contrast mris from healthy
brains, plit them into (140:25:35) for training/validation/testing. for
preprocessing, we registered t2-and pd-weighted images to t1-weighted images
using fsl-flirt [10], and other preprocessing are identical to the brats 2018.
we compared cola-diff with four state-of-the-art multi-modal mri synthesis
methods: mm-gan [23], hi-net [33], provogan [30] and ldm [20]. implementation
details. our code is publicly available at https://github. com/seemeincrown/cola
diff multimodal mri synthesis. the hyperparameters of cola-diff are defined as
follows: diffusion steps to 1000; noise schedule to linear; attention
resolutions to 32, 16, 8; batch size to 8, learning rate to 9.6e -5.the noise
variances were in the range of β 1 = 10 -4 and β t = 0.02. an exponential moving
average (ema) over model parameters with a rate of 0.9999 was employed. the
model is trained on 2 nvidia rtx a5000, 24 gb with adam optimizer on pytorch. an
acceleration method [11] based on knowledge distillation was applied for fast
sampling. quantitative results. we performed synthesis experiments for all
modalities, with each modality selected as the target modality while remaining
modalities and the generated region masks as conditions. seven cases were tested
in two datasets (table 1). the results show that cola-diff outperforms other
models by up to 6.01 db on psnr and 5.74% on ssim. even when compared to the
best of other models in each task, cola-diff is a maximum of 0.81 db higher in
psnr and 0.82% higher in ssim. qualitative results. the first three and last
three rows in fig. 2 illustrate the synthesis results of t1ce from brats and pd
from the ixi, respectively. from the generated images, we observe that cola-diff
is most comparable to the ground truth, with fewer errors shown in the heat
maps. the synthesis uncertainty for each region is derived by performing 100
generations of the same slice and calculating the pixel-wise variance. from the
uncertainty maps, cola-diff is more confident in synthesizing the gray and white
matter over other comparison models. particularly, cola-diff performs better in
generating complex brain sulcus and tumour boundaries. further, cola-diff could
better maintain the anatomical structure over comparison models.
we verified the effectiveness of each component in cola-diff by removing them
individually. we experimented on brats t1+t1ce+flair→t2 task with four absence
scenarios (table 2 top). our results show that each component contributes to the
performance improvement, with auto-weight adaptation bringing a psnr increase of
1.9450db and ssim of 4.0808%.to test the generalizability of cola-diff under the
condition of varied inputs, we performed the task of generating t2 on two
datasets with progressively increasing input modalities (table 2 bottom). our
results show that our model performance increases with more input modalities:
ssim has a maximum uplift value of 1.9603, psnr rises from 26.6355 db to 28.3126
db in brats; from 32.164 db to 32.8721 db in ixi. the results could further
illustrate the ability of cola-diff to exploit multi-modal information.
this paper presents cola-diff, a dm-based multi-modal mri synthesis model with a
bespoke design of network backbone, similar cooperative filtering, structural
guidance and auto-weight adaptation. our experiments support that cola-diff
achieves state-of-the-art performance in multi-modal mri synthesis tasks.
therefore, cola-diff could serve as a useful tool for generating mri to reduce
the burden of mri scanning and benefit patients and healthcare providers.
end-to-end convolutional neural networks (cnns) have shown remarkable
performance compared to classical algorithms [14] on mri sr. deep cnns have been
widely applied in a variety of mri sr situations; for instance, slice imputation
on the brain, liver and prostate mri [29] and brain mri sr reconstruction on
scaling factors ×2, ×3, ×4 [32]. several techniques based on deep cnns have been
proposed to improve performance, such as densely connected networks [6],
adversarial networks [5], and attention network [32]. however, their supervised
training requires paired images, which necessitates re-training every time there
is a shift in the input distribution [4,16]. as a result, such methods are
unsuitable for mri sr, as it is challenging to obtain paired training data that
cover the variability in acquisition protocols and resolution of clinical brain
mri scans across institutions [14].building image priors through generative
models has recently become a popular approach in the field of image sr, for both
computer vision [1,2,7,17,19] as well as medical imaging [18,25], as they do not
require re-training in the presence of several types of input distribution
shifts. while these methods have shown promise in mri sr, they have so far been
limited to 2d slices [18,25], rendering them unsuitable for 3d brain mris slice
imputation.in this study, we propose solving the mri sr problem by building
powerful, 3d-native image priors through a recently proposed hr image generative
model, the latent diffusion model (ldm) [21,22]. we solve the inverse problem by
finding the optimal latent code z in the latent space of the pre-trained
generative model, which could restore a given lr mri i, using a known corruption
function f . in this study, we focus on slice imputation, yet our method could
be applied to other medical image sr problems by implementing different
corruption functions f . we proposed two novel strategies for mri sr:
inverse(ldm), which additionally inverts the input image through the
deterministic ddim model, and inversesr(decoder) which inverts the input image
through the corruption function f and through the decoder d of the ldm model. we
found that for large sparsity, inversesr(ldm) had a better performance, while
for low sparsity, inversesr(decoder) performed best. while the ldm model was
trained on uk biobank, we demonstrate our methods on an external dataset (ixi)
which was inaccessible to the pre-trained generative model. both quantitative
and qualitative results show that our method achieves significantly better
performance compared to two other baseline models. furthermore, our method can
also be applied to tumour/lesion filling by creating tumour/lesion shape masks.
mri super-resolution. end-to-end deep training [27,29,32] has been proposed
recently for mri sr, which has achieved superior results compared to classical
methods. however, these methods require paired data to train, which is hard to
acquire because of the large variability present in clinical mris [14,23]. to
circumvent this limitation, several unsupervised methods have been proposed
without requiring access to hr scans [3,8,14]. dalca et al. [8] proposed a
gaussian mixture model for sparse image patches. brudfors et al. [3] presented
an algorithm which could take advantage of multimodal mri. iglesias et al. [14]
introduced a method to train a cnn for mri sr on any given combination of
contrasts, resolutions and orientations.solving inverse problems using
generative models. a common way to solve the inverse problem using an ldm is to
use the encoder e to first encode the given image x into the latent space z 0 =
e(x) [10,12,20], followed by ddim (denoising diffusion implicit models)
inversion [9,24] to encode z 0 into the noise latent code z t [20]. however,
this approach does not work for low-resolution images, because the encoder e has
only been trained on high-resolution images.our work is also similar to the
optimization-based generative adversarial network (gan) inversion approach [30],
trying to find the optimal latent representation z * in the latent space of gan,
which could be mapped to represent the given image x ≈ g(z * ). more recent
works [7,10,13,17,25] have used diffusion models for inverse problems due to
their superior performance. however, all these methods require the diffusion
model to operate directly in the image space, which for large image resolutions
can become gpu-memory intensive.
3d brain latent diffusion models. we leverage a state-of-the-art ldm [21] to
create high-quality priors for 3d brain mris. there are two components in an
ldm: an autoencoder and a diffusion model [22]. an encoder e maps each
highresolution t1w brain mri x ∼ p data (x) into a latent vector z 0 = e(x) of
size 20 × 28 × 20. the decoder d is trained to map the latent vectors z 0 back
into the mri image domain x. the autoencoder was trained on 31,740 t1w mris from
the uk biobank [26] using a combination of an l1 loss, a perceptual loss [31], a
patch-based adversarial loss [11] and a kl regularization term in the latent
space. the autoencoder was trained on pre-processed mris using unires [3] into a
common mni space with a voxel size of 1 mm 3 and was then kept unchanged during
the ldm training. the latent representations of the t1w brain mris were then
used to train the ldm. a conditional u-net θ was then trained to predict the
artificial noise by the following objective: ddim [24] has been used in brain
ldm to replace the denoising diffusion probabilistic models (ddpm) during
inference to reduce the number of reverse steps with minimal performance loss
[21,24]. this network ε θ is conditioned on four conditional variables c: age,
gender, ventricular volume and brain volume, which are all introduced by
cross-attention layers [22]. gender is a binary variable, while the rest of the
covariates are scaled to [0, 1]. finally, the pre-trained decoder maps the
latent vector into an hr mri x = d(z 0 ). the architecture of the brain ldm can
be found in fig. 1.
in order to obtain a latent representation z t capable of reconstructing a given
noisy sample into a high-resolution image, we employ deterministic ddim sampling
[24]:where α 1:t ∈ (0, 1] t is a time-dependent decreasing sequence,
zt-represents the "predicted x 0 ", and √ 1α t-1 • θ (z t , c, t) can be
understood as the "direction pointing to x t " [24].corruption function f . we
assume a corruption function f known a-priori that is applied on the hr image x
obtained from the generative model, and compute the loss function based on the
corrupted image f • x and the given lr input image i. in clinical practice, a
prevalent method for acquiring mr images is prioritizing high in-plane
resolution while sacrificing through-plane resolution to expedite the
acquisition process and reduce motion artifacts [33]. to account for this
procedure, we introduce a corruption function that generates masks for
non-acquired slices, enabling our method to in-paint the missing slices. for
instance, on 1 × 1 × 4 mm 3 undersampled volumes, we create masks for three
slices every four slices on the generated hr 1 × 1 × 1 mm 3 volumes.
in the case of high sparsity mri sr, we optimize the noise latent code z * t and
its associated conditional variables c * to restore the hr image from the given
lr input image i using the optimization method:where ddim(z t , c, t )
represents t deterministic ddim sampling steps on the latent z 0 in eq. 2. we
follow the brain ldm model to use the perceptual loss l perc and the l1
pixelwise loss. the loss function is computed on the corrupted image generated
from the generative model and the given lr input. a detailed pseudocode
description of this method can be found in algorithm 1.
for low sparsity mri sr, we directly find the optimal latent code z * t using
the decoder d:
dataset for validation: we use 100 hr t1 mris from the ixi dataset
(http://brain-development.org/ixi-dataset/) to validate our method, after
filtering out those scans where registration failed. we note that subjects in
the ixi dataset are around 10 years younger on average than those in uk
biobank.the mri scans from uk biobank also had the faces masked out, while the
scans from ixi did not. this caused the faces of our reconstructions to appear
blurred.
conditional variables are all initialized to 0.5. voxels in all input volumes
are normalized to [0,1]. when sampling the pre-trained brain ldm with the ddim
sampler, we run t = 46 timesteps due to computational limitations on our
hardware. for inversesr(ldm), z t is initialized with random gaussian noise. for
inversesr(decoder), we compute the mean latent code z0 as z0 = s i=1 1 s ddim(z
i t , c, t ) by first sampling s = 10, 000 z i t samples from n (0, i), then
passing them through the ddim model. n = 600 gradient descent steps are used for
inversesr(ldm) to guarantee converging (algorithm 1, line 5). 600 optimization
steps are also utilized in inversesr(decoder). we use the adam optimizer with α
= 0.07, β 1 = 0.9 and β 2 = 0.999.
figure 2 shows the qualitative results on the coronal slices of sr from 4 and 8
mm axial scans. the advantage of our approach is clear compared to baseline
methods because it is capable of restoring hr mris with smoothness even when the
slice thickness is large (i.e., 8 mm). this is the case because the pre-trained
ldm we use is able to build a powerful prior over the hr t1w mri domain.
therefore, the generated images of our method are hr mris with smoothness in 3
directions: axial, sagittal and coronal, no matter how sparse the input images i
are. qualitative results of applying our method on tumour and lesion filling are
available in the supplementary material.table 1 shows quantitative results on
100 hr t1 scans from the ixi dataset, which the brain ldm did not have access to
during training. we investigated mean peak signal-to-noise ratio (psnr), and
structural similarity index measure (ssim) [28] values and their corresponding
standard deviation. we compare our method to cubic interpolation, as well as a
similar unsupervised approach, unires [3]. we show our approach and the two
compared methods on two different settings of slice imputation: 4 mm and 8 mm
thick-sliced axial scans representing low sparsity and high sparsity lr mris,
respectively. all the metrics are computed on a 3d volume around the brain of
size 160 × 224 × 160. for sr at 4 mm, inversesr(decoder) achieves the highest
mean ssim and psnr scores among all compared methods, which are slightly higher
than the scores for inversesr(ldm). for sr at 8 mm, inverse(ldm) achieves the
highest mean ssim and psnr and lowest standard error than the two baseline
methods, which could be attributed to the stronger prior learned by the ddim
model.
one key limitation of our method is the need for large computational resources
to perform the image reconstruction, in particular the long markov chain of
sampling steps required by the diffusion model to generate samples. an entire
pass through the diffusion model (lines 6-8 in algorithm 1) is required for
every step in the gradient descent method. another limitation of our method is
that it is limited by the capacity and output heterogeneity of the ldm
generator.
in this study, we have developed an unsupervised technique for mri
superresolution. we leverage a recent pre-trained brain ldm [21] for building
powerful image priors over t1w brain mris. unlike end-to-end supervised
approaches, which require retraining each time there is a distribution shift
over the input, our method is capable of being adapted to different settings of
mri sr problems at test time. this feature is suitable for mri sr since the
acquisition protocols and resolution of clinical brain mri exams vary across or
even within institutions. we proposed two novel strategies for different
settings of mri sr: inversesr(ldm) for low sparsity mri and inversesr(decoder)
for high sparsity mri. we validated our method on 100 brain t1w mris from the
ixi dataset through slice imputation using input scans of 4 and 8 mm slice
thickness, and compared our method with cubic interpolation and unires
[3].experimental results have shown that our approach achieves superior
performance compared to the unsupervised baselines, and could create smooth hr
images with fine detail even on an external dataset (ixi). experiments in this
paper focus on slice imputation, but our method could be adapted to other mri
under-sampling problems by implementing different corruption functions f . for
instance, for reconstructing k-space under-sampled mr images, a new corruption
function could be designed by first converting the hr image into k-space, then
masking a chosen set of k-space measurements, and then converting back to image
space. instead of estimating a single image, future work could also estimate a
distribution of reconstructed images through either variational inference (like
the brgm model [18]) or through sampling methods such as markov chain monte
carlo (mcmc) or langevin dynamics [15].
computed tomography (ct) is a prevalent imaging modality with applications in
biology, disease diagnosis, interventional imaging, and other areas.
highresolution ct (hrct) is beneficial for clinical diagnosis and surgical
planning because it can provide detailed spatial information and specific
features, usually employed in advanced clinical routines [1]. hrct usually
requires high-precision ct machines to scan for a long time with high radiation
doses to capture the internal structures, which is expensive and can impose the
risk of radiation exposure [2]. these factors make hrct relatively less
available, especially in towns and villages, compared to low-resolution ct
(lrct). however, degradation in spatial resolution and imaging quality brought
by lrct can interfere with the original physiological and pathological
information, adversely affecting the diagnosis [3]. consequently, how to produce
high-resolution ct scans at a smaller radiation dose level with lower scanning
costs is a holy grail of the medical imaging field (fig. 1). with the
advancement of artificial intelligence, super-resolution (sr) techniques based
on neural networks indicate new approaches to this problem. by inferring
detailed high-frequency features from lrct, super-resolution can introduce
additional knowledge and restore lost information due to lowresolution scanning.
deep-learning (dl) based methods, compared to traditional methods, can
incorporate hierarchical features and representations from prior knowledge,
resulting in improved results in sr tasks [4]. according to different
neural-network frameworks, these sr methods can be broadly categorized into two
classes: 1) convolutional neural network (cnn) based model [5][6][7], and 2)
generative adversarial network (gan) based model [2,8,9]. very recently, the
diffusion model is emerging as the most promising deep generative model [11],
which usually consists of two stages: a forward stage to add noises and a
reverse stage to separate noises and recover the original images. the diffusion
model shows impressive generative capabilities for many tasks, including image
generation, inpainting, translation, and super-resolution [10,12,13].while
dl-based methods can generate promising results, there can still be geometric
distortions and artifacts along with structural edges in the superresolved
results [15,16]. these structural features always represent essential
physiological structures, including vasculature, fibrosis, tumor, and other
lesions. the distortion and infidelity of these features can lead to potential
misjudgment for diagnosis, which is unacceptable for clinical application.
moreover, the target image size and spatial resolution of hrct for most existing
sr methods is about 512 × 512 and 0.8 × 0.8 mm 2 . with the progress in hardware
settings, ultra-high-resolution ct (uhrct) with an image size of 1024 × 1024 and
spatial resolution of 0.3 × 0.3 mm 2 can be available very recently [17]. though
uhrct can provide much more detailed information, to our best knowledge, sr
tasks targeting uhrct have rarely been discussed and reported.in this paper, we
propose a novel dual-stream conditional diffusion model for ct scan
super-resolution to generate uhrct results with high image quality and structure
fidelity. the conditional diffusion model takes the form p(y|x), where x is the
lrct, and y is the targeted uhrct [14]. the novel diffusion model incorporates a
dual-stream structure-preserving network and a novel imaging enhancement
operator in the denoising process. the imaging enhancement operator can
simultaneously extract the vascular and blob structures in the ct scans and
provide structure prior to the dual-stream network. the dualstream network can
fully exploit the prior information with two branches. one branch optimizes the
sr results in the image domain, and the other branch optimizes the results in
the structure domain. in practice, we use a convolution-based lightweight module
to simulate the filtering operations, which enables faster and easier
back-propagation in the training process. furthermore, we constructed a new
ultra-high resolution ct scan dataset obtained with the most advanced ct
machines. the dataset contained 87 uhrct scans with a spatial resolution of
0.34×0.34 mm 2 and an image size of 1024×1024. extensive experiments, including
qualitative and quantitative comparisons in both image consistency, structure
fidelity, and high-level tasks, demonstrated the superiority of our method. our
contributions can be summarized as follows:1) we proposed a novel dual-stream
diffusion model framework for ct superresolution. the framework incorporates a
dual-stream structure-preserving network in the denoising process to realize
better physiological structure restoration. 2) we designed a new image
enhancement operator to model the vascular and blob structures in medical
images. to avoid non-derivative operations in image enhancement, we proposed a
novel enhancement module consisting of lightweight convolutional layers to
replace the filtering operation for faster and easier back-propagation in
structural domain optimization.3) we established an ultra-high-resolution ct
scan dataset with a spatial resolution of 0.34 × 0.34 mm 2 and an image size of
1024 × 1024 for training and testing the sr task. 4) we have conducted extensive
experiments and demonstrated the excellent performance of the proposed sr
methods in both the image and structure domains. in addition, we have evaluated
our proposed method on high-level tasks, including vascular-system segmentation
and lesion detection on the srct, indicating the reliability of our sr results.
to preserve the structure and topology relationship in the denoising progress,
we designed a novel dual-stream diffusion model (dsdm) for better
superresolution and topology restoration (fig. 2). in the dsdm framework, given
a hrct slice y, we generate a noisy version ỹ, and train the network g dssp to
denoise ỹ with the corresponding lrct slice x and a noise level indicator γ.the
optimization is defined asin the denoising process, we used a dual-stream
structure-preserving (dssp) network for supervised structure restoration. the
dssp network optimizes the denoised results in the image domain and the
structure domain, respectively. the structural domain, obtained with the image
enhancement operator, is concatenated with the lrct slice as the input of the
structure branch. the final sr results are obtained after the feature fusion
model between the image map and the structure map.
we introduced an enhancement operator in the dssp network to model the vascular
and blob structures, which can represent important physiological information
according to clinical experience, and provide the prior structural information
for the dssp network. for one pixelt in the ct slice, let i (x) denote the
imaging intensity at this point. the 2 × 2 hessian matrix at the scale s is
defined as [18] where g (x, s) is the 2d gaussian kernel. the two eigenvalues of
the hessian matrix are denoted as λ = (λ 1 , λ 2 ) and here we agree that |λ 1 |
<= |λ 2 |. the eigenvalues of the hessian matrix can reflect the geometric
shape, curvature, and brightness of the local images. for the blob-like
structures, the three eigenvalues are about the same, λ 1 ≈ λ 2 ; for the
vascular-like structures, λ 2 can be much larger than the absolute value of λ 1
, |λ 2 | >> |λ 1 | [19]. the eigenvalue relations at scale s can be indicated by
several different functions. here we proposed a novel structure kernel function,
which is defined asκ 1 and κ 2 are the parameters to control the sensitivity for
the vascular-like structures and blob-like structures, respectively. λ τ is the
self-regulating factor.when λ 1 is about the same with λ 2 , λ τ is closed to λ
1 ; and when λ 1 is much smaller to λ 2 , λ τ is closed to λ 2 , which can
achieve a balance between two conditions.
we designed a new loss function to ensure that the final sr result can be
optimized in both the image domain and the structure domain. denoting the
reference image as y t , the pixel-wise loss in the imaging domain is formulated
asg image dssp (y t ) is the recovered image from the image-domain branch. l1
loss yields a significantly higher consistency and lower diversity, while l2
loss can better capture the outliers. here we used a parameter λ l1 to balance
these two losses.in the meantime, a structure-constraint loss is also necessary
to help the network achieve better performance in structure consistency. the
loss function consists of two parts, which measure the consistency of the
image-domain branch and the structure-domain branch. denoting the
structure-domain output as g struct dssp (y t ), the structure-constraint loss
can be presented ashowever, the image enhancement described above involves
overly complex calculations, making back-propagation difficult in the training
process. here we utilized a convolution-based operator o fc (•) to simplify the
calculation, which consists of several lightweight convolutional layers to
simulate the operation of image enhancement. in this way, we transform the
complex filtering operation into a simple convolution operation, thus
back-propagation can be easily processed. the loss function is then modified
asthe total objective function is the sum of two losses.3 experiments and
conclusion
we constructed three datasets for framework training and evaluation; two of them
were in-house data collected from two ct scanners(the ethics number is
20220359), and the other was the public luna16 dataset [22]. more details about
the two in-house datasets are described in the supplementary materials.we
evaluated our sr model on three ct datasets:• dataset 1: 2d super-resolution
from 256×256 to 1024×1024, with the spatial resolution from 1.36 × 1.36 mm 2 to
0.34 × 0.34 mm 2 .• dataset 2: 3d super-resolution from 256 × 256 × 1x to 512 ×
512 × 5x, with the spatial resolution from 1.60 × 1.60 × 5.00 mm 3 to 0.80 ×
0.80 × 1.00 mm 3 . • dataset 3: 2d super-resolution from 256 × 256 to 512 × 512
on the luna16 dataset.we compare our model with other sota super-resolution
methods, including bicubic interpolation, srcnn [7], srresnet [6], cycle-gan
[2], and sr3 [12]. performance is assessed qualitatively and quantitatively,
using psnr, ssim [23], visual information fidelity (vif) [24], and structure
mean square error (smse). vif value is correlated well with the human perception
of sr images, which can measure diagnostic acceptance and information
maintenance. smse is proposed to evaluate the structure difference between the
ground truth and srct. specially, we obtained the structural features of the
ground truth and srct with frangi filtering and then calculated the pixel-wise
difference [20].
qualitative comparisons are shown in fig. 3 and the quantitative results are
shown in table 1. the super-resolution results with our proposed methods achieve
the highest scores in both image restoration and structure consistency for most
indices, and there are no obvious secondary artifacts introduced in the sr
results. although the gan-based methods and sr3 can produce sharp details, they
tend to generate artifacts for the vascular systems, which is more evident in
the structure-enhanced figures. the problem of inconsistent structure is also
reflected in the value of vif and smse on both gan-based methods and sr3.lesion
detection and vessel segmentation on super-resolved ct. to further evaluate the
information maintenance of our sr methods, we conducted some high-level tasks,
including lung nodules detection and pulmonary airway and blood vessel
segmentation on the super-resolved ct scans. we compared the performance of
different methods on srct and the ground truth. for nodule detection, these
methods included u-net, v-net [25], resnet [26], dcnn [27] and 3d-dcnn [28]. for
the vessel segmentation, these methods included 3d u-net, v-net [25], nnunet
[31], nardelli et al. [30] and qin et al. [29]. figure 4 shows the quantitative
results of the performance comparison. the performance of these high-level tasks
on the sr results is comparable to or even better than that on the ground-truth
ct. such results, to some extent, demonstrated that our sr method does not
introduce artifacts or structural inconsistencies and cause misjudgment, while
the improved spatial resolution and image quality generated by our proposed
results shows great potential in improving the performance of high-level tasks.
in this paper, we have established a dual-stream diffusion model framework to
address the problem of topology distortion and artifact introduction that
generally exists in the medical super-resolution results. we first propose a
novel image enhancement operator to model the vessel and blob structures in the
ct slice, which can provide a structure prior to the sr framework. then, we
design a dualstream diffusion model that employs a dual-stream ream
structure-preserving network in the denoising process. the final sr outputs are
optimized not only by convolutional image-space losses but also by the proposed
structure-space losses. extensive experiments have shown that our sr methods can
achieve high performance in both image restoration and structure fidelity,
demonstrating the promising performance of information preservation and the
potential of applying our sr results to downstream tasks.
cycle-gan[2] 37.32 0.993 0.901 0.462 32.31 0.918 0.881 0.822 37.82 0.921 0.915
0.282 sr3[12] 37.18 0.974 0.812 0.474 36.85 0.957 0.916 0.859 39.57 0.968 0.902
0.274 our proposed 40.75 0.992 0.
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 25.
colorectal cancer (crc) is the third most commonly diagnosed cancer and is the
second most common cause of cancer death [23]. early detection is crucial for a
good prognosis. despite the existence of other techniques, such as virtual
colonoscopy (vc), optical colonoscopy (oc) remains the gold standard for
colonoscopy screening and the removal of precursor lesions. unfortunately, we do
not yet have the ability to reconstruct densely the 3d shape of large sections
of the colon. this would usher exciting new developments, such as
post-intervention diagnosis, measuring polyps and stenosis, and automatically
evaluating exploration thoroughness in terms of the surface percentage that has
been observed. this is the problem we address here. it has been shown that the
colon 3d shape can be estimated from single images acquired during human
colonoscopies [3]. however, to model large sections of it while increasing the
reconstruction accuracy, multiple images must be used. as most endoscopes
contain a single camera, the natural way to do this is to use video sequences
acquired by these cameras in the manner of structure-from-motion algorithms. an
important first step in that direction is to register the images from the
sequences. this can now be done reliably using either batch [21] or slam
techniques [8]. unfortunately, this solves only half the problem because these
techniques provide very sparse reconstructions and going from there to dense
ones remains an open problem. and occlusions, specularities, varying albedos,
and specificities of endoscopic lighting make it a challenging one.to overcome
these difficulties, we rely on two properties of endoscopic images:-endoluminal
cavities such as the gastrointestinal tract, and in particular the human colon,
are watertight surfaces. to account for this, we represent its surface in terms
of a signed distance function (sdf), which by its very nature presents
continuous watertight surfaces. -in endoscopy the light source is co-located
with the camera. it illuminates a dark scene and is always close to the surface.
as a result, the irradiance decreases rapidly with distance t from camera to
surface; more specifically it is a function of 1/t 2 . in other words, there is
a strong correlation between light and depth, which remains unexploited to
date.to take advantage of these specificities, we build on the success of neural
implicit surfaces (neus) [25] that have been shown to be highly effective at
deriving surface 3d models from sets of registered images. as the neural
radiance fields (nerfs) [15] that inspired them, they were designed to operate
on regular images taken around a scene, sampling fairly regularly the set of
possible viewing directions. furthermore, the lighting is assumed to be static
and distant so that the brightness of a pixel and its distance to the camera are
unrelated. unfortunately, none of these conditions hold in endoscopies. the
camera is inside a cavity (in the colon, a roughly cylindrical tunnel) that
limits viewing directions. the light source is co-located with the camera and
close to the surface, which results in a strong correlation between pixel
brightness and distance to the camera. in this paper, we show that, far from
being a handicap, this correlation is a key information for neural network
self-supervision.neus training selects a pixel from an image and samples points
along its projecting ray. however, the network is agnostic to the sampling
distance. in lightneus, we explicitly feed to the renderer the distance of each
one of these sampled points to the light source, as shown in fig. 1. hence, the
renderer can exploit the inverse-square illumination decline. we also introduce
and calibrate a photometric model for the endoscope light and camera, so that
the inverse square law discussed above actually holds. together, these two
changes make the minimization problem better posed and the automatic depth
estimation more reliable.our results show that exploiting the illumination is
key to unlocking implicit neural surface reconstruction in endoscopy. it
delivers accuracies in the range of 3 mm, whereas an unmodified neus is either 5
times less accurate or even fails to reconstruct any surface at all. earlier
methods [3] have reported similar accuracies but only on very few synthetic
images and on short sections of the colon. by contrast, we can handle much
longer ones and provide a broad evaluation in a real dataset (c3vd) over
multiple sequences. this makes us the first to show accurate results of extended
3d watertight surfaces from monocular endoscopy images.
3d reconstruction from endoscopic images. it can help with the effective
localization of lesions, such as polyps and adenomas, by providing a complete
representation of the observed surface. unfortunately, many state-of theart slam
techniques based on feature matching [5] or direct methods [6,7] are impractical
for dense endoscopic reconstruction due to the lack of texture and the
inconsistent lighting that moves along with the camera. nevertheless, sparse
reconstructions by classical structure-from-motion (sfm) algorithms can be good
starting points for refinement and densification based on shape-from-shading
(sfs) [24,28]. however, classical multi-view and sfs methods require strong
suboptimal priors on colon surface shape and reflectance.in monocular dense
reconstructions, it is common practice to encode shape priors in terms of smooth
rigid surfaces [14,17,20]. recently, [22] proposes a tubular topology prior for
nrsfm aimed to process endoluminal cavities where these tubular shapes are
prevalent. in contrast, for the same environments, we propose the watertight
prior coded by implicit sdf representations.recent methods for dense
reconstruction rely on neural networks to predict per-pixel depth in the 2d
space of each image and fuse the depth maps by using multi-view stereo (mvs) [2]
or a slam pipeline [12,13]. however, holes in the reconstruction appear due to
failures in triangulation and inaccurate depth estimation or in areas not
observed in any image. wang et al. [27] show the potential of neural rendering
in reconstruction from medical images, although they use a binocular static
camera with fixed light source, which is not feasible in endoluminal endoscopy.
unfortunately, most of the previous 3d methods do not provide code [14,22], are
not evaluated in biomedical settings [17,20], or do not report reconstruction
accuracy [12,13].neural radiance fields (nerfs) were first proposed to
reconstruct novel views of non-lambertian objects [15]. this method provides an
implicit neural representation of a scene in terms of local densities and
associated colors. in effect, the scene representation is stored in the weights
of a neural network, usually a multilayer perceptron (mlp), that learns its
shape and reflectance for any coordinate and viewing direction. nerfs use volume
rendering [9], based on ray-tracing from multiple camera positions. the volume
density σ(x) can be interpreted as the differential probability of a ray
terminating at an infinitesimal particle at location x. the expected color c(r)
of the pixel with camera ray r(t) = o + td is the integration of the radiance
emitted by the field at every traveled distance t from near to far bounds t n
and t f , such that where c stands for the color. the function t denotes the
accumulated transmittance along the ray from t n to t, that is the probability
that the ray travels from t n to t without hitting any other particle. the
authors propose two mlps to estimate the volume density function σ : x → [0, 1]
and the directional emitted color function c : (x, d) → [0, 1] 3 , so the
density of a point does not depend on the viewing direction d, but the color
does. this allows them to model non-lambertian reflectance. in addition, they
propose a positional encoding for location x and direction d, which allows
high-frequency details in the reconstruction.neural implicit surfaces (neus)
were introduced in [25] to improve the quality of nerf representation modelling
watertight surfaces. for that, the volume density σ is computed so as to be
maximal at the zero-crossings of a signed distance function (sdf) f :the sdf
formulation makes it possible to estimate the surface normal as n = ∇f (x). the
reflectance of a material is usually determined as a function of the incoming
and outgoing light directions with respect to the surface normal. therefore, the
normal is added as an input to the mlp that estimates color c : (x, d, n), as
shown in fig. 1.
in this section, we present the key contributions that make lightneus a neural
implicit reconstruction method suitable for endoscopy in endoluminal cavities.
in this context, the light source is located next to the camera and moves with
it. furthermore, it is close to the surfaces to be modeled. as a result, for any
surface point x = o+td, the irradiance decreases with the square of the distance
to the camera t. hence, we can write the color of the corresponding pixel as
[3]:where l e is the radiance emitted by the light source to the surface point,
that was modeled and calibrated in the endomapper dataset [1] according to the
sls model from [16]. the bidirectional reflectance distribution function (brdf)
determines how much light is reflected to the camera, and the cosine term cos
(θ) = -d • n weights the incoming radiance with respect to the surface normal n.
equation ( 3) also takes into account the camera gain g and gamma correction γ.
the neus formulation of sect. 2 assumes distant and fixed lighting. however, in
endoscopy inverse-square light decline is significant, as quantified in eq. (
3).accounting for this is done by modifying the original neus formulation as
follows. figure 1 c(x,d,n) may learn to model non-lambertian brdf(x, d),
including specular highlights, and the cosine term of eq. ( 3). however, if the
distance t from the light to the point x is not provided to the color network,
the 1/t 2 dependency cannot be learned, and surface reconstruction will fail.
our key insight is to explicitly supply this distance as input to the volume
rendering algorithm, as shown in red in fig. 1 and reformulate eq. (1) asthis
conceptually simple change, using illumination decline while training, unlocks
all the power of neural surface reconstruction in endoscopy.
apart from illumination decline, there are several significant differences
between the images captured by endoscopes and those conventionally used to train
nerfs and neus: fish-eye lenses, strong vignetting, uneven scene illumination,
and postprocessing.endoscopes use fisheye lenses to cover a wide field of view,
usually close to 170 • . these lenses produce strong deformations, making it
unwise to use the standard pinhole camera model. instead, specific models
[10,19] must be used. hence, we also modified the original neus implementation
to support these models.the light sources of endoscopes behave like spotlights.
in other words, they do not emit with the same intensity in all directions, so l
e in eq. ( 3) is not constant for all image pixels. this effect is similar to
the vignetting effect caused by conventional lenses, that is aggravated in
fisheye lenses. fortunately, they can be accurately calibrated [1,16] and
compensated for.the post-processing software of medical endoscopes is designed
to always display well-exposed images, so that physicians can see details
correctly. an adaptive gain factor g is applied by the endoscope's internal
logic and gamma correction is also used to adapt to non-linear human vision,
achieving better contrast perception in mid tones and dark areas. endoscope
manufacturers know the post-processing logic of their devices, but this
information is proprietary and not available to users. again, gamma correction
can be calibrated assuming it is constant [3], and the gain change between
successive images can be estimated, for example, by sparse feature matching.all
these factors must be taken into account during network training. thus, our
photometric loss is computed using a normalized image:(5)
we validate our method on the c3vd dataset [4], which covers all different
sections of the colon anatomy in 22 video sequences. this dataset contains
sequences recorded with a medical video colonoscope, olympus evis exera iii
cf-hq190l. the images were recorded inside a phantom, a model of a human colon
made of silicone. the intrinsic camera parameters are provided. the camera
extrinsics for each frame are estimated by 2d-3d registration against the known
3d model. in an operational setting, we could use a structure-from-motion
approach such as colmap [21] or a slam technique such as [8], which have been
shown to work well in endoscopic settings. the gain values were easily estimated
from the dataset itself. for vignetting, we use the calibration obtained from a
colonoscope of the same brand and series from the endomapper dataset [1].during
training, we follow the neus paper approach of using a few informative frames
per scene, as separated as possible, by sampling each video uniformly. for each
sequence, we train both the vanilla neus and our lighneus using 20 frames each
time. they are extracted uniformly over the duration of the video. we use the
same batch size and number of iterations as in the original neus paper, 512 and
300k respectively. once the network is trained, we can extract triangulated
meshes from the reconstruction. since the c3vd dataset comprises a ground-truth
triangle mesh, we compute point-to-triangle distances from all the vertices in
the reconstruction to the closest ground-truth triangle.in the first rows of
table 1, we report median (medae), mean (mae), and root mean square (rmse)
values of these distances for all vertices seen in at least one image. columns
show the result for 22 sequences. we note 18 sequences where the camera moved at
least 1 cm, and the reconstruction yielded a mean error of 2.80 mm. the other
four smaller trajectories (<1 cm) lack parallax and the mean error is higher
(8.23 mm). this is in the range of reported accuracy in the literature for
monocular dense non-watertight depth estimation, 1.1 mm in [14] for high
parallax geometry in laparoscopy, which is a much more favorable geometry than
the one we have here, or 0.85 mm for the significantly smaller-size cavities of
endoscopic endonasal surgery (ess) [11].in contrast, vanilla neus assumes
constant illumination. the strong light changes typical of endoscopy fatally
mislead the method. we only report numerical results of neus in two sequences
because in all the rest, the sdf diverges and ends up blown out of the rendering
volume, giving no result at all. the neus reconstruction exhibits multiple
artifacts that make it unusable. bottom: our reconstruction is much closer to
the ground truth shape. the error is shown in blue if the reconstruction is
inside the surface, and in red otherwise. a fully saturated red or blue denotes
an error of more than 1 cm and grey denotes no error at all. we provide a
qualitative result in fig. 2 and additional ones in the supplementary material.
note that the watertight prior inherent to an sdf allows the network to
hallucinate unseen areas. remarkably, these unsurveyed areas continue the
tubular shape of the colon and we found them to be mostly accurate when compared
to the ground truth. for example, the curved areas of the colon where a wall is
occluded behind the corner of the curve is reconstructed, as shown in fig. 3.
this ability to "fill in" observation gaps may be useful in providing the
endoscopist with an estimate of the percentage of unsurveyed area during a
procedure.we hypothesize that this desirable behavior stems from the fact that
the network learns an empirical shape prior from the observed anatomy of the
colon. however, we don't expect this behavior to hold for distant unseen parts,
but only for regions closer than 20 mm to one observation. in the last rows of
table 1, we compute accuracy metrics for this extended region. it includes not
only surveyed areas, but also neighboring areas that were not observed.
we have presented a method for 3d dense multi-view reconstruction from
endoscopic images. we are the first to show that neural radiance fields can be
used to obtain accurate dense reconstructions of colon sections of significant
length. at the heart of our approach, is exploiting the correlation between
depth and brightness. we have observed that, without it, neural reconstruction
fails.the current method could be used offline for post-exploration coverage
analysis and endoscopist training. but real-time performance could be achieved
in the future as the new neus2 [26] converges in minutes, enabling automatic
coverage reporting. similar to other reconstruction methods, for now our
approach works in areas of the colon where there is little deformation. several
sub-maps of non-deformed areas can be created if necessary. however, this
limitation could be overcome by adopting the deformable nerfs formalism [18].
the online version contains supplementary material available at
https://doi.org/10.1007/978-3-031-43999-5 48.
