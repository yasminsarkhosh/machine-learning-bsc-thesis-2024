<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text-Guided Foundation Model Adaptation for Pathological Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yunkun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jin</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<region>New Jersy</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dequan</forename><surname>Wang</surname></persName>
							<email>dequanwang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shanghai AI Laboratory</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text-Guided Foundation Model Adaptation for Pathological Image Classification</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="272" to="282"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">11D4C88A639FC05BBAC8868854BEB5AE</idno>
					<idno type="DOI">10.1007/978-3-031-43904-9_27</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Foundation models</term>
					<term>Multi-modality</term>
					<term>Model Adaptation</term>
					<term>Pathological image classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recent surge of foundation models in computer vision and natural language processing opens up perspectives in utilizing multimodal clinical data to train large models with strong generalizability. Yet pathological image datasets often lack biomedical text annotation and enrichment. Guiding data-efficient image diagnosis from the use of biomedical text knowledge becomes a substantial interest. In this paper, we propose to Connect Image and Text Embeddings (CITE) to enhance pathological image classification. CITE injects text insights gained from language models pre-trained with a broad range of biomedical texts, leading to adapt foundation models towards pathological image understanding. Through extensive experiments on the PatchGastric stomach tumor pathological image dataset, we demonstrate that CITE achieves leading performance compared with various baselines especially when training data is scarce. CITE offers insights into leveraging in-domain text knowledge to reinforce data-efficient pathological image classification. Code is available at https://github.com/Yunkun-Zhang/CITE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning for medical imaging has achieved remarkable progress, leading to a growing body of parameter-tuning strategies <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. Those approaches are often designed to address disease-specific problems with limitations in their generalizability. In parallel, foundation models <ref type="bibr" target="#b3">[4]</ref> have surged in computer vision <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and natural language processing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> with growing model capacity and data size, opening up perspectives in utilizing foundation models and large-scale clinical data for diagnostic tasks. However, pure imaging data can be insufficient to adapt foundation models with large model capacity to the medical field. Given the complex tissue characteristics of pathological whole slide images (WSI), it is crucial to develop adaptation strategies allowing <ref type="bibr" target="#b0">(1)</ref> training data efficiency, and (2) data fusion flexibility for pathological image analysis. Although foundation models promise a strong generalization ability <ref type="bibr" target="#b3">[4]</ref>, there is an inherent domain shift between medical and natural concepts in both vision and language modalities. Pre-trained biomedical language models are increasingly applied to medical context understanding <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Language models prove to be effective in capturing semantic characteristics with a lower data acquisition and annotation cost in medical areas <ref type="bibr" target="#b11">[12]</ref>. Such property is desired to address the dilemma of medical imaging cohorts, where well-annotated, high-quality medical imaging cohorts are expensive to collect and curate compared with text inputs <ref type="bibr" target="#b12">[13]</ref>. In addition, vision-language models demonstrate the importance of joining multi-modal information for learning strong encoders <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>. Thus, connecting visual representations with text information from biomedical language models becomes increasingly critical to adapting foundation models for medical image classification, particularly in the challenging setting of data deficiency.</p><p>In this study, we propose CITE, a data-efficient adaptation framework that Connects Image and Text Embeddings from foundation models to perform pathological image classification with limited training samples (see Fig. <ref type="figure" target="#fig_0">1</ref>). To enable language comprehension, CITE makes use of large language models pretrained on biomedical text datasets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> with rich and professional biomedical knowledge. Meanwhile, for visual understanding, CITE only introduces a small number of trainable parameters to a pre-trained foundation model, for example, CLIP <ref type="bibr" target="#b4">[5]</ref> and INTERN <ref type="bibr" target="#b5">[6]</ref>, in order to capture domain-specific knowledge without modifying the backbone parameters. In this framework, we emphasize the utility of text information to play a substitutive role as traditional classification heads, guiding the adaptation of the vision encoder. A favorable contribution of our approach is to retain the completeness of both pre-trained models, enabling a low-cost adaptation given the large capacity of foundation models. Overall, our contributions are summarized as follows:</p><p>1. We demonstrate the usefulness of injecting biomedical text knowledge into foundation model adaptation for improved pathological image classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Medical Image Classification. Deep learning for medical image classification has long relied on training large models from scratch <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>. Also, fine-tuning or linear-probing the pre-trained models obtained from natural images <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> is reasonable. However, those methods are supported by sufficient high-quality data expensive to collect and curate <ref type="bibr" target="#b18">[19]</ref>. In addition, task-specific models do not generalize well with different image modalities <ref type="bibr" target="#b1">[2]</ref>. To tackle this issue, we emphasize the adaptation of foundation models in a data-efficient manner.</p><p>Vision-Language Pre-training. Recent work has made efforts in pre-training vision-language models. CLIP <ref type="bibr" target="#b4">[5]</ref> collects 400 million image-text pairs from the internet and trains aligned vision and text encoders from scratch. LiT <ref type="bibr" target="#b19">[20]</ref> trains a text encoder aligned with a fixed pre-trained vision encoder. BLIP-2 <ref type="bibr" target="#b13">[14]</ref> trains a query transformer by bootstrapping from pre-trained encoders. REACT <ref type="bibr" target="#b20">[21]</ref> fixes both pre-trained encoders and tunes extra gated self-attention modules. However, those methods establish vision-language alignment by pre-training on large-scale image-text pairs. Instead, we combine pre-trained unimodal models on downstream tasks and build a multi-modal classifier with only a few data.</p><p>Model Adaptation via Prompt Tuning. Prompt tuning proves to be an efficient adaptation method for both vision and language models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Originating from natural language processing, "prompting" refers to adding (manual) text instructions to model inputs, whose goal is to help the pre-trained model better understand the current task. For instance, CoOp <ref type="bibr" target="#b21">[22]</ref> introduces learnable prompt parameters to the text branch of vision-language models. VPT <ref type="bibr" target="#b22">[23]</ref> demonstrates the effectiveness of prompt tuning with pre-trained vision encoders. In this study, we adopt prompt tuning for adaptation because it is lightweight and only modifies the input while keeping the whole pre-trained model unchanged. However, existing prompt tuning methods lack expert knowledge and understanding of downstream medical tasks. To address this challenge, we leverage large language models pre-trained with biomedical text to inject medical domain knowledge.</p><p>Biomedical Language Model Utilization. Biomedical text mining promises to offer the necessary knowledge base in medicine <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Leveraging language models pre-trained with biomedical text for medical language tasks is a common application. For instance, Alsentzer et al. <ref type="bibr" target="#b8">[9]</ref> pre-train a clinical text model with BioBERT <ref type="bibr" target="#b9">[10]</ref> initialization and show a significant improvement on five clinical language tasks. However, the potential of biomedical text information in medical imaging applications has not been explicitly addressed. In our efforts, we emphasize the importance of utilizing biomedical language models for adapting foundational vision models into cancer pathological analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Figure <ref type="figure">2</ref> depicts an overview of our approach CITE for data-efficient pathological image classification. CITE jointly understands the image features extracted by vision encoders pre-trained with natural imaging, and text insights encoded in large language models pre-trained with biomedical text (e.g., BioLinkBERT <ref type="bibr" target="#b10">[11]</ref> which captures rich text insights spanning across biomedical papers via citations). We connect text and imaging by a projection and classify the images by comparing the cosine similarity between image and text embeddings. Importantly, we introduce two low-cost sets of trainable parameters to the vision encoder in order to adapt the model with the guidance of text information. They are (1) prompt tokens in the input space to model task-specific information, and (2) a projection layer in the latent space to align image and text embeddings. During model adaptation, we freeze the pre-trained encoders and only tune the introduced parameters, which not only saves remarkable training data and computational resources but also makes our approach favorable with various foundation model architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Connecting Text and Imaging</head><p>An image I to be classified is processed through a pre-trained vision encoder to generate the image embedding x v with dimension d v , where v stands for "vision":</p><formula xml:id="formula_0">x v = VisionEncoder(I) x v ∈ R dv . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>For the label information, we encode the class names T c (c ∈ [1, C]) with a pre-trained biomedical language model instead of training a classification head (see Fig. <ref type="figure">2(e)</ref>). We tokenize and process T c through the language encoder to generate the text embedding x c l with dimension d l , where l stands for "language":</p><formula xml:id="formula_2">x c l = LanguageEncoder(Tokenizer(T c )) x c l ∈ R d l . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>Vision-language models like CLIP <ref type="bibr" target="#b4">[5]</ref> contain both a vision encoder and a language encoder, which provide well-aligned embeddings in the same feature space. In this case, prediction ŷ is obtained by applying softmax on scaled cosine similarities between the image and text embeddings (see Fig. <ref type="figure">2(d))</ref>:</p><formula xml:id="formula_4">p(ŷ = c|I) = exp(sim(x c l , x v )/τ ) C c =1 exp(sim(x c l , x v )/τ ) , (<label>3</label></formula><formula xml:id="formula_5">)</formula><p>where sim(•, •) refers to cosine similarity and τ is the temperature parameter.</p><p>For irrelevant vision and language encoders, we introduce an extra projection layer to the end of the vision encoder to map the image embeddings to the same latent space as the text embeddings. We replace x v in Eq. ( <ref type="formula" target="#formula_4">3</ref>) with x v :</p><formula xml:id="formula_6">x v = Projection(x v ) x v ∈ R d l . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>During adaptation, the extra parameters are updated by minimizing the cross-entropy of the predictions from Eq. ( <ref type="formula" target="#formula_4">3</ref>) and the ground truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Visual Prompt</head><p>Medical concepts exhibit a great visual distribution shift from natural images, which becomes impractical for a fixed vision encoder to capture task-specific information in few-shot scenarios. Visual prompt tuning (VPT <ref type="bibr" target="#b22">[23]</ref>) is a lightweight adaptation method that can alleviate such an inherent difference by only tuning prompt tokens added to the visual inputs of a fixed vision transformer <ref type="bibr" target="#b23">[24]</ref>, showing impressive performance especially under data deficiency. Thus, we adopt VPT to adapt the vision encoder in our approach.</p><p>A vision transformer first cuts the image into a sequence of n patches and projects them to patch embeddings E 0 ∈ R n×dv , where d v represents the visual embedding dimension. A CLS token c 0 ∈ R dv is prepended to the embeddings, together passing through K transformer layers {L k v } k=1,2,...,K . CLS embedding of the last layer output is the image feature x v . Following the setting of shallow VPT, we concatenate the learnable prompt tokens P = [p<ref type="foot" target="#foot_0">1</ref> , . . . , p p ] ∈ R p×dv , where p is the prompt length, with CLS token c 0 and patch embeddings E 0 before they are processed through the first transformer layer:</p><formula xml:id="formula_8">[c 1 , Z 1 , E 1 ] = L 1 v ([c 0 , P , E 0 ]) [c k , Z k , E k ] = L k v ([c k-1 , Z k-1 , E k-1 ]) k = 2, 3, . . . , K x v = c K x v ∈ R dv ,<label>(5)</label></formula><p>where [•, •] refers to concatenation along the sequence length dimension, and Z k ∈ R p×dv represents the output embeddings of the k-th transformer layer at the position of the prompts (see Fig. <ref type="figure">2(a-c</ref>)). The prompt parameters are updated together with the projection layer introduced in Sect. 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Settings</head><p>Dataset. We adopt the PatchGastric <ref type="bibr" target="#b24">[25]</ref> dataset, which includes histopathological image patches extracted from H&amp;E stained whole slide images (WSI) of stomach adenocarcinoma endoscopic biopsy specimens. There are 262,777 patches of size 300 × 300 extracted from 991 WSIs at x20 magnification. The dataset contains 9 subtypes of gastric adenocarcinoma. We choose 3 major subtypes including "well differentiated tubular adenocarcinoma", "moderately differentiated tubular adenocarcinoma", and "poorly differentiated adenocarcinoma" to form a 3-class grading-like classification task with 179,285 patches from 693 WSIs. We randomly split the WSIs into train (20%) and validation (80%) subsets for measuring the model performance. To extend our evaluation into the real-world setting with insufficient data, we additionally choose 1, 2, 4, 8, or 16 WSIs with the largest numbers of patches from each class as the training set.</p><p>The evaluation metric is patient-wise accuracy, where the prediction of a WSI is obtained by a soft vote over the patches, and accuracy is averaged class-wise.</p><p>Implementation. We use CLIP ViT-B/16 <ref type="bibr" target="#b4">[5]</ref> as the visual backbone, with input image size 224 × 224, patch size 16 × 16, and embedding dimension d v = 512. We adopt BioLinkBERT-large <ref type="bibr" target="#b10">[11]</ref> as the biomedical language model, with embedding dimension d l = 1, 024. To show the extensibility of our approach, we additionally test on vision encoders including ImageNet-21k ViT-B/16 <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> and INTERN ViT-B/16 <ref type="bibr" target="#b5">[6]</ref>, and biomedical language model BioBERT-large <ref type="bibr" target="#b9">[10]</ref>.</p><p>Our implementation is based on CLIP 1 , HuggingFace<ref type="foot" target="#foot_1">2</ref> and MMClassification<ref type="foot" target="#foot_2">3</ref> .</p><p>Training Details. Prompt length p is set to 1. We resize the images to 224×224 to fit the model and follow the original data pipeline in PatchGastric <ref type="bibr" target="#b24">[25]</ref>. A class-balanced sampling strategy is adopted by choosing one image from each class in turn. Training is done with 1,000 iterations of stochastic gradient descent (SGD), and the mini-batch size is 128, requiring 11.6 GB of GPU memory and 11 min on two NVIDIA GeForce RTX 2080 Ti GPUs. All our experiment results are averaged on 3 random seeds unless otherwise specified. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>CITE Consistently Outperforms all Baselines Under all Data Scales. Figure <ref type="figure" target="#fig_2">3</ref> shows the classification accuracy on the PatchGastric dataset of our approach compared with baseline methods and related works, including (1) R50-21k: fine-tune the whole ResNet50 <ref type="bibr" target="#b26">[27]</ref> backbone pre-trained on ImageNet-21k <ref type="bibr" target="#b25">[26]</ref>.</p><p>(2) Linear probe: train a classification head while freezing the backbone encoder.</p><p>(3) Fine-tune: train a classification head together with the backbone encoder. (4) CLAM <ref type="bibr" target="#b17">[18]</ref>: apply an attention network on image features to predict pseudo labels and cluster the images. ( <ref type="formula" target="#formula_8">5</ref>) Zero-shot <ref type="bibr" target="#b4">[5]</ref>: classify images to the nearest text embeddings obtained by class names, without training. (6) Few-shot <ref type="bibr" target="#b27">[28]</ref>: cluster image features of the training data and classify images to the nearest class center. ( <ref type="formula">7</ref>) VPT <ref type="bibr" target="#b22">[23]</ref>: train a classification head together with visual prompts. Note that CLIP ViT-B/16 vision encoder is adopted as the backbone for ( <ref type="formula" target="#formula_2">2</ref>)- <ref type="bibr" target="#b6">(7)</ref> Table <ref type="table">2</ref>. CITE fits in with various pre-trained encoders. We include CLIP ViT-B/16 <ref type="bibr" target="#b4">[5]</ref>, ImageNet-21k ViT-B/16 <ref type="bibr" target="#b25">[26]</ref> and INTERN ViT-B/16 <ref type="bibr" target="#b5">[6]</ref> visual encoders, combined with CLIP textual encoder <ref type="bibr" target="#b4">[5]</ref>, BioBERT (BB) <ref type="bibr" target="#b9">[10]</ref> and BioLinkBERT (BLB) <ref type="bibr" target="#b10">[11]</ref> language models. The highest performance of each visual encoder is bolded.</p><p>For each combination, CITE consistently outperforms linear and fine-tune baselines. CITE Shows Model Extensibility. We evaluate our approach with additional backbones and biomedical language models to assess its potential extensibility. Table <ref type="table">2</ref> displays the findings of our approach compared with linear probe and fine-tune baselines. The results demonstrate that CITE is compatible with a variety of pre-trained models, making it immune to upstream model modifications. The text information encoded in biomedical language models allows vision models pre-trained with natural imaging to bridge the domain gap without taskspecific pre-training on medical imaging. Importantly, when using both the vision and language encoders of CLIP ViT-B/16, our approach still outperforms the baselines by a remarkable margin (47.7% to 60.1%), demonstrating the importance of multi-modal information. While CLIP gains such modality matching through pre-training, our CITE shows an appealing trait that irrelevant vision and language models can be combined to exhibit similar multi-modal insights on pathological tasks without a need of joint pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Adapting powerful foundation models into medical imaging constantly faces data-limited challenges. In this study, we propose CITE, a data-efficient and model-agnostic approach to adapt foundation models for pathological image classification. Our key contribution is to inject meaningful medical domain knowledge to advance pathological image embedding and classification. By tuning only a small number of parameters guided by biomedical text information, our approach effectively learns task-specific information with only limited training samples, while showing strong compatibility with various foundation models. To augment the current pipeline, the use of synthetic pathological images is promising <ref type="bibr" target="#b28">[29]</ref>. Also, foundation training on multi-modal medical images is of substantial interest to enhance model robustness under data-limited conditions <ref type="bibr" target="#b29">[30]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Connecting Image and Text Embeddings. Our CITE emphasizes a textguided model adaptation. An image with the visual prompt is processed through a vision encoder and a projection layer. The text knowledge is embedded by a text encoder, where a stop-gradient operation is applied. Classification prediction is made by the similarity between image and text embeddings. During adaptation, the visual prompt and the projection are tuned while the pre-trained encoders are frozen .</figDesc><graphic coords="2,98,46,67,31,234,64,60,76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 lFig. 2 .</head><label>32</label><figDesc>Fig. 2. An overview of CITE. (a) The pathological images are cut into patches. (b) The class token, image tokens, and learnable prompt tokens are concatenated. (c) The tokens are processed by a pre-trained vision transformer to generate image embeddings. Those 3 steps refer to learning visual prompt (Sect. 3.2). (d) The image is recognized as the class with maximum cosine similarity between image and text embeddings. (e) The class names are processed by a biomedical language model to generate text embeddings. Those 2 steps connect text and imaging (Sect. 3.1).</figDesc><graphic coords="4,58,98,54,32,334,60,125,44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. Accuracy on the PatchGastric<ref type="bibr" target="#b24">[25]</ref> 3-category classification task. R50-21k refers to ResNet50<ref type="bibr" target="#b26">[27]</ref> backbone pre-trained on ImageNet-21k<ref type="bibr" target="#b25">[26]</ref>. Other methods adopt CLIP ViT-B/16<ref type="bibr" target="#b4">[5]</ref> backbone. Averaged results and standard deviation (error bars) of 3 runs are displayed. Our CITE consistently outperforms all baselines under all data fractions, showing a remarkable improvement under data deficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation study of CITE with and without prompt and text. We report the average accuracy and standard deviation. When prompt is not used, we fine-tune the whole vision backbone. When text is not used, we adopt the traditional classification head. Each component improves the performance.</figDesc><table><row><cell>. Our</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>±0.1 49.9 ±0.1 51.2 ±0.1 60.3 ±0.1 61.4 ±0.1 65.4 ±0.1 ±1.2 39.0 ±1.2 44.1 ±1.2 51.7 ±1.2 57.1 ±1.2 66.3 ±1.2 ±0.7 45.8 ±1.6 53.4 ±1.2 59.5 ±0.5 60.6 ±0.6 66.5 ±0.8 ±0.3 49.6 ±0.1 50.8 ±0.1 59.3 ±0.3 62.2 ±0.4 66.3 ±0.2 ±1.4 51.8 ±1.3 56.6 ±1.9 62.7 ±1.0 64.0 ±0.5 67.2 ±1.4Visual Prompt and Text Information are Both Necessary. We conduct ablation studies to show the effectiveness of visual prompt learning and text information. From the results in Table1, we demonstrate that visual prompt learning outperforms fine-tuning as the adaptation method, and in-domain text information outperforms classification heads. Combining the two components yields the best results under all data scales. Importantly, text information is particularly effective when training data is extremely scarce (1 slide per class).</figDesc><table><row><cell>Visual</cell><cell cols="3">Method Textual 1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>All</cell></row><row><cell>CLIP ViT-B/16</cell><cell cols="7">Linear 47.7 Fine-tune --39.1 CITE CLIP 60.1 ±0.9 59.0 ±0.1 60.9 ±0.9 63.2 ±0.2 65.9 ±0.5 68.7 ±0.6</cell></row><row><cell></cell><cell>CITE</cell><cell>BLB</cell><cell cols="5">60.2 ±1.2 59.1 ±1.2 60.3 ±0.8 66.4 ±0.7 67.9 ±0.4 69.7 ±0.1</cell></row><row><cell>IN-21k ViT-B/16</cell><cell cols="7">Linear 46.7 Fine-tune --48.0 CITE BB 51.4 CITE BLB 52.4 ±1.5 52.7 ±0.8 57.0 ±0.9 62.8 ±1.2 64.5 ±1.1 67.4 ±0.7</cell></row><row><cell cols="2">INTERN ViT-B/16 Linear</cell><cell>-</cell><cell cols="5">47.3 ±0.2 47.2 ±0.2 52.4 ±0.5 59.7 ±0.3 63.1 ±0.2 66.8 ±0.7</cell></row><row><cell></cell><cell cols="2">Fine-tune -</cell><cell cols="5">42.0 ±0.3 46.0 ±0.3 51.0 ±0.9 60.4 ±0.1 62.7 ±0.5 68.2 ±0.4</cell></row><row><cell></cell><cell>CITE</cell><cell>BB</cell><cell cols="5">51.7 ±0.1 55.4 ±1.8 59.6 ±0.3 66.4 ±0.8 68.1 ±0.8 69.7 ±0.7</cell></row><row><cell></cell><cell>CITE</cell><cell>BLB</cell><cell cols="5">48.4 ±5.2 49.1 ±5.5 57.9 ±0.8 65.3 ±0.4 67.9 ±0.8 69.4 ±0.9</cell></row><row><cell cols="8">demonstrate that adding domain-specific text information provides an efficient</cell></row><row><cell cols="8">means to guide foundation model adaptation for pathological image diagnosis.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/openai/CLIP.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/huggingface/transformers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/open-mmlab/mmclassification.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-scale convolutional neural networks for lung nodule classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-19992-4_46</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-19992-4_46" />
	</analytic>
	<monogr>
		<title level="m">IPMI 2015</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C.-F</forename><surname>Westin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9123</biblScope>
			<biblScope unit="page" from="588" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning-based breast cancer classification through medical imaging modalities: state of the art and research challenges</title>
		<author>
			<persName><forename type="first">G</forename><surname>Murtaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1655" to="1720" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatially aware graph neural networks and cross-level molecular profile prediction in colon cancer histopathology: a retrospective multi-cohort study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="787" to="e795" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.08687</idno>
		<title level="m">Intern: a new learning paradigm towards general vision</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03323</idno>
		<title level="m">Publicly available clinical BERT embeddings</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Linkbert: pretraining language models with document links</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VisualGPT: data-efficient adaptation of pretrained language models for image captioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18030" to="18040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An annotation-free whole-slide training approach to pathological classification of lung cancer types using deep learning</title>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1193</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Blip-2: bootstrapping language-image pretraining with frozen image encoders and large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12597</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Medical image classification with convolutional neural network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International Conference on Control Automation Robotics &amp; Vision (ICARCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="844" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gastric pathology image classification using stepwise fine-tuning for deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hiruta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Terai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nosato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murakawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sakanashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Healthc. Eng</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classification and mutation prediction based on histopathology H&amp;E images in liver cancer using deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ Precis. Oncol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Data-efficient and weakly supervised computational pathology on whole-slide images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mahmood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="570" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Talius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lit: zero-shot transfer with locked-image text tuning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18123" to="18133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning customized visual models with retrieval-augmented knowledge</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15148" to="15158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to prompt for vision-language models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2337" to="2348" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.12119</idno>
		<title level="m">Visual prompt tuning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Tsuneki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kanavati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03432</idno>
		<title level="m">Inference of captions from histopathological patches</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Meta-baseline: exploring simple meta-learning for few-shot learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9062" to="9071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A large-scale synthetic pathological dataset for deep learning-enabled segmentation of breast cancer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gevaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">231</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Training like a medical resident: universal medical image segmentation via context prior learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Meta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.02416</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
