<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation</title>
				<funder ref="#_aqCw52C">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_4dBh88H">
					<orgName type="full">Shun Hing Institute of Advanced Engineering</orgName>
				</funder>
				<funder ref="#_XZNAszg">
					<orgName type="full">Shenzhen-Hong Kong-Macau Technology Research Programme</orgName>
					<orgName type="abbreviated">C</orgName>
				</funder>
				<funder ref="#_NQDAn59">
					<orgName type="full">General Research Fund</orgName>
				</funder>
				<funder ref="#_xWNgZzB">
					<orgName type="full">Hong Kong Research Grants Council</orgName>
					<orgName type="abbreviated">RGC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">An</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electronic Engineering</orgName>
								<orgName type="department" key="dep2">Shun Hing Institute of Advanced Engineering (SHIAE)</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong, Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengya</forename><surname>Xu</surname></persName>
							<email>mengya@u.nus.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electronic Engineering</orgName>
								<orgName type="department" key="dep2">Shun Hing Institute of Advanced Engineering (SHIAE)</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong, Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="institution">Hubei University of Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mobarakol</forename><surname>Islam</surname></persName>
							<email>mobarakol.islam@ucl.ac.uk</email>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Medical Physics and Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">EPSRC Centre for Interventional and Surgical Sciences (WEISS)</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>Wellcome, London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongliang</forename><surname>Ren</surname></persName>
							<email>hlren@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electronic Engineering</orgName>
								<orgName type="department" key="dep2">Shun Hing Institute of Advanced Engineering (SHIAE)</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong, Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning for Scribble-Supervised Polyp Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DB77B97B919EBCE03DDCA5843485CF1F</idno>
					<idno type="DOI">10.1007/978-3-031-43907-04.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Polyp Image Segmentation</term>
					<term>Weakly-supervised Learning</term>
					<term>Spatial-Spectral Dual Branches</term>
					<term>Mutual Teaching</term>
					<term>Ensemble Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fully-supervised polyp segmentation has accomplished significant triumphs over the years in advancing the early diagnosis of colorectal cancer. However, label-efficient solutions from weak supervision like scribbles are rarely explored yet primarily meaningful and demanding in medical practice due to the expensiveness and scarcity of denselyannotated polyp data. Besides, various deployment issues, including data shifts and corruption, put forward further requests for model generalization and robustness. To address these concerns, we design a framework of Spatial-Spectral Dual-branch Mutual Teaching and Entropy-guided Pseudo Label Ensemble Learning (S 2 ME). Concretely, for the first time in weakly-supervised medical image segmentation, we promote the dualbranch co-teaching framework by leveraging the intrinsic complementarity of features extracted from the spatial and spectral domains and encouraging cross-space consistency through collaborative optimization. Furthermore, to produce reliable mixed pseudo labels, which enhance the effectiveness of ensemble learning, we introduce a novel adaptive pixelwise fusion technique based on the entropy guidance from the spatial and spectral branches. Our strategy efficiently mitigates the deleterious effects of uncertainty and noise present in pseudo labels and surpasses previous alternatives in terms of efficacy. Ultimately, we formulate a holistic optimization objective to learn from the hybrid supervision of</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Colorectal cancer is a leading cause of cancer-related deaths worldwide <ref type="bibr" target="#b0">[1]</ref>. Early detection and efficient diagnosis of polyps, which are precursors to colorectal cancer, is crucial for effective treatment. Recently, deep learning has emerged as a powerful tool in medical image analysis, prompting extensive research into its potential for polyp segmentation. The effectiveness of deep learning models in medical applications is usually based on large, well-annotated datasets, which in turn necessitates a time-consuming and expertise-driven annotation process. This has prompted the emergence of approaches for annotation-efficient weakly-supervised learning in the medical domain with limited annotations like points <ref type="bibr" target="#b7">[8]</ref>, bounding boxes <ref type="bibr" target="#b11">[12]</ref>, and scribbles <ref type="bibr" target="#b14">[15]</ref>. Compared with other sparse labeling methods, scribbles allow the annotator to annotate arbitrary shapes, making them more flexible than points or boxes <ref type="bibr" target="#b12">[13]</ref>. Besides, scribbles provide a more robust supervision signal, which can be prone to noise and outliers <ref type="bibr" target="#b4">[5]</ref>. Hence, this work investigates the feasibility of conducting polyp segmentation using scribble annotation as supervision. The effectiveness of medical applications during in-site deployment depends on their ability to generalize to unseen data and remain robust against data corruption. Improving these factors is crucial to enhance the accuracy and reliability of medical diagnoses in real-world scenarios <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref>. Therefore, we comprehensively evaluate our approach on multiple datasets from various medical sites to showcase its viability and effectiveness across different contexts.</p><p>Dual-branch learning has been widely adopted in annotation-efficient learning to encourage mutual consistency through co-teaching. While existing approaches are typically designed for learning in the spatial domain <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>, a novel spatial-spectral dual-branch structure is introduced to efficiently leverage domain-specific complementary knowledge with synergistic mutual teaching. Furthermore, the outputs from the spatial-spectral branches are aggregated to produce mixed pseudo labels as supplementary supervision. Different from previous methods, which generally adopt the handcrafted fusion strategies <ref type="bibr" target="#b14">[15]</ref>, we design to aggregate the outputs from spatial-spectral dual branches with an entropy-guided adaptive mixing ratio for each pixel. Consequently, our incorporated tactic of pseudo-label fusion aptly assesses the pixel-level ambiguity emerging from both spatial and frequency domains based on their entropy maps, thereby allocating substantially assured categorical labels to individual pixels and facilitating effective pseudo label ensemble learning. Contributions. Overall, the contributions of this work are threefold: First, we devise a spatial-spectral dual-branch structure to leverage cross-space knowledge and foster collaborative mutual teaching. To our best knowledge, this is the first attempt to explore the complementary relations of the spatial-spectral dual branch in boosting weakly-supervised medical image analysis. Second, we introduce the pixel-level entropy-guided fusion strategy to generate mixed pseudo labels with reduced noise and increased confidence, thus enhancing ensemble learning. Lastly, our proposed hybrid loss optimization, comprising scribblessupervised loss, mutual training loss with domain-specific pseudo labels, and ensemble learning loss with fused-domain pseudo labels, facilitates obtaining a generalizable and robust model for polyp image segmentation. An extensive assessment of our approach through the examination of four publicly accessible datasets establishes its superiority and clinical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Spectral-domain learning <ref type="bibr" target="#b26">[26]</ref> has gained increasing popularity in medical image analysis <ref type="bibr" target="#b22">[23]</ref> for its ability to identify subtle frequency patterns that may not be well detected by the pure spatial-domain network like UNet <ref type="bibr" target="#b19">[20]</ref>. For instance, a recent dual-encoder network, YNet <ref type="bibr" target="#b5">[6]</ref>, incorporates a spectral encoder with Fast Fourier Convolution (FFC) <ref type="bibr" target="#b3">[4]</ref> to disentangle global patterns across varying frequency components and derives hybrid feature representation. In addition, spectrum learning also exhibits advantageous robustness and generalization against adversarial attacks, data corruption, and distribution shifts <ref type="bibr" target="#b18">[19]</ref>. In label-efficient learning, some preliminary works have been proposed to encourage mutual consistency between outputs from two networks <ref type="bibr" target="#b2">[3]</ref>, two decoders <ref type="bibr" target="#b25">[25]</ref>, and teacher-student models <ref type="bibr" target="#b13">[14]</ref>, yet only in the spatial domain. As far as we know, spatial-spectral cross-domain consistency has never been investigated to promote learning with sparse annotations of medical data. This has motivated us to develop the cross-domain cooperative mutual teaching scheme to leverage the favorable properties when learning in the spectral space.</p><p>Besides consistency constraints, utilizing pseudo labels as supplementary supervision is another principle in label-efficient learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">24]</ref>. To prevent the model from being influenced by noise and inaccuracies within the pseudo labels, numerous studies have endeavored to enhance their quality, including averaging the model predictions from several iterations <ref type="bibr" target="#b10">[11]</ref>, filtering out unreliable pixels <ref type="bibr" target="#b24">[24]</ref>, and mixing dual-branch outputs <ref type="bibr" target="#b14">[15]</ref> following</p><formula xml:id="formula_0">p mix = α × p 1 + (1 -α) × p 2 , α = random(0, 1), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where α is the random mixing ratio. p 1 , p 2 , and p mix denote the probability maps from the two spatial decoders and their mixture. These approaches only operate in the spatial domain, regardless of single or dual branches, while we consider both spatial and spectral domains and propose to adaptively merge dual-branch outputs with respective pixel-wise entropy guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">S 2 ME: Spatial-Spectral Mutual Teaching and Ensemble Learning</head><p>Spatial-Spectral Cross-domain Mutual Teaching. In contrast to prior weakly-supervised learning methods that have merely emphasized spatial considerations, our approach designs a dual-branch structure consisting of a spatial branch f spa (x, θ spa ) and a spectral branch f spe (x, θ spe ), with x and θ being the input image and randomly initialized model parameters. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, the spatial and spectral branches take the same training image as the input and extract domain-specific patterns. The raw model outputs, i.e., the logits l spa and l spe , will be converted to probability maps p spa and p spe with Softmax normalization, and further to respective pseudo labels ŷspa and ŷspe by ŷ = arg max p. The spatial and spectral pseudo labels supervise the other branch collaboratively during mutual teaching and can be expressed as</p><formula xml:id="formula_2">ŷspa → f spe and ŷspe → f spa ,<label>(2)</label></formula><p>where "→" denotes supervision<ref type="foot" target="#foot_0">1</ref> . Through cross-domain engagement, these two branches complement each other, with each providing valuable domain-specific insights and feedback to the other. Consequently, such a scheme can lead to better feature extraction, more meaningful data representation, and domain-specific knowledge transmission, thus boosting model generalization and robustness.</p><p>Entropy-Guided Pseudo Label Ensemble Learning. In addition to mutual teaching, we consider aggregating the pseudo labels from the spatial and spectral branches in ensemble learning, aiming to take advantage of the distinctive yet complementary properties of the cross-domain features. As we know, a pixel characterized by a higher entropy value indicates elevated uncertainty in terms of its corresponding prediction. We can observe from the entropy maps H spa and H spe in Fig. <ref type="figure" target="#fig_0">1</ref> that the pixels of the polyp boundary exhibit greater difficulties in accurate segmentation, presenting with higher entropy values (the white contours). Considering such property, we propose a novel adaptive strategy to automatically adjust the mixing ratio for each pixel based on the entropy of its categorical probability distribution. Hence, the mixed pseudo labels are more reliable and beneficial for ensemble learning. Concretely, with the spatial and spectral probability maps p spa and p spe , the corresponding entropy maps H spa and H spe can be computed with</p><formula xml:id="formula_3">H = - C-1 c=0 p(c) × log p(c), (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where C is the number of classes that equals 2 in our task. Unlike previous image-level fixed-ratio mixing or random mixing as Eq. ( <ref type="formula" target="#formula_0">1</ref>), we can update the mixing ratio between the two probability maps p spa and p spe with the weighted entropy guidance at each pixel location by</p><formula xml:id="formula_5">p s 2 = H spe H spa + H spe ⊗ p spa + H spa H spa + H spe ⊗ p spe ,<label>(4)</label></formula><p>where "⊗" denotes pixel-wise multiplication. p s 2 is the merged probability map and can be further converted to the pseudo label by ŷs 2 = arg max p s 2 to supervise the spatial and spectral branch in the context of ensemble learning following ŷs 2 → f spa and ŷs 2 → f spe .</p><p>(</p><formula xml:id="formula_6">)<label>5</label></formula><p>By absorbing strengths from the spatial and spectral branches, ensemble learning from the mixed pseudo labels facilitates model optimization with reduced overfitting, increased stability, and improved generalization and robustness.</p><p>Hybrid Loss Supervision from Scribbles and Pseudo Labels. Besides the scribble annotations for partial pixels, the aforementioned three types of pseudo labels ŷspa , ŷspe , and ŷs 2 can offer complementary supervision for every pixel, with different learning regimes. Overall, our hybrid loss supervision is based on Cross Entropy loss CE and Dice loss Dice . Specifically, we employ the partial Cross Entropy loss <ref type="bibr" target="#b12">[13]</ref> pCE , which only calculates the loss on the labeled pixels, for learning from scribbles following</p><formula xml:id="formula_7">L scrib = pCE (l spa , y) + pCE (l spe , y), (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where y denotes the scribble annotations. Furthermore, the mutual teaching loss with supervision from domain-specific pseudo labels is . <ref type="bibr" target="#b7">(8)</ref> Holistically, our hybrid loss supervision can be stated as</p><formula xml:id="formula_9">L hybrid = L scrib + λ mt × L mt + λ el × L el ,<label>(9)</label></formula><p>where λ mt and λ el serve as weighting coefficients that regulate the relative significance of various modes of supervision. The hybrid loss considers all possible supervision signals in the spatial-spectral dual-branch network and exceeds partial combinations of its constituent elements, as evidenced in the ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Datasets. We employ the SUN-SEG <ref type="bibr" target="#b9">[10]</ref> dataset with scribble annotations for training and assessing the in-distribution performance. This dataset is based on the SUN database <ref type="bibr" target="#b15">[16]</ref>, which contains 100 different polyp video cases. To reduce data redundancy and memory consumption, we choose the first of every five consecutive frames in each case. We then randomly split the data into 70, 10, and 20 cases for training, validation, and testing, leaving 6677, 1240, and 1993 frames in the respective split. For out-of-distribution evaluation, we utilize three public datasets, namely Kvasir-SEG <ref type="bibr" target="#b8">[9]</ref>, CVC-ClinicDB <ref type="bibr" target="#b1">[2]</ref>, and PolypGen <ref type="bibr" target="#b0">[1]</ref> with 1000, 612, and 1537 polyp frames, respectively. These datasets are collected from diversified patients in multiple medical centers with various data acquisition systems. Varying data shifts and corruption like motion blur and specular reflections<ref type="foot" target="#foot_1">2</ref> pose significant challenges to model generalization and robustness.</p><p>Implementation Details. We implement our method with PyTorch <ref type="bibr" target="#b17">[18]</ref> and run the experiments on a single NVIDIA RTX3090 GPU. The SGD optimizer is utilized for training 30k iterations with a momentum of 0.9, a weight decay of 0.0001, and a batch size of 16. The execution time for each experiment is approximately 4 h. The initial learning rate is 0.03 and updated with the polyscheduling policy <ref type="bibr" target="#b14">[15]</ref>. The loss weighting coefficients λ mt and λ el are empirically set the same and exponentially ramped up <ref type="bibr" target="#b2">[3]</ref> from 0 to 5 in 25k iterations. All the images are randomly cropped at the border with maximally 7 pixels and resized to 224×224 in width and height. Besides, random horizontal and vertical flipping are applied with a probability of 0.5, respectively. We utilize UNet <ref type="bibr" target="#b19">[20]</ref> and YNet <ref type="bibr" target="#b5">[6]</ref> as the respective segmentation model in the spatial and spectral branches. The performance of the scribble-supervised model with partial Cross Entropy <ref type="bibr" target="#b12">[13]</ref> loss (Scrib-pCE) and the fully-supervised model with Cross Entropy loss (Fully-CE) are treated as the lower and upper bound, respectively. Five classical and relevant methods, including EntMin <ref type="bibr" target="#b6">[7]</ref>, GCRF <ref type="bibr" target="#b16">[17]</ref>, USTM <ref type="bibr" target="#b13">[14]</ref>, CPS <ref type="bibr" target="#b2">[3]</ref>, and DMPLS <ref type="bibr" target="#b14">[15]</ref> are employed as the comparative baselines and implemented with UNet <ref type="bibr" target="#b19">[20]</ref> as the segmentation backbone referring to the WSL4MIS<ref type="foot" target="#foot_2">3</ref> repository. For a fair comparison, the output from the spatial branch is taken as the final prediction and utilized in evaluation without post-processing. In addition, statistical evaluations are conducted with multiple seeds, and the mean and standard deviations of the results are reported.  <ref type="bibr" target="#b16">[17]</ref> 0.656±0.019 0.541±0.022 0.690±0.017 4.983±0.089 USTM <ref type="bibr" target="#b13">[14]</ref> 0.654±0.008 0.533±0.009 0.663±0.011 5.207±0.138 CPS <ref type="bibr" target="#b2">[3]</ref> 0.658±0.004 0.539±0.005 0.676±0.005 5.092±0.063 DMPLS <ref type="bibr" target="#b14">[15]</ref> 0.656±0.006 0.539±0.005 0.659±0.011 5.208±0.061 S 2 ME (Ours) 0.674±0.003 0.565±0.001 0.719±0.003 4.583±0.014</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Analysis</head><p>Fully-CE 0.713±0.021 0.617±0.023 0.746±0.027 4.405±0.119</p><p>The performance of weakly-supervised methods is assessed with four metrics,i.e., Dice Similarity Coefficient (DSC), Intersection over Union (IoU), Precision (Prec), and a distance-based measure of Hausdorff Distance (HD). As shown in Table <ref type="table" target="#tab_1">1</ref> and Fig. <ref type="figure" target="#fig_1">2</ref>, our S 2 ME achieves superior in-distribution performance quantitatively and qualitatively compared with other baselines on the SUN-SEG <ref type="bibr" target="#b9">[10]</ref> dataset. Regarding generalization and robustness, as indicated in Table <ref type="table" target="#tab_2">2</ref>, our method outperforms other weakly-supervised methods by a significant margin on three unseen datasets, and even exceeds the fully-supervised upper bound on two of them <ref type="foot" target="#foot_3">4</ref> . These results suggest the efficacy and reliability of the proposed solution S 2 ME in fulfilling polyp segmentation tasks with only scribble annotations. Notably, the encouraging performance on unseen datasets exhibits promising clinical implications in deploying our method to real-world scenarios.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies</head><p>Network Structures. We first conduct the ablation analysis on the network components. As shown in Table <ref type="table" target="#tab_4">3</ref>, the spatial-spectral configuration of our S 2 ME yields superior performance compared to single-domain counterparts with ME, confirming the significance of utilizing cross-domain features. Pseudo Label Fusion Strategies. To ensure the reliability of the mixed pseudo labels for ensemble learning, we present the pixel-level adaptive fusion strategy according to entropy maps of dual predictions to balance the strengths and weaknesses of spatial and spectral branches. As demonstrated in Table <ref type="table" target="#tab_5">4</ref>, our method achieves improved performance compared to two image-level fusion strategies, i.e., random <ref type="bibr" target="#b14">[15]</ref> and equal mixing.</p><p>Hybrid Loss Supervision. We decompose the proposed hybrid loss L hybrid in Eq. ( <ref type="formula" target="#formula_9">9</ref>) to demonstrate the effectiveness of holistic supervision from scribbles, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>To our best knowledge, we propose the first spatial-spectral dual-branch network structure for weakly-supervised medical image segmentation that efficiently leverages cross-domain patterns with collaborative mutual teaching and ensemble learning. Our pixel-level entropy-guided fusion strategy advances the reliability of the aggregated pseudo labels, which provides valuable supplementary supervision signals. Moreover, we optimize the segmentation model with the hybrid mode of loss supervision from scribbles and pseudo labels in a holistic manner and witness improved outcomes. With extensive in-domain and out-ofdomain evaluation on four public datasets, our method shows superior accuracy, generalization, and robustness, indicating its clinical significance in alleviating data-related issues such as data shift and corruption which are commonly encountered in the medical field. Future efforts can be paid to apply our approach to other annotation-efficient learning contexts like semi-supervised learning, other sparse annotations like points, and more medical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of our Spatial-Spectral Dual-branch Mutual Teaching and Pixel-level Entropy-guided Pseudo Label Ensemble Learning (S 2 ME) for scribble-supervised polyp segmentation. Spatial-spectral cross-domain consistency is encouraged through mutual teaching. High-quality mixed pseudo labels are generated with pixel-level guidance from the dual-space entropy maps, ensuring more reliable supervision for ensemble learning.</figDesc><graphic coords="3,70,98,53,72,310,90,130,99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Qualitative performance comparison of one camouflaged polyp image with DSC values on the left top. The contour of the ground-truth mask is displayed in black, in comparison with that of each method shown in different colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Likewise, the ensemble learning loss with supervision from the enhanced mixed pseudo labels can be formulated asL el = { CE (lspa, ŷs 2 ) + Dice(pspa, ŷs 2 )} ŷs 2 →fspa+ { CE (lspe, ŷs 2 ) + Dice(pspe, ŷs 2 )}</figDesc><table><row><cell>ŷs 2 →fspe</cell></row></table><note><p><p>Lmt = { CE (lspa, ŷspe)+ Dice(pspa, ŷspe)} ŷspe→fspa + { CE (lspe, ŷspa)+ Dice(pspe, ŷspa)} ŷspa→fspe .</p><ref type="bibr" target="#b6">(7)</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison of the in-distribution segmentation performance. The shaded grey and blue rows are the lower and upper bound. The best results of the scribble-supervised methods are in bold.</figDesc><table><row><cell>Method</cell><cell>DSC ↑</cell><cell>SUN-SEG [10] IoU ↑ Prec ↑</cell><cell>HD ↓</cell></row><row><cell cols="4">Scrib-pCE [13] 0.633±0.010 0.511±0.012 0.636±0.021 5.587±0.149</cell></row><row><cell>EntMin [7]</cell><cell cols="3">0.642±0.012 0.519±0.013 0.666±0.016 5.277±0.063</cell></row><row><cell>GCRF</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Generalization comparison on three unseen datasets. The underlined results surpass the upper bound.</figDesc><table><row><cell>Method</cell><cell>Kvasir-SEG [9] DSC ↑ HD ↓</cell><cell>CVC-ClinicDB [2] DSC ↑ HD ↓</cell><cell>PolypGen [1] DSC ↑ HD ↓</cell></row><row><cell cols="4">Scrib-pCE [13] 0.679±0.010 6.565±0.173 0.573±0.016 6.497±0.156 0.524±0.012 6.084±0.189</cell></row><row><cell>EntMin [7]</cell><cell cols="3">0.684±0.004 6.383±0.110 0.578±0.016 6.308±0.254 0.542±0.003 5.887±0.063</cell></row><row><cell>GCRF [17]</cell><cell cols="3">0.702±0.004 6.024±0.014 0.558±0.008 6.192±0.290 0.530±0.006 5.714±0.133</cell></row><row><cell>USTM [14]</cell><cell cols="3">0.693±0.005 6.398±0.138 0.587±0.019 5.950±0.107 0.538±0.007 5.874±0.068</cell></row><row><cell>CPS [3]</cell><cell cols="3">0.703±0.011 6.323±0.062 0.591±0.017 6.161±0.074 0.546±0.013 5.844±0.065</cell></row><row><cell>DMPLS [15]</cell><cell cols="3">0.707±0.006 6.297±0.077 0.593±0.013 6.194±0.028 0.547±0.007 5.897±0.045</cell></row><row><cell>S 2 ME</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>(Ours) 0.750±0.003 5.449±0.150 0.632±0.010 5.633±0.008 0.571±0.002 5.247±0.107</head><label></label><figDesc></figDesc><table><row><cell>Fully-CE</cell><cell>0.758±0.013 5.414±0.097 0.631±0.026 6.017±0.349 0.569±0.016 5.252±0.128</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation comparison of dual-branch network architectures. Results are from outputs of Model-1 on the SUN-SEG<ref type="bibr" target="#b9">[10]</ref> dataset.</figDesc><table><row><cell>Model-1 Model-2 Method</cell><cell>DSC ↑</cell><cell>IoU ↑</cell><cell>Prec ↑</cell><cell>HD ↓</cell></row><row><cell>UNet [20] UNet [20] ME (Ours)</cell><cell cols="4">0.666 ± 0.002 0.557 ± 0.002 0.715 ± 0.008 4.684 ± 0.034</cell></row><row><cell>YNet [6] YNet [6]</cell><cell cols="4">0.648 ± 0.004 0.538 ± 0.005 0.695 ± 0.004 4.743 ± 0.006</cell></row><row><cell cols="5">UNet [20] YNet [6] S 2 ME (Ours) 0.674 ± 0.003 0.565 ± 0.001 0.719 ± 0.003 4.583 ± 0.014</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation on the pseudo label fusion strategies on the SUN-SEG<ref type="bibr" target="#b9">[10]</ref> dataset.</figDesc><table><row><cell>Fusion</cell><cell>Metrics</cell><cell></cell></row><row><cell>Strategy</cell><cell>Level DSC ↑</cell><cell>HD ↓</cell></row><row><cell>Random [15]</cell><cell cols="2">Image 0.665 ± 0.008 4.750 ± 0.169</cell></row><row><cell>Equal (0.5)</cell><cell cols="2">Image 0.667 ± 0.001 4.602 ± 0.013</cell></row><row><cell cols="3">Entropy (Ours) Pixel 0.674 ± 0.003 4.583 ± 0.014</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on the loss components on the SUN-SEG<ref type="bibr" target="#b9">[10]</ref> dataset.mutual teaching, and ensemble learning. As shown in Table5, our proposed hybrid loss, involving L scrib , L mt , and L el , achieves the optimal results.</figDesc><table><row><cell>Loss</cell><cell></cell><cell></cell><cell>Metrics</cell></row><row><cell cols="4">L scrib L mt L el DSC ↑</cell><cell>HD ↓</cell></row><row><cell>✓</cell><cell>✗</cell><cell>✗</cell><cell cols="2">0.627 ± 0.004 5.580 ± 0.112</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✗</cell><cell cols="2">0.668 ± 0.007 4.782 ± 0.020</cell></row><row><cell>✓</cell><cell>✗</cell><cell>✓</cell><cell cols="2">0.662 ± 0.004 4.797 ± 0.146</cell></row><row><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell cols="2">0.674 ± 0.003 4.583 ± 0.014</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For convenience, we omit the input x and model parameters θ.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Some exemplary polyp frames are presented in the supplementary materials.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/HiLab-git/WSL4MIS.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Complete results of all four metrics are present in the supplementary materials.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>. This work was supported by <rs type="funder">Hong Kong Research Grants Council (RGC) Collaborative Research Fund</rs> (<rs type="grantNumber">CRF C4063-18G</rs>), the <rs type="funder">Shun Hing Institute of Advanced Engineering</rs> (<rs type="projectName">SHIAE</rs> project <rs type="grantNumber">BME-p1-21</rs>) at the <rs type="affiliation">Chinese University of Hong Kong (CUHK</rs>), <rs type="funder">General Research Fund</rs> (<rs type="grantNumber">GRF 14203323</rs>), <rs type="funder">Shenzhen-Hong Kong-Macau Technology Research Programme (Type C) STIC</rs> Grant <rs type="grantNumber">SGDX20210823103535014 (202108233000303</rs>), and (GRS) #<rs type="grantNumber">3110167</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xWNgZzB">
					<idno type="grant-number">CRF C4063-18G</idno>
				</org>
				<org type="funded-project" xml:id="_4dBh88H">
					<idno type="grant-number">BME-p1-21</idno>
					<orgName type="project" subtype="full">SHIAE</orgName>
				</org>
				<org type="funding" xml:id="_NQDAn59">
					<idno type="grant-number">GRF 14203323</idno>
				</org>
				<org type="funding" xml:id="_XZNAszg">
					<idno type="grant-number">SGDX20210823103535014 (202108233000303</idno>
				</org>
				<org type="funding" xml:id="_aqCw52C">
					<idno type="grant-number">3110167</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">a multi-centre polyp detection and segmentation dataset for generalisability assessment</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">75</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">WM-DOVA maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fernández-Esparrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Med. Imaging Graph</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast Fourier convolution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4479" to="4488" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="203" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Y-net: a spatiospectral dualencoder network for medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeganeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehlbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention -MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13432</biblScope>
			<biblScope unit="page" from="582" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Intra-and inter-slice contrastive learning for point supervised oct fluid segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1870" to="1881" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kvasir-SEG: a segmented polyp dataset</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-37734-2_37</idno>
		<idno>978-3-030-37734-2 37</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MMM 2020</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11962</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video polyp segmentation: a deep learning perspective</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Intell. Res</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scribble2Label: scribble-supervised cell segmentation via self-generating pseudo-labels with consistency</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Jeong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_2</idno>
		<idno>978-3-030-59710-8 2</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MIC-CAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptive box-supervised instance segmentation network for mitosis detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2469" to="2485" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scribblesup: scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Weakly supervised segmentation of covid19 infection with scribble annotation on CT images. Pattern Recogn</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page">108341</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scribble-supervised medical image segmentation via dual-branch network and dynamically mixed pseudo labels supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16431-6_50</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16431-650" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13431</biblScope>
			<biblScope unit="page" from="528" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Development of a computer-aided detection system for colonoscopy and a publicly accessible large colonoscopy video database (with video)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Misawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastrointest. Endosc</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="960" to="967" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gated CRF loss for weakly supervised semantic image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04651</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pyTorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<editor>NIPS-W</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Global filter networks for image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="980" to="993" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">U-net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to segment from scribbles using multi-scale adversarial attention gates</title>
		<author>
			<persName><forename type="first">G</forename><surname>Valvano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1990">1990-2001 (2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking surgical instrument segmentation: a background image can be all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16449-1_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16449-134" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13437</biblScope>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ffcnet: Fourier transform-based frequency learning and complex convolutional network for colon disease classification</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">13433</biblScope>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-88" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Freematch: self-adaptive thresholding for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.07246</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised left atrium segmentation with mutual consistency training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-328" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning in the frequency domain</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1740" to="1749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Class-incremental domain adaptation with smoothing and calibration for surgical report generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_26</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-126" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning domain adaptation with model calibration for surgical report generation in robotic surgery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12350" to="12356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cyclemix: a holistic strategy for medical image segmentation from scribble supervision</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11656" to="11665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ShapePU: a new PU learning framework regularized by global consistency for scribble supervised cardiac segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_16</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-116" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="162" to="172" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
