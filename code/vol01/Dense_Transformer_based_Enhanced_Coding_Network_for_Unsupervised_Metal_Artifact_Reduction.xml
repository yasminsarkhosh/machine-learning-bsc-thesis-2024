<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction</title>
				<funder ref="#_fdeBX4F">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wangduo</forename><surname>Xie</surname></persName>
							<email>wangduo.xie@esat.kuleuven.be</email>
							<idno type="ORCID">0009-0004-4510-3502</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Processing Speech and Images</orgName>
								<orgName type="department" key="dep2">Department of ESAT</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
							<email>matthew.blaschko@esat.kuleuven.be</email>
							<idno type="ORCID">0000-0002-2640-181X</idno>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Processing Speech and Images</orgName>
								<orgName type="department" key="dep2">Department of ESAT</orgName>
								<orgName type="institution">KU Leuven</orgName>
								<address>
									<settlement>Leuven</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="77" to="86"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">7DDF39E69AB55162CC072741E1B10EB1</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Metal artifact reduction</term>
					<term>CT image restoration</term>
					<term>Unsupervised learning</term>
					<term>Enhanced coding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>CT images corrupted by metal artifacts have serious negative effects on clinical diagnosis. Considering the difficulty of collecting paired data with ground truth in clinical settings, unsupervised methods for metal artifact reduction are of high interest. However, it is difficult for previous unsupervised methods to retain structural information from CT images while handling the non-local characteristics of metal artifacts. To address these challenges, we proposed a novel Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal artifact reduction. Specifically, we introduce a Hierarchical Disentangling Encoder, supported by the high-order dense process, and transformer to obtain densely encoded sequences with long-range correspondence. Then, we present a second-order disentanglement method to improve the dense sequence's decoding process. Extensive experiments and model discussions illustrate DTEC-Net's effectiveness, which outperforms the previous state-of-the-art methods on a benchmark dataset, and greatly reduces metal artifacts while restoring richer texture details.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>CT technology can recover the internal details of the human body in a noninvasive way and has been widely used in clinical practice. However, if there is metal in the tissue, metal artifacts (MA) will appear in the reconstructed CT image, which will corrupt the image and affect the medical diagnosis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>In light of the clinical need for MA reduction, various traditional methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref> have been proposed to solve the problem by using interpolation and iterative optimization. As machine learning research increasingly impacts medical imaging, deep learning based methods have been proposed for MA reduction. Specifically, these methods can be roughly divided into supervised and unsupervised categories according to the degree of supervision. In the supervised category, the methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> based on the dual domain (sinogram and image domains) can achieve good performance for MA reduction. However, supervised learning methods are hindered by the lack of large-scale real-world data pairs consisting of "images with MA" and "images without MA" representing the same region. The lack of such data can lead algorithms trained on synthetic data to over-fit simulated data pairs, resulting in difficulties in generalizing to clinical settings <ref type="bibr" target="#b10">[11]</ref>. Furthermore, although sinogram data can bring additional information, it is difficult to collect in realistic settings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>. Therefore, unsupervised methods based only on the image domain are strongly needed in practice.</p><p>For unsupervised methods in the image domain, Liao et al. <ref type="bibr" target="#b7">[8]</ref> used Generative Adversarial Networks (GANs) <ref type="bibr" target="#b1">[2]</ref> to disentangle the MA from the underlying clean structure of the artifact-affected image in latent space by using unpaired data with and without MA. Although the method can separate the artifact component in the latent space, the features from the latent space can't represent rich low-level information of the original input. Further, it's also hard for the encoder to represent long-range correspondence across different regions. Accordingly, the restored image loses texture details and can't retain structure from the CT image. In the same unsupervised setting, Lyu et al. <ref type="bibr" target="#b10">[11]</ref> directly separate the MA component and clean structure in image space using a CycleGAN-based method <ref type="bibr" target="#b25">[25]</ref>. Although implementation in the image space makes it possible to construct dual constraints, directly operating in the image space affects the algorithm's performance upper limit, because it is difficult to encode in the image space as much low-level information as the feature space.</p><p>Considering the importance of low-level features in the latent space for generating the artifact-free component, we propose a novel Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal artifact reduction, which can obtain low-level features with hierarchical information and map them to a clean image space through adversarial training. DTEC-Net contains our developed Hierarchical Disentangling Encoder (HDE), which utilizes longrange correspondences obtained by a lightweight transformer and a high-order dense process to produce the enhanced coded sequence. To ease the burden of decoding the sequence, we also propose a second-order disentanglement method to finish the sequence decomposition. Extensive empirical results show that our method can not only reduce the MA greatly and generate high-quality images, but also surpasses the competing unsupervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We design a Hierarchical Disentangling Encoder(HDE) that can capture lowlevel sequences and enable high-performance restoration. Moreover, to reduce the burden of the decoder group brought by the complicated sequences, we propose a second-order disentanglement mechanism. The intuition is shown in Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hierarchical Disentangling Encoder (HDE)</head><p>As shown in Fig. <ref type="figure">2</ref>(a), the generator of DTEC-Net consists of three encoders and four decoders. We design the HDE to play the role of Encoder1 for enhanced  <ref type="bibr" target="#b7">[8]</ref>. The data relationship is shown in <ref type="bibr" target="#b7">[8]</ref>. In addition to the difference in disentanglement, DTEC-Net and ADN also have different inner structures.</p><p>coding. Specifically, for the HDE's input image x a ∈ R 1×H×W with MA, HDE first uses a convolution for the preliminary feature extraction and produces a high-dimensional tensor x l0 with c channels. Then, x l0 will be encoded by three Dense Transformers for Disentanglement (DTDs) in a first-order reuse manner <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">23]</ref>. Specifically, the output x li of the ith DTD can be characterized as:</p><formula xml:id="formula_0">x li = f DTDi (f s-hde (cat(x li-1 , x li-2 ..., , x l0 ))), i = 2, ..., N. f DTDi (x li-1 ), i = 1.<label>(1)</label></formula><p>In Eq. (1), f s-hde represents the channel compression of the concatenation of multiple DTDs' outputs, and N represents the total number of DTDs in the HDE. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, HDE can obtain the hierarchical information sequence X l {x l0 , ...x lN } and high-level semantic features x h x lN .</p><p>As shown in Fig. <ref type="figure">2</ref>(b), Encoder1 of ADN cannot characterize upstream low-level information, and results in limited performance. By using HDE, the upstream of the DTEC-Net's Encoder1 can represent rich low-level information, and be encoded in the efficient way described in Eq. ( <ref type="formula" target="#formula_0">1</ref>). After generating the enhanced coding sequences X l with long-range correspondence and densely reused information, DTEC-Net can decode it back to the clean image domain by using the proposed second-order disentanglement for MA reduction, which reduces the decoder group's burden to a large extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dense Transformer for Disentanglement (DTD)</head><p>In addition to the first-order feature multiplexing given in Eq. ( <ref type="formula" target="#formula_0">1</ref>), HDE also uses the DTD to enable second-order feature reuse. The relationship between HDE and DTD is shown in Fig. <ref type="figure" target="#fig_1">3</ref>. Inspired by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>, DTD first uses a lightweight transformer based on the Swin transformer <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> to represent content-based information with long-range correspondence inside of every partition window. It then performs in-depth extraction and second-order reuse.</p><p>Specifically, the input x 1 ∈ R C×H×W of the DTD will be processed sequentially by the lightweight transformer and groups of convolutions in the form of second-order dense connections. The output x j+1 of the jth convolution with ReLU, which is connected in a second-order dense pattern, can be expressed as:</p><formula xml:id="formula_1">x j+1 = f cj (cat(x 1 , x 2 , ..., x j )), j = 2, 3, ..., J. f cj (f transformer-light (x j )), j = 1.<label>(2)</label></formula><p>In Eq. ( <ref type="formula" target="#formula_1">2</ref>), f cj indicates the jth convolution with ReLU after the lightweight transformer, and the J indicates the total number of convolutions after the lightweight transformer and is empirically set to six. The dense connection method can effectively reuse low-level features <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">23]</ref> so that the latent space including these type of features will help the decoder to restore clean images without metal artifacts. Because the low-level information on different channels has different importance to the final restoration task, we use the channel attention mechanism <ref type="bibr" target="#b2">[3]</ref> to filter the output of the final convolution layer:</p><formula xml:id="formula_2">x out = x J+1 f MLP (f pooling (x 1 )) + x 1 ,<label>(3)</label></formula><p>where represents the Hadamard product, f MLP indicates a multi-layer perceptron with only one hidden layer, and f pooling represents global pooling.</p><p>Because the transformer usually requires a large amount of data for training and CT image datasets are usually smaller than those for natural images, we do lightweight processing for the Swin transformer. Specifically, for an input tensor x ∈ R C×H×W of the lightweight transformer, the number of channels will be reduced from C to C in to lighten the burden of the attention matrix. Then, a residual block is employed to extract information with low redundancy.</p><p>After completing lightweight handling, the tensor will first be partitioned into multiple local windows and flattened to x in ∈ R ( HW P 2 )×P 2 ×Cin according the pre-operation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref> of the Swin transformer. P × P represents the window size for partitioning as shown in Fig. <ref type="figure">1(b)</ref>. Then, the attention matrix belonging to the ith window can be calculated by pairwise multiplication between converted vectors in S i {x in (i, j, :)|j = 0, 1, ..., P 2 -1}. Specifically, by using a linear map from R Cin to R Ca for every vector in S i , the query key and value: Q, K, V ∈ R P 2 ×Ca can be derived. Afterwards, the attention matrix for each window can be obtained by the following formula <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_3">Attention(Q, K, V ) = SoftMax(QK T / √ C a )V.<label>(4)</label></formula><p>In actual operation, we use window-based multi-head attention (MSA) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref> to replace the single-head attention because of the performance improvement <ref type="bibr" target="#b6">[7]</ref>. The output of the Swin transformer layer will be unflattened and operated by post processing (POP) which consists of a classic convolution and layer norm (LN) with flatten and unflatten operations. After POP, the lightweight tensor with fewer channels will be unsqueezed to expand the channels, and finally added to the original input x in the form of residuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Second-Order Disentanglement for MA Reduction (SOD-MAR)</head><p>As mentioned in Sect. 2.1, X l represents the hierarchical sequence and facilitates the generator's representation. However, X l needs to be decoded by a highcapacity decoder to match the encoder. Considering that Decoder2 does not directly participate in the restoration branch and already loaded up the complicated artifact part x m in traditional first-order disentanglement learning <ref type="bibr" target="#b7">[8]</ref>, to reduce the burden of the decoder group, we propose and analyze SOD-MAR. Specifically, Decoder2 of DTEC-Net doesn't decode sequence X l , it only decodes the combination of second-order disentangled information x h ∈ X l and the latent feature x m representing the artifact parts shown in Fig. <ref type="figure">2</ref> to complete the process, Decoder2 uses the structure shown in Fig. <ref type="figure" target="#fig_2">4</ref> to finish the decoding step, which is also used by Decoder1 to decode the sequence X l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a). In order</head><p>Moreover, we don't only map the x h into Decoder1 and Decoder2 while dropping the X l \{x h } to implement the burden reduction, because the low-level information in X l \{x h } is vital for restoring artifact-free images. Furthermore, x h will be disturbed by noise from the approaching target x a of Decoder2 while information X l \ {x h } upstream from the HDE can counteract the noise disturbance to a certain extent. The reason behind the counteraction is that the update to upstream parameters is not as large as that of the downstream parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Loss Function</head><p>Following <ref type="bibr" target="#b7">[8]</ref>, we use discriminators D 0 , D 1 to constrain the output x c and y a :</p><formula xml:id="formula_4">L adv = E[log(1 -D 0 (x c )) + log(D 0 (y c ))] + E[log(D 1 (x a )) + log(1 -D 1 (y a ))].</formula><p>(5) The above x a , y c represent the input as shown in Fig. <ref type="figure">2(a)</ref>. Following <ref type="bibr" target="#b7">[8]</ref>, we use the reconstruction loss L rec to constrain the identity map, and also use the artifact consistency loss L art and self-reduction loss L self to control the optimization process. The coefficients for each of these losses are set as in <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Results</head><p>Synthesized DeepLesion Dataset. Following <ref type="bibr" target="#b7">[8]</ref>, we randomly select 4186 images from DeepLesion <ref type="bibr" target="#b16">[17]</ref> and 100 metal templates <ref type="bibr" target="#b20">[21]</ref> to build a dataset. The simulation is consistent with <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>. For training, we randomly select 3986 images from DeepLesion combined with 90 metal templates for simulation. The 3986 images will be divided to two disjoint image sets with and without MA after simulation. Then a random combination can form the physically unpaired data with and without MA in the training process. Besides, another 200 images combined with the remaining 10 metal templates are used for the testing process.</p><p>Real Clinic Dataset. We randomly combine 6165 artifacts-affected images and 20729 artifacts-free images from SpineWeb<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b7">[8]</ref> for training, and 105 artifactsaffected images from SpineWeb for testing. Implementation Details. We use peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) to measure performance. We use mean squared error (MSE) only for measuring ablation experiments. For Synthesized DeepLesion dataset (and Real Clinic dataset), we set the batch size to 2 (and 2) and trained the network for 77 (and 60) epochs using the Adam optimizer. Our DTEC-Net was implemented in Pytorch using an Nvidia Tesla P100. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ablation Study</head><p>To verify the effectiveness of the proposed methods, ablation experiments were carried out on Synthesized DeepLesion. The results are shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The Impact of DTD in HDE. In this experiment, we change the encoding ability of HDE by changing the number of DTDs. We first use only one DTD to build the HDE, then the PSNR is 0.65 dB lower than our DTEC-Net using three DTDs. Additionally, the average MSE in this case is much higher than DTEC-Net. When the number of DTDs increases to two, the performance improves by 0.25 dB and is already better than the SOTA method <ref type="bibr" target="#b10">[11]</ref>. As we further increase the number of DTDs to three, the PSNR and SSIM increase 0.4 dB and 0.003, respectively. The number of DTDs is finally set to three in a trade-off between computation and performance. To match different encoders and decoders and facilitate training, we also adjust the accept headers of Decoder1 to adapt to the sequence length determined by the different numbers of DTDs.</p><p>Only Transformer in DTD. Although the transformer can obtain better longrange correspondence than convolutions, it lacks the multiplexing of low-level information. For every DTD in DTEC-Net, we delete the second-order feature reuse pattern and only keep the lightweight transformer, the degraded version's results are 0.8 dB lower than our DTEC-Net. At the same time, great instability appears in generative adversarial training. So, only using the transformer cannot achieve good results in reducing metal artifacts.</p><p>Removing SOD-MAR. Although SOD-MAR mainly helps by easing the burden of decoding as discussed in Sect. 2.3, it also has a performance gain compared to first-order disentanglement. We delete the SOD-MAR in DTEC-Net and let x h be the unique feature decoded by Decoder1. The Performance is 0.2 dB lower than our DTEC-Net, while MSE increases by 1.47.  Unsupervised RCN <ref type="bibr" target="#b24">[24]</ref> 32.98 <ref type="bibr" target="#b10">[11]</ref> 0.918 <ref type="bibr" target="#b10">[11]</ref> Unsupervised ADN <ref type="bibr" target="#b7">[8]</ref> 33.60 <ref type="bibr" target="#b7">[8]</ref> 0.924 <ref type="bibr" target="#b7">[8]</ref> Unsupervised U-DuDoNet <ref type="bibr" target="#b10">[11]</ref> 34.54 <ref type="bibr" target="#b10">[11]</ref> 0.934 <ref type="bibr" target="#b10">[11]</ref> Unsupervised DTEC-Net(Ours) 35.11 0.941</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison to State-of-the-Art (SOTA)</head><p>For a fair comparison, we mainly compare with SOTA methods under unsupervised settings: ADN <ref type="bibr" target="#b7">[8]</ref>, U-DuDoNet <ref type="bibr" target="#b10">[11]</ref>, RCN <ref type="bibr" target="#b24">[24]</ref>, and CycleGAN <ref type="bibr" target="#b25">[25]</ref>. We also compare with the traditional method LI <ref type="bibr" target="#b4">[5]</ref> and classical supervised method CNNMAR <ref type="bibr" target="#b20">[21]</ref>. The quantitative results of ADN, CycleGAN, CNNMAR and LI are taken from <ref type="bibr" target="#b7">[8]</ref>, the results of U-DuDoNet and RCN are taken from <ref type="bibr" target="#b10">[11]</ref>.</p><p>Because ADN has open-source code, we run their code for qualitative results.</p><p>Quantitative Results. As shown in Table <ref type="table" target="#tab_1">2</ref>. For the Synthesized DeepLesion Dataset, our method has the highest PSNR and SSIM value and outperforms the baseline ADN by 1.51 dB in PSNR and 0.017 in SSIM. At the same time, it also exceeds the SOTA method U-DuDoNet by 0.57 dB. For the Real Clinic Dataset, the numerical results can't be calculated because the ground truth does not exist. We will present the qualitative results in the appendix. Furthermore, as our work is single-domain based, it has the potential to be easily applied in clinical practice.</p><p>Qualitative Results. A visual comparison is shown in Fig. <ref type="figure" target="#fig_3">5</ref>. Our method not only reduces artifacts to a large extent, but also has sharper edges and richer textures than the compared method. More results are shown in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we proposed a Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal-artifact reduction. In DTEC-Net, we developed a Hierarchical Disentangling Encoder (HDE) to represent longrange correspondence and produce an enhanced coding sequence. By using this sequence, the DTEC-Net can better recover low-level characteristics. In addition, to decrease the burden of decoding, we specifically design a Second-order Disentanglement for MA Reduction (SOD-MAR) to finish the sequence decomposition. The extensive quantitative and qualitative experiments demonstrate our DTEC-Net's effectiveness and show it outperforms other SOTA methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. (a) CT image with metal artifacts. (b) Blue/Orange arrows: Reuse of low-level features/Long-range correspondence. (c) Output of our DTEC-Net. (d) Ground truth. (Color figure online)</figDesc><graphic coords="3,62,97,54,08,326,56,90,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The architecture of Hierarchical Disentangling Encoder (HDE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The architecture of Decoder1 (Decoder2). (xm * ) represents the Decoder2 case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Visual comparison(Metal implants are colored in red. The bottom values represent PSNR/SSIM). Our method has sharper edges and richer textures than ADN. (Color figure online)</figDesc><graphic coords="8,49,29,54,11,325,72,136,60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study on Synthesized DeepLesion under different settings. ↑: Higher value is better; ↓: Lower value is better. The best values are in bold.</figDesc><table><row><cell>Model</cell><cell cols="2">PSNR↑ SSIM↑ MSE↓</cell></row><row><cell>HDE with one DTD</cell><cell>34.46</cell><cell>0.937 27.12</cell></row><row><cell>HDE with two DTD</cell><cell>34.71</cell><cell>0.938 24.96</cell></row><row><cell>HDE with three DTD (only Transformer)</cell><cell>34.31</cell><cell>0.936 27.40</cell></row><row><cell>HDE with three DTD (without SOD-MAR)</cell><cell>34.91</cell><cell>0.940 24.36</cell></row><row><cell cols="3">HDE with three DTD (with SOD-MAR, Ours) 35.11 0.941 22.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative comparison of different methods on Synthesized DeepLesion. The best results are in bold.</figDesc><table><row><cell cols="2">Method Classification Method</cell><cell>PSNR↑</cell><cell>SSIM↑</cell></row><row><cell>Conventional</cell><cell>LI [5]</cell><cell cols="2">32.00 [8] 0.910 [8]</cell></row><row><cell>Supervised</cell><cell>CNNMAR [21]</cell><cell cols="2">32.50 [8] 0.914 [8]</cell></row><row><cell>Unsupervised</cell><cell>CycleGAN [25]</cell><cell cols="2">30.80 [8] 0.729 [8]</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>spineweb.digitalimaginggroup.ca.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research work was undertaken in the context of <rs type="programName">Horizon 2020 MSCA ETN project "xCTing" (Project ID</rs>: <rs type="grantNumber">956172</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fdeBX4F">
					<idno type="grant-number">956172</idno>
					<orgName type="program" subtype="full">Horizon 2020 MSCA ETN project &quot;xCTing&quot; (Project ID</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_8.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Artifacts in CT: recognition and avoidance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Keat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiographics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1679" to="1691" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reduction of CT artifacts caused by metallic implants</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Kalender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ebersberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="576" to="577" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Suppression of metal artifacts in CT using a reconstruction procedure that combines map and projection completion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lemmens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Faul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nuyts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="250" to="260" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SwinIR: image restoration using swin transformer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ADN: artifact disentanglement network for unsupervised metal artifact reduction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="634" to="643" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dudonet: dual domain network for CT metal artifact reduction</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10512" to="10521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Swin transformer: hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">U-DuDoNet: unpaired dual-domain network for CT metal artifact reduction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_29</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_29" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="296" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Normalized metal artifact reduction (NMAR) in computed tomography</title>
		<author>
			<persName><forename type="first">E</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raupach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kachelrieß</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5482" to="5493" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Indudonet+: a deep unfolding dual domain network for metal artifact reduction in CT images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">102729</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Orientation-shared convolution representation for CT metal artifact learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16446-0_63</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16446-0_63" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13436</biblScope>
			<biblScope unit="page" from="665" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual-domain adaptive-scaling non-local network for CT metal artifact reduction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-1_24" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12906</biblScope>
			<biblScope unit="page" from="243" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeplesion: automated mining of largescale lesion annotations and universal lesion detection with deep learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="36501" to="036501" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep sinogram completion with image prior for metal artifact reduction in CT images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="228" to="238" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Iterative metal artifact reduction for x-ray computed tomography using unmatched projector/backprojector pairs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6Part1</biblScope>
			<biblScope unit="page" from="3019" to="3033" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.01427</idno>
		<title level="m">Accurate image restoration with attention retractable transformer</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural network based metal artifact reduction in x-ray computed tomography</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1370" to="1381" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11211</biblScope>
			<biblScope unit="page" from="294" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01234-2_18" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised reused convolutional network for metal artifact reduction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-63820-7_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-63820-7_67" />
	</analytic>
	<monogr>
		<title level="m">ICONIP 2020</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Pasupa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Leung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">-S</forename><surname>Kwok</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>King</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1332</biblScope>
			<biblScope unit="page" from="589" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
