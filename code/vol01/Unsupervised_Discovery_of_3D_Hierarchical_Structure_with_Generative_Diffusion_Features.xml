<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features</title>
				<funder ref="#_rW9tSUH #_Qdkntnf #_QCuM5Bn">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nurislam</forename><surname>Tursynbek</surname></persName>
							<email>nurislam@cs.unc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Niethammer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina</orgName>
								<address>
									<addrLine>Chapel Hill</addrLine>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="320" to="330"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">0607281F8094C3462F584FE6537CB494</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_31</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inspired by findings that generative diffusion models learn semantically meaningful representations, we use them to discover the intrinsic hierarchical structure in biomedical 3D images using unsupervised segmentation. We show that features of diffusion models from different stages of a U-Net-based ladder-like architecture capture different hierarchy levels in 3D biomedical images. We design three losses to train a predictive unsupervised segmentation network that encourages the decomposition of 3D volumes into meaningful nested subvolumes that represent a hierarchy. First, we pretrain 3D diffusion models and use the consistency of their features across subvolumes. Second, we use the visual consistency between subvolumes. Third, we use the invariance to photometric augmentations as a regularizer. Our models perform better than prior unsupervised structure discovery approaches on challenging biologically-inspired synthetic datasets and on a real-world brain tumor MRI dataset. Code is available at github.com/uncbiag/diffusion-3D-discovery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have been successfully applied to various supervised 3D biomedical image analysis tasks, such as classification <ref type="bibr" target="#b10">[11]</ref>, segmentation <ref type="bibr" target="#b6">[7]</ref>, and registration <ref type="bibr" target="#b34">[35]</ref>. Acquiring volumetric annotations manually to supervise deep learning models is costly and labor intensive. For example, the supervised training of 3D DNNs for segmentation requires the manual labeling of every voxel of the structures of interest for the entire training set. Additionally, the diversity of existing biomedical 3D volumetric image types (e.g. MRI, CT, electron tomography) and different tasks associated with them precludes image annotations for all existing problems in practice. Furthermore, experts may focus on annotating objects they are already aware of, thereby restricting the possibility of new structural discoveries in large datasets using deep learning. We hypothesize that the nested hierarchical structure intrinsic to many 3D biomedical images <ref type="bibr" target="#b12">[13]</ref> might be useful for unsupervised segmentation. As a step in this direction, our goal in this work is to develop a computational approach for unsupervised structure discovery.</p><p>Recently, unsupervised part discovery in 2D natural images has gained significant attention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref>. These methods are based on the finding that intermediate activations of deep ImageNet-pre-trained classification models capture semantically meaningful conceptual regions <ref type="bibr" target="#b7">[8]</ref>. These regions are robust to pose and viewpoint variations and help high-level image understanding by providing local object representations, leading to more explainable recognition <ref type="bibr" target="#b14">[15]</ref>. However, a naive application of part discovery methods to 3D volumetric segmentation is not feasible, due to the lack of good feature extractors for 3D biomedical images <ref type="bibr" target="#b4">[5]</ref> and ImageNet-pretrained networks operate only on 2D images.</p><p>We hypothesize that deep generative models are good feature extractors for unsupervised structure discovery for the following reasons. First, these models do not require expert labels as they are trained in a self-supervised way. Second, the ability to generate high-quality images suggests that these models capture semantically meaningful information. Third, generative representation learning has been successfully applied to global and dense prediction tasks in 2D images <ref type="bibr" target="#b8">[9]</ref> and has shown improvements in label efficiency and generalization <ref type="bibr" target="#b18">[19]</ref>.</p><p>Besides creating stunning image generation results, diffusion-based generative models <ref type="bibr" target="#b11">[12]</ref> are applied to other downstream tasks. Several works use pretrained diffusion models for 2D label-efficient semantic segmentation of natural images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. In 2D medical imaging, diffusion models are used for self-supervised vessel segmentation <ref type="bibr" target="#b17">[18]</ref>, anomaly detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>, denoising <ref type="bibr" target="#b13">[14]</ref>, and improving supervised segmentation models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. In 3D medical imaging, diffusion models are used for CT and MR image synthesis <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">33]</ref>. Inspired by the success of unsupervised part discovery methods in 2D images and the effective abilities of diffusion models for many downstream tasks we hypothesize that feature representations of generative diffusion models discover intrinsic hierarchical structures in 3D biomedical images. Our work explores this hypothesis. Our Contributions Are:</p><p>1) We pretrain 3D diffusion models, use them as feature extractors (Fig. <ref type="figure" target="#fig_0">1</ref>), and design losses (Fig. <ref type="figure" target="#fig_1">2</ref>) for unsupervised 3D structure discovery. 2) We show that features from different stages of ladder-like U-Net-based diffusion models capture different hierarchy levels in 3D biomedical volumes. 3) Our approach outperforms previous 3D unsupervised discovery methods on challenging synthetic datasets and on a real-world brain tumor segmentation (BraTS'19) dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background on Diffusion Models</head><p>Diffusion models <ref type="bibr" target="#b11">[12]</ref> consist of two parts: a forward pass and a reverse pass. The forward pass is a T -step process of adding a small Gaussian noise, gradually destroying image information and transforming a clean image x 0 into pure Gaussian noise x T . Each step t ∈ 1, T is:</p><formula xml:id="formula_0">q(x t |x t-1 ) := N (x t ; 1 -β t x t-1 , β t I),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">{β t } T t=1 is a variance schedule. With α t = t i=1 (1 -β i ), the noisy image x t at a timestep t, following q(x t |x 0 ) = t i=1 q(x i |x i-1</formula><p>), can be written as:</p><formula xml:id="formula_2">x t = √ α t x 0 + √ 1 -α t , ∼ N (0, I).<label>(2)</label></formula><p>The reverse pass is a corresponding T -step denoising process using a neural network (usually, U-Net <ref type="bibr" target="#b27">[28]</ref>) with parameters θ. For small noises, the reverse pass is also Gaussian:</p><formula xml:id="formula_3">p θ (x t-1 |x t ) := N (x t-1 ; μ θ (x t , t), Σ θ (x t , t)).</formula><p>(</p><p>Practically, instead of μ θ (x t , t) and Σ θ (x t , t), models are designed to predict either the noise t at timestep t, or a less noisier version of image x t-1 directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We formulate the 3D structure discovery task in biomedical images as an unsupervised segmentation into K parts. Given a one-channel 3D image</p><formula xml:id="formula_5">x 0 ∈ R 1×H×W ×D , our segmentation model f predicts a mask M ∈ [0, 1] K×H×W ×D . For all voxels u ∈ 0, H -1 × 0, W -1 × 0, D -1 , we have K k=1 M ku = 1.</formula><p>We use three losses for unsupervised training (see Fig. <ref type="figure" target="#fig_1">2</ref>):</p><formula xml:id="formula_6">L = λ v L v + λ f L f + λ inv L inv . (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>For an arbitrary representation h(x 0 ) of an image x 0 with voxels u, the consistency of this representation C(h(x 0 )) across K predicted parts in the form of segmentation M is defined as:</p><formula xml:id="formula_8">C(h(x 0 )) = 1 N K k=1 u M ku z k -[h(x 0 )] u 2 2 , where z k = u M ku [h(x 0 )] u u M ku , (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>where N is the number of voxels. This is a form of volume-normalized K-means loss with z k describing the mean feature value of partition k.</p><p>Feature Consistency. We pretrain generative 3D diffusion models and use them as feature extractors <ref type="bibr" target="#b3">[4]</ref>. Noise is added to a clean image x 0 based on Eq. ( <ref type="formula" target="#formula_2">2</ref>) and the noisy image x t ∈ R 1×H×W ×D is passed to the 3D diffusion model. Intermediate activations (either from different stages of ladder-like U-Nets or their concatenation, see Fig. <ref type="figure" target="#fig_0">1</ref>) upsampled to the original image size serve as a p-dimensional feature extractor φ(x 0 ) ∈ R p×H×W ×D . The feature consistency loss encourages voxels corresponding to the same parts to have similar features:</p><formula xml:id="formula_10">L f = C(φ(x 0 )). (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>Visual Consistency. The extracted features are upsampled from low spatial resolutions and therefore do not accurately align with image boundaries. To alleviate this problem, we use a voxel visual consistency loss:</p><formula xml:id="formula_12">L v = C(I(x 0 )) = C(x 0 )<label>(7)</label></formula><p>where I(x 0 ) is the identity feature extractor, i.e. I(x 0 ) = x 0 .</p><p>Photometric Invariance. As biomedical images often show acquisition differences (e.g., based on MR or CT scanner), they can be heterogeneous in their voxel intensities <ref type="bibr" target="#b24">[25]</ref>. Therefore, robustness of models to voxel-level photometric perturbations might be helpful for unsupervised discovery. We use the Dice loss <ref type="bibr" target="#b21">[22]</ref> to encourage invariance to such a photometric transformation T :</p><formula xml:id="formula_13">L inv = 1 - 2 u [f (x 0 )] u [f (T (x 0 ))] u u [f (x 0 )] 2 u + u [f (T (x 0 ))] 2 u . (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>We assume our images are min-max normalized (x 0 ∈ [0, 1]). We then use gamma-correction of the form T (x 0 ) = x γ 0 as a photometric transformation. We draw γ from the uniform distribution: </p><formula xml:id="formula_15">γ ∼ U [γ min , γ max ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To compare with state-of-the-art unsupervised 3D segmentation methods we follow <ref type="bibr" target="#b12">[13]</ref> and evaluate our method on challenging biologically inspired 3D synthetic datasets and a real-world brain tumor segmentation (BraTS'19) dataset.</p><p>The synthetic dataset of <ref type="bibr" target="#b12">[13]</ref>, consists of 120 volumes (80-20-20 split) of size 50 × 50 × 50. Inspired by cryo-electron tomography images, it contains a three-level structure, representing a biological cell, vesicles and mitochondria, as well as protein aggregates. The intensities and locations of the objects are randomized without destroying the hierarchy. The regular variant of the dataset contains cubical and spherical objects, while the irregular variant contains more complex shapes. Pink noise of magnitude m = 0.25 which is commonly seen in biological data <ref type="bibr" target="#b29">[30]</ref> is applied to the volume. Figure <ref type="figure" target="#fig_2">3</ref> shows sample slices of both variants.</p><p>The BraTS'19 dataset <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21]</ref> is an established benchmark for 3D tumor segmentation of brain MRIs. Volumes are co-registered to the same template, interpolated to (1 mm) 3 resolution and brain-extracted. Following <ref type="bibr" target="#b12">[13]</ref>, images are cropped to volumes of size 200 × 200 × 155. As in <ref type="bibr" target="#b12">[13]</ref>, FLAIR images and corresponding whole tumor (WT) annotations are used for unsupervised segmentation evaluation with the same split of 259 high grade glioma training examples into 180 train, 39 validation, and 40 test samples. The official BraTS'19 validation and test sets are not used as their segmentation masks are not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>All diffusion models use the same architecture shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We pretrain them for 50k epochs with batch size 4, using an L1 loss between the denoised and the original images. We use the Adam optimizer, a cosine noise schedule, learning rate 10 -4 and T = 250 steps. The first layer has 64 channels and this number is doubled for the proceeding downsampling layers. Due to memory constraints for BraTS'19, we trained diffusion models at 128 × 128 × 128 resolution. However, the extracted features are upsampled to the original 200 × 200 × 155 resolution.</p><p>Our segmentation networks (f in Fig. <ref type="figure" target="#fig_1">2</ref>) use a 3D U-Net architecture <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>. We trained them for 100 epochs using the Adam optimizer, a learning rate of 3 * 10 -4 and the losses in Eq. ( <ref type="formula" target="#formula_6">4</ref>). We selected the epoch that gave the best average probability of the segmentation mask for all inputs <ref type="bibr" target="#b25">[26]</ref> as our final model. Noisy images at timestep t = 25 are used as input to the diffusion models. Due to the  memory limits, for BraTS'19, we used Stage 2 features, as they have the least number of channels. We set λ f = λ v = λ inv = 1 and γ ∼ U [0.9, 1.1] for all cases.</p><p>For all experiments we used Pytorch and 4 NVIDIA A6000 GPUs (48 Gb).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We compare our method with state-of-the-art unsupervised 3D structure discovery approaches including clustering using 3D feature learning <ref type="bibr" target="#b22">[23]</ref>, a 3D convolutional autoencoder <ref type="bibr" target="#b23">[24]</ref>, and self-supervised hyperbolic representations <ref type="bibr" target="#b12">[13]</ref>.</p><p>For the synthetic datasets, we used K = 2 (background and cell) for Level 1, K = 4 (background, cell, vesicle, mitochondria) for Level 2, and K = 8 (background, cell, vesicle, mitochondria, and 4 small protein aggregates) for Level 3 predictions. The evaluation metric is the average Dice score on the annotated test labels. As the label order may differ we use the Hungarian algorithm to match the predicted masks with the ground truth segmentations. Table <ref type="table" target="#tab_0">1</ref> shows the results for the regular and irregular variants of the cryo-ET-inspired synthetic dataset. Our models outperform all previous unsupervised work at all hierarchy levels. For some levels, our models even outperform semi-supervised methods (C ¸içek et al. <ref type="bibr" target="#b6">[7]</ref> used 2% of annotated data, Zhao et al. <ref type="bibr" target="#b35">[36]</ref> used one annotated volume). We found that simple unsupervised denoising (BM4D <ref type="bibr" target="#b19">[20]</ref>) followed by k-means clustering provides a good baseline, although vanilla kmeans clustering on voxel intensities does not perform well due to noise. Results in Fig. <ref type="figure" target="#fig_2">3</ref> demonstrate that our proposed unsupervised method indeed discovers  the hierarchical structure of different levels. We also show in Table <ref type="table" target="#tab_1">2</ref> that features from early decoder stages of the U-Net-based diffusion models better discover larger objects in the hierarchy, features at intermediate stages better capture intermediate objects, and features at later stages better find smaller objects.</p><p>For the Brain Tumor Segmentation (BraTS'19) dataset, we use the whole tumor (WT) segmentation mask for evaluation, which is detectable based on the FLAIR images alone. We train segmentation models with K = 3 parts (background, brain, tumor). The evaluation metric, as in the BraTS'19 challenge <ref type="bibr" target="#b20">[21]</ref>, is Dice score and the 95th percentile of the symmetric Hausdorff distance, which quantifies the surface distance of the predicted segmentation from the manual tumor segmentation in millimeters. Table <ref type="table" target="#tab_2">3</ref> shows that our model outperforms all prior unsupervised methods for both evaluation metrics. As an approximate upper bound we show for reference the reported results of the 1st place solution <ref type="bibr" target="#b16">[17]</ref> on BraTS'19 which is based on supervised training on the full train set and evaluated on the BraTS'19 test set. The qualitative results in Fig. <ref type="figure" target="#fig_3">4</ref> show that our model can detect tumors of different sizes. Our predictions look smoother and do not capture fine details of tumor segmentations.</p><p>We perform ablation studies on the BraTS'19 dataset (Table <ref type="table" target="#tab_2">3</ref>: below the line). Measuring the impact of each loss, we see that the smallest performance drop is due to a deactivated invariance loss (λ inv = 0) while deactivating the visual consistency (λ v = 0) and feature consistency (λ f = 0) losses results in larger, but similar performance drops. However, to achieve best performance all three components are necessary. We also perform k-means clustering on intensities and features. We observe that using our deep network model dramatically improves performance, although our losses are similar to k-means clustering.</p><p>This might be due to the fact that predictive modeling involves learning from a distribution of images and a model may therefore extract useful knowledge from a collection of images. To evaluate the significance of the diffusion features, we replaced our diffusion feature extractor with a 3D ResNet from Med3D <ref type="bibr" target="#b4">[5]</ref> trained on 23 medical datasets. We use the "layer1 2 conv2" features as they showed the best performance. Although performance does not drop significantly when Med3D features are used with our losses, Med3D features do not produce good results when directly used for k-means clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we showed that features from 3D generative diffusion models using a ladder-like U-Net-based architecture can discover intrinsic 3D structures in biomedical images. We trained predictive unsupervised segmentation models using losses that encourage the decomposition of biomedical volumes into nested subvolumes aligned with their hierarchical structures. Our method outperforms existing unsupervised segmentation approaches and discovers meaningful hierarchical concepts on challenging biologically-inspired synthetic datasets and on the BraTS brain tumor dataset. While we tested our approach for unsupervised image segmentation it is conceivable that it could also be useful in semisupervised settings and that could be applied to data types other than images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Feature extractor. Given a clean 3D image x0, we add Gaussian noise corresponding to diffusion timestep t to the image following the distribution q(xt|x0) using Eq. (2). The noisy image xt is passed to our pretrained 3D diffusion model. We upsample intermediate activations to the original image size and use them as feature extractor φ for each voxel. Features from different stages of a U-Net-based ladder-like architecture for a diffusion model capture different hierarchy levels.</figDesc><graphic coords="2,72,48,53,90,276,76,155,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Predictive unsupervised structure discovery. Our unsupervised segmentation network f : x0 → M is trained with three losses. Feature consistency loss L f encourages features φ(x0), extracted using diffusion models (see Fig.1), of voxels belonging to the same parts to be similar to each other. Visual consistency loss Lv encourages models to learn parts that align with image boundaries. Photometric invariance loss Linv encourages invariance in models to photometric transformation T .</figDesc><graphic coords="3,43,80,59,90,337,00,173,80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of unsupervised 3D structure discovery with our method on the biologically-inspired synthetic datasets. GT indicates ground truth.</figDesc><graphic coords="6,218,10,362,87,166,12,177,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples of discovered structures on BraTS'19. Our method discovers meaningful regions and detects tumors of different sizes in an unsupervised manner.</figDesc><graphic coords="8,78,15,54,14,311,92,185,56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dice scores on the biologically inspired synthetic datasets. Our method outperforms all previous work on unsupervised 3D segmentation for all levels of hierarchy.</figDesc><table><row><cell></cell><cell>Regular</cell><cell></cell><cell></cell><cell></cell><cell>Irregular</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Level 1 Level 2 Level 3</cell><cell></cell><cell cols="3">Level 1 Level 2 Level 3</cell></row><row><cell>C ¸içek et al. [7]</cell><cell>0.968</cell><cell>0.829</cell><cell>0.668</cell><cell cols="2">Semi-supervised 0.970</cell><cell>0.825</cell><cell>0.601</cell></row><row><cell>Zhao et al. [36]</cell><cell>0.989</cell><cell>0.655</cell><cell>0.357</cell><cell cols="2">Semi-supervised 0.978</cell><cell>0.641</cell><cell>0.333</cell></row><row><cell cols="2">Nalepa et al. [24] 0.530</cell><cell>0.276</cell><cell>0.112</cell><cell>Unsupervised</cell><cell>0.527</cell><cell>0.280</cell><cell>0.144</cell></row><row><cell>Ji et al. [16]</cell><cell>0.589</cell><cell>0.291</cell><cell>0.150</cell><cell>Unsupervised</cell><cell>0.527</cell><cell>0.280</cell><cell>0.144</cell></row><row><cell cols="2">Moriya et al. [23] 0.628</cell><cell>0.311</cell><cell>0.141</cell><cell>Unsupervised</cell><cell>0.525</cell><cell>0.232</cell><cell>0.094</cell></row><row><cell>Hsu et al. [13]</cell><cell>0.952</cell><cell>0.541</cell><cell>0.216</cell><cell>Unsupervised</cell><cell>0.953</cell><cell>0.488</cell><cell>0.199</cell></row><row><cell>Ours</cell><cell cols="4">0.986 0.577 0.397 Unsupervised</cell><cell cols="3">0.967 0.565 0.382</cell></row><row><cell>k-means</cell><cell>0.808</cell><cell>0.326</cell><cell>0.149</cell><cell>Non-DL</cell><cell>0.771</cell><cell>0.299</cell><cell>0.118</cell></row><row><cell cols="2">BM4D+k-means 0.949</cell><cell>0.529</cell><cell>0.335</cell><cell>Non-DL</cell><cell>0.950</cell><cell>0.533</cell><cell>0.324</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Dice</figDesc><table><row><cell cols="3">scores when using features from different stages of</cell><cell cols="2">Regular Slice</cell><cell>Iregular Slice</cell></row><row><cell cols="3">ladder-like U-Net-based diffusion</cell><cell></cell><cell></cell></row><row><cell cols="3">models (see Fig. 1) for unsuper-</cell><cell></cell><cell></cell></row><row><cell cols="3">vised segmentation of the differ-</cell><cell></cell><cell></cell></row><row><cell cols="3">ent hierarchy levels of the syn-</cell><cell>Pred.</cell><cell>GT</cell><cell>Pred.</cell><cell>GT</cell></row><row><cell cols="3">thetic dataset. Features at lower resolutions (Stage 1) are more suit-able for discovering larger objects</cell><cell>Level 1</cell><cell></cell></row><row><cell cols="3">(Level 1). Intermediate features</cell><cell></cell><cell></cell></row><row><cell cols="3">(Stage 2) are more suitable for intermediate discoveries (Level 2). Features at higher resolutions (Stage 3) are more suitable for</cell><cell>Level 2</cell><cell></cell></row><row><cell cols="3">more detailed discoveries (Level 3).</cell><cell>Level 3</cell><cell></cell></row><row><cell cols="2">Predictions</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Level 1 Level 2 Level 3</cell><cell></cell><cell></cell></row><row><cell cols="2">Stage 1 features 0.986 0.366</cell><cell>0.273</cell><cell></cell><cell></cell></row><row><cell>Stage 2 features 0.923</cell><cell cols="2">0.577 0.327</cell><cell></cell><cell></cell></row><row><cell>Stage 3 features 0.878</cell><cell>0.489</cell><cell>0.397</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 . BraTS'19 results with ablation studies.</head><label>3</label><figDesc>Our method outperforms previous unsupervised methods in both the Dice score and the 95% Hausdorff distance. Dice and HD95 numbers for these models are taken from<ref type="bibr" target="#b12">[13]</ref>.</figDesc><table><row><cell></cell><cell cols="2">Dice W T ↑ HD95 W T ↓</cell><cell></cell></row><row><cell cols="2">1st place solution [17] 0.888</cell><cell>4.618</cell><cell>Supervised</cell></row><row><cell>a Nalepa et al. [24]</cell><cell>0.211</cell><cell>170.434</cell><cell>Unsupervised</cell></row><row><cell>a Ji et al. [16]</cell><cell>0.425</cell><cell>114.400</cell><cell>Unsupervised</cell></row><row><cell>a Moriya et al. [23]</cell><cell>0.495</cell><cell>110.803</cell><cell>Unsupervised</cell></row><row><cell>a Hsu et al. [13]</cell><cell>0.684</cell><cell>97.641</cell><cell>Unsupervised</cell></row><row><cell>Ours</cell><cell>0.719</cell><cell>27.838</cell><cell>Unsupervised</cell></row><row><cell>λinv = 0</cell><cell>0.696</cell><cell>38.645</cell><cell>Unsupervised</cell></row><row><cell>λ f = 0</cell><cell>0.677</cell><cell>42.318</cell><cell>Unsupervised</cell></row><row><cell>λv = 0</cell><cell>0.671</cell><cell>41.801</cell><cell>Unsupervised</cell></row><row><cell cols="2">w/ Med3D [5] features 0.657</cell><cell>29.906</cell><cell>Unsupervised</cell></row><row><cell>k-means</cell><cell>0.439</cell><cell>63.811</cell><cell>Non-DL</cell></row><row><cell>Features k-means</cell><cell>0.471</cell><cell>45.917</cell><cell>Unsupervised</cell></row><row><cell>Med3D [5] k-means</cell><cell>0.231</cell><cell>55.846</cell><cell>Unsupervised</cell></row><row><cell>a</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This work was supported by <rs type="funder">NIH</rs> grants <rs type="grantNumber">1R01AR072013</rs>, <rs type="grantNumber">1R01HL149877</rs>, and <rs type="grantNumber">R41MH118845</rs>. The work expresses the views of the authors, not of <rs type="funder">NIH</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rW9tSUH">
					<idno type="grant-number">1R01AR072013</idno>
				</org>
				<org type="funding" xml:id="_Qdkntnf">
					<idno type="grant-number">1R01HL149877</idno>
				</org>
				<org type="funding" xml:id="_QCuM5Bn">
					<idno type="grant-number">R41MH118845</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Decoder denoising pretraining for semantic segmentation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Asiedu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.11423</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BraTS challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Labelefficient semantic segmentation with diffusion models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rubachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Voynov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00625</idno>
		<title level="m">Med3D: transfer learning for 3D medical image analysis</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised part discovery from contrastive reconstruction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28104" to="28118" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3D U-Net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46723-8_49</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46723-8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2016</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Joskowicz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Unal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Wells</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page">49</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep feature factorization for concept discovery</title>
		<author>
			<persName><forename type="first">E</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01264-9_21</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01264-921" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">11218</biblScope>
			<biblScope unit="page" from="352" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Three-dimensional medical image synthesis with denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dorjsembe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Odonchimed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIDL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Classification of CT brain images based on deep learning networks</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="49" to="56" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Capturing implicit hierarchical structure in 3D biomedical images with self-supervised hyperbolic representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5112" to="5123" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised denoising of retinal OCT with diffusion probabilistic model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oguz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="25" to="34" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SCOPS: self-supervised co-part segmentation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="869" to="878" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
		<respStmt>
			<orgName>ICCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two-stage cascaded U-Net: 1st place solution to BraTS challenge 2019 segmentation task</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-46640-4_22</idno>
		<idno>978-3-030-46640-4 22</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2019</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Crimi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11992</biblScope>
			<biblScope unit="page" from="231" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Diffusion adversarial representation learning for selfsupervised vessel segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14566</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semantic segmentation with generative models: semi-supervised learning and strong out-of-domain generalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8300" to="8311" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonlocal transform-domain filter for volumetric data denoising and reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (BraTS)</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">V-Net: fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of 3D medical images based on clustering and deep representation learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moriya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biomedical Applications in Molecular, Structural, and Functional Imaging</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">10578</biblScope>
			<biblScope unit="page" from="483" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of hyperspectral images using 3-D convolutional autoencoders</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nalepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Myller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Antoniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1948" to="1952" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data augmentation for brain-tumor segmentation: a review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nalepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawulok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Comput. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Encoder-weighted W-Net for unsupervised segmentation of cervix region in colposcopy images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancers</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">3400</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fast unsupervised brain anomaly detection and segmentation with diffusion models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Pinaya</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-167" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-24574-428" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2015</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What is healthy? Generative counterfactual diffusion for lesion localization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kascenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tsaftaris</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-18576-2_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-18576-24" />
	</analytic>
	<monogr>
		<title level="m">DGM4MICCAI 2022</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Mukhopadhyay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Oksuz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13609</biblScope>
			<biblScope unit="page" from="34" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Necessity of noise in physiology and medicine</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sejdić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Lipsitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="459" to="470" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Diffusion models for medical anomaly detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_4</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-14" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Diffusion models for implicit image segmentation ensembles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolleb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sandkühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valmaggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Cattin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1336" to="1348" />
		</imprint>
		<respStmt>
			<orgName>MIDL</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.00611</idno>
		<title level="m">MedSegDiff: medical image segmentation with diffusion probabilistic model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">AnoDDPM: anomaly detection with denoising diffusion probabilistic models using simplex noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wyatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Schmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Willcocks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="650" to="656" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Quicksilver: fast predictive image registration-a deep learning approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Styner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="378" to="396" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Data augmentation using learned transformations for one-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8543" to="8553" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
