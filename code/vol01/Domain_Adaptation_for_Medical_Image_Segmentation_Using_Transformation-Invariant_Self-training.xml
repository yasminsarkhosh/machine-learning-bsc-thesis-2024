<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training</title>
				<funder>
					<orgName type="full">Haag-Streit Switzerland</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Negin</forename><surname>Ghamsarian</surname></persName>
							<email>negin.ghamsarian@unibe.ch</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for AI in Medicine</orgName>
								<orgName type="department" key="dep2">Faculty of Medicine</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Javier</forename><surname>Gamazo Tejero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for AI in Medicine</orgName>
								<orgName type="department" key="dep2">Faculty of Medicine</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pablo</forename><surname>Márquez-Neila</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for AI in Medicine</orgName>
								<orgName type="department" key="dep2">Faculty of Medicine</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Wolf</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology</orgName>
								<address>
									<settlement>Inselspital, Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Zinkernagel</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Ophthalmology</orgName>
								<address>
									<settlement>Inselspital, Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Klaus</forename><surname>Schoeffmann</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Klagenfurt University</orgName>
								<address>
									<settlement>Klagenfurt</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raphael</forename><surname>Sznitman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for AI in Medicine</orgName>
								<orgName type="department" key="dep2">Faculty of Medicine</orgName>
								<orgName type="institution">University of Bern</orgName>
								<address>
									<settlement>Bern</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Adaptation for Medical Image Segmentation Using Transformation-Invariant Self-training</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="331" to="341"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">69C47AD84B2F757EE1745D9EC9D7350C</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_32</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Semi-Supervised Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Models capable of leveraging unlabelled data are crucial in overcoming large distribution gaps between the acquired datasets across different imaging devices and configurations. In this regard, selftraining techniques based on pseudo-labeling have been shown to be highly effective for semi-supervised domain adaptation. However, the unreliability of pseudo labels can hinder the capability of self-training techniques to induce abstract representation from the unlabeled target dataset, especially in the case of large distribution gaps. Since the neural network performance should be invariant to image transformations, we look to this fact to identify uncertain pseudo labels. Indeed, we argue that transformation invariant detections can provide more reasonable approximations of ground truth. Accordingly, we propose a semi-supervised learning strategy for domain adaptation termed transformation-invariant self-training (TI-ST). The proposed method assesses pixel-wise pseudo-labels' reliability and filters out unreliable detections during self-training. We perform comprehensive evaluations for domain adaptation using three different modalities of medical images, two different network architectures, and several alternative state-of-theart domain adaptation methods. Experimental results confirm the superiority of our proposed method in mitigating the lack of target domain annotation and boosting segmentation performance in the target domain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation is a prerequisite for a broad range of medical imaging applications, including disease diagnosis and treatment <ref type="bibr" target="#b12">[13]</ref>, surgical workflow analysis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, operation room planning, and surgical outcome prediction <ref type="bibr" target="#b6">[7]</ref>. While supervised deep learning approaches have yielded satisfactory performance in semantic segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>, their performance is heavily limited by the labeled training dataset distribution. Indeed, a network trained on a dataset acquired with a specific device or configuration can dramatically underperform when evaluated on a different device or conditions. Overcoming this entails new annotations per device, a demand that is hard to meet, especially for semantic segmentation, and even more so in the medical domain, where expert knowledge is essential.</p><p>Driven by the need to overcome this challenge, numerous semi-supervised learning paradigms have looked to alleviate annotation requirements in the target domain. Semi-supervised learning refers to methods that encourage learning abstract representations from an unlabeled dataset and extending the decision boundaries towards a more-generalized or target dataset distribution. These techniques can be categorized into (i) consistency regularization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref>, (ii) contrastive learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, (iii) adversarial learning <ref type="bibr" target="#b21">[22]</ref>, and (iv) self-training <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>. Consistency regularization techniques aim to inject knowledge via penalizing inconsistencies for identical images that have undergone different distortions, such as transformations or dropouts, or fed into networks with different initializations <ref type="bibr" target="#b3">[4]</ref>. Specifically, the Π model <ref type="bibr" target="#b14">[15]</ref> penalizes differences between the predictions of two transformed versions of each input image to reinforce consistent and augmentation-invariant predictions. Temporal ensembling <ref type="bibr" target="#b14">[15]</ref> is designed to alleviate the negative effect of noisy predictions by integrating predictions of consecutive training iterations. Cross-pseudo supervision regularizes the networks by enforcing similar predictions from differently initialized networks.</p><p>More recent deep self-training approaches based on pseudo labels have emerged as promising techniques for unsupervised domain adaptation. These techniques assume that a trained network can approximate the ground-truth labels for unlabeled images. Since no metric guarantees pseudo-label reliability, several methods have been developed to alleviate pseudo-label error back-propagation. To progressively improve pseudo-labeling performance, reciprocal learning <ref type="bibr" target="#b24">[25]</ref> adopts a teacher-student framework where the student network performance on the source domain drives the teacher network weights updates. ST++ <ref type="bibr" target="#b23">[24]</ref> proposes to evaluate the reliability of image-based pseudo labels based on the consistency of predictions in different network checkpoints. Subsequently, half of the more reliable images are utilized to re-train the network in the first step, and the trained network is used for pseudo-labeling the whole dataset for a second re-training step. Despite the effectiveness of state-of-the-art pseudo-labeling strategies, we argue that one important aspect has been underexplored: how can a trained network self-assess the reliability of its pixel-level predictions?</p><p>To this end, we propose a novel self-training framework with a selfassessment strategy for pseudo-label reliability. The proposed framework uses transformation-invariant highly-confident predictions in the target dataset for self-training. This objective is achieved by considering an ensemble of highconfidence predictions from transformed versions of identical inputs. To validate the effectiveness of our proposed framework on a variety of tasks, we evaluate our approach on three different semantic segmentation imaging modalities, including video (cataract surgery), optical coherence tomography (retina), and MRI (prostate), as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. We perform comprehensive experiments to validate the performance of the proposed framework, namely "Transformation-Invariant Self-Training" 1 (TI-ST). The experimental results indicate that TI-ST significantly improves segmentation performance for unlabeled target datasets compared to numerous state-of-the-art alternatives. the ensemble of high-confidence predictions from two versions of the same target sample. Our proposed TI-ST framework simultaneously trains on the source and target domains, so as to progressively bridge the intra-domain distribution gap. Figure <ref type="figure" target="#fig_1">2</ref> depicts our TI-ST framework, which we detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model</head><p>At training time, images from the source dataset are augmented using spatial g(•) and non-spatial f (•) transformations and passed through a segmentation network, N (•), by which the network is trained using a standard supervision loss. At the same time, images from the target dataset are also passed to the network. Specifically, we feed two versions of each target image to the network: (1) the original target image x T , and (2) its non-spatially transformed version, xT = f (x T ). Once fed through the network, the corresponding predictions can be defined as ỹT = σ(N (x T )) and ỹT = σ(N ( xT )), where σ(•) is the Softmax operation. We then define a confidence-mask ensemble as</p><formula xml:id="formula_0">M cnf = Cnf( ỹT ) Cnf( ỹT ),<label>(1)</label></formula><p>where refers to Hadamard product used for element-wise multiplication, and Cnf is the high confidence masking function,</p><formula xml:id="formula_1">Cnf ∈ (W ×H) (y) = 1, if max C (y) &gt; τ 0, else.<label>(2)</label></formula><p>where τ ∈ (0.5, 1) is the confidence threshold, and H, W , and C are the height, width, and number of classes in the output, respectively. Specifically, M cnf encodes regions of confident predictions that are invariant to transformations. We can then compute the pseudo-ground-truth mask for each input from the target dataset as</p><formula xml:id="formula_2">ŷT = argmax C ( ỹT ), if M cnf = 1 ignore, else.<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training</head><p>To train our model, we simultaneously consider both the source and target samples by minimizing the following loss,</p><formula xml:id="formula_3">L overall = L Sup ( ỹS , y S ) + λ L P s ( ỹT , ŷT ) , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where L Sup and L P s indicate the supervised and pseudo-supervised loss functions used, respectively. We set λ as a time-dependent weighing function that gradually increases the share of pseudo-supervised loss. Intuitively, our pseudosupervised loss enforces predictions on transformation-invariant highly-confident regions for unlabeled images.</p><p>Discussion: The quantity and distribution of supervised data are determining factors in neural networks' performance. With highly distributed large-scale supervisory data, neural networks converge to an optimal state efficiently. However, when only limited supervisory data with heterogeneous distribution from the inference dataset are available, using more sophisticated methods to leverage a priori knowledge is essential. Our proposed use of invariance of network predictions with respect to data augmentation is a strong form of knowledge that can be learned through dataset-dependent augmentations. The trained network is then expected to provide consistent predictions under diverse transformations. Hence, the transformation variance of the network predictions can indicate the network's prediction doubt and low confidence correspondingly. We take advantage of this characteristic to assess the reliability of predictions and filter out unreliable pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>Datasets: We validate our approach on three cross-device/site datasets for three different modalities:</p><p>-Cataract: instrument segmentation in cataract surgery videos <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref>. We set the "Cat101" <ref type="bibr" target="#b20">[21]</ref> as the source dataset and the "CaDIS" as the target domain dataset <ref type="bibr" target="#b11">[12]</ref>. -OCT: IRF Fluid segmentation in retinal OCTs <ref type="bibr" target="#b0">[1]</ref>. We use the high-quality "Spectralis" dataset as the source and the lower-quality "Topcon" dataset as the target domain.</p><p>-MRI: multi-site prostate segmentation <ref type="bibr" target="#b17">[18]</ref>. We sample volumes from "BMC" and "BIDMC" as the source and target domain, respectively.</p><p>We follow a four-fold validation strategy for all three cases and report the average results over all folds. The average number of labeled training images (from the source domain), unlabeled training images (from the target domain), and test images per fold are equal to (207, 3189, 58) for Cataract, (391, 569, 115) for <ref type="bibr">OCT,</ref><ref type="bibr">and (273,</ref><ref type="bibr">195,</ref><ref type="bibr">65)</ref> for MRI dataset.</p><p>Baseline Methods: We compare the performance of our proposed transformation-invariant self-training (SI-ST) method against seven state-of-theart semi-supervised learning methods: Π models <ref type="bibr" target="#b14">[15]</ref>, temporal ensembling <ref type="bibr" target="#b14">[15]</ref>, mean teacher <ref type="bibr" target="#b18">[19]</ref>, cross pseudo supervision (CSP) <ref type="bibr" target="#b3">[4]</ref>, reciprocal learning (RL) <ref type="bibr" target="#b24">[25]</ref>, self-training (ST) <ref type="bibr" target="#b23">[24]</ref>, and mutual correction framework (MCF) <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Networks and Training Settings:</head><p>We evaluate our TI-ST framework using two different architectures: (1) DeepLabV3+ <ref type="bibr" target="#b2">[3]</ref> with ResNet50 backbone <ref type="bibr" target="#b13">[14]</ref> and ( <ref type="formula" target="#formula_1">2</ref>) scSE <ref type="bibr" target="#b19">[20]</ref> with VGG16 backbone. Both backbones are initialized with the ImageNet <ref type="bibr" target="#b4">[5]</ref> pre-trained parameters. We use a batch size of four for the Cataract and MRI datasets and a batch size of two for the OCT dataset. For all training strategies, we set the number of epochs to 100. The initial learning rate is set to 0.001 and decayed by a factor of γ = 0.8 every two epochs. The input size of the networks is 512×512 for cataract and OCT and 384×384 for the MRI dataset. As spatial transformations g(•), we apply cropping and random rotation (up to 30 degrees). The non-spatial transformations, f (•), include color jittering (brightness = 0.7, contrast = 0.7, saturation = 0.7), Gaussian blurring, and random sharpening. The confidence threshold τ for the self-training framework and the proposed TI-ST framework is set to 0.85 except in the ablation studies (See the next section). In Eq. ( <ref type="formula" target="#formula_3">4</ref>), the weighting function λ ramps up from the first epoch along a Gaussian curve equal to exp[-5(1current-epoch/total-epochs)]. The self-supervised loss is set to the cross-entropy loss, and the supervised loss is set to the cross entropy log dice loss, which is a weighted sum of cross-entropy and the logarithm of soft dice coefficient. For the TI-ST framework, we only use non-spatial transformations for the self-training branch for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Table <ref type="table" target="#tab_0">1</ref> compares the performance of our transformation-invariant self-training (TI-ST) approach with alternative methods across three tasks and using two network architectures. According to the quantitative results, TI-ST, RL, ST, and CPS are the best-performing methods. Nevertheless, our proposed TI-ST achieves the highest average relative improvement in dice score compared to naive supervised learning (16.18% average improvement). Considering our main competitor (RL), we note that our proposed TI-ST method is a one-stage framework using one network. In contrast, RL is a two-stage framework (requiring a pre-training stage) and uses a teacher-student network. Hence, TI-ST is also more efficient than RL in terms of time and computation. Furthermore, the proposed strategy demonstrates the most consistent results when evaluated on different tasks, regardless of the utilized neural network architecture. Figure <ref type="figure">3-(a-b</ref>) demonstrates the effect of the pseudo-labeling threshold on TI-ST performance compared with regular ST. We observe that filtering out unreliable pseudo-labels based on transformation variance can remarkably boost pseudo-supervision performance regardless of the threshold. Figure <ref type="figure">3-(c</ref>) compares the performance of the supervised baseline, ST, and TI-ST with respect to the number of source-domain labeled training images. While ST performance converges when the number of labeled images increases, our TI-ST pushes deci-Fig. <ref type="figure">3</ref>. Ablation studies on the pseudo-labeling threshold and size of the labeled dataset.  sion boundaries toward the target domain dataset by avoiding training with transformation variant pseudo-labels. We validates the stability of TI-ST vs. ST with different labeling thresholds (0.80 and 0.85) over four training folds in Fig. <ref type="figure" target="#fig_2">4</ref>, where TI-ST achieves a higher average improvement relative to supervised learning for different tasks and network architectures. This analysis also shows that the performance of ST is sensitive to the pseudo-labeling threshold and generally degrades by reducing the threshold due to resulting in wrong pseudo labels. However, TI-ST can effectively ignore false predictions in lower thresholds and take advantage of a higher amount of correct pseudo labels. This superior performance is depicted qualitatively in Fig. <ref type="figure" target="#fig_3">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a novel self-training framework with a self-assessment strategy for pseudo-label reliability, namely "Transformation-Invariant Self-Training" (TI-ST). This method uses transformation-invariant highly-confident predictions in the target dataset by considering an ensemble of high-confidence predictions from transformed versions of identical inputs. We experimentally show the effectiveness of our approach against numerous existing methods across three different source-to-target segmentation tasks, and when using different model architectures. Beyond this, we show that our approach is resilient to changes in the methods hyperparameter, making it well-suited for different applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example images from the three adopted datasets: (1) cross-device-and-center instrument segmentation in cataract surgery videos (Cat101 vs. CaDIS), cross-device fluid segmentation in OCT (Spectralis vs. Topcon), and cross-institution prostate segmentation in MRI (BMC vs. BIDMC).</figDesc><graphic coords="3,62,97,53,69,326,68,142,00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the proposed semi-supervised domain adaptation framework based on transformation-invariant self-training (TI-ST). Ignored pseudo-labels during unsupervised loss computation are shown in turquoise.</figDesc><graphic coords="4,59,34,99,50,313,36,101,20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Ablation study on the performance stability of TI-ST vs. ST across the different experimental segmentation tasks.</figDesc><graphic coords="8,97,89,59,78,283,60,83,68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Qualitative comparisons between the performance of TI-ST and four existing methods.</figDesc><graphic coords="8,73,38,207,17,291,88,169,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons in Dice score (%) among the proposed (TI-ST) and alternative methods for DeepLabV3+<ref type="bibr" target="#b2">[3]</ref> (DLV3+) and scSENet<ref type="bibr" target="#b19">[20]</ref> and the three datasets. Relative Dice computed over the Supervised baseline. The best results are shown in green.</figDesc><table><row><cell>Modality</cell><cell cols="2">Cataract Surgery</cell><cell cols="2">OCT</cell><cell cols="2">MRI</cell><cell>Avg. Rel.</cell></row><row><cell>Network</cell><cell>DLV3+</cell><cell cols="5">scSENet DLV3+ scSENet DLV3+ scSENet</cell><cell></cell></row><row><cell>Supervised</cell><cell>15.42</cell><cell>37.67</cell><cell>22.87</cell><cell>24.08</cell><cell>52.39</cell><cell>65.93</cell><cell>N/A</cell></row><row><cell>Π Model [15]</cell><cell>27.55</cell><cell>35.56</cell><cell>1.12</cell><cell>0.00</cell><cell>10.00</cell><cell>6.87</cell><cell>-22.88</cell></row><row><cell>TE [15]</cell><cell>33.10</cell><cell>42.32</cell><cell>42.13</cell><cell>39.86</cell><cell>63.41</cell><cell>67.25</cell><cell>11.62</cell></row><row><cell cols="2">Mean Teacher [19] 11.06</cell><cell>39.54</cell><cell>19.11</cell><cell>4.70</cell><cell>64.82</cell><cell>66.87</cell><cell>-2.04</cell></row><row><cell>RL [25]</cell><cell>34.40</cell><cell>45.13</cell><cell>48.73</cell><cell>47.70</cell><cell>60.79</cell><cell>70.20</cell><cell>14.77</cell></row><row><cell>CPS [4]</cell><cell>36.24</cell><cell>39.40</cell><cell>47.31</cell><cell>14.71</cell><cell>76.00</cell><cell>68.80</cell><cell>10.68</cell></row><row><cell>ST [24]</cell><cell>34.34</cell><cell>41.10</cell><cell>36.84</cell><cell>33.01</cell><cell>68.63</cell><cell>71.97</cell><cell>11.26</cell></row><row><cell>MCF [23]</cell><cell>26.97</cell><cell>40.19</cell><cell>40.12</cell><cell>36.52</cell><cell>54.17</cell><cell>50.23</cell><cell>7.46</cell></row><row><cell>TI-ST</cell><cell>37.69</cell><cell>45.31</cell><cell>50.93</cell><cell>40.87</cell><cell>66.56</cell><cell>74.07</cell><cell>16.18</cell></row><row><cell></cell><cell>(+22.27)</cell><cell>(+7.46)</cell><cell>(+28.06)</cell><cell>(+16.79)</cell><cell>(+14.17)</cell><cell>(+8.14)</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>MethodologyConsider a labeled source dataset, S, with training images X S and corresponding segmentation labels Y S , while we denote a target dataset T , containing only target images X T . We aim to train a network using X S , Y S , and X T for semantic segmentation in the target dataset.We propose to train the model using a self-supervised approach on the images X T by assigning pseudo labels during training. Typical pseudo labels are computed from independent predictions of unlabeled images. Instead, our proposed framework adopts a self-assessment strategy to determine the reliability of predictions in an unsupervised fashion. Specifically, we propose to target highlyreliable predictions generated by a network aiming for transformation-invariant confidence. Compared to self-ensembling strategies that penalize the distant predictions corresponding to the transformed versions of identical inputs, our goal is to filter out transformation-variant predictions. Indeed, our method reinforces<ref type="bibr" target="#b0">1</ref> The PyTorch implementation of TI-ST is publicly available at https://github.com/ Negin-Ghamsarian/Transformation-Invariant-Self-Training-MICCAI23.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was funded by <rs type="funder">Haag-Streit Switzerland</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0_32.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">RETOUCH: the retinal oct fluid detection and segmentation benchmark and challenge</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bogunović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1858" to="1874" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contrastive learning of global and local features for medical image segmentation with limited annotations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12546" to="12558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Encoder-decoder with Atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross pseudo supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2613" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: a large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enabling relevance-based exploration of cataract videos</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ghamsarian</surname></persName>
		</author>
		<idno type="DOI">10.1145/3372278.3391937</idno>
		<ptr target="https://doi.org/10.1145/3372278.3391937" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval, ICMR &apos;20</title>
		<meeting>the 2020 International Conference on Multimedia Retrieval, ICMR &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="378" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">LensID: a CNN-RNN-based framework towards lens irregularity detection in cataract surgery videos</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ghamsarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taschwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Putzgruber-Adamitsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>El-Shabrawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87237-3_8</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87237-3_8" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12908</biblScope>
			<biblScope unit="page" from="76" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ReCal-Net: joint region-channel-wise calibrated network for semantic segmentation in cataract surgery videos</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ghamsarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taschwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Putzgruber-Adamitsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>El-Shabrawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schöffmann</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-92238-2_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-92238-2_33" />
	</analytic>
	<monogr>
		<title level="m">ICONIP 2021</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Mantoro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ayu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Hidayanto</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13110</biblScope>
			<biblScope unit="page" from="391" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relevance detection in cataract surgery videos by Spatio-temporal action localization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ghamsarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taschwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Putzgruber-Adamitsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10720" to="10727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DeepPyramid: enabling pyramid view and deformable pyramid reception for semantic segmentation in cataract surgery videos</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ghamsarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taschwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_27" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13435</biblScope>
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with contrastive learning for OCT segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gomariz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_34</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-1_34" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Fletcher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Speidel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13438</biblScope>
			<biblScope unit="page" from="351" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CaDIS: cataract dataset for surgical RGB-image segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grammatikopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102053</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using domain knowledge for robust and generalizable deep learningbased CT-free PET attenuation and scatter correction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5882</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno>CoRR abs/1610.02242</idno>
		<ptr target="http://arxiv.org/abs/1610.02242" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for the histopathological cell segmentation through self-ensembling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MICCAI Workshop on Computational Pathology. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Atzori</surname></persName>
		</editor>
		<meeting>the MICCAI Workshop on Computational Pathology. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformationconsistent self-ensembling model for semisupervised medical image segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="523" to="534" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MS-Net: multi-site network for improving prostate segmentation with heterogeneous MRI data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2713" to="2724" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for medical imaging segmentation with self-ensembling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Perone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen-Adad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recalibrating fully convolutional networks with spatial and channel &quot;squeeze and excitation&quot; blocks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="540" to="549" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cataract-101: video dataset of 101 cataract surgeries</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schoeffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taschwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Münzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Primus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Putzgruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Multimedia Systems Conference</title>
		<meeting>the 9th ACM Multimedia Systems Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="421" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Test-time unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Varsavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orbes-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59710-8_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59710-8_42" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Martel</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12261</biblScope>
			<biblScope unit="page" from="428" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MCF: mutual correction framework for semi-supervised medical image segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15651" to="15660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ST++: make self-training work better for semi-supervised semantic segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4268" to="4277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reciprocal learning for semi-supervised segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87196-3_33</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87196-3_33" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12902</biblScope>
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2018</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="297" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01219-9_18</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-01219-9_18" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
