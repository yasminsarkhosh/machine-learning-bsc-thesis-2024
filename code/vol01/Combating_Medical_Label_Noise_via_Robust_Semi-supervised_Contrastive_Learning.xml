<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning</title>
				<funder ref="#_VtmFHDC">
					<orgName type="full">Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies</orgName>
				</funder>
				<funder ref="#_9QZ6jEr">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_tMrRUSC">
					<orgName type="full">Shenzhen Fundamental Research Fund</orgName>
				</funder>
				<funder ref="#_txsHzgQ">
					<orgName type="full">Shenzhen Key Technical Project</orgName>
				</funder>
				<funder ref="#_ttMZ54F">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_WDMWB5g">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bingzhi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">South China Normal University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhanhao</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">South China Normal University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yishu</forename><surname>Liu</surname></persName>
							<email>liuyishu@stu.hit.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiahui</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">South China Normal University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Biqing</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">South China Normal University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangming</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combating Medical Label Noise via Robust Semi-supervised Contrastive Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">35D1BAFB7FE7B7A210DBE7D105791419</idno>
					<idno type="DOI">10.1007/978-3-031-43907-054.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Medical Label Noise</term>
					<term>Mixup</term>
					<term>Semi-supervised Learning</term>
					<term>Contrastive Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning-based AI diagnostic models rely heavily on high-quality exhaustive-annotated data for algorithm training but suffer from noisy label information. To enhance the model's robustness and prevent noisy label memorization, this paper proposes a robust Semisupervised Contrastive Learning paradigm called SSCL, which can efficiently merge semi-supervised learning and contrastive learning for combating medical label noise. Specifically, the proposed SSCL framework consists of three well-designed components: the Mixup Feature Embedding (MFE) module, the Semi-supervised Learning (SSL) module, and the Similarity Contrastive Learning (SCL) module. By taking the hybrid augmented images as inputs, the MFE module with momentum update mechanism is designed to mine abstract distributed feature representations. Meanwhile, a flexible pseudo-labeling promotion strategy is introduced into the SSL module, which can refine the supervised information of the noisy data with pseudo-labels based on initial categorical predictions. Benefitting from the measure of similarity between classification distributions, the SCL module can effectively capture more reliable confident pairs, further reducing the effects of label noise on contrastive learning. Furthermore, a noise-robust loss function is also leveraged to ensure the samples with correct labels dominate the learning process. Extensive experiments on multiple benchmark datasets demonstrate the superiority of SSCL over state-of-the-art baselines. The code and pretrained models are publicly available at https://github.com/Binz-Chen/ MICCAI2023 SSCL.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The advancement of deep learning models heavily depends on the availability of large-scale datasets with high-quality annotated labels <ref type="bibr" target="#b3">[4]</ref>. However, it is costly and time-consuming to obtain a sufficient number of accurate annotations from clinical systems, which inevitably introduces a certain level of noisy label <ref type="bibr" target="#b15">[16]</ref>. This phenomenon seriously affects the stability and robustness of medical training and prediction procedures, leading to the production of corrupted representations and inaccurate classification boundaries <ref type="bibr" target="#b26">[27]</ref>. Early approaches for combating the label noise mainly focused on the marginal improvements of model robustness, including designing robust loss functions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>, preprocessing the image <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref>, and estimating the noise transition matrix <ref type="bibr" target="#b7">[8]</ref>. In recent years, some advanced methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref> with semi-supervised learning <ref type="bibr" target="#b21">[22]</ref> are designed to leverage the supervision signal provided by pseudo labels to re-correct the noise bias. With the development of contrastive learning technologies, some scholars <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref> attempt to learn the contrastive representations between the clean and noisy labels by maximizing the similarity of positive pairs and minimizing the similarity of negative pairs. Despite the advances in conventional image processing, few studies have proposed to overcome the medical label noise.</p><p>To overcome this issue, this paper proposes a robust Semi-supervised Contrastive Learning (SSCL) paradigm, that simultaneously benefits from semisupervised learning and contrastive learning for combating the medical noisy labels and promoting stability and robustness of the diagnostic model. Three important components, i.e., the Mixup Feature Embedding (MFE) module, the Semi-supervised Learning (SSL) module, and the Similarity Contrastive Learning (SCL) module, are proposed in the SSCL framework. The architecture of the SSCL framework is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Specifically, the MFE module is built on a multi-branch architecture with the momentum update mechanism, which can effectively capture the abstract distributed feature representations from various levels of mixup augmented images. Based on the confidence scores provided by the initial classifier, a flexible pseudo-labeling promotion strategy is introduced into the SSL module to effectively select confident samples and generate the pseudo-labels, resulting in a more accurate supervision signal reconstruction. By calculating their similarity distribution of representation learning, a novel pairwise selection strategy in the SCL module is designed to efficiently identify and select more reliable confident pairs out of noisy pairs for contrasting learning. In the training phase, we broaden the scope of penalization by incorporating loss functions, further reducing the impact of noise on statistical classification. The main contributions of our work are as follows:</p><p>-This paper presents a robust semi-supervised contrastive learning paradigm that effectively incorporates semi-supervised learning and contrastive learning to mitigate the effect of medical label noise. Our approach represents the first attempt to address this issue in the field of medical image analysis. -The pseudo-labeling promotion strategy can re-correct the supervised information of noisy labels, while the pair-wise selection strategy can guide the confident pairs to dominate the contrasting learning process.</p><p>-The proposed SSCL framework is evaluated on multiple benchmark datasets, and extensive experiments demonstrate the generalization performance of our method in comparison with state-of-the-art baselines.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conventional Methods with Noisy Labels</head><p>To eliminate the memorization effect of noise labels in the training phase, recent works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> were mainly devoted to exploring the effectiveness of robust loss function. Wang et al. <ref type="bibr" target="#b25">[26]</ref> proposed a noise-robust loss function that combined with Cross-Entropy (CE) loss, to address the hard class learning problem and noisy label overfitting problem. Yi et al. <ref type="bibr" target="#b13">[14]</ref> investigated the representational benefits of the contrastive regularization loss function to learn contrastive representations with noisy labels. With continuous in-depth research on image processing, data augmentation has been proven to have a significant role in combating noisy labels. Zhang et al. <ref type="bibr" target="#b27">[28]</ref> presented a data-agnostic and straightforward data augmentation principle to mix up different images geometrically in the feature space, which has been widely used in the field of noise labeling. Moreover, researchers have attempted to leverage the label noise transfer matrix extracted from the data set to solve the noise label problem. Hendrycks et al. <ref type="bibr" target="#b7">[8]</ref> directly used the matrix summarizing the probability of one class being flipped into another under noise. Ramaswamy et al. <ref type="bibr" target="#b17">[18]</ref> proposed an efficient kernel mean embedding to overcome mixture proportion estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-supervised Learning</head><p>Compared with the existing learning methods, semi-supervised learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref> has been recognized as an effective technique to solve the problem of noisy labels. As a semi-supervised learning technique, pseudo-labeling is frequently used in conjunction with confidence-based thresholding to retain unlabeled examples and increase the size of labeled training data. In recent years, some advanced works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19]</ref> demonstrate the capacity of pseudo-label based semi-supervised learning methods in combating medical label noise. Reed et al. <ref type="bibr" target="#b18">[19]</ref> augmented the usual prediction objective with a notion of perceptual consistency, and the article referred to this approach as static hard guidance. Arazo et al. <ref type="bibr" target="#b1">[2]</ref> proposed dynamic hard and soft bootstrapping losses by individual weight of each sample. Furthermore, the combination of semi-supervised learning and pseudo-labels can be used not only for single-label classification but also for negative learning and multi-label classification <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Learning</head><p>With the advancement of deep learning, researchers have found the potential of contrastive-based similarity learning frameworks for representation learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25]</ref>. Unsupervised contrastive learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref> aims to maximize the similarity of positive pairs and minimize the similarity of negative pairs at the instance level. By incorporating clean label information, supervised contrastive learning <ref type="bibr" target="#b12">[13]</ref> can obtain more supervised information and achieve better performance. Recently, many state-of-the-art works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref> have been proposed to make full use of contrastive learning for combating label noise. MOIT <ref type="bibr" target="#b16">[17]</ref> adopted the method of interpolation contrastive learning, and used the supervised information obtained after semi-supervised learning for contrastive learning, so as to reduce the damage of noise labels on contrastive learning. Sel-CL <ref type="bibr" target="#b14">[15]</ref> improved MOIT by introducing the concepts of confident use cases and confident pairs which improved the ability to filter noise labels with new detection strategies. 3 Semi-supervised Contrastive Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mixup Feature Embedding</head><p>Mixup. Let D = {(x i , y i )} N i denotes the training minibatch of image-label pairs x i and y i , where N is the batch size. Initially, the operations of data augmentation are first conducted in the MFE module, to generate various levels of hybrid augmented images with Mixup <ref type="bibr" target="#b27">[28]</ref>. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the obtained hybrid data set involves a group of weak augmentation images, i.e., D k , and two groups of strong augmentation images, i.e., D q and D v . The principle of Mixup can be defined as:</p><formula xml:id="formula_0">x i ← λx a + (1 -λ) x b ,<label>(1)</label></formula><p>where λ ∈ [0, 1]∼Beta(α l , α r ) is used to control the mixing strength of training samples, x a and x b are the training samples randomly drawn from each minibatch, and x i is the enhanced image generated by the mixup preprocessing.</p><p>Feature Embedding. By taking the corresponding augmented images as inputs, the MFE module aims to capture abstract distributed feature representations.</p><p>The MFE module consists of different branches, including a deep encoder with projection and classifier heads, and a momentum encoder with an information bottleneck (IB) <ref type="bibr" target="#b5">[6]</ref>. Motivated from the physical perspective of optimization <ref type="bibr" target="#b6">[7]</ref>, the momentum update mechanism can be defined as:</p><formula xml:id="formula_1">θ v ← mθ v + (1 -m)θ k , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where θ denotes the encoder parameters, and m ∈ [0, 1) is a momentum coefficient. Specifically, the feature representations Q would be mapped to the samedimensional representations Q by the projection head, while K is used to predict the categorical outputs with classifier. The feature representation V is derived from the momentum encoder, possessing identical dimensions to Q. Moreover, we also utilize Kullback-Leibler divergence <ref type="bibr" target="#b10">[11]</ref> to implement the criterion of exploiting IB, resulting in a compact feature representation Ṽ . By alternately learning robust representations from D q and D v , a symmetry objective function is designed to gain the IB loss,</p><formula xml:id="formula_3">L IB = xi∈D q xj ∈D v KL Qi V j + xi∈D v xj ∈D q KL Qi V j . (<label>3</label></formula><formula xml:id="formula_4">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semi-Supervised Learning</head><p>Selecting Confident Samples. The main core of the proposed SSL module is to recognize the confident samples with clean labels, and re-correct the supervision information of label noise. To this end, we first select confident samples based on their confidence score provided by the classifier. Denote the confident samples with clean label belonging to n-th class as</p><formula xml:id="formula_5">D n c , D n c = {(x i , y i ) | y i • p i &gt; γ n }, (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where p i ∈ [0, 1] is the classification probability of the enhanced image x i , and γ n is a dynamic confidence threshold for the n-th class to ensure a class-balanced set of identified confident examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo-Labeling.</head><p>To generate accurate supervision signals, a flexible pseudolabeling promotion strategy is introduced to replace noisy labels with pseudolabels,</p><formula xml:id="formula_7">ỹi = y i , x i ∈ D c , p i , x i / ∈ D c . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>Benefiting from the unique semi-supervised learning structure, our SSL module can effectively reduce the impact of noise based on statistical classification. The objective function for semi-supervised learning is defined as:</p><formula xml:id="formula_9">L SSL = xi∈D 1 -(ỹ i • p i ) ω ω , (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where ω ∈ (0, 1] is a tunable focusing parameter, which is utilized to exploit the benefits of both the noise-robustness and the implicit weighting scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Similarity Contrastive Learning</head><p>Selecting Confident Pairs. To achieve a precise estimation of noisy pairs, a novel pair-wise selection strategy is proposed to identify the reliable confident pairs out of noisy pairs. By calculating their similarity distribution of representation learning, the SCL module can transform identified confident examples into a set of associated confident pairs S without knowing noise rates,</p><formula xml:id="formula_11">S = {(x i , x j ) | p i p j ≥ τ }, (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where τ is considered as a confidence threshold. Therefore, the objective loss on each sample pair for contrastive learning can be defined as:</p><formula xml:id="formula_13">L (x i , x j ) = log 1 -Qi , V j 1l [(x i , x j ) ∈ S] . (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>Consistent with Eq. 3, a symmetry loss function is applied for each minibatch,</p><formula xml:id="formula_15">L SCL = xq∈D q xv∈D v L (x q , x v ) + xv∈D v xq∈D q L (x v , x q ). (<label>9</label></formula><formula xml:id="formula_16">)</formula><p>Queue. It is noted that blindly increasing the size of the minibatch will be limited by computing resources <ref type="bibr" target="#b14">[15]</ref>. To overcome these issues, a queue with the length of L is also introduced into the SCL module, which can maintain a feature dictionary to store features and decouple the dictionary size from the mini-batch size <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>. As the new minibatch is added to the queue, the queue M = {x i , V i } L i=1 is gradually replaced and the oldest features in the queue are removed. The objective loss of the queue is defined as:</p><formula xml:id="formula_17">L QU E = xq∈D q x l ∈M L (x q , x l ) + xv∈D v x l ∈M L (x v , x l ). (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>Objective Loss of SSCL. Based on the analysis of the above modules, the total objective loss for the proposed SSCL method can be obtained by,</p><formula xml:id="formula_19">L SSCL = L SSL + α • L QU E + β • L IB ,<label>(11)</label></formula><p>where α and β are loss weight. In our experiments, both α and β are set to 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details and Settings</head><p>By randomly replacing labels for a percentage of the training data with all possible labels, the proposed SSCL method is extensively validated on four benchmarks with symmetric noise. ISIC-19 <ref type="bibr" target="#b29">[30]</ref> dataset boasts a training set of 20,400 We compare the proposed SSCL method with several state-of-the-art baselines, including three conventional noise-robustness methods (i.e., CE <ref type="bibr" target="#b8">[9]</ref>, GCE <ref type="bibr" target="#b28">[29]</ref>, and Mixup <ref type="bibr" target="#b27">[28]</ref>), three state-of-the-art baselines (i.e., MOIT <ref type="bibr" target="#b16">[17]</ref>, Sel-CL <ref type="bibr" target="#b14">[15]</ref>, and CTRR <ref type="bibr" target="#b13">[14]</ref>). More implementation details are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with the State-of-the-arts</head><p>In this part, we evaluate the performance of the proposed SSCL framework with classification accuracy under different label noise rates (NR). As shown in Table <ref type="table" target="#tab_0">1</ref>, the proposed SSCL significantly outperforms the state-of-the-art baselines on almost all evaluation metrics. For example, our SSCL achieves the highest classification accuracy on ISIC-19 and BUSI datasets, which can verify the effectiveness of our SSCL for medical image analysis with noisy supervision. Especially in the case of higher noise, SSCL has a more powerful capability to capture discriminative and robust features and minimize the effect of noisy labels. When the noise rate is 0.8, the proposed SSCL framework achieves a classification accuracy of 93.0% and 66.5% on CIFA-10 and CIFA-100, surpassing Sel-CL by 3.8% and 6.9%. The comparative results consistently demonstrate the superiority and generalizability of the proposed SSCL framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Analysis and Ablation Studies and Visualizations</head><p>As the key hyperparameter in Eq. 7, the threshold τ is designed to reduce the wrong sample pairs to achieve the best classification boundary construction. In this part, we empirically conduct the proposed SSCL framework with a range of different values τ . As shown in Fig. <ref type="figure" target="#fig_1">2</ref>(a), our SSCL achieves the best performance when τ is increased to 0.8, which can avoid the adverse impact of noisy pairs. Moreover, we also conduct ablation studies by systematically removing each component within the SSCL. In Fig. <ref type="figure" target="#fig_1">2</ref>(b), we can observe that all the modules are necessary for the function of the proposed SSCL framework. To further validate the discriminative power of the SSCL framework, we utilized t-SNE <ref type="bibr" target="#b4">[5]</ref> to visualize the features extracted by vanilla ResNet-18 and SSCL. Compared with vanilla ResNet-18, the visualization results in Fig. <ref type="figure" target="#fig_2">3</ref> and Fig. <ref type="figure" target="#fig_3">4</ref> clearly demonstrate that SSCL has better characteristics for clustering and finer classification boundary, which can demonstrate the effectiveness of SSCL.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper presents a robust and reliable semi-supervised contrastive learning method that benefits greatly from the potential synergistic efficacy of semisupervised learning and contrastive learning, which aims to tackle the challenge of learning with medical noisy labels. By explicitly selecting confident samples and pairs, our approach exhibits a powerful ability to learn discriminative feature representations, mitigating the impact of medical label noise. To demonstrate the effectiveness and versatility of our proposed approach in various practical scenarios, our future works would extend the proposed SSCL method to a broader range of real-world noisy datasets and tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the proposed SSCL framework for combating medical label noise.</figDesc><graphic coords="4,57,48,204,89,337,66,171,28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison of accuracy (%) in (a) parameter analysis and (b) ablation studies on CIFAR-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. T-SNE visualization of the features learned by ResNet and SSCL on ISIC19.</figDesc><graphic coords="8,62,97,487,01,327,04,69,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. T-SNE visualization of the features learned by ResNet and SSCL on CIFAR-10.</figDesc><graphic coords="9,48,30,54,29,327,40,69,40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons with the state-of-the-art baselines on benchmark datasets.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="4">CE Mixup GCE MOIT</cell><cell>Sel-CL</cell><cell>CTRR</cell><cell>SSCL Improv.</cell></row><row><cell>Datasets</cell><cell>NR</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">CVPR'21 CVPR'22 CVPR'22 -</cell><cell>-</cell></row><row><cell>ISIC19</cell><cell>0.2</cell><cell cols="2">75.4 72.2</cell><cell cols="2">75.5 79.7</cell><cell>-</cell><cell>79.9</cell><cell>81.1 1.2 ↑</cell></row><row><cell></cell><cell>0.3</cell><cell cols="2">73.1 70.2</cell><cell cols="2">72.6 75.9</cell><cell>-</cell><cell>75.8</cell><cell>76.7 0.9 ↑</cell></row><row><cell></cell><cell>0.5</cell><cell cols="2">64.1 62.1</cell><cell cols="2">64.8 65.1</cell><cell>-</cell><cell>65.4</cell><cell>66.8 1.4 ↑</cell></row><row><cell></cell><cell cols="3">Mean 70.9 68.2</cell><cell cols="2">71.0 73.6</cell><cell>-</cell><cell>73.7</cell><cell>74.9 1.2 ↑</cell></row><row><cell>BUSI</cell><cell>0.2</cell><cell cols="2">80.6 72.9</cell><cell cols="2">81.9 84.5</cell><cell>-</cell><cell>83.8</cell><cell>86.9 2.4 ↑</cell></row><row><cell></cell><cell>0.3</cell><cell cols="2">76.7 67.8</cell><cell cols="2">77.4 77.2</cell><cell>-</cell><cell>78.3</cell><cell>81.2 2.9 ↑</cell></row><row><cell></cell><cell>0.5</cell><cell cols="2">72.3 66.5</cell><cell cols="2">72.3 73.5</cell><cell>-</cell><cell>72.5</cell><cell>74.9 1.4 ↑</cell></row><row><cell></cell><cell cols="3">Mean 76.5 69.1</cell><cell cols="2">77.2 78.4</cell><cell>-</cell><cell>78.2</cell><cell>81.0 2.6 ↑</cell></row><row><cell cols="2">CIFAR-10 0.2</cell><cell cols="2">82.7 92.3</cell><cell cols="2">86.6 94.1</cell><cell>95.5</cell><cell>93.9</cell><cell>95.0 0.5 ↓</cell></row><row><cell></cell><cell>0.5</cell><cell cols="2">57.9 77.6</cell><cell cols="2">81.9 91.8</cell><cell>93.9</cell><cell>91.7</cell><cell>94.2 0.3 ↑</cell></row><row><cell></cell><cell>0.8</cell><cell cols="2">26.1 46.7</cell><cell cols="2">54.6 81.1</cell><cell>89.2</cell><cell>88.1</cell><cell>93.0 3.8 ↑</cell></row><row><cell></cell><cell cols="3">Mean 55.6 72.2</cell><cell cols="2">74.4 89.0</cell><cell>92.9</cell><cell>91.2</cell><cell>94.1 1.2 ↑</cell></row><row><cell cols="2">CIFAR-100 0.2</cell><cell cols="2">61.8 66.0</cell><cell cols="2">59.2 75.9</cell><cell>76.5</cell><cell>73.8</cell><cell>75.0 1.5 ↓</cell></row><row><cell></cell><cell>0.5</cell><cell cols="2">37.3 46.6</cell><cell cols="2">47.8 70.6</cell><cell>72.4</cell><cell>72.2</cell><cell>73.4 1.0 ↑</cell></row><row><cell></cell><cell>0.8</cell><cell cols="2">8.8 17.6</cell><cell cols="2">15.8 47.6</cell><cell>59.6</cell><cell>63.9</cell><cell>66.5 2.6 ↑</cell></row><row><cell></cell><cell cols="3">Mean 36.0 43.4</cell><cell cols="2">40.9 64.7</cell><cell>69.5</cell><cell>70.0</cell><cell>71.6 1.6 ↑</cell></row><row><cell cols="8">dermoscopic images and a test set of 4,291 images, while BUSI [1] dataset encom-</cell></row><row><cell cols="8">passes 625 ultrasound images in the training set and 155 images in the test set.</cell></row><row><cell cols="8">Both CIFAR-10 and CIFAR-100 contain 50,000 training images and 10,000 test</cell></row><row><cell cols="8">images. The backbone of our SSCL framework is built on ResNet-18. In addition</cell></row><row><cell cols="8">to Mixup, a series of augmentation techniques, such as random flipping, crop-</cell></row><row><cell cols="8">ping, and Gaussian blur, are randomly applied to generate the hybrid augmented</cell></row><row><cell cols="5">images during the training process.</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was supported in part by <rs type="funder">NSFC</rs> fund <rs type="grantNumber">62176077</rs>, in part by <rs type="funder">Guangdong Basic and Applied Basic Research Foundation</rs> under Grant <rs type="grantNumber">2023A1515010057</rs>, in part by <rs type="funder">Shenzhen Science and Technology Program</rs> (Grant No. <rs type="grantNumber">RCYX20221008092852077</rs>), in part by the <rs type="funder">Shenzhen Key Technical Project</rs> under Grant <rs type="grantNumber">2022N001</rs>, in part by the <rs type="funder">Shenzhen Fundamental Research Fund</rs> under Grant <rs type="grantNumber">JCYJ20210324132210025</rs>, and in part by the <rs type="funder">Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies</rs> (<rs type="grantNumber">2022B1212010005</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_9QZ6jEr">
					<idno type="grant-number">62176077</idno>
				</org>
				<org type="funding" xml:id="_ttMZ54F">
					<idno type="grant-number">2023A1515010057</idno>
				</org>
				<org type="funding" xml:id="_WDMWB5g">
					<idno type="grant-number">RCYX20221008092852077</idno>
				</org>
				<org type="funding" xml:id="_txsHzgQ">
					<idno type="grant-number">2022N001</idno>
				</org>
				<org type="funding" xml:id="_tMrRUSC">
					<idno type="grant-number">JCYJ20210324132210025</idno>
				</org>
				<org type="funding" xml:id="_VtmFHDC">
					<idno type="grant-number">2022B1212010005</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dataset of breast ultrasound images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Al-Dhabyani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fahmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Brief</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">104863</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised evidential active learning for diagnostic radiograph classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kassim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>the Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="675" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-label chest x-ray image classification via semantic similarity graph embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circ. Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2455" to="2468" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic-interactive graph convolutional network for multilabel image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Syst</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4887" to="4899" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving generalization by controlling label-noise information in neural network weights</title>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4071" to="4081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using trusted data to train deep networks on labels corrupted by severe noise</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Maga</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal contrastive learning for prospective personalized estimation of CT organ dose</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A Z</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="634" to="643" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kullback-leibler divergence metric learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2047" to="2058" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic bank learning for semi-supervised federated image diagnosis with class imbalance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="196" to="206" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adversarial vertex mixup: toward better adversarially robust generalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="272" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selective-supervised contrastive learning with noisy labels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="316" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1910" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-objective interpolation training for robustness to label noise</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6606" to="6615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mixture proportion estimation via kernel embeddings of distributions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2052" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">In defense of pseudo-labeling: an uncertainty-aware pseudo-label selection framework for semi-supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rizve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Brain-aware replacements for supervised contrastive learning in detection of alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Seyfioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>the Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fixmatch: simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="596" to="608" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">S5cl: unifying fully-supervised, selfsupervised, and semi-supervised learning through hierarchical contrastive learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boxberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Medical Image Computing and Computer Assisted Intervention</title>
		<meeting>the Medical Image Computing and Computer Assisted Intervention</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Alignmixup: improving representations by interpolating aligned features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kijak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="19174" to="19183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contrastive functional connectivity graph learning for population-based fmri classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rekik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Medical Image Computing and Computer Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="221" to="230" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust early-learning: hindering the memorization of noisy labels</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">Mixup: beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04291</idno>
		<title level="m">Learning to bootstrap for combating label noise</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
