<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy</title>
				<funder ref="#_M7jqzvd">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_wVHuuvR">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">O</forename><forename type="middle">León</forename><surname>Barbed</surname></persName>
						</author>
						<author>
							<persName><forename type="first">José</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DIIS-i3A</orgName>
								<orgName type="institution">University of Zaragoza</orgName>
								<address>
									<settlement>Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CVLAB</orgName>
								<orgName type="institution" key="instit2">École Polytechnique Fédérale de Lausanne</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ana</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">DIIS-i3A</orgName>
								<orgName type="institution">University of Zaragoza</orgName>
								<address>
									<settlement>Zaragoza</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tracking Adaptation to Improve SuperPoint for 3D Reconstruction in Endoscopy</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="583" to="593"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">4883E5F122E4FC153BEE97FE7B21A11E</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_56</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>structure from motion</term>
					<term>local features</term>
					<term>endoscopy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Endoscopy is the gold standard procedure for early detection and treatment of numerous diseases. Obtaining 3D reconstructions from real endoscopic videos would facilitate the development of assistive tools for practitioners, but it is a challenging problem for current Structure From Motion (SfM) methods. Feature extraction and matching are key steps in SfM approaches, and these are particularly difficult in the endoscopy domain due to deformations, poor texture, and numerous artifacts in the images. This work presents a novel learned model for feature extraction in endoscopy, called SuperPoint-E, which improves upon existing work using recordings from real medical practice. SuperPoint-E is based on the SuperPoint architecture but it is trained with a novel supervision strategy. The supervisory signal used in our work comes from features extracted with existing detectors (SIFT and SuperPoint) that can be successfully tracked and triangulated in short endoscopy clips (building a 3D model using COLMAP). In our experiments, SuperPoint-E obtains more and better features than any of the baseline detectors used as supervision. We validate the effectiveness of our model for 3D reconstruction in real endoscopy data. Code and model: https://github. com/LeonBP/SuperPointTrackingAdaptation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Endoscopy is an important medical procedure with many applications, from routine screening to detection of early signs of cancer and minimally invasive treatment. Automatic analysis and understanding of these videos raises many opportunities for novel assistive and automatization tasks on endoscopy procedures. Obtaining 3D models from the intracorporeal scenes captured in endoscopies is an essential step to enable these novel tasks and build applications, for example, for improved monitoring of existing patients or augmented reality during training or real explorations.</p><p>3D reconstruction strategies have been studied for long, and one crucial step in these strategies is feature detection and matching which serves as input for Structure from Motion (SfM) pipelines. Endoscopic images are a challenging case for feature detection and matching, due to several well known challenges for these tasks, such as lack of texture, or the presence of frequent artifacts, like specular reflections. These problems are accentuated when all the elements in the scene are deformable, as it is the case in most endoscopy scenarios, and in particular in the real use case studied in our work, the lower gastrointestinal tract explored with colonoscopies. Existing 3D reconstruction pipelines are able to build small 3D models out of short clips from real and complete recordings <ref type="bibr" target="#b0">[1]</ref>. One of the current bottle-necks to obtain better 3D models is the lack of more abundant and higher quality correspondences in real data.</p><p>This work introduces SuperPoint-E, a new model to extract interest points from endoscopic images. We build on the well known SuperPoint architecture <ref type="bibr" target="#b4">[5]</ref>, a seminal work that delivers state-of-the-art results when coupled with downstream tasks<ref type="foot" target="#foot_0">1</ref> . Our main contribution is a novel supervision strategy to train the model. We propose to automatically generate reliable training data from video sequences by tracking feature points from existing detection methods, which do not require training. We select good features with the COLMAP SfM pipeline <ref type="bibr" target="#b20">[21]</ref>, generating training examples with feature points that can be tracked across several images according to COLMAP result. When used to train SuperPoint, our approach yields a self-supervised method outperforming current ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>3D reconstruction is an open problem for laparoscopic and endoscopic settings <ref type="bibr" target="#b13">[14]</ref> of high interest for the community. This idea is supported for example by recent efforts on collecting new public dataset, to further advance in this field, such as endoscopic recordings from EndoSLAM <ref type="bibr" target="#b15">[16]</ref> and EndoMapper <ref type="bibr" target="#b0">[1]</ref> datasets. Earlier works like Grasa et al. <ref type="bibr" target="#b6">[7]</ref> have evaluated the performance of modern SLAM approaches on endoscopic sequences. Mahmoud et al. <ref type="bibr" target="#b12">[13]</ref> improved the performance of such methods in laparoscopic sequences. More recent approaches attempt to tackle specific endoscopy challenges, such as the deformation <ref type="bibr" target="#b17">[18]</ref> or the artifacts due to specular reflections in the feature extraction step <ref type="bibr" target="#b1">[2]</ref>.</p><p>Well known SfM and SLAM pipelines rely on accurate and robust feature extraction methods. COLMAP <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, a public SfM tool, uses SIFT <ref type="bibr" target="#b10">[11]</ref> features while ORB-SLAM <ref type="bibr" target="#b14">[15]</ref> extracts ORB <ref type="bibr" target="#b18">[19]</ref> features because of their efficiency. Both these feature extraction methods count with classical, hand-crafted descriptors that allowed to build such complex applications. However, transferring that performance to endoscopy settings remains a difficult task due to several challenges. Artifacts or the lack of texture result in low amount of correspondences along real endoscopy videos, what motivates the need for improved strategies.</p><p>Deep learning methods for feature extraction and matching is a very active research field. The survey Ma et al. <ref type="bibr" target="#b11">[12]</ref> shows the introduction of deep learning methods to feature detection and matching. Notable mentions are SuperPoint <ref type="bibr" target="#b4">[5]</ref> for its self-supervised approach, R2D2 <ref type="bibr" target="#b16">[17]</ref> for using reliability metrics as output of the network instead of the features themselves and D2-Net <ref type="bibr" target="#b5">[6]</ref> that built a describe-and-detect strategy that aims to improve SfM applicability. Exporting this progress to the matching stage, DISK <ref type="bibr" target="#b23">[24]</ref> proposes a formulation of the problem to optimize in an end-to-end manner. Other recent works have extended the networks to take advantage of the advances in attention for the matching task, as in SuperGlue <ref type="bibr" target="#b19">[20]</ref> and LoFTR <ref type="bibr" target="#b22">[23]</ref>.</p><p>In this work we improve the performance of SuperPoint <ref type="bibr" target="#b4">[5]</ref> on endoscopy images. We chose SuperPoint because it is a seminal work that has inspired many follow up works, and it is still among the top performers on current feature matching challenges <ref type="bibr" target="#b9">[10]</ref>. Similar to DeTone et al. <ref type="bibr" target="#b3">[4]</ref>, we explore improvements on feature extraction that provide good properties for downstream tasks. They design an end-to-end method to optimize the visual odometry computed with their features. Differently, we propose to supervise our training with points that have been successfully used for 3D reconstruction using existing SfM pipelines. With this supervision, we train a model able to extract more features with good properties for SfM algorithms, e.g., being spread and out of large specularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tracking Adaptation for Local Feature Learning</head><p>Superpoint supervision is referred to as Homographic Adaptation and assumes that the surfaces are locally plane, which is not the case in our data. Instead, we propose to use 3D reconstructions of points tracked along image sequences. This makes no assumptions about the local surface shapes and we will show in Sect. 4 that this yields a better trained network. We will refer to this as Tracking Adaptation and we will here describe how we obtain the tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SfM as Supervision for Feature Extraction.</head><p>We generate examples of good features by identifying features that were successfully reconstructed with existing methods for each sequence in our training set. Our training set contains short sequences (4-7 s) from the complete colonoscopy recordings in EndoMapper dataset where COLMAP software was able to obtain a 3D reconstruction. This is a very challenging domain, and existing SfM pipelines fail in longer videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Reconstruction of Training Set Videos.</head><p>We generate 3D reconstructions for all our training sequences with out-of-the-box COLMAP. In particular, we use the following blocks: feature extractor, exhaustive matcher and mapper. Configuration parameters are detailed in the supplementary materials. We turn on the "guided matching" option for the exhaustive matcher module to find the best matches possible. We additionally compute the 3D reconstruction for the same sequences with a modified COLMAP pipeline that uses the official Super-Point and SuperGlue<ref type="foot" target="#foot_1">2</ref> implementation with the indoor set of weights. All the parameters are left as default except for the keypoint threshold= 0.015 and the nms radius= 1. After providing the SuperGlue resulting matches to COLMAP, we execute only the mapper module with the same configuration as before. Re-project Good Features to the Training Set Frames. A successful 3D reconstruction includes the computed positions of the cameras that took the images and a point cloud with 3D coordinates of the triangulated points. We use the camera poses, the points' coordinates and the camera calibration parameters to reproject the 3D point cloud points into every image. Not all points were originally detected and triangulated at all frames, so we establish two types of reprojected points. If they were "originally" detected and matched in a particular image, we set them to green. Otherwise, we set them to blue (see Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>For supervision, we only use reprojected points that fall within a reliable track. A reliable track is an interval bounded by green points. So, the reprojected points selected for training are either green or have preceding and subsequent green points along its track.</p><p>The different appearances of the same 3D point in different frames of the track are our correspondences for training our models. Figure <ref type="figure" target="#fig_0">1</ref> contains examples of reprojected points and an example of a reliable track.</p><p>Deep Feature Extraction for Endoscopy. SuperPoint uses a fully-convolutional network as backbone and learns to extract good features using homographic adaptation: extracting features that are robust to homographic deformations. It achieves this by using as supervision Y the average detections over several random homographic deformations of the same image. The feature extraction network then is run on an image I and a warped version I of it with a new homography. The network optimizes the loss function</p><formula xml:id="formula_0">LSP X , X , D, D ; Y, Y , S = Lp (X , Y ) + Lp X , Y + λL d D, D , S ,<label>(1)</label></formula><p>where X and D are the detection and description heads' outputs, respectively. Y is the supervision for the detection. S is the correspondence between I and I computed from the homography. L p is the detection loss that measures the discrepancies between the supervision Y and the detection head's output X . λ = 1 is a weighting parameter. L d is the description loss that measures the discrepancies between both description head's outputs D and D using S.</p><p>Using our new supervision from SfM in the form of tracks of points, we propose a new loss to train SuperPoint that is more aligned with our goal, called tracking adaptation. Instead of an image I and a warped version I , we use different images I a and I b from the same sequence. The supervision Y for the detection in this case is the set of points that have been reprojected on I a and I b from the 3D reconstruction. The detection loss L p is calculated as in the original SuperPoint. We replace the description loss L d for a new tracking loss</p><formula xml:id="formula_1">L t (D a , D b , T ) = 1 |T | 2 |T | i=1 |T | j=1 l t d ai , d bj , i, j ,<label>(2)</label></formula><formula xml:id="formula_2">with l t d ai , d bj , i, j = λ t max(0, m p -d T ai d bj ) if i = j, max(0, d T ai d bj -m n ) if i = j ,<label>(3)</label></formula><p>where D a and D b are the description head's outputs for I a and I b , respectively. T is the set of all the tracks that appear in both images. l t is a common triplet loss that measures the distance between positive pairs (weighting parameter λ t = 1 and positive margin m p = 1) and the distance between negative pairs (negative margin m n = 0.2). Two descriptors from different images d ai and d bj are a positive pair if they belong to the same track (i = j), and negative pair otherwise (i = j).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The following experiments demonstrate the proposed feature detection efficacy to obtain 3D models on real colonoscopy videos, comparing different variations of our approach and relevant baseline methods.</p><p>Dataset. We seek techniques that are applicable to real medical data, so we train and evaluate with subsequences from the EndoMapper dataset <ref type="bibr" target="#b0">[1]</ref>, which contains a hundred complete endoscopy recordings obtained during regular medical practice. We use COLMAP 3D reconstructions obtained from subsequences from this dataset (11260 frames from 65 reconstructions obtained along 14 different videos for training, and 838 frames from 7 reconstructions from 6 different videos for testing). The exact details are in the supplementary material.</p><p>Baselines and our Variations. We use COLMAP as our first baseline. It uses SIFT features and a standard guided matching algorithm to produce very accurate camera pose estimates. We also include as baseline the results of SuperPoint (SP) with SuperGlue matches and the COLMAP reconstruction module. The configuration for both baselines is the same as detailed in Sect. 3. We evaluate different variations of the original SuperPoint. All models were trained with a modification of a PyTorch implementation of SuperPoint <ref type="bibr" target="#b8">[9]</ref>. Training parameters in supplementary material. The models differ in the supervision used and the loss applied in the training, as detailed in the first four columns of Table <ref type="table" target="#tab_0">1</ref>. Ablation Study. Table <ref type="table" target="#tab_0">1</ref> (last five columns) summarizes the performance of our approach variations. We run all the models on the Test set subsequences to extract points. Matches between the points in two images are obtained with bi-directional nearest neighbor algorithm with L2 distance. Points and matches are given to COLMAP and the mapper module (configuration in supplementary material) attempts to generate a 3D reconstruction. The reconstruction quality statistics used to illustrate the performance of each detector are:</p><p>-3DIm : Fraction of images from the subsequence successfully introduced in the reconstruction. The closer to 100% the better. -3DP ts : Number of points that were successfully reconstructed. The more points the better, since it means a denser coverage of the scene. -Err: Mean reprojection error of the 3D points after being reprojected onto the images of the subsequence. -Err-10K: Mean reprojection error of the best 10000 points of the reconstruction. Since all reconstructions have outliers that skew the average, this metric is more representative of the performance of the models.</p><p>-len(Tr): Mean track length represents the average number of images where a point is being consecutively matched, tracked.</p><p>SP-E v2 (SP-E moving forward) is our best variation, with the highest amount of reconstructed points and the lowest reprojection error for top 10000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SfM Results</head><p>Comparison. This experiment compares the performance of the considered baselines against the best configuration of our feature extraction model. Table <ref type="table" target="#tab_1">2</ref> contains a summary of the results. In most metrics we observe a significant improvement using SP-E compared to the others. For example, the number of points at the final reconstruction is more than three times higher (see  the example in Fig. <ref type="figure" target="#fig_1">2</ref>). The mean reprojection error of all the points is the lowest for SIFT, possibly due to it being more restrictive in all other aspects (number of images reconstructed, number of points, track length). However, the mean error for the top 10000 points is always lower for SP-E. The reprojection error plots in Figs. <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref> provide more insight on this metric. Figures <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref> show a more detailed visualization of two representative reconstructions, including a summary of the sequence frames, the point cloud obtained by each method and a plot of the reprojection error for each point in the reconstruction, sorted in increasing error value. Note that even though SP-E obtains many more points, it is not at the cost of quality. Figure <ref type="figure" target="#fig_2">3</ref> shows a scenario where SIFT fails to reconstruct a large part of the subsequence, because it fails on the feature matching on the darker frames depicted in the middle of the sequence. Note how the reconstruction from SP-E is notably denser than the others. Figure <ref type="figure" target="#fig_3">4</ref> shows a scenario where all approaches perform well and SIFT achieves the lowest reprojection error.  We analyze additional aspects of our detected features to showcase the higher quality with respect to other methods in Table <ref type="table" target="#tab_3">3</ref>. To measure the spread of the features over the images we defined a 16 × 16 grid over each image and computed the percentage of those cells that have at least one reconstructed point. We also measure how many extracted points fall on top of specularities (we consider a pixel as part of a specularity if its intensity is higher than 180). For both metrics, our detector achieves significantly better results, showcasing the better properties of our detector for 3D reconstruction.</p><p>To provide quantitative evaluation of the camera motion estimation, we use a simulated dataset <ref type="bibr" target="#b2">[3]</ref> to have ground truth available for the camera trajectory. We took 5 sequences of 100-150 frames from this dataset, and we tested the baselines and our model. We align the ground truth trajectories with the reconstructed ones with Horn's method <ref type="bibr" target="#b7">[8]</ref>. SP only reconstructed 3 out of the 5 sequences while SIFT and SP-E correctly reconstructed the 5 sequences, with an average RMSE of 4.61mm and 4.71 mm respectively. Simulated data lacks some of the biggest challenges of endoscopy images (e.g. specularities, deformations), but this experiment suggests that the camera motion estimation quality is similarly good for all methods when they manage to converge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This work presents a novel training strategy for SuperPoint to improve its performance in SfM from endoscopy images. This strategy has two main benefits: we show how to use 3D reconstructions of endoscopy sequences as supervision to train feature extraction models; and we design a new tracking loss to perform tracking adaptation using this supervision. The benefits of our method are explored with an ablation study and against established baselines on SfM and feature extraction. Our proposed model is able to obtain more suitable features for 3D reconstruction, and to reconstruct larger sets of images with much denser point clouds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Supervision points obtained from a COLMAP reconstruction. (a) All 3D points are reprojected into each video frame. We distinguish points that were originally detected in this frame (green) and points that were not (blue). (b-d) analyze a complete point track, i.e., all the positions of the same 3D point along the sequence. The reliable track for this point is the green segment. (b) The track starts when a point is first detected. (c) Movement of the point along the video. (d) When the feature is not detected anymore (e.g., because of occlusion), it is depicted in blue from then on. (Color figure online)</figDesc><graphic coords="4,83,31,136,97,257,56,58,12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of the points reconstructed by each method. Each point in each image has been reconstructed after the corresponding COLMAP reconstruction process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of reconstructions obtained on Seq 017 1 by SIFT, SP, and our best model SP-E. The plot shows the reprojection error of each point reconstructed.</figDesc><graphic coords="8,44,31,366,23,335,92,108,88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of reconstructions obtained on Seq 095 1 by SIFT, SP, and our best model SP-E. The plot shows the reprojection error of each point reconstructed.</figDesc><graphic coords="9,59,46,219,86,333,40,107,92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Ablation study. Configuration of the training (left), and average reconstruction results, i.e., quality metrics (right). Best results highlighted in bold.</figDesc><table><row><cell></cell><cell cols="3">Supervision &amp; Train Config.</cell><cell cols="2">Reconstruction Test Results</cell></row><row><cell></cell><cell>point</cell><cell>match</cell><cell>loss</cell><cell>3DIm</cell><cell cols="2">3DP ts Err Err-10K len (Tr)</cell></row><row><cell>SP [5]</cell><cell>SP-O</cell><cell>H</cell><cell cols="2">SP (original) 93.9%</cell><cell>6421.3 1.47 1.47</cell><cell>6.86</cell></row><row><cell cols="2">SP-E v0 SF*</cell><cell>H</cell><cell cols="2">SP (original) 97.3%</cell><cell>12707.9 1.66 1.50</cell><cell>8.39</cell></row><row><cell cols="2">SP-E v1 SF*</cell><cell>TR</cell><cell>tr-2</cell><cell>98.6%</cell><cell>13255.1 1.69 1.51</cell><cell>8.95</cell></row><row><cell cols="3">SP-E v1 SF*+SP* TR</cell><cell>tr-2</cell><cell>99.1%</cell><cell>28308.3 1.74 1.13</cell><cell>9.45</cell></row><row><cell cols="3">SP-E v2 SF*+SP* TR</cell><cell>tr-N</cell><cell>99.1%</cell><cell>34838.0 1.75 1.02</cell><cell>9.53</cell></row><row><cell cols="4">SP-E v3 SF*+SP* H + TR SP + tr-N</cell><cell>99.2%</cell><cell>30777.6 1.74 1.09</cell><cell>9.65</cell></row></table><note><p>point (Base point detector): SP-O: original superpoint detector; SF*/SP*: SIFT/SP points that were successfully reconstructed after the COLMAP optimization, reprojected in each video frame. match (Matches Supervision): H: Homography based, i.e., Homographic adaptation from original SuperPoint work; TR: The proposed Tracking adaptation. loss (Loss used for training): SP: original SuperPoint training loss; Tr-2 or Tr-N: track -based loss. Tr-2 means that the loss is computed for every pair of images in the track. Tr-N means we optimize simultaneously N views of the track (N=4 in our experiments).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Reconstruction quality metrics for the comparison to the baselines.</figDesc><table><row><cell cols="9">Subsequence 001 1 002 1 014 1 016 1 017 1 095 1 095 2 Avg</cell><cell>(Std)</cell></row><row><cell cols="4">Reconstructed images ( 3DIm )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Total +</cell><cell>107</cell><cell>155</cell><cell>109</cell><cell>119</cell><cell>125</cell><cell>118</cell><cell>105</cell><cell>119.7</cell><cell>(15.9)</cell></row><row><cell>SIFT</cell><cell cols="8">98.1% 91.6% 71.6% 100% 52.0% 97.5% 99.0% 87.1%</cell><cell>(17.0)</cell></row><row><cell>SP</cell><cell cols="8">100% 100% 93.6% 100% 89.6% 99.2% 100% 97.5%</cell><cell>(3.9)</cell></row><row><cell cols="9">SP-E (Ours) 100% 100% 93.6% 100% 100% 100% 100% 99.1%</cell><cell>(2.3)</cell></row><row><cell cols="4">Reconstructed points ( 3DP ts )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIFT</cell><cell>13470</cell><cell cols="2">6599 26225</cell><cell>5700</cell><cell>2505</cell><cell>7666</cell><cell cols="3">9608 10253.3 (7237.6)</cell></row><row><cell>SP</cell><cell>12941</cell><cell cols="2">9057 17451</cell><cell>6489</cell><cell>4093</cell><cell cols="4">8911 12535 10211.0 (4133.1)</cell></row><row><cell>SP-E (Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>34851 45471 42727 33277 36403 19286 31851 34838.0 (</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>7846.5)</cell></row><row><cell cols="3">Mean reprojection error (Err)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIFT</cell><cell>1.34</cell><cell>1.38</cell><cell>0.95</cell><cell>1.45</cell><cell>1.40</cell><cell>1.30</cell><cell>1.34</cell><cell>1.31</cell><cell>(0.15)</cell></row><row><cell>SP</cell><cell>1.52</cell><cell>1.49</cell><cell>1.38</cell><cell>1.58</cell><cell>1.48</cell><cell>1.51</cell><cell>1.51</cell><cell>1.50</cell><cell>(0.06)</cell></row><row><cell cols="2">SP-E (Ours) 1.69</cell><cell>1.68</cell><cell>1.71</cell><cell>1.90</cell><cell>1.73</cell><cell>1.81</cell><cell>1.75</cell><cell>1.75</cell><cell>(0.07)</cell></row><row><cell cols="7">Mean reprojection error of the best 10K points* (Err-10K)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIFT</cell><cell>1.08</cell><cell>1.38</cell><cell>0.46</cell><cell>1.45</cell><cell>1.40</cell><cell>1.30</cell><cell>1.34</cell><cell>1.20</cell><cell>(0.32)</cell></row><row><cell>SP</cell><cell>1.30</cell><cell>1.49</cell><cell>1.00</cell><cell>1.58</cell><cell>1.48</cell><cell>1.51</cell><cell>1.30</cell><cell>1.38</cell><cell>(0.19)</cell></row><row><cell cols="2">SP-E (Ours) 0.92</cell><cell>0.73</cell><cell>0.84</cell><cell>1.30</cell><cell cols="2">0.91 1.41</cell><cell>1.06</cell><cell>1.02</cell><cell>(0.23)</cell></row><row><cell cols="3">Mean track length (len(Tr))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SIFT</cell><cell>6.57</cell><cell>5.73</cell><cell cols="3">10.88 12.48 7.74</cell><cell cols="2">12.88 7.56</cell><cell>9.12</cell><cell>(2.70)</cell></row><row><cell>SP</cell><cell>5.54</cell><cell>4.52</cell><cell>7.86</cell><cell>8.73</cell><cell>5.16</cell><cell>8.20</cell><cell>5.38</cell><cell>6.49</cell><cell>(1.59)</cell></row><row><cell cols="2">SP-E (Ours) 7.05</cell><cell>6.78</cell><cell>9.63</cell><cell cols="2">14.73 8.42</cell><cell cols="2">11.29 8.78</cell><cell>9.53</cell><cell>(2.55)</cell></row></table><note><p>+ Total number of images in the subsequence. * If 10K points are not available, average is computed over all available reconstructed points.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Analysis of the feature locations for each method.Spread of features ↑ % of features on specularities ↓</figDesc><table><row><cell>SIFT</cell><cell>43.9%</cell><cell>28.6%</cell></row><row><cell>SP</cell><cell>56.9%</cell><cell>19.6%</cell></row><row><cell>SP-E (Ours)</cell><cell>67.5%</cell><cell>9.9%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.cs.ubc.ca/research/image-matching-challenge/2021/leaderboard/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/magicleap/SuperGluePretrainedNetwork.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This project has been funded by the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under grant agreement No <rs type="grantNumber">863146</rs> and <rs type="person">Aragón Government</rs> project <rs type="grantNumber">T45 23R</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wVHuuvR">
					<idno type="grant-number">863146</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_M7jqzvd">
					<idno type="grant-number">T45 23R</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 56.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Azagra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14240</idno>
		<title level="m">Endomapper dataset of complete calibrated endoscopy procedures</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Superpoint features in endoscopy</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Barbed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chadebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morlana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-21083-9_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-21083-95" />
	</analytic>
	<monogr>
		<title level="m">Imaging Systems for GI Endoscopy, and Graphs in Biomedical Image Analysis: First MICCAI Workshop, ISGIE 2022, and Fourth MICCAI Workshop, GRAIL 2022, Held in Conjunction with MICCAI 2022, Singapore</title>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-18">18 September 2022. 2022</date>
			<biblScope unit="page" from="45" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Bobrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Golhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Akshintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Durr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08903</idno>
		<title level="m">Colonoscopy 3d video dataset with paired depth from 2d-3d registration</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03245</idno>
		<title level="m">Self-improving visual odometry</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Superpoint: self-supervised interest point detection and description</title>
		<author>
			<persName><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">D2-net: a trainable cnn for joint description and detection of local features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dusmanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual slam for handheld monocular endoscope</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">G</forename><surname>Grasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Casado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Montiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Closed-form solution of absolute orientation using unit quaternions</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josa a</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="629" to="642" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep keypoint-based camera pose estimation with geometric constraints</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Jau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<ptr target="https://github.com/eric-yyjau/pytorch-superpoint" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image matching across wide baselines: from paper to practice</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="517" to="547" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image matching from handcrafted to deep features: a survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="1" to="57" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Live tracking and dense reconstruction for handheld monocular endoscopy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hostettler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Soler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="89" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optical techniques for 3d surface reconstruction in computerassisted laparoscopic surgery</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="974" to="996" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Rob</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Endoslam dataset and an unsupervised monocular visual odometry and depth estimation approach for endoscopic videos</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Ozyoruk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">102058</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">R2D2: repeatable and reliable detector and descriptor</title>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tracking monocular camera pose and deformation for slam inside the human body</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J G</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5278" to="5285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ORB: an efficient alternative to sift or surf</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Superglue: learning feature matching with graph neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46487-9_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46487-931" />
	</analytic>
	<monogr>
		<title level="m">ECCV 2016</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9907</biblScope>
			<biblScope unit="page" from="501" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Loftr: detector-free local feature matching with transformers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disk: learning local features with policy gradient</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tyszkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="14254" to="14265" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
