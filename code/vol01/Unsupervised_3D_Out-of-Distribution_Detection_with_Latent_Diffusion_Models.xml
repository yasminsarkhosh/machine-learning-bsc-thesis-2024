<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models</title>
				<funder>
					<orgName type="full">InnovateUK-funded London AI centre for Value-based Healthcare</orgName>
				</funder>
				<funder ref="#_esW238r">
					<orgName type="full">MRC</orgName>
				</funder>
				<funder ref="#_Fqp2xZA">
					<orgName type="full">Wellcome/EPSRC Centre for Medical Engineering</orgName>
				</funder>
				<funder ref="#_PjqJPqb">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_p9fp8bh">
					<orgName type="full">Wellcome Trust</orgName>
				</funder>
				<funder ref="#_52BE9wd">
					<orgName type="full">UCLH NIHR Biomedical Research Centre</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Graham</surname></persName>
							<email>mark.graham@kcl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><forename type="middle">Hugo Lopez</forename><surname>Pinaya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Wright</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Petru-Daniel</forename><surname>Tudosiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yee</forename><forename type="middle">H</forename><surname>Mah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">King&apos;s College Hospital NHS Foundation Trust</orgName>
								<address>
									<addrLine>Denmark Hill</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Teo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">King&apos;s College Hospital NHS Foundation Trust</orgName>
								<address>
									<addrLine>Denmark Hill</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Psychiatry, Psychology and Neuroscience</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">Rolf</forename><surname>Jäger</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute of Neurology</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Werring</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Stroke Research Centre</orgName>
								<orgName type="institution">UCL Queen Square Institute of Neurology</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Parashkev</forename><surname>Nachev</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Institute of Neurology</orgName>
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastien</forename><surname>Ourselin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">Jorge</forename><surname>Cardoso</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Biomedical Engineering and Imaging Sciences</orgName>
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised 3D Out-of-Distribution Detection with Latent Diffusion Models</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="446" to="456"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">11105109C410310D4D6A4B12B6FDAAB5</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_43</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Latent diffusion models • Out-of-distribution detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Methods for out-of-distribution (OOD) detection that scale to 3D data are crucial components of any real-world clinical deep learning system. Classic denoising diffusion probabilistic models (DDPMs) have been recently proposed as a robust way to perform reconstruction-based OOD detection on 2D datasets, but do not trivially scale to 3D data. In this work, we propose to use Latent Diffusion Models (LDMs), which enable the scaling of DDPMs to high-resolution 3D medical data. We validate the proposed approach on near-and far-OOD datasets and compare it to a recently proposed, 3D-enabled approach using Latent Transformer Models (LTMs). Not only does the proposed LDM-based approach achieve statistically significant better performance, it also shows less sensitivity to the underlying latent representation, more favourable memory scaling, and produces better spatial anomaly maps. Code is available at https://github.com/marksgraham/ddpm-ood.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Methods for out-of-distribution (OOD) detection are a crucial component of any machine learning pipeline that is deployed in the real world. They are particularly necessary for pipelines that employ neural networks, which perform well on data drawn from the distribution they were trained on but can produce unexpected results when given OOD data. For medical applications, methods for OOD detection must be able to detect both far-OOD data, such as images of a different organ or modality to the in-distribution data, and near-OOD data, such as in-distribution data corrupted by imaging artefacts. It is also necessary that these methods can operate on high-resolution 3D data. In this work, we focus on methods trained in a fully unsupervised way; without any labels or access to OOD data at train time.</p><p>Recently, Latent Transformer Models (LTMs) <ref type="bibr" target="#b8">[9]</ref> have proven themselves to be effective for anomaly detection and synthesis in medical data <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27]</ref>. These two-stage models first use a VQ-VAE <ref type="bibr" target="#b19">[20]</ref> or VQ-GAN <ref type="bibr" target="#b8">[9]</ref> to provide a compressed, discrete representation of the imaging data. An autoregressive Transformer <ref type="bibr" target="#b28">[29]</ref> can then be trained on a flattened sequence of this representation. LTMs are particularly valuable in medical data, where the high input size makes training a Transformer on raw pixels infeasible. Recently, these models have been shown to be effective for 3D OOD detection by using the Transformer's likelihood of the compressed sequence to identify both far-and near-OOD samples <ref type="bibr" target="#b10">[11]</ref>. These models can also provide spatial anomaly maps that highlight the regions of the image considered to be OOD, particularly valuable for highlighting localised artefacts in near-OOD data.</p><p>However, LTMs have some disadvantages. Firstly, likelihood models have well documented weaknesses when used for OOD detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>, caused by focusing on low-level image features <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. It can help to measure likelihood in a more abstract representation space, such as that provided by a VQ-VAE <ref type="bibr" target="#b7">[8]</ref>, but how to train models that provide optimal representations for assessing likelihood is still an open research problem. For example, <ref type="bibr" target="#b10">[11]</ref> showed in an ablation study that LTMs fail at OOD when lower levels of VQ-VAE compression are used. Secondly, the memory requirements of Transformers mean that even with high compression rates, the technique cannot scale to very high-resolution medical data, such as a whole-body CT with an image dimension 512 3 . Finally, the spatial anomalies maps produced by LTMs are low resolution, being in the space of the latent representation rather than that of the image itself.</p><p>A promising avenue for OOD detection is denoising diffusion probabilistic model (DDPM)-based OOD detection <ref type="bibr" target="#b9">[10]</ref>. This approach works by taking the input images and noising them multiple times to different noise levels. The model is used to denoise each of these noised images, which are compared to the input; the key idea is that the model will only successfully denoise in-distribution (ID) data. The method has shown promising results on 2D data <ref type="bibr" target="#b9">[10]</ref> but cannot be trivially extended to 3D; as even extending DDPMs to work on high-resolution 2D data is an area of active research. We propose to scale it to 3D volumetric data through the use of Latent Diffusion Models (LDMs). These models, analogous to LTMs, use a first-stage VQ-GAN to compress the input. The DDPM then learns to denoise these compressed representations, which are then decoded and their similarity to the input image is measured directly in the original image space.</p><p>The proposed LDM-based OOD detection offers the potential to address the three disadvantages of an LTM-based approach. Firstly, as the method is not likelihood based, it is not necessary that the VQ-GAN provides an ill-defined 'good representation'. Rather, the only requirement is that it reconstructs the inputs well, something easy to quantify using reconstruction quality metrics. Secondly, DDPMs have more favourable memory scaling behaviour than Transformers, allowing them to be trained on higher-dimensional representations. Finally, as the comparisons are performed at the native resolution, LDMs can produce high-resolution spatial anomaly maps. We evaluate both the LTM and the proposed LDM model on several far-and near-OOD detection tasks and show that LDMs overcome the three main failings of LTMs: that their performance is less reliant on the quality of the first stage model, that they can be trained on higher dimensional inputs, and that they produce higher resolution anomaly maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We begin with a brief overview of LDMs and relevant notation before describing how they are used for OOD detection and to estimate spatial anomaly maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Latent Diffusion Models</head><p>LDMs are trained in two stages. A first stage model, here a VQ-GAN, is trained to compress the input image into a latent representation. A DDPM <ref type="bibr" target="#b13">[14]</ref> is trained to learn to sample from the distribution of these latent representations through iterative denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VQ-GAN:</head><p>The VQ-GAN operates on a 3D input of size x ∈ R H×W ×D and consists of an encoder E that compresses to a latent space z ∈ R h×w×d×n , where n is the dimension of the latent embedding vector. This representation is quantised by looking up the nearest value of each representation in a codebook containing K elements and replacing the embedding vector of length d with the codebook index, k, producing z q ∈ R h×w×d . A decoder G operates on this quantised representation to produce a reconstruction, x ∈ R H×W ×D .</p><p>In a VQ-VAE <ref type="bibr" target="#b19">[20]</ref>, E, G and the codebook are jointly learnt with a L 2 loss on the reconstructions and a codebook loss. The VG-GAN <ref type="bibr" target="#b8">[9]</ref> aims to produce higher quality reconstructions by employing a discriminator D and training adversarially, and including a perceptual loss component <ref type="bibr" target="#b31">[32]</ref> in addition to the L 2 reconstruction loss. Following <ref type="bibr" target="#b27">[28]</ref>, we also add a spectral loss component to the reconstruction losses <ref type="bibr" target="#b6">[7]</ref>.</p><p>The encoder and decoder are convolutional networks of l levels. There is a simple relationship between the spatial dimension of the latent space, the input, and number of levels: h, w, d = H 2 l , W 2 l , D 2 l , so the latent space is 2 3l times smaller spatially than the input image, with a 4 × 2 3l reduction in memory size when accounting for the conversion from a float to integer representation. In practice, most works use l = 3 (512× spatial compression) or l = 4 (4096× spatial compression); it is challenging to train a VQ-GAN at higher compression rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DDPM:</head><p>A DDPM is then trained on the latent embedding z (the de-quantised latent). During training, noise is added to z according to a timestep t and a fixed Gaussian noise schedule defined by β t to produce noised samples z t , such that</p><formula xml:id="formula_0">q(z t |z 0 ) = N z t | √ ᾱt z 0 , (1 -ᾱ)I (1)</formula><p>where we use z 0 to refer to the noise-free latent z, we have 0 ≤ t ≤ T , and α t := 1β t and ᾱt := t s=1 α s . We design β t to increase with t such that the latent z T is close to an isotropic Gaussian. We seek to train a network that can perform the reverse or denoising process, which can also be written as a Gaussian transition:</p><formula xml:id="formula_1">p θ (z t-1 |z t ) = N (z t-1 |μ θ (z t , t), Σ θ (z t , t)) (2)</formula><p>In practice, following <ref type="bibr" target="#b13">[14]</ref>, we can train a network θ (z t , t) to directly predict the noise used in the forward noising process, . We can train with a simplified loss L simple (θ) = E t,z0, θ (z t ) 2 , and denoise according to</p><formula xml:id="formula_2">z t-1 = 1 √ α t z t - β t √ 1 -ᾱt θ (z t , t) + σ t n<label>(3)</label></formula><p>where n ∼ N (0, I).</p><p>While in most applications an isotropic Gaussian is drawn and iteratively denoised to draw samples from the model, in this work, we take a latent input z 0 and noise to z t for a range of values of t &lt; T and obtain their reconstructions, ẑ0,t = p θ (z 0 |z t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">OOD Detection with LDMs</head><p>In <ref type="bibr" target="#b9">[10]</ref>, an input image x that has been noised to a range of t-values spanning the range 0 &lt; t &lt; T is then denoised to obtain x0,t , and we measure the similarity for each reconstruction, S(x 0,t , x). These multiple similarity measures are then combined to produce a single score per input, with a high similarity score suggesting the input is more in-distribution. Typically, reconstruction methods work by reconstruction through some information bottleneck -for an autoencoder, this might be the dimension of the latent space; for a denoising model, this is the amount of noise applied -with the principal that ID images will be successfully reconstructed through the bottleneck, yielding high similarity with the input, and OOD images will not. Prior works have shown the performance becomes dependent on the choice of the bottleneck -too small and even ID inputs are poorly reconstructed, too large and OOD inputs are well reconstructed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>. Reconstructing from multiple t-values addresses this problem by considering reconstructions from multiple bottlenecks per image, outperforming prior reconstruction-based methods <ref type="bibr" target="#b9">[10]</ref>.</p><p>In order to scale to 3D data, we reconstruct an input x in the latent space of the VQ-GAN, z = E (x). Reconstructions are performed using the PLMS sampler <ref type="bibr" target="#b16">[17]</ref>, which allows for high-quality reconstructions with significantly fewer reconstruction steps. The similarity is measured in the original image space by decoding the reconstructed latents, S (G (ẑ 0,t ) , x). As recommended by <ref type="bibr" target="#b9">[10]</ref>, we measure both the mean-squared error (MSE) and the perceptual similarity <ref type="bibr" target="#b31">[32]</ref> for each reconstruction, yielding a total of 2N similarity measures for the N reconstructions performed. As the perceptual loss operates on 2D images, we measure it on all slices in the coronal, axial, and sagittal planes and average these values to produce a single value per 3D volume. Each similarity metric is converted into a z-score using mean and standard deviation parameters calculated on the validation set, and are then averaged to produce a single score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Spatial Anomaly Maps</head><p>To highlight spatial anomalies, we aggregate a set of reconstruction error maps. We select reconstructions from t-values = [100, 200, 300, 400], calculate the pixelwise mean absolute error (MAE), z-score these MAE maps using the pixelwise mean and standard deviation from the validation set, and then average to produce a single spatial map per input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>We use three datasets to test the ability of our method to flag OOD values in both the near-and far-OOD cases. The CROMIS dataset <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> consists of 683 head CT scans and was used as the train and validation set for all models, with a 614/69 split. The KCH dataset consists of 47 head CTs acquired independently from CROMIS, and was used as the in-distribution test set. To produce near-OOD data, a number of corruptions were applied to this dataset, designed to represent a number of acquisition/ data preprocessing errors. These were: addition of Gaussian noise to the images at three levels (σ = 0.01, 0.1, 0.2), setting the background to values different to the 0 used during training (0.3, 0.6, 1), inverting the image through either of the three imaging planes, removing a chunk of adjacent slices from either the top or centre of the volume, skullstripping (the models were trained on unstripped images), and setting all pixel values to either 1% or 10% of their true values (imitating an error in intensity scaling during preprocessing). Applying each corruption to each ID image yielded a total of 705 near-OOD images. The Decathlon dataset <ref type="bibr" target="#b0">[1]</ref> comprises a range of 3D imaging volumes that are not head CTs and was used to represent far-OOD data. We selected 22 images from each of the ten classes. All CT head images were affinely registered to MNI space, resampled to 1 mm isotropic, and cropped to a 176 × 208 × 176 grid. For the images in the Decathlon dataset, all were resampled to be 1mm isotropic and either cropped or zero-padded depending on size to produce a 176 ×216 ×176 grid. All CT images had their intensities clamped between [-15,100] and then rescaled to lie in the range [0, 1]. All non-CT images were rescaled based on their minimum and maximum values to lie in the [0, 1] range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>All models were implemented in PyTorch v1.13.1 using the MONAI framework v1.1.0 <ref type="bibr" target="#b1">[2]</ref>. Code is available at https://github.com/marksgraham/ddpmood. LTM model code can be found at https://github.com/marksgraham/ transformer-ood.</p><p>LDMs: VQ-GANS were trained with levels l = 2, 3, or 4 levels with 1 convolutional layer and 3 residual blocks per level, each with 128 channels. Training with l = 3/4 represents standard practice, training with l = 2 (64× spatial compression) was done to simulate a situation with higher-resolution input data. All VQ-GANs had an embedding dim of 64, and the 2, 3, 4 level models have a codebook size of 64, 256, 1024, respectively. Models were trained with a perceptual loss weight of 0.001, an adversarial weight loss of 0.01, and all other losses unweighted. Models were trained with a batch size of 64 for 500 epochs on an A100, using the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> with a learning rate of 3 × 10 -4 and early stopping if the validation loss did not decrease over 15 epochs. The LDM used a time-conditioned UNet architecture as in <ref type="bibr" target="#b24">[25]</ref>, with three levels with (128, 256, 256) channels, 1 residual block per level, and attention in the deepest level only. The noise schedule had T = 1000 steps with a scaled linear noise schedule with β 0 = 0.0015 and β T = 0.0195. Models were trained with a batch size of 112 on an A100 with the Adam optimizer, learning rate 2.5 × 10 -5 for 12,000 epochs, with early stopping. During reconstruction, the PLMS scheduler was used with 100 timesteps. Reconstructions were performed from 50 t values spaced evenly over the interval [0, 1000].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LTM:</head><p>The Latent Transformer Models were trained on the same VQ-GAN bases using the procedure described in <ref type="bibr" target="#b10">[11]</ref>, using a 22-layer Transformer with dimension 256 in the attention layers and 8 attention heads. The authors in <ref type="bibr" target="#b10">[11]</ref> used the Performer architecture <ref type="bibr" target="#b3">[4]</ref>, which uses a linear approximation to the attention matrix to reduce memory costs and enable training on larger sequence lengths. Instead, we use the recently introduced memory efficient attention mechanism <ref type="bibr" target="#b23">[24]</ref> to calculate exact attention with reduced memory costs. This enables us to train a full Transformer on a 3-level VQ-GAN embedding, with a sequence length of 22 × 27 × 22 = 13, 068. Neither the Performer nor the memory-efficient Transformer was able to train on the 2-level embedding, with a sequence length of 44 × 52 × 44 = 100, 672. Models were trained on an A100 with a batch size of 128 using Adam with a learning rate of 10 -4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Results and associated statistical tests are shown in Table <ref type="table">1</ref> as AUC scores, with tests for differences in AUC performed using Delong's method <ref type="bibr" target="#b4">[5]</ref>. At 4-levels, the LDM and LTM both perform well, albeit with the proposed LDM performing better on certain OOD datasets. LTM performance degrades when trained on a 3-level model, but LDM performance remains high. The 3-level LTM result Table <ref type="table">1</ref>. AUC scores for identifying OOD data, with the CT-2 dataset used as the in-distribution test set. Results shown split according to the number of levels in the VQ-GAN. Tests for difference in AUC compare each LTM and LDM models with the same VQ-GAN base, bold values are differences significant with p &lt; 0.001 and underlined values significant with p &lt; 0.05. Results are shown as N/A for the 2-level LTM as it was not possible to train a Transformer on such a long sequence. is in agreement with the findings in <ref type="bibr" target="#b10">[11]</ref>. This is likely caused by the previously discussed tendency for likelihood-based models, such as Transformers, to be sensitive to the quality of the underlying representation. For instance, <ref type="bibr" target="#b11">[12]</ref> showed that likelihood-based models can fail unless forced to focus on high-level image features. We posit that at the high compression rates of a 4-level VQ-GAN the representation encodes higher-level features, but at 3-levels the representation can encode lower-level features, making it harder for likelihood-based models to perform well. By contrast, the LDM-based method only requires that the VG-GAN produces reasonable reconstructions. While memory constraints prevented training a 2-level LTM, the more modest requirements on the UNet-based LDM meant it was possible to train. This result has implications for the application of very high-resolution medical data: for instance, a whole-body CT with an image dimension 512 3 would have a latent dimension 32 3 even with 4-level compression, too large to train an LTM on but comfortably within the reach of a LDM. The 2-level LDM had reduced performance on two classes that have many pixels with an intensity close to 0 (Hippocampal MR, and Scaling 1%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Recent research shows that at higher resolutions, the effective SNR increases if the noise schedule is kept constant <ref type="bibr" target="#b14">[15]</ref>. It seems this effect made it possible for the 2-level LDM to reconstruct these two OOD classes with low error for many values of t. In future work we will look into scaling the noise schedule with LDM input size. Anomaly maps are shown in Fig. <ref type="figure" target="#fig_0">1</ref> for near-OOD cases with a spatially localised anomaly. The LDM-based maps are high-resolution, as they are generated in image space, and localise the relevant anomalies. The LTM maps are lower resolution, as they are generated in latent space, but more significantly often fail to localise the relevant anomalies. This is most obvious in anomalies that cause missing signal, such as missing chunks, skulls, or image scaling, which are flagged as low-anomaly regions. This is caused by the tendency of likelihood-based models to view regions with low complexity, such as blank areas, as high-likelihood <ref type="bibr" target="#b25">[26]</ref>. The anomaly is sometimes picked up but not well localised, notable in the 'chunk top' example at 4-levels. Here, the transition between brain tissue and the missing chunk is flagged as anomalous rather than the chunk itself.</p><p>Memory and time requirements for all models are tabulated in Supplementary Table <ref type="table">A</ref>. These confirm the LDM's reduced memory use compared to the LTM. All models run in &lt; 30s, making them feasible in a clinical setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced Latent Diffusion Models for 3D out-of-distribution detection. Our method outperforms the recently proposed Latent Transformer Model when assessed on both near-and far-OOD data. Moreover, we show LDMs address three key weaknesses of LTMs: their performance is less sensitive to the quality of the latent representation they are trained on, they have more favourable memory scaling that allows them to be trained on higher resolution inputs, and they provide higher resolution and more accurate spatial anomaly maps. Overall, LDMs show tremendous potential as a general-purpose tool for OOD detection on high-resolution 3D medical imaging data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example anomaly maps for models based on 3-and 4-level VQ-GANs. Maps for each model are shown on the same colour scale, but the scales vary between each model to obtain the best display for each model. Brighter regions are more anomalous.</figDesc><graphic coords="8,123,54,78,38,239,68,290,32" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. MSG, WHLP, RG, PW, PN, SO, and MJC are supported by the <rs type="funder">Wellcome Trust</rs> (<rs type="grantNumber">WT213038/Z/18/Z</rs>). MJC and SO are also supported by the <rs type="funder">Wellcome/EPSRC Centre for Medical Engineering</rs> (<rs type="grantNumber">WT203148/Z/16/Z</rs>), and the <rs type="funder">InnovateUK-funded London AI centre for Value-based Healthcare</rs>. PTD is supported by the <rs type="funder">EPSRC</rs> (<rs type="grantNumber">EP/R513064/1</rs>). YM is supported by an <rs type="funder">MRC</rs> <rs type="grantName">Clinical Academic Research Partnership grant</rs> (<rs type="grantNumber">MR/T005351/1</rs>). PN is also supported by the <rs type="funder">UCLH NIHR Biomedical Research Centre</rs>. Datasets CROMIS and KCH were used with ethics <rs type="grantNumber">20/ES/0005</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_p9fp8bh">
					<idno type="grant-number">WT213038/Z/18/Z</idno>
				</org>
				<org type="funding" xml:id="_Fqp2xZA">
					<idno type="grant-number">WT203148/Z/16/Z</idno>
				</org>
				<org type="funding" xml:id="_PjqJPqb">
					<idno type="grant-number">EP/R513064/1</idno>
				</org>
				<org type="funding" xml:id="_esW238r">
					<idno type="grant-number">MR/T005351/1</idno>
					<orgName type="grant-name">Clinical Academic Research Partnership grant</orgName>
				</org>
				<org type="funding" xml:id="_52BE9wd">
					<idno type="grant-number">20/ES/0005</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at https://doi.org/10.1007/978-3-031-43907-0 43.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The medical segmentation decathlon</title>
		<author>
			<persName><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Commu</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4128</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Monai: An open-source framework for deep learning in healthcare</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cardoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.02701</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01392</idno>
		<title level="m">Waic, but why? generative ensembles for robust anomaly detection</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Clarke-Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="page" from="837" to="845" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denouden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Abdelzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vernekar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02765</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00341</idno>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<ptr target="https://benanne.github.io/2020/09/01/typicality.html" />
		<title level="m">Musings on typicality</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Denoising diffusion models for out-of-distribution detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Pinaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Tudosiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nachev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ourselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cardoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2947" to="2956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformer-based out-of-distribution detection for clinically safe segmentation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Graham</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Imaging with Deep Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="457" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical vaes know what they don&apos;t know</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Havtorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frellsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4117" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with outlier exposure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11093</idno>
		<title level="m">Simple diffusion: End-to-end diffusion for high resolution images</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pseudo numerical methods for diffusion models on manifolds</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Outlier detection using autoencoders</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lyudchik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Do deep generative models know what they don&apos;t know?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matsukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gorur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00937</idno>
		<title level="m">Neural discrete representation learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross attention transformers for multi-modal unsupervised wholebody pet anomaly detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-18576-2_2</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-18576-22" />
	</analytic>
	<monogr>
		<title level="j">MICCAI Workshop on Deep Generative Models</title>
		<imprint>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="2022">2022</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A review of novelty detection. Signal Process</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tarassenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="215" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised brain imaging 3D anomaly detection and segmentation with transformers</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Pinaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page">102475</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Self-attention does not need o(n 2 ) memory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Staats</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05682</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Input complexity and out-of-distribution detection with likelihood-based generative models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Serrà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Slizovskaia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Núñez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Morphology-preserving autoregressive 3D generative modelling of the brain</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Tudosiu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16980-9_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16980-97" />
	</analytic>
	<monogr>
		<title level="m">International Workshop on Simulation and Synthesis in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="66" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neuromorphologicaly-preserving volumetric data encoding using VQ-VAE</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Tudosiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05692</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Clinical trial: Clinical relevance of microbleeds in stroke (cromis-2)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Werring</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-11">Nov 2017</date>
		</imprint>
		<respStmt>
			<orgName>University College London</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. NCT02513316</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cerebral microbleeds and intracranial haemorrhage risk in patients anticoagulated for atrial fibrillation after acute ischaemic stroke or transient ischaemic attack (cromis-2): a multicentre observational cohort study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet Neurol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="539" to="547" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
