<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks</title>
				<funder>
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_cD2kv4X">
					<orgName type="full">Erlangen National High Performance Computing Center (NHR @ FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg</orgName>
					<orgName type="abbreviated">FAU</orgName>
				</funder>
				<funder>
					<orgName type="full">German Research Foundation (DFG) -440719683</orgName>
				</funder>
				<funder ref="#_jZNCUWU">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Matthew</forename><surname>Baugh</surname></persName>
							<email>matthew.baugh17@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeremy</forename><surname>Tan</surname></persName>
							<idno type="ORCID">0000-0002-9769-068X</idno>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Johanna</forename><forename type="middle">P</forename><surname>Müller</surname></persName>
							<idno type="ORCID">0000-0001-8636-7986</idno>
							<affiliation key="aff2">
								<orgName type="institution">Friedrich-Alexander University Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mischa</forename><surname>Dombrowski</surname></persName>
							<idno type="ORCID">0000-0003-1061-8990</idno>
							<affiliation key="aff2">
								<orgName type="institution">Friedrich-Alexander University Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Batten</surname></persName>
							<idno type="ORCID">0000-0002-8028-5709</idno>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernhard</forename><surname>Kainz</surname></persName>
							<idno type="ORCID">0000-0002-7813-5023</idno>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Friedrich-Alexander University Erlangen-Nürnberg</orgName>
								<address>
									<settlement>Erlangen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">947206361477587948B122076EDD4FCC</idno>
					<idno type="DOI">10.1007/978-3-031-43907-016.</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is a growing interest in single-class modelling and outof-distribution detection as fully supervised machine learning models cannot reliably identify classes not included in their training. The long tail of infinitely many out-of-distribution classes in real-world scenarios, e.g., for screening, triage, and quality control, means that it is often necessary to train single-class models that represent an expected feature distribution, e.g., from only strictly healthy volunteer data. Conventional supervised machine learning would require the collection of datasets that contain enough samples of all possible diseases in every imaging modality, which is not realistic. Self-supervised learning methods with synthetic anomalies are currently amongst the most promising approaches, alongside generative auto-encoders that analyse the residual reconstruction error. However, all methods suffer from a lack of structured validation, which makes calibration for deployment difficult and datasetdependant. Our method alleviates this by making use of multiple visuallydistinct synthetic anomaly learning tasks for both training and validation. This enables more robust training and generalisation. With our approach we can readily outperform state-of-the-art methods, which we demonstrate on exemplars in brain MRI and chest X-rays. Code is available at https://github.com/matt-baugh/many-tasks-make-light-work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the workload of radiologists has grown drastically, quadrupling from 2006 to 2020 in Western Europe <ref type="bibr" target="#b3">[4]</ref>. This huge increase in pressure has led to long patient-waiting times and fatigued radiologists who make more mistakes <ref type="bibr" target="#b2">[3]</ref>. The most common of these errors is underreading and missing anomalies (42%); followed by missing additional anomalies when concluding their search after an initial finding (22%) <ref type="bibr" target="#b9">[10]</ref>. Interestingly, despite the challenging work environment, only 9% of errors reviewed in <ref type="bibr" target="#b9">[10]</ref> were due to mistakes in the clinicians' reasoning. Therefore, there is a need for automated second-reader capabilities, which brings any kind of anomalies to the attention of radiologists. For such a tool to be useful, its ability to detect rare or unusual cases is particularly important. Traditional supervised models would not be appropriate, as acquiring sufficient training data to identify such a broad range of pathologies is not feasible. Unsupervised or self-supervised methods to model an expected feature distribution, e.g., of healthy tissue, is therefore a more natural path, as they are geared towards identifying any deviation from the normal distribution of samples, rather than a particular type of pathology.</p><p>There has been rising interest in using end-to-end self-supervised methods for anomaly detection. Their success is most evident at the MICCAI Medical Outof-Distribution Analysis (MOOD) Challenge <ref type="bibr" target="#b30">[31]</ref>, where all winning methods have followed this paradigm so far (2020-2022). These methods use the variation within normal samples to generate diverse anomalies through sample mixing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. However all these methods lack a key component: structured validation. This creates uncertainty around the choice of hyperparameters for training. For example, selecting the right training duration is crucial to avoid overfitting to proxy tasks. Yet, in practice, training time is often chosen arbitrarily, reducing reproducibility and potentially sacrificing generalisation to real anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution:</head><p>We propose a cross-validation framework, using separate selfsupervision tasks to minimise overfitting on the synthetic anomalies that are used for training. To make this work effectively we introduce a number of non-trivial and seamlessly-integrated synthetic tasks, each with a distinct feature set so that during validation they can be used to approximate generalisation to unseen, real-world anomalies. To the best of our knowledge, this is the first work to train models to directly identify anomalies on tasks that are deformation-based, tasks that use Poisson blending with patches extracted from external datasets, and tasks that perform efficient Poisson image blending in 3D volumes, which is in itself a new contribution of our work. We also introduce a synthetic anomaly labelling function which takes into account the natural noise and variation in medical images. Together our method achieves an average precision score of 76.2 for localising glioma and 78.4 for identifying pathological chest X-rays, thus setting the state-of-the-art in self-supervised anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work:</head><p>The most prevalent methods for self-supervised anomaly detection are based on generative auto-encoders that analyse the residual error from reconstructing a test sample. This is built on the assumption that a reconstruction model will only be able to correctly reproduce data that is similar to the instances it has been trained on, e.g. only healthy samples. Theoretically, at test time, the residual reconstruction error should be low for healthy tissues but high for anomalous features. This is an active area of research with several recent improvements upon the initial idea <ref type="bibr" target="#b21">[22]</ref>, e.g., <ref type="bibr" target="#b20">[21]</ref> applied a diffusion model to a VQ-VAE <ref type="bibr" target="#b26">[27]</ref> to resample the unlikely latent codes and <ref type="bibr" target="#b29">[30]</ref> gradually transition from a U-Net architecture to an autoencoder over the training process in order to improve the reconstruction of finer details. Several other methods aim to ensure that the model will not reproduce anomalous regions by training it to restore samples altered by augmentations such as masking out regions <ref type="bibr" target="#b31">[32]</ref>, interpolating heavily augmented textures <ref type="bibr" target="#b28">[29]</ref> or adding coarse noise <ref type="bibr" target="#b8">[9]</ref>. <ref type="bibr" target="#b4">[5]</ref> sought to identify more meaningful errors in image reconstructions by comparing the reconstructions of models trained on only healthy data against those trained on all available data.</p><p>However, the general assumption that reconstruction error is a good basis for an anomaly scoring function has recently been challenged. Auto-encoders are unable to identify anomalies with extreme textures <ref type="bibr" target="#b15">[16]</ref>, are reliant on empirical post-processing to reduce false-positives in healthy regions <ref type="bibr" target="#b1">[2]</ref> and can be outperformed by trivial approaches like thresholding of FLAIR MRI <ref type="bibr" target="#b14">[15]</ref>.</p><p>Self-supervised methods take a more direct approach, training a model to directly predict an anomaly score using synthetic anomalies. Foreign patch interpolation (FPI) <ref type="bibr" target="#b23">[24]</ref> was the first to do this at a pixel-level, by linearly interpolating patches extracted from other samples and predicting the interpolation factor as the anomaly score. Similar to CutPaste <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b6">[7]</ref> fully replaces 3D patches with data extracted from elsewhere in the same sample, but then trains the model to segment the patches. Poisson image interpolation (PII) <ref type="bibr" target="#b24">[25]</ref> seamlessly integrates sample patches into training images, preventing the models from learning to identify the anomalies by their discontinuous boundaries. Natural synthetic anomalies (NSA) <ref type="bibr" target="#b22">[23]</ref> relaxes patch extraction to random locations in other samples and introduces an anomaly labelling function based on the changes introduced by the anomaly. Some approaches combine self-supervised and reconstruction-based methods by training a discriminator to compute more exact segmentations from reconstruction model errors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref>. Other approaches have also explored contrasting self-supervised learning for anomaly detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The core idea of our method is to use synthetic tasks for both training and validation. This allows us to monitor performance and prevent overfitting, all without the need for real anomalous data. Each self-supervised task involves introducing a synthetic anomaly into otherwise normal data whilst also producing the corresponding label. Since the relevant pathologies are unknown a priori, we avoid simulating any specific pathological features. Instead, we use a wide range of subtle and well-integrated anomalies to help the model detect many different kinds of deviations, ideally including real unforeseen anomalies. In our experiments, we use five tasks, but more could be used as long as each one is sufficiently unique. Distinct tasks are vital because we want to use these validation tasks to estimate the model's generalisation to unseen classes of anomalies. If the training and validation tasks are too similar, the performance on the validation set may be an overly optimistic estimate of how the model would perform on unseen real-world anomalies.</p><p>When performing cross-validation over all synthetic tasks and data partitions independently, the number of possible train/validation splits increases significantly, requiring us to train F • (T N CT ) independent models, where T N is the total number of tasks, T is the number of tasks used to train each model and F is the number of data folds, which is computationally expensive. Instead, as in our case T N = F = 5, we opt to associate each task with a single fold of the training data (Fig. <ref type="figure" target="#fig_0">1</ref>). We then apply 5CT -fold cross-validation over each combination. In each iteration, the corresponding data folds are collected and used for training or validation, depending on which partition forms the majority.  Synthetic Tasks: Figure <ref type="figure" target="#fig_1">2</ref> shows examples of our self-supervised tasks viewed in both one and two dimensions. Although each task produces visually distinct anomalies, they fall into three overall categories, based on blending, deformation, or intensity variation. Also, all tasks share a common recipe: the target anomaly mask M h is always a randomly sized and rotated ellipse or rectangle (ellipsoids/cuboids in 3D); all anomalies are positioned such that at least 50% of the mask intersects with the foreground of the image; and after one augmentation is applied, the process is randomly repeated (based on a fair coin toss, p = 0.5), for up to a maximum of 4 anomalies per image.</p><p>The Intra-dataset Blending Task. Poisson image blending is the current state-of-the-art for synthetic anomaly tasks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>, but it does not scale naturally to more than two dimensions or non-convex interpolation regions <ref type="bibr" target="#b16">[17]</ref>. Therefore, we extend <ref type="bibr" target="#b19">[20]</ref> and propose a D-dimensional variant of Poisson image editing following earlier ideas by <ref type="bibr" target="#b16">[17]</ref>.</p><p>Poisson image editing <ref type="bibr" target="#b19">[20]</ref> uses the image gradient to seamlessly blend a patch into an image. It does this by combining the target gradient with Dirichlet boundary conditions to define a minimisation problem min fin h |∇f in -v| Also, by defining h as the axis-aligned bounding box of M h , we can ensure the boundaries coincide with coordinate lines. This enables us to use the Fourier transform method to solve this partial differential equation <ref type="bibr" target="#b16">[17]</ref>, which yields a direct relationship between Fourier coefficients of Δf in and v after padding to a symmetric image. To simplify for our use case, an image with shape N 0 × • • •×N D-1 , we replace the Fourier transformation with a discrete sine transform (DST</p><formula xml:id="formula_0">) fu = D-1 d=0 N d -1 n=0 n sin π(n+1)(u d +1) N d +1</formula><p>. This follows as a DST is equivalent to a discrete Fourier transform of a real sequence that is odd around the zeroth and middle points, scaled by 0.5, which can be established for our images. With this, the Poisson equation becomes congruent to a relationship of the coefficients,</p><formula xml:id="formula_1">D-1 d=0 π(u d +1) N d +1 2 fu ∼ = D-1 d=0 π(u d +1) N d +1</formula><p>vd where v=(v 0 , ..., v D-1 ) and v is the DST of each component. The solution for fu can then be computed in DST space by dividing the right side through the terms on the left side and the destination image can be obtained through x i = DST -1 ( fu ). Because this approach uses a frequency transform-based solution, it may slightly alter areas outside of M h (where image gradients are explicitly edited) in order to ensure the changes are seamlessly integrated. We refer to this blending process as x = P oissonBlend(x i , x j , M h ) in the following. The intra-dataset blending task therefore results from xintra = P oissonBlend(x, x , M h ) with x, x ∈ D with samples from a common dataset D and is therefore similar to the self-supervision task used in <ref type="bibr" target="#b22">[23]</ref> for 2D images.</p><p>The inter-dataset blending task follows the same process as intra-dataset blending but uses patches extracted from an external dataset D , allowing for a greater variety of structures. Therefore, samples from this task can be defined as xinter = P oissonBlend(x, x , M h ) with x ∈ D, x ∈ D .</p><p>The sink/source tasks shift all points in relation to a randomly selected deformation centre c. For a given point p, we resample intensities from a new location p. To create a smooth displacement centred on c, we consider the distance p-c 2 in relation to the radius of the mask (along this direction), d. The extent of this displacement is controlled by the exponential factor f &gt; 1. For example, the sink task (Eqn. 1) with a factor of f = 2 would take the intensity at 0.75d and place it at 0.5d, effectively pulling these intensities closer to the centre. Note that unlike the sink equation in <ref type="bibr" target="#b23">[24]</ref> this formulation cannot sample outside of the boundaries of P M h meaning it seamlessly blends into the surrounding area. The source task (Eqn. 2) performs the reverse, appearing to push the pixels away from the centre by sampling intensities towards it.</p><formula xml:id="formula_2">xp = x p, p = c+d p -c p -c 2 1 -1 - p -c 2 d f , c ∈ P M h , ∀ p ∈ P M h (1) xp = x p, p = c + d p -c p -c 2 p -c 2 d f , c ∈ P M h , ∀ p ∈ P M h (2)</formula><p>The smooth intensity change task aims to either add or subtract an intensity over the entire anomaly mask. To avoid sharp discontinuities at the boundaries, this intensity change is gradually dampened for pixels within a certain margin of the boundary. This smoothing starts at a random distance from the boundary, d s , and the change is modulated by d p /d s .</p><p>Anomaly Labelling: In order to train and validate with multiple tasks simultaneously we use the same anomaly labelling function across all of our tasks. The scaled logistic function, used in NSA <ref type="bibr" target="#b22">[23]</ref>, helps to translate raw intensity changes into more semantic labels. But, it also rounds imperceptible differences up to a minimum score of about 0.1. This sudden and arbitrary jump creates noisy labels and can lead to unstable training. We correct this semantic discontinuity by computing labels as <ref type="bibr" target="#b22">[23]</ref>. This flipped Gaussian shape is C1 continuous and smoothly approaches zero, providing consistent labels even for smaller changes.</p><formula xml:id="formula_3">y = 1 -pX (x) pX (0) with X ∼ N (0, σ 2 ), instead of y = 1 1+e -k(x-x 0 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Data: We evaluate our method on T2-weighted brain MR and chest X-ray datasets to provide direct comparisons to state-of-the-art methods over a wide range of real anomalies. For brain MRI we train on the Human Connectome Project (HCP) dataset <ref type="bibr" target="#b27">[28]</ref> which consists of 1113 MRI scans of healthy, young adults acquired as part of a scientific study. To evaluate, we use the Brain Tumor Segmentation Challenge 2017 (BraTS) dataset <ref type="bibr" target="#b0">[1]</ref>, containing 285 cases with either high or low grade glioma, and the ischemic stroke lesion segmentation challenge 2015 (ISLES) dataset <ref type="bibr" target="#b12">[13]</ref>, containing 28 cases with ischemic stroke lesions. The data from both test sets was acquired as part of clinical routine. The HCP dataset was resampled to have 1mm isotropic spacing to match the test datasets. We apply z-score normalisation to each sample and then align the bounding box of each brain before padding it to a size of 160 × 224 × 160. Lastly, samples are downsampled by a factor of two.</p><p>For chest X-rays we use the VinDr-CXR dataset <ref type="bibr" target="#b17">[18]</ref> including 22 different local labels. To be able to compare with the benchmarks reported in <ref type="bibr" target="#b5">[6]</ref> we use the same healthy subset of 4000 images for training along with their test set (DDAD ts ) of 1000 healthy and 1000 unhealthy samples, with some minor changes outlined as follows. First note that <ref type="bibr" target="#b5">[6]</ref> derives VinDr-CXR labels using the majority vote of the 3 annotators. Unfortunately, this means there are 52 training samples, where 1/3 of radiologists identified an anomaly, but the majority label is counted as healthy. The same applies to 10 samples within the healthy testing subset. To avoid this ambiguity, we replace these samples with leftover training data that all radiologists have labelled as healthy. We also evaluate using the true test set (VinDr ts ), where two senior radiologists have reviewed and consolidated all labels. For preprocessing, we clip pixel intensities according to the window centre and width attributes in each DICOM file, and apply histogram equalisation, before scaling intensities to the range [-1, 1]. Finally, images are resized to 256 × 256. • indicates that the metrics are evaluated over the same region and at the same resolution as CRADL <ref type="bibr" target="#b11">[12]</ref>. Upper right part: Metrics on VinDr-CXR, presented as AP/AUROC on the VinDr and DDAD test splits. Random is the baseline performance of a random classifier. Lower part: a sensitivity analysis of the average AP of each individual fold (mean±s.d.) alongside that of the model ensemble, varying how many tasks we use for training versus validation. Best results are highlighted in bold. Comparison to State-of-the-Art Methods: Validating on synthetic tasks is one of our main motivations; as such, we use a 1/4 (train/val.) task split to compare with benchmark methods. For brain MRI, we evaluate results at the slice and voxel level, computing average precision (AP) and area under the receiver operating characteristic curve (AUROC), as implemented in scikit learn <ref type="bibr" target="#b18">[19]</ref>. Note that the distribution shift between training and test data (research vs. clinical scans) adds further difficulty to this task. In spite of this, we substantially improve upon the current state-of-the-art (Table <ref type="table" target="#tab_0">1</ref> upper left). In particular, we achieve a pixel-wise AP of 76.2 and 45.9 for BraTS and ISLES datasets respectively. To make our comparison as faithful as possible, we also re-evaluate after post-processing our predictions to match the region and resolution used by CRADL, where we see similar improvement. Qualitative examples are shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Note that all baseline methods use a validation set consisting of real anomalous samples from BraTS and ISLES to select which anomaly scoring function to use. We, however, only use synthetic validation data. This further verifies that our method of using synthetic data to estimate generalisation works well.</p><p>For both VinDr-CXR test sets we evaluate at a sample and pixel level, although previous publications have only reported their results at a sample level. We again show performance above the current state-of-the-art (Table <ref type="table" target="#tab_0">1</ref> upper right). Our results are also substantially higher than previously proposed selfsupervised methods, improving on the current state-of-the-art NSA <ref type="bibr" target="#b22">[23]</ref> by 12.6 to achieve 78.4 image-level AP. This shows that our use of synthetic validation data succeeds where their fixed training schedule fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation and Sensitivity Analysis on Cross-Validation Structure:</head><p>We also investigate how performance changes as we vary the number of tasks used for training and validation (Table <ref type="table" target="#tab_0">1</ref> lower). For VinDr-CXR, in an individual fold, the average performance increases as training becomes more diverse (i.e. more tasks); however, the performance of the ensemble plateaus. Having more training tasks can help the model to be sensitive to a wider range of anomalous features. But as the number of training tasks increases, so does the overlap between different models in the ensemble, diminishing the benefit of pooling predictions. This could also explain why the standard deviation (across folds) decreases as the number of training tasks increases, since the models are becoming more similar. Our best configuration is close to being competitive with the state-of-the-art semi -supervised method DDAD-ASR <ref type="bibr" target="#b5">[6]</ref>. Even though their method uses twice as much training data, as well as some real anomalous data, our purely synthetic method begins to close the gap (AP of <ref type="bibr" target="#b5">[6]</ref> 84.3 vs. ours 80.7 on DDAD ts ). For the brain datasets, all metrics generally decrease as the number of training tasks increases. This could be due to the distribution shift between training and test data. Although more training tasks may increase sensitivity to diverse irregularities, this can actually become a liability if there are differences between (healthy) training and test data (e.g. acquisition parameters). More sensitive models may then lead to more "false" positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion:</head><p>We demonstrate the effectiveness of our method in multiple settings and across different modalities. A unique aspect of the brain data is the domain shift. The HCP training data was acquired at a much higher isotropic resolution than the BraTS and ISLES test data, which are both anisotropic. Here we achieve the best performance using more tasks for validation, which successfully reduces overfitting and hypersensitivity. Incorporating greater data augmentations, such as simulating anisotropic spacing, could further improve results by training the model to ignore these transformations. We also achieve strong results for the X-ray data, although precise localisation remains a challenging task. The gap between current performance and clinicially useful localisation should therefore be high priority for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work we use multiple synthetic tasks to both train and validate selfsupervised anomaly detection models. This enables more robust training without the need for real anomalous training or validation data. To achieve this we propose multiple diverse tasks, exposing models to a wide range of anomalous features. These include patch blending, image deformations and intensity modulations. As part of this, we extend Poisson image editing to images of arbitrary dimensions, enabling the current state-of-the-art tasks to be applied beyond just 2D images. In order to use all of these tasks in a common framework we also design a unified labelling function, with improved continuity for small intensity changes. We evaluate our method on both brain MRI and chest X-rays and achieve state-of-the-art performance and above. We also report pixel-wise results, even for the challenging case of chest X-rays. We hope this encourages others to do the same, as accurate localisation is essential for anomaly detection to have a future in clinical workflows.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our pipeline performs cross-validation over the synthetic task and data fold pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples of changes introduced by synthetic anomalies, showing the before (grey) and after (green) of a 1D slice across the affected area. -deformation centre for sink/source. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2</head><label></label><figDesc>with f in | ∂h = f out | ∂h , and f in representing the intensity values within the patch h. The goal is to find intensity values of f in that will match the surrounding values, f out , of the destination image x i , along the border of the patch and follow the image gradient, v = ∇• = ∂• ∂x , ∂• ∂y , of the source image x j . Its solution is the Poisson equation Δf in = divv over h with f in | ∂h = f out | ∂h . Note that the divergence of v is equal to the Laplacian of the source image Δx j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Examples of predictions on randomly selected BraTS and ISLES samples after training on HCP. The red contour outlines the ground truth segmentation. (Color figure online)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Upper left part: Metrics on Brain MRI, evaluated on BraTS and ISLES, presented as AP/AUROC.</figDesc><table><row><cell>CRADL setup</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We thank <rs type="funder">EPSRC</rs> for DTP funding and HPC resources provided by the <rs type="funder">Erlangen National High Performance Computing Center (NHR @ FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU)</rs> under the <rs type="projectName">NHR</rs> project <rs type="grantNumber">b143dc</rs>. NHR funding is provided by federal and Bavarian state authorities. NHR@FAU hardware is partially funded by the <rs type="funder">German Research Foundation (DFG) -440719683</rs>. Support was also received by the <rs type="funder">ERC</rs> -project <rs type="projectName">MIA-NORMAL 101083647</rs> and <rs type="grantNumber">DFG KA 5801/2-1, INST 90/1351-1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_cD2kv4X">
					<idno type="grant-number">b143dc</idno>
					<orgName type="project" subtype="full">NHR</orgName>
				</org>
				<org type="funded-project" xml:id="_jZNCUWU">
					<idno type="grant-number">DFG KA 5801/2-1, INST 90/1351-1</idno>
					<orgName type="project" subtype="full">MIA-NORMAL 101083647</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Autoencoders for unsupervised anomaly segmentation in brain mr images: a comparative study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101952</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Brady</surname></persName>
		</author>
		<title level="m">Error and discrepancy in radiology: inevitable or avoidable? Insights Imaging</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Workload for radiologists during on-call hours: dramatic increase in the past 15 years</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bruls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kwee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Insights Imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual-distribution discrepancy for anomaly detection in chest x-rays</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16437-8_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16437-856" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part III</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="584" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Dual-distribution discrepancy with self-supervised refinement for anomaly detection in medical images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.04227</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-supervised 3d out-of-distribution detection via pseudoanomaly generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Image Registration</title>
		<imprint>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kascenas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.08330</idno>
		<title level="m">The role of noise in denoising models for anomaly detection in medical images</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fool me twice: delayed diagnoses in radiology with emphasis on perpetuated errors</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Mansfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Roentgenol</title>
		<imprint>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="470" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cutpaste: self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cradl: contrastive representations for unsupervised anomaly detection and localization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Lüth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.02126</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ISLES 2015 -a public evaluation benchmark for ischemic stroke lesion segmentation from multispectral MRI</title>
		<author>
			<persName><forename type="first">O</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Der Gablentz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Häni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Heinrich</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2016.07.009</idno>
		<ptr target="https://doi.org/10.1016/j.media.2016.07.009" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="250" to="269" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abnormality detection in chest x-ray images using uncertainty prediction autoencoders</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="529" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Challenging current semi-supervised anomaly segmentation methods for brain mri</title>
		<author>
			<persName><forename type="first">F</forename><surname>Meissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-08999-2_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-08999-25" />
	</analytic>
	<monogr>
		<title level="m">BrainLes 2021 at MICCAI 2021, 27 Sept 2021, Part I</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the pitfalls of using the residual error as anomaly score</title>
		<author>
			<persName><forename type="first">F</forename><surname>Meissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wiestler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno>PMLR (06-08</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 5th International Conference on Medical Imaging with Deep Learning. Proceedings of Machine Learning Research</title>
		<meeting>The 5th International Conference on Medical Imaging with Deep Learning. Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="914" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fourier implementation of poisson image editing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Petro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="342" to="348" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vindr-cxr: an open dataset of chest x-rays with radiologist&apos;s annotations</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">429</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scikit-learn: machine learning in python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2003 Papers</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast unsupervised brain anomaly detection and segmentation with diffusion models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Pinaya</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16452-1_67</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16452-167" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part VIII</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">f-AnoGAN: fast unsupervised anomaly detection with generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.01.010</idno>
		<ptr target="https://doi.org/10.1016/j.media.2019.01.010" />
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="30" to="44" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Natural synthetic anomalies for selfsupervised anomaly detection and localization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-19821-2_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-19821-227" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2022</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature Switzerland</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="474" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting outliers with foreign patch interpolation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Batten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn. Biomed. Imaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting outliers with poisson image interpolation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_56</idno>
		<idno>978-3-030-87240-3 56</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="581" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Constrained contrastive distribution learning for unsupervised anomaly detection and localisation in medical images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87240-3_13</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87240-313" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12905</biblScope>
			<biblScope unit="page" from="128" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The human connectome project: a data acquisition perspective</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Essen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ugurbil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Auerbach</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2012.02.018</idno>
		<ptr target="https://doi.org/10.1016/j.neuroimage.2012.02.018" />
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2222" to="2231" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Draem-a discriminatively trained reconstruction embedding for surface anomaly detection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zavrtanik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Skočaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8330" to="8339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A multi-task network with weight decay skip connection training for anomaly detection in retinal fundus images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2022, Part II</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="656" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mood 2020: a public benchmark for out-of-distribution detection and localization on medical images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zimmerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2728" to="2738" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Context-encoding variational autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zimmerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05941</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
