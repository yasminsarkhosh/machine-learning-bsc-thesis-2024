<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training</title>
				<funder ref="#_tpAjZF7">
					<orgName type="full">Intergovernmental Cooperation Project of the National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher>Springer Nature Switzerland</publisher>
				<availability status="unknown"><p>Copyright Springer Nature Switzerland</p>
				</availability>
				<date type="published" when="2023">2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaofei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University)</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuting</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University)</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rongjun</forename><surname>Ge</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University)</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University of Aeronautics and Astronautics</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">Case Western Reserve University</orgName>
								<address>
									<settlement>Cleveland</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guanyu</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education</orgName>
								<orgName type="laboratory">Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University)</orgName>
								<address>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Joint International Research Laboratory of Medical Information Processing</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Centre de Recherche en Information Biomédicale Sino-Français (CRIBs)</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-training</title>
					</analytic>
					<monogr>
						<title level="m">Lecture Notes in Computer Science</title>
						<idno type="ISSN">0302-9743</idno>
						<idno type="eISSN">1611-3349</idno>
						<imprint>
							<publisher>Springer Nature Switzerland</publisher>
							<biblScope unit="page" from="405" to="415"/>
							<date type="published" when="2023" />
						</imprint>
					</monogr>
					<idno type="MD5">71B82E67638F0CD44EF6CAC6406BEEFB</idno>
					<idno type="DOI">10.1007/978-3-031-43907-0_39</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-23T22:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The foundation models based on pre-training technology have significantly advanced artificial intelligence from theoretical to practical applications. These models have facilitated the feasibility of computer-aided diagnosis for widespread use. Medical contrastive visionlanguage pre-training, which does not require human annotations, is an effective approach for guiding representation learning using description information in diagnostic reports. However, the effectiveness of pretraining is limited by the large-scale semantic overlap and shifting problems in medical field. To address these issues, we propose the Knowledge-Boosting Contrastive Vision-Language Pre-training framework (KoBo), which integrates clinical knowledge into the learning of vision-language semantic consistency. The framework uses an unbiased, open-set samplewise knowledge representation to measure negative sample noise and supplement the correspondence between vision-language mutual information and clinical knowledge. Extensive experiments validate the effect of our framework on eight tasks including classification, segmentation, retrieval, and semantic relatedness, achieving comparable or better performance with the zero-shot or few-shot settings. Our code is open on https:// github.com/ChenXiaoFei-CS/KoBo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Foundation models have become a significant milestone in artificial intelligence, from theoretical research to practical applications <ref type="bibr" target="#b1">[2]</ref>, like world-impacting large language model ChatGPT <ref type="bibr" target="#b4">[5]</ref> and art-history-defining large generative model   DALL-E <ref type="bibr" target="#b19">[20]</ref>. In medical image analysis, foundation models are showing promising future, and pre-training technologies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref>, as the cornerstone of foundation models, facilitated feasibility of computer-aided diagnosis for widespread use.</p><p>Medical contrastive vision-language pre-training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> has shown great superiority in medical image analysis, because it utilizes easy-accessible expert interpretation from reports to precisely guide the understanding of image semantics. Therefore, contrastive vision-language pre-training will break through the bottleneck of time-consuming and expensive expert annotation <ref type="bibr" target="#b25">[26]</ref> and difficulty in learning fine-grained clinical features with pure-image self-supervised methods <ref type="bibr" target="#b27">[28]</ref>. It will improve data efficiency, and achieve comparable or better performance when transferred with the zero-shot or few-shot setting, demonstrating the potential of promoting the ecology of medical artificial intelligence.</p><p>However, semantic overlap and semantic shifting are two significant challenges in medical vision-language contrastive learning (Fig. <ref type="figure" target="#fig_1">2</ref>). (a) Semantic Overlap Problem: There is overlapping semantics between negative samples which should be semantic-distinct, e.g. two medical images sharing the same disease are contrasted which brings noise <ref type="bibr" target="#b24">[25]</ref>. Once directly learning, crossmodal representations of the same disease are falsely pulled apart, making the model unable to capture the disease-corresponding image feature. (b) Semantic Shifting Problem: Radiologists have writing preferences, e.g. biased for their own familiar concepts and observation view towards similar visual features, and inclined for negation expression towards opposite visual features. Distinct   concepts describing the same image are morphologically dissimilar for text encoder, while the negation expression of concepts is morphologically similar <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Report Ti Report Ti</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph</head><p>Once lack of concept correlation and negation identification, representations with similar semantics are falsely pushed apart and those with opposite semantics are falsely pushed together, interfering with the learning of significant representation <ref type="bibr" target="#b6">[7]</ref>.</p><p>Rethinking the existing methods and challenges of medical contrastive visionlanguage pre-training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, the lack of clinical knowledge constraints in dual-free-encoding contrastive learning structure is the key problem. Existing methods utilize sample-wise differences to learn mutual information between modalities, improving the representation quality based on the correspondence of learned mutual information and clinical knowledge. However, semantic overlap reduces the learning efficiency of mutual information with the noisy difference, and the mentioned correspondence is vulnerable to semantic shifting. Therefore, if we are able to embed an unbiased, comprehensive representation as knowledge boosting, it will reduce the negative noise and supplement the lacking correspondence. It motivates us to measure the noise with similarities between knowledge representation, and fuse the correspondence between knowledge and modality.</p><p>In this paper, we propose a novel knowledge-boosting medical contrastive vision-language pre-training framework (KoBo). Our contributions are as followed. 1) Our KoBo pre-trains a powerful image encoder including visual information corresponding with the disease described in texts, where knowledge is embedded in our paradigm (Fig. <ref type="figure" target="#fig_0">1</ref>) to boost the learning of vision-language consistency. 2) We propose Knowledge Semantic Enhancement (KSE) module to reduce the negative sample noise with the similarity between open-set sample-wise knowledge embeddings. 3) We propose Knowledge Semantic Guidance (KSG) module to adjust the semantic shifting during pre-training, fusing the modality feature with unbiased knowledge embeddings for supplementing the correspondence between modality mutual information and clinical knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Our Knowledge-Boosting Contrastive Vision-Language Pre-training framework (Fig. <ref type="figure" target="#fig_3">3</ref>) boosts vision-language learning with additional clinical knowledge. It contains two modules: KSE for reducing the negative effect of semantic overlap, and KSG for adjusting semantic shifting, aimed at learning effective representation by maximizing semantic consistency between paired image and text features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Framework Formulation</head><p>In the framework, a powerful image encoder Enc I and text encoder Enc T is pretrained, alongside a graph encoder Enc G . Given a pair of medical image and diagnostic report </p><formula xml:id="formula_0">{I i , T Report i }, I i ∈ R H×W ×C , a</formula><formula xml:id="formula_1">t i ∈ R DS , L i = {l i1 , l i2 , ..., l iNL } ∈ R NL×DS .</formula><p>Besides using reports and images as the input for our pre-training network, we also input an external knowledge graph to the whole framework for improving the correspondence of modality features and clinical knowledge. The knowledge refers to relations between clinical pathology concepts in the radiology domain in the format of triplet G = {(c h k , r k , c t k )} NG k=1 , such as UMLS <ref type="bibr" target="#b13">[14]</ref>. Domain knowledge embedding for each concept</p><formula xml:id="formula_2">E = {e s } NE s=1 ∈ R NE ×DS is the output of Enc G (G).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Semantic Enhancement</head><p>To relieve the semantic overlap problem, where negative sample noise harms the effective learning of vision-language mutual information, we propose a semantic enhancement module to identify the noise using sample-wise similarities. The similarity is estimated upon sample knowledge k i , calculated from domain knowledge embedding E and concept set from texts with negation marker.</p><p>Getting Sample knowledge: Firstly, we acquire a concept set that contains pathology concepts extracted from texts with Negbio N (•) <ref type="bibr" target="#b16">[17]</ref>. The image-view concept set which involves the overall observation is from the whole report, while the text-view set only covers the chosen sentence. Secondly, the image and text sample knowledge, as an auxiliary semantic estimation, is selected from domain knowledge embedding E according to the corresponding concept set from the report and sentence respectively, if not considering the negation problem.</p><p>Furthermore, considering the challenge that negation expression of concepts commonly exists in radiology reports, which has opposite semantics with similar morphology for text encoder (converging shifting), we randomly generate a No Finding embedding N F and a variant of domain knowledge embedding E = { e 1 , e 2 , ..., e NE } of the same size as E with Xavier distribution. Upon the negation mark of concept, sample knowledge embedding k i = {k i,s } NES s=1 is denoted below:</p><formula xml:id="formula_3">k i,s = e i,s c i,s ∈ N (T i ), P (c i,s ) = Neg • N F + (1 -) e i,s c i,s ∈ N (T i ), P (c i,s ) = Neg (<label>1</label></formula><formula xml:id="formula_4">)</formula><p>where P is the negation mark of concepts, and e i,s , e i,s is the corresponding position of c i,s in E and E. tunes the variance of negative sample knowledge.</p><formula xml:id="formula_5">k Image i,s and k T ext i,s</formula><p>are k i from the image-view and text-view concept set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation of Similarities:</head><p>The semantic similarity is calculated upon sample knowledge. For each image-text pair, a max-match strategy is adopted to match each two sample knowledge embedding with the most similar one for calculating cosine similarities. Sample-wise similarities are aggregated with averages.</p><formula xml:id="formula_6">λ IT ij = 1 N ES N ES s=1 NES max s =1 (k Image i,s ) T k T ext j,s , λ T I ij = 1 N ES NES s=1 N ES max s =1 (k T ext i,s ) T k Image j,s</formula><p>(2) where N ES is the number of concepts in T Sent i , while N ES is that in T Report i . Knowledge Semantic Enhancement Loss: We utilize the sample-wise semantic similarity to estimate negative sample noise, placed in the sample weight of the contrastive loss <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref>, where paired cross-modal embedding are pushed together and unpaired ones are pulled apart. The importance of estimated noisy negative samples is relatively smaller for a subtle pulling between cross-modal embeddings. The semantic enhancement loss is below:</p><formula xml:id="formula_7">L SE = - 1 N N i=1 (log exp(v T i t i /τ G ) N j=1 (1 -λ IT ij ) exp(v T i t j /τ G ) +log exp(t T i v i /τ G ) N j=1 (1 -λ T I ij ) exp(t T i v j /τ G ) )<label>(3</label></formula><p>) where τ G is the global temperature, and λ IT , λ T I is the sample similarity measurement. specifically, λ i,i is fixed to zero to persist the positive sample weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Knowledge Semantic Guidance</head><p>In this section, we propose a semantic guidance module to solve the semantic shifting problem. Utilizing sample knowledge from Sect. 2.2 which contains concept correlation and negation information, the adverse effects of both disperse and converging shifting are alleviated by fusing domain-sample knowledge with global-local modality embeddings. We design four contrast schemes: knowledge anchor guidance for adjusting disperse shifting, semantic knowledge refinement for filtering converging shifting, vision semantic response for consolidating knowledge fusion, and semantic bridge guidance for narrowing the modality gap. Knowledge Anchor Guidance: Disperse shifting will be adjusted if there are unbiased anchors in semantic space as priors to attract modality embeddings towards clinical semantics, and domain knowledge embedding does a good job. We define knowledge fused embeddings <ref type="figure"></ref>and<ref type="figure">AT T N(Q, K, V</ref> ) means the attention function <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_8">H IK i = AT T N(v i , E, E) and H T K i = AT T N(t i , E, E),</formula><formula xml:id="formula_9">L KAG = - 1 N N i=1 (log exp(H IK i • H T K i /τ G ) N j=1 exp(H IK i • H T K j /τ G ) + log exp(H T K i • H IK i /τ G ) N j=1 exp(H T K i • H IK j /τ G ) )<label>(4)</label></formula><p>where image-weighted and text-weighted knowledge is globally contrasted.</p><p>Semantic Knowledge Refinement: Wrong-converging pairs have distinct intrinsic responses on sample knowledge from image and text. Hence, we propose to utilize sample knowledge to refine these falsely gathered dissimilar pairs. We define</p><formula xml:id="formula_10">H SI ij = AT T N(k T ext i , R j , R j ) and H ST ij = AT T N(k T ext i , L j , L j ): L SKR = - 1 N N i=1 log exp( 1 NES •τL NES k=1 H SI iik • H ST iik ) N j=1 exp( 1 NES •τL NES k=1 H SI ijk • H ST ijk )<label>(5)</label></formula><p>where local semantic-weighted image and text embeddings are contrasted.</p><p>Vision Semantic Response: Instead of matching single token with image subregions in <ref type="bibr" target="#b9">[10]</ref>, we propose to match the concept with sub-regions. As the concept is a more complete and atomic semantic unit, local response upon concept will better guide the representation learning with a fine-grained semantic match through an in-sample contrast. We define</p><formula xml:id="formula_11">H IS i = AT T N(R i , k T ext i , k T ext i</formula><p>), and the fusion of knowledge will be consolidated as below:</p><formula xml:id="formula_12">L V SR = - 1 N • N I N i=1 NI k=1 log exp(H IS ik • r ik /τ L ) NI k =1 exp(H IS ik • r ik /τ L )<label>(6)</label></formula><p>where there is an in-sample local contrast between H IS i and vision features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Bridge Guidance:</head><p>We propose to narrow disperse shifting enlarged by the modality gap between vision and language. Specifically, the gap is bridged by the fusion of domain knowledge which is better compatible with text:</p><formula xml:id="formula_13">L SBG = - 1 N N i=1 (log exp(H IK i • t i /τ G ) N j=1 exp(H IK i • t j /τ G ) + log exp(t i • H IK i /τ G ) N j=1 exp(t i • H IK j /τ G ) )<label>(7)</label></formula><p>where the image-weighted domain knowledge is contrasted with text features between samples. Finally, L SG is aggregated by these four parts as below:</p><formula xml:id="formula_14">L SG = λ 1 L KAG + λ 2 L SKR + λ 3 L V SR + λ 4 L SBG (8)</formula><p>3 Experiment</p><p>Experiment Protocol: Pre-training performs on MIMIC-CXR <ref type="bibr" target="#b11">[12]</ref> following the pre-process style of <ref type="bibr" target="#b8">[9]</ref>. The impression section of reports and frontal view of images are selected to generate 203k image-report pair. Five downstream task datasets (CheXpert <ref type="bibr" target="#b10">[11]</ref>, Covidx <ref type="bibr" target="#b23">[24]</ref>, MIMIC-CXR, UMNSRS <ref type="bibr" target="#b15">[16]</ref> and SIIM <ref type="bibr" target="#b21">[22]</ref>) are applied on eight tasks. Semantic relatedness is to verify the text understanding of radiology concepts, where text embedding with certain prompts predicts the relatedness. A new semantic relatedness benchmark is generated from MIMIC-CXR, adding in the extra negation discriminating. CheXpert5X200 <ref type="bibr" target="#b9">[10]</ref>(Multi-classification) is from CheXpert, and CheXpertlabeller <ref type="bibr" target="#b10">[11]</ref> generates retrieval labels in MIMIC-CXR. More details are in appendix.  For implementation, ResNet50 <ref type="bibr" target="#b5">[6]</ref> and Vit <ref type="bibr" target="#b12">[13]</ref> are image encoder, and Bio-ClinicalBERT <ref type="bibr" target="#b0">[1]</ref> is the text encoder. CompGCN with LTE <ref type="bibr" target="#b26">[27]</ref> is our graph encoder, and domain knowledge contains 10,244 concepts in UMLS which exist in MIMIC-CXR. Negbio <ref type="bibr" target="#b16">[17]</ref> combined with UMLS disambiguation tool <ref type="bibr" target="#b13">[14]</ref> serves as N (•). Embeddings are projected into the dim of 256. Pre-training has the batch size of 100 and max epochs of 50 based on Pytorch on two RTX3090 GPUs. Adam optimizer with the learning rate of 5e-5 and ReduceLR scheduler are applied. τ G is 0.07 and τ L is 0.1. λ in KSG loss are all 0.25, while in KSE loss is 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KoBo Init</head><p>ImageNet Init Basilar Atelectasis. Comparison Study: Table <ref type="table" target="#tab_2">1</ref> verifies our powerful representation ability, reaching state-of-art in classification, segmentation, and semantic relatedness compared with existing vision-language pre-training tasks, while our method is also top two for retrieval. In zero-shot classification tasks, our KoBo outperforms MGCA and ConVIRT 0.94% and 3.38% respectively, exceeding most methods even in their training setting. For CheXpert5X200, our framework is second only to MedCLIP which presents a superior performance in this dataset. In three fewshot setting task, our KoBo has an absolute leading position.</p><p>Ablation Study: As is demonstrated in Fig. <ref type="figure" target="#fig_5">4</ref>, we perform module ablation and data amount ablation. (a) For module ablation, both modules bring benefits in representation learning and are respectively effective. When KSG module is removed, our KoBo also extracts effective feature related to pneumonia with a subtle decrease of 0.51%. When KSE is removed, there is a reduction of 1.25% accuracy. (b) For data amount ablation, KoBo has better data robustness with a subtle decrease when training data reduce to 1%. KoBo also has a superior transfer ability with an absolutely better AUC with 1% data than ImageNet with all training data than ImageNet with all training data.</p><p>Qualitative Analysis: In Fig. <ref type="figure" target="#fig_6">5</ref>, our Kobo has learned fine-grained and effective image feature with the fusion of knowledge modeling. The deepest region in the first image gathered on the top left side, showing an obvious expansion on the right lung. There is consistency with the expert annotation and our output logit. The precise location of atelectasis region in CAM of second image and clustering trend interpret for the increase in zero-shot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In our paper, we propose a Knowledge-Boosting Contrastive Vision-Language Pre-traing framework (KoBo). Sample and domain knowledge are used to differentiate noisy negative samples and supplement the correspondence between modality and clinical knowledge. Our experiments on eight tasks verify the effectiveness of our framework. We hope that our work will encourage more research on knowledge-granularity alignment in medical vision-language learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Our knowledge boosting innovates the paradigm of medical vision-language contrastive learning, inspired by two problems in the existing architecture.</figDesc><graphic coords="2,49,38,169,97,248,20,52,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Two key challenges in medical contrastive vision-language pre-training: (a) Semantic overlap exists between negative samples, falsely pulling apart samples with similar semantics. (b) Biased expression and negative expression of radiologists cause the inconsistency of semantics and text morphology between sample pairs, causing disperse and converging semantic shifting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a) Our KoBo learns vision-language semantic consistency for better image representation learning with knowledge boosting a) Our KoBo learns vision-language semantic consistency for better image representation learning with knowledge boosting b) Knowledge Semantic Enhancement measure negative sample noise by the sample-wise similarity between estimated knowledge embedding b) Knowledge Semantic Enhancement measure negative sample noise by the sample-wise similarity between estimated knowledge embedding c) Knowledge Semantic Guidance fuse knowledge embeddings with modality features for supplementing the correspondence between modality and clinical knowledge opacity ... pneumonia. ... right lung nodules... ... lungs are hyperinflated. ... opacity ... pneumonia. ... right lung nodules... ... lungs are hyperinflated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of our proposed architecture, where additional clinical knowledge is embedded in. Image encoder, text encoder, graph encoder, knowledge semantic enhancement module, and knowledge semantic guidance module are presented.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>sentence T Sent i is randomly selected from T Report i as a caption comprised of several tokens {w 1 , w 2 , ..., w NL }. Enc I outputs global feature z I,G i and local feature z I,L i for N I sub-regions, which is from the intermediate feature map. T Sent i is fed into Enc T , obtaining global sentence feature z T,G i , and local token feature z T,L i . Distinct projectors are applied to map features into embeddings with lower semantic dim D S , finally getting global and local image embeddings v i ∈ R DS , R i = {r i1 , r i2 , ..., r iNI } ∈ R NI ×DS , and text embedding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Module ablation study of our KoBo framework is performed on Covidx dataset compared with ImageNet and random initialization, upon few-shot frozen setting. (b) Data ablation study is performed on CheXpert with frozen setting when classification training data amount reduces to 25%, 10% and 1%.</figDesc><graphic coords="7,245,40,450,53,151,00,73,24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Visualization of pneumothorax and atelectasis. AblationCAM<ref type="bibr" target="#b18">[19]</ref> generates the class activate map (CAM) upon last layer of Kobo-ResNet. There is strong consistency between CAM, prediction logits and segmentation label. t-SNE<ref type="bibr" target="#b22">[23]</ref> is applied on image embedding from CheXpert-valid, showing gathering cluster trend of disease samples.</figDesc><graphic coords="8,46,29,182,21,331,84,126,64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Encoder Enc G Graph Encoder Enc G Enc Enc Enc Enc Enc Enc Knowledge Semantic Guidance Knowledge Semantic Guidance Knowledge Semantic Enhancement Knowledge Semantic Enhancement LKAG LKAG LSBG LSBG LSKR LSKR LVSR LVSR Global feature vi Global feature vi Local feature Ri Local feature Ri Get Sample Knowledge Get Sample Knowledge Estimate Estimate LSE LSE Domain knowledge feature E Domain knowledge feature E</head><label></label><figDesc></figDesc><table><row><cell>Sample-wise Sample-wise</cell></row><row><cell>similarity λ similarity λ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparison results in eight downstream tasks. (*) defines that official pretrained weight is used, and the remaining methods are reproduced with the same batch size, pre-processing and the evaluation. CLS, RR, SR, and SEG mean classification, retrieval, semantic relatedness and semantic segmentation, V or L means vision and language tasks. Few-shot-Frozen means the frozen encoder of the backbone and only 1% of total training data. ResNet-50 is the equal-comparing backbone except for KoBo-Vit. The best two results are highlighted in underlined red and violet.</figDesc><table><row><cell></cell><cell>Zero-shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Few-shot-Frozen</cell></row><row><cell></cell><cell>CLS(V+L)</cell><cell>RR(V)</cell><cell>RR(V+L)</cell><cell>SR(L)</cell><cell>SR(L)</cell><cell>CLS(V)</cell><cell>SEG(V)</cell><cell>CLS(V)</cell></row><row><cell>Method</cell><cell>CheXpert</cell><cell>CheXpert5X200</cell><cell>MIMIC</cell><cell>UMNSRS</cell><cell>MIMIC</cell><cell>CheXpert</cell><cell>SIIM</cell><cell>Covidx</cell></row><row><cell></cell><cell>(Auroc)</cell><cell>(mAP)</cell><cell>(mAP)</cell><cell>(Pearson)</cell><cell>(Pearson)</cell><cell>(Auroc)</cell><cell>(Dice)</cell><cell>(Acc)</cell></row><row><cell>CLIP [18](*)</cell><cell>0.4702</cell><cell>0.2544</cell><cell>0.7577</cell><cell>0.1985</cell><cell cols="2">-0.2879 0.5748</cell><cell>/</cell><cell>0.8975</cell></row><row><cell>ConVIRT [26]</cell><cell>0.8252</cell><cell>0.3808</cell><cell>0.8482</cell><cell>0.2506</cell><cell cols="2">0.1429 0.8548</cell><cell>0.4992</cell><cell>0.9475</cell></row><row><cell>Gloria [10]</cell><cell>0.8257</cell><cell>0.3875</cell><cell>0.8390</cell><cell>0.2294</cell><cell cols="2">0.1100 0.8492</cell><cell>0.5479</cell><cell>0.9250</cell></row><row><cell>MGCA [23]</cell><cell>0.8496</cell><cell>0.3906</cell><cell>0.8428</cell><cell>0.1889</cell><cell cols="2">0.1809 0.8616</cell><cell>0.5696</cell><cell>0.9375</cell></row><row><cell cols="2">MedCLIP [25](*) 0.7805</cell><cell>0.4298</cell><cell>0.7258</cell><cell>0.2032</cell><cell cols="2">-0.1321 0.8214</cell><cell>0.5619</cell><cell>0.9325</cell></row><row><cell>KoBo</cell><cell>0.8590</cell><cell>0.3918</cell><cell>0.8467</cell><cell>0.2563</cell><cell cols="2">0.3712 0.8628</cell><cell>0.6393</cell><cell>0.9550</cell></row><row><cell>KoBo-Vit</cell><cell>0.8635</cell><cell>0.4123</cell><cell>0.8455</cell><cell>0.1824</cell><cell cols="2">0.4229 0.8660</cell><cell>0.6554</cell><cell>0.9525</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>87</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>84</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>81</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>78</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1%</cell><cell>10%</cell><cell></cell><cell>25%</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>(b) Ablation on data amount (CheXpert) Performance (ACC%) Performance (AUC%) ImageNet ImageNet 82.75 89.75 93.75 94.99 95.50 81.00 84.00 87.00 90.00 93.00 96.00 Scratch ImageNet w/o KSE w/o KSG Full (a) Ablation on modules (Covidx) Model setting Training data amount KoBo KoBo Better data robustness Superior transfer ability</head><label></label><figDesc></figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. This research was supported by the <rs type="funder">Intergovernmental Cooperation Project of the National Key Research and Development Program of China</rs>(<rs type="grantNumber">2022YFE0116700</rs>). We thank the <rs type="institution">Big Data Computing Center of Southeast University</rs> for providing the facility support.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tpAjZF7">
					<idno type="grant-number">2022YFE0116700</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Publicly available clinical bert embeddings</title>
		<author>
			<persName><forename type="first">E</forename><surname>Alsentzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-modal masked autoencoders for medical vision-and-language pre-training</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_65</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_65" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="679" to="689" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Align, reason and learn: enhancing medical vision-andlanguage pre-training with knowledge</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5152" to="5161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chatgpt: five priorities for research</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Van Dis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Bockting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">614</biblScope>
			<biblScope unit="issue">7947</biblScope>
			<biblScope unit="page" from="224" to="226" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning better registration to learn better few-shot medical image segmentation: Authenticity, diversity, and robustness</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Geometric visual similarity learning in 3d medical image selfsupervised pre-training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9538" to="9547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RATCHET: medical transformer for Chest X-ray diagnosis and reporting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87234-2_28</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-2_28" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12907</biblScope>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gloria: a multimodal global-local representation learning framework for label-efficient medical image recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chexpert: a large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07042</idno>
		<title level="m">Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Use of word and graph embedding to measure semantic relatedness between unified medical language system concepts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1538" to="1546" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Radiological reports improve pretraining for localized imaging tasks on chest x-rays</title>
		<author>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_62</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_62" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semantic relatedness and similarity reference standards for medical terms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pakhomov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Negbio: a highperformance tool for negation and uncertainty detection in radiology reports</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Summits Trans. Sci. Proc</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">188</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ablation-cam: visual explanations for deep convolutional network via gradient-free localization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Ramaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="983" to="991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dall-e: creating images from text</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D M</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S M</forename><surname>Basha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M C</forename><surname>Hari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Penchalaiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UGC Care Group I J</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="71" to="75" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Breaking with fixed set pathology recognition through report-guided contrastive training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Seibold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reiß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleesiek</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-16443-9_66</idno>
		<ptr target="https://doi.org/10.1007/978-3-031-16443-9_66" />
	</analytic>
	<monogr>
		<title level="m">25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-22">18-22 September 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="690" to="700" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly-supervised segmentation for disease localization in Chest X-Ray images</title>
		<author>
			<persName><forename type="first">O</forename><surname>Viniavskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dobko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dobosevych</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-59137-3_23</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-59137-3_23" />
	</analytic>
	<monogr>
		<title level="m">AIME 2020. LNCS (LNAI)</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Michalowski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Moskovitch</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12299</biblScope>
			<biblScope unit="page" from="249" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-granularity cross-modal alignment for generalized medical visual representation learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vardhanabhuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Covid-net: a tailored deep convolutional neural network design for detection of Covid-19 cases from chest x-ray images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Medclip: contrastive learning from unpaired medical images and text</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3876" to="3887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking graph convolutional networks in knowledge graph completion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="798" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Models genesis: generic autodidactic models for 3D medical image analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2019</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11767</biblScope>
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cham</forename><surname>Springer</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-32251-9_42</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-32251-9_42" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
