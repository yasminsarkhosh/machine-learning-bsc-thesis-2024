,title,header_no,header_title,text,volume
32,AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor,5.0,Conclusion,"in this work, we propose a brain tumor segmentation method for mri images using
only class labels, based on an attentive multiple-exit class activation mapping
(ame-cam). our approach extracts activation maps from different exits of the
network to capture information from multiple resolutions. we then use an
attention model to hierarchically aggregate these activation maps, learning
pixel-wise weighted sums.experimental results on the four modalities of the 2021
brats dataset demonstrate the superiority of our approach compared with other
cam-based weakly-supervised segmentation methods. specifically, ame-cam achieves
the highest dice score for all patients in all datasets and modalities. these
results indicate the effectiveness of our proposed approach in accurately
segmenting brain tumors from mri images using only class labels.",1
133,Infusing Physically Inspired Known Operators in Deep Models of Ultrasound Elastography,2.4,Dataset and Quantitative Metrics,"we use publicly available data collected from a breast phantom (model 059, cirs:
tissue simulation & phantom technology, norfolk, va) using an alpinion e-cube
r12 research us machine (bothell, wa, usa). the center frequency was 8 mhz and
the sampling frequency was 40 mhz. the young's modulus of the experimental
phantom was 20 kpa and contains several inclusions with young's modulus of
higher than 40 kpa. this data is available online at http://code.sonography.ai
in [16].in vivo data was collected at johns hopkins hospital from patients with
liver cancer during open-surgical rf thermal ablation by a research antares
siemens system using a vf 10-5 linear array with the sampling frequency of 40
mhz and the center frequency of 6.67 mhz. the institutional review board
approved the study with the consent of the patients. we selected 600 rf frame
pairs of this dataset for the training of the networks.two well-known metrics of
contrast to noise ratio (cnr) and strain ratio (sr) are utilized to evaluate the
compared methods. two regions of interest (roi) are selected to compute these
metrics and they can be defined as [10]:where the subscript t and b denote the
target and background rois. the sr is only sensitive to the mean (s x ), while
cnr depends on both the mean and the standard deviation (σ x ) of rois. for
stiff inclusions as the target, higher cnr correlates with better target
visibility, and lower sr translates to a higher difference between the target
and background strains.",1
158,Many Tasks Make Light Work: Learning to Localise Medical Anomalies from Multiple Synthetic Tasks,1.0,Introduction,"in recent years, the workload of radiologists has grown drastically, quadrupling
from 2006 to 2020 in western europe [4]. this huge increase in pressure has led
to long patient-waiting times and fatigued radiologists who make more mistakes
[3]. the most common of these errors is underreading and missing anomalies
(42%); followed by missing additional anomalies when concluding their search
after an initial finding (22%) [10]. interestingly, despite the challenging work
environment, only 9% of errors reviewed in [10] were due to mistakes in the
clinicians' reasoning. therefore, there is a need for automated second-reader
capabilities, which brings any kind of anomalies to the attention of
radiologists. for such a tool to be useful, its ability to detect rare or
unusual cases is particularly important. traditional supervised models would not
be appropriate, as acquiring sufficient training data to identify such a broad
range of pathologies is not feasible. unsupervised or self-supervised methods to
model an expected feature distribution, e.g., of healthy tissue, is therefore a
more natural path, as they are geared towards identifying any deviation from the
normal distribution of samples, rather than a particular type of pathology.there
has been rising interest in using end-to-end self-supervised methods for anomaly
detection. their success is most evident at the miccai medical
outof-distribution analysis (mood) challenge [31], where all winning methods
have followed this paradigm so far (2020-2022). these methods use the variation
within normal samples to generate diverse anomalies through sample mixing
[7,[23][24][25]. however all these methods lack a key component: structured
validation. this creates uncertainty around the choice of hyperparameters for
training. for example, selecting the right training duration is crucial to avoid
overfitting to proxy tasks. yet, in practice, training time is often chosen
arbitrarily, reducing reproducibility and potentially sacrificing generalisation
to real anomalies.",1
179,VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis,3.0,Experimental Setup,"materials. we trained our networks using a subset of the open-access intra
dataset1 published by yang et al. in 2020 [32]. this subset consisted of 1694
healthy vessel segments reconstructed from 2d mra images of patients. we
converted 3d meshes into a binary tree representation and used the network
extraction script from the vmtk toolkit2 to extract the centerline coordinates
of each vessel model. the centerline points were determined based on the ratio
between the sphere step and the local maximum radius, which was computed using
the advancement ratio specified by the user. the radius of the blood vessel
conduit at each centerline sample was determined using the computed
crosssections assuming a maximal circular shape (see fig. 2). to improve
computational efficiency during recursive tree traversal, we implemented an
algorithm that balances each tree by identifying a new root. we additionally
trimmed trees to a depth of ten in our experiments. this decision reflects a
balance between the computational demands of depth-first tree traversal in each
training step and the complexity of the training meshes. we excluded from our
study trees that exhibited greater depth, nodes with more than two children, or
with loops. however, non-binary trees can be converted into binary trees and it
is possible to train with deeper trees at the expense of higher computational
costs. ultimately, we were able to obtain 700 binary trees from the original
meshes using this approach.implementation details. for the centerline
extraction, we set the advancement ratio in the vmtk script to 1.05. the script
can sometimes produce multiple cross-sections at centerline bifurcations. in
those cases, we selected the sample with the lowest radius, which ensures proper
alignment with the centerline principal direction. all attributes were
normalized to a range of [0, 1]. for the mesh reconstruction we used 4
iterations of catmull-clark subdivision algorithm. the data pre-processing
pipeline and network code were implemented in python and pytorch
framework.training. in all stages, we set the batch size to 10 and used the adam
optimizer with β 1 = 0.9, β 2 = 0.999, and a learning rate of 1 × 10 -4 . we set
α = .3 and γ = .001 for eq. 1 in our experiments. to enhance computation speed,
we implemented dynamic batching [16], which groups together operations involving
input trees of dissimilar shapes and different nodes within a single input
graph. it takes approximately 12 h to train our models on a workstation equipped
with an nvidia a100 gpu, 80 gb vram, and 256 gb ram. however, the memory
footprint during training is very small (≤1 gb) due to the use of a lightweight
tree representation. this means that the amount of memory required to store and
manipulate our training data structures is minimal. during training, we ensure
that the reconstructed tree aligns with the original structure, rather than
relying solely on the classifier's predictions. we train the classifier using a
crossentropy loss that compares its predictions to the actual values from the
original tree. since the number of nodes in each class is unbalanced, we scale
the weight given to each class in the cross-entropy loss using the inverse of
each class count. during preliminary experiments, we observed that accurately
classifying nodes closer to the tree root is critical. this is because a
miss-classification of top nodes has a cascading effect on all subsequent nodes
in the tree (i.e. skip reconstructing a branch). to account for this, we
introduce a weighting scheme that for each node, assigns a weight to the
cross-entropy loss based on the number of total child nodes. the weight is
normalized by the total number of nodes in the tree.metrics. we defined a set of
metrics to evaluate our trained network's performance. by using these metrics,
we can determine how well the generated 3d models of blood vessels match the
original dataset distribution, as well as the diversity of the generated output.
the chosen metrics have been widely used in the field of blood vessel 3d
modeling, and have shown to provide reliable and accurate quantification of
blood vessels main characteristics [3,13]. we analyzed tortuosity per branch,
the vessel centerline total length, and the average radius of the tree.
tortuosity distance metric [4] is a widely used metric in the field of blood
vessel analysis, mainly because of its clinical importance. it measures the
amount of twistiness in each branch of the vessel. vessel's total length and
average radius were used in previous work to distinguish healthy vasculature
from cancerous malformations. finally, in order to measure the distance across
distributions for each metric, we compute the cosine similarity.",1
238,Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation,1.0,Introduction,"a common challenge for deploying deep learning to clinical problems is the
discrepancy between data distributions across different clinical sites
[6,15,20,28,29]. this discrepancy, which results from vendor or protocol
differences, can cause a significant performance drop when models are deployed
to a new site [2,21,23]. to solve this problem, many unsupervised domain
adaptation (uda) methods [6] have been developed for adapting a model to a new
site with only unlabeled data (target domain) by transferring the knowledge
learned from the original dataset (source domain). however, most uda methods
require sufficient target samples, which are scarce in medical imaging due to
the limited accessibility to patient data. this motivates a new problem of
few-shot unsupervised domain adaptation (fsuda), where only a few unlabeled
target samples are available for training.few approaches [11,22] have been
proposed to tackle the problem of fsuda. luo et. al [11] introduced adversarial
style mining (asm), which uses a pretrained style-transfer module to generate
augmented images via an adversarial process. however, this module requires extra
style images [9] for pre-training. such images are scarce in clinical settings,
and style differences across sites are subtle. this hampers the applicability of
asm to medical image analysis. sm-ppm [22] trains a style-mixing model for
semantic segmentation by augmenting source domain features to a fictitious
domain through random interpolation with target domain features. however, sm-ppm
is specifically designed for segmentation tasks and cannot be easily adapted to
other tasks. also, with limited target domain samples in fsuda, the random
feature interpolation is ineffective in improving the model's
generalizability.in a different direction, numerous uda methods have shown high
performance in various tasks [4,[16][17][18]. however, their direct application
to fsuda can result in severe overfitting due to the limited target domain
samples [22]. previous studies [7,10,24,25] have demonstrated that transferring
the amplitude spectrum of target domain images to a source domain can
effectively convey image style information and diversify training dataset. to
tackle the overfitting issue of existing uda methods, we propose a novel
approach called sensitivityguided spectral adversarial mixup (samix) to augment
training samples. this approach uses an adversarial mixing scheme and a spectral
sensitivity map that reveals model generalizability weaknesses to generate
hard-to-learn images with limited target samples efficiently. samix focuses on
two key aspects. 1) model generalizability weaknesses: spectral sensitivity
analysis methods have been applied in different works [26] to quantify the
model's spectral weaknesses to image amplitude corruptions. zhang et al. [27]
demonstrated that using a spectral sensitivity map to weigh the amplitude
perturbation is an effective data augmentation. however, existing sensitivity
maps only use single-domain labeled data and cannot leverage target domain
information. to this end, we introduce a domain-distance-modulated spectral
sensitivity (dodiss) map to analyze the model's weaknesses in the target domain
and guide our spectral augmentation. 2) sample hardness: existing studies
[11,19] have shown that mining hard-to-learn samples in model training can
enhance the efficiency of data augmentation and improve model generalization
performances. therefore, to maximize the use of the limited target domain data,
we incorporate an adversarial approach into the spectral mixing process to
generate the most challenging data augmentations. this paper has three major
contributions. 1) we propose samix, a novel approach for augmenting target-style
samples by using an adversarial spectral mixing scheme. samix enables
high-performance uda methods to adapt easily to fsuda problems. 2) we introduce
dodiss to characterize a model's generalizability weaknesses in the target
domain. 3) we conduct thorough empirical analyses to demonstrate the
effectiveness and efficiency of samix as a plug-in module for various uda
methods across different tasks.",1
252,Gall Bladder Cancer Detection from US Images with only Image Level Labels,1.0,Introduction,"gbc is a deadly disease that is difficult to detect at an early stage [12,15].
early diagnosis can significantly improve the survival rate [14]. non-ionizing
radiation, low cost, and accessibility make us a popular non-invasive diagnostic
modality for patients with suspected gall bladder (gb) afflictions. however,
identifying signs of gbc from routine us imaging is challenging for radiologists
[11]. in recent years, automated gbc detection from us images has drawn
increased interest [3,5] due to its potential for improving diagnosis and
treatment outcomes. many of these works formulate the problem as an object
detection, since training a image classification model for gbc detection seems
challenging due to the reasons outlined in the abstract (also see fig. 1).
recently, gbcnet [3], a cnn-based model, achieved sota performance on
classifying malignant gb from us images. gbcnet uses a two-stage pipeline
consisting of object detection followed by classification, and requires bounding
box annotations for gb as well as malignant regions for training. such bounding
box annotations surrounding the pathological regions are time-consuming and
require an expert radiologist for annotation. this makes it expensive and
non-viable for curating large datasets for training large dnn models. in another
recent work, [5] has exploited additional unlabeled video data for learning good
representations for downstream gbc classification and obtained performance
similar to [3] using a resnet50 [13] classifier. the reliance of both sota
techniques on additional annotations or data, limits their applicability. on the
other hand, the image-level malignancy label is usually available at a low cost,
as it can be obtained readily from the diagnostic report of a patient without
additional effort from clinicians.instead of training a classification pipeline,
we propose to solve an object detection problem, which involves predicting a
bounding box for the malignancy. the motivation is that, running a classifier on
a focused attention/ proposal region in an object detection pipeline would help
tackle the low inter-class and high intra-class variations. however, since we
only have image-level labels available, we formulate the problem as a weakly
supervised object detection (wsod) problem. as transformers are increasingly
outshining cnns due to their ability to aggregate focused cues from a large area
[6,9], we choose to use transformers in our model. however, in our initial
experiments sota wsod methods for transformers failed miserably. these methods
primarily rely on training a classification pipeline and later generating
activation heatmaps using attention and drawing a bounding box circumscribing
the heatmaps [2,10] to show localization. however, for gbc detection, this line
of work is not helpful as we discussed earlier.inspired by the success of the
multiple instance learning (mil) paradigm for weakly supervised training on
medical imaging tasks [20,22], we train a detection transformer, detr, using the
mil paradigm for weakly supervised malignant region detection. in this, one
generates region proposals for images, and then considers the images as bags and
region proposals as instances to solve the instance classification (object
detection) under the mil constraints [8]. at inference, we use the predicted
instance labels to predict the bag labels. our experiments validate the utility
of this approach in circumventing the challenges in us images and detecting gbc
accurately from us images using only image-level labels.",1
254,Gall Bladder Cancer Detection from US Images with only Image Level Labels,2.0,Datasets,"gallbladder cancer detection in ultrasound images: we use the public gbc us
dataset [3] consisting of 1255 image samples from 218 patients. the dataset
contains 990 non-malignant (171 patients) and 265 malignant (47 patients) gb
images (see fig. 2 for some sample images). the dataset contains image labels as
well as bounding box annotations showing the malignant regions.note that, we use
only the image labels for training. we report results on 5-fold
cross-validation. we did the cross-validation splits at the patient level, and
all images of any patient appeared either in the train or validation split.
polyp detection in colonoscopy images: we use the publicly available kvasir-seg
[17] dataset consisting of 1000 white light colonoscopy images showing polyps
(see fig. 2). since kvasir-seg does not contain any control images, we add 600
non-polyp images randomly sampled from the polypgen [1] dataset.since the
patient information is not available with the data, we use random stratified
splitting for 5-fold cross-validation.",1
298,Interpretable Medical Image Classification Using Prototype Learning and Privileged Information,3.0,Experiments,"data. the proposed approach is evaluated using the publicly available lidc-idri
dataset consisting of 1018 clinical thoracic ct scans from patients with
non-small cell lung cancer (nsclc) [2,3]. each lung nodule with a minimum size
of 3 mm was segmented and annotated with a malignancy score ranging from
1-highly unlikely to 5-highly suspicious by one to four expert raters. nodules
were also scored according to their characteristics with respect to predefined
attributes, namely subtlety (difficulty of detection, 1-extremely subtle,
5-obvious), internal structure (1-soft tissue, 4-air ), pattern of calcification
(1popcorn, 6-absent), sphericity (1-linear, 5-round ), margin (1-poorly defined,
5sharp), lobulation (1-no lobulation, 5-marked lobulation), spiculation (1-no
spiculation, 5-marked spiculation), and texture (1-non-solid, 5-solid ). the
pylidc framework [7] is used to access and process the data. the mean attribute
annotation and the mean and standard deviation of the malignancy annotations are
calculated. the latter was used to fit a gaussian distribution, which serves as
the ground truth label for optimization. samples with a mean expert malignancy
score of 3-indeterminate or annotations from fewer than three experts were
excluded in consistency with the literature [8,9,11].experiment designs. to
ensure comparability with previous work [8,9,11], the main metric used is
within-1-accuracy, where a prediction within one score is considered correct.
five-fold stratified cross-validation was performed using 10 % of the training
data for validation and the best run of three is reported.the algorithm was
implemented using the pytorch framework version 1.13 and cuda version 11.6. a
learning rate of 0.5 was chosen for the prototype vectors and 0.02 for the other
learnable parameters. the batch size was set to 128 and the optimizer was adam
[10]. with a maximum of 1000 epochs, but stopping early if there was no
improvement in target accuracy within 100 epochs, the experiments lasted an
average of three hours on a geforce rtx 3090 graphics card. the code is publicly
available at https://github.com/xrad-ulm/proto-caps.besides pure performance,
the effect of reduced availability of attribute annotations was investigated.
this was done by using attribute information only for a randomly selected
fraction of the nodules during the training.to investigate the effect of
prototypes on the network performance, an ablation study was performed. three
networks were compared: proto-caps (proposed) including learning and applying
prototypes during inference, proto-caps w/o use where prototypes are only
learned but ignored for inference, and proto-caps w/o learn using the proposed
architecture without any prototypes.",2
343,Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures from Routine EHRs for Pulmonary Nodule Classification,3.0,Experimental Setup,"datasets. this study used an imaging-only cohort from the nlst [28] and three
multimodal cohorts from our home institution with irb approval (table 1). for
the nlst cohort (https://cdas.cancer.gov/nlst/), we identified cases who had a
biopsy-confirmed diagnosis of lung malignancy and controls who had a positive
screening result for an spn but no lung malignancy. we randomly sampled from the
control group to obtain a 4:6 case control ratio. next, ehr-pulmonary was the
unlabeled dataset used to learn clinical signatures in an unsupervised manner.
we searched all records in our ehr archives for patients who had billing codes
from a broad set of pulmonary conditions, intending to capture pulmonary
conditions beyond just malignancy. additionally, image-ehr was a labeled dataset
with paired imaging and ehrs. we searched our institution's imaging archive for
patients with three chest cts within five years. in the ehr-image cohort,
malignant cases were labeled as those with a billing code for lung malignancy
and no cancer of any type prior. importantly, this case criteria includes
metastasis from cancer in non-lung locations. benign controls were those who did
not meet this criterion. finally, image-ehr-spn was a subset of image-ehr with
the inclusion criteria that subjects had a billing code for an spn and no cancer
of any type prior to the spn. we labeled malignant cases as those with a lung
malignancy billing code occurring within three years after any scan and only
used data collected before the lung malignancy code. all data within the
five-year period were used for controls. we removed all billing codes relating
to lung malignancy. a description of the billing codes used to define spn and
lung cancer events are provided in supplementary 1.2. training and validation.
all models were pretrained with the nlst cohort after which we froze the
convolutional embedding layer. while this was the only pretraining step for
image-only models (csimage and tdimage), the multimodal models underwent another
stage of pretraining using the image-ehr cohort with subjects from image-ehr-spn
subtracted. in this stage, we randomly selected one scan and the corresponding
clinical signature expressions for each subject and each training epoch. models
were trained until the running mean over 100 global steps of the validation loss
increased by more than 0.2. for evaluation, we performed five-fold
cross-validation with image-ehr-spn, using up to three of the most recent scans
in the longitudinal models. we report the mean auc and 95% confidence interval
from 1000 bootstrapped samples, sampling with replacement from the pooled
predictions across all test folds. a two-sided wilcoxon signed-rank test was
used to test if differences in mean auc between models were
significant.reclassification analysis. we performed a reclassification analysis
of low, medium, and high-risk tiers separated by thresholds of 0.05 and 0.65,
which are the cutoffs used to guide clinical management. given a baseline
comparison, our approach reclassifies a subject correctly if it predicts a
higher risk tier than the baseline in cases, or a lower risk tier than the
baseline in controls (fig. 2).",2
355,Centroid-Aware Feature Recalibration for Cancer Grading in Pathology Images,1.0,Introduction,"globally, cancer is a leading cause of death and the burden of cancer incidence
and mortality is rapidly growing [1]. in cancer diagnosis, treatment, and
management, pathologydriven information plays a pivotal role. cancer grade is,
in particular, one of the major factors that determine the treatment options and
life expectancy. however, the current pathology workflow is sub-optimal and
low-throughput since it is, by and large, manually conducted, and the large
volume of workloads can result in dysfunction or errors in cancer grading, which
have an adversarial effect on patient care and safety [2]. therefore, there is a
high demand to automate and expedite the current pathology workflow and to
improve the overall accuracy and robustness of cancer grading.recently, many
computational tools have shown to be effective in analyzing pathology images
[3]. these are mainly built based upon deep convolutional neural networks
(dcnns). for instance, [4] used dccns for prostate cancer detection and grading,
[5] classified gliomas into three different cancer grades, and [6] utilized an
ensemble of dcnns for breast cancer classification. to further improve the
efficiency and effectiveness of dcnns in pathology image analysis, advanced
methods that are tailored to pathology images have been proposed. for example,
[7] proposed to incorporate both local and global contexts through the
aggregation learning of multiple context blocks for colorectal cancer
classification; [8] extracted and utilized multi-scale patterns for cancer
grading in prostate and colorectal tissues; [9] proposed to re-formulate cancer
classification in pathology images as both categorical and ordinal
classification problems. built based upon a shared feature extractor, a
categorical classification branch, and an ordinal classification branch, it
simultaneously conducts both categorical and ordinal learning for colorectal and
prostate cancer grading; a hybrid method that combines dccns with hand-crafted
features was developed for mitosis detection in breast cancer [10]. moreover,
attention mechanisms have been utilized for an improved pathology image
analysis. for instance, [11] proposed a two-step framework for glioma sub-type
classification in the brain, which consists of a contrastive learning framework
for robust feature extractor training and a sparse-attention block for
meaningful multiple instance feature aggregation. such attention mechanisms have
been usually utilized in a multiple instance learning framework or as
self-attention for feature representations. to the best of our knowledge,
attention mechanisms have not been used for feature representations of class
centroids.in this study, we propose a centroid-aware feature recalibration
network (cafenet) for accurate and robust cancer grading in pathology images.
cafenet is built based upon three major components: 1) a feature extractor, 2) a
centroid update (cup) module, and 3) a centroid-aware feature recalibration
(cafe) module. the feature extractor is utilized to obtain the feature
representation of pathology images. cup module obtains and updates the centroids
of class labels, i.e., cancer grades. cafe module adjusts the input embedding
vectors with respect to the class centroids (i.e., training data distribution).
assuming that the classes are well separated in the feature space, the centroid
embedding vectors can serve as reference points to represent the data
distribution of the training data. this indicates that the centroid embedding
vectors can be used to recalibrate the input embedding vectors of pathology
images. during inference, we fix the centroid embedding vectors so that the
recalibrated embedding vectors do not vary much compared to the input embedding
vectors even though the data distribution substantially changes, leading to
improved stability and robustness of the feature representation. in this manner,
the feature representations of the input pathology images are re-calibrated and
stabilized for a reliable cancer classification. the experimental results
demonstrate that cafenet achieves the state-of-the-art cancer grading
performance in colorectal cancer grading datasets. the source code of cafenet is
available at https://github.com/col in19950703/cafenet.",2
370,Explaining Massive-Training Artificial Neural Networks in Medical Image Analysis Task Through Visualizing Functions Within the Models,3.1,Dynamic Contrast-Enhanced Liver CT,"our xai technique was applied to explain the mtann model's decision in a liver
tumor segmentation task [20]. dynamic contrast-enhanced liver ct scans
consisting of 42 patients with 194 liver tumors in the portal venous phase from
the lits database [21] were used in this study. each slice of the ct volumes in
the dataset has a matrix size of 512 × 512 pixels, with in-plane pixel sizes of
0.60-1.00 mm and thicknesses of 0.20-0.70 mm. the dataset consists of the
original hepatic ct image with the liver mask and the ""gold-standard"" liver
tumor region manually segmented by a radiologist, as illustrated in fig. 2.
firstly, to have the same physical scale on spatial coordinates, bicubic
interpolation was applied on the original hepatic ct images together with the
corresponding liver mask and ""gold-standard"" tumor segmentation to obtain
isotropic images with a voxel size of 0.60 × 0.60 × 0.60 mm 3 . then, to unify
the image size into the same size, the isotropic image was cropped to obtain the
liver region volume of interest (voi) with an in-plane matrix size of 512 × 512.
an anisotropic diffusion filter was applied to reduce the quantum noise, which
could substantially reduce the noise while major structures such as tumors and
vessels maintained [22]. finally, a z-score normalization was applied to unify
complex histograms of tumors in different cases. the final pre-processed ct
images were used as the input images.in addition, since most liver tumors' shape
is ellipsoidal, the liver tumors can also be enhanced by the hessian-based
method and utilized in the model to improve the performance [23,24]. hence, the
model consisted of these two input channels: segmented liver ct image and its
hessian-enhanced image. also, the patches were extracted from input images from
both channels: a 5 × 5 × 5 sized patch in the same spatial position was
extracted to form a training patch with a size of 2 × 5 × 5 × 5 pixels.seven
cases and 24 cases in the dynamic contrast-enhanced ct scans dataset were used
for training and testing, respectively. 10,000 patches were randomly selected
from the liver mask region in each case, summing up to a total of 70,000
training samples for training. the number of input units in the mtann model with
one hidden layer was 250. the structure optimization process started with 80
hidden units in the hidden layer. the binary cross-entropy (bce) loss function
was used to train the model. the mtann model classified the input patches into
tumor or non-tumor classes, and the output pixels represented the probability of
being a tumor class. during the structure optimization process, the f1 score on
the training patches and the dice coefficient on the training images were also
calculated as the reference to select a suitable compact model that performed
equivalently to the original large model.as observed in the four evaluation
metric curves in fig. 3, as the number of hidden units was reduced from 80 to 9,
the performance of the model fluctuated up and down, and after it was reduced
below 9, the performance of the model dropped dramatically. therefore, we chose
a number of hidden units of 9 as the optimized structure.then, we applied the
unsupervised hierarchical clustering algorithm to the weighted function maps
from the optimized compact model with 9 hidden units. figure 4 shows that the 9
hidden units are clearly divided into 3 different groups. we denote hidden units
3, 4, and 7 as group a, hidden units 2, 6, 1, and 8 as group b, and hidden units
0 and 5 as group c. the hidden units in the same group should have a similar
function, and the function maps from each group should show the function of the
group. as illustrated in fig. 5, the low-intensity areas in the function maps of
hidden units 0 and 5 in group c match the high-intensity areas in the
hessian-enhanced input image, which means they suppress the high-intensity
areas. likewise, group a enhances the liver area, and group b suppresses the
non-tumor area. we also understood that groups a and b worked together to
enhance the tumor area, and group c suppressed the liver's boundary as well as
reduced the false enhancements inside the liver. thus, our xai method was able
to reveal the learned functions of groups of neurons in the neural network,
which we call ""functional explanations"" and define as the explanations of the
model behavior by a combination of functions. our method is a post-hoc method
that offers both instance-based and model-based functional explanations.",2
447,Localized Region Contrast for Enhancing Self-supervised Learning in Medical Image Segmentation,3.2,Fine-Tuning Datasets,"during the fine-tuning stage, we perform extensive experiments on three datasets
with respect to different regions of the human body. abd-110 is an abdomen
dataset from [25] that contains 110 ct images from patients with various
abdominal tumors and these ct images were taken during the treatment planning
stage. we report the average dsc on 11 abdominal organs (large bowel, duodenum,
spinal cord, liver, spleen, small bowel, pancreas, left kidney, right kidney,
stomach and gallbladder).thorax-85 is a thorax dataset from [5] that contains 85
thoracic ct images. we report the average dsc on 6 thoracic organs (esophagus,
trachea, spinal cord, left lung, right lung, and heart).han is from [24] and
contains 120 ct images covering the head and neck region. we report the average
dsc on 9 organs (brainstem, mandible, optical chiasm, left optical nerve, right
optical nerve, left parotid, right parotid, left submandibular gland, and right
submandibular gland).",2
463,Faithful Synthesis of Low-Dose Contrast-Enhanced Brain MRI Scans Using Noise-Preserving Conditional GANs,1.0,Introduction,"magnetic resonance imaging (mri) of the brain is an essential imaging modality
to accurately diagnose various neurological diseases ranging from inflammatory
t. pinetz and a. effland-are funded the german research foundation under
germany's excellence strategy -exc-2047/1 -390685813 and -exc2151 -390873048 and
r. haase is funded by a research grant (bonfor; o-194.0002.1). t. pinetz and e.
kobler-contributed equally to this work. lesions to brain tumors and metastases.
for accurate depictions of said pathologies, gadolinium-based contrast agents
(gbca) are injected intravenously to highlight brain-blood barrier dysfunctions.
however, these contrast agents are expensive and may cause nephrogenic systemic
fibrosis in patients with severely reduced kidney function [31]. moreover, [17]
reported that gadolinium accumulates inside patients with unclear health
consequences, especially after repeated application. the american college of
radiology recommends administering the lowest gbca dose to obtain the needed
clinical information [1].driven by this recommendation, several research groups
have recently published dose-reduction techniques focusing on maintaining image
quality. complementary to the development of higher relaxivity contrast agents
[28], virtual contrast [3,8] -replacing a large fraction of the gbca dose by
deep learninghas been proposed. these approaches typically acquire a
contrast-enhanced (ce) scan with a lower gbca dose along with non-ce scans,
e.g., t1w, t2w, flair, or adc. these input images are then processed by a deep
neural network (dnn) to replicate the corresponding standard-dose scan. while
promising, virtual contrast techniques have not been integrated into clinical
practice yet due to falsepositive signals or missed small lesions [3,23]. as
with all deep learning-based approaches, the availability of large datasets is
essential, which is problematic in the considered case since the additional ce
low-dose scan is not acquired in clinical routine exams. hence, there are no
public datasets to easily benchmark and compare different algorithms or evaluate
their performance. in general, the enhancement behavior of pathological tissues
at various gbca dosages has barely been researched due to a lack of data [12].in
recent years, generative models have been used to overcome data scarcity in the
computer vision and medical imaging community. frequently, generative
adversarial networks (gans) [9] are applied as state-of-the-art in image
generation [30] or semantic translation/interpolation [5,18,21]. in a nutshell,
the gan framework trains two competing dnns -the generator and the
discriminator. the generator learns a non-linear transformation of a predefined
noise distribution to fit the distribution of a target dataset, while the
discriminator provides feedback by simultaneously approximating a distance or
divergence between the generated and the target distribution. the choice of this
distance leads to the well-known different gan algorithms, e.g., wasserstein
gans [4,10], least squares gans [24], or non-saturating gans [9]. however, lucic
et al. [22] showed that this choice has only a minor impact on the
performance.learning conditional distributions between images can be
accomplished by additionally feeding a condition (additional scans, dose level,
etc.) into both the generator and discriminator. in particular, for
image-to-image translation tasks, these conditional gans have been successfully
applied using paired [14,25,27] and unpaired training data [35]. within these
methods, an additional content (cycle) loss typically penalizes pixel-wise
deviations (e.g., 1 ) from a corresponding reference to enforce structural
similarity, whereas a local adversarial loss (discriminator with local receptive
field) controls textural similarity. in addition, embeddings have been used to
inject metadata [7,18]. to study the gbca accumulation behavior, we collected
453 ce scans with non-standard gbca doses in the set of {10%, 20%, 33%} along
with the corresponding standard-dose (0.1 mmol/kg) scan after applying the
remaining contrast agent. using this dataset, we aim at the semantic
interpolation of the gbca signal at various fractional dose levels. to this end,
we use gans to learn the contrast enhancement behavior from the dataset
collective and thereby enable the synthesis of contrast signals at various dose
levels for individual cases. further, to minimize the smoothing effect [19] of
typical content losses (e.g., 1 or perceptual [16]), we develop a
noise-preserving content loss function based on the wasserstein distance between
paired image patches calculated using a sinkhornstyle algorithm. this novel loss
enables a faithful generation of noise, which is important for the
identification of enhancing pathologies and their usability as additional
training data.with this in mind, the contributions of this work are as
follows:-synthesis of gbca behavior at various doses using conditional gans,
-loss enabling interpolation of dose levels present in training data,
-noise-preserving content loss function to generate realistic synthetic images.",2
566,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,2.2,Knowledge Condensation and Interaction,"knowledge condensation. it is difficult to directly learn cross-modal
dependencies using the features obtained by the encoder because ct and x-ray
data were collected from different patients. this means that the data may not
have a direct correspondence between two modalities, making it challenging to
capture their relationship. as shown in fig. 1(a), we design a knowledge
condensation (kc) module by introducing a momentum-updated prototype learning
strategy to condensate valuable knowledge in each modality from the learned
features. for the x-ray modality, given its prototypes p cxr = {p cxr 1 , p cxr
2 , ..., p cxr k } initialized randomly and the feature sequence f cxr , kc
module first reduces the spatial resolution of f cxr and groups the reduced f
cxr into k prototypes by calculating the distance between each feature point and
prototypes, shown as followswhere c cxr i suggests the feature points closing to
the i-th prototype. σ(•) represents a linear projection to reduce the feature
sequence length to relieve the computational burden. then we introduce a
momentum learning function to update the prototypes with c cxr i , which means
that the updates at each iteration not only depend on the current c cxr i but
also consider the direction and magnitude of the previous updates, defined
aswhere λ is the momentum factor, which controls the influence of the previous
update on the current update. similarly, the prototypes p ct for ct modality can
be calculated and updated with the feature set f ct . the prototypes effectively
integrate the informative features of each modality and can be considered
modality-specific knowledge to improve the subsequent cross-modal interaction
learning. the momentum term allows prototypes to move more smoothly and
consistently towards the optimal position, even in the presence of noise or
other factors that might cause the prototypes to fluctuate. this can result in a
more stable learning process and more accurate prototypes, thus contributing to
condensate the knowledge of each modality better.knowledge-guided interaction.
the knowledge-guided interaction (ki) module is proposed for unpaired
cross-modality learning, which accepts the learned prototypes from one modality
and features from another modality as inputs. as shown in fig. 1(b), the ki
module contains two multi-head attention (mha) blocks. take ct features f ct and
x-ray prototypes p cxr as input example, the first block considers p cxr as the
query and reduced f ct as the key and value of the attention. it embeds the
x-ray prototypes through the calculated affinity map between f ct and p cxr ,
resulting in the adapted prototype p cxr . the first block can be seen as a
warm-up to make the prototype adapt better to the features from another
modality. the second block treats f ct as the query and the concatenation of
reduced f ct and p cxr as the key and value, improving the f ct through the
adapted prototypes. similarly, for the f cxr and p ct as inputs, the ki module
is also used to boost the x-ray representations. inspired by the knowledge
prototypes, ki modules boost the interaction between the two modalities and
allow for the learning of strong representations for covid-19 segmentation and
x-ray classification tasks.",3
568,Unpaired Cross-Modal Interaction Learning for COVID-19 Segmentation on Limited CT Images,3.1,Materials,"we used the public covid-19 segmentation benchmark [15] to verify the proposed
uci. it is collected from two public resources [5,8] on chest ct images
available on the cancer imaging archive (tcia) [4]. all ct images were acquired
without intravenous contrast enhancement from patients with positive reverse
transcription polymerase chain reaction (rt-pcr) for sars-cov-2. in total, we
used 199 ct images including 149 training images and 50 test images. we also
used two chest x-ray-based classification datasets including chestx-ray14 [18]
and chestxr [1] to assist the uci training. the chestx-ray14 dataset comprises
112,120 x-ray images showing positive cases from 30,805 patients, encompassing
14 disease image labels pertaining to thoracic and lung ailments. an image may
contain multiple or no labels. the chestxr dataset consists of 21,390 samples,
with each sample classified as healthy, pneumonia, or covid-19.",3
735,Asymmetric Contour Uncertainty Estimation for Medical Image Segmentation,3.1,Data,"camus. the camus dataset [20] contains cardiac ultrasounds from 500 patients,
for which two-chamber and four-chamber sequences were acquired.manual
annotations for the endocardium and epicardium borders of the left ventricle
(lv) and the left atrium were obtained from a cardiologist for the end-diastolic
(ed) and end-systolic (es) frames. the dataset is split into 400 training
patients, 50 validation patients, and 50 testing patients. contour points were
extracted by finding the basal points of the endocardium and epicardium and then
the apex as the farthest points along the edge. each contour contains 21
points.private cardiac us. this is a proprietary multi-site multi-vendor dataset
containing 2d echocardiograms of apical two and four chambers from 890 patients.
data comes from patients diagnosed with coronary artery disease, covid, or
healthy volunteers. the dataset is split into a training/validation set (80/20)
and an independent test set from different sites, comprised of 994
echocardiograms from 684 patients and 368 echocardiograms from 206 patients,
respectively. the endocardium contour was labeled by experts who labeled a
minimum of 7 points based on anatomical landmarks and add as many other points
as necessary to define the contour. we resampled 21 points equally along the
contour.jsrt. the japanese society of radiological technology (jsrt) dataset
consists of 247 chest x-rays [26]. we used the 120 points for the lungs and
heart annotation made available by [10]. the set of points contains specific
anatomical points for each structure (4 for the right lung, 5 for the left lung,
and 4 for the heart) and equally spaced points between each anatomical point. we
reconstructed the segmentation map with 3 classes (background, lungs, heart)
with these points and used the same train-val-test split of 70%-10%-20% as [10].",3
776,Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels,3.1,Datasets,"qubiq is a recent challenge held at miccai 2020 and 2021, specifically designed
to evaluate the inter-rater variability in medical imaging. following [23,41],
we use qubiq 2020, which contains 7 segmentation tasks in 4 different ct and mr
datasets: prostate (55 cases, 2 tasks, 6 raters), brain growth (39 cases, 1
task, 7 raters), brain tumor (32 cases, 3 tasks, 3 raters), and kidney (24
cases, 1 task, 3 raters). for each dataset, we calculate the average dice score
between each rater and the majority votes in table 1. in some datasets, such as
brain tumor t2, the inter-rater disagreement can be quite substantial. in line
with [23], we resize all images to 256 × 256. lits contains 201 high-quality ct
scans of liver tumors. out of these, 131 cases are designated for training and
70 for testing. as the ground-truth labels for the test set are not publicly
accessible, we only use the training set. following [36], all images are resized
to 512×512 and the hu values of ct images are windowed to the range of [-60,
140]. kits includes 210 annotated ct scans of kidney tumors from different
patients. in accordance with [36], all images are resized to 512 × 512 and the
hu values of ct images are windowed to the range of [-200, 300].",3
803,BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation,3.1,Experimental Setup,"dataset and preprocessing. the data used in this experiment are obtained from
lidc-idri [2,7] and brats 2021 [4] datasets. lidc-idri contains 1,018 lung ct
scans with plausible segmentation masks annotated by four radiologists. we adopt
a standard preprocessing pipeline for lung ct scans and the trainvalidation-test
partition as in previous work [5,15,23]. brats 2021 consists of four different
sequence (t1, t2, flair, t1ce) mri images for each patient. all 3d scans are
sliced into axial slices and discarded the bottom 80 and top 26 slices. note
that we treat the original four types of brain tumors as one type following
previous work [25], converting the multi-target segmentation problem into
binary. our training set includes 55,174 2d images scanned from 1,126 patients,
and the test set comprises 3,991 2d images scanned from 125 patients. finally,
the sizes of images from lidc-idri and brast 2021 are resized to a resolution of
128 × 128 and 224 × 224, respectively. implementation details. we implement all
the methods with the pytorch library and train the models on nvidia v100 gpus.
all the networks are trained using the adamw [19] optimizer with a mini-batch
size of 32. the initial learning rate is set to 1 × 10 -4 for brats 2021 and 5 ×
10 -5 for lidc-idri. the bernoulli noise estimation u-net network in fig. 1 of
our berdiff is the same as previous diffusion-based models [20]. we employ a
linear noise schedule for t = 1000 timesteps for all the diffusion models. and
we use the sub-sequence sampling strategy of ddim to accelerate the segmentation
process. during minibatch training of lidc-idri, our berdiff learns diverse
expertise by randomly sampling one from four annotated segmentation masks for
each image. four metrics are used for performance evaluation, including
generalized energy distance (ged), hungarian-matched intersection over union
(hm-iou), soft-dice and dice coefficient. we compute ged using varying numbers
of segmentation samples (1, 4, 8, and 16), hm-iou and soft-dice using 16
samples.",4
825,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,0.0,Related Work.,"a literature review by zhang et al. [24] divides deep learning (dl)-based
multimodal segmentation methods into three fusion strategy groups: early, late
and hybrid (also named layer ) fusion. the first two groups of methods are most
commonly applied; early fusion comprises simple concatenation of modalities
along the channel dimension before feeding them into the deep neural network.
additionally, concatenating feature maps (fms) from separate modality encoders
can also be considered as early fusion [7]. late fusion, on the other hand,
employs separate branches for each input modality and then fuses the output
features by either plain concatenation or by weighing the contributions of
separate branches at the decision level. for example, zhang et al. [23] proposed
an attention mechanism to fuse fms from two separate u-nets that accepted
contrast-enhanced arterial and venous phase ct images. the third group, hybrid
fusion, aims to combine the strengths of early and late fusion [24] by employing
two or more separate encoders (i.e. one for each modality) and a single decoder,
where features from different resolution levels of the encoder are fused and fed
into the decoder that produces the final full-resolution segmentation. such
hybrid or multi-level fusion along with the adaptive fusion method represents
the current trend in computer vision [24], with the self-supervised model
adaptation method as a prime example [18]. one important aspect is also the
missing modality scenario, meaning that the multimodal model should produce
satisfactory results even if only one input modality is available. nevertheless,
the optimal fusion strategy remains an open question in need of further
exploration. similar conclusions were reached in a review of multimodal
segmentation methods in the medical imaging community by zhou et al. [25]. most
methods implement either early or late fusion, however, the layer fusion
strategy was identified as a better choice, since dense connections among layers
can exploit more complex and complementary information to enhance training. the
highlight is hyperdensenet, a dual-path 3d network proposed by dolz et al. [4]
that employs dense connections between two convolutional paths, and achieves
improvements compared to other fusion strategies and single modality variants.
however, other studies have shown that the best fusion strategy depends on the
specific nature of the problem, e.g. yan et al. [22] demonstrated that the late
fusion outperforms the other two approaches for the longitudinal detection of
diabetic retinopathy. relevant to the field of multimodal segmentation are also
developments on unpaired multimodal segmentation, where cross-modality learning
is employed to take advantage of different image modalities covering the same
anatomy, but without the constraint to collect images from the same patients
[5,10,19]. although the methodologies comprising cyclegans and/or multiple
segmentation networks [10,19] seem promising, they can be excessively complex
for the task of han oar segmentation where both ct and mr image modalities from
the same patient are often available. consequently, our primary focus is the
paired multimodal segmentation problem, including the missing modality
scenario.motivation. when segmenting oars in the han region for the purpose of
rt planning, a multimodal segmentation model that can leverage the information
from ct and mr images of the same patient might be beneficial compared to
separate single-modal models. firstly, as intuition suggests, such a model would
rely on the ct image for bone structures and on the mr image for soft tissues,
and therefore improve the overall segmentation quality by exploiting the
complementary information from both modalities. secondly, a multimodal model
would facilitate cross-modality learning by extracting knowledge from one and
applying that knowledge to the other modality, potentially improving the
segmentation accuracy. several studies indicated that such an approach is
feasible, for example, for improving video classification by training a model on
an auxiliary audio reconstruction task [12], or for audio-based detection by
using the multimodal knowledge distillation concept, where teacher networks
trained on rgb, depth and thermal images improve a student network trained only
on audio data [20]. finally, from the dl infrastructure maintenance perspective,
it is easier to maintain a single model that can handle both modalities than two
separate models for each modality. however, clinical practice differs
considerably from theory, meaning that a number of considerations must be taken
into account. firstly, although mr image acquisition is recommended, it is not
always feasible due to time constraints, scanner occupancy and financial
aspects. consequently, automatic oar multimodal segmentation is required to
handle the missing modality scenario, and provide a similar segmentation quality
as a single-modality system. secondly, because ct and mr images are not acquired
simultaneously and with the same acquisition parameters (e.g. resolution), there
is an inherent misalignment between both modalities. this can be mitigated with
image registration, but not completely, mainly due to different patient
positioning that especially affects the deformation of soft tissues, and various
modality-specific artifacts (e.g. motion, implants, partial volume effect,
etc.).",4
829,Multimodal CT and MR Segmentation of Head and Neck Organs-at-Risk,3.0,Experiments and Results,"image datasets. the proposed methodology was evaluated on two publicly available
datasets: our recently released han-seg dataset [14] and the pddca dataset [15].
the han-seg dataset comprises ct and t1-weighted mr images of 56 patients, which
were deformably registered with the simpleelastix registration tool, and
corresponding curated manual delineations of 30 oars (for details, please refer
to [14]). although only a subset of images is publicly available1 due to the
ongoing han-seg challenge2 , both the publicly available training as well as the
privately withheld test images were used in our 4-fold cross-validation
experiments. on the other hand, to evaluate the generalization ability of our
method, we also conducted experiments on the ct-only pddca dataset (for details,
please refer to [15]), from which we collected 15 images from the offand on-site
test sets of the corresponding challenge for our evaluation. as this dataset is
widely used for evaluating the performance of automatic han oar segmentation
methods, it serves as a valuable benchmark for comparison with other
state-of-the-art methods. note that none of the images from the ct-only pddca
dataset were used for training, and as our model expects two inputs, we
substituted the missing mr modality with an empty matrix (i.e.
zeros).implementation details. all models were trained for all oars using the 3d
fullres configuration of nnu-net, with the only modification that we reduced
rotation around the axial axis and disabled image flipping along the sagittal
plane, which eliminated segmentation errors that were previously observed for
the paired (left and right) oars. the same modification was also used with the
maml model. to ensure a fair model comparison, we set the number of filters in
the encoder of the single modality baseline model to match the number of filters
of the entry-level concatenation encoder. we also halved the number of filters
in networks that have separate encoders so that the overall number of parameters
in the proposed model and the baselines remains approximately the same
(excluding the parameters in the localization part of mfm block). note that the
maml model, which is composed of two u-nets, had a considerably higher number of
parameters. to address the challenge of a relatively small dataset, we adopted a
4-fold cross-validation strategy without using any external training images. all
models were trained until convergence, i.e. when the validation loss plateaued,
and we selected the model with the best validation loss for inference.results.
the quality of the obtained oar segmentation masks was evaluated by computing
the dice similarity coefficient (dsc) and the 95 th -percentile hausdorff
distance (hd 95 ) against reference manual delineations, and the results for all
oars are presented in figs. 2 and3, respectively. since not all images contain
all 30 oars (due to a different field-of-view), we first calculated the mean
metric for each oar and then the overall mean across all oars to ensure that the
contributions were equally weighted. we also performed analysis of statistical
significance by applying paired sample t-tests with the bonferroni correction,
presented with bars on top of the box plots (non-significant: ns (p > 0.05),
significant: * (0.01 < p < 0.05), * * (0.001 < p < 0.01), * * * (0.0001 < p <
0.001) and * * * * (p < 0.0001)).",4
857,SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI,1.0,Introduction,"breast cancer is the most common cause of cancer-related deaths among women all
around the world [8]. early diagnosis and treatment is beneficial to improve the
survival rate and prognosis of breast cancer patients. mammography,
ultrasonography, and magnetic resonance imaging (mri) are routine imaging
modalities for breast examinations [15]. recent clinical studies have proven
that dynamic contrast-enhanced (dce)-mri has the capability to reflect tumor
morphology, texture, and kinetic heterogeneity [14], and is with the highest
sensitivity for breast cancer screening and diagnosis among current clinical
imaging modalities [17]. the basis for dce-mri is a dynamic t1-weighted contrast
enhanced sequence (fig. 1). t1-weighted acquisition depicts enhancing
abnormalities after contrast material administration, that is, the cancer
screening is performed by using the post-contrast images. radiologists will
analyze features such as texture, morphology, and then make the treatment plan
or prognosis assessment. computer-aided feature quantification and diagnosis
algorithms have recently been exploited to facilitate radiologists analyze
breast dce-mri [12,22], in which automatic cancer segmentation is the very first
and important step.to better support the radiologists with breast cancer
diagnosis, various segmentation algorithms have been developed [20]. early
studies focused on image processing based approaches by conducting graph-cut
segmentation [29] or analyzing low-level hand-crafted features [1,11,19]. these
methods may encounter the issue of high computational complexity when analyzing
volumetric data, and most of them require manual interactions. recently,
deep-learning-based methods have been applied to analyze breast mri. zhang et
al. [28] proposed a mask-guided hierarchical learning framework for breast tumor
segmentation via convolutional neural networks (cnns), in which breast masks
were also required to train one of cnns. this framework achieved a mean dice
value of 72% on 48 testing t1-weighted scans. li et al. [16] developed a
multi-stream fusion mechanism to analyze t1/t2-weighted scans, and obtained a
dice result of 77% on 313 subjects. gao et al. [7] proposed a 2d cnn
architecture with designed attention modules, and got a dice result of 81% on 87
testing samples. zhou et al. [30] employed a 3d affinity learning based
multi-branch ensemble network for the segmentation refinement and generated 78%
dice on 90 testing subjects. wang et al. [24] integrated a combined 2d and 3d
cnn and a contextual pyramid into u-net to obtain a dice result of 76% on 90
subjects. wang et al. [25] proposed a tumor-sensitive synthesis module to reduce
false segmentation and obtained 78% dice value. to reduce the huge annotation
burden for the segmentation task, zeng et al. [27] presented a semi-supervised
strategy to segment the manually cropped dce-mri scans, and attained a dice
value of 78%.although [27] has been proposed to alleviate the annotation effort,
to acquire the voxel-level segmentation masks is still time-consuming and
laborious, see fig. 1(c). weakly-supervised learning strategies such as extreme
points [5,21], bounding box [6] and scribbles [4] can be promising solutions.
roth et al. [21] utilized extreme points to generate scribbles to supervise the
training of the segmentation network. based on [21], dorent et al. [5]
introduced a regularized loss [4] derived from a conditional random field (crf)
formulation to encourage the prediction consistency over homogeneous regions. du
et al. [6] employed bounding boxes to train the segmentation network for organs.
however, the geometric prior used in [6] can not be an appropriate strategy for
the segmentation of lesions with various shapes. to our knowledge, currently
only one weakly-supervised work [18] has been proposed for breast mass
segmentation in dce-mri. this method employed three partial annotation methods
including single-slice, orthogonal-slice (i.e., 3 slices) and interval-slice (∼6
slices) to alleviate the annotation cost, and then constrained segmentation by
estimated volume using the partial annotation. the method obtained a dice value
of 83% using the interval-slice annotation, on a testing dataset containing only
28 patients.in this study, we propose a simple yet effective weakly-supervised
strategy, by using extreme points as annotations (see fig. 1(d)) to segment
breast cancer. specifically, we attempt to optimize the segmentation network via
the conventional trainfine-tuneretrain process. the initial training is
supervised by a contrastive loss to pull close positive voxels in feature space.
the fine-tune is conducted by using a similarity-aware propagation learning
(simple) strategy to update the pseudo-masks for the subsequent retrain. we
evaluate our method on a collected dce-mri dataset containing 206 subjects.
experimental results show our method achieves competitive performance compared
with fully supervision, demonstrating the efficacy of the proposed simple
strategy.",4
925,Edge-Aware Multi-task Network for Integrating Quantification Segmentation and Uncertainty Prediction of Liver Tumor on Multi-modality Non-contrast MRI,1.0,Introduction,"simultaneous multi-index quantification (i.e., max diameter (md), center point
coordinates (x o , y o ), and area), segmentation, and uncertainty prediction of
liver tumor have essential significance for the prognosis and treatment of
patients [6,16]. in clinical settings, segmentation and quantitation are
manually performed by the clinicians through visually analyzing the
contrast-enhanced mri images (cemri) [9,10,18]. however, as shown in fig. 1(b),
contrast-enhanced fig. 1. our method integrates segmentation and quantification
of liver tumor using multi-modality ncmri, which has the advantages of avoiding
contrast agent injection, mutual promotion of multi-task, and reliability and
stability. mri (cemri) has the drawbacks of being toxic, expensive, and
time-consuming due to the need for contrast agents (ca) to be injected [2,4].
moreover, manually annotating medical images is a laborious and tedious process
that requires human expertise, making it manpower-intensive, subjective, and
prone to variation [14]. therefore, it is desirable to provide a reliable and
stable tool for simultaneous segmentation, quantification, and uncertainty
analysis, without requiring the use of contrast agents, as shown in fig.
1(a).recently, an increasing number of works have been attempted on liver tumor
segmentation or quantification [25,26,28,30]. as shown in fig. 1(c), the work
[26] attempted to use the t2fs for liver tumor segmentation, while it ignored
the complementary information between multi-modality ncmri of t2fs and dwi. in
particular, there is evidence that diffusion-weighted imaging (dwi) helps to
improve the detection sensitivity of focal lesions as these lesions typically
have higher cell density and microstructure heterogeneity [20]. the study in
[25,30] attempted to quantify the multi-index of liver tumor, however, the
approach is limited to using multi-phase cemri that requires the injection of
ca. in addition, all these works are limited to a single task and ignore the
constraints and mutual promotion between multi-tasks. available evidence
suggests that uncertainty information regarding segmentation results is
important as it guides clinical decisions and helps understand the reliability
of the provided segmentation. however, current research on liver tumors tends to
overlook this vital task.to the best of our knowledge, although many works focus
on the simultaneous quantization, segmentation, and uncertainty in medical
images (i.e., heart [3,5,11,27], kidney [17], polyp [13]). no attempt has been
made to automatically liver tumor multi-task via integrating multi-modality
ncmri due to the following challenges: (1) the lack of an effective
multi-modality mri fusion mechanism. because the imaging characteristics between
t2fs and dwi have significant differences (i.e., t2fs is good at anatomy
structure information while dwi is good at location information of lesions
[29]). (2) the lack of strategy for capturing the accurate boundary information
of liver tumors. due to the lack of contrast agent injection, the boundary of
the lesion may appear blurred or even invisible in a single ncmri, making it
challenging to accurately capture tumor boundaries [29]. (3) the lack of an
associated multi-task framework. because segmentation and uncertainty involve
pixel-level classification, whereas quantification tasks involve image-level
regression [11]. this makes it challenging to integrate and optimize the
complementary information between multi-tasks.in this study, we propose an
edge-aware multi-task network (eamtnet) that integrates the multi-index
quantification (i.e., center point, max-diameter (md), and area), segmentation,
and uncertainty. our basic assumption is that the model should capture the
long-range dependency of features between multimodality and enhance the boundary
information for quantification, segmentation, and uncertainty of liver tumors.
the two parallel cnn encoders first extract local feature maps of multi-modality
ncmri. meanwhile, to enhance the weight of tumor boundary information, the sobel
filters are employed to extract edge maps that are fed into edge-aware feature
aggregation (eafa) as prior knowledge. then, the eafa module is designed to
select and fuse the information of multi-modality, making our eamtnet edge-aware
by capturing the long-range dependency of features maps and edge maps. lastly,
the proposed method estimates segmentation, uncertainty prediction, and
multi-index quantification simultaneously by combining multi-task and cross-task
joint loss.the contributions of this work mainly include: (1) for the first
time, multiindex quantification, segmentation, and uncertainty of the liver
tumor on multimodality ncmri are achieved simultaneously, providing a
time-saving, reliable, and stable clinical tool. (2) the edge information
extracted by the sobel filter enhances the weight of the tumor boundary by
connecting the local feature as prior knowledge. (3) the novel eafa module makes
our eamtnet edge-aware by capturing the long-range dependency of features maps
and edge maps for feature fusion. the source code will be available on the
author's website.",4
955,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,1.0,Introduction,"prostate segmentation from magnetic resonance imaging (mri) is a crucial step
for diagnosis and treatment planning of prostate cancer. recently, deep
learningbased approaches have greatly improved the accuracy and efficiency of
automatic prostate mri segmentation [7,8]. yet, their success usually requires a
large amount of labeled medical data, which is expensive and expertise-demanding
in practice. in this regard, semi-supervised learning (ssl) has emerged as an
attractive option as it can leverage both limited labeled data and abundant
unlabeled data [3,[9][10][11]15,16,[21][22][23][24][25][26]28]. nevertheless,
the effectiveness of ssl is heavily dependent on the quantity and quality of the
unlabeled data.regarding quantity , the abundance of unlabeled data serves as a
way to regularize the model and alleviate overfitting to the limited labeled
data. unfortunately, such ""abundance"" may be unobtainable in practice, i.e., the
local unlabeled pool is also limited due to restricted image collection
capabilities or scarce patient samples. as a specific case shown in table 1,
there are only limited prostate scans available per center. taking c1 as a case
study, if the amount of local unlabeled data is limited, existing ssl methods
may still suffer from inferior performance when generalizing to unseen test data
(fig. 1). to efficiently enrich the unlabeled pool, seeking support from other
centers is a viable solution, as illustrated in fig. 1. yet, due to differences
in imaging protocols and variations in patient demographics, this solution
usually introduces data heterogeneity, lead-ing to a quality problem. such
heterogeneity may impede the performance of ssl which typically assumes that the
distributions of labeled data and unlabeled data are independent and identically
distributed (i.i.d.) [16]. thus, proper mechanisms are called for this practical
but challenging ssl scenario.here, we define this new ssl scenario as multi-site
semi-supervised learning (ms-ssl), allowing to enrich the unlabeled pool with
multi-site heterogeneous images. being an under-explored scenario, few efforts
have been made. to our best knowledge, the most relevant work is ahdc [2].
however, it only deals with additional unlabeled data from a specific source
rather than multiple arbitrary sources. thus, it intuitively utilizes
image-level mapping to minimize dual-distribution discrepancy. yet, their
adversarial min-max optimization often leads to instability and it is difficult
to align multiple external sources with the local source using a single image
mapping network.in this work, we propose a more generalized framework called
categorylevel regularized unlabeled-to-labeled (cu2l) learning, as depicted in
fig. 2, to achieve robust ms-ssl for prostate mri segmentation. specifically,
cu2l is built upon the teacher-student architecture with customized learning
strategies for local and external unlabeled data: (i) recognizing the importance
of supervised learning in data distribution fitting (which leads to the failure
of cps [3] in ms-ssl as elaborated in sec. 3), the local unlabeled data is
involved into pseudolabel supervised-like learning to reinforce fitting of the
local data distribution; (ii) considering that intra-class variance hinders
effective ms-ssl, we introduce a non-parametric unlabeled-to-labeled learning
scheme, which takes advantage of the scarce expert labels to explicitly
constrain the prototype-propagated predictions, to help the model exploit
discriminative and domain-insensitive features from heterogeneous multi-site
data to support the local center. yet, observing that such scheme is challenging
when significant shifts and various distributions are present, we further
propose category-level regularization, which advocates prototype alignment, to
regularize the distribution of intra-class features from arbitrary external data
to be closer to the local distribution; (iii) based on the fact that
perturbations (e.g., gaussian noises [15]) can be regarded as a simulation of
heterogeneity, perturbed stability learning is incorporated to enhance the
robustness of the model. our method is evaluated on prostate mri data from six
different clinical centers and shows promising performance on tackling ms-ssl
compared to other semi-supervised methods.",4
958,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,2.3,Category-Level Regularized Unlabeled-to-Labeled Learning,"unlabeled-to-labeled learning. inherently, the challenge of ms-ssl stems from
intra-class variation, which results from different imaging protocols, disease
progress and patient demographics. inspired by prototypical networks [13,19,25]
that compare class prototypes with pixel features to perform segmentation, here,
we introduce a non-parametric unlabeled-to-labeled (u2l) learning scheme that
utilizes expert labels to explicitly constrain the prototype-propagated
predictions. such design is based on two considerations: (i) a good
prototypepropagated prediction requires both compact feature and discriminative
prototypes, thus enhancing this prediction can encourage the model to learn in a
variation-insensitive manner and focus on the most informative clues; (ii) using
expert labels as final guidance can prevent error propagation from pseudo
labels. specifically, we denote the feature map of the external unlabeled image
x u e before the penultimate convolution in the teacher model as f u,t e . note
that f u,t e has been upsampled to the same size of x u e via bilinear
interpolation but with l channels. with the argmax pseudo label ŷ u,t e and the
predicted probability map p u,t e , the object prototype from the external
unlabeled data can be computed via confidence-weighted masked average pooling:
c.likewise, the background prototype c u(bg) e can also be obtained. considering
the possible unbalanced sampling of prostate-containing slices, ema strategy
across training steps (with a decay rate of 0.9) is applied for prototype
update. then, as shown in fig. 2 , where we use cosine similarity for sim(•, •)
and empirically set the temperature t to 0.05 [19]. note that a similar
procedure can also be applied to the local unlabeled data x u local , and thus
we can obtain another prototype-propagated unlabeledto-labeled prediction p u2l
local for x l local . as such, given the accurate expert label y l local , the
unlabeled-to-labeled supervision can be computed as:category-level
regularization. being a challenging scheme itself, the above u2l learning can
only handle minor intra-class variation. thus, proper mechanisms are needed to
alleviate the negative impact of significant shift and multiple distributions.
specifically, we introduce category-level regularization, which advocates class
prototype alignment between local and external data, to regularize the
distribution of intra-class features from arbitrary external data to be closer
to the local one, thus reducing the difficulty of u2l learning. in u2l, we have
obtained prototypes from local unlabeled data {c where mean squared error is
adopted as the distance function d(•, •). the weight of background prototype
alignment is smaller due to less relevant contexts.",4
960,Category-Level Regularized Unlabeled-to-Labeled Learning for Semi-supervised Prostate Segmentation with Multi-site Unlabeled Data,3.0,Experiments and Results,"materials. we utilize prostate t2-weighted mr images from six different clinical
centers (c1-6) [1,4,5] to perform a retrospective evaluation. rizes the
characteristics of the six data sources, following [7,8], where [7,8] also
reveal the severity of inter-center heterogeneity here through extensive
experiments. the heterogeneity comes from the differences in scanners, field
strengths, coil types, disease and in-plane/through-plane resolution. compared
to c1 and c2, scans from c3 to c6 are taken from patients with prostate cancer,
either for detection or staging purposes, which can cause inherent semantic
differences in the prostate region to further aggravate heterogeneity. following
[7,8], we crop each scan to preserve the slices with the prostate region only
and then resize and normalize it to 384 × 384 px in the axial plane with zero
mean and unit variance. we take c1 or c2 as the local target center and randomly
divide their 30 scans into 18, 3, and 9 samples as training, validation, and
test sets, respectively.implementation and evaluation metrics. the framework is
implemented on pytorch using an nvidia geforce rtx 3090 gpu. considering the
large variance in slice thickness among different centers, we adopt the 2d
architecture. specifically, 2d u-net [12] is adopted as our backbone. consists
of the cross-entropy loss and the k-regional dice loss [6]. the maximum
consistency weight w max is set to 0.1 [20,26]. t max is set to 20,000. k is
empirically set to 2. the network is trained using the sgd optimizer and the
learning rate is initialized as 0.01 and decayed by multiplication with (1.0t/t
max ) 0.9 . data augmentation is applied, including random flip and rotation. we
adopt the dice similarity coefficient (dsc) and jaccard as the evaluation
metrics and the results are the average over three runs with different
seeds.comparison study. table 2 presents the quantitative results with either c1
or c2 as the local target center, wherein only 6 or 8 local scans are annotated.
besides the supervised-only baselines, we include recent top-performing ssl
methods [2,3,11,14,15,17,20,25,26] for comparison. all methods are implemented
with the same backbone and training protocols to ensure fairness. as observed,
compared to the supervised-only baselines, our cu2l with {6, 8} local labeled
scans achieves {19.15%, 17.42%} and {9.1%, 6.44%} dsc improvements in {c1, c2},
showing its effectiveness in leveraging multi-site unlabeled data. despite the
violation of the assumption of i.i.d. data, existing ssl methods can still
benefit from the external unlabeled data to some extent compared to the results
using local data only as shown in fig. 1, revealing that the quantity of
unlabeled data has a significant impact. however, due to the lack of proper
mechanisms for learning from heterogeneous data, limited improvement can be
achieved by them, especially for cps [3] and fixmatch [14] in c2. particularly,
cps relies on cross-modal pseudo labeling which exploits all the unlabeled data
in a supervised-like fashion. we attribute its degradation to the fact that
supervised learning is crucial for distribution fitting, which supports our
motivation of performing pseudo-label learning on local unlabeled data only. as
a result, its models struggle to determine which distribution to prioritize.
meanwhile, the most relevant ahdc [2] is mediocre in ms-ssl, mainly due to the
instability of adversarial training and the difficulty of aligning multiple
distributions to the local distribution via a single image-mapping network. in
contrast, with specialized mechanisms for simultaneously learning informative
representations from multi-site data and handling heterogeneity, our cu2l
obtains the best performance over the recent ssl methods. figure 3(a) further
shows that the predictions of our method fit more accurately with the ground
truth.ablation study. to evaluate the effectiveness of each component, we
conduct an ablation study under the setting with 6 local labeled scans, as shown
in fig. 2(b). firstly, when we remove l u p l (cu2l-1), the performance drops by
{5.69% (c1), 3.05%(c2)} in dsc, showing that reinforcing confirmation on local
distribution is critical. cu2l-2 represents the removal of both l u2l and l cr ,
and it can be observed that such an unlabeled-to-labeled learning approach
combined with class-level regularization is crucial for exploring multi-site
data. if we remove l cr which accompanies with l u2l (cu2l-3), the performance
degrades, which justifies the necessity of this regularization to reduce the
difficulty of unlabeled-to-labeled learning process. cu2l-4 denotes the removal
of l u sta . as observed, such a typical stability loss [15] can further improve
the performance by introducing hand-crafted noises to enhance the robustness to
real-world heterogeneity.",4
1115,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,1.0,Introduction,"the periodic acquisition and analysis of volumetric ct and mri scans of oncology
patients is essential for the evaluation of the disease status, the selection of
the treatment, and the response to treatment. currently, scans are acquired
every 2-12 months according to the patient's characteristics, disease stage, and
treatment regime. the scan interpretation consists of identifying lesions
(primary tumors, metastases) in the affected organs and characterizing their
changes over time. lesion changes include changes in the size of existing
lesions, the appearance of new lesions, the disappearance of existing lesions,
and complex lesion changes, e.g., the formation of conglomerate lesions. as
treatments improve and patients live longer, the number of scans in longitudinal
studies increases and their interpretation is more challenging and
time-consuming.radiological follow-up requires the quantitative analysis of
lesions and patterns of lesion changes in subsequent scans. it differs from
diagnostic reading since the goal is to find and quantify the differences
between the scans, rather than to find abnormalities in a single scan. in
current practice, quantification of lesion changes is partial and approximate.
the recist 1.1 guidelines call for finding new lesions (if any), identifying up
to the five largest lesions in each scan in the ct slice where they appear
largest, manually measuring their diameters, and comparing their difference [1].
while volumetric measures of individual lesions and of all lesions (tumor
burden) have long been established as more accurate and reliable than partial
linear measurements, they are not used clinically because they require manual
lesion delineation and lesion matching in unregistered scans, which is usually
time-consuming and subject to variability [2].in a previous paper, we presented
an automatic pipeline for the detection and quantification of lesion changes in
pairs of ct liver scans [3]. this paper describes a graph-based lesion tracking
method for the comprehensive analysis of lesion changes and their patterns at
the lesion level. the tasks are formalized as graph-theoretic problems (fig. 1).
complex lesion changes include merged lesions, which occurs when at least two
lesions grow and merge into one (possible disease progression), split lesions,
which occurs when a lesion shrinks and cleaves into several parts (possible
response to treatment) and conglomeration of lesions, which occurs when clusters
of lesions coalesce. while some of these lesion changes have been observed [4],
they have been poorly studied. comprehensive quantitative analysis of lesion
changes and patterns is of clinical importance, since response to treatment may
vary among lesions, so the analysis of a few lesions may not be
representative.the novelties of this paper are: 1) identification and
formalization of longitudinal lesion matching and patterns of lesion changes in
ct in a graph-theoretic framework; 2) new classification and detection of
changes of individual lesions and lesion patterns based on the properties of the
lesion changes graph and its connected components; 3) a simultaneous lesion
matching method with more than two scans; 4) graph-based methods for the
detection of changes in individual lesions and patterns of lesion changes.
experimental results on lung (83 cts, 19 patients) and liver (77 cects, 18
patients) datasets show that our method yields high classification accuracy.to
the best of our knowledge, ours is the first method to perform longitudinal
lesion matching and lesion changes pattern detection. only a few papers address
lesion matching in pairs of ct/mri scans [5][6][7][8][9][10][11][12][13] -none
performs simultaneous matching of all lesions in more than two scans. also, very
few methods [3,14] handle matching of split/merged lesions. although many
methods exist for object tracking in optical images and videos [15][16][17],
they are unsuited for analyzing lesion changes since they assume many
consecutive 2d images where objects have very similar appearance and undergo
small changes between images. overlap-based methods pair two lesions in
registered scans when their segmentations overlap, with a reported accuracy of
66-98% [3,[5][6][7][8][9][10][11]18]. these methods assume that organs and
lesions undergo minor changes, are very sensitive to registration errors, and
cannot handle complex lesion changes. similarity-based methods pair two lesions
with similar features, e.g., intensity, shape, location [13][14][15][16] with an
84-96% accuracy on the deeplesion dataset [14]. they are susceptible to major
changes in the lesion appearance and do not handle complex lesion changes.
split-andmerge matching methods are used for cell tracking in fluorescence
microscopy [19]. they are limited to 2d images, assume registration between
images, and do not handle conglomerate changes.",5
1117,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,2.1,Problem Formalization,"let s = s 1 , . . . , s n be a series of n ≥ 2 consecutive patient scans
acquired at timesis a set of vertices v i j corresponding to the lesions
associated with the lesion segmentation masks l i = l i 1 , l i 2 , . . . , l i
n i , where n i ≥ 0 is the number of lesions in scan s i at time t i . by
definition, any two lesionsj,l indicates that the lesions corresponding to
vertices v i j , v k l are the same lesion, i.e., that the lesion appears in
scans s i , s k in the same location. edges of consecutive scans s i , s i+1 are
called consecutive edges; edges of non-consecutive scans, s i , s k , i < k -1,
are called non-consecutive edges. the in-and out-degree of a vertex v i j , d in
(v i j ) and d out (v i j ), are the number of incoming and outcoming edges,
respectively.let cc = {cc m } m m=1 be the set of connected components of the
undirected graph version of g, where m is the number of connected components
andby definition, for each 1 ≤ m ≤ m , the sets v m , e m are mutually disjoint
and their unions are v , e, respectively. in a connected component cc m , there
is an undirected path between any two vertices v i j , v k l consisting of a
sequence of undirected edges in e m . . in this setup, connected components
correspond to matched lesions and their pattern of evolution over time (fig.
1d).we define seven mutually exclusive individual lesion change labels for
lesion v i j in scan s i based on the vertex in-and out-degrees (fig. 2). in the
following definitions we refer to the indices: 1 ≤ k < i < l ≤ n ; 1) lone: a
lesion present in scan s i and absent in all previous scans s k and subsequent
scans s l ; 2) new: a lesion present in scan s i and absent in all previous
scans s k ; 3) disappeared: a lesion present in scan s i and absent in all
subsequent scans s l ; 4) unique: a lesion present in scan s i and present as a
single lesion in a previous scan s k and/or in a subsequent scan s l ; 5)
merged: a lesion present in scan s i and present as two or more lesions in a
previous scan s k ; 6) split: a lesion present in scan s i and present as two or
more lesions in a subsequent scan s l ; 7) complex: a lesion present as two or
more lesions in at least one previous scan s k and at least one subsequent scan
s l . we also define as existing a lesion present in scan s i and present in at
least one previous scan s k and one subsequent scan s l , (d in (v i j ) ≥ 1, d
out (v i j ) ≥ 1). for the first and current scans s 1 and s n , we set d in (v
1 j ) = 1, d out (v n j ) = 1, i.e., the lesion existed before the first scan or
remains after the last scan. thus, lesions in the first (last) scan can only be
unique, disappeared or split (unique, new or merged). finally, when lesion v i j
is merged and d out (v i j ) = 0, i < n , it is also labeled disappeared; when
it is split and d in (v i j ) = 0, i > 1, it is also labeled new. we define five
patterns of lesion changes based on the properties of the connected components
cc m of g and on the labels of lesion changes: 1) single_p: a connected
component cc m = v i j consisting of a single lesion labeled as lone, new,
disappeared; 2) linear_p: a connected component consisting of a single earliest
vertex v the changes in individual lesions and the detection and classification
of patterns of lesion changes consist of constructing a graph whose vertices are
the corresponding lesion in the scans, computing the graph consecutive and
non-consecutive edges that correspond to lesion matchings, computing the
connected components of the resulting graph, and assigning an individual lesion
change label to each vertex and a lesion change pattern label to each connected
component according to the categories above.",5
1119,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,3.0,Experimental Results,"we evaluated our method with two studies on retrospectively collected patient
datasets that were manually annotated by an expert radiologist.dataset: lung and
liver ct studies were retrospectively obtained from two medical centers
(hadassah univ hosp jerusalem israel) during the routine clinical examination of
patients with metastatic disease. each patient study consists of at least 3
scans.dlung consists of 83 chest ct scans from 19 patients with a mean 4.4 ± 2.0
scans/patient, a mean time interval between consecutive scans of 125.9 ± 81.3
days, and voxel sizes of 0.6-1.0 × 0.6-1.0 × 1.0-3.0 mm 3 . dliver consists of
77 abdominal cect scans from 18 patients with a mean 4.3 ± 2.0 scans/patient, a
mean time interval between consecutive scans of 109.7 ± 93.5 days, and voxel
sizes of 0.6-1.0 × 0.6-1.0 × 0.8-5.0 mm 3 .lesions in both datasets were
annotated by an expert radiologist, yielding a total of 1,178 lung and 800 liver
lesions, with a mean of 14.2 ± 19.1 and 10.4 ± 7.9 lesions/scan (lesions with
<20 voxels were excluded). ground-truth lesion matching graphs and lesion
changes labeling were produced by running the method on the datasets and then
having the radiologist review and correct the resulting node labels and
edges.study 1: lesion changes labeling, lesion matching, evaluation of patterns
of lesion changes. we ran our method on the dlungs and dliver lesion
segmentations. the settings of the parameters were: dilation distance d = 1 mm,
overlap percentage p = 10%, number of iterations r = 5 and 7, and centroid
maximum distance δ = 17 and 23 mm for the lungs and liver lesions,
respectively.we compared the computed and ground-truth lesion changes graphs
with two metrics: 1) lesion changes classification accuracy, which is the % of
correct computed labels from the ground truth labels; 2) lesion matching
precision and recall based on the presence/absence of computed vs. ground truth
edges. the precision and recall definitions were adapted so that wrong or missed
non-consecutive edges are counted as true positive when there is a path between
their vertices in either the ground-truth or the computed graph. table 1
summarizes the results. the distribution of lesion changes labels for dlungs
(1,178 lesions) is unique 785 (67%), new 215 (18%), lone 109 (9%), disappeared
51 (4%), merged 12 (1%), split 6 (1%), complex 0 (0%) with class accuracy ≥ 96%
for all except split (66%). for dliver (800 lesions) it is unique 450 (56%), new
185 (23%), lone 45 (6%), disappeared 77 (10%), merged 27 (3%), split 18 (2%),
complex 1 (0.05%) with class accuracy ≥ 81% for all except disappeared (71%) and
split (67%).for the patterns of lesion changes, we compared the computed and
ground truth patterns of lesion changes. the accuracy is the % of identical
connected components in each category. table 1 summarizes the results. note that
the split_p, merged_p and complex_p patterns jointly account for 3% and 8% of
the cases. these patterns are hard to detect manually but their correct
classification and tracking are crucial for the proper application of the recist
1.1 follow-up protocol [1]. study 2: detection of missed lesions in the ground
truth. the expert radiologist was asked to examine non-consecutive edges and
lesions labeled as lone in the lesion changes graph and determine if lesions
were unseen or undetected (actual or presumed false negative) in the skipped or
contiguous scans (fig. 1d). for each non-consecutive edge connecting lesions v i
j , v k l , he analyzed the corresponding region in the skipped scans s j at t j
∈ ]t i , t k [ for possible missed lesions. for the dlungs dataset, 25 visible
and 5 faintly visible or surmised to be present unmarked lesions were found for
27 nonconsecutive edges. for the dliver dataset, 20 visible and 21 faintly
visible or surmised to be present unmarked lesions were found for 25
non-consecutive edges.after reviewing the 42 and 37 lesions labeled as lone in
dlungs and dliver with > 5mm diameter, the radiologist determined that 1 and 8
of them had been wrongly identified as a cancerous lesion. moreover, he found
that 14 and 16 lesions initially labeled as lone, had been wrongly classified:
for these lesions he found 15 and 21 previously unmarked matching lesions in the
next or previous scans. in total, 45 and 62 missing lesions were added to the
ground truth dlungs and dliver datasets, respectively. these hard-to-find
ground-truth false negatives (3.7%, 7.2% of all lesions) may change the
radiological interpretation and the disease status. see the supplemental
material for examples of these scenarios.",5
1120,Graph-Theoretic Automatic Lesion Tracking and Detection of Patterns of Lesion Changes in Longitudinal CT Studies,4.0,Conclusion,"the use of graph-based methods for lesion tracking and detection of patterns of
lesion changes was shown to achieve high accuracy in classifying changes in
individual lesion and identifying patterns of lesion changes in liver and lung
longitudinal ct studies of patients with metastatic disease. this approach has
proven to be useful in detecting missed, faint, and surmised to be present
lesions, otherwise hardly detectable by examining the scans separately or in
pairs, leveraging the added information provided by evaluating all patient's
scans simultaneously using the labels from the lesion changes graph and
non-consecutive edges.",5
1122,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,1.0,Introduction,"neuropsychiatric systemic lupus erythematosus (npsle) refers to a complex
autoimmune disease that damages the brain nervous system of patients. the
clinical symptoms of npsle include cognitive disorder, epilepsy, mental illness,
etc., and patients with npsle have a nine-fold increased mortality compared to
the general population [11]. since the pathogenesis and mature treatment of
npsle have not yet been found, it is extremely important to detect npsle at its
early stage and put better clinical interventions and treatments to prevent its
progression. however, the high overlap of clinical symptoms with other
psychiatric disorders and the absence of early non-invasive biomarkers make
accurate diagnosis difficult and time-consuming [3].although conventional
magnetic resonance imaging (mri) tools are widely used to detect brain injuries
and neuronal lesions, around 50% of patients with npsle present no brain
abnormalities in structural mri [17]. in fact, metabolic changes in many brain
diseases precede pathomorphological changes, which indicates proton magnetic
resonance spectroscopy ( 1 h-mrs) to be a more effective way to reflect the
early appearance of npsle. 1 h-mrs is a non-invasive neuroimaging technology
that can quantitatively analyze the concentration of metabolites and detect
abnormal metabolism of the nervous system to reveal brain lesions. however, the
complex noise caused by overlapping metabolite peaks, incomplete information on
background components, and low signal-tonoise ratio (snr) disturb the analysis
results of this spectroscopic method [15]. meanwhile, the individual differences
in metabolism and the interaction between metabolites under low sample size make
it difficult for traditional learning methods to distinguish npsle. figure 1
shows spectra images of four participants including healthy controls (hc) and
patients with npsle. it can be seen that the visual differences between patients
with npsle and hcs in the spectra of the volumes are subtle. therefore, it is
crucial to develop effective learning algorithms to discover metabolic
biomarkers and accurately diagnose npsle. the machine learning application for
biomarker analysis and early diagnosis of npsle is at a nascent stage [4]. most
studies focus on the analysis of mr images using statistical or machine learning
algorithms, such as mann-whitney u test [8], support vector machine (svm)
[7,24], ensemble model [16,22], etc. generally, machine learning algorithms
based on the minimum mean square error (mmse) criterion heavily rely on the
assumption that noise is of gaussian distribution. however, measurement-induced
non-gaussian noise in 1 h-mrs data undoubtedly limits the performance of
mmse-based machine learning methods.on the other hand, for the discovery task of
potential biomarkers, sparse codingbased methods (e.g., 2,1 norm, 2,0 norm,
etc.) force row elements to zero that remove some valuable features [12,21].
more importantly, different brain regions have different functions and
metabolite concentrations, which implies that the metabolic features for each
brain region have different sparsity levels. therefore, applying the same
sparsity constraint to the metabolic features of all brain regions may not
contribute to the improvement of the diagnostic performance of npsle.in light of
this, we propose a robust exclusive adaptive sparse feature selection (reasfs)
algorithm to jointly address the aforementioned problems in biomarker discovery
and early diagnosis of npsle. specifically, we first extend our feature learning
through generalized correntropic loss to handle data with complex non-gaussian
noise and outliers. we also present the mathematical analysis of the adaptive
weighting mechanism of generalized correntropy. then, we propose a novel
regularization called generalized correntropy-induced exclusive 2,1 to
adaptively accommodate various sparsity levels and preserve informative
features. the experimental results on a benchmark npsle dataset demonstrate the
proposed method outperforms comparing methods in terms of early noninvasive
biomarker discovery and early diagnosis.",5
1123,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,2.0,Method,"dataset and preprocessing: the t2-weighted mr images of 39 participants
including 23 patients with npsle and 16 hcs were gathered from our affiliated
hospital. all images were acquired at an average age of 30.6 years on a signa
3.0t scanner with an eight-channel standard head coil. then, the mr images were
transformed into spectroscopy by multi-voxel 1 h-mrs based on a point-resolved
spectral sequence (press) with a two-dimensional multi-voxel technique. the
collected spectroscopy data were preprocessed by a sage software package to
correct the phase and frequency. an lcmodel software was used to fit the
spectra, correct the baseline, relaxation, and partial-volume effects, and
quantify the concentration of metabolites. finally, we used the absolute naa
concentration in single-voxel mrs as the standard to gain the absolute
concentration of metabolites, and the naa concentration of the corresponding
voxel of multi-voxel 1 h-mrs was collected consistently. the spectra would be
accepted if the snr is greater than or equal to 10 and the metabolite
concentration with standard deviations (sd) is less than or equal to 20%. the
absolute metabolic concentrations, the corresponding ratio, and the linear
combination of the spectra were extracted from different brain regions: rpcg,
lpcg, rdt, ldt, rln, lln, ri, rpwm, and lpwm. a total of 117 metabolic features
were extracted, and each brain region contained 13 metabolic features: cr,
phosphocreatine (pcr), cr+pcr, naa, naag, naa+naag, naa+naag/cr+pcr, mi,
mi/cr+pcr, cho+phosphocholine (pch), cho+pch/cr+pcr, glu+gln, and
glu+gln/cr+pcr.",5
1127,Robust Exclusive Adaptive Sparse Feature Selection for Biomarker Discovery and Early Diagnosis of Neuropsychiatric Systemic Lupus Erythematosus,3.0,Experimental Results and Conclusion,"experimental settings: the parameters α and λ 1 are are set to 1, while β and λ
2 are searched form {0.5, 1, 2, 5} and {0.1, 0.5, 1}, respectively. we use adam
optimizer and the learning rate is 0.001. to evaluate the performance of
classification, we employ a support vector machine as the basic classifier,
where the kernel is set as the radial basis function (rbf) and parameter c is
set to 1. we average the 3-fold cross-validation results.results and discussion:
we compare the classification accuracy of the proposed reasfs with several sota
baselines, including two filter methods: maximal information coefficient (mic)
[5], gini [23], and four sparse coding-based methods: multi-task feature
learning via 2,1 norm [6,12], discriminative feature selection via 2,0 norm
[21], feature selection via 1,2 norm [10] and exclusive 2,1 [9]. the proposed
reasfs is expected to have better robustness and flexibility. it can be seen
from fig. 2 that the sparse coding-based methods achieve better performance than
filter methods under most conditions, where ""0%"" represents no noise
contamination. the highest accuracy of our reasfs demonstrates the effectiveness
and flexibility of the proposed gcie 2,1 . generally speaking, the probability
of samples being contaminated by random noise is equal. therefore, we randomly
select features from the training set and replace the selected features with
pulse noise. the number of noisy attributes is denoted by the ratio between the
numbers of selected features and total features, such as 15% and 30%. the
classification performances of the npsle dataset contaminated by attribute noise
are shown in fig. 3(a) and fig. 3(b), where one clearly perceives that our
reasfs achieves the highest accuracy under all conditions. besides, it is
unreasonable to apply the same level of sparse regularization to noise features
and uncontaminated features, and our gcie 2,1 can adaptively increase the sparse
level of noise features to remove redundant information, and vice versa. for
label noise, we randomly select samples from the training set and replace
classification labels of the selected samples with opposite values, i.e., 0 → 1
and 1 → 0. the results are shown in fig. 3(c) and fig. 3(d), where the proposed
reasfs is superior to other baselines. it can be seen from fig. 3 that our
reasfs achieves the highest accuracy in different noisy environments, which
demonstrates the robustness of generalized correntropic loss. for non-invasive
biomarkers, our method shows that some metabolic features contribute greatly to
the early diagnosis of npsle, i.e., naag, mi/cr+pcr, and glu+gln/cr+pcr in rpcg;
cr+pcr, naa+naag, naa+naag/cr+pcr, mi/cr+pcr and glu+gln in lpcg; naa, naag, and
cho+pch in ldt; pcr, cr+pcr, cho+pch, cho+pch/cr+pcr and glu+gln/cr+pcr in rln;
mi/cr+pcr, cho+pch and cho+pch/cr+pcr in lln; naa+naag/cr+pcr and cho+pch in ri;
cho+pch/cr+pcr and glu+gln/cr+pcr in rpwm; and pcr, naag and naa+naag/cr+pcr in
lpwm. moreover, we use isometric feature mapping (isomap) [19] to analyze these
metabolic features and find that this feature subset is essentially a
low-dimensional manifold. meanwhile, by combining the proposed reasfs and
isomap, we can achieve 99% accuracy in the early diagnosis of npsle. in
metabolite analysis, some studies have shown that the decrease in naa
concentration is related to chronic inflammation, damage, and tumors in the
brain [18]. in the normal white matter area, different degrees of npsle disease
is accompanied by different degrees of naa decline, but structural mri is not
abnormal, suggesting that naa may indicate the progress of npsle. we also found
that glu+gln/cr+pcr in ri decreased, which indicates that the excitatory
neurotransmitter glu in the brain of patients with npsle may have lower
activity. to sum up, the proposed method provides a shortcut for revealing the
pathological mechanism of npsle and early detection.",5
1135,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,3.1,Dataset,"our dataset nsclc-tcia for lung cancer histological subtype classification is
sourced from two online resources of the cancer imaging archive (tcia) [5]:
nsclc radiomics [1] and nsclc radiogenomics [2]. exclusion criteria involves
patients diagnosed with large cell carcinoma or not otherwise specified, along
with cases that have contouring inaccuracies or lacked tumor delineation [9,13].
finally, a total of 325 available cases (146 adc cases and 179 scc cases) are
used for our study. we evaluate the performance of nsclc classification in
five-fold cross validation on the nsclc-tcia dataset, and measure accuracy
(acc), sensitivity (sen), specificity (spe), and the area under the receiver
operating characteristic (roc) curve (auc) as evaluation metrics. we also
conduct analysis including standard deviations and 95% ci, and delong
statistical test for further auc comparison.for preprocessing, given that the ct
data from nsclc-tcia has an in-plane resolution of 1 mm × 1 mm and a slice
thickness of 0.7-3.0 mm, we resample the ct images using trilinear interpolation
to a common resolution of 1mm × 1mm × 1mm. then one 128 × 128 pixel slice is
cropped from each view as input based on the center of the tumor. finally
following [7], we clip the intensities of the input patches to the interval
(-1000, 400 hounsfield unit) and normalize them to the range of [0, 1].",5
1138,CARL: Cross-Aligned Representation Learning for Multi-view Lung Cancer Histology Classification,4.0,Conclusion,"in summary, we propose a novel multi-view method called cross-aligned
representation learning (carl) for accurately distinguishing between adc and scc
using multi-view ct images of nsclc patients. it is designed with a cross-view
representation alignment learning network which effectively generates
discriminative view-invariant representations in the common subspace to reduce
the discrepancies among multi-view images. in addition, we leverage a
view-specific representation learning network to acquire viewspecific
representations as a necessary complement. the generated view-invariant and
-specific representations together offer a holistic and disentangled perspective
of the multi-view ct images for histological subtype classification of nsclc.
the experimental result on nsclc-tcia demonstrates that carl reaches 0.817 auc,
76.8% acc, 73.2% sen, and 79.7% spe and surpasses other relative approaches,
confirming the effectiveness of the proposed carl method.",5
1186,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,0.0,abbreviated as PARE.,"at the intra-level, the contextual information of the nodules provides clues
about their shape, size, and surroundings, and the integration of this
information can facilitate a more reliable diagnosis of whether they are benign
or malignant. motivated by this, we first segment the context structure, i.e.,
nodule and its surroundings, and then aggregate the context information to the
nodule representation via the attention-based dependency modeling, allowing for
a more comprehensive understanding of the nodule itself. at the inter-level, we
hypothesize that the diagnosis process does not have to rely solely on the
current nodule itself, but can also find clues from past learned cases. this is
similar to how radiologists rely on their accumulated experience in clinical
practice. thus, the model is expected to have the ability to store and recall
knowledge, i.e., the knowledge learned can be recorded in time and then recalled
as a reference for comparative analysis. to achieve this, we condense the
learned nodule knowledge in the form of prototypes, and recall them to explore
potential inter-level clues as an additional discriminant criterion for the new
case. to fulfill both ldct and ncct screening needs, we curate a large-scale
lung nodule dataset with pathology-or follow-up-confirmed benign/malignant
labels. for the ldct, we annotate more than 12,852 nodules from 8,271 patients
from the nlst dataset [14]. for the ncct, we annotate over 4,029 nodules from
over 2,565 patients from our collaborating hospital. experimental results on
several datasets demonstrate that our method achieves outstanding performance on
both ldct and ncct screening scenarios.our contributions are summarized as
follows: (1) we propose context parsing to extract and aggregate rich contextual
information for each nodule. (2) we condense the diagnostic knowledge from the
learned nodules into the prototypes and use them as a reference to assist in
diagnosing new nodules. (3) we curate the largest-scale lung nodule dataset with
high-quality benign/malignant labels to fulfill both ldct and ncct screening
needs. (4) our method achieves advanced malignancy prediction performance in
both screening scenarios (0.931 auc), and exhibits strong generalization in
external validation, setting a new state of the art on lungx (0.801 auc).",5
1193,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,3.1,Datasets and Implementation Details,"data collection and curation: nlst is the first large-scale ldct dataset for
low-dose ct lung cancer screening purpose [14]. there are 8,271 patients
enrolled in this study. an experienced radiologist chose the last ct scan of
each for l = 1, ..., l do 12:cross prototype attention 13:end for 15:",5
1195,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,0.0,17:,"j ← seg loss(m, s) + 3 i=1 cls loss(y, p i ) update loss18: end for patient, and
localized and labeled the nodules in the scan as benign or malignant based on
the rough candidate nodule location and whether the patient develops lung cancer
provided by nlst metadata. the nodules with a diameter smaller than 4mm were
excluded. the in-house cohort was retrospectively collected from 2,565 patients
at our collaborating hospital between 2019 and 2022. unlike nlst, this dataset
is noncontrast chest ct, which is used for routine clinical care. segmentation
annotation: we provide the segmentation mask for our in-house data, but not for
the nlst data considering its high cost of pixel-level labeling. the nodule mask
of each in-house data was manually annotated with the assistance of ct labeler
[20] by our radiologists, while other contextual masks such as lung, vessel, and
trachea were generated using the totalsegmentator [21].",5
1196,Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction Like Radiologists,0.0,Train-Val-Test:,"the training set contains 9,910 (9,413 benign and 497 malignant) nodules from
6,366 patients at nlst, and 2,592 (843 benign and 1,749 malignant) nodules from
2,113 patients at the in-house cohort. the validation set contains 1,499 (1,426
benign and 73 malignant) nodules from 964 patients at nlst. the nlst test set
has 1,443 (1,370 benign and 73 malignant) nodules from 941 patients. the
in-house test set has 1,437 (1,298 benign and 139 malignant) nodules from 452
patients. we additionally evaluate our method on the lungx [2] challenge
dataset, which is usually used for external validation in previous work
[6,11,24]. lungx contains 83 (42 benign and 41 malignant) nodules, part of which
(13 scans) were contrast-enhanced. segmentation: we also evaluate the
segmentation performance of our method on the public nodule segmentation dataset
lidc-idri [3], which has 2,630 nodules with nodule segmentation mask. evaluation
metrics: the area under the receiver operating characteristic curve (auc) is
used to evaluate the malignancy prediction performance.implementation: all
experiments in this work were implemented based on the nnunet framework [8],
with the input size of 32 × 48 × 48, batch size of 64, and total training
iterations of 10k. in the context patch embedding, each patch token is generated
from a window of 8 × 8 × 8. the hyper-parameters of pare are empirically set
based on the ablation experiments on the validation set. for example, the
transformer layer is set to 4 in both sca and cpa modules, and the number of
prototypes is fixed to 40 by default. more details can be found in the ablation.
due to the lack of manual annotation of nodule masks for the nlst dataset, we
can only optimize the segmentation task using our in-house dataset, which has
manual nodule masks.",5
1228,Text-Guided Foundation Model Adaptation for Pathological Image Classification,4.0,Experimental Settings,"dataset. we adopt the patchgastric [25] dataset, which includes
histopathological image patches extracted from h&e stained whole slide images
(wsi) of stomach adenocarcinoma endoscopic biopsy specimens. there are 262,777
patches of size 300 × 300 extracted from 991 wsis at x20 magnification. the
dataset contains 9 subtypes of gastric adenocarcinoma. we choose 3 major
subtypes including ""well differentiated tubular adenocarcinoma"", ""moderately
differentiated tubular adenocarcinoma"", and ""poorly differentiated
adenocarcinoma"" to form a 3-class grading-like classification task with 179,285
patches from 693 wsis. we randomly split the wsis into train (20%) and
validation (80%) subsets for measuring the model performance. to extend our
evaluation into the real-world setting with insufficient data, we additionally
choose 1, 2, 4, 8, or 16 wsis with the largest numbers of patches from each
class as the training set.the evaluation metric is patient-wise accuracy, where
the prediction of a wsi is obtained by a soft vote over the patches, and
accuracy is averaged class-wise.implementation. we use clip vit-b/16 [5] as the
visual backbone, with input image size 224 × 224, patch size 16 × 16, and
embedding dimension d v = 512. we adopt biolinkbert-large [11] as the biomedical
language model, with embedding dimension d l = 1, 024. to show the extensibility
of our approach, we additionally test on vision encoders including imagenet-21k
vit-b/16 [24,26] and intern vit-b/16 [6], and biomedical language model
biobert-large [10].our implementation is based on clip 1 , huggingface2 and
mmclassification3 .training details. prompt length p is set to 1. we resize the
images to 224×224 to fit the model and follow the original data pipeline in
patchgastric [25]. a class-balanced sampling strategy is adopted by choosing one
image from each class in turn. training is done with 1,000 iterations of
stochastic gradient descent (sgd), and the mini-batch size is 128, requiring
11.6 gb of gpu memory and 11 min on two nvidia geforce rtx 2080 ti gpus. all our
experiment results are averaged on 3 random seeds unless otherwise specified.",5
1271,MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis,1.0,Introduction,"renal cancer is the most lethal malignant tumor of the urinary system, and the
incidence is steadily rising [13]. conventional b-mode ultrasound (us) is a good
screening tool but can be limited in its ability to characterize complicated
renal lesions. contrast-enhanced ultrasound (ceus) can provide information on
microcirculatory perfusion. compared with ct and mri, ceus is radiation-free,
cost-effective, and safe in patients with renal dysfunction. due to these
benefits, ceus is becoming increasingly popular in diagnosing renal lesions.
however, recognizing important diagnostic features from ceus videos to diagnose
lesions as benign or malignant is non-trivial and requires lots of experience.to
improve diagnostic efficiency and accuracy, many computational methods were
proposed to analyze renal us images and could assist radiologists in making
clinical decisions [6]. however, most of these methods only focused on
conventional b-mode images. in recent years, there has been increasing interest
in multi-modal medical image fusion [1]. directly concatenation and addition
were the most common methods, such as [3,4,12]. these simple operations might
not highlight essential information from different modalities. weight-based
fusion methods generally used an importance prediction module to learn the
weight of each modality and then performed sum, replacement, or exchange based
on the weights [7,16,17,19]. although effective, these methods did not allow
direct interaction between multi-modal information. to address this,
attention-based methods were proposed. they utilized cross-attention to
establish the feature correlation of different modalities and self-attention to
focus on global feature modeling [9,18]. nevertheless, we prove in our
experiments that these attentionbased methods may have the potential risks of
entangling features of different modalities.in practice, experienced
radiologists usually utilize dynamic information on tumors' blood supply in ceus
videos to make diagnoses [8]. previous researches have proved that temporal
information is effective in improving the performance of deep learning models.
lin et al. [11] proposed a network for breast lesion detection in us videos by
aggregating temporal features, which outperformed other image-based methods.
chen et al. [2] showed that ceus videos can provide more detailed blood supply
information of tumors allowing a more accurate breast lesion diagnosis than
static us images.in this work, we propose a novel multi-modal us video fusion
network (muvf-yolox) based on ceus videos for renal tumor diagnosis. our main
contributions are fourfold. (1) to the best of our knowledge, this is the first
deep learning-based multi-modal framework that integrates both b-mode and
ceusmode information for renal tumor diagnosis using us videos. (2) we propose
an attention-based multi-modal fusion (amf) module consisting of cross-attention
and self-attention blocks to capture modality-invariant and modality-specific
features in parallel. (3) we design an object-level temporal aggregation (ota)
module to make video-based diagnostic decisions based on the information from
multi-frames. (4) we build the first multi-modal us video datatset containing
b-mode and ceus-mode videos for renal tumor diagnosis. experimental results show
that the proposed framework outperforms single-modal, single-frame, and other
state-of-the-art methods in renal tumor diagnosis.",5
1302,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,1.0,Introduction,"pancreatic ductal adenocarcinoma (pdac) is one of the deadliest forms of human
cancer, with a 5-year survival rate of only 9% [16]. neoadjuvant chemotherapy
can increase the likelihood of achieving a margin-negative resection and avoid
unnecessary surgery in patients with aggressive tumor types [23]. providing
accurate and objective preoperative biomarkers is crucial for triaging patients
who are most likely to benefit from neoadjuvant chemotherapy. however, current
clinical markers such as larger tumor size and high carbohydrate antigen (ca)
19-9 level may not be sufficient to accurately tailor neoadjuvant treatment for
patients [19]. therefore, multi-phase contrast-enhanced ct has a great potential
to enable personalized prognostic prediction for pdac, leveraging its ability to
provide a wealth of texture information that can aid in the development of
accurate and effective prognostic models [2,10].previous studies have utilized
image texture analysis with hand-crafted features to predict the survival of
patients with pdacs [1], but the representational fig. 1. two examples of
spatial information between vessel (orange region) and tumor (green region). the
minimum distance, which refers to the closest distance between the superior
mesenteric artery (sma) and the pdac tumor region, is almost identical in these
two cases. we define the surface-to-surface distance based on point-to-surface
distance (weighted-average of red lines from ♦ to ) instead of point-to-point
distance (blue lines) to better capture the relationship between the tumor and
the perivascular tissue.here ♦ and are points sampled from subset vc and pc
defined in eq. power of these features may be limited. in recent years, deep
learning-based methods have shown promising results in prognosis models
[3,6,12]. however, pdacs differ significantly from the tumors in these studies.
a clinical investigation based on contrast-enhanced ct has revealed a dynamic
correlation between the internal stromal fractions of pdacs and their
surrounding vasculature [14]. therefore, focusing solely on the texture
information of the tumor itself may not be effective for the prognostic
prediction of pdac. it is necessary to incorporate tumor-vascular involvement
into the feature extraction process of the prognostic model. although some
studies have investigated tumor-vascular relationships [21,22], these methods
may not be sufficiently capable of capturing the complex dynamics between the
tumor and its environment.we propose a novel approach for measuring the relative
position relationship between the tumor and the vessel by explicitly using the
distance between them. typically, chamfer distance [7], hausdorff distance [8],
or other surfaceawareness metrics are used. however, as shown in fig. 1, these
point-to-point distances cannot differentiate the degree of tumor-vascular
invasion [18]. to address this limitation, we propose a learnable neural
distance that considers all relevant points on different surfaces and uses an
attention mechanism to compute a combined distance that is more suitable for
determining the degree of invasion. furthermore, to capture the tumor
enhancement patterns across multi-phase ct images, we are the first to combine
convolutional neural networks (cnn) and transformer [4] modules for extracting
the dynamic texture patterns of pdac and its surroundings. this approach takes
advantage of the visual transformer's adeptness in capturing long-distance
information compared to the cnn-onlybased framework in the original approach. by
incorporating texture information between pdac, pancreas, and peripancreatic
vessels, as well as the local tumor information captured by cnn, we aim to
improve the accuracy of our prognostic prediction model.in this study, we make
the following contributions: (1) we propose a novel approach for aiding survival
prediction in pdac by introducing a learnable neural distance that explicitly
evaluates the degree of vascular invasion between the tumor and its surrounding
vessels. (2) we introduce a texture-aware transformer block to enhance the
feature extraction approach, combining local and global information for
comprehensive texture information. we validate that the cross-attention is
utilized to capture cross-modality information and integrate it with in-modality
information, resulting in a more accurate and robust prognostic prediction model
for pdac. (3) through extensive evaluation and statistical analysis, we
demonstrate the effectiveness of our proposed method. the signature built from
our model remains statistically significant in multivariable analysis after
adjusting for established clinical predictors. our proposed model has the
potential to be used in combination with clinical factors for risk
stratification and treatment decisions for patients with pdac.",5
1305,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,2.2,Neural Distance: Positional and Structural Information,"between pdac and vesselsthe vascular involvement in patients with pdac affects
the resectability and treatment planning [5]. in this study, we investigate four
important vessels: portal vein and splenic vein (pvsv), superior mesenteric
artery (sma), superior mesenteric vein (smv), and truncus coeliacus (tc). we
used a semi-supervised nnunet model to segment pdac and the surrounding vessels,
following recent work [11,21]. we define a general distance between the surface
boundaries of pdac (p) and the aforementioned four types of vessels (v) as d(v,
p), which can be derived as follows:where v ∈ v and p ∈ p are points on the
surfaces of blood vessels and pdac, respectively. the point-to-surface distance
d ps (v, p) is the distance from a point v on v to p, defined as d ps (v, p) =
min p∈p v -p 2 2 , and vice versa. to numerically calculate the integrals in the
previous equation, we uniformly sample from the surfaces v and p to obtain the
sets v and p consisting of n v points and n p points, respectively. the distance
is then calculated between the two sets using the following equation:however,
the above distance treats all points equally and may not be flexible enough to
adapt to individualized prognostic predictions. therefore, we improve the above
equation in two ways. firstly, we focus on the sub-sets vc and pc of v and p,
respectively, which only contain the k closest points to the opposite surfaces p
and v, respectively. the sub-sets are defined as:secondly, we regard the entire
sets vc and pc as sequences and calculate the distance using a 2-way
cross-attention block (similar to eq. 1) to build a neural distance based on the
3d spatial coordinates of each point:neural distance allows for the flexible
assignment of weights to different points and is able to find positional
information that is more suitable for pdac prognosis prediction. in addition to
neural distance, we use the 3d-cnn model introduced in [22] to extract the
structural relationship between pdac and the vessels. specifically, we
concatenate each pdac-vessel pair x v s ∈ r 2×h×w ×d , where v ∈{pvsv, smv, sma,
tc} and obtain the structure feature f s ∈ r cs .finally, we concatenate the
features extracted from the two components and apply a fully-connected layer to
predict the survival outcome, denoted as o os , which is a value between 0 and
1. to optimize the proposed model, we use the negative log partial likelihood as
the survival loss [9].",5
1306,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,3.0,Experiments,"dataset. in this study, we used data from shengjing hospital to train our method
with 892 patients, and data from three other centers, including guangdong
provincial people's hospital, tianjin medical university and sun yatsen
university cancer center for independent testing with 178 patients. the
contrast-enhanced ct protocol included non-contrast, pancreatic, and portal
venous phases. pdac masks for 340 patients were manually labeled by a
radiologist from shengjing hospital with 18 years of experience in pancreatic
cancer, while the rest were predicted using self-learning models [11,24] and
checked by the same annotator. other vessel masks were generated using the same
semisupervised segmentation models. c-index was used as our primary evaluation
metric for survival prediction. we also reported the survival auc, which
estimates the cumulative area under the roc curve for the first 36
months.implementation details: we used nested 5-fold cross-validation and
augmented the training data by rotating volumetric tumors in the axial direction
and randomly selecting cropped regions with random shifts. we also set the
output feature dimensions to c t = 64 for the texture-aware transformer, c s =
64 for the structure extraction and k = 32 for the neural distance. the batch
size was 16 and the maximum iteration was set to 1000 epochs, and we selected
the model with the best performance on the validation set during training for
testing. we implemented our experiments using pytorch 1.11 and trained the
models on a single nvidia 32g-v100 gpu.ablation study. we first evaluated the
performance of our proposed textureaware transformer (tat) by comparing it with
the resnet18 cnn backbone and vit transformer backbone, as shown in table 1. our
model leverages the strengths of both local and global information in the
pancreas and achieved the best result. next, we compared different methods for
multi-phase stages, including lstm, early fusion (fusion), and cross-attention
(cross) in our method. cross-attention is more effective and lightweight than
lstm. moreover, we separated texture features into in-phase features and
cross-phase features, which is more reasonable than early fusion.secondly, we
evaluated each component in our proposed method, as shown in fig. 2, and
presented the results in table 1. combining the texture-aware transformer and
regular structure information improved the results from 0.630 to 0.648, as tumor
invasion strongly affects the survival of pdac patients. we also employed a
simple 4-variable regression model that used only the chamfer distance of the
tumor and the four vessels for prognostic prediction. the resulting c-index of
0.611 confirmed the correlation of the distance with the survival, which is
consistent with clinical findings [18]. explicitly adding the distance measure
further improved the results. our proposed neural distance metric outperformed
traditional surface distance metrics like chamfer distance, indicating its
suitability for distinguishing the severity of pdac.",5
1307,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,0.0,Comparisons.,"to further evaluate the performance of our proposed model, we compared it with
recent deep prediction methods [17,21] and report the results in table 2. we
modified baseline deep learning models [12,17] and used their network
architectures to take a single pancreatic phase or all three phases as inputs.
deepct-pdac [21] is the most recent method that considers both tumor-related and
tumor-vascular relationships using 3d cnns. our proposed method, which uses the
transformer and structure-aware blocks to capture tumor enhancement patterns and
tumor-vascular involvement, demonstrated its effectiveness with better
performance in both nested 5-fold cross-validation and the multi-center
independent test set.in table 3, we used univariate and multivariate cox
proportional-hazards models to evaluate our signature and other
clinicopathologic factors in the independent test set. the proposed risk
stratification was a significant prognostic factor, along with other factors
like pathological tnm stages. after selecting significant variables (p < 0.05)
in univariate analysis, our proposed staging remained strong in multivariable
analysis after adjusting for important prognostic markers like pt and resection
margins. notably, our proposed marker remained the strongest among all
pre-operative markers, such as tumor size and ca 19-9.neoadjuvant therapy
selection. to demonstrate the added value of our signature as a tool to select
patients for neoadjuvant treatment before surgery, we plotted kaplan-meier
survival curves in fig. 3. we further stratify patients by our signature after
grouping them by tumor size and ca19-9, two clinically used preoperative
criteria for selection, and also age. our signature could significantly stratify
patients in all cases and those in the high-risk group had worse outcomes and
might be considered as potential neoadjuvant treatment candidates (e.g. 33
high-risk patients with larger tumor size and high ca19-9).",5
1308,Improved Prognostic Prediction of Pancreatic Cancer Using Multi-phase CT by Integrating Neural Distance and Texture-Aware Transformer,4.0,Conclusion,"in our paper, we propose a multi-branch transformer-based framework for
predicting cancer survival. our framework includes a texture-aware transformer
that captures both local and global information about the pdac and pancreas. we
also introduce a neural distance to calculate a more reasonable distance between
pdac and vessels, which is highly correlated with pdac survival. we have
extensively evaluated and statistically analyzed our proposed method,
demonstrating its effectiveness. furthermore, our model can be combined with
established high-risk features to aid in the patient selections who might
benefit from neoadjuvant therapy before surgery.",5
1320,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,1.0,Introduction,"gastric cancer (gc) is the third leading cause of cancer-related deaths
worldwide [19]. the five-year survival rate for gc is approximately 33% [16],
which is mainly attributed to patients being diagnosed with advanced-stage
disease harboring unresectable tumors. this is often due to the latent and
nonspecific signs and symptoms of early-stage gc. however, patients with
early-stage disease have a substantially higher five-year survival rate of
around 72% [16]. therefore, early detection of resectable/curable gastric
cancers, preferably before the onset of symptoms, presents a promising strategy
to reduce associated mortality. unfortunately, current guidelines do not
recommend any screening tests for gc [22]. while several screening tools have
been developed, such as barium-meal gastric photofluorography [5], upper
endoscopy [4,7,9], and serum pepsinogen levels [15], they are challenging to
apply to the general population due to their invasiveness, moderate
sensitivity/specificity, high cost, or side effects. therefore, there is an
urgent need for novel screening methods that are noninvasive, highly accurate,
low-cost, and ready to distribute.non-contrast ct is a commonly used imaging
protocol for various clinical purposes. it is a non-invasive, relatively
low-cost, and safe procedure that exposes patients to less radiation dose and
does not require the use of contrast injection that may cause serious side
effects (compared to multi-phase contrastenhanced ct). with recent advances in
ai, opportunistic screening of diseases using non-contrast ct during routine
clinical care performed for other clinical indications, such as lung and
colorectal cancer screening, presents an attractive approach to early detect
treatable and preventable diseases [17]. however, whether early detection of
gastric cancer using non-contrast ct scans is possible remains unknown. this is
because early-stage gastric tumors may only invade the mucosal and muscularis
layers, which are difficult to identify without the help of stomach preparation
and contrast injection. additionally, the poor contrast between the tumor and
normal stomach wall/tissues on non-contrast ct scans and various shape
alterations of gastric cancer, further exacerbates this challenge.in this paper,
we propose a novel approach for detecting gastric cancer on non-contrast ct
scans. unlike the conventional ""segmentation for classification"" methods that
directly employ segmentation networks, we developed a clusterinduced mask
transformer that performs segmentation and global classification simultaneously.
given the high variability in shape and texture of gastric cancer, we encode
these features into learnable clusters and utilize cluster analysis during
inference. by incorporating self-attention layers for global context modeling,
our model can leverage both local and global cues for accurate detection. in our
experiments, the proposed approach outperforms nnunet [8] by 0.032 in auc, 5.0%
in sensitivity, and 4.1% in specificity. these results demonstrate the potential
of our approach for opportunistic screening of gastric cancer in asymptomatic
patients using non-contrast ct scans.",5
1321,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,2.0,Related Work,"automated cancer detection. researchers have explored automated tumor detection
techniques on endoscopic [13,14], pathological images [20], and the prediction
of cancer prognosis [12]. recent developments in deep learning have
significantly improved the segmentation of gastric tumors [11], which is
critical for their detection. however, our framework is specifically designed
for noncontrast ct scans, which is beneficial for asymptomatic patients. while
previous studies have successfully detected pancreatic [25] and esophageal [26]
cancers on non-contrast ct, identifying gastric cancer presents a unique
challenge due to its subtle texture changes, various shape alterations, and
complex background, e.g., irregular gastric wall; liquid and contents in the
stomach.",5
1324,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0.0,Knowledge Transfer from Contrast-Enhanced to Non-contrast CT.,"to address difficulties with tumor annotation on non-contrast cts, the
radiologists start by annotating a voxel-wise tumor mask on the
contrast-enhanced ct, referring to clinical and endoscopy reports as needed.
deeds [6] registration is then performed to align the contrast-enhanced ct with
the non-contrast ct and the resulting deformation field is applied to the
annotated mask. any misaligned ones are revised manually. in this manner (fig.
1d), a relatively coarse yet highly reliable tumor mask can be obtained for the
non-contrast ct image. cluster-induced classification with mask transformers.
segmentation for classification is widely used in tumor detection [25,26,32]. we
first train a unet [8,18] to segment the stomach and tumor regions using the
masks from the previous step. this unet considers local information and can only
extract stomach rois well during testing. however, local textures are inadequate
for accurate gastric tumor detection on non-contrast cts, so we need a network
of both local sensitivity to textures and global awareness of the organ-tumor
morphology. mask transformer [3,24] is a well-suited approach to boost the cnn
backbone with stand-alone transformer blocks. recent studies [27,28] suggest
interpreting object queries as cluster centers, which naturally exhibit
intra-cluster similarity and inter-class discrepancy. inspired by this, we
further develop a deep classification model on top of learnable cluster
representations.specifically, given image x ∈ r h×w ×d , annotation y ∈ r k×hw d
, and patient class p ∈ l, our model consists of three components: 1) a cnn
backbone to extract its pixel-wise features f ∈ r c×hw d (fig. 1a), 2) a
transformer module (fig. 1b), and 3) a multi-task cluster inference module (fig.
1c). the transformer module gradually updates a set of randomly initialized
object queries c ∈ r n ×c , i.e., to meaningful mask embedding vectors through
cross-attention between object queries and multi-scale pixel features,where c
and p stand for query and pixel features, q c , k p , v p represent linearly
projected query, key, and value. we adopt cluster-wise argmax from kmax-deeplab
[28] to substitute spatial-wise softmax in the original settings.we further
interpret the object queries as cluster centers from a cluster analysis
perspective. all the pixels in the convolutional feature map are assigned to
different clusters based on these centers. the assignment of clusters (a.k.a.
mask prediction) m ∈ r n ×hw d is computed as the cluster-wise softmax function
over the matrix product between the cluster centers c and pixel-wise feature
matrix f, i.e.,the final segmentation logits z ∈ r k×hw d are obtained by
aggregating the pixels within each cluster according to cluster-wise
classification, which treats pixels within a cluster as a whole. the aggregation
of pixels is achieved by z = c k m, where the cluster-wise classification c k is
represented by an mlp that projects the cluster centers c to k channels (the
number of segmentation classes).the learned cluster centers possess high-level
semantics with both intercluster discrepancy and intra-cluster similarity for
effective classification. rather than directly classifying the final feature
map, we first generate the clusterpath feature vector by taking the channel-wise
average of cluster centers c =additionally, to enhance the consistency between
the segmentation and classification outputs, we apply global max pooling to
cluster assignments r to obtain the pixel-path feature vector r ∈ r n . this
establishes a direct connection between classification features and segmentation
predictions. finally, we concatenate these two feature vectors to obtain the
final feature and project it onto the classification prediction p ∈ r 2 via a
two-layer mlp.the overall training objective is formulated as,where the
segmentation loss l seg (•, •) is a combination of dice and cross entropy
losses, and the classification loss l cls (•, •) is cross entropy loss.",5
1325,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,4.1,Experimental Setup,"dataset and ground truth. our study analyzed a dataset of ct scans collected
from guangdong province people's hospital between years 2018 and 2020, with
2,139 patients consisting of 787 gastric cancer and 1,352 normal cases. we used
the latest patients in the second half of 2020 as a hold-out test set, resulting
in a training set of 687 gastric cancer and 1,204 normal cases, and a test set
of 100 gastric cancer and 148 normal cases. we randomly selected 20% of the
training data as an internal validation set. to further evaluate specificity in
a larger population, we collected an external test set of 903 normal cases from
shengjing hospital. cancer cases were confirmed through endoscopy (and
pathology) reports, while normal cases were confirmed by radiology reports and a
two-year follow-up. all patients underwent multi-phase cts with a median spacing
of 0.75 × 0.75 × 5.0 mm and an average size of (512, 512, 108) voxel. tumors
were annotated on the venous phase by an experienced radiologist specializing in
gastric imaging using ctlabeler [23], while the stomach was automatically
annotated using a self-learning model [31].implementation details. we resampled
each ct volume to the median spacing while normalizing it to have zero mean and
unit variance. during training, we cropped the 3d bounding box of the stomach
and added a small margin of (32,32,4). we used nnunet [8] as the backbone, with
four transformer decoders, each taking pixel features with output strides of 32,
16, 8, and 4. we set the number of object queries n to 8, with each having a
dimension of 128, and included an eight-head self-attention layer in each block.
the patch size used during training and inference is (192, 224, 40) voxel. we
followed [8] to augment data. we trained the model with radam using a learning
rate of 10 -4 and a (backbone) learning rate multiplier of 0.1 for 1000 epochs,
with a frozen backbone of the pretrained nnunet [8] for the first 50 epochs. to
enhance performance, we added deep supervision by aligning the cross-attention
map with the final segmentation map, as per kmax-deeplab [27]. the hidden layer
dimension in the two-layer mlp is 128. we also trained a standard unet [8,18] to
localize the stomach region in the entire image in the testing phase.",5
1326,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0.0,Evaluation Metrics and Reader Study.,"for the binary classification, model performance is evaluated using area under
roc curve (auc), sensitivity (sens.), and specificity (spec.). and successful
localization of the tumors is considered when the overlap between the
segmentation mask generated by the model and the ground truth is greater than
0.01, measured by the dice score. a reader study was conducted with two
experienced radiologists, one from guangdong province people's hospital with 20
years of experience and the other from the first affiliated hospital of zhejiang
university with 9 years of experience in gastric imaging. the readers were given
248 non-contrast ct scans from the test set and asked to provide a binary
decision for each scan, indicating whether the scan showed gastric cancer. no
patient information or records were provided to the readers. readers were
informed that the dataset might contain more tumor cases than the standard
prevalence observed in screening, but the proportion of case types was not
disclosed. readers used itk-snap [30] to interpret the ct scans without any time
constraints. 1 presents a comparative analysis of our proposed method with three
baselines. the first two approaches belong to ""segmentation for classification""
(s4c) [26,32], using nnunet [8] and transunet [2]. a case is classified as
positive if the segmented tumor volume exceeds a threshold that maximizes the
sum of sensitivity and specificity on the validation set. the third baseline
(denoted as ""nnunet-joint"") integrates a cnn classification head into unet [8]
and trained end-to-end. we obtain the 95% confidence interval of auc,
sensitivity, and specificity values from 1000 bootstrap replicas of the test
dataset for statistical analysis. for statistical significance, we conduct a
delong test between two aucs (ours vs. compared method) and a permutation test
between two sensitivities or specificities (ours vs. compared method and
radiologists).",5
1329,Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans,0.0,Subgroup Analysis.,"in table 2, we report the performance of patient-level detection and tumor-level
localization stratified by tumor (t) stage. we compare our model's performance
with that of both radiologists. the results show that our model performs better
in detecting early stage tumors (t1, t2) and provides more precise tumor
localization. specifically, our model detects 60.0% (6/10) t1 cancers, and 77.8%
(7/9) t2 cancers, surpassing the best performing expert (50% t1, 55.6% t2).
meanwhile, our model maintains a reliable detection rate and credible
localization accuracy for t3 and t4 tumors (2 of 34 t3 tumors missed).comparison
with established screening tools. our method surpasses or performs on par with
established screening tools [4,7,10] in terms of sensitivity for gastric cancer
detection at a similar specificity level with a relatively large testing patient
size (n = 1151 by integrating the internal and external test sets), as shown in
table 3. this finding sheds light on the opportunity to employ automated ai
systems to screen gastric cancer using non-contrast ct scans.",5
1495,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,1.0,Introduction,"the tumor microenvironment (tme) is comprised of cancer, immune (e.g. b
lymphocytes, and t lymphocytes), stromal, and other cells together with
noncellular tissue components [3,5,24,30]. it is well acknowledged that tumors
evolve in close interaction with their microenvironment. quantitatively
characterizing tme has the potential to predict tumor aggressiveness and
treatment response [3,23,24,30]. different types of lymphocytes such as cd4+
(helper t cells), cd8+ (cytotoxic t cells), cd20+ (b cells), within the tme
naturally interact with tumor and stromal cells. studies [5,9] have shown that
quantifying spatial interplay of these different cell families within the tme
can provide more prognostic/predictive value compared to only measuring the
density of a single biomarker such as tumor-infiltrating lymphocytes (tils)
[3,24]. immunotherapy (io) is the standard treatment for patients with advanced
non-small cell lung cancer (nsclc) [19] but only 27-45% of patients respond to
this treatment [21]. therefore, better algorithms and improved biomarkers are
essential for identifying which cancer patients are most likely to respond to io
in advance of treatment. quantitative features that relate to the complex
spatial interplay between different types of b-and t-cells in the tme might
unlock attributes that are associated with io response. in this study, we
introduce a novel approach called triangular analysis of geographical interplay
of lymphocytes (triangil), representing a unique and interpretable way to
characterize the distribution, and higher-order interaction of various cell
families (e.g., cancerous cells, stromal cells, lymphocyte subtypes) across
digital histopathology slides. we demonstrate the efficacy of triaangil for
characterizing tme in the context of predicting 1) response to io with immune
checkpoint inhibitors (ici), 2) overall survival (os), in patients with nsclc,
and 3) providing novel insights into the spatial interplay between different
immune cell subtype. triangil source code is publicly available at
http://github.com/sarayar/triangil.",6
1496,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,2.0,Previous Related Work and Novel Contributions,"many studies have only looked at the density of a single biomarker (e.g. tils),
to show that a high density of tils is associated with improved patient survival
and treatment response in nsclc [3,24]. other works have attempted to
characterize the spatial arrangement of cells in tme using computational
graphbased approaches. these approaches include methods that connect cells
regardless of their type (1) using global graphs (gg) such as voronoi that
connect all nuclei [2,14], or (2) using cell cluster graphs (ccg) [16] to create
multiple nuclear subgraphs based on cell-to-cell proximity to predict tumor
aggressiveness and patient outcome [16]. others have explored (3) the spatial
interplay between two different cell types [5].one example approach is spatial
architecture of til (spatil) [9] which attempted to characterize the interplay
between immune and cancer cells and has proven to be helpful in predicting the
recurrence in early-stage nsclc. all of these approaches point to overwhelming
evidence that spatial architecture of cells in tme is critical in predicting
cancer outcome. however, these approaches have not been able to exploit
higher-order interactions and dependencies between multiple cell types (> 2),
relationships that might provide additional actionable insights. the
contributions of this work include:(1) triangil is a computational framework
that characterizes the architecture and relationships of different cell types
simultaneously. instead of measuring only simple two-by-two relations between
cells, it seeks to identify triadic spatial relations (hyperedges [18,20] have
shown great capabilities in solving complex problems in the biomedical field,
these tend to be black-box in nature. a key consideration in cancer immunology
is the need for actionable insights into the spatial relationships between
different types of immune cells. not only does triangil provide predictions that
are on par or superior compared to dl approaches, but also provides a way to
glean insights into the spatial interplay of different immune cell types. these
complex interactions enhance our understanding of the tme and will help pave the
way for new therapeutic strategies that leverage these insights.",6
1499,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.1,Dataset,"the cohort employed in this study was composed of pre-treatment tumor biopsy
specimens from patients with nsclc from five centers (two centers for training
(s t ) and three centers for independent validation (s v )). the entire analysis
was carried out using 122 patients in experiment 1 (73 in s t , and 49 in s v )
and 135 patients in experiment 2 (81 in s t , and 54 in s v ). specimens were
analyzed with a multiplexed quantitative immunofluorescence (qif) panel using
the method described in [22]. from each whole slide image, 7 representative
tiles were obtained and used to train the software inform to define background,
tumor and stromal compartments. then, individual cells were segmented based on
nuclear dapi staining and the segmentation performance was controlled by direct
visualization of samples by a trained observer. next, the software was trained
to identify cell subtypes based on marker expression (cd8, cd4, cd20, ck for
tumor epithelial cells and absence of these markers for stromal cells).",6
1501,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,TIL density (DenTIL):,"for every patient, multiple density measures including the number of different
cells types and their ratios are calculated [3,24] (supplemental table 2).",6
1502,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,"GG: A Delaunay triangulation, a Voronoi diagram, and a Minimum Spanning","tree were constructed [2,14] on all nuclei regardless of their type.
architectural features (e.g., perimeter, triangle area, edge length) were then
calculated on these global graphs for each patient.",6
1503,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,CCG:,"for every patient, subgraphs are built on nuclei regardless of their type and
only based on their euclidean distance. local graph metrics (e.g. clustering
coefficient) [16] are then calculated from these subgraphs. spatil: for each
patient, first, subgraphs are built on individual cell types based on a distance
parameter. the convex hulls are then constructed on these subgraphs. after
selecting every two cell types, features are extracted from their convex hulls
(e.g. the number of clusters of each cell type, area intersected between
clusters [9]; complete list of combinations in supplemental table 3).",6
1505,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,4.3,Experiment 1: Immunotherapy Response Prediction in Lung Cancer,"design: triangil was also trained to differentiate between patients who
responded to io and those who did not. for our study, the responders to io were
identified as those patients with complete response, partial response, and
stable disease, and non-responders were patients with progressive disease. a
linear discriminant analysis (lda) classifier was trained on s t to predict
which patients would respond to io. for creating the model, the minimum
redundancy maximum relevance (mrmr) method [1] was used to select the top
features. the same procedure using mrmr and lda was performed for the
comparative hand-crafted approaches. the ability to identify responders post-io
was assessed by the area under the receiver operating characteristic curve (auc)
in s v .",6
1506,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,0.0,Results:,"the two top predictive triangil features were found to be the number of edges
between stroma and cd4+ cells, and the number of edges between stroma and tumor
cells with more interactions between stromal cells and both cd4+ and tumor cells
being associated with response to io. this finding is concordant with other
studies [13,17,22,27] that stromal tils were significantly associated with
improved os. therefore, triangil approach is not only predictive of treatment
response but more critically it enables biological interpretations that a dl
model might not be able to provide. in s v , this lda classifier was able to
distinguish responders from non-responders to io with au c t ri =0. design: s t
was used to construct a least absolute shrinkage and selection operator (lasso)
[28] regularized cox proportional hazards model [6] using the triangil features,
to obtain risk score for each patient. lasso features are listed in supplemental
table 4. the median risk score in s t was used as a threshold in both s t and s
v to dichotomize patients into low-risk/high-risk categories. kaplan-meier (km)
survival curves [26] were plotted and the model performance was summarized by
hazard ratio (hr), with corresponding (95% confidence intervals (ci)) using the
log-rank test, and harrell's concordance index (c-index) on s v . the c-index
evaluates the correlation between risk predictions and survival times, aiming to
maximize the discrimination between high-risk and low-risk patients [11]. os is
the time between the initiation of io to the death of the patient. the patients
were censored if the date of death was unknown.result: figure 2 presents some
triangil features in a field of view for a patient with long-term survival and
another with short-term survival. more triangular relationships, shorter
triangle edges, and smaller triangles with smaller perimeters are found in the
long-term survival case when analyzing the triadic interactions within
tumor-stroma-cd4, thereby suggesting higher relative presence and closer
interaction of these cell families. figure 3 illustrates the km plots for the
six approaches. we also calculated the concordance index (c-index) for the two
prognostic approaches in s v . the c-index for triangil and gnn methods were
0.64, and 0.63 respectively. therefore, overall triangil worked marginally
better than gnn, with much higher biological interpretability.",6
1507,Triangular Analysis of Geographical Interplay of Lymphocytes (TriAnGIL): Predicting Immunotherapy Response in Lung Cancer,5.0,Concluding Remarks,"we presented a new approach, triangular analysis of geographical interplay of
lymphocytes (triangil), to quantitatively chartacterize the spatial arrangement
and relative geographical interplay of multiple cell families across
pathological images. compared to previous spatial graph-based methods, triangil
quantifies the spatial interplay between multiple cell families, providing a
more comprehensive portrait of the tumor microenvironment. triangil was
predictive of response after io (n = 122) and also demonstrated a strong
correlation with os in nsclc patients treated with io (n = 135). triangil
outperformed other graph-and dl-based approaches, with the added benefit of
provoding interpretability with regard to the spatial interplay between cell
families. for instance, triangil yielded the insight that more interactions
between stromal cells and both cd4+ and tumor cells appears to be associated
with better response to io. although five cell families were studies in this
work, triangil is flexible and could include other cell types (e.g.,
macrophages). future work will entail larger validation studies and also
evaluation on other use cases.",6
1522,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,1.0,Introduction,"radiotherapy, one of the mainstream treatments for cancer patients, has gained
notable advancements in past decades. for promising curative effect, a
high-quality radiotherapy plan is demanded to distribute sufficient dose of
radiation to the planning target volume (ptv) while minimizing the radiation
hazard to organs at risk (oars). to achieve this, radiotherapy plans need to be
manually adjusted by the dosimetrists in a trial-and-error manner, which is
extremely labor-intensive and time-consuming [1,2]. additionally, the quality of
treatment plans might be variable among radiologists due to their different
expertise and experience [3]. consequently, it is essential to develop a robust
methodology to automatically predict the dose distribution for cancer patients,
relieving the burden on dosimetrists and accelerating the radiotherapy
procedure.recently, the blossom of deep learning (dl) has promoted the automatic
medical image processing tasks [4][5][6], especially for dose prediction
[7][8][9][10][11][12][13][14]. for example, nguyen et al. [7] modified the
traditional 2d unet [15] to predict the dose of prostate cancer patients. wang
et al. [10] utilized a progressive refinement unet (prunet) to refine the
predictions from low resolution to high resolution. besides the above unetbased
frameworks, song et al. [11] employed the deeplabv3+ [16] to excavate contextual
information from different scales, thus obtaining accuracy improvements in the
dose prediction of rectum cancer. mahmood et al. [12] utilized a generative
adversarial network (gan)-based method to predict the dose maps of oropharyngeal
cancer. furthermore, zhan et al. [13] designed a multi-organ constraint loss to
enforce the deep model to better consider the dose requirements of different
organs. following the idea of multi-task learning, tan et al. [8] utilized
isodose line and gradient information to promote the performance of dose
prediction of rectum cancer. to ease the burden on the delineation of ptv and
oars, li et al. [17] constructed an additional segmentation task to provide the
dose prediction task with essential anatomical knowledge.although the above
methods have achieved good performance in predicting dose distribution, they
suffer from the over-smoothing problem. these dl-based dose prediction methods
always apply the l 1 or l 2 loss to guide the model optimization which
calculates a posterior mean of the joint distribution between the predictions
and the ground truth [17,18], leading to the over-smoothed predicted images
without important high-frequency details [19]. we display predicted dose maps
from multiple deep models in fig. 1. as shown, compared with the ground truth,
i.e., (5) in fig. 1, the predictions from (1) to (3) are blurred with fewer
high-frequency details, such as ray shapes. these high-frequency features formed
by ray penetration reveal the ray directions and dose attenuation with the aim
of killing the cancer cells while protecting the oars as much as possible, which
are critical for radiotherapy. consequently, exploring an automatic method to
generate high-quality predictions with rich high-frequency information is
important to improve the performance of dose prediction. currently, diffusion
model [20] has verified its remarkable potential in modeling complex image
distributions in some vision tasks [21][22][23]. unlike other dl models, the
diffusion model is trained without any extra assumption about target data
distribution, thus evading the average effect and alleviating the over-smoothing
problem [24]. figure 1 (4) provides an example in which the diffusion-based
model predicts a dose map with shaper and clearer boundaries of ray-penetrated
areas. therefore, introducing a diffusion model to the dose prediction task is a
worthwhile endeavor.in this paper, we investigate the feasibility of applying a
diffusion model to the dose prediction task and propose a diffusion-based model,
called diffdp, to automatically predict the clinically acceptable dose
distribution for rectum cancer patients. specifically, the diffdp consists of a
forward process and a reverse process. in the forward process, the model employs
a markov chain to gradually transform dose distribution maps with complex
distribution into gaussian distribution by progressively adding pre-defined
noise. then, in the reverse process, given a pure gaussian noise, the model
gradually removes the noise in multiple steps and finally outputs the predicted
dose map. in this procedure, a noise predictor is trained to predict the noise
added in the corresponding step of the forward process. to further ensure the
accuracy of the predicted dose distribution for both the ptv and oars, we design
a dl-based structure encoder to extract the anatomical information from the ct
image and the segmentation masks of the ptv and oars. such anatomical
information can indicate the structure and relative position of organs. by
incorporating the anatomical information, the noise predictor can be aware of
the dose constraints among ptv and oars, thus distributing more appropriate dose
to them and generating more accurate dose distribution maps.overall, the
contributions of this paper can be concluded as follows: (1) we propose a novel
diffusion-based model for dose prediction in cancer radiotherapy to address the
over-smoothing issue commonly encountered in existing dl-based dose prediction
methods. to the best of our knowledge, we are the first to introduce the
diffusion model for this task. (2) we introduce a structure encoder to extract
the anatomical information available in the ct images and organ segmentation
masks, and exploit the anatomical information to guide the noise predictor in
the diffusion model towards generating more precise predictions. (3) the
proposed diffdp is extensively evaluated on a clinical dataset consisting of 130
rectum cancer patients, and the results demonstrate that our approach
outperforms other state-of-the-art methods.",6
1523,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,2.0,Methodology,"an overview of the proposed diffdp model is illustrated in fig. 2, containing
two markov chain processes: a forward process and a reverse process. an image
set of cancer patient is defined as {x, y}, where x ∈ r h ×w ×(2+o) represents
the structure images, ""2"" signifies the ct image and the segmentation mask of
the ptv, and o denotes the total number of segmentation mask of oars. meanwhile,
y ∈ r h ×w ×1 is the corresponding dose distribution map for x. concretely, the
forward process produces a sequence of noisy images {y 0 , y 1 , . . . , y t },
y 0 = y by gradually adding a small amount of noise to y in t steps with the
noise increased at each step and a noise predictor f is constructed to predict
the noise added to y t-1 by treating y t , anatomic information from x and
embedding of step t as input. to obtain the anatomic information, a structure
encoder g is designed to extract the crucial feature representations from the
structure images. then, in the reverse process, the model progressively deduces
the dose distribution map by iteratively denoising from y t using the
well-trained noise predictor.",6
1530,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,3.0,Experiments and Results,"dataset and evaluations. we measure the performance of our model on an in-house
rectum cancer dataset which contains 130 patients who underwent volumetric
modulated arc therapy (vmat) treatment at west china hospital. concretely, for
every patient, the ct images, ptv segmentation, oars segmentations, and the
clinically planned dose distribution are included. additionally, there are four
oars of rectum cancer containing the bladder, femoral head r, femoral head l,
and small intestine. we randomly select 98 patients for model training, 10
patients for validation, and the remaining 22 patients for test. the thickness
of the cts is 3 mm and all the images are resized to the resolution of 256 × 256
before the training procedure.we measure the performance of our proposed model
with multiple metrics. considering dm represents the minimal absorbed dose
covering m% percentage volume of ptv, we involve d 98 , d 2 , maximum dose (d
max ), and mean dose (d mean ) as metrics. besides, the heterogeneity index (hi)
is used to quantify dose heterogeneity [26]. to quantify performance more
directly, we calculate the difference ( ) of these metrics between the ground
truth and the predicted results. more intuitively, we involve the dose volume
histogram (dvh) [27] as another essential metric of dose prediction performance.
when the dvh curves of the predictions are closer to the ground truth, we can
infer higher prediction accuracy.comparison with state-of-the-art methods. to
verify the superior accuracy of our proposed model, we select multiple
state-of-the-art (sota) models in dose prediction, containing unet (2017) [7],
gan (2018) [12], deeplabv3+ (2020) [11], c3d (2021) [9], and prunet (2022) [10],
for comparison. the quantitative comparison results are listed in table . 1
where our method outperforms the existing sotas in terms of all metrics.
specifically, compared with deeplabv3+ with the second-best accuracy in hi
(0.0448) and d 98 (0.0416), the results generated by the proposed are 0.0035 and
0.0014 lower, respectively. as for d 2 and d max , our method gains overwhelming
performance with 0.0008 and 0.0005, respectively. moreover, the paired t-test is
conducted to investigate the significance of the results. the p-values between
the proposed and other sotas are almost all less than 0.05, indicating that the
enhancement of performance is statistically meaningful.besides the quantitative
results, we also present the dvh curves derived by compared methods in fig. 3.
the results are compared on ptv as well as two oars: bladder and small
intestine. compared with other methods, the disparity between the dvh curves of
our method and the ground truth is the smallest, demonstrating the superior
performance of the proposed. furthermore, we display the visualization
comparison in fig. 4. as we can see, the proposed model achieves the best visual
quality with clearer and sharper high-frequency details (as indicated by red
arrows). furthermore, the error map of the proposed is the darkest, suggesting
the least disparity compared with the ground truth.ablation study. to study the
contributions of key components of the proposed method, we conduct the ablation
experiments by 1) removing the structure encoder from the proposed method and
concatenating the anatomical images x and noisy image y t together as the
original input for diffusion model (denoted as baseline); 2) the proposed diffdp
model. the quantitative results are given in table 2. we can clearly see the
performance for all metrics is enhanced with the structure encoder,
demonstrating its effectiveness in the proposed model.",6
1531,DiffDP: Radiotherapy Dose Prediction via a Diffusion Model,4.0,Conclusion,"in this paper, we introduce a novel diffusion-based dose prediction (diffdp)
model for predicting the radiotherapy dose distribution of cancer patients. the
proposed method involves a forward and a reverse process to generate accurate
prediction by progressively transferring the gaussian noise into a dose
distribution map. moreover, we propose a structure encoder to extract anatomical
information from patient anatomy images and enable the model to concentrate on
the dose constraints within several essential organs. extensive experiments on
an in-house dataset with 130 rectum cancer patients demonstrate the superiority
of our method.",6
1716,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,1.0,Introduction,"prostate cancer (pca) diagnosis and grading rely on histopathology analysis of
biopsy slides [1]. however, prostate biopsies are known to have sampling error
as pca is heterogenous and commonly multifocal, meaning cancer legions can be
missed during the biopsy procedure [2]. if significant pca is detected on
biopsies and the patient has organ-confined cancer with no contraindications,
radical prostatectomy (rp) is the standard of care [3,4]. following rp, the
prostate is processed and slices are mounted onto slides for analysis. radical
prostatectomy histopathology samples are essential for validating the
biopsydetermined grade group [5,6]. analysis of whole-mount slides, meaning
slides that include slices of the entire prostate, provide more precise tumor
boundary detection, identification of various tumor foci, and increased tissue
for identifying morphological patterns not visible on biopsy due to a larger
field of view.field effect refers to the spread of genetic and epigenetic
alterations from a primary tumor site to surrounding normal tissues, leading to
the formation of secondary tumors. understanding field effect is essential for
cancer research as it provides insights into the mechanisms underlying tumor
development and progression. tumor-associated stroma, which consists of various
cell types, such as fibroblasts, smooth muscle cells, and nerve cells, is an
integral component of the tumor microenvironment that plays a critical role in
tumor development and progression. reactive stroma, a distinct phenotype of
stromal cells, arises in response to signaling pathways from cancerous cells and
is characterized by altered stromal cells and increased extracellular matrix
components [7,8]. reactive stroma is often associated with tumor-associated
stroma and is thought to be a result of field effects in prostate cancer.
altered stroma can create a pro-tumorigenic environment by producing a multitude
of chemokines, growth factors, and releasing reactive oxygen species [9,10],
which can lead to tumor development and aggressiveness [11]. therefore,
investigating the histological characterization of tumor-associated stroma is
crucial in gaining insights into the field effect and tumor progression of
prostate cancer.manual review for tumor-associated stroma is time-consuming and
lacks quantitative metrics [12,13]. several automated methods have been applied
to analyze the tumor-stroma relationship; however, most of them focus on
identifying a tumor-stroma ratio rather than finding reactive stroma tissue or
require pathologist input. machine learning algorithms have been used to
quantify the percentage of tumor to stroma in bladder cancer patients, but
required dichotomizing patients based on a threshold [14]. software has been
used to segment tumor and stroma tissue in breast cancer patient samples, but
the method required constant supervision by a pathologist [15]. similarly, akoya
biosciences inform software was used to quantify reactive stroma in pca, but
this method required substantial pathologist input to train the software [16].
fully automated deep-learning methods have been developed to identify
tumor-associated stroma in breast cancer biopsies, achieving an auc of 0.962 in
predicting invasive ductal cancer [13]. however, identifying tumor-associated
stroma in prostate biopsies and whole-mount histopathology slides remains
challenging.analyzing tumor-associated stroma in prostate cancer requires
combining whole-mount and biopsy histopathology slides. biopsy slides provide
information on the presence of pca, while whole-mount slides provide information
on the extent and distribution of pca, including more information on
tumor-associated stroma. combining the information from both modalities can
provide a more accurate understanding of the tumor microenvironment. in this
work, we explore the field effect in prostate cancer by analyzing
tumor-associated stroma in multimodal histopathological images. our main
contributions can be summarized as follows:-to the best of our knowledge, we
present the first deep-learning approach to characterize prostate
tumor-associated stroma by integrating histological image analysis from both
whole-mount and biopsy slides. our research offers a promising computational
framework for in-depth exploration of the field effect and cancer progression in
prostate cancer. -we proposed a novel approach for stroma classification with
spatial graphs modeling, which enable more accurate and efficient analysis of
tumor microenvironment in prostate cancer pathology. given the spatial nature of
cancer field effect and tumor microenvironment, our graph-based method offers
valuable insights into stroma region analysis. -we developed a comprehensive
pipeline for constructing tumor-associated stroma datasets across multiple data
sources, and employed adversarial training and neighborhood consistency
regularization techniques to learn robust multimodal-invariant image
representations.",6
1721,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,3.1,Dataset,"in our study, we utilized three datasets for tumor-associated stroma
analysis.(1) dataset a comprises 513 tiles extracted from the whole mount slides
of 40 patients, sourced from the archives of the pathology department at
cedars-sinai medical center (irb# pro00029960). it combines two sets of tiles:
224 images from 20 patients featuring stroma, normal glands, low-grade and
highgrade cancer [22], along with 289 images from 20 patients with dense
high-grade cancer (gleason grades 4 and 5) and cribriform/non-cribriform glands
[23]. each tile measures 1200×1200 pixels and is extracted from whole slide
images captured at 20x magnification (0.5 microns per pixel). the tiles were
annotated at the pixel-level by expert pathologists to generate stroma tissue
segmentation masks and were cross-evaluated and normalized to account for stain
variability.(2) dataset b included 97 whole mount slides with an average size of
over 174,000×142,000 pixels at 40x magnification. the prostate tissue within
these slides had an average tumor area proportion of 9%, with an average tumor
area of 77 square mm. an expert pathologist annotated the tumor region
boundaries at the region-level, providing exhaustive annotations for all tumor
foci. (3) dataset c comprised 6134 negative biopsy slides obtained from 262
patients' biopsy procedures, where all samples were diagnosed as negative. these
slides are presumed to contain predominantly normal stroma tissues without
phenotypic alterations in response to cancer. dataset a was utilized for
training the stroma segmentation model. extensive data augmentation techniques,
such as image scaling and staining perturbation, were employed during the
training process. the model achieved an average test dice score of 95.57 ± 0.29
through 5-fold cross-validation. this model was then applied to generate stroma
masks for all slides in datasets b and c. to precisely isolate stroma tissues
and avoid data bleeding from epithelial tissues, we only extracted patches where
over 99.5% of the regions were identified as stroma at 40x magnification to
construct the stroma classification dataset.for positive tumor-associated stroma
patches, we sampled patches near tumor glands within annotated tumor region
boundaries, as we presumed that tumor regions represent zones in which the
greatest amount of damage has progressed. for negative stroma patches, we
calculated the tumor distance for each patch by measuring the euclidean distance
from the patch center to the nearest edge of the labeled tumor regions. negative
stroma patches were then sampled from whole mount slides with a gleason group
smaller than 3 and a tumor distance larger than 5 mm. this approach aims to
minimize the risk of mislabeling tumor-associated stroma as normal tissue.
setting a 5mm threshold accounts for the typically minimal inflammatory
responses induced by prostate cancers, particularly in lower-grade cases. to
incorporate multi-modal information, we randomly sampled negative stroma patches
from all biopsy slides in dataset c. overall, we selected over 1.1 million
stroma patches of size 256×256 pixels at 40x magnification for experiments.
during model training and testing, we performed stain normalization and standard
image augmentation methods.",6
1723,Deep Learning for Tumor-Associated Stroma Identification in Prostate Histopathology Slides,4.0,Results and Discussions,"in conclusion, our study introduced a deep learning approach to accurately
characterize the tumor-associated stroma in multi-modal prostate histopathology
slides. our experimental results demonstrate the feasibility of using deep
learning algorithms to identify and quantify subtle stromal alterations,
offering a promising tool for discovering new diagnostic and prognostic
biomarkers of prostate cancer. through exploring field effect in prostate
cancer, our work provides a computational system for further analysis of tumor
development and progression. future research can focus on validating our
approach on larger and more diverse datasets and expanding the method to a
patient-level prediction system, ultimately improving prostate cancer diagnosis
and treatment.",6
1761,Thinking Like Sonographers: A Deep CNN Model for Diagnosing Gout from Musculoskeletal Ultrasound,1.0,Introduction,"gout is the most common inflammatory arthritis and musculoskeletal ultrasound
(mskus) scanning is recommended to diagnose gout due to the non-ionizing
radiation, fast imaging speed, and non-invasive characteristics of mskus [7].
however, misdiagnosis of gout can occur frequently when a patient's clinical
characteristics are atypical. traditional mskus diagnosis relies on the
experience of the radiologist which is time-consuming and labor-intensive.
although convolutional neural networks (cnns) based ultrasound classification
models have been successfully used for diseases such as thyroid nodules and
breast cancer, conspicuously absent from these successful applications is the
use of cnns for gout diagnosis from mskus images. there are significant
challenges in cnn based gout diagnosis. firstly, the gout-characteristics
contain various types including double contour sign, synovial hypertrophy,
synovial effusion, synovial dislocation and bone erosion, and these
gout-characteristics are small and difficult to localize in mskus. secondly, the
surrounding fascial tissues such as the muscle, sarcolemma and articular capsule
have similar visual traits with gout-characteristics, and we found the existing
cnn models can't accurately pay attention to the gout-characteristics that
radiologist doctors pay attention to during the diagnosis process (as shown in
fig. 1). due to these issues, sota cnn models often fail to learn the gouty
mskus features which are key factors for sonographers' decision.in medical image
analysis, recent works have attempted to inject the recorded gaze information of
clinicians into deep cnn models for helping the models to predict correctly
based on lesion area. mall et al. [9,10] modeled the visual search behavior of
radiologists for breast cancer using cnn and injected human visual attention
into cnn to detect missing cancer in mammography. wang et al. [15] demonstrated
that the eye movement of radiologists can be a new supervision form to train the
cnn model. cai et al. [3,4] developed the sononet [1] model, which integrates
eye-gaze data of sonographers and used generative adversarial networks to
address the lack of eye-gaze data. patra et al. [11] proposed the use of a
teacher-student knowledge transfer framework for us image analysis, which
combines doctor's eye-gaze data with us images as input to a large teacher
model, whose outputs and intermediate feature maps are used to condition a
student model. although these methods have led to promising results, they can be
difficult to implement due to the need to collect doctors' eye movement data for
each image, along with certain restrictions on the network structure.different
from the existing studies, we propose a novel framework to adjust the general
cnns to ""think like sonographers"" from three different levels. (1) where to
adjust: modeling sonographers' gaze map to emphasize the region that needs
adjust; (2) what to adjust: classify the instances to systemically detect
predictions made based on unreasonable/biased reasoning and adjust; (3) how to
adjust: developing a training mechanism to strike the balance between gout
prediction accuracy and attention reasonability.",6
1825,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,1.0,Introduction,"over 430,000 new cases of renal cancer were reported in 2020 in the world [1]
and this number is expected to rise [22]. when the tumor size is large (greater
than 7 cm) often the whole kidney is removed, however, when the tumor size is
small (less than 4 cm), partial nephrectomy is the preferred treatment [20] as
it could preserve kidney's function. thus, early detection of kidney tumors can
help to improve patient's prognosis. however, early-stage renal cancers are
usually asymptomatic, therefore they are often incidentally found during other
examinations [19], which includes non-contrast ct (ncct) scans.segmentation of
kidney tumors on ncct images adds challenges compared to contrast-enhanced ct
(cect) images, due to low contrast and lack of multiphase images. on cect
images, the kidney tumors have different intensity values compared to the normal
tissues. there are several works that demonstrated successful segmentation of
kidney tumors with high precision [13,21]. however, on ncct images, as shown in
fig. 1b, some tumors called isodensity tumors, have similar intensity values to
the surrounding normal tissues. to detect such tumors, one must compare the
kidney shape with tumors to the kidney shape without the tumors so that one can
recognize regions with protuberance.3d u-net [3] is the go-to network for
segmenting kidney tumors on cect images. however, convolutional neural networks
(cnns) are biased towards texture features [5]. therefore, without any
intervention, they may fail to capture the protuberance caused by isodensity
tumors on ncct images.in this work, we present a novel framework that is capable
of capturing the protuberances in the kidneys. our goal is to segment kidney
tumors including isodensity types on ncct images. to achieve this goal, we
create a synthetic dataset, which has separate annotations for normal kidneys
and protruded regions, and train a segmentation network to separate the
protruded regions from the normal kidney regions. in order to segment whole
tumors, our framework consists of three networks. the first is a base network,
which extracts kidneys and an initial tumor region masks. the second
protuberance detection network receives the kidney region mask as its input and
predicts a protruded region mask. the last fusion network receives the initial
tumor mask and the protruded region mask to predict a final tumor mask. this
proposed framework enables a better segmentation of isodensity tumors and boosts
the performance of segmentation of kidney tumors on ncct images. the
contribution of this work is summarized as follows:1. present a pioneering work
for segmentation of kidney tumors on ncct images. 2. propose a novel framework
that explicitly captures protuberances in a kidney to enable a better
segmentation of tumors including isodensity types on ncct images. this framework
can be extended to other organs (e.g. adrenal gland, liver, pancreas). 3. verify
that the proposed framework achieves a higher dice score compared to the
standard 3d u-net using a publicly available dataset.",7
1826,Segmentation of Kidney Tumors on Non-Contrast CT Images Using Protuberance Detection Network,2.0,Related Work,"the release of two public ct image datasets with kidney and tumor masks from the
2019/2021 kidney and kidney tumor segmentation challenge [8] (kits19, kits21)
attracted researchers to develop various methods for segmentation.looking at the
top 3 teams from each challenge [6,11,13,17,21], all teams utilized 3d u-net [3]
or v-net [16], which bears a similar architecture. the winner of kits19 [13]
added residual blocks [7] to 3d u-net and predicted kidney and tumor regions
directly. however, the paper notes that modifying the architecture resulted in
only slight improvement. the other 5 teams took a similar approach to nnu-net's
coarse-to-fine cascaded network [12], where it predicts from a low-resolution
image in the first stage and then predicts kidneys and tumors from a
high-resolution image in the second stage. thus, although other attempts were
made, using 3d u-net is the go-to method for predicting kidneys and tumors. in
our work, we also make use of 3d u-net, but using this network alone fails to
learn some isodensity tumors. to overcome this issue, we developed a framework
that specifically incorporates protuberances in kidneys, allowing for an
effective segmentation of tumors on ncct images.in terms of focusing on
protruded regions in kidneys, our work is close to [14,15]. [14] developed a
computer-aided diagnosis system to detect exophytic kidney tumors on ncct images
using belief propagation and manifold diffusion to search for protuberances. an
exophytic tumor is located on the outer surface of the kidney that creates a
protrusion. while this method demonstrated high sensitivity (95%), its false
positives per patient remained high (15 false positives per patient). in our
work, we will not only segment protruded tumors but also other tumors as well.
the first base network is responsible for predicting kidney and tumor region
masks. our architecture is based on 3d u-net, which has an encoder-decoder style
architecture, with few modifications. to reduce the required size of gpu memory,
we only use the encoder that has only 16 channels at the first resolution, but
instead we make the architecture deeper by having 1 strided convolution and 4
max-pooling layers. in the decoder, we replace the up-convolution layers with a
bilinear up-sampling layer and a convolution layer. in addition, by only having
a single convolution layer instead of two in the original architecture at each
resolution, we keep the decoder relatively small. throughout this paper, we
refer this architecture as our 3d u-net.the second protuberance detection
network is the same as the base network except it starts from 8 channels instead
of 16. we train this network using synthetic datasets. the details of the
dataset and training procedures are described in sect. 3.2.the last fusion
network combines the outputs from the base network and the protuberance
detection network and makes the final tumor prediction. in detail, we perform a
summation of the initial tumor mask and the protruded region mask, and then
concatenate the result with the input image. this is the input of the last
fusion network, which also has the same architecture as the base network with an
exception of having two input channels. this fusion network do not just combine
the outputs but also is responsible for removing false positives from the base
network and the protuberance detection network.our combined three network is
fully differentiable, however, to train efficiently, we train the model in 3
steps.",7
1857,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,2.1,Data Curation,"ex-vivo: data is collected from fresh breast tissue samples from the patients
referred to bcs at kingston health sciences center over two years. the study is
approved by the institutional research ethics board and patients consent to be
included. peri-operatively, a pathologist guides and annotates the ex-vivo
pointburns, referred to as spectra, from normal or cancerous breast tissue
immediately after excision. in addition to spectral data, clinicopathological
details such as the status of hormone receptors is also provided
post-surgically. in total 51 cancer and 149 normal spectra are collected and
stratified into five folds (4 for cross validation and 1 prospectively) with
each patient restricted to one fold only.",7
1862,Bridging Ex-Vivo Training and Intra-operative Deployment for Surgical Margin Assessment with Evidential Graph Transformer,0.0,Ex-vivo Evaluation:,"the performance of the proposed network is compared with 3 baseline models
including gtn, graph convolution network [14], and non-graph convolution
network. four-fold cross validation is used for comparison of the different
approaches, to increase the generalizability (3 folds for train/validation, test
on remaining unseen fold, report average test performance). separate ablation
studies are performed for the baseline models to fine tune their structural
parameters. all experiments are implemented using pytorch with adam optimizer,
learning rate of 10 -4 , batch size of 32, and early stopping based on
validation loss. to demonstrate the robustness of the model and ensure it is not
overfitting, we also report the performance of the ensemble model from the
4-fold cross validation study on the 5th unseen prospective test fold.clinical
relevance: hormone receptor status plays an important role in determining breast
cancer prognosis and tailoring treatment plans for patients [6]. here, we
explore the correlation of the attention maps generated by egt with the status
of her2 and pr hormones associated with each spectrum. these hormones are
involved in different types of signaling that the cell depends on [5].",7
1879,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,1.0,Introduction,"breast cancer is the most common cancer and the leading cause of cancer death in
women [18]. early detection of breast cancer allows patients to receive timely
treatment, which may have less burden and a higher probability of survival [6].
among current clinical imaging modalities, magnetic resonance imaging (mri) has
the highest sensitivity for breast cancer detection [12]. especially,
contrastenhanced mri (ce-mri) can identify tumors well and has become an
indispensable technique for detecting and defining cancer [13]. however, the use
of gadolinium-based contrast agents (gbca) requires iv-cannulation, which is a
burden to patients, time consuming and cumbersome in a screening situation.
moreover, contrast administration can lead to allergic reactions and finaly
ce-mri may be associated with nephrogenic systemic fibrosis and lead to
bioaccumulation in the brain, posing a potential risk to human health
[4,9,[14][15][16]. in 2017, the european medicines agency concluded its review
of gbca, confirming recommendations to restrict the use of certain linear gbca
used in mri body scans and to suspend the authorization of other contrast
agents, albeit macrocyclic agents can still be freely used [10].with the
development of computer technology, artificial intelligence-based methods have
shown potential in image generation and have received extensive attention. some
studies have shown that some generative models can effectively perform mutual
synthesis between mr, ct, and pet [19]. among them, synthesis of ce-mri is very
important as mentioned above, but few studies have been done by researchers in
this area due to its challenging nature. li et al. analyzed and studied the
feasibility of using t1-weighted mri and t2-weighted mri to synthesize ce-mri
based on deep learning model [11]. their results showed that the model they
developed could potentially synthesize ce-mri and outperform other cohort
models. however, mri source data of too few sequences (only t1 and t2) may not
provide enough valuable informative to effectively synthesize ce-mri. in another
study, chung et al. investigated the feasibility of using deep learning (a
simple u-net structure) to simulate contrast-enhanced breast mri of invasive
breast cancer, using source data including t1-weighted non-fatsuppressed mri,
t1-weighted fat-suppressed mri, t2-weighted fat-suppressed mri, dwi, and
apparent diffusion coefficient [5]. however, obtaining a complete mri sequence
makes the examination costly and time-consuming. on the other hand, the
information provided by multi-sequences may be redundant and may not contain the
relevant information of ce-mri. therefore, it is necessary to focus on the most
promising sequences to synthesize ce-mri.diffusion-weighted imaging (dwi) is
emerging as a key imaging technique to complement breast ce-mri [3]. dwi can
provide information on cell density and tissue microstructure based on the
diffusion of tissue water. studies have shown that dwi could be used to detect
lesions, distinguish malignant from benign breast lesions, predict patient
prognosis, etc [1,3,7,8,17]. in particular, dwi can capture the dynamic
diffusion state of water molecules to estimate the vascular distribution in
tissues, which is closely related to the contrast-enhanced regions in ce-mri.
dwi may be a valuable alternative in breast cancer detection in patients with
contraindications to gbca [3]. inspired by this, we develop a multi-sequence
fusion network based on t1-weighted mri and multi-b-value dwi to synthesize
ce-mri. our contributions are as follows:i from the perspective of method, we
innovatively proposed a multi-sequence fusion model, designed for combining
t1-weighted imaging and multi-b-value dwi to synthesize ce-mri for the first
time. ii we invented hierarchical fusion module, weighted difference module and
multi-sequence attention module to enhance the fusion at different scale, to
control the contribution of different sequence and maximising the usage of the
information within and across sequences. iii from the perspective of clinical
application, our proposed model can be used to synthesize ce-mri, which is
expected to reduce the use of gbca.",7
1880,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,2.1,Patient Collection and Pre-processing,"this study was approved by institutional review board of our cancer institute
with a waiver of informed consent. we retrospectively collected 765 patients
with breast cancer presenting at our cancer institute from january 2015 to
november 2020, all patients had biopsy-proven breast cancers (all cancers
included in this study were invasive breast cancers, and ductal carcinoma in
situ had been excluded). the mris were acquired with philips ingenia all mris
were resampled to 1 mm isotropic voxels and uniformly sized, resulting in
volumes of 352 × 352 pixel images with 176 slices per mri, and subsequent
registration was performed based on advanced normalization tools (ants) [2].",7
1887,Synthesis of Contrast-Enhanced Breast MRI Using T1- and Multi-b-Value DWI-Based Hierarchical Fusion Network with Attention Mechanism,4.0,Conclusion,"we have developed a multi-sequence fusion network based on multi-b-value dwi to
synthesize ce-mri, using source data including dwis and t1-weighted
fatsuppressed mri. compared to existing methods, we avoid the challenges of
using full-sequence mri and aim to be selective on valuable source data dwi.
hierarchical fusion generation module, weighted difference module, and
multisequence attention module have all been shown to improve the performance of
synthesizing target images by addressing the problems of synthesis at different
scales, leveraging differentiable information within and across sequences. given
that current research on synthetic ce-mri is relatively sparse and challenging,
our study provides a novel approach that may be instructive for future research
based on dwis. our further work will be to conduct reader studies to verify the
clinical value of our research in downstream applications, such as helping
radiologists on detecting tumors. in addition, synthesizing dynamic
contrastenhanced mri at multiple time points will also be our future research
direction. our proposed model can potentially be used to synthesize ce-mri,
which is expected to reduce or avoid the use of gbca, thereby optimizing
logistics and minimizing potential risks to patients.",7
1899,Automated CT Lung Cancer Screening Workflow Using 3D Camera,1.0,Introduction,"lung cancer is the leading cause of cancer death in the united states, and early
detection is key to improving survival rates. ct lung cancer screening is a
lowdose ct (ldct) scan of the chest that can detect lung cancer at an early
stage, when it is most treatable. however, the current workflow for performing
ct lung scans still requires an experienced technician to manually perform
pre-scanning steps, which greatly decreases the throughput of this high volume
procedure. while recent advances in human body modeling [4,5,12,13,15] have
allowed for automation of patient positioning, scout scans are still required as
they are used by automatic exposure control system in the ct scanners to compute
the dose to be delivered in order to maintain constant image quality [3].since
ldct scans are obtained in a single breath-hold and do not require any contrast
medium to be injected, the scout scan consumes a significant portion of the
scanning workflow time. it is further increased by the fact that tube rotation
has to be adjusted between the scout and actual ct scan. furthermore, any
patient movement during the time between the two scans may cause misalignment
and incorrect dose profile, which could ultimately result in a repeat of the
entire process. finally, while minimal, the radiation dose administered to the
patient is further increased by a scout scan.we introduce a novel method for
estimating patient scanning parameters from non-ionizing 3d camera images to
eliminate the need for scout scans during pre-scanning. for ldct lung cancer
screening, our framework automatically estimates the patient's lung position
(which serves as a reference point to start the scan), the patient's isocenter
(which is used to determine the table height for scanning), and an estimate of
patient's water equivalent diameter (wed) profiles along the craniocaudal
direction which is a well established method for defining size specific dose
estimate (ssde) in ct imaging [8,9,11,18]. additionally, we introduce a novel
approach for updating the estimated wed in real-time, which allows for
refinement of the scan parameters during acquisition, thus increasing accuracy.
we present a method for automatically aborting the scan if the predicted wed
deviates from real-time acquired data beyond the clinical limit. we trained our
models on a large collection of ct scans acquired from over 60, 000 patients
from over 15 sites across north america, europe and asia. the contributions of
this work can be summarized as follows:-a novel workflow for automated ct lung
cancer screening without the need for scout scan -a clinically relevant method
meeting iec 62985:2019 requirements on wed estimation. -a generative model of
patient wed trained on over 60, 000 patients.-a novel method for real-time
refinement of wed, which can be used for dose modulation",7
1900,Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.0,Method,"water equivalent diameter (wed) is a robust patient-size descriptor [17] used
for ct dose planning. it represents the diameter of a cylinder of water having
the same averaged absorbed dose as the material contained in an axial plane at a
given craniocaudal position z [2]. the wed of a patient is thus a function
taking as input a craniocaudal coordinate and outputting the wed of the patient
at that given position. as wed is defined in an axial plane, the diameter needs
to be known on both the anterior-posterior (ap) and lateral (left-right) axes
noted respectively w ed ap (z) and w ed l (z). as our focus here is on lung
cancer screening, we define 'wed profile' to be the 1d curve obtained by
uniformly sampling the wed function along the craniocaudal axis within the lung
region.our method jointly predicts the ap and lateral wed profiles. while wed
can be derived from ct images, paired ct scans and camera images are rarely
available, making direct regression through supervised learning challenging. we
propose a semi-supervised approach to estimate wed from depth images. first, we
train a wed generative model on a large collection of ct scans. we then train an
encoder network to map the patient depth image to the wed manifold. finally, we
propose a novel method to refine the prediction using real-time scan data.",7
1901,Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.1,WED Latent Space Training,"we use an autodecoder [10] to learn the wed latent space. our model is a fully
connected network with 8 layers of 128 neurons each. we used layer normalization
and relu activation after each layer except the last one. our network takes as
input a latent vector together with a craniocaudal coordinate z and outputs w ed
ap (z) and w ed l (z), the values of the ap and lateral wed at the given
coordinate. in this approach, our latent vector represents the encoding of a
patient in the latent space. this way, a single autodecoder can learn
patient-specific continuous wed functions. since our network only takes the
craniocaudal coordinate and the latent vector as input, it can be trained on
partial scans of different sizes. the training consists of a joint optimization
of the autodecoder and the latent vector: the autodecoder is learning a
realistic representation of the wed function while the latent vector is updated
to fit the data.during training, we initialize our latent space to a unit
gaussian distribution as we want it to be compact and continuous. we then
randomly sample points along the craniocaudal axis and minimize the l1 loss
between the prediction and the ground truth wed. we also apply l2-regularization
on the latent vector as part of the optimization process.",7
1903,Automated CT Lung Cancer Screening Workflow Using 3D Camera,2.3,Real-Time WED Refinement,"while the depth image provides critical information on the patient anatomy, it
may not always be sufficient to accurately predict the wed profiles. for
example, some patients may have implants or other medical devices that cannot be
guessed solely from the depth image. additionally, since the encoder is trained
on a smaller data collection, it may not be able to perfectly project the depth
image to the wed manifold. to meet the strict safety criteria defined by the
iec, we propose to dynamically update the predicted wed profiles at inference
time using real-time scan data. first, we use our encoder network to initialize
the latent vector to a point in the manifold that is close to the current
patient. then, we use our autodecoder to generate initial wed profiles. as the
table moves and the patient gets scanned, ct data is being acquired and ground
truth wed can be computed for portion of the body that has been scanned, along
with the corresponding craniocaudal coordinate. we can then use this data to
optimize the latent vector by freezing the autodecoder and minimizing the l1
loss between the predicted and ground truth wed profiles through gradient
descent. we can then feed the updated latent vector to our autodecoder to
estimate the wed for the remaining portions of the body that have not yet been
scanned and repeat the process.in addition to improving the accuracy of the wed
profiles prediction, this approach can also help detect deviation from real
data. after the latent vector has been optimized to fit the previously scanned
data, a large deviation between the optimized prediction and the ground truth
profiles may indicate that our approach is not able to find a point in the
manifold that is close to the data. in this case, we may abort the scan, which
further reduces safety risks. overall flowchart of the proposed approach is
shown in fig. 1.",7
1904,Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.1,Data,"our ct scan dataset consists of 62, 420 patients from 16 different sites across
north america, asia and europe. our 3d camera dataset consists of 2, 742 pairs
of depth image and ct scan from 2, 742 patients from 6 different sites across
north america and europe acquired using a ceiling-mounted kinect 2 camera. our
evaluation set consists of 110 pairs of depth image and ct scan from 110
patients from a separate site in europe.",7
1905,Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.2,Patient Preparation,"patient positioning is the first step in lung cancer screening workflow. we
first need to estimate the table position and the starting point of the scan. we
propose to estimate the table position by regressing the patient isocenter and
the starting point of the scan by estimating the location of the patient's lung
top.starting position. we define the starting position of the scan as the
location of the patient's lung top. we trained a denseunet [7] taking the camera
depth image as input and outputting a gaussian heatmap centered at the patient's
lung top location. we used 4 dense blocks of 4 convolutional layers for the
encoder and 4 dense blocks of 4 convolutional layers for the decoder. each
convolutional layer (except the last one) is followed by a batch normalization
layer and a relu activation. we trained our model on 2, 742 patients using
adaloss [14] and the adam [6] optimizer with a learning rate of 0.001 and a
batch size of 32 for 400 epochs. our model achieves a mean error of 12.74 mm and
a 95 th percentile error of 28.32 mm. to ensure the lung is fully visible in the
ct image, we added a 2 cm offset on our prediction towards the outside of the
lung. we then defined the accuracy as whether the lung is fully visible in the
ct image when using the offset prediction. we report an accuracy of 100% on our
evaluation set of 110 patients. third and fourth columns show the performance of
our model with real-time refinement every 5 cm and 2 cm respectively. ground
truth is depicted in green and our prediction is depicted in red. while the
original prediction was off towards the center of the lung, the real-time
refinement was able to correct the error.isocenter. the patient isocenter is
defined as the centerline of the patient's body. we trained a densenet [1]
taking the camera depth image as input and outputting the patient isocenter. our
model is made of 4 dense blocks of 3 convolutional layers. each convolutional
layer (except the last one) is followed by a batch normalization layer and a
relu activation. we trained our model on 2, 742 patients using adadelta [16]
with a batch size of 64 for 300 epochs. on our evaluation set, our model
outperforms the technician's estimates with a mean error of 5.42 mm and a 95 th
percentile error of 8.56 mm compared to 6.75 mm and 27.17 mm respectively.
results can be seen in fig. 2.",7
1906,Automated CT Lung Cancer Screening Workflow Using 3D Camera,3.3,Water Equivalent Diameter,"we trained our autodecoder model on our unpaired ct scan dataset of 62, 420
patients with a latent vector of size 32. the encoder was trained on our paired
ct scan and depth image dataset of 2, 742 patients. we first compared our method
against a simple direct regression model. we trained a denseunet [7] taking the
camera depth image as input and outputting the water equivalent diameter
profile. we trained this baseline model on 2, 742 patients using the adadelta
[6] optimizer with a learning rate of 0.001 and a batch size of 32. we table 1.
wed profile errors on our testing set (in mm). 'w' corresponds to the portion
size of the body that gets scanned before updating the prediction (in cm). top
of the table corresponds to lateral wed profile, bottom corresponds to ap wed
profile. updating the prediction every 20 mm produces the best results.",7
1907,Automated CT Lung Cancer Screening Workflow Using 3D Camera,0.0,Method (lateral),"mean then measured the performance of our model before and after different
degrees of real-time refinement, using the same optimizer and learning rate. we
report the comparative results in table 1.we observed that our method largely
outperforms the direct regression baseline with a mean lateral error 40% lower
and a 90 th percentile lateral error over 30% lower. bringing in real-time
refinement greatly improves the results with a mean lateral error over 40% and a
90 th percentile lateral error over 20% lower than before refinement. ap
profiles show similar results with a mean ap error improvement of nearly 40% and
a 90 th percentile ap error improvement close to 30%. when using our proposed
method with a 20 mm window refinement, our proposed approach outperforms the
direct regression baseline by over 60% for lateral profile and nearly 80% for
ap.figures 3 highlights the benefits of using real-time refinement. overall, our
approach shows best results with an update frequency of 20 mm, with a mean
lateral error of 15.93 mm and a mean ap error of 10.40 mm. figure 4 presents a
qualitative evaluation on patients with different body morphology.finally, we
evaluated the clinical relevancy of our approach by computing the relative error
as described in the international electrotechnical commission (iec) standard iec
62985:2019 on methods for calculating size specific dose estimates (ssde) for
computed tomography [2]. the δ rel metric is defined as:where:-ŵ ed(z) is the
predicted water equivalent diameter -w ed(z) is the ground truth water
equivalent diameter z is the position along the craniocaudal axis of the
patient. iec standard states the median value of the set of δ rel (z) along the
craniocaudal axis (noted δ rel ) should be below 0.1. our method achieved a mean
lateral δ rel error of 0.0426 and a mean ap δ rel error of 0.0428, falling well
within the acceptance criteria.",7
1908,Automated CT Lung Cancer Screening Workflow Using 3D Camera,4.0,Conclusion,"we presented a novel 3d camera based approach for automating ct lung cancer
screening workflow without the need for a scout scan. our approach effectively
estimates start of scan, isocenter and water equivalent diameter from depth
images and meets the iec acceptance criteria of relative wed error. while this
approach can be used for other thorax scan protocols, it may not be applicable
to trauma (e.g. with large lung resections) and inpatient settings, as the
deviation in predicted and actual wed would likely be much higher. in future, we
plan to establish the feasibility as well as the utility of this approach for
other scan protocols and body regions.1",7
1948,CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction,3.1,Dataset and Implementation Details,"dataset. lidc-idri [1] is a dataset for pulmonary nodule classification or
detection based on low-dose ct, which involves 1,010 patients. according to the
annotations, we extracted 2, 026 nodules, and all of them were labeled with
scores from 1 to 5, indicating the malignancy progression. we cropped all the
nodules with a square shape of a doubled equivalent diameter at the annotated
center, then resized them to the volume of 32 × 32 × 32. following [9,11], we
modified the first layer of the image encoder to be with 32 channels. according
to existing works [11,18], we regard a nodule with an average score between 2.5
and 3.5 as unsure nodules, benign and malignant categories are those with scores
lower than 2.5 and larger than 3.5, respectively. in this paper, we construct
three sub-datasets: lidc-a contains three classes of nodules both in training
and test sets; according to [11], we construct the lidc-b, which contains three
classes of nodules only in the training set, and the test set contains benign
and malignant nodules; lidc-c includes benign and malignant nodules both in
training and test sets.experimental settings. in this paper, we apply the clip
pre-trained vit-b/16 as the text encoder for clip-lung, and the image encoder we
used is resnet-18 [6] due to the relatively smaller scale of training data. the
image encoder is initialized randomly. note that for the text branch, we froze
the parameters of the text encoder and updated the learnable tokens l and l
during training. the learning rate is 0.001 following the cosine decay, while
the optimizer is stochastic gradient descent with momentum 0.9 and weight decay
0.00005. the temperature τ is initialized as 0.07 and updated during training.
all of our experiments are implemented with pytorch [15] and trained with nvidia
a100 gpus. the experimental results are reported with average values through
five-fold cross-validation. we report the recall and f1-score values for
different classes and use ""±"" to indicate standard deviation.",7
1964,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,1.0,Introduction,"computer-aided diagnosis (cad) systems have achieved success in many clinical
tasks [5,6,12,17]. most cad studies were developed on regular and selected
datasets in the laboratory environment, which avoided the problems (data noise,
missing data, etc.) in the clinical scenarios [3,6,9,13,18]. in a real clinical
scenario, the clinicians generally synthesize all aspects of information, and
conduct consultations with multidisciplinary team (mdt), to accurately diagnose
and plan the treatment [9,10,13]. real-world studies have received increasing
attention [11,16], and it is challenging for the cad in the real-world scenarios
as: 1) consistent with the clinical workflow, cad needs to consider
multidisciplinary information to obtain multidimensional diagnosis; 2) due to
information collection, storage and manual evaluation, there are missing and
noisy medical data. this phenomenon is especially common in rare tumors like
pancreatic neuroendocrine neoplasms (pnens).in order to overcome above
challenges, some studies [3,9,13,18] used multilabel method because of the
following advantages: 1) the input of the model is only a single modality such
as images, which is easy to apply clinically; 2) the model learns multi-label
and multi-disciplinary knowledge, which is consistent with clinical logic; 3)
multi-label simultaneous prediction, which meets the need of clinical
multi-dimensional description of patients. for the above advantages, multi-label
technology is suitable for real-world cad. the previous multi-label cad studies
were designed based on simple parameter sharing methods [9,15,20] or graph
neural network (gnn) method [2]. the former implicitly interacts with
multi-label information, making it difficult to fully utilize the correlation
among labels; and the latter requires the use of word embeddings pre-trained on
public databases, which is not friendly to many medical domain proper nouns. the
generalizability of previous multi-label cad studies is poor due to these
disadvantages. in addition, none of the current multi-label cad studies have
considered the problem of missing labels and noisy labels.considering these
real-world challenges, we propose a multi-label model named self-feedback
transformer (sft), and validate our method on a realworld pnens dataset. the
main contributions of this work are listed: 1) a transformer multi-label model
based on self-feedback mechanism was proposed, which provided a novel method for
multi-label tasks in real-world medical application; 2) the structure is
flexibility and interactivity to meet the needs of realworld clinical
application by using four inference modes, such as expert-machine combination
mode, etc.; 3) sft has good noise resistance, and can maintain good performance
under noisy label input in expert-assisted mode.",7
1969,Self-feedback Transformer: A Multi-label Diagnostic Model for Real-World Pancreatic Neuroendocrine Neoplasms Data,3.1,Dataset and Evaluation,"real-world pnens dataset. we validated our method on a real-world pnens dataset
from two centers. all patients with arterial phase computed tomography (ct)
images were included. the dataset contained 264 and 28 patients in center 1 and
center 2, and a senior radiologist annotated the bounding boxes for all 408 and
28 lesions. we extracted 37 labels from clinical reports, including survival,
immunohistochemical (ihc), ct findings, etc. among them, 1)recist drug response
(rs), 2)tumor shrink (ts), 3)durable clinical benefit (dcb), 4)progression-free
survival (pfs), 5)overall survival (os), 6)grade (gd), 7)somatostatin receptor
subtype 2(sstr2), 8)vascular endothelial growth factor receptor 2 (vefgr2),
9)o6-methylguanine methyltransferase (mgmt), 10)metastatic foci (mtf), and
11)surgical recurrence (rt) are main tasks, and the remaining are auxiliary
tasks. 143 and 28 lesions were segmented by radiologists, and the radiomics
features of them were extracted, of which 162 features were selected and
binarized as auxiliary tasks because of its statistically significant
correlation with the main labels. the label distribution and the overlap ratio
(jaccard index) of lesions between pairs of labels are shown in fig. 3. it is
obvious that the real-world dataset has a large number of labels with randomly
missing data, thus, we used an adjusted 5-fold cross-validation. taking a
patient as a sample, we chose the dataset from center 1 as the internal dataset,
of which the samples with most of the main labels were used as dataset 1 (219
lesions) and was split into 5 folds, and the remaining samples are randomly
divided into the training set dataset 2 (138 lesions) and the validation set
dataset 3 (51 lesions), the training set and the validation set of the
corresponding folds were added during cross-validation, respectively. all
samples in center 2 left as external test set. details of each dataset are in
the supplementary material. dataset evaluation metrics. we evaluate the
performance of our method on the 10 main tasks for internal dataset, and due to
missing labels and too few sstr2 labels, only the performance of predicting rt,
pfs, os, gd, mtf are evaluated for external dataset. we employ accuracy (acc),
sensitivity (sen), specificity (spc), f1-score (f1) and area under the receiver
operating characteristic (auc) for each task, and compute the mean value of them
(e.g. mauc).",7
2013,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,1.0,Introduction,"brachial plexopathy is a form of peripheral neuropathy [1]. it occurs when there
is damage to the brachial plexus (bp) which is a complex nerve network under the
skin of the shoulder. there is a wide range of disease that may cause a brachial
plexopathy.radiation fibrosis, primary and metastatic lung cancer, and
metastatic breast cancer account for almost three-fourths of causes [2].
brachial plexus syndrome occurs not infrequently in patients with malignant
disease. it is due to compression or direct invasion of the nerves by tumor
which will bring many serious symptoms [3]. our research focuses on the brachial
plexopathy caused by metastatic breast cancers.magnetic resonance imaging (mri)
and ultrasound of the brachial plexus have become two reliable diagnostic tools
for brachial plexopathy [4]. automatic identification of the bp in mri and
ultrasound images has become a hot topic. currently, most of relevant research
in this field are focusing on ultrasound modality [5][6][7][8]. compared with
ultrasound, mri has become the primary imaging technique in the evaluation of
brachial plexus pathology [9]. however, to our knowledge, radiomics related bp
studies utilizing mri have not been reported previously.many radiomics studies
have experimentally demonstrated that image texture has great potential for
differentiation of different tissue types and pathologies [10]. in the past
several decades, many state-of-the-art methods have been proposed to extract
texture patterns [11,12]. however, how to most effectively combine texture
features with deep learning, called deep texture, is still an open area of
research. one prior approach, termed glcm-cnn, was proposed to carry out a polyp
differentiation task [13]. however, how to arrange these glcms to form the 3d
volume to optimize the performance is a major challenge.with the goal of
classifying normal from abnormal bp, we explored the approach of deep texture
learning. this paper constructed a bp dataset with the most commonly used bp
mris in our clinical practice. considering the shortcoming of traditional
patterns, triple point pattern (tpp) is proposed for the quantitative
representation of the heterogeneity of abnormal bp's. in contrast to glcm-cnn,
tppnet is designed to train models by feeding tpp matrices as the input with a
huge number of channels. finally, we analyze the model's performance in the
experimental section. the major contributions of this study include 1) directed
triangle construction idea for tpp, 2) huge number of tpp matrices as the
heterogeneity representations of bp, 3) tppnet with 15 layers and huge number of
channels, 4) the bp dataset containing mr images and their corresponding roi
masks.",8
2014,A Texture Neural Network to Predict the Abnormal Brachial Plexus from Routine Magnetic Resonance Imaging,2.1,Dataset Preparation and Preprocessing,"following irb approval for this study, we search for patients with metastatic
breast cancer who had a breast cancer mri performed between 2010 and 2020 and
had morphologically positive bp on the mri report from our electronic medical
records (emr) in * hospital. totally, we collect approximate 807 series which
include 274 t2, 254 t1 and 279 post-gadolinium. since some scans are seriously
degraded due to motion artifacts. therefore, each case underwent several
essential image adjustments such as multi-series splitting, two-series merging,
slice swapping, artifact checking and boundary corrections. to yield the roi,
firstly, we randomly sampled -40% of the sequences including both normal and
abnormal ones that were manually segmented with itk-snap by two skilled trainees
[14,15]. then, the manual segmentations were utilized to train a 3d nnunet model
which was utilized to train the model which was used to predict rois for the
rest series [16]. the predicted segmentations were manually divided into three
groups, i.e. good, fair and poor. good cases were added to the training set.
this process was repeated until no improvements in the predictions for the
remaining sequences was seen. the final dataset for radiomic analysis was
constructed by merging the datasets for each sequence type. only patients that
had all three sequences segmented (t2, t1 and post-gadolinium) were included in
the dataset. table 1 shows a breakdown of the final dataset.",8
2105,Relaxation-Diffusion Spectrum Imaging for Probing Tissue Microarchitecture,3.3,In Vivo Data: Neurite Morphology,"figure 4(a) shows the relaxation times of the restricted compartment in white
matter lesions, indicating that relaxation times are longer in gliomas than
normal white matter tissue. the higher t 2 in grade 4 glioma is associated with
changes in metabolite compositions, resulting in remarkable changes in neurite
morphology in lesioned tissues (fig. 4(c-d)), consistent with previous
observations [12,23]. the rate of longitudinal relaxation time has been shown to
be positively correlated with myelin content. our results indicate that mte dmri
is more sensitive to neurite morphology than ste dmri (fig. 4(b)).figures 4(c-d)
show that the estimated mean nr in the gray matter is approximately in the range
of 10 µm, which is in good agreement with the sizes of somas in human brains,
i.e., 11 ± 7 µm [26]. rdsi improves the detection of small metastases,
delineation of tumor extent, and characterization of the intratumoral
microenvironment when compared to conventional microstructure models (fig.
4(c)). our studies suggest that rdsi provides useful information on
microvascularity and necrosis helpful for facilitating early stratification of
patients with gliomas (fig. 4(d)).",8
2139,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,1.0,Introduction,"in clinical practice, magnetic resonance imaging (mri) provides important
information for diagnosing and monitoring patient conditions [4,16]. to capture
the complex pathophysiological aspects during disease progression,
multiparametric mri (such as t1w, t2w, dir, flair) is routinely acquired. image
acquisition inherently poses a trade-off between scan time, resolution, and
signalto-noise ratio (snr) [19]. to maximize the source of information within a
reasonable time budget, clinical protocol often combines anisotropic 2d scans of
different contrasts in complementary viewing directions. although acquired 2d
scans offer an excellent in-plane resolution, they lack important details in the
orthogonal out-of-plane. for a reliable pathological assessment, radiologists
often resort to a second scan of a different contrast in the orthogonal viewing
direction. furthermore, poor out-of-plane resolution significantly affects the
accuracy of volumetric downstream image analysis, such as radiomics and lesion
volume estimation, which usually require isotropic 3d scans. as multi-parametric
isotropic 3d scans are not always feasible to acquire due to time-constraints
[19], motion [9], and patient's condition [10], super-resolution offers a
convenient alternative to obtain the same from anisotropic 2d scans. recently,
it has been shown that acquiring three complementary 2d views of the same
contrast may yield higher snr at reduced scan time [19,29]. however, it remains
under-explored if orthogonal anisotropic 2d views of different contrasts can
benefit from each other based on the underlying anatomical consistency.
additionally, whether such strategies can further decrease scan times while
preserving similar resolution and snr remains unanswered. moreover, unlike
conventional super-resolution models trained on a cohort, a personalized model
is of clinical relevance to avoid the danger of potential misdiagnosis caused by
cohort-learned biases. in this work, we mitigate these gaps by proposing a novel
multi-contrast super-resolution framework that only requires the
patient-specific low-resolution mr scans of different sequences (and views) as
supervision. as shown in various settings, our approach is not limited to
specific contrasts or views but provides a generic framework for
super-resolution. the contributions in this paper are three-fold: 1. to the best
of our knowledge, our work is the first to enable subject-specific
multi-contrast super-resolution from low-resolution scans without needing any
high-resolution training data. we demonstrate that implicit neural
representations (inr) are good candidates to learn from complementary views of
multi-parametric sequences and can efficiently fuse low-resolution images into
anatomically faithful super-resolution. 2. we introduce mutual information (mi)
[26] as an evaluation metric and find that our method preserves the mi between
high-resolution ground truths in its predictions. further observation of its
convergence to the ground truth value during training motivates us to use mi as
an early stopping criterion. 3. we extensively evaluate our method on multiple
brain mri datasets and show that it achieves high visual quality for different
contrasts and views and preserves pathological details, highlighting its
potential clinical usage.related work. single-image super-resolution (sisr) aims
at restoring a highresolution (hr) image from a low-resolution (lr) input from a
single sequence and targets applications such as low-field mr upsampling or
optimization of mri acquisition [3]. recent methods [3,8] incorporate priors
learned from a training set [3], which is later combined with generative models
[2]. on the other hand, multi-image super-resolution (misr) relies on the
information from complementary views of the same sequence [29] and is especially
relevant to capturing temporal redundancy in motion-corrupted low-resolution mri
[9,27]. multi-contrast super-resolution (mcsr) targets using inter-contrast
priors [20]. in conventional settings [15], an isotropic hr image of another
contrast is used to guide the reconstruction of an anisotropic lr image. zeng et
al. [30] use a two-stage architecture for both sisr and mcsr. utilizing a
feature extraction network, lyu et al. [14] learn multi-contrast information in
a joint feature space. later, multi-stage integration networks [6], separatable
attention [7] and transformers [13] have been used to enhance joint feature
space learning. however, all current mcsr approaches are limited by their need
for a large training dataset. consequently, this constrains their usage to
specific resolutions and further harbors the danger of hallucination of features
(e.g., lesions, artifacts) present in the training set and does not generalize
well to unseen data.originating from shape reconstruction [18] and multi-view
scene representations [17], implicit neural representations (inr) have achieved
state-of-the-art results by modeling a continuous function on a space from
discrete measurements. key reasons behind inr's success can be attributed to
overcoming the low-frequency bias of multi-layer perceptrons (mlp) [21,24,25].
although mri is a discrete measurement, the underlying anatomy is a continuous
space. we find inr to be a good fit to model a continuous intensity function on
the anatomical space. once learned, it can be sampled at an arbitrary resolution
to obtain the super-resolved mri. following this spirit, inrs have recently been
successfully employed in medical imaging applications ranging from k-space
reconstruction [11] to sisr [29]. unlike [22,29], which learn anatomical priors
in single contrasts, and [1,28], which leverage inr with latent embeddings
learned over a cohort, we focus on employing inr in subject-specific,
multi-contrast settings.",8
2140,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,2.0,Methods,"in this section, we first formally introduce the problem of joint
super-resolution of multi-contrast mri from only one image per contrast per
patient. next, we describe strategies for embedding information from two
contrasts in a shared space. subsequently, we detail our model architecture and
training configuration.problem statement. we denote the collection of all 3d
coordinates of interest in this anatomical space as ω = {(x, y, z)} with
anatomical function q : ω → a. the image intensities are a function of the
underlying anatomical properties a. two contrasts c 1 and c 2 can be scanned in
a low-resolution subspace ω 1 , ω 2 ⊂ ω. let us consider g 1 , g 2 : a → r that
map from anatomical properties to contrast intensities c 1 and c 2 ,
respectively. we obtain sparse observations, where f i is composition of g i and
q. however, one can easily obtain the global anatomical space ω by knowing ω 1
and ω 2 , e.g., by rigid registration between the two images. in this paper, we
aim to estimate f 1 , f 2 : ω → r given i 1 and i 2 .joint multi-contrast
modelling. since both component-functions f 1 and f 2 operate on a subset of the
same input space, we argue that it is beneficial to model them jointly as a
single function f : ω → r 2 and optimize it based on their estimation error
incurred in their respective subsets. this will enable information transfer from
one contrast to another, thus improving the estimation and preventing
over-fitting in single contrasts, bringing consistency to the prediction.to this
end, we propose to leverage inr to model a continuous multi-contrast function f
from discretely sampled sparse observations i 1 and i 2 .mcsr setup. without
loss of generalization, let us consider two lr input contrasts scanned in two
orthogonal planes p 1 and p 2 , where p 1 , p 2 ∈ {axial, sagittal, coronal}. we
assume they are aligned by rigid registration requiring no coordinate
transformation. their corresponding in-plane resolutions are (s 1 ×s 1 ) and (s
2 × s 2 ) and slice thickness is t 1 and t 2 , respectively. note that s 1 < t 1
and s 2 < t 2 imply high in-plane and low out-of-plane resolution. in the end,
we aim to sample an isotropic (s × s × s) grid for both contrasts where s ≤ s 1
, s 2 .implicit neural representations for mcsr. we intend to project the
information available in one contrast into another by embedding both in the
shared weight space of a neural network. however, a high degree of weight
sharing could hinder contrast-specific feature learning. based on this
reasoning, we aim to hit the sweet spot where maximum information exchange can
be encouraged without impeding contrast-specific expressiveness. we propose a
split-head architecture, as shown in fig. 1, where the initial layers jointly
learn the common anatomical features, and subsequently, two heads specialize in
contrast-specific information. the model takes fourier [25] features v =
[cos(2πbx), sin(2πbx)] t as input and predicts [ î1 , î2 ] = f (v), where x =
(x, y, z) and b is sampled from a gaussian distribution n (μ, σ 2 ). we use
mean-squared error loss, l mse , for training.where α and β are coefficients for
the reconstruction loss of two contrasts. note that for points {(x, y, z)} ∈ ω 2
\ ω 1 , there is no explicit supervision coming from low resolution c 1 . for
these points, one can interpret learning c 1 from the loss in c 2 , and vice
versa, to be a weakly supervised task.",8
2142,Single-subject Multi-contrast MRI Super-resolution via Implicit Neural Representations,3.0,Experiments and Results,"datasets. to enable fair evaluation between our predictions and the reference hr
ground truths, the in-plane snr between the lr input scan and corresponding
ground truth has to match. to synthetically create 2d lr images, it is necessary
to downsample out-of-plane in the image domain anisotropically [32] while
preserving in-plane resolution. consequently, to mimic realistic 2d clinical
protocol, which often has higher in-plane details than that of 3d scans, we use
spline interpolation to model partial volume and downsampling. we demonstrate
our network's modeling capabilities for different contrasts (t1w, t2w, flair,
dir), views (axial, coronal, sagittal), and pathologies (ms, brain tumor). we
conduct experiments on two public datasets, brats [16], and msseg [4], and an
in-house clinical ms dataset (cms). in each dataset, we select 25 patients that
fulfill the isotropic acquisition criteria for both ground truth hr scans. note
that we only use the ground truth hr for evaluation, not anywhere in training.
we optimize separate inrs for each subject with supervision from only its two lr
scans. if required, we employ skull-stripping [12] and rigid registration to the
mni152 (msseg, cms) or sri24 (brats) templates. for details, we refer to table 2
in the supplementary.metrics. we evaluate our results by employing common sr
[5,14,29] quality metrics, namely psnr and ssim. to showcase perceptual image
quality, we additionally compute the learned perceptual image patch similarity
(lpips) [31] and measure the absolute error mi in mutual information of two
upsampled images to their ground truth counterparts as follows:baselines and
ablation. to the best of our knowledge, there are no prior data-driven methods
that can perform mcsr on a single-subject basis. hence, we provide
single-subject baselines that operate solely on single contrast and demonstrate
the benefit of information transfer from other contrasts with our proposed
models. quantitative analysis. table 1 demonstrates that our proposed framework
poses a trustworthy candidate for the task of mcsr. as observed in [32], lrtv
struggles for anisotropic up-sampling while smore's overall performance is
better than cubic-spline, but slightly worse to single-contrast inr. however,
the benefit of single-contrast inr may be limited if not complemented by
additional views as in [29]. for mcsr from single-subject scans, we achieve
encouraging results across all metrics for all datasets, contrasts, and views.
since t1w and t2w both encode anatomical structures, the consistent improvement
in brats for both sequences serves as a proof-of-concept for our approach. as
flair is the go-to-sequence for ms lesions, and t1w does not encode such
information, the results are in line with the expectation that there could be a
relatively higher transfer of anatomical information to pathologically more
relevant flair than vice-versa. lastly, given their similar physical acquisition
and lesion sensitivity, we note that dir/flair benefit to the same degree in the
cms dataset.qualitative analysis. figure 2 shows the typical behavior of our
models on cms dataset, where one can qualitatively observe that the split-head
inr pre-serves the lesions and anatomical structures shown in the yellow boxes,
which other models fail to capture. while our reconstruction is not identical to
the gt hr, the coronal view confirms anatomically faithful reconstructions
despite not receiving any in-plane supervision from any contrast during
training. we refer to fig. 4 in the supplementary for similar observations on
brats and msseg.",8
2187,Pelvic Fracture Segmentation Using a Multi-scale Distance-Weighted Neural Network,2.4,Data and Annotation,"although large-scale datasets on pelvic segmentation have been studied in some
research [13], to the best of our knowledge, currently there is no
well-annotated fractured pelvic dataset publicly available. therefore, we
curated a dataset of 100 preoperative ct scans covering all common types of
pelvic fractures. these data is collected from 100 patients (aged 18-74 years,
41 females) who were to undergo pelvic reduction surgery at beijing jishuitan
hospital between 2018 and 2022, under irb approval (202009-04). the ct scans
were acquired on a toshiba aquilion scanner. the average voxel spacing is 0.82 ×
0.82 × 0.94 mm 3 . the average image shape is 480 × 397 × 310.to generate
ground-truth labels for bone fragments, a pre-trained segmentation network was
used to create initial segmentations for the ilium and sacrum [13]. then, these
labels were further modified and annotated by two annotators and checked by a
senior expert.",9
2283,Detecting the Sensing Area of a Laparoscopic Probe in Minimally Invasive Cancer Surgery,1.0,Introduction,"cancer remains a significant public health challenge worldwide, with a new
diagnosis occurring every two minutes in the uk (cancer research uk 1 ). surgery
is one of the main curative treatment options for cancer. however, despite
substantial advances in pre-operative imaging such as ct, mri, or pet/spect to
aid diagnosis, surgeons still rely on the sense of touch and naked eye to detect
cancerous tissues and disease metastases intra-operatively due to the lack of
reliable intraoperative visualization tools. in practice, imprecise
intraoperative cancer tissue detection and visualization results in missed
cancer or the unnecessary removal of healthy tissues, which leads to increased
costs and potential harm to the patient. there is a pressing need for more
reliable and accurate intraoperative visualization tools for minimally invasive
surgery (mis) to improve surgical outcomes and enhance patient care. a recent
miniaturized cancer detection probe (i.e., 'sensei r ' developed by lightpoint
medical ltd.) leverages the cancer-targeting ability of nuclear agents typically
used in nuclear imaging to more accurately identify cancer intraoperatively from
the emitted gamma signal (see fig. 1b) [6]. however, the use of this probe
presents a visualization challenge as the probe is non-imaging and is air-gapped
from the tissue, making it challenging for the surgeon to locate the
probe-sensing area on the tissue surface.it is crucial to accurately determine
the sensing area, with positive signal potentially indicating cancer or affected
lymph nodes. geometrically, the sensing area is defined as the intersection
point between the gamma probe axis and the tissue surface in 3d space, but
projected onto the 2d laparoscopic image. however, it is not trivial to
determine this using traditional methods due to poor textural definition of
tissues and lack of per-pixel ground truth depth data. similarly, it is also
challenging to acquire the probe pose during the surgery.problem redefinition.
in this study, in order to provide sensing area visualization ground truth, we
modified a non-functional 'sensei' probe by adding a miniaturized laser module
to clearly optically indicate the sensing area on the laparoscopic images -i.e.
the 'probe axis-surface intersection'. our system consists of four main
components: a customized stereo laparoscope system for capturing stereo images,
a rotation stage for automatic phantom movement, a shutter for illumination
control, and a daq-controlled switchable laser module (see fig. 1a). with this
setup, we aim to transform the sensing area localization problem from a
geometrical issue to a high-level content inference problem in 2d. it is
noteworthy that this remains a challenging task, as ultimately we need to infer
the probe axis-surface intersection without the aid of the laser module to
realistically simulate the use of the 'sensei' probe.",9
2332,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,1.0,Introduction,"tomographic imaging estimates body density using hundreds of x-ray projections,
but it's slow and harmful to patients. acquisition time may be too high for
certain applications, and each projection adds dose to the patient. a quick,
low-cost 3d estimation of internal structures using only bi-planar x-rays can
revolutionize radiology, benefiting dental imaging, orthopedics, neurology, and
more. this can improve image-guided therapies and preoperative planning,
especially for radiotherapy, which requires precise patient positioning with
minimal radiation exposure.however, this task is an ill-posed inverse problem:
x-ray measurements are the result of attenuation integration across the body,
which makes them very fig. 1. current methods vs our method. feed-forward
methods do not manage to predict a detailed and matching tomographic volume from
a few projections. iterative methods based on neural radiance fields lack prior
for good reconstruction. by learning an embedding for the possible volumes, we
can recover an accurate volume from very few projections with an optimization
based on a bayesian formulation.ambiguous. traditional reconstruction methods
require hundreds of projections to get sufficient constraints on the internal
structures. with very few projections, it is very difficult to disentangle the
structures for even coarse 3d estimation. in other words, many 3d volumes may
have generated such projections a priori.classical analytical and iterative
methods [8] fail when very few projections are available. several works have
attempted to largely decrease the number of projections needed for an accurate
volumetric reconstruction. some deep learning methods [7,12,24,25,30] predict
directly a 3d volume in a forward way from very few projections. the volume is
however not guaranteed to be consistent with the projections and it is not clear
which solution is retrieved. other recent methods have adapted nerfs [20] to
tomographic reconstruction [23,31]. these non-learning methods show good results
when the number of input projections remains higher than a dozen but fail when
very few projections are provided, as our experiments in sect. 3.3 show.as
illustrated in fig. 1, to be able to reconstruct a volume accurately given as
low as two projections only, we first learn a prior on the volume. to do this,
we leverage the potential of generative models to learn a low-dimensional
manifold of the target body part. given projections, we find by a bayesian
formulation the intermediate latent vectors conditioning the generative model
that minimize the error between synthesized projections of our reconstruction
and these input projections. our work builds on hong et al. [10]'s 3d
style-based generative model, which we extend via a more complex network and
training framework.compared to other 3d gans, it is proven to provide the best
disentanglement of the feature space related to semantic features [2].by
contrast with feed-forward methods, our approach does not require paired
projections-reconstructions, which are very tedious to acquire, and it can be
used with different numbers of projections and different projection geometries
without retraining. compared to nerf-based methods, our method exploits prior
knowledge from many patients to require only two projections. we evaluate our
method on reconstructing cancer patients' head-and-neck cts, which involves
intricate and complicated structures. we perform several experiments to compare
our method with a feed-forward-based method [30] and a recent nerf-based method
[23], which are the previous state-of-the-art methods for the very few or few
projections cases, respectively.we show that our method allows to retrieve
results with the finest reconstructions and better matching structures, for a
variety of number of projections. to summarize, our contributions are two-fold:
(i) a new paradigm for 3d reconstruction with biplanar x-rays: instead of
learning to invert the measurements, we leverage a 3d style-based generative
model to learn deep image priors of anatomic structures and optimize over the
latent space to match the input projections; (ii) a novel unsupervised method,
fast and robust to sampling ratio, source energy, angles and geometry of
projections, all of which making it general for downstream applications and
imaging systems.",10
2336,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,2.3,Reconstruction from Biplanar Projections,"since our generative model provides a volume v as a function of vectors w and n,
we can reparameterize our optimization from eq. ( 1) into:note that by contrast
with [18] for example, we optimize on the noise vectors n as well: as we
discovered in our early experiments, the n are also useful to embed
high-resolution details. we take our regularization term r(w, n) as:term l w (w)
=k log n (w k |μ, σ) ensures that w lies on the same distribution as during
training. n (•|μ, σ) represents the density of the standard normal distribution
of mean μ and standard deviation σ.term l c (w) =i,j log m(θ i,j |0, κ)
encourages the w i vectors to be collinear so to keep the generation of
coarse-to-fine structures coherent. m(•; μ, κ) is the density of the von mises
distribution of mean μ and scale κ, which we take fixed, and θ i,j = arccos(
wi•wj wi wj ) is the angle between vectors w i and w j .term l n (n) =j log n (n
j |0, i ) ensures that the n j lie on the same distribution as during training,
i.e., a multivariate standard normal distribution. the λ * are fixed
weights.projection operator. in practice, we take operator a as a 3d cone beam
projection that simulates x-ray attenuation across the patient, adapted from
[21,27]. we model a realistic x-ray attenuation as a ray tracing projection
using material and spectrum awareness:with μ(m, e) the linear attenuation
coefficient of material m at energy state e that is known [11], t m the material
thickness, i 0 the intensity of the source x-ray.for materials, we consider the
bones and tissues that we separate by threshold on electron density. a inverts
the attenuation intensities i atten to generate an x-ray along few directions
successively. we make a differentiable using [21] to allow end-to-end
optimization for reconstruction.3 experiments and results",10
2337,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.1,Dataset and Preprocessing,"manifold learning. we trained our model with a large dataset of 3500 cts of
patients with head-and-neck cancer, more exactly 2297 patients from the publicly
available the cancer imaging archive (tcia) [1,6,16,17,28,32] and 1203 from
private internal data, after obtention of ethical approbations. we split this
data into 3000 cases for training, 250 for validation, and 250 for testing. we
focused ct scans on the head and neck region above shoulders, with a resolution
of 80 × 96 × 112, and centered on the mouth after automatic segmentation using a
pre-trained u-net [22]. the cts were preprocessed by min-max normalization after
clipping between -1024 and 2000 hounsfield units (hu).3d reconstruction. to
evaluate our approach, we used an external private cohort of 80 patients who had
undergone radiotherapy for head-and-neck cancer, with their consent. planning ct
scans were obtained for dose preparation, and cbct scans were obtained at each
treatment fraction for positioning with full gantry acquisition. as can be seen
in fig. 3 and the supplementary material, all these cases are challenging as
there are large changes between the original ct scan and the cbct scans. we
identified these cases automatically by comparing the cbcts with the planning
cts. to compare our reconstruction in the calibrated hu space, we registered the
planning cts on the cbcts by deformable registration with mrf minimization [4].
we hence obtained 3d volumes as virtual cts we considered as ground truths for
our reconstructions after normalization. from these volumes, we generated
projections using the projection module described in sect. 2.3.",10
2339,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,3.3,Results and Discussion,"manifold learning. we tested our model's ability to learn the low-dimensional
manifold. we used fid [9] to measure the distance between the distribution of
generated volumes and real volumes, and ms-ssim [29] to evaluate volumes'
diversity and quality. we obtained a 3d fid of 46 and a ms-ssim of 0.92. for
reference, compared to 3dstylegan [10], our model achieved half their fid score
on another brain mri dataset, with comparable ms-ssim. this may be due to a more
complex architecture, discriminator augmentation, or simpler anatomy.baselines.
we compared our method against the main feed-forward method x2ct-gan [30] and
the neural radiance fields with prior image embedding method nerp [23] meant for
modest sparsely-sampled reconstruction. recent methods like [24] and [12] were
excluded because they provide only minor improvements compared to x2ct-gan [30]
and have similar constraints to feed-forward methods. additionally, no public
implementation is available. [26] uses a flow-based generative model, but the
results are of lower quality compared to gans and similar to x2ct-gan [30].3d
reconstruction. to evaluate our method's performance with biplanar projections,
we focused on positioning imaging for radiotherapy. figure 3 compares our
reconstruction with those of the baselines from biplanar projections. our method
achieves better fitting of the patient structure, including bones, tissues, and
air separations, almost matching the real ct volume. x2ct-gan [30] produced
realistic structures, but failed to match the actual structures as it does not
enforce consistency with the projections. in some clinical procedures, an
earlier ct volume of the patient may be available and can be used as an
additional input for nerp [23]. without a previous ct volume, nerp lacks the
necessary prior to accurately solve the ill-posed problem. even when initialised
with a previous ct volume, nerp often fails to converge to the correct volume
and introduces many artifacts when few projections are used. in contrast, our
method is more versatile and produces better results. we used quantitative
metrics (psnr and ssim) to evaluate reconstruction error and human perception,
respectively. table 1 shows these metrics for our method and baselines with 1 to
8 cone beam projections. deviation from projections, as in x2ct-gan, leads to
inaccurate reconstruction. however, relying solely on projection consistency is
inadequate for this ill-posed problem. nerp matches projections but cannot
reconstruct the volume correctly. our approach balances between instant and
iterative methods by providing a reconstruction in 25 s with 100 optimization
steps, while ensuring maximal consistency. in contrast, nerp requires 7 min, and
x2ct-gan produces structures instantly but unmatching. clinical cbct acquisition
and reconstruction by fdk [3] take about 1-2 min and 10 s respectively. our
approach significantly reduces clin-ical time and radiation dose by using
instant biplanar projections, making it promising for fast 3d visualization
towards complex positioning.",10
2340,X2Vision: 3D CT Reconstruction from Biplanar X-Rays with Deep Structure Prior,4.0,"Conclusion, Limitations, and Future Work","we proposed a new unsupervised method for 3d reconstruction from biplanar x-rays
using a deep generative model to learn the structure manifold and retrieve the
maximum a posteriori volume with the projections, leading to stateof-the-art
reconstruction. our approach is fast, robust, and applicable to various human
body parts, making it suitable for many clinical applications, including
positioning and visualization with reduced radiation.future hardware
improvements may increase resolution, and our approach could benefit from other
generative models like latent diffusion models. this approach may provide coarse
reconstructions for patients with rare abnormalities, as most learning methods,
but a larger dataset or developing a prior including tissue abnormalities could
improve robustness.",10
2342,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,1.0,Introduction,"magnetic resonance imaging (mri) and computed tomography (ct) are two commonly
used cross-sectional medical imaging techniques. mri and ct produce different
tissue contrast and are often used in tandem to provide complementary
information. while mri is useful for visualizing soft tissues (e.g. muscle, [20]
fails to preserve the smooth anatomy of the mri. (c) attentiongan [12] inflates
the head area in the synthetic ct, which is inconsistent with the original mri.
quantitative evaluations in mae (lower is better) are shown in yellow.fat), ct
is superior for visualizing bony structures. some medical procedures, such as
radiotherapy for brain tumors, craniosynostosis, and spinal surgery, typically
require both mri and ct for planning. unfortunately, ct imaging exposes patients
to ionizing radiation, which can damage dna and increase cancer risk [9],
especially in children and adolescents. given these issues, there are clear
advantages for synthesizing anatomically accurate ct data from mri.most
synthesis methods adopt supervised learning paradigms and train generative
models to synthesize ct [1][2][3]6,17]. despite the superior performance,
supervised methods require a large amount of paired data, which is prohibitively
expensive to acquire. several unsupervised mri-to-ct synthesis methods [4,6,14],
leverage cyclegan with cycle consistency supervision to eliminate the need for
paired data. unfortunately, the performance of unsupervised ct synthesis methods
[4,14,15] is inferior to supervised counterparts. due to the lack of direct
constraints on the synthetic outputs, cyclegan [20] struggles to preserve the
anatomical structure when synthesizing ct images, as shown in fig. 1(b). the
structural distortion in synthetic results exacerbates when data from the two
modalities are heavily misaligned, which usually occurs in pediatric scanning
due to the rapid growth in children.recent unsupervised methods impose
structural constraints on the synthesized ct through pixel-wise or shape-wise
consistency. pixel-wise consistency methods [8,14,15] capture and align
pixel-wise correlations between mri and synthesized ct. however, enforcing
pixel-wise consistency may introduce undesirable artifacts in the synthetic
results. this problem is particularly relevant in brain scanning, where both the
pixel-wise correlation and noise statistics in mr and ct images are different,
as a direct consequence of the signal acquisition technique. the alternative
shape-wise consistency methods [3,4,19] aim to preserve the shapes of major body
parts in the synthetic image. notably, shape-cyclegan [4] segments synthesized
ct and enforces consistency with the ground-truth mri segmentation. however,
these methods rely on segmentation annotations, which are time-consuming,
labor-intensive, and require expert radiological annotators. a recent natural
image synthesis approach, called attention-gan [12], learns attention masks to
identify discriminative structures. atten-tiongan implicitly learns prominent
structures in the image without using the ground-truth shape. unfortunately, the
lack of explicit mask supervision can lead to imprecise attention masks and, in
turn, produce inaccurate mappings of the anatomy, as shown in fig. 1(c). in this
paper, we propose maskgan, a novel unsupervised mri-to-ct synthesis method, that
preserves the anatomy under the explicit supervision of coarse masks without
using costly manual annotations. unlike segmentationbased methods [4,18],
maskgan bypasses the need for precise annotations, replacing them with standard
(unsupervised) image processing techniques, which can produce coarse anatomical
masks. such masks, although imperfect, provide sufficient cues for maskgan to
capture anatomical outlines and produce structurally consistent images. table 1
highlights our differences compared with previous shape-aware methods [4,12].
our major contributions are summarized as follows. 1) we introduce maskgan, a
novel unsupervised mri-to-ct synthesis method. maskgan is the first framework
that maintains shape consistency without relying on human-annotated
segmentation. 2) we present two new structural supervisions to enforce
consistent extraction of anatomical structures across mri and ct domains. 3)
extensive experiments show that our method outperforms state-of-the-art methods
by using automatically extracted coarse masks to effectively enhance structural
consistency.",10
2347,Structure-Preserving Synthesis: MaskGAN for Unpaired MR-CT Translation,3.1,Experimental Settings,"data collection. we collected 270 volumetric t1-weighted mri and 267 thinslice
ct head scans with bony reconstruction performed in pediatric patients under
routine scanning protocols1 . we targeted the age group from 6-24 months since
pediatric patients are more susceptible to ionizing radiation and experience a
greater cancer risk (up to 24% increase) from radiation exposure [7].
furthermore, surgery for craniosynostosis, a birth defect in which the skull
bones fuse too early, typically occurs during this age [5,16]. the scans were
acquired by ingenia 3.0t mri scanners and philips brilliance 64 ct scanners. we
then resampled the volumetric scans to the same resolution of 1.0 × 1.0 × 1.0 mm
3 . the dataset comprises brain mr and ct volumes from 262 subjects. 13 mri-ct
volumes from the same patients that were captured less than three months apart
are registered using rigid registration algorithms. the dataset is divided into
249, 1 and 12 subjects for training, validating and testing set. following [13],
we conducted experiments on sagittal slices. each mr and ct volume consists of
180 to 200 slices, which are resized and padded to the size of 224 × 224. the
intensity range of ct is clipped into [-1000, 2000]. all models are trained
using the adam optimizer for 100 epochs, with a learning rate of 0.0002 which
linearly decays to zero over the last 50 epochs. we use a batch size of 16 and
train on two nvidia rtx 3090 gpus.evaluation metrics. to provide a quantitative
evaluation of methods, we compute the same standard performance metrics as in
previous works [6,14] including mean absolute error (mae), peak signal-to-noise
ratio (psnr), and structural similarity (ssim) between ground-truth and
synthesized ct. the scope of the paper centers on theoretical development;
clinical evaluations such as dose calculation and treatment planning will be
conducted in future work.",10
2368,Co-learning Semantic-Aware Unsupervised Segmentation for Pathological Image Registration,0.0,3D Pseudo Brain MRI.,"to evaluate the performance of atlas-based registration, it is essential to have
the correct mapping of pathological regions to healthy brain regions. to create
such a mapping, we created a pseudo dataset by utilizing images from the oasis-1
and brats2020. from the resulting t1 sequences, a pseudo dataset of 300 images
was randomly selected for further analysis. appendix b provides a detailed
process for creating the pseudo dataset.real data with landmarks. brats-reg 2022
[2] provides extensive annotations of landmarks points within both the
pre-operative and the follow-up scans that have been generated by clinical
experts. a total of 140 images are provided, of which 112 are for training, and
28 for testing. comparison to pathology registration. we compared our method
(gir-net) with competitive algorithms: 1) three cutting-edge deep learning-based
unsupervised deformable registration approaches: voxelmorph [3], voxelmorph-df
[8] and symnet [14]. 2) two unsupervised deformable registration methods for
pathological images: dramms [18] and dirac [16]. dramms is an optimization-based
method that reduces the impact of non-corresponding regions. dirac jointly
estimates regions with absent correspondence and bidirectional deformation
fields and ranked first in the bratsreg2022 challenge.atlas-based registration.
after creating the pseudo dataset, we warped brain mr images without tumors to
the atlas and used the resulting deformation field as the gold standard for
evaluation. we then evaluated the mean deformation error (mde) [10], which is
calculated as the average euclidean distance between the coordinates of the
deformation field and the gold standard within specific regions of interest.
these regions include: 1) the tumor region. 2) the normal region near the tumor
(within 30 voxels). 3) the normal region far from the tumor (over 30 voxels but
within brain tissue). our results, presented in fig. 2, show that our method
with histogram matching (hm) outperforms other methods in all three regions,
particularly in the normal regions (near and far). by utilizing hm, our network
achieves an mde of less than 1 mm compared to the gold standard deformations.
these results demonstrate the effectiveness of our method in differentiating the
impact of pathology in atlas-based registration tasks. specifically, dirac is
unable to eliminate the influence of domain differences and resulting in the
largest registration error among the evaluated methods.longitudinal
registration. to perform the longitudinal registration task, we registered each
pre-operative scan to the corresponding follow-up scan of the same patient and
measured the mean target registration error (tre) of the paired landmarks using
the resulting deformation field. for this purpose, we leveraged segnet, trained
on brats2020, to segment the tumor of brat-sreg2022 and separated the landmarks
into two regions: near tumor and far from tumor. figure 3 shows the mean tre for
the various registration approaches. in our proposed framework, we replaced
regnet with cir-dm [15] (denoted as gir(cirdm)) without the need for supervised
training or pretraining, and achieved comparable performance with the
state-of-the-art method dirac. moreover, our gir approach outperforms other deep
learning-based methods and achieved accurate segmentation of pathological
images.to quantitatively evaluate the segmentation capability of our proposed
framework, we compared its performance with other unsupervised segmentation
techniques methods, including unsupervised clustering toolbox aucseg [25], joint
non-correspondence segmentation and registration method ncrnet [1], and dirac.
we used the mean dice similarity coefficient (dsc) to evaluate the similarity
between predicted masks and the ground truth. as shown in table 1, aucseg fails
to detect the lesion in t1 scans. our proposed framework achieved the highest
dsc result of 0.83, following post-processing.ablation study. we compared the
performance of the inpnet trained with histogram matching (hm) and the segnet
trained with ground truth masks (supervised). the results, shown in table 1 and
fig. 2, demonstrate that domain differences between s and t have a significant
effect on segmentation accuracy (without hm), leading to lower registration
quality overall. additionally, fig. 4 shows an example of a pseudo image. we
reconstructed the spatial correspondence by first using segnet to localize the
lesion and then using inpnet to inpaint it with the normal appearance.",10
2425,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,1.0,Introduction,"computed tomography (ct) is one of the most widely used technologies in medical
imaging, which can assist doctors for diagnosing the lesions in human internal
organs. due to harmful radiation exposure of standard-dose ct, the low dose ct
is more preferable in clinical application [4,6,34]. however, when the dose is
low together with the issues like sparse-view or limited angles, it becomes
quite challenging to reconstruct high-quality ct images. the high-quality ct
images are important to improve the performance of diagnosis in clinic [27]. in
mathematics, we model the ct imaging as the following procedure y = t (x r ) +
δ, (1) where x r ∈ r d denotes the unknown ground-truth picture, y ∈ r m denotes
the received measurement, and δ is the noise. the function t represents the
forward operator that is analogous to the radon transform, which is widely used
in medical imaging [23,28]. the problem of ct reconstruction is to recover x r
from the received y.solving the inverse problem of ( 1) is often very
challenging if there is no any additional information. if the forward operator t
is well-posed and δ is neglectable, we know that an approximate x r can be
easily obtained by directly computing t -1 (y). however, t is often ill-posed,
which means the inverse function t -1 does not exist and the inverse problem of
(1) may have multiple solutions. moreover, when the ct imaging is low-dose, the
filter backward projection (fbp) [11] can produce serious detrimental artifact.
therefore, most of existing approaches usually incorporate some prior knowledge
during the reconstruction [14,17,26]. for example, a commonly used method is
based on regularization:where • p denotes the p-norm and r(x) denotes the
penalty item from some prior knowledge.in the past years, a number of methods
have been proposed for designing the regularization r. the traditional
model-based algorithms, e.g., the ones using total variation [3,26], usually
apply the sparse gradient assumptions and run an iterative algorithm to learn
the regularizers [12,18,24,29]. another popular line for learning the
regularizers comes from deep learning [13,17]; the advantage of the deep
learning methods is that they can achieve an end-to-end recovery of the true
image x r from the measurement y [1,21]. recent researches reveal that
convolutional neural networks (cnns) are quite effective for image denoising,
e.g., the cnn based algorithms [10,34] can directly learn the reconstructed
mapping from initial measurement reconstructions (e.g., fbp) to the ground-truth
images. the dual-domain network that combines the sinograms with reconstructed
low-dose ct images were also proposed to enhance the generalizability [15,30].a
major drawback of the aforementioned reconstruction methods is that they deal
with the input ct 2d slices independently (note that the goal of ct
reconstruction is to build the 3d model of the organ). namely, the neighborhood
correlations among the 2d slices are often ignored, which may affect the
reconstruction performance in practice. in the field of computer vision,
""optical flow"" is a common technique for tracking the motion of object between
consecutive frames, which has been applied to many different tasks like video
generation [35], prediction of next frames [22] and super resolution synthesis
[5,31]. to estimate the optical flow field, existing approaches include the
traditional brightness gradient methods [2] and the deep networks [7]. the idea
of optical flow has also been used for tracking the organs movement in medical
imaging [16,20,33]. however, to the best of our knowledge, there is no work
considering gans with using optical flow to capture neighbor slices coherence
for low dose 3d ct reconstruction.in this paper, we propose a novel optical flow
based generative adversarial network for 3d ct reconstruction. our intuition is
as follows. when a patient is located in a ct equipment, a set of consecutive
cross-sectional images are generated. if the vertical axial sampling space of
transverse planes is small, the corresponding ct slices should be highly
similar. so we apply optical flow, though there exist several technical issues
waiting to solve for the design and implementation, to capture the local
coherence of adjacent ct images for reducing the artifacts in low-dose ct
reconstruction. our contributions are summarized below:1. we introduce the
""local coherence"" by characterizing the correlation of consecutive ct images,
which plays a key role for suppressing the artifacts. 2. together with the local
coherence, our proposed generative adversarial networks (gans) can yield
significant improvement for texture quality and stability of the reconstructed
images. 3. to illustrate the efficiency of our proposed approach, we conduct
rigorous experiments on several real clinical datasets; the experimental results
reveal the advantages of our approach over several state-of-the-art ct
reconstruction methods.",10
2429,Solving Low-Dose CT Reconstruction via GAN with Local Coherence,4.0,Experiment,"datasets. first, our proposed approaches are evaluated on the ""mayo-clinic
low-dose ct grand challenge"" (mayo-clinic) dataset of lung ct images [19].the
dataset contains 2250 two dimensional slices from 9 patients for training, and
the remaining 128 slices from 1 patient are reserved for testing. the lowdose
measurements are simulated by parallel-beam x-ray with 200 (or 150) uniform
views, i.e., n v = 200 (or n v = 150), and 400 (or 300) detectors, i.e., n d =
400 (or n d = 300). in order to further verify the denoising ability of our
approaches, we add the gaussian noise with standard deviation σ = 2.0 to the
sinograms after x-ray projection in 50% of the experiments. to evaluate the
generalization of our model, we also consider another dataset rider with
nonsmall cell lung cancer under two ct scans [36] for testing. we randomly
select 4 patients with 1827 slices from the dataset. the simulation process is
identical to that of mayo-clinic. the proposed networks were implemented in the
pytorch framework and trained on nvidia 3090 gpu with 100 epochs.baselines and
evaluation metrics. we consider several existing popular algorithms for
comparison. ( 1) fbp [11]: the classical filter backward projection on low-dose
sinograms. ( 2) fbpconvnet [10]: a direct inversion network followed by the cnn
after initial fbp reconstruction. ( 3) lpd [1]: a deep learning method based on
proximal primal-dual optimization. ( 4) uar [21]: an end-toend reconstruction
method based on learning unrolled reconstruction operators and adversarial
regularizers. our proposed method is denoted by gan-lc.we set λ pix = 1.0, λ adv
= 0.01 and λ per = 1.0 for the optimization objective in eq. ( 7) during our
training process. following most of the previous articles on 3d ct
reconstruction, we evaluate the experimental performance by two metrics: the
peak signal-to-noise ratio (psnr) and the structural similarity index (ssim)
[32]. psnr measures the pixel differences of two images, which is negatively
correlated with mean square error. ssim measures the structure similarity
between two images, which is related to the variances of the input images. for
both two measures, the higher the better.results. a similar increasing trend
with our approach across different settings but has worse reconstruction
quality. to evaluate the stability and generalization of our model and the
baselines trained on mayo-clinic dataset, we also test them on the rider
dataset. the results are shown in table 2. due to the bias in the datasets
collected from different facilities, the performances of all the models are
declined to some extents. but our proposed approach still outperforms the other
models for most testing cases.to illustrate the reconstruction performances more
clearly, we also show the reconstruction results for testing images in fig. 3.
we can see that our network can reconstruct the ct image with higher quality.
due to the space limit, the experimental results of different views n v and more
visualized results are placed in our supplementary material.",10
2432,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,1.0,Introduction,"radiotherapy (rt) is one of the cornerstones of cancer patients. it utilizes
ionizing radiation to eradicate all cells of a tumor. the total radiation dose
is typically divided over 3-30 daily fractions to optimize its effect. as the
surrounding normal tissue is also sensitive to radiation, highly accurate
delivery is vital. image guided rt (igrt) is a technique to capture the anatomy
of the day using in room imaging in order to align the treatment beam with the
tumor location [1]. cone beam ct (cbct) is the most widely used imaging modality
for igrt.a major challenge especially for cbct imaging of the thorax and
upperabdomen is the respiratory motion that introduces blurring of the anatomy,
reducing the localization accuracy and the sharpness of the image.a technique
used to alleviate motion artifacts is respiratory correlated cbct (4dcbct) [16].
from the projections, it is possible to extract a respiratory signal [12], which
indicates the position of the organs within the patient during breathing. with
this, subsets of the projections can be defined to create reconstructions that
resolve the motion. however, only 20 to 60 respiratory periods are imaged. this
limits the number of projections available and results in view-aliasing [16].
additionally, the projections are affected by stochastic measurement noise
caused by the finite imaging dose used, which further degrades the quality of
the reconstruction even when all projections are used.several traditional
methods based on iterative reconstruction algorithms and motion compensation
techniques are used to reduce view-aliasing in 4dcbcts [7,10,11,14,15]. although
effective, these methods suffer from motion modeling uncertainty and prolonged
reconstruction times.deep learning has been proposed as a way to address
view-aliasing with accelerated reconstruction [6]. however, the method cannot
reduce measurement noise because it is still present in the images used as
targets during training.a different method, called noise2inverse, uses an
unsupervised approach to reduce measurement noise in the traditional ct setting
[4]. there are two ways to apply it to 4dcbct and both fail to reduce stochastic
noise effectively. the first is to apply noise2inverse to each
respiratory-correlated reconstruction. in this case, the method will struggle
because of the very low number of projections that are available. the second is
to apply noise2inverse directly to all the projections. in this case, the motion
artifacts that blur the image will appear again, as noise2inverse requires
averaging the sub-reconstructions to obtain a clean reconstruction.we propose
noise2aliasing to address these limitations. the method can be used to provably
train models to reduce both view-aliasing artifacts and stochastic noise from
4dcbcts in an unsupervised way. training deep learning models for medical
applications often needs new data. this was not the case for noise2aliasing, and
historical clinical data sufficed for training.we validated our method on
publicly available data [15] against a supervised approach [6] and applied it to
an internal clinical dataset of 30 lung cancer patients. we explore different
dataset sizes to understand their effects on the reconstructed images.",10
2436,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,4.0,Experiments,"first, we used the spare varian dataset to study whether noise2aliasing can
match the performance of the supervised baseline and if it can outperform it
when adding noise to the projections. then, we use the internal dataset to
explore the requirements for the method to be applied to an existing clinical
dataset. these required around 64 gpu days on nvidia a100 gpus.training of the
model is done on 2d slices. the projections obtained during a scan are
sub-sampled according to the pseudo-average subset selection method described in
[6] and then used to obtain 3d reconstructions. in noise2aliasing these are used
for both input and target during training. given two volumes (x, y), the
training pairs (x i (k) , y i (k) ) are the same i-th slice along the k-th
dimension of each volume chosen to be the axial plane.the datasets used in this
study are two:1. the spare varian dataset was used to provide performance
results on publicly available patient data. to more closely resemble normal
respiratory motion per projection image, the 8 min scan has been used from each
patient (five such scans are available in the dataset). training is performed
over 4 patients while 1 patient is used as a test set. the hyperparameters are
optimized over the training dataset.2. an internal dataset (irb approved) of 30
lung cancer patients' 4dcbcts from 2020 to 2022, originally used for igrt, with
25 patients for training and 5 patients for testing. the scans are 4 min 205 •
scans with 120kev source and 512 × 512 sized detector, using elekta linacs. the
data were anonymized prior to analysis.projection noise was added using the
poisson distribution to the spare varian dataset to evaluate the ability of the
unsupervised method to reduce it. given a projected value of p and a photon
count π (chosen to be 2500), the rate of the poisson distribution is defined as
πe -p and given a sample q from this distribution, then the new projected value
is p =log q π .the architecture used in this work is the mixed scale dense cnn
(msd) [8], the most successful architecture from noise2inverse [4]. the msd
makes use of dilated convolutions to process features at all scales of the
image. we use the msd with depth 200 and width 1, adam optimizer, mse loss, a
batch size of 16, and a learning rate of 0.0001.the baselines we compare against
are two. the first is the traditional fdk obtained using rtk [13]. the second is
the supervised approach proposed by [6], where we replace the model with the
msd, for a fair comparison. in the supervised approach, the model is trained by
using as input reconstructions obtained from subsets defined with pseudo-average
subset selection while the targets use all of the projections available.the
metrics used in this work are the root mean squared error (rmse), peak
signal-to-noise ratio (psnr), and structural similarity index measure (ssim)
[17] all the metrics are defined between the output of the neural network and a
3d (cb)ct scan. for the spare varian dataset, we use the rois defined provided
[15] and used the 3d reconstruction using all the projections available as a
ground truth. for the internal dataset, we deformed the planning ct to each of
the phases reconstructed using the fdk algorithm and evaluate the metric over
only the 4dcbct volume boundaries.",10
2437,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,5.0,Results and Discussion,"spare varian. inference speed with the nvidia a100 gpu averages 600ms per volume
made of 220 slices. from the qualitative evaluation of the methods in fig. 1,
noise2aliasing matches the visual quality of the supervised approach on the
low-noise dataset on both soft tissue and bones. the metrics in table 1 show
mean and standard deviation across all phases for a single patient. in the
lownoise setting, both supervised and noise2aliasing outperform fdk with very
similar results, often within a single standard deviation. noise2aliasing
successfully matches the performance of the supervised baseline. noisy spare
varian. from fig. 1 and table 1, the supervised approach reproduces the noise
that was seen during training, while noise2aliasing manages to remove it
consistently, outperforming the supervised approach, especially in the soft
tissue area around the lungs, where the noise affects attenuation coefficients
the most.noise2aliasing is capable of reducing the artifacts present in
reconstructions caused by stochastic noise in the projections used,
outperforming the supervised baseline.internal dataset. noise2alisting trained
on 25 patients and tested on 5 achieved mean psnr of 35.24 and ssim of 0.91,
while the clinical method achieved mean psnr of 29.97 and 0.74 ssim with p-value
of 0.048 for the psnr and 0.0015 for the ssim, so noise2aliasing was
significantly better according to both metrics. additionally, from fig. 3 we can
see how the breathing extent is matched with sharp reconstruction of the
diaphragm. overall, using more patients results in better noise reduction and
sharper reconstructions (see fig. 2), fig. 2. reconstruction using
noise2aliasing with different-sized datasets. with fewer patients, the model is
more conservative and tends to keep more noise, but also smudges the interface
between tissues and bones. with more patients, more of the view-aliasing is
addressed, and the reconstruction is sharper, however, a few small anatomical
structures tend to be suppressed by the model.especially between fat tissue and
skin and around the bones. however, the model also tends to remove small
anatomical structures as high-frequency objects that cannot be distinguished
from the noise.when applied to a clinical dataset, noise2aliasing benefits from
more patients being included in the dataset, however, qualitatively good
performance is already achieved with 5 patients. no additional data collection
was required and the method can be applied without major changes to the current
clinical practice.",10
2438,Noise2Aliasing: Unsupervised Deep Learning for View Aliasing and Noise Reduction in 4DCBCT,6.0,Conclusion,"we have presented noise2aliasing, a method to provably remove both viewaliasing
and stochastic projection noise from 4dcbcts using an unsupervised deep learning
method. we have empirically demonstrated its performance on a publicly available
dataset and on an internal clinical dataset. noise2aliasing outperforms a
supervised approach when stochastic noise is present in the projections and
matches its performance on a popular benchmark. noise2aliasing can be trained on
existing historical datasets and does not require changing current clinical
practices. the method removes noise more reliably when the dataset size is
increased, however further analysis is required to establish a good quantitative
measurement of this phenomenon. as future work, we plan to study noise2aliasing
in the presence of changes in the breathing frequency and amplitude between
patients and during a scan.",10
2447,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,3.1,Experimental Evaluations,"forty 128 × 128 × 40 3d zubal brain phantoms [24] were used in the simulation
study as ground truth, and one clinical patient brain images with different dose
level were used for the robust analysis. two tumors with different size were
added in each zubal brain phantom. the ground truth images were firstly
forward-projected to generate the noise-free sinogram with count of 10 6 for
each transverse slice and then poisson noise were introduced. 20 percent of
uniform random events were simulated. in total, 1600 (40 × 40) 2d sinograms were
generated. among them, 1320 (33 samples) were used in training, 200 (5 samples)
for testing, and 80 (2 samples) for validation. a total of 5 realizations were
simulated and each was trained/tested independently for bias and variance
calculation [15]. we used peak signal to noise ratio (psnr), structural
similarity index (ssim) and root mean square error (rmse) for overall
quantitative analysis. the contrast recovery coefficient (crc) [25] was used for
the comparison of reconstruction results in the tumor region of interest (roi)
area.",10
2449,DULDA: Dual-Domain Unsupervised Learned Descent Algorithm for PET Image Reconstruction,4.0,Discussion,"to test the robustness of proposed dulda, we forward-project one patient brain
image data with different dose level and reconstructed it with the trained dulda
model. the results compared with mlem are shown in fig. 3. the patient is
scanned with a ge discovery mi 5-ring pet/ct system. the real image has very
different cortex structure and some deflection compared with the training data.
it can be observed that dulda achieves excellent reconstruction results in both
details and edges across different dose level and different slices.table 2 shows
the ablation study on phase numbers and loss function for dulda. it can be
observed that the dual domain loss helps improve the performance and when the
phase number is 4, dulda achieves the best performance.",10
2459,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,1.0,Introduction,"image registration is a fundamental requirement for medical image analysis and
has been an active research focus for decades [1]. it aims to find a spatial
transformation between a pair of fixed and moving images, through which the
moving image can be warped to spatially align with the fixed image. similar to
natural image registration [2], medical image registration usually requires
affine registration to eliminate rigid misalignments and then performs
additional deformable registration to address non-rigid deformations.
traditional methods usually formulate medical image registration as a
time-consuming iterative optimization problem [3,4]. recently, deep registration
methods based on deep learning have been widely adopted to perform end-to-end
registration [5,6]. deep registration methods learn a mapping from image pairs
to spatial transformations based on training data in an unsupervised manner,
which have shown advantages in registration accuracy and computational
efficiency [7][8][9][10][11][12][13][14][15][16][17][18].many deep registration
methods perform coarse-to-fine registration to improve registration accuracy,
where the registration is decoupled into multiple coarse-to-fine registration
steps that are iteratively performed by using multiple cascaded networks
[10][11][12][13] or repeatedly running a single network for multiple iterations
[14,15]. mok et al. [13] proposed a laplacian pyramid image registration network
(lapirn), where multiple networks at different pyramid levels were cascaded. shu
et al. [14] proposed to use a single network (ulae-net) to perform
coarse-to-fine registration with multiple iterations. these methods perform
iterative coarse-to-fine registration and extract image features repeatedly in
each iteration, which inevitably increases computational loads and prolongs the
registration runtime. recently, non-iterative coarse-to-fine (nice) registration
methods have been proposed to perform coarse-to-fine registration with a single
network in a single iteration [16][17][18]. for example, we previously proposed
a nice registration network (nice-net) [18,19], where multiple coarse-to-fine
registration steps are performed with a single network in a single iteration.
these nice registration methods show advantages in both registration accuracy
and runtime on the benchmark task of intra-patient brain mri registration.
nevertheless, we identified that existing nice registration methods still have
two main limitations.firstly, existing nice registration methods merely focus on
deformable coarseto-fine registration, while affine registration, a common
prerequisite, is still reliant on traditional registration methods [16,18] or
extra affine registration networks [17]. using traditional registration methods
incurs time-consuming iterative optimization, while cascading extra networks
consumes additional computational resources (e.g., extra gpu memory and
runtime). secondly, existing nice registration methods are based on convolution
neural networks (cnn) and thus are limited by the intrinsic locality (i.e.,
limited receptive field) of convolution operations. transformers have been
widely adopted in many medical applications for their capabilities to capture
long-range dependency [20]. recently, transformers have also been shown to
improve registration with conventional voxelmorph [7]-like architecture
[21][22][23]. however, the benefits of using transformers for nice registration
have not been explored.in this study, we propose a non-iterative coarse-to-fine
transformer network (nice-trans) for joint affine and deformable registration.
our technical contributions are two folds: (i) we extend the existing nice
registration framework to affine registration, where multiple steps of both
affine and deformable coarse-to-fine registration are performed with a single
network in a single iteration. (ii) we explore the benefits of transformers for
nice registration, where swin transformer [24] is embedded into the nice-trans
to model long-range relevance between fixed and moving images. this is the first
deep registration method that integrates previously separated affine and
deformable coarse-to-fine registration into a single network, and this is also
the first deep registration method that exploits transformers for nice
registration. extensive experiments with seven public datasets show that our
nice-trans outperforms state-of-the-art registration methods on both
registration accuracy and runtime.",10
2464,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,3.1,Dataset and Preprocessing,"we evaluated the proposed nice-trans on the task of inter-patient brain mri
registration, which is a common benchmark task in medical image registration
studies [7][8][9][12][13][14][15][16][17][18]. we followed the dataset settings
in [18]: 2,656 brain mri images acquired from four public datasets (adni [27],
abide [28], adhd [29], and ixi [30]) were used for training; two public brain
mri datasets with anatomical segmentation (mindboggle [31] and buckner [32])
were used for validation and testing. the mindboggle dataset contains 100 mri
images and were randomly split into 50/50 images for validation/testing. the
buckner dataset contains 40 mri images and were used for testing only. in
addition to the original settings of [18], we adopted an additional public brain
mri dataset (lpba [33]) for testing, which contains 40 mri images.we performed
brain extraction and intensity normalization for each mri image with freesurfer
[32]. each image was placed at the same position via center of mass (com)
initialization [34], and then was cropped into 144 × 192 × 160 voxels.",10
2468,Non-iterative Coarse-to-Fine Transformer Networks for Joint Affine and Deformable Image Registration,4.0,Results and Discussion,"table 1 presents the registration performance of our nice-trans and all
comparison methods. the registration accuracy of all methods degraded by 1-3% in
dsc when affine registration was not performed, which demonstrates the
importance of affine registration. however, using flirt or affinenet for affine
registration incurred extra computational loads and increased the registration
runtime. our nice-trans performed joint affine and deformable registration,
which enabled it to realize affine registration with negligible additional
runtime. moreover, we suggest that integrating affine and deformable
registration into a single network also brings convenience for network training.
training two separate affine and deformable registration networks will prolong
the whole training time, while joint training will consume more gpu memory. as
for registration accuracy, the transmorph and swin-vm achieved higher dscs than
the conventional vm and difvm, but still cannot outperform the existing
cnn-based coarse-to-fine registration methods (lapirn, ulae-net, and nice-net).
our nice-trans leverages swin transformer to perform coarse-to-fine
registration, which enabled it to achieve the highest dscs among all methods.
this means that our nice-trans also has advantages on registration accuracy. we
present a qualitative comparison in the supplementary materials, which shows
that the registration result produced by our nice-trans is more consistent with
the fixed image. in addition, there usually exists a trade-off between dsc and
njd as imposing constraints on the spatial transformations limits their
flexibility, which results in degraded registration accuracy [13,18]. for
example, compared with vm, the difvm with diffeomorphic constraints achieved
better njds and worse dscs. nevertheless, our nice-trans achieved both the best
dscs and njds. we suggest that, if we set λ as 0 to maximize the registration
accuracy with the cost of transformation invertibility, our nice-trans can
achieve higher dscs and outperform the comparison methods by a larger margin
(refer to the regularization analysis in the supplementary materials).table 2
shows the results of our ablation study. swin transformer improved the
registration performance when embedded into the decoder, but had limited
benefits in the encoder. this suggests that swin transformer can benefit
registration in modeling inter-image spatial relevance while having limited
benefits in learning intra-image representations. this finding is intuitive as
image registration aims to find spatial relevance between images, instead of
finding the internal relevance within an image. under this aim, embedding
transformers in the decoder helps to capture long-range relevance between images
and improves registration performance. we noticed that previous studies gained
improvements by embedding swin transformer in the encoder [21] or leveraging a
full transformer network [22]. this is attributed to the fact that they used a
vm-like architecture that entangles image representation learning and spatial
relevance modeling throughout the whole network. our nice-trans decouples these
two parts and provides further insight on using transformers for registration:
leveraging transformers to learn intra-image relevance might not be beneficial
but merely incurs extra computational loads. it should be acknowledged that
there are a few limitations in our study. first, the experiment (table 1)
demonstrated that our nice-trans can well address the inherent misalignments
among inter-patient brain mri images, but the sensitivity of affine registration
to different degrees of misalignments is still awaiting further exploration.
second, in this study, we evaluated the nice-trans on the benchmark task of
inter-patient brain mri registration, while we believe that our nice-trans also
could apply to other image registration applications (e.g., brain tumor
registration [37]).",10
